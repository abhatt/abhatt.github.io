<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-11-26T22:35:52Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7939</id>
    <link href="https://windowsontheory.org/2020/11/25/making-tcs-more-connected-less-insular/" rel="alternate" type="text/html"/>
    <title>Making TCS more connected / less insular</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[Announcement from Jelani Nelson –Boaz] TL;DR: https://tinyurl.com/tcs-connections A task force has been convened by CATCS to investigate possibleapproaches to modifying aspects of the TCS community, especially ourpublishing culture, to enhance connections with other areas of CS andbe as welcoming as possible to a broad range of contributions withintheory. This committee will collect and synthesize feedback … <a class="more-link" href="https://windowsontheory.org/2020/11/25/making-tcs-more-connected-less-insular/">Continue reading <span class="screen-reader-text">Making TCS more connected / less insular</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Announcement from Jelani Nelson –Boaz]</em><br/><br/>TL;DR:  <a href="https://tinyurl.com/tcs-connections" rel="nofollow">https://tinyurl.com/tcs-connections</a></p>



<p/>



<p>A task force has been convened by CATCS to investigate possible<br/>approaches to modifying aspects of the TCS community, especially our<br/>publishing culture, to enhance connections with other areas of CS and<br/>be as welcoming as possible to a broad range of contributions within<br/>theory. This committee will collect and synthesize feedback from the<br/>community via the questionnaire on <a href="https://tinyurl.com/tcs-connections" rel="nofollow">https://tinyurl.com/tcs-connections</a> then make suggestions. <br/><br/>Since adjusting conference formats may help with this, we would like to get<br/>a general idea of community opinion on format choices. If you have<br/>some opinions you would like to share, please use this form. Though<br/>the questions below focus primarily on FOCS/STOC, we are welcome to<br/>receiving any and all suggestions that would make the TCS community as<br/>broad and well-connected to other areas of TCS as possible (see the<br/>last question of the survey).</p>



<p>Task force members:<br/>Ken Clarkson (IBM Research)<br/>Sandy Irani (UC Irvine)<br/>Bobby Kleinberg (Cornell)<br/>Adam Klivans (UT Austin)<br/>Ravi Kumar (Google)<br/>Jelani Nelson (UC Berkeley)<br/>Yuval Rabani (Hebrew University)</p></div>
    </content>
    <updated>2020-11-25T20:20:03Z</updated>
    <published>2020-11-25T20:20:03Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-26T22:26:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7921</id>
    <link href="https://windowsontheory.org/2020/11/24/on-galileo-galilei-and-denialism-from-elections-to-climate-to-covid/" rel="alternate" type="text/html"/>
    <title>On Galileo Galilei and “denialism” from elections to climate to COVID</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Galileo Galileo has many self-appointed intellectual heirs these days. Whether it’s a claim that the election has been stolen, that COVID-19 is less fatal than the flu, that climate change or evolution are hoaxes, or that P=NP, we keep hearing from people considering themselves as bold truth-tellers railing against conventional wisdom. We are encouraged to … <a class="more-link" href="https://windowsontheory.org/2020/11/24/on-galileo-galilei-and-denialism-from-elections-to-climate-to-covid/">Continue reading <span class="screen-reader-text">On Galileo Galilei and “denialism” from elections to climate to COVID</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Galileo Galileo has many self-appointed intellectual heirs these days. Whether it’s a claim that the <a href="https://twitter.com/KlasfeldReports/status/1331275065061761025?s=20">election has been stolen</a>, that <a href="https://statmodeling.stat.columbia.edu/2020/04/19/fatal-flaws-in-stanford-study-of-coronavirus-prevalence/">COVID-19 is less fatal than the flu</a>, that <a href="https://www.cnn.com/2020/11/24/opinions/climate-apocalypse-talking-to-skeptics-sutter-kirk/index.html">climate change</a> or <a href="https://www.pewresearch.org/fact-tank/2019/02/11/darwin-day/">evolution </a>are hoaxes, or that <a href="https://www.win.tue.nl/~gwoegi/P-versus-NP.htm">P=NP</a>, we keep hearing from people considering themselves as bold truth-tellers railing against conventional wisdom. We are encouraged to “teach the debate” and that if only paid attention, we will see that their Tweet, declaration, or arxiv paper contains an irrefutable proof of their assertions.</p>



<p>In the <a href="https://www.washingtonpost.com/news/energy-environment/wp/2015/03/26/ted-cruz-invokes-galileo-to-defend-his-climate-skepticism-and-historians-arent-happy/">words of Ted Cruz</a>, <em>“They brand you a heretic. Today, the global warming alarmists are the equivalent of the flat-Earthers. It used to be [that] it is accepted scientific wisdom the Earth is flat, and this heretic named Galileo was branded a denier”</em>. </p>



<p>Of course by Galileo’s time it was well known that the earth was spherical, and Magellan circumnavigated the earth more than 40 years before Galileo was born. But putting aside Cruz’s confusion of flat earth and geocentrism, the story of heliocentric theory is not one  of an outsider railing against the scientific mainstream. Galileo himself was a chaired professor of mathematics at the University of Padua, and later philosopher and mathematician to the grand duke of Tuscany. He was very much part of the scientific establishment of his time. Moreover, though Galileo did provide important evidence for heliocentrism, he was not the only one doing so. <a href="https://www.scientificamerican.com/article/galileo-kepler-iya/">Kepler</a> found a heliocentric model with elliptical orbits that actually made correct predictions, and, though it took a decade or so, Kepler’s book eventually became the standard textbook for astronomy.</p>



<p>My point in this post is not to rehash the history of heliocentrism or Galileo but rather to call out a misconception which, to use Sean Carrol’s phrasing, amounts to valorization of puzzle-solving over wisdom.</p>



<figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">People valorize a certain puzzle-solving kind of intelligence. And solving puzzles is important. But the ultimate goal isn't to be clever, it's to be correct. For that, knowing what information to pay attention to and what ideas to take seriously is more relevant.</p>— Sean Carroll (@seanmcarroll) <a href="https://twitter.com/seanmcarroll/status/1330573562915143681?ref_src=twsrc%5Etfw">November 22, 2020</a></blockquote></div>
</div></figure>



<p>It is tempting to think that an argument, regardless whether it comes from an expert or a random person on Twitter, can be presented in a self-contained way and judged on its merits. However, this is not how things work in any interesting setting. Even in the case of a purported P vs NP proof, there is background knowledge on computational complexity without which it would be hard to spot holes in the argument. This is doubly so for any claim involving empirical facts, whether it’s about elections, infections, climate etc. It is not possible to evaluate such claims without context, and to get this context you need to turn to the experts that have studied the topic.</p>



<p>I have written before in <a href="https://windowsontheory.org/2020/04/04/in-defense-of-expertise/">defense of expertise</a> (see also <a href="https://windowsontheory.org/2020/04/09/experts-shmexperts/">here</a>) but Carroll puts it very well. Another way to say it is that the operational interpretation of the common refrain</p>



<p><em>Extraordinary claims require extraordinary evidence </em></p>



<p>is</p>



<p><em>Treat claims conforming to conventional wisdom with charity, and claims disputing it with skepticism.</em></p>



<p>(There is a question of how to define “conventional wisdom” but interestingly there is usually agreement in practice by both sides. Most “deniers” of various sorts are proud of going against conventional wisdom, but don’t acknowledge that this means they are more likely to be wrong.)</p>



<p>As an example, even if someone has expertise in analytic number theory, and so presumably has plenty of so-called “puzzle-solving intelligence”, that doesn’t mean that they can evaluate <a href="https://liorpachter.wordpress.com/2020/11/22/williams-math-professor-investigating-voter-fraud-in-pennsylvania-finds-no-evidence/">a statistical claim</a> on election fraud and their analysis should be considered <a href="https://justthenews.com/politics-policy/elections/federal-election-commission-chairman-trump-campaign-bringing-legitimate">evidence</a> (apparently at this point the number theorist himself  <a href="https://www.berkshireeagle.com/news/local/williams-prof-disavows-own-finding-of-mishandled-gop-ballots/article_9cfd4228-2e03-11eb-b2ac-bb9c8b2bfa7f.html">agrees</a>). We can try to read and debunk what they wrote, or we can assume that if there was evidence for large-scale fraud, then the president of the United States and his well-funded campaign would have managed to find actual statisticians and experts on election to make the case.</p>



<p>There can be debate if Trump’s attempt to overthrow the election should be considered as dangerous or merely absurd, but the constant attacks on the very notions of truth, science, and expertise are causing far-reaching harm. </p>



<p>(H/T:<a href="https://www.scottaaronson.com/blog/?p=5106"> Scott Aaronson</a>, who makes a similar point around the election conspiracies.)</p></div>
    </content>
    <updated>2020-11-24T22:30:57Z</updated>
    <published>2020-11-24T22:30:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-26T22:26:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7933</id>
    <link href="https://windowsontheory.org/2020/11/24/announcing-the-wiml-t-mentorship-program-guest-post/" rel="alternate" type="text/html"/>
    <title>Announcing the WiML-T Mentorship Program (guest post)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[Guest post by Claire Vernade, Jessica Sorrell, Kamalika Chaudhuri, Lee Cohen, Mary Anne Smart, Michal Moshkovitz, and Ruth Urner. I am very happy about this initiative – mentoring and community is so important for success in science, and as I’ve written before, there is much work to do so women will have the same access … <a class="more-link" href="https://windowsontheory.org/2020/11/24/announcing-the-wiml-t-mentorship-program-guest-post/">Continue reading <span class="screen-reader-text">Announcing the WiML-T Mentorship Program (guest post)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Claire Vernade, Jessica Sorrell, Kamalika Chaudhuri, Lee Cohen, Mary Anne Smart, Michal Moshkovitz, and Ruth Urner. </em><br/><em><br/>I am very happy about this initiative – mentoring and community is so important for success in science, and <a href="https://windowsontheory.org/2017/08/16/men-in-computer-science/">as I’ve written before</a>, there is much work to do so women will have the same access to these as men. –Boaz]</em></p>



<p/>



<p><strong>TL;DR:</strong> we are organizing a new mentorship program for women in machine learning. Please consider applying as a mentor, mentee, or both at <a href="https://www.wiml-t.org/mentoring-program">https://www.wiml-t.org/mentoring-program</a>. </p>



<h1>What is WIML-T?</h1>



<p><a href="https://wimlworkshop.org/">Women in machine learning </a>(or WiML for short) was established more than ten years ago, and its main goals are to <strong>1.</strong> Increase the number of women in machine learning <strong>2.</strong> Help women in machine learning succeed professionally <strong>3. </strong>Increase the impact of women in machine learning in the community. Towards this goal, they create different opportunities for women to showcase their work. Chief among them is the annual Women in Machine Learning (WiML) Workshop, typically co-located with NeurIPS, which presents women’s cutting-edge research. </p>



<p><a href="https://www.wiml-t.org/home">Women in machine learning theory</a> (or WiML-T for short) shares the same goals as WiML but focuses on the smaller learning theory community. The vision of WiML-T is to give visibility and legitimacy to under-represented groups in learning theory, to create stronger bonds within the community and beyond, and to provide support and advice. As part of this vision, we have decided to facilitate a mentoring program that will connect women and non-binary researchers who are newer to learning theory with more experienced mentors.  </p>



<h1>Why mentorship?</h1>



<p>Mentoring programs have been shown to greatly help underrepresented communities develop [1]. More resources on mentoring can be found on the website of Stanford’s Women’s Leadership Innovation Lab and on <a href="http://www.wiml-t.org">our website</a>. Mentoring creates strong bonds of trust within the community, it helps mentees find career advice and connections, and it helps mentors grow their leadership skills while keeping in touch with the newcomers of the community.  We hope that creating a world-wide program will also help reduce inequality between less privileged areas and the most famous institutions. Indeed, it is a well-known fact that the more competitive a career path, the less diverse it is, due in part to network effects from which under-represented groups are excluded. We believe that uniting individuals from these groups (i.e. women, non-binary people, persons of color and other minorities) will help reduce these effects and contribute to finding solutions to the community’s problems. </p>



<p><strong>Why be a mentor?  </strong></p>



<p>Remember the beginning of your research journey, with all the difficulties, uncertainties, and unanswered questions? This is your chance to give all the advice you wish you got, and make an impact on a future colleague. As a mentor you will be a role model and help the young generation of researchers in learning theory. You will help them develop and enhance their careers by giving them the support they need. Need more reasons to mentor? It will grow your leadership skills, self-confidence, communication skills, and you will feel happier after you help others. </p>



<p><strong>Who can be a mentor?</strong> </p>



<p>Nearly everyone who has some experience in academia or industry can be a mentor. It can be interesting for an undergrad student to receive advice from senior PhD students or postdocs who have recently had to reflect about career decisions and can share knowledge about their work environment. We indeed expect the most senior researchers to apply as mentors, but we would also like to encourage PhDs and postdocs to consider mentoring (while possibly having a mentor as well!).    </p>



<p><strong>Can men mentor? </strong></p>



<p>We thank everyone who wants to help the community!  </p>



<p>We will prioritize women mentors as they can give their unique perspective, BUT, we acknowledge that there might be a limited number of mentors. To mitigate this issue, we will be happy to pair male mentors provided the mentee agrees.</p>



<p><strong>Why be a mentee?</strong>   </p>



<p>Having a mentor is one of the best ways to get external career advice, to get some feedback from someone with a possibly similar background. Managing to find one’s way into academia or science is not easy. It can be even harder for under-represented groups who may lack role models within their institution, or who may not connect with common advice that implicitly assumes or relies on some class privilege. Having a mentor can help you navigate professional and personal issues that men may not always have. It is also a way to get connected to other members of the community, or have second opinions on research strategies.   </p>



<h1>Timeline</h1>



<ul><li>The program launched on October 29 2020 and will run on a continuous basis.</li><li>We will start with pairings of mentors and mentees in December 2020. This process can take a few months.  </li><li>Frequency of the meetings: totally depends on the mentor and mentee. It can be either weekly meetings or once every two months or anything in between. </li><li>Duration of the mentorship: totally depends on the mentor and mentee. It can be a few months, a year, or even more. </li></ul>



<p>Have questions? You can mail us at: <strong>contact@wiml-t.org</strong></p>



<p>[1] Ginther, D. K., Currie, J., Blau, F. D., &amp; Croson, R. (2020). Can Mentoring Help Female Assistant Professors in Economics? An Evaluation by Randomized Trial. <em>NBER Working Paper No. 26864</em>. <a href="https://doi.org/10.3386/w26864&#xA0;" rel="nofollow">https://doi.org/10.3386/w26864 </a>    </p></div>
    </content>
    <updated>2020-11-24T17:53:30Z</updated>
    <published>2020-11-24T17:53:30Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-26T22:26:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=80</id>
    <link href="https://dstheory.wordpress.com/2020/11/24/friday-dec-04-adam-smith-from-boston-university/" rel="alternate" type="text/html"/>
    <title>Friday Dec 04 — Adam Smith from Boston University</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Friday, Dec 04th at 10:00 AM Pacific Time (13:00 Eastern Time, 18:00 Central European Time, 17:00 UTC, 23:30 Indian Time).  Adam Smith from Boston University will speak about “When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning?”. Abstract: Modern machine learning models<a class="more-link" href="https://dstheory.wordpress.com/2020/11/24/friday-dec-04-adam-smith-from-boston-university/">Continue reading <span class="screen-reader-text">"Friday Dec 04 — Adam Smith from Boston University"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next Foundations of Data Science virtual talk will take place on <strong>Friday, Dec 04</strong>th at <strong>10:00 AM Pacific Time</strong> (13:00 Eastern Time, 18:00 Central European Time, 17:00 UTC, 23:30 Indian Time).  <strong>Adam Smith</strong> from <strong>Boston University</strong> will speak about “<strong>When is Memorization of Irrelevant Training Data Necessary for High-Accuracy Learning?</strong>”.</p>



<p><strong>Abstract</strong>: Modern machine learning models are complex, and frequently encode surprising amounts of information about individual inputs. In extreme cases, complex models appear to memorize entire input examples, including seemingly irrelevant information (social security numbers from text, for example). In this paper, we aim to understand whether this sort of memorization is necessary for accurate learning. We describe natural prediction problems in which every sufficiently accurate training algorithm must encode, in the prediction model, essentially all the information about a large subset of its training examples. This remains true even when the examples are high-dimensional and have entropy much higher than the sample size, and even when most of that information is ultimately irrelevant to the task at hand. Further, our results do not depend on the training algorithm or the class of models used for learning.</p>



<p>Our problems are simple and fairly natural variants of the next-symbol prediction and the cluster labeling tasks. These tasks can be seen as abstractions of image- and text-related prediction problems. To establish our results, we reduce from a family of one-way communication problems for which we prove new information complexity lower bounds.</p>



<p>Joint work with Gavin Brown, Mark Bun, Vitaly Feldman, and Kunal Talwar.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-11-24T17:46:10Z</updated>
    <published>2020-11-24T17:46:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-11-26T22:33:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://mycqstate.wordpress.com/?p=1271</id>
    <link href="https://mycqstate.wordpress.com/2020/11/22/what-it-is-that-we-do/" rel="alternate" type="text/html"/>
    <title>What it is that we do</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post is a follow-up on some somewhat off-hand comments that I made earlier regarding the notion of truth in a “proof-based” discipline such as pure mathematics or theoretical computer science. Since the former is easier to circumscribe and also … <a href="https://mycqstate.wordpress.com/2020/11/22/what-it-is-that-we-do/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post is a follow-up on some somewhat off-hand comments that I made <a href="https://mycqstate.wordpress.com/2020/09/29/it-happens-to-everyonebut-its-not-fun/">earlier</a> regarding the notion of truth in a “proof-based” discipline such as pure mathematics or theoretical computer science. Since the former is easier to circumscribe and also has a larger literature available on it, for the purposes of the post I will discuss my emerging take on truth in mathematics; what I say applies to TCS as well. (I wasn’t able to find satisfactory writings on the practice of computer science, even more broadly interpreted than just “theory”; any pointers on this are very welcome.) I obviously don’t claim any originality here; I suspect that to some the points I make might be interesting while to others they could feel passé–in the latter case, please help me make progress in the comments!</p>



<div class="wp-block-image"><figure class="alignright size-large is-resized"><a href="https://mycqstate.files.wordpress.com/2020/11/circle.jpg"><img alt="" class="wp-image-1273" height="272" src="https://mycqstate.files.wordpress.com/2020/11/circle.jpg?w=762" width="261"/></a>A <em>circle</em> is a plane figure contained by one line such that all the straight lines falling upon it from one point among those lying within the figure equal one another. (Euclid, Elements, Book I, Definition 15.)</figure></div>



<p>The question is more interesting than it may seem. First, let’s set aside the Platonician answer that mathematics is about proving statements that apply to abstract objects. None of this makes sense: there is no such thing as an abstract object (What is an “abstract circle?” Euclid’s <em>definition</em> of an abstract circle exists, but that definition has nothing abstract; it is plain English—or well, Greek, which is no better). Neither is there anything fundamentally “true” about the statements that mathematicians make about these objects. This last point is made clear in Voevodsky’s retelling of the story that brought him to start his research program on univalent foundations: see in particular the paragraph starting with “The primary challenge that needed to be addressed” <a href="https://www.ias.edu/ideas/2014/voevodsky-origins">here</a>. While a subset of mathematicians has been carefully examining the logical foundations of mathematics and pursuing a program of formalizing these foundations with the goal of automating verification, two facts are historically evident: (1) this formalization has always come <em>after</em> the fact, i.e. first the theorem is proven, and second only is there some attempt at modifying the format of the “proof” so as to reach some abstract “higher standard”; (2) what the higher standard exactly is is a moving target. In other words, mathematicians do whatever it is that they do <em>first</em>, and only <em>after</em> that does some subset of them look back on what has been done and make an attempt to reformulate it in a way that seems more “rigorous” by the standards of the day. And even once this process has been completed (or rather, iterated over a few times), no absolute truth has been obtained; merely, one has gained higher confidence that some set of formal rules can be applied in a certain order to derive one statement for another. In summary, the logical answer “I define as true any statement that can be derived by said rules starting from said axioms” will not satisfy us. (My point is not to criticize this enterprise; only to point that this is not what mathematics is about. I don’t expect any mathematician would disagree.)</p>



<p>Once this point has been cleared we have opened up the possibility for a much richer interpretation of what mathematics is about, what it is that mathematicians do, and what is so special about the role played by the notion of truth in this enterprise.</p>



<h2>The Greeks</h2>



<p>Mathematicians have been around since Greek antiquity. I highly recommend the wonderful book <a href="https://doi.org/10.1017/CBO9780511543296">The Shaping of Deduction in Greek Mathematics</a> by Reviel Netz, a historian at Stanford. The book provides a fantastic historical introduction to the birth of modern mathematics, as practiced in the Western world. In this book Netz gives a detailed account of the practice of Greek mathematics that is based on a sometimes almost comical literal reading of the original texts. Netz is not a mathematician, and he takes the Greek writings at face value, without preconceptions as to any deeper meaning, mathematical or otherwise. To see how fantastic an enterprise this might be I highly recommend taking the time to read through a few paragraphs of Euclid’s elements, see e.g. <a href="https://www.claymath.org/library/historical/euclid/index.html">here</a>. (I wasn’t able to find an online version that has clear figures; Netz’ book contains many reproductions and he spends a great deal of time examining their significance as a companion to the text.) Wait, and they called this a <em>proof</em>? </p>



<div class="wp-block-image"><figure class="alignleft size-large is-resized"><a href="https://mycqstate.files.wordpress.com/2020/11/phil-2017-0007_05.jpg"><img alt="" class="wp-image-1276" height="241" src="https://mycqstate.files.wordpress.com/2020/11/phil-2017-0007_05.jpg?w=1024" width="320"/></a>Netz places a particular emphasis on the role of lettered diagrams such as this one in Greek mathematicians’ proofs. This one is taken from Apollonius, as reproduced in <a href="https://doi.org/10.1515/phil-2017-0007">this book</a>.</figure></div>



<p>What I find most amazing about Netz’ work is that his book makes it absolutely clear that what Greek mathematics did is invent a very special world, with its own set of rules and specifications as to how things should be presented, what counts as valid and what does not, etc; most importantly this set of rules and specifications is absolutely <em>arbitrary</em>. There is nothing <em>universally</em> true about any of the statements made in Euclid’s book ; what there is however, and which is just as important, is a form of <em>self-consistent</em>, <em>self-perpetuating</em> consistency. This is the Greek mathematician’s most important discovery, and most enduring legacy: a system of thought, based on the use of formal rules, such that users of that system of thought can easily and unequivocally agree on the same statements. Netz contrasts this with the political discourse that the Greeks were so famously fond of; while in rhetoric endless arguments can be made in favor or against any given statement, in mathematics once an argument has been made there is no discussing it; either the argument is correct and accepted by all or it is flawed and all will agree on the presence of a flaw. By saying that mathematics does not establish universal truths we do not take away any of the strength of its evident success as a system of thought. </p>



<p>I insist that in the previous sentence by “mathematics” I mean “Greek mathematics.” There is nothing universal about the formal “proof system” used by the Greeks, and in fact that system is arguably quite different from the one used today (contrast a proof in Euclid’s Elements from one in e.g. Kerodon (see figure below); clearly either school would reject the other’s papers!). There is nothing absolutely perfect or even fool-proof about it either; when I write that “either the argument is correct or it is flawed” I do not dismiss the existence of a substantial grey zone in which there might be, and generally is, discussion. (Netz in his presentation makes it plainly obvious that Greek proofs had many “gaps” and that completely arbitrary choices are made as to what is valid and what is not.) The point is that this grey zone is by orders of magnitude smaller than in other disciplines, and this is what makes mathematics special.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><a href="https://mycqstate.files.wordpress.com/2020/11/diagram.jpg"><img alt="" class="wp-image-1278" height="118" src="https://mycqstate.files.wordpress.com/2020/11/diagram.jpg?w=943" width="470"/></a>A lettered diagram taken from the proof of Lemma 3.5.3.11 in <a href="https://kerodon.net/">Kerodon</a>.</figure></div>



<p>Greek mathematics has its limitations. In particular, it seems like they were not able to expand their explorations much beyond geometry; possibly because they did not have a formal way of re-using proofs. Indeed, only propositions from Euclid’s elements were part of the general corpus of “truths” that could be used without justification; any other proof could only appeal to either a statement from the Elements or a statement proved within the same text (as opposed to a statement made by another mathematician). The current system has a much higher level of re-usability, allowing it to much deeper. It is possible that current attempt at formalization will provide yet another layer to this by enabling re-using statements without the mathematician herself even being cognizant of the statement that is being re-used (e.g. the proof assistant would figure out that there is some theorem somewhere that could be useful). </p>



<h2>The moderns</h2>



<div class="wp-block-image"><figure class="alignright size-large is-resized"><a href="https://mycqstate.files.wordpress.com/2020/11/qkd.jpg"><img alt="" class="wp-image-1280" height="420" src="https://mycqstate.files.wordpress.com/2020/11/qkd.jpg?w=831" width="382"/></a>A lettered diagram taken from the proof of security of quantum key distribution in Renner’s <a href="https://arxiv.org/abs/quant-ph/0512258">Ph.D. thesis</a>.</figure></div>



<p>Once one has taken this somewhat distanced look at the practice of Greek mathematicians it becomes clear that there is no reason for contemporary practice to escape similar distanciation. As arbitrary as the rules that govern the former might seem now, just as arbitrary our own rules will seem later. The observation prompts us to re-evaluate the notion of proof and truth as follows: <em>The goal of the mathematical proof is to convince the mathematician; a statement is true whenever the mathematician is convinced.</em> I emphasize that the mathematician in this sentence is a human being, not a machine. <br/><br/>Propositions in Euclid’s elements are true because Euclid’s text convinced all other Greek mathematicians that they were true; the same hold of Voevodsky’s univalent foundations. To speak about my personal experience, the point was carried home vividly about a decade ago at the end of an entire afternoon spent sitting by the sea nearby Marseille in the South of France together with an illustrious French operator algebraist. For the whole afternoon I had been trying to explain, of all things, the Clause-Horne-Shimony-Holt inequality formulated as a nonlocal game—an elementary construction on which half of my research is based (see <a href="https://mycqstate.wordpress.com/2012/10/10/tsirelsons-bound/">this post</a> for example, although we  certainly didn’t get that far). To no avail! Alice, Bob, games, the mathematician would have none of it. How could it be so hard? The message I was trying to carry across is bread and butter to quantum information theorists; moreover the language of functional analysis was not foreign to me either and so I believed I ought to have more than it should take to make the explanations clear; in my mind I had a clear theorem, together with an airtight proof, to communicate. Not so: I ended the afternoon defeated, and the point hammered in: as much as we may think that our language is formal, our reasoning rational, our goals clear, as soon as we step out of our self-referential cocoon we have to face the evidence: not so; there is nothing clear, formal, self-evident, in the mathematical truths we take as such. (If you’re not convinced yet, look at the pictures.)</p>



<p>After having read through the viewpoint of a historian on Greek mathematics, it is interesting to come back to the present and read up on the modern mathematician’s own account of their practice. Having delighted in Hardy’s <a href="https://en.wikipedia.org/wiki/A_Mathematician%27s_Apology">A Mathematician’s Apology</a> in my student years I was naturally attracted to Harris’ explicitly referential book <a href="https://press.princeton.edu/books/hardcover/9780691154237/mathematics-without-apologies">Mathematics Without Apologies</a>. The book aims to provide a personal answer to precisely the same question as Netz’ (“What do pure mathematicians do, and why do they do it?”—first line on back cover), with the essential difference that Harris’ book is written by a contemporary mathematician about the practice of modern mathematics. The perspective taken is thus very different: Harris does not waste a drop of ink to examine the material products of modern mathematics (e.g. printed articles), which naturally from his point of view do not have the least interest besides their mathematical significance; nor does he question the formal system that enables mathematics (he does discuss foundation issues, but these are themselves part of the formal system). </p>



<p>Harris delights in telling us stories about the practice of mathematics from the inside: the way one acquires prestige (he calls it “charisma”), how mathematicians perceive when a mathematical statement is interesting, what drives a mathematician to work on one problem or another, the influence of large research programs such as the Langlands program, etc. Just as Netz, Harris agrees that the proof itself, while an important product of the mathematician’s practice, is not a goal in itself. The goal is to introduce new objects, find relations between them, and build structures that support this exploration (Harris quotes extensively from Grothendieck, a builder of structures if there is any). Crucially the mathematician’s goal is not at all to find <em>truth</em>: her goal is to find <em>beauty</em>. Harris (un-apologetically!) defends the mathematician’s position as playing a privileged and essential role in our (Western) society: mathematics is the rare, if not unique, discipline where thought is valued <em>for itself</em>: not for its consequences or potential applications, nor even for some kind of universal validity or truth that it might reach — for itself, the sheer beauty of it. No apologies. While one might think that the arts are in a similar position, Harris points out that in contemporary discussions about art the notion of beauty has all but disappeared; instead, one talks of society, ecology, politics—all of this is good and important, but it is not about beauty. Mathematics is about exploring the beauty of human thought. This exploration is carried out within the very special, narrow and arbitrary corner that mathematical practice has delineated for itself since the Greeks got us started. And yet does not matter that it is narrow and arbitrary; what matters is that it is purely and uniquely human.</p>



<p>I don’t have a conclusion to give to this post. I suppose it is the kind of digression that one is naturally inclined to make while on <a href="https://mycqstate.wordpress.com/2020/09/20/announcing-a-short-course-in-paris/">sabbatical</a>. (I was delighted to see Harris reference his time at the Institut Henri Poincaré in Paris extensively and spend multiple pages gossiping about Ed Frenkel, then a holder of the same FSMP chair I am now occupying.) Certainly it helps to gain a sense of what it is that one is doing. While these explorations destroyed a number of simple comforting myths about what it is that I do every day, in the end I find the void that lies beneath the surface much more appealing; I feel privileged and comforted in my desire to make use of that privilege.</p>



<h2>Post-scriptum</h2>



<p>In addition to the two books referenced in the post there are a couple articles that I found helpful. I am listing them here because I found such writings non-trivial to come by, and so they might be helpful to anyone interested in the topic. I welcome additional references. </p>



<ul><li>Barton, <a href="https://link.springer.com/article/10.1007/s11858-999-0009-7">Ethnomathematics and Philosophy</a>. This is Barton’s Ph.D. thesis, in which he studies the social emergence of mathematics not as inevitable, but as cultural, yet without negating what is so special about it. .</li><li>Wallet and Neuwirth, <a href="https://hal.archives-ouvertes.fr/hal-01943079/">Enquete sur les modes d’existence des etres mathematiques</a> (unfortunately in French only it seems). The authors build on Netz’ work and others to examine the notion of mathematical proof within Latour’s framework of “modes of existence,” which Latour mostly applied to experimental sciences. </li></ul></div>
    </content>
    <updated>2020-11-22T16:14:00Z</updated>
    <published>2020-11-22T16:14:00Z</published>
    <category term="meta"/>
    <category term="Science"/>
    <category term="Uncategorized"/>
    <category term="meta-mathematics"/>
    <author>
      <name>Thomas</name>
    </author>
    <source>
      <id>https://mycqstate.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://mycqstate.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://mycqstate.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://mycqstate.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://mycqstate.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>In superposition</subtitle>
      <title>MyCQstate</title>
      <updated>2020-11-26T22:30:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=514</id>
    <link href="https://tcsplus.wordpress.com/2020/11/20/the-coin-problem-with-applications-to-data-streams/" rel="alternate" type="text/html"/>
    <title>The Coin Problem with Applications to Data Streams</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, November 25th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Sumegha Garg from Harvard University will speak about “The Coin Problem with Applications to Data Streams” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, November 25th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Sumegha Garg</strong> from Harvard University will speak about “<em>The Coin Problem with Applications to Data Streams</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: Consider the problem of computing the majority of a stream of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n"/> i.i.d. uniformly random bits. This problem, known as the coin problem, is central to a number of counting problems in different data stream models. We show that any streaming algorithm for solving this problem with large constant advantage (over the uniform distribution) must use <img alt="\Omega(\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Clog+n%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\Omega(\log n)"/> bits of space. Previously, it was known that computing the majority on every input with a constant probability takes <img alt="\Omega(\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Clog+n%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\Omega(\log n)"/> space. We extend our lower bound to proving tight lower bounds for solving multiple, randomly interleaved copies of the coin problem, as well as for solving the OR of multiple copies of a variant of the coin problem. Our proofs involve new measures of information complexity that are well-suited for data streams.<br/><br/>We use these lower bounds to obtain a number of new results for data streams. In each case there is an underlying <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="d"/>-dimensional vector <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="x"/> with additive updates to its coordinates given in a stream of length <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="m"/>. The input streams arising from our coin lower bound have nice distributional properties, and consequently for many problems for which we only had lower bounds in general turnstile streams, we now obtain the same lower bounds in more natural models, such as the bounded deletion model, in which <img alt="\|x\|_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Cx%5C%7C_2&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\|x\|_2"/> never drops by a constant fraction of what it was earlier, or in the random order model, in which the updates are ordered randomly. <br/><br/>Based on joint work with Mark Braverman and David P. Woodruff.</p></div>
    </content>
    <updated>2020-11-20T15:00:57Z</updated>
    <published>2020-11-20T15:00:57Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-26T22:27:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7906</id>
    <link href="https://windowsontheory.org/2020/11/19/updated-research-masters-programs-database-by-aviad-rubinstein-and-matt-weinberg/" rel="alternate" type="text/html"/>
    <title>Updated Research Masters programs database by Aviad Rubinstein and Matt Weinberg</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Guest post by Aviad Rubinstein and Matt Weinberg As explained in Boaz’s previous posts [1] [2], the PhD admission process can be challenging for students who discover their passion for Theory of Computer Science late in their undergraduate studies. Discovering TCS earlier is especially challenging for students who aren’t exposed to CS in high school, … <a class="more-link" href="https://windowsontheory.org/2020/11/19/updated-research-masters-programs-database-by-aviad-rubinstein-and-matt-weinberg/">Continue reading <span class="screen-reader-text">Updated Research Masters programs database by Aviad Rubinstein and Matt Weinberg</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Guest post by <a href="https://cs.stanford.edu/~aviad/">Aviad Rubinstein</a> and <a href="https://www.cs.princeton.edu/~smattw/">Matt Weinberg</a></p>



<p>As explained in Boaz’s previous posts <a href="https://windowsontheory.org/2018/02/20/research-masters/">[1]</a> <a href="https://windowsontheory.org/2020/07/02/crowdsourcing-masters-program/">[2]</a>, the PhD admission process can be challenging for students who discover their passion for Theory of Computer Science late in their undergraduate studies. Discovering TCS earlier is especially challenging for students who aren’t exposed to CS in high school, and this bias aggravates the diversity issues we have in our community. Masters programs are one way to mitigate this issue.<br/>On a personal note, one of us (Aviad), worked half-time during undergraduate and would not have been in academia today -let alone TCS of all subjects- if it weren’t for the awesome TCS masters program at Tel-Aviv University.</p>



<p><br/>But where would you go (or send your students) to do a masters in TCS?A little over a year ago, we were discussing how useful it would be to have a unified resource that can help students choose. We decided that we should create one! Months passed, and right before we ran out of excuses to procrastinate, Boaz made this <a href="https://windowsontheory.org/2020/07/02/crowdsourcing-masters-program/">post</a> crowdsourcing information on TCS masters programs.<br/>After some more procrastination, we eventually did send a lot of emails and tried to clean and organize the information to the best of our ability. Thanks to everyone who contributed! You can find the latest version <a href="https://www.cs.princeton.edu/~smattw/masters/masters.html">here</a>. (Short url: <a href="http://tiny.cc/tcsmasters">http://tiny.cc/tcsmasters</a>)</p>



<p>Now you should share it with your brightest students!</p>



<p><br/>This is also meant to be a live project. Please email aviad [at] <a href="http://cs.stanford.edu/" rel="noreferrer noopener" target="_blank">cs.stanford.edu</a> if you have more new information or find any inaccuracies.</p>



<p>Thanks!</p>



<p>-Aviad and Matt</p></div>
    </content>
    <updated>2020-11-19T14:40:09Z</updated>
    <published>2020-11-19T14:40:09Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-26T22:26:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7895</id>
    <link href="https://windowsontheory.org/2020/11/17/election-insecurity/" rel="alternate" type="text/html"/>
    <title>Election insecurity</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Election security has been studied for many years by computer scientists, but it is not as often that it attracts so much mainstream attention. I would never have expected to see my former Princeton colleague Andrew Appel on a Sean Hannity segment tweeted by President Trump. It may seem that even if it has partisan … <a class="more-link" href="https://windowsontheory.org/2020/11/17/election-insecurity/">Continue reading <span class="screen-reader-text">Election insecurity</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Election security has been studied for many years by <a href="https://www.quantamagazine.org/rsa-cryptographer-ronald-rivest-seeks-secure-elections-20200312/">computer scientists</a>, but it is not as often that it attracts so much mainstream attention. I would never have expected to see my former Princeton colleague <a href="https://www.cs.princeton.edu/~appel/">Andrew Appel</a> on a Sean Hannity segment tweeted by President Trump.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/11/image.png"><img alt="" class="wp-image-7899" src="https://windowsontheory.files.wordpress.com/2020/11/image.png?w=1024"/></a></figure>



<p/>



<p>It may seem that even if it has partisan motivations, the recent GOP interest in election security is overall a positive thing. Who wouldn’t want elections to be more secure? Who wouldn’t want less fraud? However, in a very precise sense, the definition of “election security” used by the GOP these days corresponds to <strong>election insecurity</strong>.</p>



<p>To understand this claim, consider what it means for an election to be secure. (Let’s focus just on the correctness aspect of the count, since it is at the heart of the current issues, and not on the very interesting privacy aspect.) Computer scientists use <a href="https://arxiv.org/abs/1605.08554">the technical terms</a> “<strong>cast as intended</strong>“, “<strong>recorded as cast</strong>“, and “<strong>tallied as recorded</strong>“. In other words: if a voter X intends to cast vote for candidate Y,  then this vote should be recorded and tallied, and only such votes should be tallied.</p>



<p>With mail-in voting, there are several potential points of failure on the path between voter intent and the final tally:</p>



<ol><li>Mail can be lost or delayed too much, leading to the vote not counting.</li><li>A third party could intercept the ballot and impersonate the voter.</li><li>A ballot may not be formatted properly in some way, leading to it being disqualified.</li><li>There can be errors or hacks in the tallying process.</li></ol>



<p>Election security is about combatting points 1-4 (of which the last 3 are also applicable to in-person voting) , ideally in a way that is <em>verifiable</em> to the individual voters. Achieving verifiability while maintaining secrecy and not requiring the voter to trust complex technology is a challenging task, but there have been some proposed solutions (see above links).<br/><br/>The Hannity segment and much of the “Dominion” non story focused on point 4. This is an important point, but as <a href="https://freedom-to-tinker.com/2020/11/13/did-sean-hannity-misquote-me/">Appel himself notes</a>, paper ballots, which are mostly used in the US, serve as a way to audit counting. Re-counting is important, and is commonly done, but such recounts often change the total counts by relatively little (and the changes mostly cancel out). For example, here is the list of ballots changed and reasons from the Wisconsin 2016 count (taken from <a href="https://t.co/AUagggIp6j?amp=1">this paper</a>)</p>



<figure class="wp-block-image"><img alt="Image" src="https://pbs.twimg.com/media/Emk1EJaU4AAtAJm?format=jpg&amp;name=4096x4096"/></figure>



<p>In contrast, many of the legal cases by the Trump campaign focused on signature verification and other ballot irregularities. There are two main reasons why a signature would not match between a ballot and driver’s license or other records:</p>



<ol><li>The signature may have been forged by someone trying to impersonate the voter.<br/></li><li>The voter’s signature might not very consistent, or maybe they have more than one signature (for example, I sometimes sign in Hebrew and sometimes in English) .</li></ol>



<p>Empirically, reason 2 is much more common than reason 1. If a ballot is tossed out because of the second reason it corresponds to a <strong>break between the voter intent and the final tally</strong>, and hence it is a case of <em>election insecurity</em>. For this reason, making more stringent signature checks could make elections less secure!</p>



<p>While President Trump might claim on Twitter that the election was stolen by a massive conspiracy involving forging of tens of thousands of ballots, this is not the actual content of the court cases (especially after some <a href="https://lawandcrime.com/2020-election/lawyers-pan-significant-shrinkage-of-trump-campaigns-pennsylvania-lawsuit-only-addresses-a-handful-of-ballots/">recent</a> <a href="https://www.scribd.com/document/484456818/Redline-Trump-Complaint">amendments</a>). For example, at the heart of the PA case is the process of “<a href="https://www.factcheck.org/2020/11/ballot-curing-in-pennsylvania/">curing a ballot</a>“. This is when a ballot is disqualified due to some technical issue, and a voter has a chance to fix it. Curing a ballot ensures that the voters intent is captured, and hence makes elections more secure. </p>



<p>In PA, the decision of whether to notify voters in such cases was left to the counties, and apparently Democrat-controlled counties were more likely to do so Republican-controlled counties. This is a shame, and had the Trump campaign asked to extend the deadline for curing  ballots, then I would think it makes perfect sense. However, this is not what their lawsuit is about. To quote their complaint: <em>“plaintiffs seek a permanent injunction requiring the County Election Boards to invalidate ballots cast by voters who were notified and given an opportunity to cure their invalidly cast mail-in ballot.”</em> This are ballots where there is no question of the eligibility of the voter, nor of the accuracy of their intent, yet the Trump campaign seeks to prevent them from counting. I call this election insecurity.</p>



<p/>



<p>p.s. See <a href="https://twitter.com/KlasfeldReports">Adam Klasfeld’s feed</a> for more about the various Trump campaign cases</p>



<figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">The DNC's lawyer Uzoma Nkwonta, from Perkins Coie, is up. <br/><br/>He reiterates a point made earlier by the counties that Trump's co-plaintiffs sued the wrong counties.<br/><br/>They're from Trump-backing Fayette and Lancaster Counties. <br/><br/>They sued seven blue counties, alleging vote denial.</p>— Adam Klasfeld (@KlasfeldReports) <a href="https://twitter.com/KlasfeldReports/status/1328823948319076355?ref_src=twsrc%5Etfw">November 17, 2020</a></blockquote></div>
</div></figure></div>
    </content>
    <updated>2020-11-17T22:31:11Z</updated>
    <published>2020-11-17T22:31:11Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-26T22:26:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1802</id>
    <link href="https://theorydish.blog/2020/11/16/hiring-postdocs/" rel="alternate" type="text/html"/>
    <title>Hiring Postdocs</title>
    <summary>I am looking to hire postdocs in the coming years on topics of algorithmic fairness with relation to a Simons Collaboration and on the meaning of individual probabilities with relation to a Sloan Foundation project. Please contact me (reingold@stanford.edu) for details and please forward to any relevant individual. Some comments: The Simons Collaboration will offer additional postdoc opportunities across the participating institutions. To be advertised soon. The perspective taken in both these projects is of TOC but the research will gain from collaborations with other fields within and outside of CS (Statistics, Economics, Philosophy, Law, Social Sciences at large). I will therefore be open to postdocs with various backgrounds (and would consider co-hosting with colleagues in other fields). Candidates from under-represented populations are encouraged to apply and I promise to give those applications my close attention.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am looking to hire postdocs in the coming years on topics of algorithmic fairness with relation to a <a href="https://www.simonsfoundation.org/2020/06/18/foundation-announces-simons-collaboration-on-the-theory-of-algorithmic-fairness/">Simons Collaboration</a> and on the meaning of individual probabilities with relation to a <a href="https://sloan.org/">Sloan Foundation</a> project. Please contact me (reingold@stanford.edu) for details and please forward to any relevant individual. Some comments:</p>



<ol type="1"><li>The Simons Collaboration will offer additional postdoc opportunities across the participating institutions. To be advertised soon.</li></ol>



<ul><li>The perspective taken in both these projects is of TOC but the research will gain from collaborations with other fields within and outside of CS (Statistics, Economics, Philosophy, Law, Social Sciences at large). I will therefore be open to postdocs with various backgrounds (and would consider co-hosting with colleagues in other fields).</li></ul>



<ul><li>Candidates from under-represented populations are encouraged to apply and I promise to give those applications my close attention.</li></ul></div>
    </content>
    <updated>2020-11-16T17:39:07Z</updated>
    <published>2020-11-16T17:39:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-11-26T22:28:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=797</id>
    <link href="https://emanueleviola.wordpress.com/2020/11/15/marijuana-brings-good-to-your-community/" rel="alternate" type="text/html"/>
    <title>Marijuana brings good to your community</title>
    <summary>Subsequence of official communication from Newton: […] recently reviewed applications from […] marijuana retailers who have submitted proposals […]. […] has a diverse management team with experience in the cannabis industry, equity, community relations and public health. […] certified by the Cannabis Control Commission as an Economic Empowerment Applicant, signifying that the applicant demonstrates experience […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Subsequence of official communication from Newton:</p>



<figure class="wp-block-table"><table><tbody><tr><td><br/>[…] recently reviewed applications from […] marijuana retailers who have submitted proposals […].<br/><br/>[…] has a diverse management team with experience in the cannabis industry, equity, community relations and public health. […] certified by the Cannabis Control Commission as an Economic Empowerment Applicant, signifying that the applicant demonstrates experience in or business practices that promote economic empowerment in disproportionately impacted communities. Their plan involves […] additional parking on-site.<br/><br/>[…] also signing […] is a family owned and operated business. The company is certified by the Massachusetts Supplier Diversity Office as both a Minority Business and a Women Business Enterprise. […] will renovate the building and create surface parking. It has indicated it is willing to make transportation infrastructure improvements to the intersection […].<br/><br/>I am also signing […]. […] brings experience in the cannabis industry along with commitments to equity, public health, and community relations. […] will be constructing a new building and parking on what is currently a vacant parcel. <br/><br/>[…] with a revised plan to […] include more parking.<br/><br/></td></tr></tbody></table></figure>



<figure class="wp-block-table"><table><tbody><tr><td>Et cetera.</td></tr></tbody></table></figure>



<p/></div>
    </content>
    <updated>2020-11-15T12:40:43Z</updated>
    <published>2020-11-15T12:40:43Z</published>
    <category term="Uncategorized"/>
    <category term="marijuana"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2020-11-26T22:27:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7892</id>
    <link href="https://windowsontheory.org/2020/11/12/mops-and-junior-senior-meeting-disc-2020/" rel="alternate" type="text/html"/>
    <title>MoPS and Junior-Senior Meeting (DISC 2020)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(Guest post by Shir Cohen and Mouna Safir) The 34th International Symposium on Distributed Computing (DISC 2020) was held on October 12-16, 2020, as a virtual conference. As such, the opportunity for community members to get to know each other in an informal environment was lacking. To address this need, we arranged two types of … <a class="more-link" href="https://windowsontheory.org/2020/11/12/mops-and-junior-senior-meeting-disc-2020/">Continue reading <span class="screen-reader-text">MoPS and Junior-Senior Meeting (DISC 2020)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><br/>(Guest post by Shir Cohen and Mouna Safir)</p>



<p><br/>The 34th International Symposium on Distributed Computing (DISC 2020) was held on October 12-16, 2020, as a virtual conference. As such, the opportunity for community members to get to know each other in an informal environment was lacking. To address this need, we arranged two types of virtual networking events. We hope that these events planted the seeds for many future collaborations and that there will be an opportunity for those involved to meet in person next time.</p>



<p><strong>MoPS (Meet other Postdoc and Students) Sessions<br/></strong>Webpage: <a href="https://sites.google.com/view/disc2020mops/home" rel="noreferrer noopener" target="_blank">https://sites.google.com/view/disc2020mops/home</a></p>



<p>To allow junior members of the community to get to know one another, we arranged MoPS sessions, which we have not seen done before. There were more than 50 participants who took part in the sessions, with representation from a host of countries throughout the world. Sessions were held in 10-time slots before, during, and after DISC. In each session, there would typically be 5 members representing a mixture of Bachelor’s students, Masters students, PhDs, postdocs, and others. Care was taken to include at least one postdoc or Ph.D. in each session so that Bachelors and Masters students might benefit from their experience. Groups were formed with the goal of allowing participants from different countries and institutions to share their experiences and research journeys with one another. Based on the feedback for this event, it would appear that that goal was met and the participants came away with more of a sense of community.</p>



<p><strong>Junior-Senior Meetings<br/></strong>Webpage: <a href="https://sites.google.com/view/disc2020junior-seniormeeting/home" rel="noreferrer noopener" target="_blank">https://sites.google.com/view/disc2020junior-seniormeeting/home</a></p>



<p>The Junior-Senior meetings were organized to provide an opportunity for junior researchers to meet with senior researchers. Fourteen sessions were conducted, where each one brought together one senior and 3-5 juniors. In these sessions,  the juniors got a chance to interact with seniors in the field and profit from their experience. Discussions covered a variety of topics such as how to approach research, how to deal with the job market, or perhaps more personal concerns like work-life balance. We collected amazing feedback from the participants, who claimed that this was a fruitful and interesting experience.</p></div>
    </content>
    <updated>2020-11-12T13:53:04Z</updated>
    <published>2020-11-12T13:53:04Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-26T22:26:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=75</id>
    <link href="https://dstheory.wordpress.com/2020/11/11/friday-nov-20-himanshu-tyagi-for-the-indian-institute-of-science-iisc/" rel="alternate" type="text/html"/>
    <title>Friday Nov 20 — Himanshu Tyagi from the Indian Institute of Science (IISc)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Friday, Nov 20th at 10:00 AM Pacific Time (13:00 Eastern Time, 18:00 Central European Time, 17:00 UTC, 23:30 Indian Time).  Himanshu Tyagi from IISc will speak about “General lower bounds for estimation under information constraints”. Abstract:  We present very general lower bounds for parametric<a class="more-link" href="https://dstheory.wordpress.com/2020/11/11/friday-nov-20-himanshu-tyagi-for-the-indian-institute-of-science-iisc/">Continue reading <span class="screen-reader-text">"Friday Nov 20 — Himanshu Tyagi from the Indian Institute of Science (IISc)"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next Foundations of Data Science virtual talk will take place on Friday, Nov 20th at 10:00 AM Pacific Time (13:00 Eastern Time, 18:00 Central European Time, 17:00 UTC, 23:30 Indian Time).  <strong>Himanshu Tyagi </strong>from IISc will speak about “<strong>General lower bounds for estimation under information constraints</strong>”.</p>



<p><strong>Abstract</strong>:  We present very general lower bounds for parametric estimation when only limited information per sample is allowed. These limitations can arise, for example, in form of communication constraints, privacy constraints, or linear measurements. Our lower bounds hold for discrete distributions with large alphabet as well as continuous distributions with high-dimensional parameters, apply for any information constraint, and are valid for any $\ell_p$ loss function. Our bounds recover both strong data processing inequality based bounds and Cramér-Rao based bound as special cases.</p>



<p>This talk is based on joint work with Jayadev Acharya and Clément Canonne.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-11-11T21:47:20Z</updated>
    <published>2020-11-11T21:47:20Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-11-26T22:33:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://mycqstate.wordpress.com/?p=1262</id>
    <link href="https://mycqstate.wordpress.com/2020/11/11/lecture-notes-on-the-mahadev-verification-protocol/" rel="alternate" type="text/html"/>
    <title>Lecture notes on the Mahadev verification protocol</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">As announced earlier I am currently teaching a course on “interactive proofs with quantum devices” in Paris. The course is proceeding apace, even though the recent lockdown order in France means that we had to abandon our beautiful auditorium at … <a href="https://mycqstate.wordpress.com/2020/11/11/lecture-notes-on-the-mahadev-verification-protocol/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-image"><figure class="alignright size-large is-resized"><a href="https://mycqstate.files.wordpress.com/2020/11/index-1.jpg"><img alt="" class="wp-image-1266" height="206" src="https://mycqstate.files.wordpress.com/2020/11/index-1.jpg?w=1024" width="367"/></a>My street in lockdown </figure></div>



<p>As announced <a href="https://mycqstate.wordpress.com/2020/09/20/announcing-a-short-course-in-paris/">earlier</a> I am currently teaching a course on “interactive proofs with quantum devices” in Paris. The course is proceeding apace, even though the recent lockdown order in France means that we had to abandon our beautiful auditorium at the Institut Henri Poincaré and retreat behind the fanciful Zoom backgrounds whose pretension is a sad reminder of what our summers used to be (Banff, anyone?). A possible upshot is that more may be able to attend the now-online course; if you are interested see the <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/">course page</a> for info. (Things are actually fairly good here–in spite of the apparently strict restrictions on daily outings (max 1h, 1km) you can count on the French to bend things their way; most shops are closed but the streets remain quite busy.)</p>



<p>We just finished a sequence of four lectures on the Mahadev “classical verification of quantum computation” protocol. In the process of preparing the lectures I arrived at a presentation of the protocol that is fairly self-contained, so I decided to compile the associated lecture notes as a stand-alone group of 4 lectures that is available <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/fsmp_verification.pdf">here</a>. The notes are aimed at beginning graduate students with a first course in quantum computing and in complexity desiring to gain a concrete understanding of the inner workings of the result; the notes are a bit lengthy but this in part because they take the time to introduce related concepts and slowly build up to the main result. Overall, my hope is that these should be relatively easily readable and provide a good technical introduction to the Mahadev result on classical verification, including an almost complete analysis of her protocol (there are a few explicitly declared shortcuts that help simplify the presentation without hiding any important aspects). For additional background you can also consult the full <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/fsmp.pdf">10-week notes</a>. Comments on the notes are welcome; I’m afraid they most likely contain numerous typos so if you find any please feel free to correct them directly on the associated <a href="https://www.overleaf.com/9892211158zrwmqnxtvcxm">overleaf document</a>.</p>



<p>The last three lectures of the course will be devoted to the problem of testing under spatial assumptions, building up to an introduction to the proof of MIP* = RE. If all goes well I’ll aim to prepare some stand-alone notes for that part as well.</p></div>
    </content>
    <updated>2020-11-11T15:43:12Z</updated>
    <published>2020-11-11T15:43:12Z</published>
    <category term="Quantum"/>
    <category term="Science"/>
    <category term="teaching"/>
    <category term="Uncategorized"/>
    <category term="lecture notes"/>
    <category term="quantum verification"/>
    <author>
      <name>Thomas</name>
    </author>
    <source>
      <id>https://mycqstate.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://mycqstate.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://mycqstate.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://mycqstate.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://mycqstate.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>In superposition</subtitle>
      <title>MyCQstate</title>
      <updated>2020-11-26T22:30:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=64</id>
    <link href="https://francisbach.com/cauchy-residue-formula/" rel="alternate" type="text/html"/>
    <title>The Cauchy residue trick: spectral analysis made “easy”</title>
    <summary>In many areas of machine learning, statistics and signal processing, eigenvalue decompositions are commonly used, e.g., in principal component analysis, spectral clustering, convergence analysis of Markov chains, convergence analysis of optimization algorithms, low-rank inducing regularizers, community detection, seriation, etc. Understanding how the spectral decomposition of a matrix changes as a function of a matrix is...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">In many areas of machine learning, statistics and signal processing, eigenvalue decompositions are commonly used, e.g., in principal component analysis, spectral clustering, convergence analysis of Markov chains, convergence analysis of optimization algorithms, low-rank inducing regularizers, community detection, seriation, etc.</p>



<p class="justify-text">Understanding how the spectral decomposition of a matrix changes as a function of a matrix is thus of primary importance, both algorithmically and theoretically. We thus need a perturbation analysis or more generally some differentiability properties for eigenvalues or eigenvectors [1], or any spectral function [2]. These properties can be obtained from many angles, but a generic tool can be used for all of these: it is a surprising and elegant application of Cauchy’s residue formula, which is due to Kato [3].</p>



<p class="justify-text">Before diving into spectral analysis, I will first present the Cauchy residue theorem and some nice applications in computing integrals that are needed in machine learning and kernel methods.</p>



<h2>Cauchy residue formula </h2>



<p class="justify-text">A function \(f : \mathbb{C} \to \mathbb{C}\) is said <em>holomorphic</em> in \(\lambda \in \mathbb{C}\) with derivative \(f'(\lambda) \in \mathbb{C}\), if is differentiable in \(\lambda\), that is if \(\displaystyle \frac{f(z)-f(\lambda)}{z-\lambda}\) tends to \(f'(\lambda)\) when \(z\) tends to \(\lambda\). Many classical functions are holomorphic on \(\mathbb{C}\) or portions thereof, such as the exponential, sines, cosines and their hyperbolic counterparts, rational functions, portions of the logarithm.</p>



<p class="justify-text">We consider a function which is holomorphic in a region of \(\mathbb{C}\) except in \(m\) values \(\lambda_1,\dots,\lambda_m \in \mathbb{C}\), which are usually referred to as <em>poles</em>. We also consider a simple closed directed contour \(\gamma\) in \(\mathbb{C}\) that goes strictly around the \(m\) values above. </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5002" height="236" src="https://francisbach.com/wp-content/uploads/2020/10/contour_generic-1-1024x440.png" width="549"/></figure></div>



<p class="justify-text">The Cauchy residue formula gives an explicit formula for the contour integral along \(\gamma\):<br/> $$ \oint_\gamma f(z) dz = 2 i \pi \sum_{j=1}^m {\rm Res}(f,\lambda_j), \tag{1}$$<br/> where \({\rm Res}(f,\lambda)\) is called the <em>residue</em> of \(f\) at \(\lambda\) . If around \(\lambda\),  \(f(z)\) has a series expansions in powers of \((z − \lambda)\), that is, \(\displaystyle f(z) =  \sum_{k=-\infty}^{+\infty}a_k (z −\lambda)^k\), then \({\rm Res}(f,\lambda)=a_{-1}\).</p>



<p class="justify-text">For example, if \(\displaystyle f(z) =   \frac{g(z)}{z-\lambda}\) with \(g\) holomorphic around \(\lambda\), then \({\rm Res}(f,\lambda) = g(\lambda)\), and more generally, if \(\displaystyle f(z) = \frac{g(z)}{(z-\lambda)^k}\) for \(k \geqslant  1\), then \(\displaystyle {\rm Res}(f,\lambda) = \frac{g^{(k-1)}(\lambda) }{(k-1)!}\). For more details on complex analysis, see [4].</p>



<p class="justify-text">The result above can be naturally extended to vector-valued functions (and thus to any matrix-valued function), by applying the identity to all components of the vector.</p>



<p class="justify-text">This result is due to <a href="https://en.wikipedia.org/wiki/Augustin-Louis_Cauchy">Cauchy</a> [<a href="https://archive.org/details/mmoiresurlesin00cauc">10</a>] in 1825. The <a href="https://archive.org/details/mmoiresurlesin00cauc">original paper</a> where this is presented is a nice read in French where you can find some pepits like “la fonction s’évanouit pour \(x = \infty\)”.</p>



<p>If you are already familiar with complex residues, you can skip the next section.</p>



<h2>Where does it come from?</h2>



<p class="justify-text">At first, the formula in Eq. (1) seems unsettling. Why doesn’t the result depend more explicitly on the contour \(\gamma\)? Where does the multiplicative term \( {2i\pi}\) come from? Here is a very partial and non rigorous account (go to the <a href="https://terrytao.wordpress.com/tag/residue-theorem/">experts</a> for more rigor!).</p>



<p class="justify-text">Complex-valued functions on \(\mathbb{C}\) can be seen as functions from \(\mathbb{R}^2\) to itself, by writing $$ f(x+iy) = u(x,y) + i v(x,y),$$ where \(u\) and \(v\) are real-valued functions. We have thus a function \((x,y) \mapsto (u(x,y),v(x,y))\) from \(\mathbb{R}^2\) to \(\mathbb{R}^2\). Expanding \(f(z+dz) = f(z) + f'(z) dz\), which is the definition of complex differentiability, into real and imaginary parts, we get (using \(i^2 = -1\)): $$\left\{ \begin{array}{l} u(x+dx,y+dy) = u(x,y) + {\rm Re}(f'(z)) dx\ – {\rm Im}(f'(z)) dy \\ v(x+dx,y+dy) = v(x,y) + {\rm Re}(f'(z)) dy + {\rm Im}(f'(z)) dx. \end{array}\right.$$ This leads to $$\left\{ \begin{array}{l} \displaystyle \frac{\partial u}{\partial x}(x,y) = {\rm Re}(f'(z)) \\ \displaystyle \frac{\partial u}{\partial y}(x,y) = \ – {\rm Im}(f'(z)) \\ \displaystyle \frac{\partial v}{\partial x}(x,y) = {\rm Im}(f'(z)) \\ \displaystyle \frac{\partial v}{\partial y}(x,y) = {\rm Re}(f'(z)). \end{array}\right.$$</p>



<p class="justify-text">This in turn leads to the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations">Cauchy-Riemann equations</a> \(\displaystyle \frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}\) and \(\displaystyle \frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}\), which are essentially necessary and sufficient conditions to be holomorphic. Thus holomorphic functions correspond to differentiable functions on \(\mathbb{R}^2\) with some equal partial derivatives. These equations are key to obtaining the Cauchy residue formula.</p>



<p class="justify-text"><strong>Contour integral with no poles. </strong>We first consider a contour integral over a contour \(\gamma\) enclosing a region \(\mathcal{D}\) where the function \(f\) is holomorphic everywhere. The contour \(\gamma\) is defined as a differentiable function \(\gamma: [0,1] \to \mathbb{C}\), and the integral is equal to $$\oint_\gamma f(z) dz = \int_0^1 \!\!f(\gamma(t)) \gamma'(t) dt = \int_0^1 \!\![u(x(t),y(t)) +i v(x(t),y(t))] [ x'(t) + i y'(t)] dt,$$ where \(x(t) = {\rm Re}(\gamma(t))\) and \(y(t) = {\rm Im}(\gamma(t))\). By expanding the product of complex numbers, it is thus equal to $$\int_0^1 [ u(x(t),y(t)) x'(t) \ – v(x(t),y(t))y'(t)] dt +i  \int_0^1 [ v(x(t),y(t)) x'(t) +u (x(t),y(t))y'(t)] dt,$$ which we can rewrite in compact form as (with \(dx = x'(t) dt\) and \(dy = y'(t)dt\)): $$\oint_\gamma ( u \, dx\  – v \, dy ) + i \oint_\gamma ( v \, dx + u \, dy ).$$ We can then use <a href="https://en.wikipedia.org/wiki/Green%27s_theorem">Green’s theorem</a> because our functions are differentiable on the entire region \(\mathcal{D}\) (the set “inside” the contour), to get $$\oint_\gamma ( u \, dx\ – v \, dy ) + i \oint_\gamma ( v \, dx + u \, dy ) =\  – \int\!\!\!\!\int_\mathcal{D} \! \Big( \frac{\partial v}{\partial x} + \frac{\partial u}{\partial y} \Big) dx dy \ – i \!\! \int\!\!\!\!\int_\mathcal{D} \!\Big( \frac{\partial u}{\partial x} – \frac{\partial v}{\partial y} \Big) dx dy.$$ Thus, because of the Cauchy-Riemann equations, the contour integral is always zero within the domain of differentiability of \(f\). Note that this extends to piecewise smooth contours \(\gamma\).</p>



<p class="justify-text"><strong>Circle and rational functions.</strong> For a circle contour of center \(\lambda \in \mathbb{C}\) and radius \(r\), we have, with \(\gamma(t) = \lambda + re^{ 2i \pi t}\): $$\oint_{\gamma} \frac{dz}{(z-\lambda)^k} =\int_0^{1} \frac{ 2r i \pi e^{2i\pi t}}{ r^k e^{2i\pi kt}}dt= \int_0^{1} r^{1-k} i e^{2i\pi (1-k)t} dt,$$ which is equal to zero if \(k \neq 1\), and to \(\int_0^{1} 2i\pi dt = 2 i \pi\) for \(k =1\). Thus, for a function with a series expansion, the Cauchy residue formula is true for the circle around a single pole, because only the term in \(\frac{1}{z-\lambda}\) contributes.</p>



<p class="justify-text"><strong>No dependence on the contour.</strong> Now that the Cauchy formula is true for the circle around a single pole, we can “deform” the contour below to a circle.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5021" height="207" src="https://francisbach.com/wp-content/uploads/2020/10/contour_generic_circle-1024x532.png" width="398"/></figure></div>



<p>This can be done considering two contours \(\gamma_1\) and \(\gamma_2\) below with no poles inside, and thus with zero contour integrals, and for which the integrals along the added lines cancel.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5022" height="205" src="https://francisbach.com/wp-content/uploads/2020/10/contour_generic_circle_cut-1-1024x537.png" width="391"/></figure></div>



<p class="justify-text">This “shows” that the integral does not depend on the contour, and so in applications we can be quite liberal in the choice of contour. Note that similar constructions can be used to take into account several poles.</p>



<p class="justify-text">Before going to the spectral analysis of matrices, let us explore some cool choices of contours and integrands, and (again!) some positive definite kernels.</p>



<h2>Classical examples</h2>



<p class="justify-text">The Cauchy residue theorem can be used to compute integrals, by choosing the appropriate contour, looking for poles and computing the associated residues. Here are classical examples, before I show applications to kernel methods. See more examples in <a href="http://residuetheorem.com/">http://residuetheorem.com/</a>, and many in [11].</p>



<p class="justify-text"><strong>Fourier transforms. </strong> For \(\omega&gt;0\), we can compute \( \displaystyle \int_{-\infty}^\infty \!\! f(x) e^{ i \omega x} dx\) for holomorphic functions \(f\) by integrating on the real line and a big upper circle as shown below, with \(R\) tending to infinity (so that the contribution of the half-circle goes to zero because of the exponential term). This leads to \(2i \pi\) times the sum of all residues of the function \(z \mapsto f(z) e^{ i \omega z}\) in the upper half plane. See an example below related to kernel methods.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5026" height="214" src="https://francisbach.com/wp-content/uploads/2020/10/contour_upper-circle-2-1024x570.png" width="385"/></figure></div>



<p class="justify-text"><strong>Trigonometric integrals</strong>. For holomorphic functions \(Q\), we can compute the integral \(\displaystyle \int_0^{2\pi} \!\!\! Q(\cos \theta, \sin \theta) d\theta\). Indeed, letting \(f(z) = \frac{1}{iz} Q\big( \frac{z+z^{-1}}{2}, \frac{z-z^{-1}}{2i} \big)\), it is exactly equal to the integral on the unit circle. The desired integral is then equal to \(2i\pi\) times the sum of all residues of \(f\) within the unit disk.</p>



<p class="justify-text">For example, when \(Q(\cos \theta, \sin \theta) = \frac{1}{2 + \sin \theta}\), we have \(f(z) = \frac{2}{z^2+4iz-1}\), with a single pole inside the unit circle, namely \(\lambda = i ( \sqrt{3}-2)\), and residue equal to \(-i / \sqrt{3}\), leading to \(\int_0^{2\pi} \frac{d\theta}{2+\sin \theta} = \frac{2\pi}{\sqrt{3}}\).</p>



<p class="justify-text"><strong>Series.</strong> If the function \(f\) is holomorphic and has no poles at integer real values, and satisfies some basic boundedness conditions, then $$\sum_{n \in \mathbb{Z}} f(n) = \ – \!\!\! \sum_{ \lambda \in {\rm poles}(f)} {\rm Res}\big( f(z) \pi \frac{\cos \pi z}{\sin \pi z} ,\lambda\big).$$ This is a simple consequence of the fact that the function \(z \mapsto \pi \frac{\cos \pi z}{\sin \pi z}\) has all integers \(n \in \mathbb{Z}\) as poles, with corresponding residue equal to \(1\). This is obtained from the contour below with \(m\) tending to infinity.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5028" height="250" src="https://francisbach.com/wp-content/uploads/2020/10/contour_square-1-1024x903.png" width="284"/></figure></div>



<p class="justify-text">The same trick can be applied to  \(\displaystyle \sum_{n \in \mathbb{Z}} (-1)^n f(n) =\  –  \!\!\! \sum_{ \lambda \in {\rm poles}(f)} {\rm Res}\big( f(z) \pi \frac{1}{\sin \pi z} ,\lambda\big).\) See [7, Section 11.2] for more details. Experts will see an interesting link with the <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Maclaurin_formula">Euler-MacLaurin formula</a> and <a href="https://en.wikipedia.org/wiki/Bernoulli_polynomials">Bernoulli polynomials</a>.</p>



<p class="justify-text"><strong>Applications to kernel methods.</strong> In non-parametric estimation, regularization penalties are used to constrain real-values functions to be smooth. One such examples are combinations of squared \(L_2\) norms of derivatives. For functions \(f\) defined on an interval \(I\) of the real line, penalties are typically of the form \(\int_I \sum_{k=0}^s \alpha_k | f^{(k)}(x)|^2 dx\), for non-negative weights \(\alpha_0,\dots,\alpha_k\). For these Sobolev space norms, a positive definite kernel \(K\) can be used for estimation (see, e.g., <a href="https://francisbach.com/hermite-polynomials/">last month blog post</a>). </p>



<p class="justify-text">A classical question is: given the norm defined above, how to compute \(K\)? For \(I = \mathbb{R}\), then this can be done using Fourier transforms as: $$K(x,y) =  \frac{1}{2\pi} \int_\mathbb{R} \frac{e^{i\omega(x-y)}}{\sum_{k=0}^s \alpha_k \omega^{2k}} d\omega.$$ This is exactly an integral of the form above, for which we can use the contour integration technique. For example, for \(\alpha_0=1\) and \(\alpha_1=a^2\), we get for \(x-y&gt;0\), one pole \(i/a\) in the upper half plane for the function \(\frac{1}{1+a^2 z^2} = \frac{1}{(1+iaz)(1-iaz)}\), with residue \(-\frac{i}{2a} e^{-(x-y)/a}\), leading to the familiar exponential kernel \(K(x,y) = \frac{1}{2a} e^{-|x-y|/a}\). More complex kernels can be considered (see, e.g., [8, page 277], for \(\sum_{k=0}^s \alpha_k \omega^{2k} = 1 + \omega^{2s}\)).</p>



<p class="justify-text">We can also consider the same penalty on the unit interval \([0,1]\) with periodic functions, leading to the kernel (see [9] for more details): $$ K(x,y) = \sum_{n \in \mathbb{Z}} \frac{ e^{2in\pi(x-y)}}{\sum_{k=0}^s \alpha_k( 2n\pi)^s}.$$ For the same example as above, that is, \(\alpha_0=1\) and \(\alpha_1=a^2\), this leads to an infinite series on which we can apply the Cauchy residue formula as explained above. This leads to, for \(x-y \in [0,1]\), \(K(x,y) =  \frac{1}{2a}  \frac{ \cosh (\frac{1-2(x-y)}{2a})}{\sinh (\frac{1}{2a})}\). We can then extend by \(1\)-periodicity to all \(x-y\). See the detailed computation at the end of the post.</p>



<p>Now that you are all experts in residue calculus, we can move on to spectral analysis.</p>



<h2>Spectral analysis of symmetric matrices</h2>



<p class="justify-text">We consider a symmetric matrix \(A \in \mathbb{R}^{n \times n}\), with its \(n\) ordered real eigenvalues \(\lambda_1 \geqslant  \cdots \geqslant \lambda_n\), counted with their orders of multiplicity, and an orthonormal basis of their eigenvectors \(u_j \in \mathbb{R}^n\), \(j=1,\dots,n\). We have \(A = \sum_{j=1}^n \lambda_j u_j u_j^\top\).  When we consider eigenvalues as functions of \(A\), we use the notation \(\lambda_j(A)\), \(j=1,\dots,n\). These functions are always well-defined even when eigenvalues are multiple (this is not the case for eigenvectors because of the invariance by orthogonal transformations).</p>



<p class="justify-text">The key property that we will use below is that we can express the so-called resolvent matrix \((z I – A)^{-1} \in \mathbb{C}^{n \times n}\), for \(z \in \mathbb{C}\), as: $$  (z I- A)^{-1}  = \sum_{j=1}^n \frac{1}{z-\lambda_j} u_j u_j^\top. $$ The dependence on \(z\) of the form \( \displaystyle \frac{1}{z- \lambda_j}\)  leads to a nice application of Cauchy residue formula.</p>



<p class="justify-text">Assuming the \(k\)-th eigenvalue \(\lambda_k\) is simple, we consider the contour \(\gamma\) going strictly around \(\lambda_k\) like below (for \(k=5\)).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5039" height="182" src="https://francisbach.com/wp-content/uploads/2020/10/contour_single_eigenvalue-1024x433.png" width="431"/></figure></div>



<p class="justify-text">We consider integrating the matrix above, which leads to: $$ \oint_\gamma<br/>  (z I- A)^{-1} dz = \sum_{j=1}^m \Big( \oint_\gamma \frac{1}{z – \lambda_j} dz \Big) u_j u_j^\top<br/>  = 2 i \pi \  u_k u_k^\top $$  using the identity \(\displaystyle \oint_\gamma \frac{1}{z – \lambda_j} dz = 1\) if \(j=k\) and \(0\) otherwise (because the pole is outside of \(\gamma\)). We thus obtain an expression for projectors on the one-dimensional eigen-subspace associated with the eigenvalue \(\lambda_k\).</p>



<p class="justify-text">With simple manipulations, we can also access the eigenvalues. Indeed, we have: $$ \oint_\gamma<br/>   (z I- A)^{-1} z dz = \sum_{j=1}^m \Big( \oint_\gamma \frac{z}{z – \lambda_j} dz \Big) u_j u_j^\top<br/>  = 2 i \pi \lambda_k  u_k u_k^\top, $$ and by taking the trace, we obtain $$ \oint_\gamma<br/>  {\rm tr} \big[ z (z I- A)^{-1} \big]  dz    = \lambda_k. $$ The key benefit of these representations is that when the matrix \(A\) is slightly perturbed, then the same contour \(\gamma\) can be used to enclose the corresponding eigenvalues of the perturbed matrix, and perturbation results are simply obtained by taking gradients within the contour integral. Note that several eigenvalues may be summed up by selecting a contour englobing more than one eigenvalues.</p>



<h2>Gradients of eigenvalues</h2>



<p class="justify-text">The expression with contour integrals allows to derive simple formulas for gradients of eigenvalues. These can be obtained by other means [5], but using contour integrals shows that this is simply done by looking at the differential of \((z I – A)^{-1}\) and integrating it. The central component is the following expansion, which is a classical result in matrix differentiable calculus, with \(\|\Delta\|_2\) the operator norm of \(\Delta\) (i.e., its largest singular value):  $$<br/> (z I- A – \Delta)^{-1} = (z I – A)^{-1} + (z I- A)^{-1} \Delta (z I- A)^{-1} + o(\| \Delta\|_2).  $$ Note here that the asymptotic remainder \(o(\| \Delta\|_2)\) can be made explicit. </p>



<p class="justify-text">By expanding the expression on the basis of eigenvectors of \(A\), we get  $$<br/> z (z I- A – \Delta)^{-1}  – z (z I- A)^{-1}  =  \sum_{j=1}^n \sum_{\ell=1}^n u_j u_\ell^\top  \frac{ z \cdot u_j^\top \Delta u_\ell}{(z-\lambda_j)(z-\lambda_\ell)} + o(\| \Delta \|_2).<br/> $$ Taking the trace, the cross-product terms \({\rm tr}(u_j u_\ell^\top) =  u_\ell^\top u_j\) disappear for \(j \neq \ell\), and we get: $$<br/> {\rm tr} \big[ z (z I – A – \Delta)^{-1}  \big] – {\rm tr} \big[ z (z I – A)^{-1}  \big]=  \sum_{j=1}^n   \frac{ z \cdot u_j^\top \Delta u_j}{(z-\lambda_j)^2} + o(\| \Delta \|_2).<br/> $$ This leads to, by contour integration:<br/>$$<br/> \lambda_{k}(A+\Delta) -\lambda_k(A)<br/> =<br/> \frac{1}{2i \pi} \oint_\gamma \Big[ <br/>   \sum_{j=1}^n   \frac{ z \cdot u_j^\top \Delta u_j}{(z-\lambda_j)^2} \Big] dz +  o(\| \Delta \|_2).<br/> $$ By keeping only the pole \(\lambda_k\) which is inside the contour \(\gamma\), we get  $$ \lambda_{k}(A+\Delta) -\lambda_k(A)<br/>  =<br/> \frac{1}{2i \pi} \oint_\gamma \Big[ <br/>    \frac{ z \cdot u_k^\top \Delta u_k}{(z-\lambda_k)^2} \Big] dz +  o(| \Delta |_2) \<br/>    =  u_k^\top \Delta u_k + o(\| \Delta \|_2),<br/> $$ using the identity \(\displaystyle <br/>  \oint_\gamma \frac{z dz}{(z – \lambda_k)^2} dz = <br/>   \oint_\gamma \Big( \frac{\lambda_k}{(z – \lambda_k)^2}  + \frac{1}{z – \lambda_k} \Big) dz = 1\).  </p>



<p class="justify-text">Thus the gradient of \(\lambda_k\) at a matrix \(A\) where the \(k\)-th eigenvalue is simple is simply \( u_k u_k^\top\), where \(u_k\) is a corresponding eigenvector. Note that this result can be simply obtained by the simple (rough) calculation: if \(x\) is a unit eigenvector of \(A\), then \(Ax =\lambda x\), and \(x^\top x = 1\), leading to \(x^\top dx = 0\) and \(dA\ x + A dx = d\lambda \ x + \lambda dx\), and by taking the dot product with \(x\), \(d\lambda = x^\top dA\ x + x^\top A dx = x^\top dA \ x + \lambda x^\top  dx = x^\top dA \ x\), which is the same result. However, this reasoning is more cumbersome, and does not lead to neat approximation guarantees, in particular in the extensions below.</p>



<h2>Other perturbation results</h2>



<p class="justify-text">Given the gradient, other more classical perturbation results could de derived, such as Hessians of eigenvalues, or gradient of the projectors \(u_k u_k^\top\).  Here I derive a perturbation result for the projector \(\Pi_k(A)=u_k u_k^\top\), when \(\lambda_k\) is a simple eigenvalue. Using the same technique as above, we get: $$ \Pi_k(A+\Delta )\  – \Pi_k(A) = \frac{1}{2i \pi} \oint_\gamma (z I- A)^{-1} \Delta (z I – A)^{-1}dz  + o(\| \Delta\|_2),$$ which we can expand to the basis of eigenvectors as $$ \frac{1}{2i \pi} \oint_\gamma \sum_{j=1}^n \sum_{\ell=1}^n u_j u_j^\top \Delta u_\ell u_\ell^\top \frac{  dz}{(z-\lambda_\ell) (z-\lambda_j)  } + o(\| \Delta\|_2).$$ We can then split in two, with the two terms (all others are equal to zero by lack of poles within \(\gamma\)): $$ \frac{1}{2i \pi} \oint_\gamma \sum_{j \neq k}  u_j^\top \Delta u_k  ( u_j u_k^\top + u_k u_j^\top)   \frac{  dz}{(z-\lambda_k) (z-\lambda_j)  }= \sum_{j \neq k}  u_j^\top \Delta u_k  ( u_j u_k^\top + u_k u_j^\top) \frac{1}{\lambda_k – \lambda_j}  $$ and $$\frac{1}{2i \pi} \oint_\gamma   u_k^\top \Delta u_k   u_k u_k^\top  \frac{  dz}{(z-\lambda_k)^2  } = 0 ,$$ finally leading to $$\Pi_k(A+\Delta ) \ – \Pi_k(A) =  \sum_{j \neq k}  \frac{u_j^\top \Delta u_k}{\lambda_k – \lambda_j}    ( u_j u_k^\top + u_k u_j^\top)    + o(\| \Delta\|_2),$$ from which we can compute the Jacobian of \(\Pi_k\).</p>



<h2>Spectral functions</h2>



<p class="justify-text">Spectral functions are functions on symmetric matrices defined as \(F(A) = \sum_{k=1}^n f(\lambda_k(A))\), for any real-valued function \(f\). For \(f(x) = x\), we get back the trace, for \(f(x) = \log x\) we get back the log determinant, and so on. The function \(F\) can be represented as $$F(A) = \sum_{k=1}^n f(\lambda_k(A)) = \frac{1}{2i \pi} \oint_\gamma f(z) {\rm tr} \big[ (z I  – A)^{-1} \big] dz,$$ where the contour \(\gamma\) encloses all eigenvalues (as shown below).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5087" height="185" src="https://francisbach.com/wp-content/uploads/2020/11/contour_all_eigenvalues-1024x433.png" width="438"/></figure></div>



<p class="justify-text">This representation can be used to compute derivatives of \(F\), by simple derivations, to obtain the same result as [<a href="https://epubs.siam.org/doi/pdf/10.1137/S089547980036838X">12</a>].</p>



<h2>Singular values of rectangular matrices</h2>



<p class="justify-text">Singular value decompositions are also often used, for a rectangular matrix \(W \in \mathbb{R}^{n \times d}\). It consists in finding \(r\) pairs \((u_j,v_j) \in \mathbb{R}^{n} \times \mathbb{R}^d\), \(j=1,\dots,r\), of singular vectors and \(r\) positive singular values \(\sigma_1 \geqslant \cdots \geqslant \sigma_r &gt; 0\) such that \(W = \sum_{j=1}^r \sigma_j u_j v_j^\top\) and  \((u_1,\dots,u_r)\) and \((v_1,\dots,v_r)\) are orthonormal families.</p>



<p class="justify-text">There are two natural ways to relate the singular value decomposition to the classical eigenvalue decomposition of a symmetric matrix, �first through \(WW^\top\) (or similarly \(W^\top W\)). Here it is more direct to consider the so-called Jordan-Wielandt matrix, defined by blocks as $$<br/> \bar{W} = \left( \begin{array}{cc}<br/>0 &amp; W \\<br/>W^\top &amp; 0 \end{array} \right). $$ The matrix \(\bar{W}\) is symmetric, and its non zero eigenvalues are \(+\sigma_i\) and \(-\sigma_i\), \(i=1,\dots,r\), associated with the eigenvectors \(\frac{1}{\sqrt{2}}  \left( \begin{array}{cc}<br/>u_i \\ v_i \end{array} \right)\) and \(\frac{1}{\sqrt{2}}  \left( \begin{array}{cc}<br/>u_i \\ -v_i \end{array} \right)\).</p>



<p class="justify-text">All necessary results (derivatives of singular values \(\sigma_j\), or projectors \(u_j v_j^\top\) can be obtained from there); see more details, in, e.g., the appendix of [6].</p>



<h2>Going beyond</h2>



<p class="justify-text">In this post, I have shown various applications of the Cauchy residue formula, for computing integrals and for the spectral analysis of matrices. I have just scratched the surface of spectral analysis, and what I presented extends to many interesting situations, for example, to more general linear operators in infinite-dimensional spaces [3], or to the analysis fo the eigenvalue distribution of random matrices (see a nice and reasonably simple derivation of the semi-circular law from <a href="https://terrytao.wordpress.com/2010/02/02/254a-notes-4-the-semi-circular-law/">Terry Tao’s blog</a>).</p>



<h2>References</h2>



<p class="justify-text">[1] Gilbert W. Stewart and Sun Ji-Huang. <em>Matrix Perturbation Theory</em>. Academic Press, 1990.<br/>[2] Adrian Stephen Lewis. Derivatives of spectral functions. <em>Mathematics of Operations Research</em>, 21(3):576–588, 1996. <br/>[3] Tosio Kato. <em>Perturbation Theory for Linear Operators</em>, volume 132. Springer, 2013. <br/>[4] Serge Lang. <em>Complex Analysis</em>, volume 103. Springer, 2013. <br/>[5] Jan R. Magnus. On differentiating eigenvalues and eigenvectors. <em>Econometric Theory</em>, 1(2):179–191, 1985. <br/>[6] Francis Bach. Consistency of trace norm minimization. <em>Journal of Machine Learning Research</em>, 9:1019-1048, 2008.<br/>[7] Joseph Bak, Donald J. Newman. <em>Complex analysis</em>. New York: Springer, 2010.<br/>[8] Alain Berlinet, and Christine Thomas-Agnan. <em>Reproducing kernel Hilbert spaces in probability and statistics</em>. Springer Science &amp; Business Media, 2011.<br/>[9] Grace Wahba. <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970128">Spline models for observational data</a>. Society for Industrial and Applied Mathematics, 1990.<br/>[10] Augustin Louis Cauchy, <a href="https://archive.org/details/mmoiresurlesin00cauc">Mémoire</a><a href="http://www.numdam.org/article/BSMA_1874__7__265_0.pdf"> sur les intégrales définies, prises entre des limites imaginaires</a>, 1825, <a href="http://www.numdam.org/article/BSMA_1874__7__265_0.pdf">re-published</a> in Bulletin des Sciences Mathématiques et Astronomiques, Tome 7, 265-304, 1874.<br/>[11] Dragoslav S. Mitrinovic, and Jovan D. Keckic. <em>The Cauchy method of residues: theory and applications</em>. Vol. 9. Springer Science &amp; Business Media, 1984.<br/>[12] Adrian S. Lewis, and Hristo S. Sendov. <a href="https://epubs.siam.org/doi/pdf/10.1137/S089547980036838X">Twice differentiable spectral functions</a>. <em>SIAM Journal on Matrix Analysis and Applications</em> 23.2: 368-386, 2001.</p>



<h2>Computing the Sobolev kernel</h2>



<p class="justify-text">The goal is to compute the infinite sum $$\sum_{n \in \mathbb{Z}} \frac{e^{2i\pi q \cdot n}}{1+(2a \pi n)^2}$$ for \(q \in (0,1)\). We consider the function $$f(z) = \frac{e^{i\pi (2q-1) z}}{1+(2a \pi z)^2} \frac{\pi}{\sin (\pi z)}.$$ It is holomorphic on \(\mathbb{C}\) except at all integers \(n \in \mathbb{Z}\), where it has a simple pole with residue \(\displaystyle \frac{e^{i\pi (2q-1) n}}{1+(2a \pi n)^2} (-1)^n = \frac{e^{i\pi 2q n}}{1+(2a \pi n)^2}\), at \(z = i/(2a\pi)\) where it has a residue equal to \(\displaystyle \frac{e^{ – (2q-1)/(2a)}}{4ia\pi} \frac{\pi}{\sin (i/(2a))} = \ – \frac{e^{ – (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))}\), and at \(z = -i/(2a\pi)\) where it has a residue equal to \(\displaystyle \frac{e^{  (2q-1)/(2a)}}{4ia\pi} \frac{\pi}{\sin (i/(2a))} =\ – \frac{e^{ (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))}\). With all residues summing to zero (note that this fact requires a precise analysis of limits when \(m\) tends to infinity for the contour defined in the main text), we get: $$\sum_{n \in \mathbb{Z}} \frac{e^{2i\pi q \cdot n}}{1+(2a \pi n)^2} =\frac{e^{ – (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))}+ \frac{e^{ (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))} = \frac{1}{2a} \frac{ \cosh (\frac{2q-1}{2a})}{\sinh (\frac{1}{2a})}.$$</p></div>
    </content>
    <updated>2020-11-07T12:10:25Z</updated>
    <published>2020-11-07T12:10:25Z</published>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-11-26T22:35:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=509</id>
    <link href="https://tcsplus.wordpress.com/2020/11/06/tcs-talk-wednesday-november-11-shuai-shao-uw-madison/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, November 11 — Shuai Shao, UW-Madison</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, November 11th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Shuai Shao from UW-Madison will speak about “A Dichotomy for Real Boolean Holant Problems” (abstract below). You can reserve a spot as an individual or a group to […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, November 11th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Shuai Shao</strong> from UW-Madison will speak about “<em>A Dichotomy for Real Boolean Holant Problems</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: In this talk, we present a complexity dichotomy for Holant problems on the boolean domain with arbitrary sets of real-valued constraint functions. These constraint functions need not be symmetric nor do we assume any auxiliary functions as in previous results. It is proved that for every set <img alt="F" class="latex" src="https://s0.wp.com/latex.php?latex=F&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="F"/> of real-valued constraint functions, <img alt="\text{Holant}(F)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BHolant%7D%28F%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\text{Holant}(F)"/> is either <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="P"/>-time computable or <img alt="\#P" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%23P&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\#P"/>-hard. The classification has an explicit criterion. This is a culmination of much research on a decade-long classification program for Holant problems, and it uses previous results and techniques from many researchers.  However, as it turned out, the journey to the present theorem has been arduous. Some particularly intriguing concrete functions f6, f8 and their associated families with extraordinary closure properties related to Bell states in quantum information theory play an important role in this proof.  <br/><br/>Based on joint work with Jin-Yi Cai.</p></div>
    </content>
    <updated>2020-11-07T04:30:21Z</updated>
    <published>2020-11-07T04:30:21Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-26T22:27:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=69</id>
    <link href="https://dstheory.wordpress.com/2020/11/04/monday-nov-09-tal-rabin-from-university-of-pennsylvania/" rel="alternate" type="text/html"/>
    <title>Monday, Nov 09 — Tal Rabin from University of Pennsylvania</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Monday, Nov 09th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  Tal Rabin from UPenn will speak about “You Only Speak Once — Secure MPC with Stateless Ephemeral Roles”. Abstract: The inherent difficulty of maintaining stateful environments<a class="more-link" href="https://dstheory.wordpress.com/2020/11/04/monday-nov-09-tal-rabin-from-university-of-pennsylvania/">Continue reading <span class="screen-reader-text">"Monday, Nov 09 — Tal Rabin from University of Pennsylvania"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next Foundations of Data Science virtual talk will take place on Monday, Nov 09th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Tal Rabin </strong>from UPenn will speak about “<strong>You Only Speak Once — Secure MPC with Stateless Ephemeral Roles</strong>”.</p>



<p><strong>Abstract</strong>: The inherent difficulty of maintaining stateful environments over long periods of time gave rise to the paradigm of serverless computing, where mostly-stateless components are deployed on demand to handle computation tasks, and are teared down once their task is complete. Serverless architecture could offer the added benefit of improved resistance to targeted denial-of-service attacks. Realizing such protection,<br/>requires that the protocol only uses stateless parties. Perhaps the most famous example of this style of protocols is the Nakamoto consensus protocol used in Bitcoin. We refer to this stateless property as the You-Only-Speak-Once (YOSO) property, and initiate the formal study of it within a new YOSO model. Our model is centered around the notion of roles, which are stateless parties that can only send a single message. Furthermore, we describe several techniques for achieving YOSO MPC; both computational and information theoretic.</p>



<p>The talk will be self contained.</p>



<p>Based on joint works with: Fabrice Benhamouda, Craig Gentry, Sergey Gorbunov, Shai Halevi, Hugo Krawczyk, Chengyu Lin, Bernardo Magri, Jesper Nielsen, Leo Reyzin, Sophia Yakoubov.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-11-04T15:40:16Z</updated>
    <published>2020-11-04T15:40:16Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-11-26T22:33:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7875</id>
    <link href="https://windowsontheory.org/2020/11/03/yet-another-backpropagation-tutorial/" rel="alternate" type="text/html"/>
    <title>Yet another backpropagation tutorial</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I am teaching deep learing this week in Harvard’s CS 182 (Artificial Intelligence) course. As I’m preparing the back-propagation lecture, Preetum Nakkiran told me about Andrej Karpathy’s awesome micrograd package which implements automatic differentiation for scalar variables in very few lines of code. I couldn’t resist using this to show how simple back-propagation and stochastic … <a class="more-link" href="https://windowsontheory.org/2020/11/03/yet-another-backpropagation-tutorial/">Continue reading <span class="screen-reader-text">Yet another backpropagation tutorial</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am teaching deep learing this week in Harvard’s CS 182 (Artificial Intelligence) course. As I’m preparing the back-propagation lecture, Preetum Nakkiran told me about <a href="https://github.com/karpathy/micrograd">Andrej Karpathy’s awesome micrograd package</a> which implements automatic differentiation for scalar variables in very few lines of code.</p>



<p>I couldn’t resist using this to show how simple back-propagation and stochastic gradient descents are. To make sure we leave nothing “under the hood” we will not import anything from the package but rather only copy paste the few things we need. I hope that the text below is generally accessible to anyone familiar with partial derivatives. See this <a href="https://colab.research.google.com/drive/1eLEIMxlGZfAIYPWZMcBCPDLeVJzBMdYf?usp=sharing">colab notebook</a> for all the code in this tutorial. In particular, aside from libraries for plotting and copy pasting a few dozen lines from Karpathy this code uses absolutely no libraries (no numpy, no pytorch, etc..) and can train (slowly..) neural networks using stochastic gradient descent.  (This <a href="https://deepnote.com/publish/f898cdd4-4815-42ad-ba57-ae0b8b733492">notebook</a> builds the code more incrementally.)</p>



<p><strong>Automatic differentiation</strong> is a mechanism that allows you to write a Python functions such as</p>


<pre class="brush: python; gutter: false; title: ; notranslate">def f(x,y): return (x+y)+x**3
</pre>


<p>and enables one to automatically obtain the partial derivatives <img alt="\tfrac{\partial f}{\partial \text{x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Ctext%7Bx%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial f}{\partial \text{x}}"/> and <img alt="\tfrac{\partial f}{\partial \text{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Ctext%7By%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial f}{\partial \text{y}}"/>. Numerically we could do this by choosing some small value <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> and computing both <img alt="\tfrac{f(x+\delta,y)-f(x,y)}{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7Bf%28x%2B%5Cdelta%2Cy%29-f%28x%2Cy%29%7D%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{f(x+\delta,y)-f(x,y)}{\delta}"/> and <img alt="\tfrac{f(x,y+\delta)-f(x,y)}{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7Bf%28x%2Cy%2B%5Cdelta%29-f%28x%2Cy%29%7D%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{f(x,y+\delta)-f(x,y)}{\delta}"/>.<br/>However, if we generalize this approach to <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> variables, we get an algorithm that requires roughly <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> evaluations of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>. <em>Back-propagation</em> enables computing <em>all</em> of the partial derivatives at only constant overhead over the cost of a single evaluation of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>.</p>



<h2>Back propagation and the chain rule</h2>



<p>Back-propagation is a direct implication of the <strong>multi-variate chain rule</strong>. Let’s illustrate this for the case of two variables. Suppose that <img alt="v,w: \mathbb{R} \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=v%2Cw%3A+%5Cmathbb%7BR%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v,w: \mathbb{R} \rightarrow \mathbb{R}"/> and <img alt="z:\mathbb{R}^2 \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=z%3A%5Cmathbb%7BR%7D%5E2+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z:\mathbb{R}^2 \rightarrow \mathbb{R}"/> are functions, and define</p>



<p><img alt="f(u) = z(v(u),w(u))" class="latex" src="https://s0.wp.com/latex.php?latex=f%28u%29+%3D+z%28v%28u%29%2Cw%28u%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(u) = z(v(u),w(u))"/>.</p>



<p>That is, we have the following situation:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/KGVphzL.png"/></figure>



<p>where <img alt="f(u)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(u)"/> is the value <img alt="z=z(v(u),w(u))" class="latex" src="https://s0.wp.com/latex.php?latex=z%3Dz%28v%28u%29%2Cw%28u%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z=z(v(u),w(u))"/></p>



<p>Then the <strong>chain rule</strong> states that</p>



<p><img alt="\tfrac{\partial f}{\partial u} = ( \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w} )" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+f%7D%7B%5Cpartial+u%7D+%3D+%28+%5Ctfrac%7B%5Cpartial+v%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D+%2B+%5Ctfrac%7B%5Cpartial+w%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial f}{\partial u} = ( \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w} )"/></p>



<p>You can take this on faith, but it also has a simple proof. To see the intuition, note that for small <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/>, <img alt="v(u+\delta) \approx v(u) + \delta \tfrac{\partial v}{\partial u}(u)" class="latex" src="https://s0.wp.com/latex.php?latex=v%28u%2B%5Cdelta%29+%5Capprox+v%28u%29+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+v%7D%7B%5Cpartial+u%7D%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v(u+\delta) \approx v(u) + \delta \tfrac{\partial v}{\partial u}(u)"/> and <img alt="w(u+\delta) \approx w(u) + \delta \tfrac{\partial w}{\partial u}(u)" class="latex" src="https://s0.wp.com/latex.php?latex=w%28u%2B%5Cdelta%29+%5Capprox+w%28u%29+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+w%7D%7B%5Cpartial+u%7D%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w(u+\delta) \approx w(u) + \delta \tfrac{\partial w}{\partial u}(u)"/>. For small <img alt="\delta_1,\delta_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_1%2C%5Cdelta_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta_1,\delta_2"/>, <img alt="z(v+\delta_1,w+\delta_2) \approx z(v,w) + \delta_1 \tfrac{\partial z}{\partial v}(z,w) + \delta_2 \tfrac{\partial z}{\partial w}" class="latex" src="https://s0.wp.com/latex.php?latex=z%28v%2B%5Cdelta_1%2Cw%2B%5Cdelta_2%29+%5Capprox+z%28v%2Cw%29+%2B+%5Cdelta_1+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D%28z%2Cw%29+%2B+%5Cdelta_2+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z(v+\delta_1,w+\delta_2) \approx z(v,w) + \delta_1 \tfrac{\partial z}{\partial v}(z,w) + \delta_2 \tfrac{\partial z}{\partial w}"/>. Hence, if we ignore terms with powers of delta two or higher,</p>



<p><img alt="f(u +\delta)= z(w(u+\delta),v(u+\delta)) \approx f(u) + \delta \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \delta \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28u+%2B%5Cdelta%29%3D+z%28w%28u%2B%5Cdelta%29%2Cv%28u%2B%5Cdelta%29%29+%5Capprox+f%28u%29+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+v%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D+%2B+%5Cdelta+%5Ctfrac%7B%5Cpartial+w%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(u +\delta)= z(w(u+\delta),v(u+\delta)) \approx f(u) + \delta \tfrac{\partial v}{\partial u} \cdot \tfrac{\partial z}{\partial v} + \delta \tfrac{\partial w}{\partial u} \cdot \tfrac{\partial z}{\partial w}"/></p>



<p/>



<p>The chain rule generalizes naturally to the case that <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> is a function of more variables than <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/>. Generally, if the value <img alt="f(u)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28u%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(u)"/> is obtained by first computing some intermediate values <img alt="v_1,\ldots,v_k" class="latex" src="https://s0.wp.com/latex.php?latex=v_1%2C%5Cldots%2Cv_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v_1,\ldots,v_k"/> from <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> and then computing <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> in some arbitrary way from <img alt="v_1,\ldots,v_k" class="latex" src="https://s0.wp.com/latex.php?latex=v_1%2C%5Cldots%2Cv_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v_1,\ldots,v_k"/>, then <img alt="\tfrac{\partial z}{\partial u} \sum_{i=1}^k \tfrac{\partial v_i}{\partial u} \cdot \tfrac{\partial z}{\partial v_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D+%5Csum_%7Bi%3D1%7D%5Ek+%5Ctfrac%7B%5Cpartial+v_i%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v_i%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial z}{\partial u} \sum_{i=1}^k \tfrac{\partial v_i}{\partial u} \cdot \tfrac{\partial z}{\partial v_i}"/>.</p>



<p>As a corollary, if you already managed to compute the values <img alt="\tfrac{\partial z}{\partial v_1},\ldots, \tfrac{\partial z}{\partial v_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v_1%7D%2C%5Cldots%2C+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v_k%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial z}{\partial v_1},\ldots, \tfrac{\partial z}{\partial v_k}"/>, and you kept track of the way that <img alt="v_1,\ldots,v_k" class="latex" src="https://s0.wp.com/latex.php?latex=v_1%2C%5Cldots%2Cv_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v_1,\ldots,v_k"/> were obtained from <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/>, then you can compute <img alt="\tfrac{\partial z}{\partial u}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial z}{\partial u}"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/35jj2wz.png"/></figure>



<p>This suggests a simple recursive algorithm by which you compute the derivative of the final value <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> with respect to an intermediate value <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/> in the computation using recursive calls to compute the values <img alt="\tfrac{\partial z}{\partial w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%27%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial z}{\partial w'}"/> for all the values <img alt="w'" class="latex" src="https://s0.wp.com/latex.php?latex=w%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w'"/> that were directly computed from <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/>. Back propagation is this algorithm.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/11/backprop_example.png"><img alt="" class="wp-image-7887" src="https://windowsontheory.files.wordpress.com/2020/11/backprop_example.png?w=1024"/></a>Computing <img alt="\tfrac{\partial z}{\partial u},\tfrac{\partial z}{\partial v}, \tfrac{\partial z}{\partial w}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D%2C%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+v%7D%2C+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial z}{\partial u},\tfrac{\partial z}{\partial v}, \tfrac{\partial z}{\partial w}"/>  for the assignment <img alt="u=5" class="latex" src="https://s0.wp.com/latex.php?latex=u%3D5&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u=5"/> using back propagation</figure>



<h2>Implementing automatic differentiation using back propagation in Python</h2>



<p>We now describe how to do this in Python, following Karpathy’s code. The basic class we use is <code>Value</code>. Every member <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> of <code>Value</code> is a container that holds:</p>



<ol><li>The actual scalar (i.e., floating point) value that <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> holds. We call this <code>data</code>.</li><li>The gradient of <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> with respect to some future unknown value <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> that will use it. We call this <code>grad</code> and it is initialized to zero.</li><li>Pointers to all the values that were used in the computation of <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/>. We call this <code>_prev</code></li><li>The method that adds (using the current value of <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> and other values) the contribution of <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> to the gradient of all its previous values <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/> to their gradients. We call this function <code>_backward</code>. Specifically, at the time we call <code>_backward</code> we assume that <code>u.grad</code> already contains <img alt="\tfrac{\partial z}{\partial u}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial z}{\partial u}"/> where <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> is the final value we are interested in. For every value <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/> that was used to compute <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/>, we add to <code>v.grad</code> the quantity <img alt="\tfrac{\partial z}{\partial u} \cdot \tfrac{\partial u}{\partial v}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+u%7D+%5Ccdot+%5Ctfrac%7B%5Cpartial+u%7D%7B%5Cpartial+v%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial z}{\partial u} \cdot \tfrac{\partial u}{\partial v}"/>. For the latter quantity we need to keep track of <em>how</em> <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> was computed from <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="b"/>.</li><li>If we call the method <code>backwards</code> (without an underscore) on a variable <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> then this will compute the derivative of <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> with respect to <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/> for all values <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/> that were used in the computation of <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/>. We do this by applying <code>_backward</code> to <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> and then recursively (just like in DFS) going over the “children” (values used to compute <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/>), calling <code>_backward</code> on each one and keeping track the ones we visited just like the Depth First Search (DFS) algorithm.</li></ol>



<p>Let’s now describe this in code. We start off with a simple version that only supports addition and multiplication. The constructor for the class is the following:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">class Value:
    """ stores a single scalar value and its gradient """

    def __init__(self, data, _children=()):
        self.data = data
        self.grad = 0
        self._backward = lambda: None
        self._prev = set(_children)
</pre>


<p>which fairly directly matches the description above. This constructor creates a value not using prior ones, which is why the <code>_backward</code> function is empty.<br/>However, we can also create values by adding or multiplying prior ones, by adding the following methods:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">  def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data + other.data, (self, other))

        def _backward():
            self.grad += out.grad
            other.grad += out.grad
        out._backward = _backward

        return out

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        out = Value(self.data * other.data, (self, other))

        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward

        return out
</pre>


<p>That is, if we create <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/> by adding the values <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> and <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/>, then the <code>_backward</code> function of <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/> works by adding <code>w.grad</code> <img alt="= \tfrac{\partial z}{\partial w}" class="latex" src="https://s0.wp.com/latex.php?latex=%3D+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="= \tfrac{\partial z}{\partial w}"/> to both <code>u.grad</code> and <code>v.grad</code>.<br/>If we <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/> is obtain by multiplying <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> and <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/> then we add <code>w.grad</code> <img alt="\cdot" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ccdot&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\cdot"/> <code>v.data</code> <img alt="= \tfrac{\partial z}{\partial w} v" class="latex" src="https://s0.wp.com/latex.php?latex=%3D+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="= \tfrac{\partial z}{\partial w} v"/> to <code>u.grad</code> and similarly add <code>w.grad</code> <img alt="\cdot" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ccdot&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\cdot"/> <code>u.data</code> <img alt="= \tfrac{\partial z}{\partial w} u" class="latex" src="https://s0.wp.com/latex.php?latex=%3D+%5Ctfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="= \tfrac{\partial z}{\partial w} u"/> to <code>v.grad</code>.</p>



<p>The <code>backward</code> function is obtained by setting the gradient of the current value to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/> and then running <code>_backwards</code> on all other values in reverse topological order:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">def backward(self, visited= None): # slightly shorter code to fit in the blog 
    if visited is None:
        visited= set([self])
        self.grad = 1
    self._backward()
    for child in self._prev:
        if not child in visited:
            visited.add(child)
            child.backward(visited)
</pre>


<p>For example, if we run the following code</p>


<pre class="brush: python; gutter: false; title: ; notranslate">a = Value(5)
print(a.grad)
def f(x): return (x+2)**2 + x**3
f(a).backward()
print(a.grad)
</pre>


<p>then the values printed will be <code>0</code> and <code>89</code> since the derivative of <img alt="(x+2)^2 + x^3 = x^3 + x^2 + 4x + 4" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2B2%29%5E2+%2B+x%5E3+%3D+x%5E3+%2B+x%5E2+%2B+4x+%2B+4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x+2)^2 + x^3 = x^3 + x^2 + 4x + 4"/> equals <img alt="3x^2 + 2x +42" class="latex" src="https://s0.wp.com/latex.php?latex=3x%5E2+%2B+2x+%2B42&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="3x^2 + 2x +42"/>.</p>



<p>In the <a href="https://colab.research.google.com/drive/1eLEIMxlGZfAIYPWZMcBCPDLeVJzBMdYf?usp=sharing#scrollTo=0_nveKyfxXQK">notebook</a> you can see that we implement also the power function, and have some “convenience methods” (division etc..).</p>



<h3>Linear regression using back propagation and stochastic gradient descent</h3>



<p>In <em>stochastic gradient descent</em> we are given some data <img alt="(x_1,y_1),\ldots,(x_n,y_n)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_1%2Cy_1%29%2C%5Cldots%2C%28x_n%2Cy_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x_1,y_1),\ldots,(x_n,y_n)"/> and want to find an hypothesis <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="h"/> that minimizes the empirical loss <img alt="L(h) = \tfrac{1}{n}\sum_{i=1}^n L(h(x_i),y_i)" class="latex" src="https://s0.wp.com/latex.php?latex=L%28h%29+%3D+%5Ctfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En+L%28h%28x_i%29%2Cy_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(h) = \tfrac{1}{n}\sum_{i=1}^n L(h(x_i),y_i)"/> where <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L"/> is a <em>loss function</em> mapping two labels <img alt="y, y'" class="latex" src="https://s0.wp.com/latex.php?latex=y%2C+y%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y, y'"/> to a real number. If we let <img alt="L_i(h)" class="latex" src="https://s0.wp.com/latex.php?latex=L_i%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_i(h)"/> be the <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/>-th term of this sum, then, identifying <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="h"/> with the parameters (i.e., real numbers) that specify it, stochastic gradient descent is the following algorithm:</p>



<ol><li>Set <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="h"/> to be a random vector. Set <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/> to be some small number (e.g., <img alt="\eta = 0.1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta+%3D+0.1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta = 0.1"/>)</li><li>For <img alt="t \in {1,\ldots, T}" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Cin+%7B1%2C%5Cldots%2C+T%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="t \in {1,\ldots, T}"/> (where <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T"/> is the number of <em>epochs</em>):</li></ol>



<ul><li>For <img alt="i \in {1,\ldots, n}" class="latex" src="https://s0.wp.com/latex.php?latex=i+%5Cin+%7B1%2C%5Cldots%2C+n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i \in {1,\ldots, n}"/>: (in random order)<ul><li>Let <img alt="h \leftarrow h - \eta \nabla_h L_i(h)" class="latex" src="https://s0.wp.com/latex.php?latex=h+%5Cleftarrow+h+-+%5Ceta+%5Cnabla_h+L_i%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="h \leftarrow h - \eta \nabla_h L_i(h)"/></li></ul></li></ul>



<p>If <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="h"/> is specified by the parameters <img alt="h_1,\ldots,h_k" class="latex" src="https://s0.wp.com/latex.php?latex=h_1%2C%5Cldots%2Ch_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="h_1,\ldots,h_k"/> <img alt="\nabla_h L_i(h)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla_h+L_i%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla_h L_i(h)"/> is the vector <img alt="( \tfrac{\partial L_i}{\partial h_1}(h), \tfrac{\partial L_i}{\partial h_2}(h),\ldots, \tfrac{\partial L_i}{\partial h_k}(h))" class="latex" src="https://s0.wp.com/latex.php?latex=%28+%5Ctfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+h_1%7D%28h%29%2C+%5Ctfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+h_2%7D%28h%29%2C%5Cldots%2C+%5Ctfrac%7B%5Cpartial+L_i%7D%7B%5Cpartial+h_k%7D%28h%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="( \tfrac{\partial L_i}{\partial h_1}(h), \tfrac{\partial L_i}{\partial h_2}(h),\ldots, \tfrac{\partial L_i}{\partial h_k}(h))"/>. This is exactly the vector we can obtain using back propagation.</p>



<p>For example, if we want a linear model, we can use <img alt="(a,b)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a%2Cb%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(a,b)"/> as our parameters and the function will be <img alt="x \mapsto a\cdot + b" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+a%5Ccdot+%2B+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x \mapsto a\cdot + b"/>. We can generate random points <code>X</code>,<code>Y</code> as follows:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/4Do0KI4.png"/></figure>



<p>Now we can define a linear model as follows:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">class Linear:
  def __init__(self):
    self.a,self.b = Value(random.random()),Value(random.random())
  def __call__(self,x): return self.a*x+self.b

  def zero_grad(self):
    self.a.grad, self.b.grad = 0,0
</pre>


<p>And train it directly by using SGD:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">η = 0.03, epochs = 20
for t in range(epochs):
  for x,y in zip(X,Y):
    model.zero_grad()
    loss = (model(x)-y)**2
    loss.backward()
    model.a , model.b = (model.a - η*model.a.grad  , model.b - η*model.b.grad)
</pre>


<p>Which as you can see works very well:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/19kDCPM.gif"/></figure>



<h2>From linear classifiers to Neural Networks.</h2>



<p>The above was somewhat of an “overkill” for linear models, but the beautify of automatic differentiation is that we can easily use more complex computation.</p>



<p>We can follow <a href="https://github.com/karpathy/micrograd/blob/master/demo.ipynb">Karpathy’s demo</a> and us the same approach to train a neural network.</p>



<p>We will use a neural network that takes two inputs and has two hidden layers of width 16. A neuron that takes input <img alt="x_1,\ldots,x_k" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_k"/> will apply the ReLU function (<img alt="max{0,x}" class="latex" src="https://s0.wp.com/latex.php?latex=max%7B0%2Cx%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="max{0,x}"/>) to <img alt="\sum w_i x_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+w_i+x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum w_i x_i"/> where <img alt="w_1,\ldots,w_k" class="latex" src="https://s0.wp.com/latex.php?latex=w_1%2C%5Cldots%2Cw_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_1,\ldots,w_k"/> are its <em>weight</em> parameters. (It’s easy to add support for relu for our <code>Value</code> class. Also we won’t have a bias term in this example.)</p>



<p>The code for this Neural Network is as follows: (when <code>Value() </code>is called without a parameter the value is random number in <img alt="[-1,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="[-1,1]"/>)</p>


<pre class="brush: python; gutter: false; title: ; notranslate">def Neuron(weights,inputs, relu =True):
  # Evaluate neuron with given weights on given inputs
  v =  sum(weights[i]*x for i,x in enumerate(inputs))
  return v.relu() if relu else v


class Net:
  # Depth 3 fully connected neural net with one two inputs and output
  def __init__(self,  N=16):
    self.layer_1 = [[Value(),Value()] for i in range(N)]
    self.layer_2 = [ [Value() for j in range(N)] for i in range(N)]
    self.output =  [ Value() for i in range(N)]
    self.parameters = [v for L in [self.layer_1,self.layer_2,[self.output]] for w in L for v in w]


  def __call__(self,x):
    layer_1_vals = [Neuron(w,x) for w in self.layer_1]
    layer_2_vals = [Neuron(w,layer_1_vals) for w in self.layer_2]
    return Neuron(self.output,layer_2_vals,relu=False) 
    # the last output does not have the ReLU on top

  def zero_grad(self):
    for p in self.parameters:
      p.grad=0
</pre>


<p>We can train it in the same way as above.<br/>We will follow Karpathy and train it to classify the following points:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/cXv93KU.png"/></figure>



<p>The training code is very similar, with the following differences:</p>



<ul><li>Instead of the square loss, we use the function <img alt="L(y,y')= \max{ 1- y\cdot y', 0 }" class="latex" src="https://s0.wp.com/latex.php?latex=L%28y%2Cy%27%29%3D+%5Cmax%7B+1-+y%5Ccdot+y%27%2C+0+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(y,y')= \max{ 1- y\cdot y', 0 }"/> which is <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> if <img alt="y \cdot y' \geq 1" class="latex" src="https://s0.wp.com/latex.php?latex=y+%5Ccdot+y%27+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y \cdot y' \geq 1"/>. This makes sense since our data labels will be <img alt="\pm 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpm+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\pm 1"/> and we say we classify correctly if we get the same sign. We get zero loss if we classify correctly all samples with a margin of at least <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/>.</li><li>Instead of stochastic gradient descent we will do standard gradient descent, using all the datapoints before taking a gradient step. The optimal for neural networks is actually often something in the middle – <em>batch gradient descent</em> where we take a batch of samples and perform the gradient over them.</li></ul>



<p>The resulting code is the following:</p>


<pre class="brush: python; gutter: false; title: ; notranslate">for t in range(epochs):
  loss = sum([(1+ -y*model(x)).relu() for (x,y) in zip(X,Y)])/len(X)
  model.zero_grad()
  loss.backward()
  for p in model.parameters:
    p.data -= η*p.grad
</pre>


<p>If we use this, we get a decent approximation for this training set (see image below). As Karpathy shows, by adjusting the learning rate and using regularization, one can in fact get 100% accuracy.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/nbpNZyu.png"/></figure></div>
    </content>
    <updated>2020-11-03T23:14:08Z</updated>
    <published>2020-11-03T23:14:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-26T22:26:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7857</id>
    <link href="https://windowsontheory.org/2020/10/30/digging-into-election-models/" rel="alternate" type="text/html"/>
    <title>Digging into election models</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">With election on my mind, and constantly looking at polls and predictions, I thought I would look a little more into how election models are made. (Disclaimer: I am not an expert statistician / pollster and this is based on me trying to read their methodological description as well as looking into results of simulations … <a class="more-link" href="https://windowsontheory.org/2020/10/30/digging-into-election-models/">Continue reading <span class="screen-reader-text">Digging into election models</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>With election on my mind, and constantly looking at polls and predictions, I thought I would look a little more into how election models are made. (<strong>Disclaimer:</strong> I am not an expert statistician / pollster and this is based on me trying to read their methodological description as well as looking into results of simulations in Python. However, there is a <a href="https://colab.research.google.com/drive/1GaoBn71PwIk_uAisNWzp2Naqev9JHW0I?usp=sharing">colab notebook</a> so you can try this on your own!)</p>



<p>If polls were 100% accurate, then we would not need election models – we will know that the person polling at more than 50% in a given state will win, and we can just sum up the electoral votes. However, polls have various sources of errors:</p>



<ol><li><strong>Statistical sample error</strong> –  this is simply the deviation between the fraction of people that would say “I will vote for X” at time T in the population, and the empirical fraction reported by the poll based on their sample. As battleground states get polled frequently with large samples, this error is likely to be negligible.</li><li><strong>Sampling bias</strong> – this is the bias incurred by the fact that we cannot actually sample a random subset of the population and get them to answer our questions – the probability that people will pick up their phone may be correlated with their vote. Pollsters hope that these correlations all disappear once you condition on certain demographic variables (race, education, etc..) and so try to ensure the sample is balanced according to these metrics. I believe this was part of the reason that polls were off in 2016, since they didn’t explicitly adjust for levels of education (which were not strongly correlated with party before) and ended up under-representing white voters without college degrees.</li><li><strong>Lying responses or “shy” voters</strong> – Some people suggest that voters lie to pollsters because their choice is considered “socially undesirable”. There is <a href="https://fivethirtyeight.com/features/trump-supporters-arent-shy-but-polls-could-still-be-missing-some-of-them/">not much support</a> that this is a statistically significant effect. In particular <a href="https://morningconsult.com/form/shy-trump-2020/">one study</a> showed there was no statistically significant difference between responders’ responses in online and live calling. Also in  2016 polls equally under-estimated the votes for Trump and Republican senators (which presumably didn’t have the same “social stigma” to them).</li><li><strong>Turnout estimates</strong> – Estimating the probability that a person supporting candidate X will actually show up to vote (or mail it in) is a bit of a dark art, and account for the gap in polls representing registered voters (which make no such estimates) and polls representing likely voters (which do). Since traditionally the Republican electorate is older and more well off, they tend to vote more reliably and hence likely voter estimates are typically better for republicans. The effect seems not to be very strong this year. Turnout might be particularly hard to predict this year, though it seems likely to be historically high.</li><li><strong>Voters changing their mind</strong> –  The poll is done at a given point in time and does not necessarily reflect voters views in election day. For example in 2016 it seems that many undecided voters broke for Trump. In this cycle the effect might be less pronounced since there are few undecided voters and “election day” is smoothed over a 2-4 week period due to early and mail-in voting.</li></ol>



<p>To a first approximation, a poll-based election model does the following:<br/><br/>1. Aggregates polls into national and state-wise predictions </p>



<p>2. Computes a probability distribution over the correlated error vectors (i.e. the vector <img alt="\vec{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Be%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\vec{e}"/>  with coordinate for each jurisdiction containing the deviation from the prediction)</p>



<p>3. Samples from the probability distribution over vectors to obtain probabilities over outcomes.</p>



<p>From a skim of <a href="https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/">538’s methodology</a> it seems that they do the following:</p>



<ol><li>Aggregate polls (weighing by quality, timeliness, adjusting for house effects, etc..). Earlier in the election cycle they also mix in “fundamentals” such as state of the economy etc.. though their weight decreases with time.</li><li>Estimate magnitude of national error (i.e., sample a value <img alt="E \in \mathbb{R}_+" class="latex" src="https://s0.wp.com/latex.php?latex=E+%5Cin+%5Cmathbb%7BR%7D_%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="E \in \mathbb{R}_+"/> according to some distribution that reflects the amount of national uncertainty.</li><li>(This is where I may be understanding wrong.) Sample a vector <img alt="\vec{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Be%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\vec{e}"/> whose entries sum up to <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> according to a correlated distribution, where the correlations between states depends on demographic, location, and other  factors. For each particular choice of $E$, because the sum is fixed, if a state has $E+X$ bias then on average the other states will need to compensate for this $-X$ bias, and hence this can create <a href="https://statmodeling.stat.columbia.edu/2020/10/24/reverse-engineering-the-problematic-tail-behavior-of-the-fivethirtyeight-presidential-election-forecast/">negative correlations</a> between states. (It is not clear that negative correlations are unreasonable – one could imagine policies that are deeply popular with population A and deeply unpopular with population B)</li></ol>



<p>From a skim of the <a href="https://projects.economist.com/us-2020-forecast/president/how-this-works">Economist’s methodology</a> it seems that they do the following:</p>



<ol><li>They start again with some estimate on the national popular vote, based on polls and fundamentals, and then assume it is distributed according to some probability distribution to account for errors.</li><li>They then compute some prior on “partisan lean” (difference between state and national popular vote) for each state. If we knew the popular vote and partisan lean perfectly then we would know the result. Again like good Bayesians they assume that the lean is distributed according to some probability distribution.</li><li>They update the prior based on state polls and other information</li><li>They sample from an error distribution <img alt="\vec{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Be%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\vec{e}"/> according to some explicit pairwise correlation matrix that has only non-negative entries (and hence you don’t get negative correlations in their model).</li></ol>



<p>So, given all of the above, how much do these models differ? Perhaps surprisingy, the answer is “not by much”.  To understand how they differ, I plotted for both models the following: </p>



<ol><li>The histogram of Biden’s popular vote margin</li><li>The probability of Biden to win conditioned on a particular margin</li></ol>



<p>Much of the methodological difference, including the issue of pairwise correlations, should manifest in 2, but eyeballing it, they don’t seem to differ that much. It seems that conditioned on a particular margin, both models give Biden similar probability to win. (In particular both models think that 3% margin is about 50/50, while 4% margin gives Biden about 80/20 chance). The main difference is actually in the first part of estimating the popular vote margin – 538 is more “conservative” and has fatter tails.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/election_models.png"><img alt="" class="wp-image-7867" src="https://windowsontheory.files.wordpress.com/2020/10/election_models.png?w=993"/></a></figure>



<p>If you want to check my data, see if I have a bug, or try your own analysis, you can use this <a href="https://colab.research.google.com/drive/1GaoBn71PwIk_uAisNWzp2Naqev9JHW0I?usp=sharing">colab notebook</a>.</p>



<h2>“Make your own needle”</h2>



<p>Another applications for such models is to help us adjust the priors as new information comes in. For example, it’s possible that Florida, North Carolina and Texas will report results early. If Biden loses one of these states, should we adjust our estimate of win probability significantly? It turns out that the answer depends on by how much he loses.</p>



<p>The following graphs show the updated win probability conditioned on a particular margin in a state. We see that winning or losing Florida, North Carolina, and Texas on their own doesn’t make much difference to the probability – it’s all about the margin. In contrast, losing Pennsylvania’s 20 electoral votes will make a significant difference to Biden’s chances.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/predict_states.png"><img alt="" class="wp-image-7869" src="https://windowsontheory.files.wordpress.com/2020/10/predict_states.png?w=1024"/></a></figure>



<p>(The non monotonicity is simply a side effect of having a finite number of simulation runs and would disappear in the limit.)</p></div>
    </content>
    <updated>2020-10-30T16:16:57Z</updated>
    <published>2020-10-30T16:16:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-26T22:26:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7851</id>
    <link href="https://windowsontheory.org/2020/10/30/im-with-her-but-4-years-too-late/" rel="alternate" type="text/html"/>
    <title>I’m with her (but 4 years too late)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In May 2016, after Donald Trump was elected as the republican nominee for president, I wrote the following blog post. I ended up not publishing it – this has always been a technical blog (and also more of a group blog, at the time). While the damage of a Donald Trump presidency was hypothetical at … <a class="more-link" href="https://windowsontheory.org/2020/10/30/im-with-her-but-4-years-too-late/">Continue reading <span class="screen-reader-text">I’m with her (but 4 years too late)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In May 2016, after Donald Trump was elected as the republican nominee for president, I wrote the following blog post. I ended up not publishing it – this has always been a technical blog (and also more of a group blog, at the time).  While the damage of a Donald Trump presidency was hypothetical at the time, it is now very real and in a second term the stakes are only higher. I once again hope American readers of this blog would do what they can to support Joe Biden.<br/></p>



<p>Note: this is not an invitation for a debate on who to vote for in the comments. At this point, if you are educated and following the news (as I imagine all readers of this blog are) then if you are not already convinced that Donald Trump is a danger to this country and the world, nothing I will say will change your mind. Similarly, nothing you will say will change mine. Hence this is just a call for those readers who already support Joe Biden to make sure they vote and think of how they can help with their money or time in other ways.</p>



<h2>I’m with Her</h2>



<p><strong>Boaz Barak / May 3, 2016</strong><br/><br/>The republican electorate, in their infinite wisdom, have just (essentially) finalized the election of Donald Trump as their nominee for the position of the president of the United States.</p>



<p>While I have my political views, I don’t consider myself a very political person, and have not (as far as I remember) ever written about politics in this blog. However, extreme times call for extreme measures. It is tempting to be fatalist or cynical about this process, and believe that whether Donald Trump or Hillary Clinton is elected doesn’t make much of a difference. Some people believe that the presidency shapes the person more than the other way around, and others feel that all politicians are anyway corrupt or that Hillary is not much better than trump since she voted for the Iraq war and gave paid speeches for Wall Street firms. I think the last two presidencies of George W. Bush and Barack Obama demonstrated that the identity of the president makes a huge difference. All evidence points out that with Trump this difference will be entirely in the negative direction.</p>



<p>While his chances might not be great, this is not a bet I’m comfortable taking. I plan to support Hillary Clinton as much as I can, and hope that other American readers of this blog will do the same</p></div>
    </content>
    <updated>2020-10-30T14:20:47Z</updated>
    <published>2020-10-30T14:20:47Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-26T22:26:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=505</id>
    <link href="https://tcsplus.wordpress.com/2020/10/29/tcs-talk-wednesday-november-4-shalev-ben-david-university-of-waterloo/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, November 4 — Shalev Ben-David, University of Waterloo</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, November 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Shalev Ben-David from University of Waterloo will speak about “Forecasting Algorithms, Minimax Theorems, and Randomized Lower Bounds” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, November 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Shalev Ben-David</strong> from University of Waterloo will speak about “<em>Forecasting Algorithms, Minimax Theorems, and Randomized Lower Bounds</em>” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: I will present a new approach to randomized lower bounds, particularly in the setting where we wish to give a fine-grained analysis of randomized algorithms that achieve small bias. The approach is as follows: instead of considering ordinary randomized algorithms which give an output in <img alt="\{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\{0,1\}"/> and may err, we switch models to look at “forecasting” randomized algorithms which output a confidence in <img alt="[0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="[0,1]"/> for whether they think the answer is 1. When scored by a proper scoring rule, the performance of the best forecasting algorithm is closely related to the bias of the best (ordinary) randomized algorithm, but is more amenable to analysis. <br/>As an application, I’ll present a new minimax theorem for randomized algorithms, which can be viewed as a strengthening of Yao’s minimax theorem. Yao’s minimax theorem guarantees the existence of a hard distribution for a function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="f"/> such that solving <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="f"/> against this distribution (to a desired error level) is as hard as solving <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="f"/> in the worst case (to that same error level). However, the hard distribution provided by Yao’s theorem depends on the chosen error level. Our minimax theorem removes this dependence, giving a distribution which certifies the hardness of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="f"/> against all bias levels at once. In recent work, we used this minimax theorem to give a tight composition theorem for randomized query complexity. <br/><br/>Based on joint work with Eric Blais.</p></div>
    </content>
    <updated>2020-10-29T19:35:16Z</updated>
    <published>2020-10-29T19:35:16Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-26T22:27:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7831</id>
    <link href="https://windowsontheory.org/2020/10/23/full-replica-symmetry-breaking-based-algorithms-for-dummies/" rel="alternate" type="text/html"/>
    <title>Full-replica-symmetry-breaking based algorithms for dummies</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">One of the fascinating lines of research in recent years has been a convergence between the statistical physics and theoretical computer science points of view on optimization problems.`This blog post is mainly a note to myself (i.e., I’m the “dummy” 😃), trying to work out some basic facts in some of this line of work. … <a class="more-link" href="https://windowsontheory.org/2020/10/23/full-replica-symmetry-breaking-based-algorithms-for-dummies/">Continue reading <span class="screen-reader-text">Full-replica-symmetry-breaking based algorithms for dummies</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One of the fascinating lines of research in recent years has been a convergence between the statistical physics and theoretical computer science points of view on optimization problems.<br/>`<br/>This blog post is mainly a note to myself (i.e., I’m the “dummy” <img alt="&#x1F603;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f603.png" style="height: 1em;"/>), trying to work out some basic facts in some of this line of work. it was inspired by this <a href="https://simons.berkeley.edu/talks/breaking-1rsb-random-max-nae-sat">excellent talk of Eliran Subag</a>, itself part of a great <a href="https://simons.berkeley.edu/workshops/schedule/14243">Simons institute workshop</a> which I am still planning to watch the talks of. I am posting this in case it’s useful for others, but this is quite rough, missing many references, and I imagine I have both math mistakes as well as inaccuracies in how I refer to the literature – would be grateful for comments!</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/ekkarv2xiaam8jw.png"><img alt="" class="wp-image-7833" src="https://windowsontheory.files.wordpress.com/2020/10/ekkarv2xiaam8jw-e1603479127948.png"/></a>Screen shot from <a href="https://simons.berkeley.edu/talks/breaking-1rsb-random-max-nae-sat">Eliran Subag’s talk</a> demonstrating the difference between “easy” and “hard” instances.</figure>



<p>In computer science, <em>optimization</em> is the task of finding an assignment <img alt="x_1,\ldots,x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_n"/> that minimizes some function <img alt="J(x_1,\ldots,x_n)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x_1,\ldots,x_n)"/>. In statistical physics we think of <img alt="x_1,\ldots,x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_n"/> as the states of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> particles, and <img alt="J(x_1,\ldots,x_n)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x_1,\ldots,x_n)"/> as the <em>energy</em> of this state. Finding the minimum assignment corresponds to finding the <em>ground state</em>, and another computational problem is sampling from the <em>Gibbs distribution</em> where the probability of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is proportional to <img alt="\exp(-\beta J(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+J%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\exp(-\beta J(x))"/> for some <img alt="\beta&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\beta&gt;0"/>.</p>



<p>Two prototypical examples of such problems are:</p>



<ol><li>Random 3SAT – in this case <img alt="x\in { \pm 1 }^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin+%7B+%5Cpm+1+%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\in { \pm 1 }^n"/> and <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/> is the number of clauses violated by the assignment <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> for a random formula.</li><li>Sherrington-Kirpatrick model – in this case <img alt="x \in { \pm 1 }^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B+%5Cpm+1+%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x \in { \pm 1 }^n"/> and <img alt="J(x)= \sum_{i,j} J_{i,j}x_ix_j" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29%3D+%5Csum_%7Bi%2Cj%7D+J_%7Bi%2Cj%7Dx_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)= \sum_{i,j} J_{i,j}x_ix_j"/> where <img alt="J_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=J_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J_{i,j}"/> are independent normal variables with variance <img alt="1/n" class="latex" src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/n"/> for <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/> and variance <img alt="2/n" class="latex" src="https://s0.wp.com/latex.php?latex=2%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2/n"/> for <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/>. (Another way to say it is that <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/> is the matrix <img alt="A+A^\top" class="latex" src="https://s0.wp.com/latex.php?latex=A%2BA%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A+A^\top"/> where <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/>‘s entries are chosen i.i.d from <img alt="N(0,\tfrac{1}{2n}))" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7B2n%7D%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,\tfrac{1}{2n}))"/>.)</li></ol>



<p>The physics and CS intuition is that these two problems have very different computational properties. For random 3SAT (of the appropriate density), it is believed that the set of solutions is “shattered” in the sense that it is partitioned to exponentially many clusters, separated from one another by large distance. It is conjectured that in this setting the problem will be computationally hard. Similarly from the statistical physics point of view, it is conjectured that if we were to start with the uniform distribution (i.e., a “hot” system) and “lower the temperature” (increase <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\beta"/>) at a rate that is not exponentially slow then we will get “stuck” at a “metastable” state. This is analogous how when we heat up sand and then cool it quickly then rather than returning to its original state, the sand will get stuck at the metastable state of glass.</p>



<p>In contrast for the Sherrington-Kirpatrick (SK) model, the geometry is more subtle, but interestingly this enables better algorithms. The SK model is extermely widely studied, with hundreds of papers, and was the inspiration for the simulated annealing algorithm. If memory serves me right, Sherrington and Kirpatrick made the wrong conjecture on the energy of the ground state, and then Parisi came up in 1979 with a wonderful and hugely influential way to compute this value. Parisi’s calculation was heuristic, but about 30 years later, first Talagrand and later Panchenko proved rigorously many of Parisi’s conjectures. (See this <a href="https://arxiv.org/abs/1211.1094v1">survey of Panchenko</a>.)</p>



<p>Recently <a href="https://arxiv.org/abs/1812.10897">Montanari</a> gave a polynomial time algorithm to find a state with energy that is arbitrarily close to the ground state’s. The algorithm relies on Parisi’s framework and in particular on the fact that the solution space has a property known as “full replica symmetry breaking (RSB)” / “ultrametricity”. Parisi’s derivations (and hence also Montanari’s analysis) are highly elaborate and I admit that I have not yet been able to fully follow it. The nice thing is that (as we’ll see) it is possible to describe at least some of the algorithmic results without going into this theory. In the end of the post I will discuss a bit some of the relation to this theory, which is the underlying inspiration for Subag’s results described here.</p>



<p><strong>Note:</strong> These papers and this blog post deal with the <em>search problem</em> of finding a solution that minimizes the objective. The <em>refutation problem</em> of certifying that this minimum is at least <img alt="-C" class="latex" src="https://s0.wp.com/latex.php?latex=-C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-C"/> for some <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="C"/> has often been studied. The computational complexity of these problems need not be identical. In particular there are cases where the search problem has an efficient algorithm achieving value <img alt="-C^*" class="latex" src="https://s0.wp.com/latex.php?latex=-C%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-C^*"/> but the best refutation algorithm can only certify that the value is at most <img alt="-C'" class="latex" src="https://s0.wp.com/latex.php?latex=-C%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-C'"/> for <img alt="C' \gg C^*" class="latex" src="https://s0.wp.com/latex.php?latex=C%27+%5Cgg+C%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="C' \gg C^*"/>.</p>



<h2>Analysis of a simpler setting</h2>



<p>Luckily, there is a similar computational problem, for which the analysis of analogous algorithm, which was <a href="https://arxiv.org/abs/1812.04588">discovered by Subag</a> and was the partial inspiration for Montanari’s work, is much simpler. Specifically, we consider the case where <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is an element of the unit sphere, and <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/> is a degree <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> polynomial with random Gaussian coefficients. Specifically, for every vector <img alt="\gamma = (\gamma_2,\ldots,\gamma_d)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%28%5Cgamma_2%2C%5Cldots%2C%5Cgamma_d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma = (\gamma_2,\ldots,\gamma_d)"/>, we let <img alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D+%2B+%5Ccdots+%2B+%5Cgamma_d+J%5Ed+%5Ccdot+x%5E%7B%5Cotimes+p%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}"/> where for every <img alt="p \in {2,\ldots, d }" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cin+%7B2%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p \in {2,\ldots, d }"/>, <img alt="J_p" class="latex" src="https://s0.wp.com/latex.php?latex=J_p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J_p"/> is a random tensor of order <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> whose <img alt="n^p" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n^p"/> coefficients are all chosen i.i.d in <img alt="N(0,1/n)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,1/n)"/>. (We assume that polynomial does not have constant or linear components.)</p>



<p>Depending on <img alt="\gamma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma"/>, the computational and geometrical properties of this problem can vary considerably. The case that <img alt="\gamma = (1,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%281%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma = (1,0,\ldots,0)"/> (i.e., only <img alt="J^2" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J^2"/> has a non-zero coeffiecent) corresponds to finding the unit vector <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> minimizing <img alt="x^\top J x" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%5Ctop+J+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^\top J x"/> for a random matrix <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, which of course corresponds to the efficiently solveable minimum eigenvector problem. In contrast, the case <img alt="\gamma = (0,1,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%280%2C1%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma = (0,1,0,\ldots,0)"/> corresponds to finding a rank one component of a random three-tensor, which is believed to be computationally difficult. The Parisi calculations give a precise condition <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> on the vector <img alt="\gamma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma"/> such that if <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> holds then the solution space has the “full RSB” property (and hence the problem is computationally easy) and if <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> does not hold then the solution space does not have this property (and potentially the problem is hard).</p>



<p>These calculations also give rise to the following theorem:</p>



<p><strong>Theorem (<a href="https://arxiv.org/abs/1512.08492">Chen and Sen, Proposition 2</a>):</strong> If <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> holds then in the limit <img alt="n \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n \rightarrow \infty"/>, <img alt="\min_{x : |x|=1} J(x) = -\int_0^1 \sqrt{\nu''(q)} dq" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bx+%3A+%7Cx%7C%3D1%7D+J%28x%29+%3D+-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min_{x : |x|=1} J(x) = -\int_0^1 \sqrt{\nu''(q)} dq"/>, where <img alt="\nu(q) = \sum_{p \geq 2} \gamma_p^2 q^p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu%28q%29+%3D+%5Csum_%7Bp+%5Cgeq+2%7D+%5Cgamma_p%5E2+q%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nu(q) = \sum_{p \geq 2} \gamma_p^2 q^p"/>. (That is, <img alt="\nu''" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nu''"/> is the second derivative of this univariate polynomial)</p>



<p>We will not discuss the proof of this theorem, but rather how, taking it as a black box, it leads to an algorithm for minimizing <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/> that achieves a near-optimal value (assuming <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> holds).</p>



<p>It is a nice exercise to show that for every two vectors <img alt="x,x'\in\mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cx%27%5Cin%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x,x'\in\mathbb{R}^n"/>, <img alt="\mathbb{E}_J [J(x)J(x')] = \nu(\langle x,x' \rangle)/n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_J+%5BJ%28x%29J%28x%27%29%5D+%3D+%5Cnu%28%5Clangle+x%2Cx%27+%5Crangle%29%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_J [J(x)J(x')] = \nu(\langle x,x' \rangle)/n"/>. Hence for any unit vector <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/> is a random variable with mean zero and standard deviation <img alt="\sqrt{\nu(1)/n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cnu%281%29%2Fn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sqrt{\nu(1)/n}"/>. Since (after some coarsening) the number of unit vectors of dimension <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> can be thought of as <img alt="c^n" class="latex" src="https://s0.wp.com/latex.php?latex=c%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="c^n"/> for some <img alt="c&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=c%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="c&gt;1"/>, and we expect the probability of deviating <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="t"/> standard deviations to be <img alt="\exp(-c' t^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-c%27+t%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\exp(-c' t^2)"/>, the minimum value of <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/> should be <img alt="-c'' \sqrt{\nu(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=-c%27%27+%5Csqrt%7B%5Cnu%281%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-c'' \sqrt{\nu(1)}"/> for some constant <img alt="c''" class="latex" src="https://s0.wp.com/latex.php?latex=c%27%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="c''"/>. However determining this constant is non trivial and is the result of the Parisi theory.</p>



<p>To get a better sense for the quantity <img alt="-\int_0^1 \sqrt{\nu''(q)} dq" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\int_0^1 \sqrt{\nu''(q)} dq"/>, let’s consider two simple cases:</p>



<ul><li>If <img alt="\gamma = (1,0,\ldots)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%281%2C0%2C%5Cldots%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma = (1,0,\ldots)"/> (i.e., <img alt="J(x) = \sum_{i,j}M_{i,j}x_ix_j" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Csum_%7Bi%2Cj%7DM_%7Bi%2Cj%7Dx_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) = \sum_{i,j}M_{i,j}x_ix_j"/> for random matrix <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M"/>) then <img alt="\nu(q)= q^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu%28q%29%3D+q%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nu(q)= q^2"/> and <img alt="\nu''(q) = 2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27%28q%29+%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nu''(q) = 2"/>, meaning that <img alt="-\int_0^1 \sqrt{\nu''(q)} dq = -\sqrt{2}" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%3D+-%5Csqrt%7B2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\int_0^1 \sqrt{\nu''(q)} dq = -\sqrt{2}"/>. This turns out to be the actual minimum value. Indeed in this case <img alt="\min_{|x|^2=1} J(x) = \tfrac{1}{2} \lambda_{min}(M + M^\top)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin_%7B%7Cx%7C%5E2%3D1%7D+J%28x%29+%3D+%5Ctfrac%7B1%7D%7B2%7D+%5Clambda_%7Bmin%7D%28M+%2B+M%5E%5Ctop%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min_{|x|^2=1} J(x) = \tfrac{1}{2} \lambda_{min}(M + M^\top)"/>. But the matrix <img alt="A=\tfrac{1}{2}(M+M^\top)" class="latex" src="https://s0.wp.com/latex.php?latex=A%3D%5Ctfrac%7B1%7D%7B2%7D%28M%2BM%5E%5Ctop%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A=\tfrac{1}{2}(M+M^\top)"/>‘s non diagonal entries are distributed like <img alt="N(0,\tfrac{1}{2n})" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7B2n%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,\tfrac{1}{2n})"/> and the diagonal entries like <img alt="N(0,\tfrac{1}{n})" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,\tfrac{1}{n})"/> which means that <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> is distributed as <img alt="\tfrac{1}{\sqrt{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{1}{\sqrt{2}}"/> times a random matrix <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/> from the <em>Gaussian Orthogonal Ensemble (GOE)</em> where <img alt="B_{i,j} \sim N(0,1/n)" class="latex" src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Cj%7D+%5Csim+N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B_{i,j} \sim N(0,1/n)"/> for off diagonal entries and <img alt="B_{i,i} \sim N(0,2/n)" class="latex" src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Ci%7D+%5Csim+N%280%2C2%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B_{i,i} \sim N(0,2/n)"/> for diagonal entries. The minimum eigenvalue of such matrices is known to be <img alt="-2\pm o(1)" class="latex" src="https://s0.wp.com/latex.php?latex=-2%5Cpm+o%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-2\pm o(1)"/> with high probability.<br/></li><li>If <img alt="\gamma = (0,\ldots,1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%280%2C%5Cldots%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma = (0,\ldots,1)"/> (i.e. <img alt="J(x) = T \cdot x^{\otimes d}" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+T+%5Ccdot+x%5E%7B%5Cotimes+d%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) = T \cdot x^{\otimes d}"/> for a random <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/>-tensor <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T"/>) then <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> does not hold. Indeed, in this case the value <img alt="-\int_0^1 \sqrt{\nu''(q)} dq = - \int_0^1 \sqrt{d (d-1)q^{d-2}} dq = - \sqrt{d(d-1)}\tfrac{1}{d/2-1} \approx -2" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%3D+-+%5Cint_0%5E1+%5Csqrt%7Bd+%28d-1%29q%5E%7Bd-2%7D%7D+dq+%3D+-+%5Csqrt%7Bd%28d-1%29%7D%5Ctfrac%7B1%7D%7Bd%2F2-1%7D+%5Capprox+-2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\int_0^1 \sqrt{\nu''(q)} dq = - \int_0^1 \sqrt{d (d-1)q^{d-2}} dq = - \sqrt{d(d-1)}\tfrac{1}{d/2-1} \approx -2"/> for large <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/>. However I believe (though didn’t find the reference) that the actual minimum tends to <img alt="-\infty" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\infty"/> with <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/>. </li></ul>



<p>While the particular form of the property <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> is not important for this post, there are several equivalent ways to state it, see Proposition 1 in <a href="https://arxiv.org/abs/1812.04588">Subag’s paper</a>. One of them is that the function <img alt="\nu''(t)^{-1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27%28t%29%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nu''(t)^{-1/2}"/> (note the negative exponent) is concave on the interval <img alt="(0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%280%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(0,1]"/>.<br/>It can be shown that this condition cannot be satisfied if <img alt="\gamma_2 = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_2 = 0"/>, and that for every setting of <img alt="\gamma_3,\ldots,\gamma_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_3%2C%5Cldots%2C%5Cgamma_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_3,\ldots,\gamma_d"/>, if <img alt="\gamma_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_2"/> is large enough then it will be satisfied.</p>



<p>The central result of Subag’s paper is the following:</p>



<p><strong>Theorem:</strong> For every <img alt="\epsilon&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\epsilon&gt;0"/>, there is a polynomial-time algorithm <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> such that on input random <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/> chosen according to the distribution above, with high probability <img alt="A(J)=x" class="latex" src="https://s0.wp.com/latex.php?latex=A%28J%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A(J)=x"/> such that <img alt="J(x) \leq -\int_0^1 \sqrt{\nu''(q)} dq + \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%5Cleq+-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%2B+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) \leq -\int_0^1 \sqrt{\nu''(q)} dq + \epsilon"/>.</p>



<p>The algorithm itself, and the idea behind the analysis are quite simple. In some sense it’s the second algorithm you would think of (or at least the second algorithm according to some ordering).</p>



<p>The first algorithm one would think of is gradient descent. We start at some initial point <img alt="x^0" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^0"/>, and repeat the transformation <img alt="x^{t+1} \leftarrow x^t - \eta \nabla J(x^t)" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+%5Cnabla+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{t+1} \leftarrow x^t - \eta \nabla J(x^t)"/> for some small <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/> (and normalizing the norm). Unfortunately, we will generally run into <em>saddle points</em> when we do so, with the gradient being zero. In fact, for simplicity, below we will always make the pessimistic assumption that we are constantly on a saddle point. (This assumption turns out to be true in the actual algorithm, and if it was not then we can always use gradient descent until we hit a saddle.)</p>



<p>The second algorithm one could think of would be to use the Hessian instead of the gradient. That is, repeat the transformation <img alt="x^{t+1} \leftarrow x^t - \eta u" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{t+1} \leftarrow x^t - \eta u"/> where <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> is the minimal eigenvector of <img alt="\nabla^2 J(x^t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J(x^t)"/> (i.e., the Hessian matrix <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H"/> such that <img alt="H_{i,j} = \tfrac{\partial J(x^t)}{\partial x_i \partial x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=H_%7Bi%2Cj%7D+%3D+%5Ctfrac%7B%5Cpartial+J%28x%5Et%29%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H_{i,j} = \tfrac{\partial J(x^t)}{\partial x_i \partial x_j}"/> ). By the Taylor approximation <img alt="J(x - \eta u) \approx J(x) - \eta \nabla J(x) \cdot u + \tfrac{1}{2} \eta^2 u^\top \nabla^2 J(x) u" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x+-+%5Ceta+u%29+%5Capprox+J%28x%29+-+%5Ceta+%5Cnabla+J%28x%29+%5Ccdot+u+%2B+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+u%5E%5Ctop+%5Cnabla%5E2+J%28x%29+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x - \eta u) \approx J(x) - \eta \nabla J(x) \cdot u + \tfrac{1}{2} \eta^2 u^\top \nabla^2 J(x) u"/> (and since we assume the gradient is zero) the change in the objective will be roughly <img alt="\tfrac{1}{2} \eta^2 \lambda_{min}(\nabla^2 J(x^t))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda_%7Bmin%7D%28%5Cnabla%5E2+J%28x%5Et%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{1}{2} \eta^2 \lambda_{min}(\nabla^2 J(x^t))"/>. (Because we assume the gradient vanishes, it will not make a difference whether we update with <img alt="-\eta u " class="latex" src="https://s0.wp.com/latex.php?latex=-%5Ceta+u+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\eta u "/> or <img alt="+\eta u" class="latex" src="https://s0.wp.com/latex.php?latex=%2B%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="+\eta u"/>, but we use <img alt="-\eta" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\eta"/> for consistency with gradient descent.)</p>



<p>The above approach is promising, but we still need some control over the norm. The way that Subag handles this is that he starts with <img alt="x^0 = 0" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E0+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^0 = 0"/>, and at each step takes a step in an orthogonal direction, and so within <img alt="1/\eta^2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\eta^2"/> steps he will get to a unit norm vector. That is, the algorithm is as follows:</p>



<p><strong>Algorithm:</strong></p>



<p><strong>Input:</strong> <img alt="J:\mathbb{R}^n \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=J%3A%5Cmathbb%7BR%7D%5En+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J:\mathbb{R}^n \rightarrow \mathbb{R}"/>.</p>



<p><strong>Goal:</strong> Find unit <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> approximately minimizing <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/></p>



<ol><li>Initialize <img alt="x^0 = 0^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E0+%3D+0%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^0 = 0^n"/></li><li>For <img alt="t=0,\ldots,1/\eta^2-1" class="latex" src="https://s0.wp.com/latex.php?latex=t%3D0%2C%5Cldots%2C1%2F%5Ceta%5E2-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="t=0,\ldots,1/\eta^2-1"/>: i. Let <img alt="u^t" class="latex" src="https://s0.wp.com/latex.php?latex=u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u^t"/> be a unit vector <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> such that <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> is orthogonal to <img alt="u^0,\ldots,u^{t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=u%5E0%2C%5Cldots%2Cu%5E%7Bt-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u^0,\ldots,u^{t-1}"/> and <img alt="u^\top \nabla^2 J(x^t) u \approx \lambda_{min}(\nabla^2 J(x^t))" class="latex" src="https://s0.wp.com/latex.php?latex=u%5E%5Ctop+%5Cnabla%5E2+J%28x%5Et%29+u+%5Capprox+%5Clambda_%7Bmin%7D%28%5Cnabla%5E2+J%28x%5Et%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u^\top \nabla^2 J(x^t) u \approx \lambda_{min}(\nabla^2 J(x^t))"/>. (Since the bottom eigenspace of <img alt="\nabla^2 J(x^t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J(x^t)"/> has large dimention, we can find a vector <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> that is not only nearly minimal eigenvector but also orthogonal to all prior ones. Also, as mentioned, we assume that <img alt="\nabla \cdot u \approx 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+%5Ccdot+u+%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla \cdot u \approx 0"/>.) ii. Set <img alt="x^{t+1} \leftarrow x^t - \eta u^t" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{t+1} \leftarrow x^t - \eta u^t"/>.</li><li>Output <img alt="x^{1/\eta^2} = -\eta\sum_{t=0}^{1/\eta^2-1} u^t" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B1%2F%5Ceta%5E2%7D+%3D+-%5Ceta%5Csum_%7Bt%3D0%7D%5E%7B1%2F%5Ceta%5E2-1%7D+u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{1/\eta^2} = -\eta\sum_{t=0}^{1/\eta^2-1} u^t"/></li></ol>



<p>(The fact that the number of steps is <img alt="1/\eta^2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\eta^2"/> and not <img alt="1/\eta" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\eta"/> is absolutely crucial for the algorithm’s success; without it we would not have been able to use the second order contribution that arise from the Hessian.)</p>



<p>If we define <img alt="\lambda_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda_t"/> to be the minimum eigenvalue at time <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="t"/>, we get that the final objective value achieved by the algorithm satisfies</p>



<p><img alt="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda_t" class="latex" src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Csum_%7Bt%3D1%7D%5E%7B1%2F%5Ceta%5E2%7D+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda_t"/></p>



<p>Now due to rotation invariance, the distribution of <img alt="\nabla^2 J" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J"/> at the point <img alt="x^t" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^t"/> is the same as <img alt="\nabla^2J" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2J"/> at the point <img alt="(|x^t|,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7Cx%5Et%7C%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(|x^t|,0,\ldots,0)"/>. Using concentration of measure arguments, it can be shown that the minimum eigenvalue of <img alt="\nabla^2 J(x^t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J(x^t)"/> will be close with high probability to the minimum eigenvalue of <img alt="\nabla^2 J(|x^t|,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28%7Cx%5Et%7C%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J(|x^t|,0,\ldots,0)"/>.<br/>Since <img alt="\|x^t\|^2 = \eta^2 t" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Cx%5Et%5C%7C%5E2+%3D+%5Ceta%5E2+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\|x^t\|^2 = \eta^2 t"/> we can write</p>



<p><img alt="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda(\sqrt{\eta^2 t})" class="latex" src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Csum_%7Bt%3D1%7D%5E%7B1%2F%5Ceta%5E2%7D+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda%28%5Csqrt%7B%5Ceta%5E2+t%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda(\sqrt{\eta^2 t})"/></p>



<p>where <img alt="\lambda(q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%28q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda(q)"/> is the minimum eigenvalue of <img alt="\nabla^2 J" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J"/> at the point <img alt="(q,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28q%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(q,0,\ldots,0)"/>.<br/>Taking <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/> to zero, we get that (approximately) the value of the solution output by the algorithm will satisfy</p>



<p><img alt="VAL = \int_0^1 \tfrac{1}{2} \lambda(\sqrt{q}) dq" class="latex" src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Cint_0%5E1+%5Ctfrac%7B1%7D%7B2%7D+%5Clambda%28%5Csqrt%7Bq%7D%29+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="VAL = \int_0^1 \tfrac{1}{2} \lambda(\sqrt{q}) dq"/></p>



<p>and hence the result will be completed by showing that</p>



<p><img alt="\lambda(\sqrt{q}) = 2 \sqrt{\nu''(q)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%28%5Csqrt%7Bq%7D%29+%3D+2+%5Csqrt%7B%5Cnu%27%27%28q%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda(\sqrt{q}) = 2 \sqrt{\nu''(q)}"/></p>



<p>To do this, let’s recall the definition of <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/>:</p>



<p><img alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D+%2B+%5Ccdots+%2B+%5Cgamma_d+J%5Ed+%5Ccdot+x%5E%7B%5Cotimes+p%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}"/> where for every <img alt="p \in {2,\ldots, d }" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cin+%7B2%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p \in {2,\ldots, d }"/>, <img alt="J_p" class="latex" src="https://s0.wp.com/latex.php?latex=J_p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J_p"/> is a random tensor of order <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> whose <img alt="n^p" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n^p"/> coefficients are all chosen i.i.d in <img alt="N(0,1/n)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,1/n)"/>.</p>



<p>For simplicity, let’s assume that <img alt="d=3" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d=3"/> and hence</p>



<p><img alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3}\;." class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D%5C%3B.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3}\;."/></p>



<p>(The calculations in the general case are similar)</p>



<p>The <img alt="i,j" class="latex" src="https://s0.wp.com/latex.php?latex=i%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i,j"/> entry of <img alt="\nabla^2 J(\sqrt{q},0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28%5Csqrt%7Bq%7D%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J(\sqrt{q},0,\ldots,0)"/> equals <img alt="\tfrac{\partial J(q,0,\ldots,0)}{\partial x_i \partial x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+J%28q%2C0%2C%5Cldots%2C0%29%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial J(q,0,\ldots,0)}{\partial x_i \partial x_j}"/>. The contribution of the <img alt="J^2" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J^2"/> component to this term only arises from the terms corresponding to either <img alt="x_ix_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_ix_j"/> or <img alt="x_jx_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_jx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_jx_i"/> and hence for <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/> it equals <img alt="\gamma_2 (J_{i,j}+J_{j,i})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+%28J_%7Bi%2Cj%7D%2BJ_%7Bj%2Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_2 (J_{i,j}+J_{j,i})"/> which is distributed like <img alt="\gamma_2 N(0,\tfrac{2}{n}) = N(0, \tfrac{2\gamma_2^2}{n})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+N%280%2C%5Ctfrac%7B2%7D%7Bn%7D%29+%3D+N%280%2C+%5Ctfrac%7B2%5Cgamma_2%5E2%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_2 N(0,\tfrac{2}{n}) = N(0, \tfrac{2\gamma_2^2}{n})"/>. For <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/>, since <img alt="(x_i^2)'' = 2" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_i%5E2%29%27%27+%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x_i^2)'' = 2"/>, the contributioon equals <img alt="2 \gamma_2 J_{i,i}" class="latex" src="https://s0.wp.com/latex.php?latex=2+%5Cgamma_2+J_%7Bi%2Ci%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2 \gamma_2 J_{i,i}"/> which is distributed like <img alt="N(0,\tfrac{4\gamma_2^2}{n})" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B4%5Cgamma_2%5E2%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,\tfrac{4\gamma_2^2}{n})"/>.</p>



<p>The contribution from the <img alt="J^3" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J^3"/> component comes (in the case <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/>) from all the <img alt="6=3!" class="latex" src="https://s0.wp.com/latex.php?latex=6%3D3%21&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="6=3!"/> terms involving <img alt="1,i,j" class="latex" src="https://s0.wp.com/latex.php?latex=1%2Ci%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1,i,j"/> that is, <img alt="\gamma_3 (J_{1,i,j}\sqrt{q} + J_{1,j,i}\sqrt{q} + J_{i,1,j}\sqrt{q}+J_{j,1,i}\sqrt{q}+J_{i,j,1}\sqrt{q}+J_{j,i,1})\sqrt{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_3+%28J_%7B1%2Ci%2Cj%7D%5Csqrt%7Bq%7D+%2B+J_%7B1%2Cj%2Ci%7D%5Csqrt%7Bq%7D+%2B+J_%7Bi%2C1%2Cj%7D%5Csqrt%7Bq%7D%2BJ_%7Bj%2C1%2Ci%7D%5Csqrt%7Bq%7D%2BJ_%7Bi%2Cj%2C1%7D%5Csqrt%7Bq%7D%2BJ_%7Bj%2Ci%2C1%7D%29%5Csqrt%7Bq%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_3 (J_{1,i,j}\sqrt{q} + J_{1,j,i}\sqrt{q} + J_{i,1,j}\sqrt{q}+J_{j,1,i}\sqrt{q}+J_{i,j,1}\sqrt{q}+J_{j,i,1})\sqrt{q}"/> which is distributed like <img alt="\gamma_3 N(0, \tfrac{6}{n}) = N(0, \tfrac{6\gamma_3^2 q}{n})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_3+N%280%2C+%5Ctfrac%7B6%7D%7Bn%7D%29+%3D+N%280%2C+%5Ctfrac%7B6%5Cgamma_3%5E2+q%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_3 N(0, \tfrac{6}{n}) = N(0, \tfrac{6\gamma_3^2 q}{n})"/>. For <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/> the contribution will be from the <img alt="3" class="latex" src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="3"/> terms involving <img alt="1,i" class="latex" src="https://s0.wp.com/latex.php?latex=1%2Ci&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1,i"/>, with each yielding a contribution of <img alt="2\sqrt{q}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bq%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2\sqrt{q}"/>, and hence the result will be distributed like <img alt="N(0,\tfrac{12 \gamma_3^2 q}{n})" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B12+%5Cgamma_3%5E2+q%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,\tfrac{12 \gamma_3^2 q}{n})"/>.</p>



<p>(More generally, for larger <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, the number of terms for distinct <img alt="i,j" class="latex" src="https://s0.wp.com/latex.php?latex=i%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i,j"/> is <img alt="p(p-1)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28p-1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(p-1)"/>, each contributing a Gaussian of standard deviation <img alt="(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Csqrt%7Bq%7D%29%5E%7Bp-2%7D%5Cgamma_p%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}"/>, while for <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/> we have <img alt="p(p-1)/2" class="latex" src="https://s0.wp.com/latex.php?latex=p%28p-1%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(p-1)/2"/> terms, each contributing a Gaussian of standard deviation <img alt="2(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=2%28%5Csqrt%7Bq%7D%29%5E%7Bp-2%7D%5Cgamma_p%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}"/>.)</p>



<p>Since the sum of Gaussians is a Gaussian we get that <img alt="\nabla^2J(q,0,\ldots,0){i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J%28q%2C0%2C%5Cldots%2C0%29%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2J(q,0,\ldots,0){i,j}"/> is distributed like a Gaussian with variance <img alt="\sum \gamma_p^2 p(p-1)q^{p-2}/n = \nu''(q)/n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+%5Cgamma_p%5E2+p%28p-1%29q%5E%7Bp-2%7D%2Fn+%3D+%5Cnu%27%27%28q%29%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum \gamma_p^2 p(p-1)q^{p-2}/n = \nu''(q)/n"/> for <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/>, and twice that for <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/>. This means that the minimum eigenvalue of <img alt="\nabla^2J(q,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J%28q%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2J(q,0,\ldots,0)"/> equals <img alt="\sqrt{\nu''(q)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cnu%27%27%28q%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sqrt{\nu''(q)}"/> times the minimum eigenvalue of a random matrix <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M"/> from the Gaussian Orthogonal Ensemble (i.e. <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M"/> is sampled via <img alt="M{i,j} \sim N(0,1/n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%7Bi%2Cj%7D+%5Csim+N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M{i,j} \sim N(0,1/n)"/>, <img alt="M_{i,i} \sim N(0,2/n)" class="latex" src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Ci%7D+%5Csim+N%280%2C2%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M_{i,i} \sim N(0,2/n)"/>). As mentioned above, it is known that this minimum eigenvalue is <img alt="-2+o(1)" class="latex" src="https://s0.wp.com/latex.php?latex=-2%2Bo%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-2+o(1)"/>, and in fact by the semi-circle law, for every <img alt="\epsilon&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\epsilon&gt;0"/>, the number of eigenvalues of value <img alt="\leq -2+\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+-2%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\leq -2+\epsilon"/> is <img alt="\Omega(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Omega(n)"/>, and so we can also pick one that is orthogonal to the previous directions. QED</p>



<h2>Full replica symmetry breaking and ultra-metricity</h2>



<p>The point of this blog post is that at least in the “mixed <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> spin” case considered by Subag, we can understand what the algorithm does and the value that it achieves without needing to go into the theory of the geometry of the solution space, but let me briefly discuss some of this theory. (As I mentioned, I am still reading through this, and so this part should be read with big grains of salt.)</p>



<p>The key object studied in this line of work is the probability distribution <img alt="\xi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cxi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\xi"/> of the dot product <img alt="\langle x,x' \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x%2Cx%27+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle x,x' \rangle"/> for <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> and <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x'"/> sampled independently from the Gibbs distribution induced by <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/>. (This probability distribution will depend on the number of dimensions <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/>, but we consider the case that <img alt="n \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n \rightarrow \infty"/>.)</p>



<p>Intuitively, there are several ways this probability distribution can behave, depending on how the solution space is “clustered”:</p>



<ul><li>If all solutions are inside a single “cluster”, in the sense that they are all of the form <img alt="x = x_* + e" class="latex" src="https://s0.wp.com/latex.php?latex=x+%3D+x_%2A+%2B+e&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x = x_* + e"/> where <img alt="x_*" class="latex" src="https://s0.wp.com/latex.php?latex=x_%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_*"/> is the “center” of the cluster and <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="e"/> is some random vector, then <img alt="\langle x,x' \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x%2Cx%27+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle x,x' \rangle"/> will be concentrated on the point <img alt="\| x_*\|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7C+x_%2A%5C%7C%5E2&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\| x_*\|^2"/>.<br/></li><li>If the solutions are inside a finite number <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/> of clusters, with centers <img alt="x_1,\ldots,x_k" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_k"/>, then the support of the distribution will be on the <img alt="k^2" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k^2"/> points <img alt="\{ \langle x_i,x_j \rangle \}_{i,j \in [k]}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Clangle+x_i%2Cx_j+%5Crangle+%5C%7D_%7Bi%2Cj+%5Cin+%5Bk%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\{ \langle x_i,x_j \rangle \}_{i,j \in [k]}"/>.<br/></li><li>Suppose that the solutions are inside a <em>hierarchy</em> of clusters. That is, suppose we have some rooted tree <img alt="\mathcal{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{T}"/> (e.g., think of a depth <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> full binary tree), and we associate a vector <img alt="x_v" class="latex" src="https://s0.wp.com/latex.php?latex=x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_v"/> with every vertex <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/> of <img alt="\mathcal{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{T}"/>, with the property that <img alt="x_v" class="latex" src="https://s0.wp.com/latex.php?latex=x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_v"/> is orthogonal to all vectors associated with <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/>‘s ancestors on the tree. Now imagine that the distribution is obtained by taking a random leaf <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> of <img alt="\mathcal{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{T}"/> and outputting <img alt="\sum x_v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum x_v"/> for all vertices <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/> on the path from the root to <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/>. In such a case the dot product of <img alt="x_u" class="latex" src="https://s0.wp.com/latex.php?latex=x_u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_u"/> and <img alt="x_{u'}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bu%27%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_{u'}"/> will be <img alt="\sum_v \|x_v\|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_v+%5C%7Cx_v%5C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_v \|x_v\|^2"/> taken over all the common ancestors of <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/>. As the dimension and depth of the tree goes to infinity, the distribution over dot product can have continuous support, and it is this setting (specifically when the support is an interval <img alt="[0,q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5B0%2Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="[0,q)"/>) that is known as <em>full replica symmetry breaking</em>. Because the dot product is determined by common ancestor, for every three vectors <img alt="x_u,x_v,x_w" class="latex" src="https://s0.wp.com/latex.php?latex=x_u%2Cx_v%2Cx_w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_u,x_v,x_w"/> in the support of the distribution <img alt="\langle x_v,x_w \rangle \geq \min \{ \langle x_u,x_v \rangle, \langle x_u,x_w \rangle \}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x_v%2Cx_w+%5Crangle+%5Cgeq+%5Cmin+%5C%7B+%5Clangle+x_u%2Cx_v+%5Crangle%2C+%5Clangle+x_u%2Cx_w+%5Crangle+%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle x_v,x_w \rangle \geq \min \{ \langle x_u,x_v \rangle, \langle x_u,x_w \rangle \}"/> or <img alt="\| x_v - x_w \| \leq \max \{ \|x_u - x_v \|, \|x_u -x_w \|\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7C+x_v+-+x_w+%5C%7C+%5Cleq+%5Cmax+%5C%7B+%5C%7Cx_u+-+x_v+%5C%7C%2C+%5C%7Cx_u+-x_w+%5C%7C%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\| x_v - x_w \| \leq \max \{ \|x_u - x_v \|, \|x_u -x_w \|\}"/>. It is this condition that known as <em>ultra-metricity</em>.</li></ul>



<p>In Subag’s algorithm, as mentioned above, at any given step we could make an update of either <img alt="x^{t+1} \leftarrow x^t - \eta u" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{t+1} \leftarrow x^t - \eta u"/> or <img alt="x^{t+1} \leftarrow x^t + \eta u" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+%2B+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{t+1} \leftarrow x^t + \eta u"/>. If we think of all the possible choices for the signs in the <img alt="d=1/\eta^2" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d=1/\eta^2"/> of the algorithms, we see that the algorithm does not only produce a single vector but actually <img alt="2^d" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2^d"/> such vectors that are arranged in an ultrametric tree just as above. Indeed, this ultrametric structure was the inspiration for the algorithm and is the reason why the algorithm produces the correct result precisely in the full replica symmetry breaking regime.</p>



<p><strong>Acknowledgements:</strong> Thanks to Tselil Schramm for helpful comments.</p></div>
    </content>
    <updated>2020-10-23T19:01:17Z</updated>
    <published>2020-10-23T19:01:17Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-26T22:26:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=499</id>
    <link href="https://tcsplus.wordpress.com/2020/10/22/tcs-talk-wednesday-october-28-omar-montasser-ttic/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 28 — Omar Montasser, TTIC</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Omar Montasser from TTIC will speak about “Adversarially Robust Learnability: Characterization and Reductions” (abstract below). You can reserve a spot as an individual or a group to join […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Omar Montasser</strong> from TTIC will speak about “<em>Adversarially Robust Learnability: Characterization and Reductions</em>” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We study the question of learning an adversarially robust predictor from uncorrupted samples. We show that any VC class <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="H"/> is robustly PAC learnable, but we also show that such learning must sometimes be improper (i.e. use predictors from outside the class), as some VC classes are not robustly properly learnable.  In particular, the popular robust empirical risk minimization approach (also known as adversarial training), which is proper, cannot robustly learn all VC classes.  After establishing learnability, we turn to ask whether having a tractable non-robust learning algorithm is sufficient for tractable robust learnability and give a reduction algorithm for robustly learning any hypothesis class <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="H"/> using a non-robust PAC learner for <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="H"/>, with nearly-optimal oracle complexity.<br/>This is based on joint work with Steve Hanneke and Nati Srebro, available at <a href="https://arxiv.org/abs/1902.04217">https://arxiv.org/abs/1902.04217</a>.</p></div>
    </content>
    <updated>2020-10-22T14:48:14Z</updated>
    <published>2020-10-22T14:48:14Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-26T22:27:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=493</id>
    <link href="https://tcsplus.wordpress.com/2020/10/16/tcs-talk-wednesday-october-21-aayush-jain-ucla/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 21 — Aayush Jain, UCLA</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Aayush Jain from UCLA will speak about “Indistinguishability Obfuscation from Well-Founded Assumptions” (abstract below). You can reserve a spot as an individual or a group to join us […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Aayush Jain</strong> from UCLA will speak about “<em>Indistinguishability Obfuscation from Well-Founded Assumptions</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We present a construction of an indistinguishability obfuscation scheme, whose security rests on the subexponential hardness of four well-founded assumptions. We show the existence of an indistinguishability Obfuscation scheme for all circuits assuming sub-exponential security of the following assumptions:</p>



<ul class="wp-block-quote"><li>The Learning with Errors (LWE) assumption with arbitrarily small subexponential modulus-to-noise ratio,</li><li>The SXDH assumption with respect to bilinear groups of prime order <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="p"/>,</li><li>Existence of a Boolean Pseudorandom Generator (PRG) in <img alt="\mathsf{NC}^0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathsf{NC}^0"/> with arbitrary polynomial stretch, that is, mapping <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n"/> bits to <img alt="n^{1+\tau}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2B%5Ctau%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n^{1+\tau}"/> bits, for any constant \tau&gt;0.</li><li>The Learning Parity with Noise (LPN) assumption over <img alt="\mathbb{Z}_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathbb{Z}_p"/> with error-rate <img alt="\ell^{-\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell%5E%7B-%5Cdelta%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\ell^{-\delta}"/>, where <img alt="\ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\ell"/> is the dimension of the secret and <img alt="\delta&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%3E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\delta&gt;0"/> is an arbitrarily small constant.<br/>Further, assuming only polynomial security of these assumptions, there exists a compact public-key functional encryption scheme for all circuits.</li></ul>



<p class="wp-block-quote">The main technical novelty is the introduction and construction of a cryptographic pseudorandom generator that we call a Structured-Seed PRG (sPRG), assuming LPN over <img alt="\mathbb{Z}_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathbb{Z}_p"/> and PRGs in <img alt="\mathsf{NC}^0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathsf{NC}^0"/>. During the talk, I will discuss how structured seed PRGs have evolved from different notions of novel pseudorandom generators proposed in the past few years, and how an interplay between different areas of theoretical computer science played a major role in providing valuable insights leading to this work. Time permitting, I will go into the details of how to construct sPRGs. <br/><br/>Joint work with Huijia (Rachel) Lin and Amit Sahai</p></div>
    </content>
    <updated>2020-10-16T06:33:00Z</updated>
    <published>2020-10-16T06:33:00Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-26T22:27:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=1380</id>
    <link href="https://francisbach.com/hermite-polynomials/" rel="alternate" type="text/html"/>
    <title>Polynomial magic III : Hermite polynomials</title>
    <summary>After two blog posts earlier this year on Chebyshev and Jacobi polynomials, I am coming back to orthogonal polynomials, with Hermite polynomials. This time, in terms of applications to machine learning, no acceleration, but some interesting closed-form expansions in positive-definite kernel methods. Definition and first properties There are many equivalent ways to define Hermite polynomials....</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">After two blog posts earlier this year on <a href="https://francisbach.com/chebyshev-polynomials/">Chebyshev</a> and <a href="https://francisbach.com/jacobi-polynomials/">Jacobi</a> polynomials, I am coming back to orthogonal polynomials, with Hermite polynomials. </p>



<p class="justify-text">This time, in terms of applications to machine learning, no acceleration, but some interesting closed-form expansions in positive-definite kernel methods. </p>



<h2>Definition and first properties</h2>



<p class="justify-text">There are many equivalent ways to define Hermite polynomials. A natural one is through the so-called <a href="https://en.wikipedia.org/wiki/Rodrigues%27_formula">Rodrigues’ formula</a>: $$H_k(x) = (-1)^k e^{x^2} \frac{d^k}{d x^k}\big[ e^{-x^2} \big],$$ from which we can deduce \(H_0(x) = 1\), \(H_1(x) =\   – e^{x^2} \big[ -2x e^{-x^2} \big] = 2x\), \(H_2(x) = e^{x^2} \big[ (-2x)^2e^{-x^2} -2 e^{-x^2}  \big] =  4x^2 – 2\), etc.</p>



<p class="justify-text">Other simple properties which are consequences of the definition (and can be shown by recursion) are that \(H_k\) is a polynomial of degree \(k\), with the same parity as \(k\), and with a leading coefficient equal to \(2^k\).</p>



<p class="justify-text"><strong>Orthogonality for Gaussian distribution.</strong> Using integration by parts, one can show (see end of the post) that for \(k \neq \ell\), we have $$\int_{-\infty}^{+\infty}  \!\!\!H_k(x) H_\ell(x) e^{-x^2} dx =0, $$ and that for \(k=\ell\), we have $$\int_{-\infty}^{+\infty} \!\!\! H_k(x)^2 e^{-x^2}dx = \sqrt{\pi} 2^k k!.$$ </p>



<p class="justify-text">In other words, the Hermite polynomials are orthogonal for the Gaussian distribution with mean \(0\) and variance \(\frac{1}{2}\). Yet in other words, defining the <em>Hermite functions</em> as \( \displaystyle \psi_k(x) = (\sqrt{\pi} 2^k k!)^{-1/2} H_k(x) e^{-x^2/2}\), we obtain an orthonormal basis of \(L_2(dx)\). As illustrated below, the Hermite functions, as the index \(k\) increases, have an increasing “support” (the support is always the entire real line, but most of the mass is concentrated in centered balls of increasing sizes, essentially at \(\sqrt{k}\)) and, like cosines and sines, an increasingly oscillatory behavior.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4579" height="305" src="https://francisbach.com/wp-content/uploads/2020/08/hermite.gif" width="349"/>Plot of Hermite functions \(\psi_k(x) = (\sqrt{\pi} 2^k k!)^{-1/2} H_k(x) e^{-x^2/2}\), from \(k=0\) to \(k=20\).</figure></div>



<p class="justify-text">Among such orthonormal bases, the Hermite functions happen to be diagonalizing the Fourier tranform operator.  In other words, the Fourier transform of \(\psi_k\) (for the definition making it an isometry of \(L_2(dx)\)) is equal to $$ \mathcal{F}(\psi_k)(\omega)  =  \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{+\infty} \psi_k(x) e^{- i \omega x} dx = (-i)^k \psi_k(\omega).$$ (note that the eigenvalues are all of unit modulus as we have an isometry). See a proof at the end of the post. I am not aware of any applications of this property in machine learning or statistics (but there are probably some).</p>



<p class="justify-text"><strong>Recurrence.</strong> In order to compute Hermite polynomials, the following recurrence relation is the most useful $$ H_{k+1}(x) = 2x H_k(x) \ – 2k H_{k-1}(x). \tag{1}$$  Such recursions are always available for orthogonal polynomials (see [4]), but it takes here a particularly simple form (see a proof at the end of the post).</p>



<p class="justify-text"><strong>Generating function.</strong> The following property is central in many proofs of properties of Hermite polynomials: for all \(t \in \mathbb{R}\), we have $$\sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) =e^{ 2xt \ – \ t^2}, \tag{2}$$ with a proof at the end of the post based on the residue theorem.</p>



<h2>Further (less standard) properties</h2>



<p class="justify-text">For the later developments, we need other properties which are less standard (there are many other interesting properties, which are not useful for this post, see <a href="https://en.wikipedia.org/wiki/Hermite_polynomials">here</a>).</p>



<p class="justify-text"><strong>Mehler formula. </strong>For \(|\rho| &lt; 1\), it states: $$ \exp \Big( – \frac{\rho}{1- \rho^2} (x-y)^2\Big) = \sqrt{1-\rho^2} \sum_{k=0}^\infty \frac{\rho^k}{2^k k!} H_k(x) H_k(y) \exp \Big( – \frac{\rho}{1+\rho} (x^2 + y^2) \Big).$$ The proof is significantly more involved; see [<a href="https://academic.oup.com/jlms/article-pdf/s1-8/3/194/2363185/s1-8-3-194.pdf">1</a>] for details (with a great last sentence: “Prof. Hardy tells me that he has not seen his proof in print, though the inevitability of the successive steps makes him think that it is unlikely to be new”). Note that we will in fact obtain a new proof from the relationship with kernel methods (see below).</p>



<p class="justify-text"><strong>Expectation for Gaussian distributions. </strong>We will need this property for \(|\rho|&lt;1\) (see proof at the end of the post), which corresponds to the expectation of \(H_k(x)\) for \(x\) distributed as a non-centered Gaussian distribution: $$\int_{-\infty}^\infty H_k(x) \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big)dx= \sqrt{\pi} \rho^k \sqrt{1-\rho^2} H_k (y). \tag{3}$$</p>



<p class="justify-text">Given the relationship with the Gaussian distribution, it is no surprise that Hermite polynomials pop up whenever Gaussians are used, as distributions or kernels. Before looking into it, let’s first give a brief review of kernel methods.</p>



<h2>From positive-definite kernel to Hilbert spaces</h2>



<p class="justify-text">Given a prediction problem with inputs in a set \(\mathcal{X}\), a traditional way of parameterizing real-valued functions on \(\mathcal{X}\) is to use <em>positive-definite kernels</em>.</p>



<p class="justify-text">A positive-definite kernel is a function \(K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\) such that for all sets \(\{x_1,\dots,x_n\}\) of \(n\) elements of \(\mathcal{X}\), the “kernel matrix” in \(\mathbb{R}^{n \times n}\) composed of pairwise evaluations is symmetric positive semi-definite. This property happens to be equivalent to the existence of a Hilbert feature space \(\mathcal{H}\) and a feature map \(\varphi: \mathcal{X} \to \mathcal{H}\) such that $$K(x,x’) = \langle \varphi(x), \varphi(x’) \rangle_{\mathcal{H}},$$ with an elegant constructive proof [<a href="https://www.ams.org/journals/tran/1950-068-03/S0002-9947-1950-0051437-7/S0002-9947-1950-0051437-7.pdf">15</a>].</p>



<p class="justify-text">This allows to define the space of linear functions on the features, that is, functions of the form $$f(x) = \langle f, \varphi(x) \rangle_{\mathcal{H}},$$ for \(f \in \mathcal{H}\). </p>



<p class="justify-text">This space is often called the <a class="" href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space</a> (RKHS) associated to the kernel \(K\) (we can prove that it is indeed uniquely defined). In such a space, we can also define the squared norm of the function \(f\), namely \(\| f\|_{\mathcal{H}}^2\), which can be seen as a specific regularization term in kernel methods.</p>



<p class="justify-text">The space satisfies the so-called reproducing property (hence its name): \(f(x) = \langle f, K(\cdot,x) \rangle_{\mathcal{H}}\). In other words, the feature \(\varphi(x)\) is the kernel function evaluated at \(x\), that is,  \(\varphi(x) = K(\cdot,x)\). These spaces have been a source of many developments in statistics [5] and machine learning [6, 7].</p>



<p class="justify-text"><strong>Orthonormal basis.</strong> A difficulty in working with infinite-dimensional Hilbert spaces of functions is that it is sometimes hard to understand what functions are actually considered. One simple way to enhance understanding of the regularization property is to have an orthonormal basis (in very much the same way as the Fourier basis), as we can then identify \(\mathcal{H}\) to the space of squared-integrable sequences.</p>



<p class="justify-text">For kernel-based Hilbert spaces, if we have an orthonormal basis \((g_k)_{k \geqslant 0}\) of the Hilbert space \(\mathcal{H}\), then, by decomposing \(\varphi(x)\) in the basis, we have $$\varphi(x) = \sum_{k =0}^\infty \langle \varphi(x), g_k \rangle_\mathcal{H} g_k,$$ we get $$K(x,y) = \langle \varphi(y), \varphi(x) \rangle = \sum_{k =0}^\infty \langle \varphi(x), g_k \rangle_\mathcal{H} \langle  \varphi(y), g_k \rangle_\mathcal{H} =\sum_{k=0}^\infty g_k(x) g_k(y), \tag{4}$$ that is, we have an expansion of the kernel as an infinite sum (note here, that we ignore summability issues).</p>



<p class="justify-text">Among orthonormal bases, some are more interesting than others. The ones composed of eigenfunctions for particular operators are really more interesting, in particular for the covariance operator that we now present, and their use in statistical learning theory.</p>



<h2>Analyzing ridge regression through covariance operators</h2>



<p class="justify-text">The most classical problem where regularization by RKHS norms occurs is <em>ridge regression</em>, where, given some observations \((x_1,y_1),\dots,(x_n,y_n) \in \mathcal{X} \times \mathbb{R}\), one minimizes with respect to \(f \in \mathcal{H}\): $$ \frac{1}{n} \sum_{i=1}^n \big( y_i \ – \langle f, \varphi(x_i) \rangle_{\mathcal{H}} \big)^2 +  \lambda \| f\|_{\mathcal{H}}^2.$$</p>



<p class="justify-text">In finite dimensions, the convergence properties are characterized by the (non-centered) covariance matrix \(\Sigma = \mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big]\), where the expectation is taken with respect to the underlying distribution of the observations \(x_1,\dots,x_n\) (which are assumed independently and identically distributed for simplicity). If \(\mathcal{H} = \mathbb{R}^d\), then \(\Sigma\) is a \(d \times d\) matrix. </p>



<p class="justify-text">For infinite-dimensional \(\mathcal{H}\), the same expression \(\Sigma = \mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big]\) defines a linear <em>operator</em> from \(\mathcal{H}\) to  \(\mathcal{H}\), so that for \(f,g \in \mathcal{H}\), we have $$\langle f, \Sigma g \rangle_{\mathcal{H}} = \mathbb{E} \big[ \langle f, \varphi(x)\rangle_{\mathcal{H}}\langle g, \varphi(x)\rangle_{\mathcal{H}}\big] = \mathbb{E} \big[ f(x) g(x) \big].$$</p>



<p class="justify-text">The generalization property of ridge regression has been thoroughly studied (see, e.g., [8, 9]), and if there exists \(f_\ast \in \mathcal{H}\) such that \(y_i = \langle f_\ast, \varphi(x_i) \rangle + \varepsilon_i\) for a noise \(\varepsilon_i\) which is independent of \(x_i\), with zero mean and variance equal to \(\sigma^2\), then the expected error on unseen data is asymptotically upper-bounded by $$\sigma^2 + \lambda \| f_\ast\|_{\mathcal{H}}^2 + \frac{\sigma^2}{n} {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big].$$ The first term \(\sigma^2\) is the best possible expected performance, the term \(\lambda \| f_\ast\|_{\mathcal{H}}^2\) is usually referred to as the <em>bias</em> term and characterizes the bias introduced by regularizing towards zero, while the third term \(\frac{\sigma^2}{n} {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]\) is the <em>variance</em> term, which characterizes the loss in performance due to the observation of only \(n\) observations.</p>



<p class="justify-text">The quantity \({\rm df}(\lambda) = {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]\) is often referred to as the degrees of freedom [10]. When \(\lambda\) tends to infinity, then \({\rm df}(\lambda)\) tends to zero; when \(\lambda\) tends to zero, then \({\rm df}(\lambda)\) tends to the number of non-zero eigenvalues of \(\Sigma\). Thus, in finite dimension, this typically leads to the underlying dimension. Given the usual variance term in \(\sigma^2 \frac{d}{n}\) for ordinary least-squares with \(d\)-dimensional features, \({\rm df}(\lambda)\) is often seen as an implicit number of parameters for kernel ridge regression.</p>



<p class="justify-text">In infinite dimensions, under mild assumptions, there are infinitely many eigenvalues for \(\Sigma\), which form a decreasing sequence \((\lambda_i)_{i \geqslant 0}\) that tends to zero (and is summable, with a sum equal to the trace of \(\Sigma\)). The rate of such a decay is key to understanding the generalization capabilities of kernel methods. With the following classical types of decays:</p>



<ul class="justify-text"><li><em>Polynomial decays</em>: If \(\lambda_i \leqslant \frac{C}{(i+1)^{\alpha}}\) for \(\alpha &gt; 1\), then one can upper bound the sum by an integral as $$ {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big] = \sum_{i=0}^\infty \frac{\lambda_i}{\lambda_i + \lambda} \leqslant \sum_{i=1}^\infty \frac{1}{1  + \lambda i^\alpha / C} \leqslant \int_0^\infty \frac{1}{1+\lambda t^\alpha / C} dt.$$ With the change of variable \(u = \lambda t^\alpha / C\), we get that \({\rm df}(\lambda) = O(\lambda^{-\alpha})\). We can then balance bias and variance with \(\lambda \sim n^{-\alpha/(\alpha+1)}\) and an excess risk proportional to \(n^{-\alpha/(\alpha+1)}\). This type of decay is typical of <a href="https://en.wikipedia.org/wiki/Sobolev_space">Sobolev spaces</a>.</li><li><em>Exponential decays</em>: If \(\lambda_i \leqslant {C}e^{-\alpha i}\), for some \(\alpha &gt;0\), we have $$ {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]  \leqslant \sum_{i=0}^\infty \frac{{C}e^{-\alpha i}}{  \lambda + {C}e^{-\alpha i}} \leqslant \int_{0}^\infty \frac{{C}e^{-\alpha t}}{ \lambda + {C}e^{-\alpha t}}dt.$$ With the change of variable \(u = e^{-\alpha t}\), we get an upper bound $$\int_{0}^1 \frac{C}{\alpha}\frac{1}{ \lambda + {C}u}du = \frac{1}{\alpha}\big[ \log(\lambda + C) \ – \log (\lambda) \big] = \frac{1}{\alpha} \log \big( 1+\frac{C}{\lambda} \big).$$ We can then balance bias and variance with \(\lambda \sim 1/n \) and an excess risk proportional to \((\log n) / n \), which is very close to the usual parametric (finite-dimensional) rate in \(O(1/n)\). We will see an example of this phenomenon for the Gaussian kernel.</li></ul>



<p class="justify-text">In order to analyze the generalization capabilities, we consider a measure \(d \mu\) on \(\mathcal{X}\), and the following (non-centered) <em>covariance operator</em> defined above as $$\mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big],$$ which is now an self-adjoint operator from \(\mathcal{H}\) to \(\mathcal{H}\) with a finite trace. The traditional empirical estimator \(\hat\Sigma = \frac{1}{n} \sum_{i=1}^n \varphi(x_i) \otimes \varphi(x_i)\), whose eigenvalues are the same as the eigenvalues of \(1/n\) times the \(n \times n\) kernel matrix of pairwise kernel evaluations (see simulation below).</p>



<p class="justify-text"><strong>Characterizing eigenfunctions.</strong> If \((g_k)\) is the eigenbasis associated to the eigenfunctions of \(\Sigma\), then it has to be an orthogonal family that span the entire space \(\mathcal{H}\) and such that \(\Sigma g_k = \lambda_k g_k\). Applying it to \(\varphi(y) = K(\cdot,y)\), we get $$ \langle K(\cdot,y), \Sigma g_k \rangle_{\mathcal{H}} = \mathbb{E} \big[ K(x,y) g_k(x) \big] = \lambda_k \langle g_k, \varphi(y)\rangle_\mathcal{H} =  \lambda_k g_k(y),$$ which implies that the functions also have to be eigenfunctions of the self-adjoint so-called <em>integral operator</em> \(T\) defined on \(L_2(d\mu)\) as \(T f(y) = \int_{\mathcal{X}} K(x,y) f(y) d\mu(y)\). Below, we will check this property. Note that this other notion of integral operator (defined on \(L_2(d\mu)\) and not in \(\mathcal{H}\)), which has the same eigenvalues and eigenfunctions, is important to deal with mis-specified models (see [9]). Note that the eigenfunctions \(g_k\) are orthogonal for both dot-products in \(L_2(d\mu)\) and \(\mathcal{H}\), but that the normalization to unit norm differs. If \(\| g_k \|_{L_2(d\mu)}=1\) for all \(k \geqslant 0\), then we have \( \| g_k \|^2_\mathcal{H}=  \lambda_k^{-1} \langle g_k, \Sigma g_k \rangle_\mathcal{H} = \lambda_k^{-1}\mathbb{E} [ g_k(x)^2] =\lambda_k^{-1}\) , and thus, \(\| \lambda_k^{1/2} g_k \|_{\mathcal{H}}=1\), and we have the kernel expansion from an orthonormal basis of \(\mathcal{H}\): $$K(x,y) = \sum_{k=0}^\infty\lambda_k g_k(x) g_k(y),$$ which will lead to a new proof for Mehler formula.</p>



<h2>Orthonormal basis for the Gaussian kernel</h2>



<p class="justify-text">Hermite polynomials naturally lead to orthonormal basis of some reproducing kernel Hilbert spaces (RKHS). For simplicity, I will focus on one-dimensional problems, but this extends to higher dimension. </p>



<p class="justify-text"><strong>Translation-invariant kernels.</strong> We consider a function \(q: \mathbb{R} \to \mathbb{R}\) which is integrable, with <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a> (note the different normalization than before) which is defined for all \(\omega \in \mathbb{R}\) because of the integrability: $$\hat{q}(\omega) = \int_{\mathbb{R}} q(x) e^{-i \omega x} dx.$$ We consider the kernel $$K(x,y) = q(x-y).$$ It can be check that as soon as  \(\hat{q}(\omega) \in \mathbb{R}_+\)  for all \(\omega \in \mathbb{R}\), then the kernel \(K\) is positive-definite.</p>



<p class="justify-text">For a translation-invariant kernel, we can write using the inverse Fourier transform formula: $$K(x,y) = q(x-y) = \frac{1}{2\pi} \int_{\mathbb{R}} \hat{q}(\omega) e^{i \omega ( x- y)} d \omega = \int_{\mathbb{R}} \varphi_\omega(x)^* \varphi_\omega(y) d \omega,$$ with \(\varphi_\omega(x) = \sqrt{\hat{q}(\omega) / (2\pi) } e^{i\omega x}\). Intuitively, for a function \(f: \mathbb{R} \to \mathbb{R}\), with \(\displaystyle f(x) = \frac{1}{2\pi} \int_{\mathbb{R}} \hat{f}(\omega)e^{i\omega x} d\omega = \int_{\mathbb{R}} \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}}\varphi_\omega(x) d\omega\), which is a “dot-product” between the family \((\varphi_\omega(x))_\omega\) and \(\Big( \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}} \Big)_\omega\), the squared norm \(\| f\|_{\mathcal{H}}^2\) is equal to the corresponding “squared norm” of \(\Big( \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}}\Big)_\omega\), and we thus have $$ \| f\|_{\mathcal{H}}^2 = \int_{\mathbb{R}} \Big| \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}} \Big|^2 d\omega =  \frac{1}{2\pi} \int_{\mathbb{R}} \frac{ | \hat{f}(\omega) |^2}{\hat{q}(\omega)} d\omega,$$ where \(\hat{f}\) is the Fourier transform of \(f\). While the derivation above is not rigorous, the last expression is.</p>



<p class="justify-text">In this section, I will focus on the Gaussian kernel defined as \(K(x,y) = q(x-y) =  \exp \big( – \alpha ( x- y )^2 \big)\), for which \(\displaystyle \hat{q}(\omega)= \sqrt{\frac{\pi}{\alpha}} \exp\big( – \frac{\omega^2}{4 \alpha} \big)\).</p>



<p class="justify-text">Given that \(\displaystyle \frac{1}{\hat{q}(\omega)} = \sqrt{\frac{\alpha}{\pi}} \exp\big(  \frac{\omega^2}{4 \alpha} \big)= \sqrt{\frac{\alpha}{\pi}} \sum_{k=0}^\infty  \frac{\omega^{2k}}{(4 \alpha)^k k!} \), the penalty \(\|f\|_\mathcal{H}^2\) is a linear combination of squared \(L_2\)-norm of \(\omega^k \hat{f}(\omega)\), which is the squared \(L_2\)-norm of the \(k\)-th derivative of \(f\). Thus, functions in the RKHS are infinitely differentiable, and thus very smooth (this implies that to have the fast rate \((\log n) / n \) above, the optimal regression function has to be very smooth).</p>



<p class="justify-text"><strong>Orthonormal basis of the RKHS</strong>. As seen in Eq. (4), an expansion in an infinite sum is necessary to obtain an orthonormal basis. We have: $$K(x,y) = e^{-\alpha x^2} e^{-\alpha y^2} e^{2 \alpha x y} = e^{-\alpha x^2} e^{-\alpha y^2} \sum_{k=0}^\infty \frac{ (2\alpha)^k}{k!} x^k y^k.$$ Because of Eq. (4), with \(g_k(x) = \sqrt{ \frac{(2\alpha)^k}{k!}} x^k \exp \big( – \alpha x^2 \big)\), we have a good candidate for an orthonornal basis. Let us check that this is the case. Note that the expansion above alone cannot be used as a proof that \((g_k)\) is an orthonormal basis of \(\mathcal{H}\).</p>



<p class="justify-text">Given the function \(f_k(x) = x^k \exp \big( – \alpha x^2 \big)\), we can compute its Fourier transform as $$ \hat{f}_k(\omega) = i^{-k} ( 4 \alpha)^{-k/2} \sqrt{\frac{\pi}{\alpha}} H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big) .$$ Indeed, we have, from Rodrigues’ formula, $$H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big) =(-1)^k (4 \alpha)^{k/2} \frac{d^k}{d \omega^k}\big[ \exp\big( – \frac{\omega^2}{4 \alpha} \big) \big],$$ and thus its inverse Fourier transform is equal to \((ix)^k\) times the one of \((-1)^k (4 \alpha)^{k/2} \exp\big( – \frac{\omega^2}{4 \alpha} \big)\), which is thus equal to \((-i)^k (4 \alpha)^{k/2} \sqrt{ \alpha / \pi }e^{-\alpha x^2} \), which leads to the Fourier transform formula above.</p>



<p class="justify-text">We can now compute the RKHS dot products, to show how to obtain the orthonormal basis described in [11]. This leads to $$ \langle f_k, f_\ell \rangle = \frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k/2}( 4 \alpha)^{-\ell/2}  \int_{\mathbb{R}} H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) H_\ell \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big)  d\omega,$$ which leads to, with a change of variable $$ \langle f_k, f_\ell \rangle = \frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k/2}( 4 \alpha)^{-\ell/2} \sqrt{4 \alpha} \int_{\mathbb{R}} H_k (u) H_\ell (u)  \exp(-u^2) du,$$ which is equal to zero if \(k \neq \ell\), and equal to \(\frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k} \sqrt{4 \alpha} \sqrt{\pi} 2^k k!   = ( 2 \alpha)^{-k} k!\) if \(k = \ell\). Thus the sequence \((f_k)\) is an orthogonal basis of the RKHS, and the sequence \((g_k)\) defined as \(g_k(x) = \sqrt{ \frac{(2\alpha)^k}{k!}} f_k(x)\) is an orthonormal basis of the RKHS, from which, using the expansion as in Eq. (4), we recover the expansion: $$K(x,y) = \sum_{k=0}^\infty g_k(x) g_k(y) = e^{-\alpha x^2} e^{-\alpha y^2} \sum_{k=0}^\infty \frac{ (2\alpha)^k}{k!} x^k y^k.$$</p>



<p class="justify-text">This expansion can be used to approximate the Gaussian kernel by finite-dimensional explicit feature spaces, by just keeping the first basis elements (see an application to optimal transport in [<a href="https://arxiv.org/pdf/1810.10046">12</a>], with an improved behavior using an adaptive low-rank approximation through the Nyström method in [<a href="https://papers.nips.cc/paper/8693-massively-scalable-sinkhorn-distances-via-the-nystrom-method.pdf">13</a>]).</p>



<h2>Eigenfunctions for the Gaussian kernels</h2>



<p class="justify-text">In order to obtain explicit formulas for the eigenvalues of the covariance operator, we need more than a mere orthonormal basis, namely an eigenbasis.</p>



<p class="justify-text">An orthogonal basis will now be constructed with arguably better properties as it is also an orthonormal basis for both the RKHS and \(L_2(d\mu)\) for a Gaussian measure, that diagonalizes the integral operator associated to this probability measure, as well as the covariance operator.</p>



<p class="justify-text">As seen above, we simply need an orthogonal family \((f_k)_{k \geqslant 0}\), such that given a distribution \(d\mu\),  \((f_k)_{k \geqslant 0}\) is a family in \(L_2(d\mu)\) such that $$\int_{\mathbb{R}} f_k(x) K(x,y) d\mu(x) = \lambda_k f_k(y), \tag{5}$$ for eigenvalues \((\lambda_k)\). In the next paragraph, we will do exactly this for the Gaussian kernel \(K(x,y) = e^{-\alpha (x-y)^2}\) for \(\alpha = \frac{\rho}{1- \rho^2}\) for some \(\rho \in (0,1)\); this particular parameterization in \(\rho\) is to make the formulas below not (too) complicated.</p>



<p class="justify-text">With \(f_k(x) = \frac{1}{\sqrt{N_k}} H_k(x) \exp \Big( – \frac{\rho}{1+\rho} x^2 \Big)\), where \(N_k = {2^k k!} \sqrt{ \frac{1-\rho}{1+\rho}}\), then \((f_k)_{k \geqslant 0}\) is an <em>orthonormal</em> basis for \(L_2(d\mu)\) for \(d\mu\) the Gaussian distribution with mean zero and variance \(\frac{1}{2} \frac{1+\rho}{1-\rho}\) (this is a direct consequence of the orthogonality property of Hermite polynomials).</p>



<p class="justify-text">Moreover, the moment of the Hermite polynomial in Eq. (3) exactly leads to Eq. (5) for the chosen kernel and \(\lambda_k = (1-\rho) \rho^k\). Since the eigenvalues sum to one, and the trace of \(\Sigma\) is equal to one (as a consequence of \(K(x,x)=1\)), the family \((f_k)\) has to be a basis of \(\mathcal{H}\).</p>



<p>From properties of the eigenbasis, since \((f_k)\) is an orthonormal eigenbasis of \(L_2(d\mu)\) and the eigenvalues are \(\lambda_k = (1-\rho)\rho^k\),  we get: $$ K(x,y) = \exp \Big( – \frac{\rho}{1- \rho^2} (x-y)^2\Big) = \sum_{k=0}^\infty (1-\rho)\rho^k f_k(x) f_k(y),$$ which is exactly the Mehler formula, and thus we obtain an alternative proof.</p>



<p class="justify-text">We then get an explicit basis and the exponential decay of eigenvalues, which was first outlined by [2]. See an application to the estimation of the Poincaré constant in [<a href="https://hal.archives-ouvertes.fr/hal-02327453v1/document">14</a>] (probably a topic for another post in a few months).</p>



<p class="justify-text"><strong>Experiments.</strong> In order to showcase the exact eigenvalues of the expectation \(\Sigma\) (for the correct combination of Gaussian kernel and Gaussian distribution), we compare the eigenvalues with the ones of the empirical covariance operator \(\hat\Sigma\), for various values of the number of observations. We see that as \(n\) increases, the empirical eigenvalues match the exact ones for higher \(k\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4889" height="307" src="https://francisbach.com/wp-content/uploads/2020/10/gaussian_kernel-1.gif" width="363"/>Eigenvalues of the covariance operator \(\Sigma\) (“expectation”) compared to the ones of the empirical covariance operator \(\hat\Sigma\), averaged over 20 replications (“empirical”), for several values of \(n\).</figure></div>



<h2>Conclusion</h2>



<p class="justify-text">In this post, I only presented applications of Hermite polynomials to the Gaussian kernel, but these polynomials appear in many other areas of applied mathematics, for other types of kernels within machine learning such as dot-product kernels [3], in random matrix theory (see <a href="https://terrytao.wordpress.com/2011/02/20/topics-in-random-matrix-theory/">here</a>), in statistics for <a href="https://en.wikipedia.org/wiki/Edgeworth_series">Edgeworth expansions</a>, and of course for <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Hermite_quadrature">Gauss-Hermite quadrature</a>.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Loucas Pillaud-Vivien and Alessandro Rudi for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] George Neville Watson. <a href="https://academic.oup.com/jlms/article-pdf/s1-8/3/194/2363185/s1-8-3-194.pdf">Notes on Generating Functions of Polynomials: (2) Hermite Polynomials</a>. <em>Journal of the London Mathematical Soc</em>iety, 8, 194-199, 1933.<br/>[2] Huaiyu Zhu, Christopher K. I. Williams, Richard Rohwer, and Michal Morciniec. <a href="https://publications.aston.ac.uk/id/eprint/38366/1/NCRG_97_011.pdf">Gaussian regression and optimal finite dimensional linear models</a>. In <em>Neural Networks and Machine Learning</em>. Springer-Verlag, 1998.<br/>[3] A. Daniely, R. Frostig, and Y. Singer. <a href="https://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity.pdf">Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity</a>. In Advances In Neural Information Processing Systems, 2016.<br/>[4] Gabor Szegö. <em>Orthogonal polynomials</em>. American Mathematical Society, 1939.<br/>[5] Grace Wahba. <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970128">Spline models for observational data</a>. Society for Industrial and Applied Mathematics, 1990.<br/>[6] Bernhard Schölkopf, Alexander J. Smola. <a href="https://mitpress.mit.edu/books/learning-kernels">Learning with kernels: support vector machines, regularization, optimization, and beyond</a>. MIT Press, 2002.<br/>[7] John Shawe-Taylor, Nello Cristianini. <em>Kernel methods for pattern analysis</em>. Cambridge University Press, 2004.<br/>[8] Andrea Caponnetto, Ernesto De Vito. <a href="https://link.springer.com/content/pdf/10.1007/s10208-006-0196-8.pdf">Optimal rates for the regularized least-squares algorithm</a>. Foundations of Computational Mathematics 7.3: 331-368, 2007.<br/>[9] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. <a href="http://web.stanford.edu/~hastie/Papers/ESLII.pdf">The elements of statistical learning</a>. Vol. 1. No. 10. Springer series in statistics, 2001.<br/>[10] Trevor Hastie and Robert Tibshirani. <em>Generalized Additive Models</em>. Chapman &amp; Hall, 1990.<br/>[11] Ingo Steinwart, Don Hush, and Clint Scovel. <a href="http://[PDF] ieee.org">An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels</a>. <em>IEEE Transactions on Information Theory</em>, 52.10:4635-4643, 2006.<br/>[12] Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Niles-Weed. <a href="https://arxiv.org/pdf/1810.10046">Approximating the quadratic transportation metric in near-linear time</a>. Technical report arXiv:1810.10046, 2018.<br/>[13] Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Niles-Weed. <a href="https://papers.nips.cc/paper/8693-massively-scalable-sinkhorn-distances-via-the-nystrom-method.pdf">Massively scalable Sinkhorn distances via the Nyström method</a>. <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019.<br/>[14] Loucas Pillaud-Vivien, Francis Bach, Tony Lelièvre, Alessandro Rudi, Gabriel Stoltz. <a href="https://hal.archives-ouvertes.fr/hal-02327453v1/document">Statistical Estimation of the Poincaré constant and Application to Sampling Multimodal Distributions</a>. <em>Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),</em> 2020.<br/>[15] Nachman Aronszajn. <a href="https://www.ams.org/journals/tran/1950-068-03/S0002-9947-1950-0051437-7/S0002-9947-1950-0051437-7.pdf">Theory of Reproducing Kernels</a>. <em>Transactions of the American Mathematical Society</em>, 68(3): 337–404, 1950.</p>



<h2>Proof of properties of Hermite polynomials</h2>



<p class="justify-text">In this small appendix, I give “simple” proofs (that sometimes require knowledge of <a href="https://en.wikipedia.org/wiki/Complex_analysis">complex analysis</a>) to the properties presented above.</p>



<p class="justify-text"><strong>Generating function.</strong> We have, using <a href="https://en.wikipedia.org/wiki/Residue_theorem">residue theory</a>, $$H_k(x)=(-1)^k e^{x^2} \frac{d^k}{d x^k}\big[ e^{-x^2} \big] = (-1)^k \frac{k!}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{(z-x)^{k+1}}dz, $$ where \(\gamma\) is a contour in the complex plane around \(x\). This leads to, for any \(t\) (here, we ignore on purpose the summability issues, for more details, see [4, Section 5.5]): $$ \sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) =  \frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{z-x} \sum_{k=0}^\infty \frac{t^k} {(x-z)^{k}}dz, $$ which can be simplified using the sum of the geometric series, leading to $$\frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{z-x} \frac{z-x}{z-x- t} dz =  \frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}} {z-x- t} dz.$$ Using the first-order residue at \(x+t\). This is thus equal to \(e^{x^2-(t+x)^2} = e^{-t^2 + 2tx}\), which is exactly the generating function statement from Eq. (2).</p>



<p class="justify-text"><strong>Orthogonality for Gaussian distribution.</strong> We can prove through integration by parts, but there is a nicer proof through the generating function. Indeed, with $$ a_{k \ell} = \int_{-\infty}^{+\infty} e^{-x^2} H_k(x) H_\ell(x) dx, $$ for \(k, \ell \geqslant 0\), we get $$\sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!} = \int_{-\infty}^{+\infty}e^{-x^2}\Big(  \sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!}  H_k(x) H_\ell(x) \Big) dx.$$ Using the generating function, this leads to $$\sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!} =  \int_{-\infty}^{+\infty} e^{-x^2 + 2xu-u^2 + 2xt – t^2} dx= e^{2uv} \int_{-\infty}^{+\infty} e^{-(x-u-v)^2}dx, $$ which can be computed explicitly using normalization constants of the Gaussian distribution, as \( \sqrt{\pi} e^{2uv} = \sqrt{\pi} \sum_{k=0}^\infty \frac{ (2  u v)^k}{k!},\) leading to all desired orthogonality relationships using the uniqueness of all coefficients for factors \(t^k u^\ell\).</p>



<p class="justify-text"><strong>Recurrence relationship.</strong> Taking the derivative of the generating function with respect to \(t\), one gets \( \displaystyle (2x-2t) e^{2tx-t^2} = \sum_{k=0}^\infty \frac{t^{k-1}}{(k-1)!} H_k(x),\) which is equal to (using again the generating function) \(\displaystyle \sum_{k=0}^\infty \frac{t^{k}}{k!} 2x H_k(x) \ – \sum_{n=0}^\infty \frac{t^{k+1}}{k!} 2 H_k(x).\) By equating the coefficients for all powers of \(t\), this leads to the desired recursion in Eq. (1).</p>



<p class="justify-text"><strong>Fourier transform.</strong> Again using the generating function, written $$ e^{-x^2/2 + 2xt – t^2} = \sum_{k=0}^\infty \frac{t^k}{k!} e^{-x^2/2} H_k(x), $$ we can take Fourier transforms and use the fact that the Fourier transform of \(e^{-x^2/2}\) is itself (for the chosen normalization), and then equate coefficients for all powers of \(t\) to conclude (see more details <a href="https://en.wikipedia.org/wiki/Hermite_polynomials#Hermite_functions_as_eigenfunctions_of_the_Fourier_transform">here</a>).</p>



<p class="justify-text"><strong>Expectation for Gaussian distributions.</strong> We finish the appendix by proving Eq. (3). We consider computing for any \(t\), $$\sum_{k=0}^\infty \rho^k \frac{t^k}{k!} H_k (y) = e^{2\rho t y – \rho^2 t^2},$$ using the generating function from Eq. (2). We then compute $$A=\int_{-\infty}^\infty \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big) \sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) dx = \int_{-\infty}^\infty \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big) \exp( 2tx – t^2) dx.$$ We then use \( \frac{(x-\rho y)^2}{1-\rho^2} – 2tx + t^2 = \frac{x^2}{1-\rho^2}  – \frac{2x[ t(1-\rho^2) + \rho y]}{1-\rho^2}  + t^2 + \frac{\rho^2 y^2}{1-\rho^2}\), leading to $$A = \sqrt{\pi} \sqrt{1-\rho^2} \exp\Big( -t^2 – \frac{\rho^2 y^2}{1-\rho^2} +(1-\rho^2) \big( t + \frac{\rho y}{1-\rho^2} \big)^2 \Big) = \sqrt{\pi} \sqrt{1-\rho^2} e^{2\rho t y – \rho^2 t^2}.$$ By equating powers of \(t\), this leads to Eq. (3).</p></div>
    </content>
    <updated>2020-10-08T19:33:36Z</updated>
    <published>2020-10-08T19:33:36Z</published>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-11-26T22:33:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=488</id>
    <link href="https://tcsplus.wordpress.com/2020/10/07/tcs-talk-wednesday-october-14-jayadev-acharya-cornell-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 14 — Jayadev Acharya, Cornell University</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Jayadev Acharya from Cornell University will speak about “Distributed Statistical Inference under Local Information Constraints ” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Jayadev Acharya</strong> from Cornell University will speak about  “<em>Distributed Statistical Inference under Local Information Constraints</em> ” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<blockquote class="wp-block-quote"><p>Abstract: We consider statistical inference tasks in a distributed setting where access to data samples is subjected to strict “local constraints,” through a unified framework that captures communication limitations and (local) privacy constraints as special cases. We study estimation (learning) and goodness-of-fit (testing) for both discrete and high-dimensional distributions. Our goal is to understand how the sample complexity increases under the information constraints.<br/><br/>In this talk we will provide an overview of this field and a sample of some of our results. We will discuss the role of (public) randomness  and interactivity in information-constrained inference, and make a case for thinking about randomness and interactivity as resources.<br/><br/>The work is part of a long-term ongoing collaboration with Clément Canonne (IBM Research) and Himanshu Tyagi (IISc), and includes works done with Cody Freitag (Cornell), Yanjun Han (Stanford), Yuhan Liu (Cornell), and Ziteng Sun (Cornell). </p></blockquote></div>
    </content>
    <updated>2020-10-07T20:11:36Z</updated>
    <published>2020-10-07T20:11:36Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-26T22:27:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=63</id>
    <link href="https://dstheory.wordpress.com/2020/10/07/friday-oct-09-alexandr-andoni-from-columbia-university/" rel="alternate" type="text/html"/>
    <title>Friday, Oct 09 — Alexandr Andoni from Columbia University</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Friday, Oct 09th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  Alexandr Andoni from Columbia University will speak about “Approximating Edit Distance in Near-Linear Time”. Abstract: Edit distance is a classic measure of similarity between strings, with<a class="more-link" href="https://dstheory.wordpress.com/2020/10/07/friday-oct-09-alexandr-andoni-from-columbia-university/">Continue reading <span class="screen-reader-text">"Friday, Oct 09 — Alexandr Andoni from Columbia University"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next Foundations of Data Science virtual talk will take place on Friday, Oct 09th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Alexandr Andoni </strong>from Columbia University will speak about “<em><strong>Approximating Edit Distance in Near-Linear Time</strong></em>”.</p>



<p><strong>Abstract</strong>: Edit distance is a classic measure of similarity between strings, with applications ranging from computational biology to coding. Computing edit distance is also a classic dynamic programming problem, with a quadratic run-time solution, often taught in the “Intro to Algorithms” classes. Improving this runtime has been a decades-old challenge, now ruled likely-impossible using tools from the modern area of fine-grained complexity. We show how to approximate the edit distance between two strings in near-linear time, up to a constant factor. Our result completes a research direction set forth in the breakthrough paper of [Chakraborty, Das, Goldenberg, Koucky, Saks; FOCS’18], which showed the first constant-factor approximation algorithm with a (strongly) sub-quadratic running time.</p>



<p>Joint work with Negev Shekel Nosatzki, available at<a href="https://arxiv.org/abs/2005.07678"> https://arxiv.org/abs/2005.07678</a>.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-10-07T15:44:56Z</updated>
    <published>2020-10-07T15:44:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-11-26T22:33:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://minimizingregret.wordpress.com/?p=389</id>
    <link href="https://minimizingregret.wordpress.com/2020/10/06/blackwell-approachability-meets-online-convex-optimization/" rel="alternate" type="text/html"/>
    <title>Blackwell approachability meets Online Convex Optimization</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">David Blackwell was still roaming the corridors of UC Berkeley’s stat department during my postdoc years, circa 2009. Jake Abernethy, Sasha Rakhlin, Peter Bartlett and myself were discussing his results, and his seminal contributions to prediction theory were already well known. At that time, still the early days of online convex optimization, we were contemplating … <a class="more-link" href="https://minimizingregret.wordpress.com/2020/10/06/blackwell-approachability-meets-online-convex-optimization/">Continue reading <span class="screen-reader-text">Blackwell approachability meets Online Convex Optimization</span> <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>David Blackwell was still roaming the corridors of UC Berkeley’s stat department during my postdoc years, circa 2009. Jake Abernethy, Sasha Rakhlin, Peter Bartlett and myself were discussing his results, and his seminal contributions to prediction theory were already well known.</p>



<figure class="wp-block-image size-large is-resized"><img alt="" class="wp-image-395" height="183" src="https://minimizingregret.files.wordpress.com/2020/10/download.jpeg?w=275" width="275"/>David Blackwell</figure>



<figure class="wp-block-image size-large is-resized"><img alt="" class="wp-image-397" height="200" src="https://minimizingregret.files.wordpress.com/2020/10/16-figure11-1-1.png?w=662" width="204"/>James Hannan</figure>



<p>At that time, still the early days of online convex optimization, we were contemplating everything from adaptive gradient methods to bandit convex optimization. Blackwell’s famed approachability theorem was looming in the background, considered to be one of the strongest theorems in the ML-theorists toolkit. </p>



<p>I’ve recently added a new draft chapter to to V2.0 of <a href="http://ocobook.cs.princeton.edu">Introduction to Online Convex Optimization</a>, about the connection between OCO and Blackwell approachability: they are algorithmically equivalent! More precisely, an efficient algorithm for approachability gives rise to an efficient algorithm for OCO with sublinear regret, and vice versa. </p>



<p>Details are in the chapter draft, and  I recommend this music for reading it: (the song is called “together we won despite everything”, dedicated to music and science)</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"/>
</div></figure>



<p>.</p>



<p/></div>
    </content>
    <updated>2020-10-06T00:11:46Z</updated>
    <published>2020-10-06T00:11:46Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Elad Hazan</name>
    </author>
    <source>
      <id>https://minimizingregret.wordpress.com</id>
      <logo>https://minimizingregret.files.wordpress.com/2017/08/cropped-pu1.png?w=32</logo>
      <link href="https://minimizingregret.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://minimizingregret.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://minimizingregret.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://minimizingregret.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Google Princeton AI and Hazan Lab @ Princeton University</subtitle>
      <title>Minimizing Regret</title>
      <updated>2020-11-26T22:30:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1794</id>
    <link href="https://theorydish.blog/2020/10/05/forc-2021-is-on-its-way/" rel="alternate" type="text/html"/>
    <title>FORC 2021 Is on Its Way</title>
    <summary>After a powerful launch at 2021, the second meeting of The Symposium on Foundations of Responsible Computing (FORC) is on its way. The call for papers for FORC 2021 is out, with a wonderful PC, headed by Katrina Ligett. Please consider sending your strong submissions down our way.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>After <a href="https://theorydish.blog/2020/03/30/forc-2020-going-strong-going-virtual/">a powerful launch</a> at 2021, the second meeting of  <a href="https://responsiblecomputing.org/">The Symposium on Foundations of Responsible Computing (FORC)</a> is on its way. The <a href="https://responsiblecomputing.org/forc-2021-call-for-papers/">call for papers</a> for FORC 2021 is out, with a wonderful PC, headed by <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a>. Please consider sending your strong submissions down our way.</p></div>
    </content>
    <updated>2020-10-05T15:00:47Z</updated>
    <published>2020-10-05T15:00:47Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-11-26T22:28:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=479</id>
    <link href="https://tcsplus.wordpress.com/2020/09/30/479/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 7 — Susanna F. de Rezende, Mathematical Institute of the Czech Academy of Sciences</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 7th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Susanna F. de Rezende from Mathematical Institute of the Czech Academy of Sciences will speak about “Lifting with Simple Gadgets and Applications to Circuit and Proof Complexity” (abstract […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 7th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Susanna F. de Rezende</strong> from Mathematical Institute of the Czech Academy of Sciences will speak about “<em>Lifting with Simple Gadgets and Applications to Circuit and Proof Complexity</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: Lifting theorems in complexity theory are a method of transferring lower bounds in a weak computational model into lower bounds for a more powerful computational model, via function composition. There has been an explosion of lifting theorems in the last ten years, essentially reducing communication lower bounds to query complexity lower bounds. These theorems only hold for composition with very specific “gadgets” such as indexing and inner product. <br/><br/> In this talk, we will present a generalization of the theorem lifting Nullstellensatz degree to monotone span program size by Pitassi and Robere (2018) so that it works for any gadget with high enough rank, in particular, for useful gadgets such as equality and greater-than. We will then explain how to apply our generalized theorem to solve three open problems: <br/>– We present the first result that demonstrates a separation in proof power for cutting planes with unbounded versus polynomially bounded coefficients. Specifically, we exhibit CNF formulas that can be refuted in quadratic length and constant line space in cutting planes with unbounded coefficients, but for which there are no refutations in subexponential length and subpolynomial line space if coefficients are restricted to be of polynomial magnitude.<br/>– We give the strongest separation to-date between monotone Boolean formulas and monotone Boolean circuits. Namely, we show that the classical GEN problem, which has polynomial-size monotone Boolean circuits, requires monotone Boolean formulas of size <img alt="2^{\Omega(n / \textrm{polylog} n)}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B%5COmega%28n+%2F+%5Ctextrm%7Bpolylog%7D+n%29%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="2^{\Omega(n / \textrm{polylog} n)}"/>.<br/>– We give the first explicit separation between monotone Boolean formulas and monotone real formulas. Namely, we give an explicit family of functions that can be computed with monotone real formulas of nearly linear size but require monotone Boolean formulas of exponential size. Previously only a non-explicit separation was known.<br/><br/>This talk is based on joint work with Or Meir, Jakob Nordström, Toniann Pitassi, Robert Robere, and Marc Vinyals, available at <a href="https://arxiv.org/abs/2001.02144" rel="nofollow">https://arxiv.org/abs/2001.02144</a> </p></div>
    </content>
    <updated>2020-09-30T23:43:35Z</updated>
    <published>2020-09-30T23:43:35Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-26T22:27:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://mycqstate.wordpress.com/?p=1252</id>
    <link href="https://mycqstate.wordpress.com/2020/09/29/it-happens-to-everyonebut-its-not-fun/" rel="alternate" type="text/html"/>
    <title>It happens to everyone…but it’s not fun</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A recent post on this blog concerned the posting of our paper MIP*=RE on the arXiv and gave a personal history of the the sequence of works that led to the result. Quite unfortunately (dramatically?) a few weeks after initial … <a href="https://mycqstate.wordpress.com/2020/09/29/it-happens-to-everyonebut-its-not-fun/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h1/>



<p>A <a href="https://mycqstate.wordpress.com/2020/01/14/a-masters-project/">recent post</a> on this blog concerned the posting of our paper <a href="https://arxiv.org/abs/2001.04383">MIP*=RE</a> on the arXiv and gave a personal history of the the sequence of works that led to the result. Quite unfortunately (dramatically?) a few weeks after initial posting of the paper (and the blog post) John Wright discovered an important error in the proof of a key result in this sequence: my paper <a href="https://doi.org/10.1137/140956622">Three-player entangled XOR games are NP-hard to approximate</a>, published in 2016 in a special issue of the SIAM journal on computing dedicated to selected papers from the FOCS’13 conference. While I did not mention this paper directly in the previous blog post, its main result, a proof of soundness of the Raz-Safra low-degree test against entangled-player strategies, is a key ingredient in the proof of the <a href="https://arxiv.org/abs/1801.03821">quantum low-degree test</a>, itself a key ingredient in the MIP*=RE paper. (Strictly speaking the latter paper relies on an extension of my result to two-player games obtained in a <a href="https://arxiv.org/abs/1710.03062">follow-up</a> with Natarajan. Since that paper re-used the flawed part of my earlier analysis in a black-box manner it is similarly impacted.) So then…?</p>



<h2 id="scientific-aspects">Scientific aspects</h2>



<p>I’ll start with the science. The result MIP*=RE, to the best of our knowledge, remains correct. In order to remove the dependence of the proof on the flawed paper we extended the soundness analysis of Babai, Fortnow and Lund (BFL)’s multilinearity test against entangled provers from my <a href="https://arxiv.org/abs/1207.0550">paper with Ito</a> to the case of multivariate polynomials of low individual degree. (This extension, for the case of classical provers, is already mentioned in the BFL paper.) We just posted a self-contained analysis of that test on the arXiv <a href="https://arxiv.org/abs/2009.12982">here</a> and updated the MIP*=RE paper to account for the replacement (see v2.). The latter paper is currently under review; on this I will simply say that, as for all mathematical works, it is advised to wait until the outcome of the refereeing process is complete before declaring confidence in the validity of the result. For a more in-depth description of the changes made I refer to the introduction of the <a href="https://arxiv.org/abs/2009.12982">new paper</a>.</p>



<p>Our analysis of the “low individual-degree test” mentioned in the preceding paragraph can be used to recover the main result of my SICOMP’16 paper in a weakened form. Since the proof is different and does not directly fix the error I have decided to withdraw the paper from SICOMP. For more details on the error itself and consequences to other works, such as the quantum low-degree test and the quantum games PCP, I refer to the <a href="http://users.cms.caltech.edu/~vidick/errata.pdf">short note</a> I wrote to accompany the withdrawal of the paper. The one-sentence summary is that essentially all subsequent results expressed in terms of “high” complexity classes such as QMA-EXP, NEEXP, etc., still hold, while “scaled-down” results on the hardness of entangled games can only be recovered by allowing a substantial weakening of parameters. In particular, the <a href="https://arxiv.org/abs/1801.03821">quantum low-degree test</a> holds in its scaled-up version (testing exp(n) EPR pairs using poly(n) resources), but the scaled-down version requires polylog(n) communication to test n EPR pairs, instead of O(log n) as claimed.</p>



<h2 id="personal-aspects">Personal aspects</h2>



<p>In addition to notifying researchers in the area of the bug, my goal in writing this blog post is to help me exorcise the demon of having a large mistake in one of my papers. In doing so I was inspired by Scott Aaronson’s <a href="https://www.scottaaronson.com/blog/?p=2854">blog post</a> on a similar topic. (I’ll admit that even just linking explicitly to his post helps reassure myself, a power which I believe was one of Scott’s aims in writing the post. So, thanks Scott, and allow me to pass it on!) The faulty paper is not based on a minor back-of-the-envelope observation; in fact it is one that I was quite proud of. The mistake in it is not small either; it’s a mistake that I cannot find any excuse for having made. Yet here I am: after having spent the past 6 months trying to find an alternative proof, I now strongly believe that the problem cannot be solved using the kind of techniques that I had imagined could do so. Whether the theorem statement is true or not, I don’t know; but at the moment I am unable to prove it. I have to accept that there is a bug.</p>



<p>As painful as it is I realize that I am writing this post from a relatively comfortable position. Who knows if I would have been able to do the same had we not been able to recover a full proof of MIP*=RE. Moreover, after having banged my head against the problem for 6 months straight (COVID helping, walls were never far) I am now able to see my failure in a more positive light: the story I told in the previous blog post is not yet closed; there is an open challenge for me to solve. It is a very personal challenge; having spent the past 6 months delineating it I have accumulated sufficient grounds on which to believe that it is an interesting one. I feel grateful for this.</p>



<p>Getting there wasn’t easy. So, even though I am writing from a place of comfort, I want to share the pain that the whole adventure has caused me. This simple acknowledgment is especially directed at younger readers: so that when it happens to you, you will remember this post and know that you’re not the only one. That it happens to others as well and that it is possible to face, accept, and move away from such errors. Of course you will try to fix it first. Here are some quick tips. While banging your head on the problem, make sure that your understanding increases every day. To start with, do you really understand why there is a mistake? Of course some step doesn’t go through, but what is the simplest form of the incriminated statement that fails? Can you write it down? Can you formulate and prove a weaker form of it? Probe the issue with examples. Try to isolate it as much as you can: take it outside of the paper and formulate an entirely self-contained version of it, stripped of all the baggage. Place it in as many different contexts that you can think of: do you still believe it, does it stand on its own? Again, make sure that you learn. Even if you’re not able to fix the claim, are you exploring a new technique, discovering a new perspective? If it didn’t work yesterday it probably won’t work today either: make sure that you always find something new to inject. When you can no longer do this, it is time to stop. So make sure to set yourself some near-term (how much to think about this on any given day) and long-term (when to admit defeat) limits. Always remember that problems are much more often solved in the shower or while walking the dog than at the desk. Finally, be ready to move on. Realize that as bad as it may seem to you, there are more important things in life. You can’t reduce yourself to this one problem: you’ll be stronger for accepting what happened than trying to bury it at all costs. If you don’t see this by yourself, try to talk about it. Explain the situation you’re in to your close non-academia friends, to your parents; practice on your pet first if it helps. You will realize, as I eventually did (although it took quite a while) that <em>it is ok</em>.</p>



<h2 id="social-aspects">Social aspects</h2>



<p>After the scientific and the personal aspects, let me end with the sociological. This is a semi-tangent but it is a good opportunity to discuss a topic that we scientists, possibly even more so us working in the “hard sciences” (as the French call “proof-based” disciplines), are insufficiently sensitized to. This is the topic of how science is made, and what is the reality of this “absolute truth” that we claim to discover and establish in our mathematical results.</p>



<p>My paper was posted on arXiv in 2013, it was accepted to the FOCS conference and published in its proceedings the same year, and it appeared in the journal SICOMP in 2016. Both publications were refereed. Since its posting the paper has been cited 47 times (google scholar) and its main result is used in an essential way in at least half a dozen papers (my best guess). 7 years later a big hole has been found in the proof. How did the “truth value of my result evolve in the process? Was it always wrong or was there a time where it had truth, in whatever appropriate sense of the word?</p>



<p>I realize that these questions can be given trivial answers—I know what is an axiom and what is a proof system. Yet I am trying to push myself, and my reader, to look a little deeper. An analogy might help. The situation brought to mind a book by French philosopher of science Bruno Latour, called (in its English translation) <a href="https://www.amazon.com/Laboratory-Life-Construction-Scientific-Facts/dp/069102832X">Laboratory Life: The Construction of Scientific Facts</a>. This is a wonderful book, which goes well beyond the classic misconceptions from Popper or even Kuhn; it should be mandatory reading for every scientist. In one of the early chapters of the book Latour makes a detailed study of how subsequent citations can collectively enshrine an initial claim based entirely on the citer’s conscious or unconscious biases in making use of the citation (i.e. in complete independence from any ground “truth” or “importance” of the cited work). An entertaining example of this can be found in <a href="https://journals.sagepub.com/doi/full/10.1177/0306312714535679">this article</a>, which dissects the claim that “The myth from the 1930s that spinach is a rich source of iron was due to misleading information in the original publication: a malpositioned decimal point gave a 10-fold overestimate of iron content.” The example, pursued in great depth in the article, shows very well how one citation at a time the (spoiler: unjustified) claim is given more and more credibility until it eventually becomes a fact: from initial citations written in a tentative tone “according to Z, it could be that…” to more assertive citations “Z has shown that” by more and more well-known researchers in highly-read journals to pure fact (citation above). I highly recommend the article!</p>



<p>It is easy to dismiss this story as being the result of “sloppy” authors misrepresenting a “soft” claim whose truth value is not well-determined in the first place, being a statement about the world rather than about some hypothetical mathematical universe. Yet I believe that it is worth taking the time to examine with an open mind what exactly, if anything, distinguishes a claim about the iron content of spinach from the main “theorem” of my paper. From its initial posting on the arXiv to its presentation in a conference and its journal publication to the multiple citations it received through its use in subsequent works, and including multiple other considerations such as my own credibility (itself the result of so many other considerations) and the results base “believability”, when was the logical statement itself evaluated? Does it matter? Did the unchallenged existence of the result for 7 years impact the course of science? Or was it a mistake that was bound to be discovered and has no lasting consequences?</p>



<p>These are questions for the reader, that can be (and are probably better) asked in other contexts than the limited one of my result. Indeed there is a much broader point to all this, that I only meant to raise in an indirect manner. It is impossible to disregard the fact that our scientific work is grounded in cultural and societal effects, but we may disagree on the impact that this grounding has. We owe it to ourselves and to our readers (broadly interpreted—from colleagues to funding agencies to the broader public) to refuse to hide behind the thin veil of “hard science”, mathematics or logic, and educate ourselves to what it is that we really are doing.</p></div>
    </content>
    <updated>2020-09-29T15:50:43Z</updated>
    <published>2020-09-29T15:50:43Z</published>
    <category term="meta"/>
    <category term="Quantum"/>
    <category term="Science"/>
    <category term="Uncategorized"/>
    <author>
      <name>Thomas</name>
    </author>
    <source>
      <id>https://mycqstate.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://mycqstate.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://mycqstate.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://mycqstate.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://mycqstate.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>In superposition</subtitle>
      <title>MyCQstate</title>
      <updated>2020-11-26T22:30:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=473</id>
    <link href="https://tcsplus.wordpress.com/2020/09/27/tcs-talk-wednesday-september-30-alex-wein-nyu/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, September 30 — Alex Wein, NYU</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, September 30th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Alex Wein from NYU will speak about “Low-Degree Hardness of Random Optimization Problems” (abstract below). You can reserve a spot as an individual or a group to join […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, September 30th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Alex Wein</strong> from NYU will speak about “<em>Low-Degree Hardness of Random Optimization Problems</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: In high-dimensional statistical problems (including planted clique, sparse PCA, community detection, etc.), the class of “low-degree polynomial algorithms” captures many leading algorithmic paradigms such as spectral methods, approximate message passing, and local algorithms on sparse graphs. As such, lower bounds against low-degree algorithms constitute concrete evidence for average-case hardness of statistical problems. This method has been widely successful at explaining and predicting statistical-to-computational gaps in these settings. <br/>While prior work has understood the power of low-degree algorithms for problems with a “planted” signal, we consider here the setting of “random optimization problems” (with no planted signal), including the problem of finding a large independent set in a random graph, as well as the problem of optimizing the Hamiltonian of mean-field spin glass models. I will define low-degree algorithms in this setting, argue that they capture the best known algorithms, and explain new proof techniques for giving lower bounds against low-degree algorithms in this setting. The proof involves a variant of the so-called “overlap gap property”, which is a structural property of the solution space.<br/><br/>Based on joint work with David Gamarnik and Aukosh Jagannath, available at <a href="https://arxiv.org/abs/2004.12063">arXiv:2004.12063</a>.</p></div>
    </content>
    <updated>2020-09-27T17:15:50Z</updated>
    <published>2020-09-27T17:15:50Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-26T22:27:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://mycqstate.wordpress.com/?p=1244</id>
    <link href="https://mycqstate.wordpress.com/2020/09/20/announcing-a-short-course-in-paris/" rel="alternate" type="text/html"/>
    <title>Announcing a short course in Paris</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This coming academic year I am on sabbatical, in Paris. It’s certainly a funny year to be on sabbatical. (It’s a funny year to be doing anything, isn’t it? Or is “funny” not the appropriate word…Yet I can’t find any … <a href="https://mycqstate.wordpress.com/2020/09/20/announcing-a-short-course-in-paris/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This coming academic year I am on sabbatical, in Paris. It’s certainly a funny year to be on sabbatical. (It’s a funny year to be doing anything, isn’t it? Or is “funny” not the appropriate word…Yet I can’t find any other way to look at it that doesn’t send me straight into the abyss. So, let it be “funny”—knowing that, no, I’m not actually laughing right now.) On the one hand, I am lucky to have escaped the incessant debates on the format of teaching, how many people per square foot are allowed in each building on campus, what distance I should stay from my students were I to attempt to meet them in person, and so many other similar decisions that have come to take up a larger and larger fraction of our professional lives (not to mention of course the incommensurate challenges that many are facing at the personal and familial level). On the other hand, the situation makes it much harder to meet others and engage in new collaborations, one of the goals of my sabbatical. I’ll see how it plays out; I’ll be sure to write more on this blog as time progresses.</p>



<p>During the sabbatical I am being hosted successively by different French institutions. For the first 6 months I had the good fortune of being awarded a “chair” from the “<a href="https://www.sciencesmaths-paris.fr/en/">Fondation Sciences Mathématiques de Paris</a>” (FSMP), a private foundation which supports, in very general terms, the development of the mathematics community in Paris, from the organization of general-public conferences to the support of research collaborations. My only formal obligation during these 6 months is to give 20 hours of lecture on a theme of my choosing. The goal that I elected for the course is provide an in-depth introduction to two major works in quantum complexity and cryptography of the past few years: first, Mahavev’s 2018 result on <a href="https://arxiv.org/abs/1804.01082">classical verification of quantum computation</a> (a result for which I already shared my enthusiasm <a href="https://mycqstate.wordpress.com/2018/08/06/the-cryptographic-leash/">here</a>); second, my result <a href="https://arxiv.org/abs/2001.04383">MIP*=RE</a> with Ji, Natarajan, Wright and Yuen on the power of quantum multi-prover interactive proof systems, which I mentioned in the <a href="https://mycqstate.wordpress.com/2020/01/14/a-masters-project/">previous post</a>, and its consequences. For more about the course, including a tentative breakdown of lectures and some resources, see the <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/">course webpage</a>. </p>



<p>While at the time of writing the course is still scheduled to start as an in-person meeting (to take place in a very large layered amphitheater with ample space for social distancing), there is no telling how the situation, and regulations, will evolve in the near future. To accommodate participants who are unable or prefer not to travel in person, all lectures starting with the first one will be recorded. In addition I will post course materials, including lecture notes, <a href="http://users.cms.caltech.edu/~vidick/teaching/fsmp/">here</a>. The purpose of this post is to advertise the course: participants from everywhere are welcome to watch the recorded videos, read the notes, and write to me with any questions in suggestions. In particular I plan to outsource the proof-reading of the notes via overleaf and I welcome any participant’s interest in helping with that; draft notes for the first lecture are already available <a href="https://www.overleaf.com/2293291658twkjfbtctsdb">here</a>. Anyone is welcome to make direct corrections, or add inline comments pointing to issues that may need my attention.</p>



<p>The program that I chose is ambitious, and we will see how far we get along. My goal is to start slow, so as to remain inclusive with respect to varying backgrounds in computer science, mathematics or physics. At first I will give complete definitions, state and prove simple lemmas, etc., in order to establish common language. As time progresses I expect that things will become a little more high-level, less self-contained, and more technical. Depending on your background and interests, you may find the first few lectures, or the last few ones, more interesting. Teaching the course will certainly be beneficial for me because I believe that there is a strong unity behind the two works I chose to present. I hope to make that unity apparent by presenting them together. Moreover, both works introduce new techniques that leave many avenues open; I hope that a “clean” presentation will help me, and others, build on them. </p>



<p>A side benefit of an “un-necessary” course such as this one is that it contributes to bringing a certain community together. (By “un-necessary” I mean that the course will not be required for any curriculum; if it did not take place, as long as it was replaced by other research-level activities its absence would not be felt.) COIVD-19 unfortunately turns that opportunity into a challenge. It is because of it that I insist–regulations allowing– on having the course take place in person: as much as we are getting used to Zoom, and as well as it may be working as a replacement for many aspects of our interactive lives, from in-person classes to conferences to research collaborations, a scientific event such as this one, with sustained involvement by a small set of participants coming from distant backgrounds, is probably one of the more challenging ones to make work online. I hope it doesn’t come to that. Even if it does, one of the lessons learned from the Spring 2020 semester on quantum computing at the Simons Institute in Berkeley, which was interrupted half-ways due to the pandemic, is that having an initial in-person phase was of great help to cement future online interactions. So, I hope that I am able to lecture on Tuesday; after that, we will see.</p></div>
    </content>
    <updated>2020-09-20T15:19:18Z</updated>
    <published>2020-09-20T15:19:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Thomas</name>
    </author>
    <source>
      <id>https://mycqstate.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://mycqstate.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://mycqstate.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://mycqstate.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://mycqstate.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>In superposition</subtitle>
      <title>MyCQstate</title>
      <updated>2020-11-26T22:30:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5573</id>
    <link href="https://adamsheffer.wordpress.com/2020/09/19/combinatorial-journals-are-changing/" rel="alternate" type="text/html"/>
    <title>Combinatorial Journals are Changing</title>
    <summary>I used to ask most combinatorialists I met for their opinion about the level of various journals. With this feedback, I compiled a rough journal ranking for combinatorics papers (for personal use). This was a very educational experience for me as a new combinatorialist. I learned that different people have rather different opinions. For example, […]</summary>
    <updated>2020-09-19T21:46:10Z</updated>
    <published>2020-09-19T21:46:10Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-11-26T22:27:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=457</id>
    <link href="https://tcsplus.wordpress.com/2020/09/18/tcs-talk-wednesday-september-23-fotis-iliopoulos-princeton-and-ias/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, September 23 — Fotis Iliopoulos, Princeton and IAS</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, September 23th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Fotis Iliopoulos from Princeton and IAS will speak about “Stochastic Local Search and the Lovász Local Lemma” (abstract below). You can reserve a spot as an individual or […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, September 23th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Fotis Iliopoulos</strong> from Princeton and IAS will speak about “<em>Stochastic Local Search and the Lovász Local Lemma</em>” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<blockquote class="wp-block-quote"><p>Abstract: The Lovasz Local Lemma (LLL) is a powerful tool in probabilistic combinatorics which can be used to establish the existence of objects that satisfy certain properties. The breakthrough of Moser and Tardos (who recently received the Godel Prize for their work) and follow-up works revealed that the LLL has intimate connections with a class of stochastic local search algorithms for finding such desirable objects.<br/><br/>In this talk, I will survey this line of work through the perspective of recent unifying results, and also talk about recent applications to solving pseudo-random constraint satisfaction problems.</p></blockquote>



<p/></div>
    </content>
    <updated>2020-09-18T16:22:59Z</updated>
    <published>2020-09-18T16:22:59Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-26T22:27:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=448</id>
    <link href="https://tcsplus.wordpress.com/2020/09/09/tcs-talk-thursday-september-17-richard-peng-georgia-tech/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Thursday, September 17 — Richard Peng, Georgia Tech</title>
    <summary>The next TCS+ talk will take place this coming Thursday, September 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Richard Peng from Georgia Tech will speak about “Solving Sparse Linear Systems Faster than Matrix Multiplication” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Thursday, September 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Richard Peng</strong> from Georgia Tech will speak about “<em>Solving Sparse Linear Systems Faster than Matrix Multiplication</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>. </p>



<blockquote class="wp-block-quote"><p>Abstract: Can linear systems be solved faster than matrix multiplication? While there has been remarkable progress for the special cases of graph structured linear systems, in the general setting, the bit complexity of solving an n-by-n linear system <img alt="Ax=b" class="latex" src="https://s0.wp.com/latex.php?latex=Ax%3Db&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="Ax=b"/> is <img alt="n^\omega" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%5Comega&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n^\omega"/>, where <img alt="\omega &lt; 2.372864" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega+%3C+2.372864&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\omega &lt; 2.372864"/> is the matrix multiplication exponent. Improving on this has been an open problem even for sparse linear systems with <img alt="\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\text{poly}(n)"/> condition number.</p><p>We present an algorithm that solves linear systems in sparse matrices asymptotically faster than matrix multiplication for any <img alt="\omega&gt;2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%3E2&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\omega&gt;2"/>. This speedup holds for any input matrix <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="A"/> with <img alt="o(n^{\omega-1}/\log(\kappa(A)))" class="latex" src="https://s0.wp.com/latex.php?latex=o%28n%5E%7B%5Comega-1%7D%2F%5Clog%28%5Ckappa%28A%29%29%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="o(n^{\omega-1}/\log(\kappa(A)))"/> non-zeros, where <img alt="\kappa(A)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ckappa%28A%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\kappa(A)"/> is the condition number of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="A"/>. For <img alt="\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\text{poly}(n)"/>-conditioned matrices with <img alt="O(n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="O(n)"/> nonzeros, and the current value of <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\omega"/>, the bit complexity of our algorithm to solve to within any <img alt="1/\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="1/\text{poly}(n)"/> error is <img alt="O(n^{2.331645})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2.331645%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="O(n^{2.331645})"/>.</p><p>Our algorithm can be viewed as an efficient randomized implementation of the block Krylov method via recursive low displacement rank factorizations. It is inspired by the algorithm of [Eberly-Giesbrecht-Giorgi-Storjohann-Villard ISSAC <code>06</code>07] for inverting matrices over finite fields. In our analysis of numerical stability, we develop matrix anti-concentration techniques to bound the smallest eigenvalue and the smallest gap in eigenvalues of semi-random matrices.</p><p>Joint work with Santosh Vempala, manuscript at <a href="https://arxiv.org/abs/2007.10254" rel="nofollow">https://arxiv.org/abs/2007.10254</a>.</p></blockquote></div>
    </content>
    <updated>2020-09-10T03:09:50Z</updated>
    <published>2020-09-10T03:09:50Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-26T22:27:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2727</id>
    <link href="https://francisbach.com/integration-by-parts-randomized-smoothing-score-functions/" rel="alternate" type="text/html"/>
    <title>The many faces of integration by parts – II : Randomized smoothing and score functions</title>
    <summary>This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">This month I will follow-up on last month blog post and look at another application of integration by parts, which is central to many interesting algorithms in machine learning, optimization and statistics. In this post, I will consider extensions in higher dimensions, where we take integrals on a subset of \(\mathbb{R}^d\), and focus primarily on property of the so-called “score function” of a density \(p: \mathbb{R}^d \to \mathbb{R}\), namely the gradient of its logarithm: $$\nabla  \log  p(z)  = \frac{1}{p(z)} \nabla p(z) \in \mathbb{R}^d,$$ or, done coordinate by coordinate, $$ \big(\nabla \log p(z)\big)_i = \frac{\partial [ \log p]}{\partial z_i}(z) = \frac{1}{p(z)} \frac{\partial  p }{\partial z_i}(z) .$$ Note here that we take derivatives with respect to \(z\) and not with respect to some hypothetical external parameter, which is often the case in statistics (see <a href="https://en.wikipedia.org/wiki/Score_(statistics)">here</a>).</p>



<p class="justify-text">As I will show below, this quantity comes up in many different areas, most often used with integration by parts. After a short review on integration by parts and its applications to score functions, I will present four quite diverse applications, to (1) optimization and randomized smoothing, (2) differentiable perturbed optimizers, (3) learning single-index models in statistics, and (4) score matching for density estimation.</p>



<h2>Integration by parts in multiple dimensions</h2>



<p class="justify-text">I will focus only on situations where we have some random variable \(Z\) defined on \(\mathbb{R}^d\), with differentiable strictly positive density \(p(\cdot)\) with respect to the Lebesgue measure (I could also consider bounded supports, but then I would need to use the <a href="https://en.wikipedia.org/wiki/Divergence_theorem">divergence theorem</a>). I will consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), and my goal is to provide an expression of \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] \in \mathbb{R}^d\) using the gradient of \(f\).</p>



<p class="justify-text">Assuming that \(f(z) p(z)\) goes to zero when \(\| z\| \to +\infty\), we have: $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big]  = \int_{\mathbb{R}^d} f(z)\Big( \frac{1}{p(z)} \nabla p (z) \Big) p(z) dz = \int_{\mathbb{R}^d}  f (z) \nabla p(z) dz .$$ We can then use integration by parts (together with the zero limit at infinity), to get $$\int_{\mathbb{R}^d} f (z) \nabla p(z) dz = \ – \int_{\mathbb{R}^d} p (z) \nabla f(z) dz.$$ This leads to $$\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\  – \mathbb{E} \big[ \nabla f(Z) \big]. \tag{1}$$ In other words, expectations of the gradient of \(f\) can be obtained through expectations of \(f\) times the negative of the score function.  </p>



<p class="justify-text">Note that Eq. (1) can be used in the two possible directions: to estimate the right hand side (expectation of gradients) when the score function is known, and vice-versa to estimate expectations (as a simple example, when \(f\) is constant equal to one, we get the traditional identity \(\mathbb{E} \big[ \nabla \log p(Z) \big] = 0\)).</p>



<p class="justify-text"><strong>Gaussian distribution.</strong> Assuming that \(p(z) = \frac{1}{(2\pi \sigma^2)^{d/2}} \exp\big( – \frac{1}{2 \sigma^2}\|  z – \mu\|_2^2 \big)\), that is, \(Z\) is normally distributed with mean vector \(\mu \in \mathbb{R}^d\) and covariance matrix \(\sigma^2 I\), we get a particularly simple expression $$\frac{1}{\sigma^2} \mathbb{E} \big[ f(Z) (Z-\mu)  \big] =  \mathbb{E} \big[ \nabla f(Z) \big],$$ which is often referred to as <a href="https://en.wikipedia.org/wiki/Stein%27s_lemma">Stein’s lemma</a> (see for example an application to <a href="https://en.wikipedia.org/wiki/Stein%27s_unbiased_risk_estimate">Stein’s unbiased risk estimation</a>).</p>



<p class="justify-text"><strong>Vector extension.</strong> If now \(f\) has values in \(\mathbb{R}^d\), still with the product \(f(z) p(z)\) going to zero when \(\| z\| \to +\infty\), we get $$\mathbb{E} \big[ f(Z)^\top \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla \!\cdot \! f(Z) \big], \tag{2}$$ where \(\nabla\! \cdot \! f\) is the <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a> of \(f\) defined as \(\displaystyle \nabla\! \cdot\! f(z) = \sum_{i=1}^d \frac{\partial f}{\partial z_i}(z)\). </p>



<h2>Optimization and randomized smoothing</h2>



<p class="justify-text">We consider a function \(f: \mathbb{R}^d \to \mathbb{R}\), which is  non-differentiable everywhere. There are several ways of <em>smoothing</em> it. A very traditional way is to convolve it with a smooth function. In our context, this corresponds to considering $$f_\varepsilon(x) = \mathbb{E} f(x+ \varepsilon Z) = \int_{\mathbb{R}^d} f(x+\varepsilon z) p(z) dz,$$ where \(z\) is a random variable with strictly positive sufficiently differentiable density, and \(\varepsilon \) is a positive parameter. Typically, if \(f\) is Lipschitz-continuous, \(| f – f_\varepsilon|\) is uniformly bounded by a constant times \(\varepsilon\).</p>



<p class="justify-text">Let us now assume that we can take gradients within the integral, leading to: $$\nabla f_\varepsilon(x) = \int_{\mathbb{R}^d}   \nabla f(x+\varepsilon z) p(z) dz = \mathbb{E} \big[  \nabla f(x+\varepsilon z) \big].$$ This derivation is problematic as the whole goal is to apply this to functions \(f\) which are not everywhere differentiable, so the gradient \(\nabla f\) is not always defined. It turns out that when \(p\) is sufficiently differentiable, integration by parts exactly provides an expression which does not imply the gradient of \(f\).</p>



<p class="justify-text">Indeed, still imagining that \(f\) is differentiable, we can apply Eq. (1) to the function \(z \mapsto \frac{1}{\varepsilon} f(x+\varepsilon z)\), whose gradient is the function \(z \mapsto \nabla f(x+\varepsilon z)\), and get $$\nabla f_\varepsilon(x) = \ – \frac{1}{\varepsilon} \int_{\mathbb{R}^d} f(x+\varepsilon z) \nabla p(z) dz = \frac{1}{\varepsilon} \mathbb{E} \big[ – f(x+\varepsilon Z) \nabla \log p(Z)\big].$$ These computations can easily be made rigorous and we obtain an expression of the gradient of \(f_\varepsilon\) without invoking the gradient of \(f\) (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>, <a href="https://arxiv.org/pdf/2002.08676">14</a>] for details).</p>



<p class="justify-text">Moreover, if \(p\) is a differentiable function, we can expect the expectation in the right hand side of the equation above to be bounded, and therefore the function \(f_\varepsilon\) has gradients bounded by \(\frac{1}{\varepsilon}\).</p>



<p class="justify-text">This can be used within (typically convex) optimization in two ways:</p>



<ul class="justify-text"><li><strong>Zero-th order optimization</strong>: if our goal is to minimize the function \(f\), which is non-smooth, and for which we only have access to function values (so-called “zero-th order oracle), then we can obtain an unbiased stochastic gradient of the smoothed version \(f_\varepsilon\) as \(– f(x+\varepsilon z) \nabla \log p(z)\) where \(z\) is sampled from \(p\). The variance of the stochastic gradient grows with \(1/\varepsilon\) and the bias due to the use of \(f_\varepsilon\) instead of \(f\) is proportional to \(\varepsilon\). There is thus a sweet spot for the choice of \(\varepsilon\), with many variations; see, e.g., [<a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">5</a>, <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">6</a>, <a href="https://arxiv.org/pdf/cs/0408007">7</a>]. </li><li><strong>Randomized smoothing with acceleration</strong> [<a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">8</a>, <a href="https://arxiv.org/pdf/1204.0665">9</a>]: Here the goal is to follow the “Nesterov smoothing” idea [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">10</a>] and minimize a non-smooth function \(f\) using accelerated gradient descent on the smoothed version \(f_\varepsilon\), but this time with a stochastic gradient. Stochastic versions of Nesterov accelerations are then needed; this is useful when a full deterministic smoothing of \(f\) is too costly, see [<a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">11</a>, <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">12</a>] for details.</li></ul>



<p class="justify-text"><strong>Example.</strong> We consider minimizing a quadratic function in two dimensions, and we compare below plain gradient descent, stochastic gradient descent (left) and zero-th order optimization where we take a step towards the direction \(– f(x+\varepsilon Z) \nabla \log p(Z)\) for a standard normal \(Z\). We compare stochastic zero-th order optimization to plain stochastic gradient descent (SGD) below: SGD is a first-order method requiring access to stochastic gradients with a variance that is bounded, while zero-th order optimization only requires function values, but with significantly higher variance and thus requiring more iterations to converge.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4633" height="254" src="https://francisbach.com/wp-content/uploads/2020/09/paths_video_zeroth_order.gif" width="556"/>Left: gradient descent (GD) and stochastic gradient descent (SGD). Right: zero-th order optimization. All with constant step-sizes.</figure></div>



<h2>Differentiable perturbed optimizers</h2>



<p class="justify-text">The randomized smoothing technique can be used in a different context with applications to differentiable programming. We now assume that the function \(f\) can be written as the <a href="https://en.wikipedia.org/wiki/Support_function">support function</a> of a polytope \(\mathcal{C}\), that is, for all \(u \in \mathbb{R}^d\), $$f(u) = \max_{y \in \mathcal{C}} u^\top y,$$ where \(\mathcal{C}\) is the convex hull of a finite family \((y_i)_{i \in I}\). </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4598" height="292" src="https://francisbach.com/wp-content/uploads/2020/09/polytope_intro-1024x842.png" width="357"/>Polytope \(C\), convex hull of 8 vectors in \(\mathbb{R}^2\).</figure></div>



<p class="justify-text">Typically, the family is very large (e.g., \(|I|\) is exponential in \(d\)), but a polynomial-time algorithm exists for computing an arg-max \(y^\ast(u)\) above. Classical examples, from simpler to more interesting, are:</p>



<ul class="justify-text"><li><strong>Simplex</strong>: \(\mathcal{C}\) is the set of vectors with non-negative components that sum to one, and is the convex hull of canonical basis vectors. Then \(f\) is the maximum function, and there are many classical ways of smoothing it (see link with the <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> below).</li><li><strong>Hypercube</strong>: \(\mathcal{C} = [0,1]^n\), which is the convex hull of all vectors in \(\{0,1\}^n\). The maximization of linear functions can then be done independently for each bit.</li><li><strong>Permutation matrices</strong>: \(\mathcal{C}\) is then the <a href="https://en.wikipedia.org/wiki/Birkhoff_polytope">Birkhoff polytope</a>, the convex hull of all <a href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrices</a> (square matrices with elements in \(\{0,1\}\), and with exactly a single \(1\) in each row and column). Maximizing linear functions is the classical <a href="https://en.wikipedia.org/wiki/Assignment_problem">linear assignment problem</a>.</li><li><strong>Shortest paths</strong>: given a graph, a path is a sequence of vertices which are connected to each other in the graph. They can classically be represented as a vector of of 0’s and 1’s indicating the edges which are followed by the paths. Minimizing linear functions is then equivalent to <a href="https://en.wikipedia.org/wiki/Shortest_path_problem">shortest path</a> problems.</li></ul>



<p class="justify-text">In many supervised applications, the vector \(u\) is as a function of some input \(x\) and some parameter vector \(\theta\). In order to learn the pararameter \(\theta\) from data, one needs to be able to differentiate with respect to \(\theta\), and this is typically done through the chain rule by differentiating \(y^\ast(u)\) with respect to \(u\). There come two immediate obstacles: (1) the element \(y^\ast(u)\) is not even well-defined when the arg-max is not unique, which is not a real problem because this can only be the case for a set of \(u\)’s with zero Lebesgue measure; and (2) the function \(y^\ast(u)\) is locally constant for most \(u\)’s, that is, the gradient is equal to zero almost everywhere. Thus, in the context of differentiable programming, this is non informative and essentially useless.</p>



<p class="justify-text">Randomized smoothing provides a simple and generic way to define an approximation which is differentiable and with informative gradient everywhere (there are others, such as adding a strongly convex regularizer \(\psi(y)\), and maximizing \(u^\top y\  – \psi(y)\) instead, see [<a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">20</a>] for details. See also [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>]).</p>



<p class="justify-text">In order to obtain a differentiable function through randomized smoothing, we can consider \(y^\ast(u + \varepsilon z)\), for a random \(z\), which is an instance of the more general “perturb-and-MAP” paradigm [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242">21</a>, <a href="https://icml.cc/Conferences/2012/papers/528.pdf">22</a>].</p>



<p class="justify-text">Since \(y^\ast(u)\) is a subgradient of \(f\) at \(u\) and \(f_\varepsilon(u) = \int_{\mathbb{R}^d} f(u+\varepsilon z) p(z) dz\), by swapping integration (with respect to \(z\)) and differentiation (with respect to \(u\)), we have the following identities: $$ \mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big] = \nabla f_\varepsilon(u),$$ that is, the expectation of the perturbed arg-max is the gradient of the smoothed function \(f_\varepsilon\). I will use the notation \(y^\ast_\varepsilon(u) =\mathbb{E} \big[ y^\ast(u + \varepsilon Z) \big]\) to denote this gradient; see an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4545" height="322" src="https://francisbach.com/wp-content/uploads/2020/08/polytope-1024x671.png" width="491"/>Polytope \(\mathcal{C}\), with a direction \(u\), the non-perturbed maximizer \(y^\ast(u)\), a perturbed direction \(u+\varepsilon Z\) and the perturbed maximizer \(y^\ast(u+\varepsilon Z)\). The areas of the red circles are proportional to the probability of selecting the corresponding extreme point after the perturbation. The expected perturbed maximizer \(y^\ast_\varepsilon(u)\) is in the interior of \(\mathcal{C}\).</figure></div>



<p class="justify-text">In a joint work with Quentin Berthet, Mathieu Blondel, Oliver Teboul, Marco Cuturi, and Jean-Philippe Vert [<a href="http://arxiv.org/pdf/2002.08676(opens in a new tab)">14</a>], we detail theoretical and practical properties of \(y^\ast_\varepsilon(u)\), in particular:</p>



<ul class="justify-text"><li>Estimation: \(y^\ast_\varepsilon(u)\) can be estimated by replacing the expectation by empirical averages.</li><li>Differentiability: if \(Z\) has a strictly positive density over \(\mathbb{R}^d\), then the function \(y^\ast_\varepsilon\) is infinitely differentiable, with simple expression of  the Jacobian, obtained by integration by parts (see [<a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">23</a>] for details).</li><li>The <a href="https://francisbach.com/the-gumbel-trick/">Gumbel trick</a> is the simplest instance of such a smoothing technique, with \(\mathcal{C}\) being the simplex, and \(Z\) having independent Gumbel distributions. The function \(f_\varepsilon\) is then a “<a href="https://en.wikipedia.org/wiki/LogSumExp">log-sum-exp</a>” function.</li></ul>



<p class="justify-text"><strong>Illustration</strong>. Following [<a href="https://openreview.net/pdf?id=BkevoJSYPB">24</a>], this can be applied to learn the travel costs in graphs based on features. The vectors \(y_i\) represent shortest path between the top-left and bottom-right corners, with costs corresponding to the terrain type. See [<a href="https://arxiv.org/pdf/2002.08676">14</a>] for details on the learning procedure. Here I just want to highlight the effect of varying the amount of smoothing characterized by \(\varepsilon\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4605" height="225" src="https://francisbach.com/wp-content/uploads/2020/09/paths-1024x518.png" width="446"/>Left: Warcraft terrain. Right: Cost associated to each terrain type.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full"><img alt="" class="wp-image-4624" height="288" src="https://francisbach.com/wp-content/uploads/2020/09/anim_smoothed.gif" width="432"/>Shortest paths \(y^\ast_\varepsilon(u)\), from \(\varepsilon=0\) (no smoothing) to \(\varepsilon=2\). From an essentially single shortest path, as smoothing increases, we obtain a mixture of two potential paths, before having many extreme points.</figure></div>



<h2>Learning single-index models</h2>



<p class="justify-text">Given a random vector \((X,Y) \in \mathbb{R}^d \times \mathbb{R}\), we assume that \(Y = f(X) + \varepsilon\), where \(f(x) = \sigma(w^\top x)\) for some unknown function \(\sigma: \mathbb{R} \to \mathbb{R}\) and \(w \in \mathbb{R}^d\), with \(\varepsilon\) a zero-mean noise independent from \(X\).  Given some observations \((x_1,y_1), \dots, (x_n,y_n)\) in \(\mathbb{R}^d \times \mathbb{R}\), the goal is to estimate the direction \(w \in \mathbb{R}^d\). This model is referred to as single-index regression models in the statistics literature [<a href="https://www.jstor.org/stable/pdf/1913713.pdf">1</a>, <a href="https://www.jstor.org/stable/pdf/3035585.pdf">2</a>]</p>



<p class="justify-text">One possibility if \(\sigma\) was known would be to perform least-squares estimation and minimize with respect to \(w\) $$ \frac{1}{2n} \sum_{i=1}^n \big( y_i\  – \sigma(w^\top x_i) \big)^2, $$ which is a non-convex optimization problem in general. When \(\sigma\) is unknown, one could imagine adding the estimation of \(\sigma\) into the optimization, making it even more complicated.</p>



<p class="justify-text">Score functions provide an elegant solution that leads to the “average derivative method” (ADE) [<a href="https://www.jstor.org/stable/pdf/1914309.pdf">3</a>], which I will now describe. We consider \(p\) the density of \(X\). We then have, using Eq. (1): $$ \mathbb{E} \big[ Y \nabla \log p(X) \big] =\mathbb{E} \big[ f(X) \nabla \log p(X) \big] = \ – \mathbb{E} \big[ \nabla f(X)  \big] =\ –  \Big( \mathbb{E} \big[ \sigma'(w^\top X) \big] \Big) w, $$ which is proportional to \(w\). When replacing the expectation by an empirical mean, this provides a way to estimate \(w\) (up to a constant factor) without even knowing the function \(\sigma\), but assuming the density of \(X\) is known so that the score function is available.</p>



<p class="justify-text"><strong>Extensions.</strong> The ADE method can be extended in a number of ways to deal with more complex situations. Here are some examples below:</p>



<ul class="justify-text"><li><em>Multiple index models</em>: if the response/output \(Y\) is instead assumed of the form $$ Y = f(X) + \varepsilon =  \sigma(W^\top x) + \varepsilon, $$ where \(W \in \mathbb{R}^{d \times k}\) is a matrix with \(k\) columns, we obtained a “multiple index model”, for which a similar technique seems to apply since now \(\nabla f(x) = W \nabla  \sigma(W^\top x) \in \mathbb{R}^d\), and thus, for the assumed model \(\mathbb{E} \big[ Y \nabla \log p(X) \big]\) is in the linear span of the columns of \(W\); this is not enough for recovering the entire subspace if \(k&gt;1\) because we have only a single element of the span. There are two solutions for this. The first one is is to condition on some values of \(Y\) being in some set \(\mathcal{Y}\), where one can show that \(\mathbb{E} \big[ Y \nabla \log p(X) | Y \in \mathcal{Y} \big]\) is also in the desired subspace; thus, with several sets \(\mathcal{Y}\), one can generate several elements, and after \(k\) of these, one can expect to estimate the full \(k\)-dimensional subspace. The idea of conditioning on \(Y\) is called <a href="https://en.wikipedia.org/wiki/Sliced_inverse_regression">sliced inverse regression</a> [<a href="https://www.jstor.org/stable/pdf/2290563.pdf">15</a>], and the application to score function can be found in [<a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>]. The second one is to consider higher-order moments and derivatives of the score functions, that is, using integration by parts twice! (see [<a href="https://arxiv.org/pdf/1412.2863">17</a>, <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">18</a>, <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">16</a>] for details).</li><li><em>Neural networks</em>: when the function \(\sigma\) is the sum of functions that depends on single variables, multiple-index models are exactly one-hidden-layer neural networks. Similar techniques can be used for deep networks with more than a single hidden layer (see [<a href="https://arxiv.org/pdf/1506.08473">19</a>]).</li></ul>



<p class="justify-text"><strong>Moment matching vs. empirical risk minimization. </strong>In all cases mentioned above, the use of score functions can be seen as an instance of the <a href="https://en.wikipedia.org/wiki/Method_of_moments_(statistics)">method of moments</a>: we assume a specific model for the data, derive identities satisfied by expectations of some functions under the model, and use these identities to identify a parameter vector. In the situations above, direct empirical risk minimization would lead to a potentially hard optimization problem. However, moment matching techniques rely heavily on the model being well-specified, which is often not the case in practice, while empirical risk minimization techniques try to fit the data as much as the model allows, and is thus typically more robust to model misspecification.</p>



<h2>Score matching for density estimation</h2>



<p class="justify-text">We consider the problem of density estimation. That is, given some observations \(x_1,\dots,x_n \in \mathbb{R}^d\) sampled independently and identically distributed from some distribution with density \(p\), we want to estimate \(p\) from the data. Given a model \(q_\theta \) with some parameters \(\theta\), the most standard method is maximum likelihood estimation, which corresponds to the following optimization problem: $$\max_{\theta \in \Theta} \frac{1}{n} \sum_{i=1}^n \log q_\theta(x_i).$$ It requires <em>normalized</em> densities that is, \(\int_{\mathbb{R}^d} q_\theta(x) dx = 1\), and dealing with normalized densities often requires to explicitly normalize them and thus to compute integrals, which is difficult when the underlying dimension \(d\) gets large.</p>



<p class="justify-text">Score matching is a recent method proposed by Aapo Hyvärinen [4] based on score functions. The simple (yet powerful) idea is to perform least-squares estimation on the score functions. That is, in the population case, the goal is to minimize $$\mathbb{E} \big\| \nabla \log p(X) \ – \nabla  \log q_\theta(X) \big\|_2^2 = \int_{\mathbb{R}^d} \big\| \nabla \log p(x)\  – \nabla  \log q_\theta(x) \big\|_2^2 p(x) dx.$$ Apparently, this expectation does not lead to an estimation procedure where \(p(x)\) is replaced by the empirical distribution of the data because of the presence of \(\nabla \log p(x)\). Integration by parts will solve this.</p>



<p class="justify-text">We can expand \(\mathbb{E} \big\| \nabla \log p(X) \ – \nabla \log q_\theta(X) \big\|_2^2\) as  $$ \mathbb{E} \big\| \nabla \log p(X) \|_2^2 + \mathbb{E} \big\|\nabla \log q_\theta(X) \big\|_2^2 – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big]. $$ The first term is independent of \(q_\theta\) so it does not count when minimizing. The second term is an expectation with respect to \(p(\cdot)\) so it can be replaced by the empirical mean. The third term can be dealt with with integration by parts, that is Eq. (2), leading to: $$ – 2 \mathbb{E} \big[ \nabla \log p(X)^\top \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \nabla \cdot \nabla \log q_\theta(X) \big] = 2 \mathbb{E} \big[ \Delta \log q_\theta(X) \big],$$ where \(\Delta\) is the <a href="https://en.wikipedia.org/wiki/Laplace_operator">Laplacian</a>.</p>



<p class="justify-text">We now have an expectation with respect to the data distribution \(p\), and we can replace the expectation with an empirical average to estimate the parameter \(\theta\) from data \(x_1,\dots,x_n\). We then use the cost function $$\frac{1}{n} \sum_{i=1}^n \big\|\nabla \log q_\theta(x_i) \big\|_2^2 + \frac{2}{n} \sum_{i=1}^n \Delta \log q_\theta(x_i), $$ which is linear in \(\log q_\theta\). Hence, when the unnormalized log-density is linearly parameterized, which is common, we obtain a quadratic problem. This procedure has a number of attractive properties, in particular consistency [<a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">4</a>], but the key benefit is to allow estimation without requiring normalizing constants.</p>



<h2>Conclusion</h2>



<p class="justify-text">Overall, the simple identity from Eq. (1), that is, \(\mathbb{E} \big[ f(Z) \nabla \log p(Z) \big] =\ – \mathbb{E} \big[ \nabla f(Z) \big]\), has many applications in diverse somewhat unrelated areas of machine learning, optimization and statistics. There are of course many other uses of integration by parts within this field. Feel free to add your preferred one as comment.</p>



<p class="justify-text">It has been a while since the last post on polynomial magic. I will revive the thread next month. I let you guess which polynomials will be the stars of my next blog post.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Quentin Berthet for producing the video of shortest paths, proofreading this blog post, and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] James L. Powell, James H. Stock, Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1913713.pdf">Semiparametric estimation of index coefficients</a>. <em>Econometrica: Journal of the Econometric Society</em>. 57(6):1403-1430, 1989.<br/>[2] Wolfgang Hardle, Peter Hall, Hidehiko Ichimura. <a href="https://www.jstor.org/stable/pdf/3035585.pdf">Optimal smoothing in single-index models</a>. <em>Annals of Statistics</em>. 21(1): 157-178(1993): 157-178.<br/>[3] Thomas M. Stoker. <a href="https://www.jstor.org/stable/pdf/1914309.pdf">Consistent Estimation of Scaled Coefficients</a>. <em>Econometrica</em>, 54(6):1461-1481, 1986.<br/>[4] Aapo Hyvärinen. <a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Estimation of non-normalized statistical models by score matching</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(Apr), 695-709, 2005.<br/>[5] Yurii Nesterov. <a href="https://econpapers.repec.org/scripts/redir.pf?u=http%3A%2F%2Fuclouvain.be%2Fcps%2Fucl%2Fdoc%2Fcore%2Fdocuments%2Fcoredp2011_1web.pdf;h=repec:cor:louvco:2011001">Random gradient-free minimization of convex functions</a>. Technical report, Université Catholique de Louvain (CORE), 2011.<br/>[6] Boris T. Polyak and Alexander B. Tsybakov. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=ppi&amp;paperid=605&amp;what=fullt&amp;option_lang=eng">Optimal order of accuracy of search algorithms in stochastic optimization</a>. <em>Problemy Peredachi Informatsii</em>, 26(2):45–53, 1990.<br/>[7]  Abraham D. Flaxman, Adam Tauman Kalai, H. Brendan McMahan. <a href="https://arxiv.org/pdf/cs/0408007">Online convex optimization in the bandit setting: gradient descent without a gradient</a>. In Proc. Symposium on Discrete algorithms (SODA), 2005.<br/>[8] John C. Duchi, Peter L. Bartlett, and Martin J. Wainwright. <a href="https://epubs.siam.org/doi/pdf/10.1137/110831659">Randomized Smoothing for Stochastic Optimization</a>. SIAM Journal on Optimization, 22(2), 674–701, 2012.<br/>[9] Alexandre d’Aspremont, Nourredine El Karoui, <a href="https://www.di.ens.fr/~aspremon/stochsmooth.html">A Stochastic Smoothing Algorithm for Semidefinite Programming.</a> <em>SIAM Journal on Optimization</em>, 24(3): 1138-1177, 2014.<br/>[10] Yurii Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions</a>. Mathematical Programming, 103(1):127–152, 2005.<br/>[11] Lin Xiao. <a href="http://www.jmlr.org/papers/volume11/xiao10a/xiao10a.pdf">Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization</a>. <em>Journal of Machine Learning Research</em>, 11(88): 2543−2596, 2010.<br/>[12] Guanghui Lan. <a href="https://link.springer.com/content/pdf/10.1007/s10107-010-0434-y.pdf">An optimal method for stochastic composite optimization</a>. <em>Mathematical Programming</em>, 133(1):365–397, 2012.<br/>[13] Tamir Hazan, George Papandreou, and Daniel Tarlow. <a href="https://mitpress.mit.edu/books/perturbations-optimization-and-statistics">Perturbation, Optimization, and Statistics</a>. MIT Press, 2016.<br/>[14] Quentin Berthet, Matthieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach, <a href="https://arxiv.org/pdf/2002.08676">Learning with differentiable perturbed optimizers</a>. Technical report arXiv 2002.08676, 2020.<br/>[15] Ker-Chau Li. <a href="https://www.jstor.org/stable/pdf/2290563.pdf">Sliced inverse regression for dimension reduction</a>. <em>Journal of the American Statistical Association</em>, <em>86</em>(414), 316-327, 1991.<br/>[16] Dmitry Babichev and Francis Bach. <a href="https://projecteuclid.org/download/pdfview_1/euclid.ejs/1526889626">Slice inverse regression with score functions</a>. <em>Electronic Journal of Statistics</em>, 12(1):1507-1543, 2018.<br/>[17] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1412.2863">Score function features for discriminative learning: Matrix and tensor framework</a>. Technical report arXiv:1412.2863, 2014.<br/>[18] David R. Brillinger. <a href="https://link.springer.com/chapter/10.1007/978-1-4614-1344-8_34">A generalized linear model with “Gaussian” regressor variables</a>.  <em>Selected Works of David Brillinger</em>, 589-606, 2012.<br/>[19] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. <a href="https://arxiv.org/pdf/1506.08473">Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods</a>.  Technical report arXiv:1506.08473, 2015.<br/>[20] Vlad Niculae, André F. T. Martins, Mathieu Blondel, and Claire Cardie. <a href="http://proceedings.mlr.press/v80/niculae18a/niculae18a.pdf">SparseMAP: Differentiable sparse structured inference</a>. <em>Proceedings of the International Conference on Machine Learning (ICML)</em>, 2017.<br/>[21] George Papandreou and Alan L. Yuille.<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126242"> Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models</a>. <em>International Conference on Computer Vision</em>, 2011.<br/>[22] Tamir Hazan and Tommi Jaakkola. <a href="https://icml.cc/Conferences/2012/papers/528.pdf">On the partition function and random maximum a-posteriori perturbations</a>. <em>Proceedings of the International Conference on International Conference on Machine Learning (ICML),</em> 2012.<br/>[23] Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. <a href="http://dept.stat.lsa.umich.edu/~tewaria/research/abernethy16perturbation.pdf">Perturbation techniques in online learning and optimization</a>. <em>Perturbations, Optimization, and Statistics</em>, 233-264, 2016.<br/>[24] Marin Vlastelica, Anselm Paulus, Vít Musil, Georg Martius, Michal Rolínek. <a href="https://openreview.net/pdf?id=BkevoJSYPB">Differentiation of Blackbox Combinatorial Solvers</a>. <em>International Conference on Learning Representations</em>. 2019.</p></div>
    </content>
    <updated>2020-09-07T19:06:36Z</updated>
    <published>2020-09-07T19:06:36Z</published>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-11-26T22:35:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5519</id>
    <link href="https://adamsheffer.wordpress.com/2020/09/04/mathematics-for-human-flourishing/" rel="alternate" type="text/html"/>
    <title>Mathematics for Human Flourishing</title>
    <summary>I read a lot of “popular math” books. I also wrote about some in past posts. But I’ve never read a book similar to Mathematics for Human Flourishing by Francis Su. I already knew the math presented in this book and almost all other topics covered. I even knew some of Su’s personal stories from […]</summary>
    <updated>2020-09-04T21:21:41Z</updated>
    <published>2020-09-04T21:21:41Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-11-26T22:27:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=52</id>
    <link href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/" rel="alternate" type="text/html"/>
    <title>Friday, Sept 11 — Bin Yu from UC Berkeley</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Friday, Sept 11th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  Bin Yu from UC Berkeley will speak about “Veridical Data Science”. Abstract: Building and expanding on principles of statistics, machine learning, and the sciences, we propose<a class="more-link" href="https://dstheory.wordpress.com/2020/09/04/friday-sept-11-bin-yu-from-uc-berkeley/">Continue reading <span class="screen-reader-text">"Friday, Sept 11 — Bin Yu from UC Berkeley"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next Foundations of Data Science virtual talk will take place on Friday, Sept 11th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Bin Yu </strong>from UC Berkeley will speak about “<em>Veridical Data Science</em>”.</p>



<p><strong>Abstract</strong>: Building and expanding on principles of statistics, machine learning, and the sciences, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework is comprised of both a workflow and documentation and aims to provide responsible, reliable, reproducible, and transparent results across the entire data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle for the data science life cycle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. We develop inference procedures that build on PCS, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations.</p>



<p>Moreover, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis.</p>



<p>The PCS framework will be illustrated through our DeepTune approach to model and characterize neurons in the difficult visual cortex area V4.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-09-04T00:47:54Z</updated>
    <published>2020-09-04T00:47:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-11-26T22:33:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://nisheethvishnoi.wordpress.com/?p=99</id>
    <link href="https://nisheethvishnoi.wordpress.com/2020/08/31/algorithms-for-convex-optimization-book/" rel="alternate" type="text/html"/>
    <title>Algorithms for Convex Optimization Book</title>
    <summary>I am excited to announce that a pre-publication draft of my book Algorithms for Convex Optimization (to be published by Cambridge University Press) is now available for download here: Algorithms for Convex Optimization Book The goal of this book is to enable a reader to gain an in-depth understanding of algorithms for convex optimization. The emphasis is […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am excited to announce that a pre-publication draft of my book <strong>Algorithms for Convex Optimization</strong> (to be published by Cambridge University Press) is now available for download here:</p>
<p><a href="https://convex-optimization.github.io/">Algorithms for Convex Optimization Book</a></p>
<p>The goal of this book is to enable a reader to gain an in-depth understanding of algorithms for convex optimization. The emphasis is to derive key algorithms for convex optimization from first principles and to establish precise running time bounds in terms of the input length.</p>
<p>The intended audience includes advanced undergraduate students, graduate students and researches from theoretical computer science, discrete optimization, and machine learning.</p>
<div class="page" title="Page 8">
<div class="layoutArea">
<div class="column">
<p>The book has four parts:</p>
</div>
</div>
</div>
<ol>
<li class="p1">Chapters 3,4, and 5: Introduction to convexity, models of computation and notions of efficiency in convex optimization, Lagrangian duality, Legendre-Fenchel duality, and KKT conditions.</li>
<li class="p1">Chapters 6,7, and 8: First-order methods such as gradient descent, mirror descent and the multiplicative weights update method, and accelerated gradient descent.</li>
<li class="p1">Chapters 9,10, and 11: Newton’s method, path-following interior point methods for linear programming, and self-concordant barrier functions.</li>
<li class="p1">Chapter 11 and 12: Cutting plane methods such as the ellipsoid method for linear and general convex programs.</li>
</ol>
<p class="p1">Chapter 1 summarizes the book via a brief history of the interplay between continuous and discrete optimization: how the search for fast algorithms for discrete problems is leading to improvements in algorithms for convex optimization.</p>
<p class="p1">Many chapters contain applications ranging from finding maximum flows, minimum cuts, and perfect matchings in graphs, to linear optimization over 0-1-polytopes, to submodular function minimization, to computing maximum entropy distributions over combinatorial polytopes.</p>
<p class="p1">The book is self-contained and starts with a review of calculus, linear algebra, geometry, dynamical systems, and graph theory in Chapter 2. Exercises posed in this book not only play an important role in checking one’s understanding, but sometimes important methods and concepts are introduced and developed entirely through them. Examples include the Frank-Wolfe method, coordinate descent, stochastic gradient descent, online convex optimization, the min-max theorem for zero-sum games, the Winnow algorithm for classification, the conjugate gradient method, primal-dual interior point method, and matrix scaling.</p>
<p>Feedback, corrections, and comments are welcome.</p></div>
    </content>
    <updated>2020-08-31T13:30:40Z</updated>
    <published>2020-08-31T13:30:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>nisheethvishnoi</name>
    </author>
    <source>
      <id>https://nisheethvishnoi.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://nisheethvishnoi.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://nisheethvishnoi.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Algorithms, Nature, and Society</title>
      <updated>2020-11-26T22:30:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=790</id>
    <link href="https://emanueleviola.wordpress.com/2020/08/25/my-monitor-setup/" rel="alternate" type="text/html"/>
    <title>My monitor setup</title>
    <summary>In the not-distant future there will be a single monitor that gets you the best of both worlds. For the contemporaneous, I maintain that the above is the best monitor setup available to us in 2020. I use the tiny E-ink monitor as much as possible, including now, for my blitz matches on chess.com, and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="wp-block-image size-large"><img alt="" class="wp-image-792" src="https://emanueleviola.files.wordpress.com/2020/08/webcam-toy-photo7.jpg?w=800"/></figure>

<p>In the not-distant future there will be a single monitor that gets you the best of both worlds. For the contemporaneous, I maintain that the above is the best monitor setup available to us in 2020. I use the tiny E-ink monitor as much as possible, including now, for my blitz matches on chess.com, and of course for writing and sometimes reviewing papers. But as I mentioned earlier unfortunately for certain bureaucratic tasks that not all of us can skip altogether you just need a bigger monitor with color. So I push a button on the hdmi switch, and the image blasts open on the 30-inch screen, <em>m’illumino d’immenso</em>, and suddenly the mouse feels like the interface from <em>Minority Reports</em>.</p></div>
    </content>
    <updated>2020-08-25T19:35:48Z</updated>
    <published>2020-08-25T19:35:48Z</published>
    <category term="Uncategorized"/>
    <category term="health"/>
    <category term="tech"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2020-11-26T22:27:48Z</updated>
    </source>
  </entry>
</feed>
