<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-07-23T03:22:30Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/07/23/tenure-track-or-tenured-faculty-positions-at-center-on-frontiers-of-computing-studies-peking-university-apply-by-september-30-2020/</id>
    <link href="https://cstheory-jobs.org/2020/07/23/tenure-track-or-tenured-faculty-positions-at-center-on-frontiers-of-computing-studies-peking-university-apply-by-september-30-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track or Tenured Faculty Positions at Center on Frontiers of Computing Studies, Peking University (apply by September 30, 2020)</title>
    <summary>The Center on Frontiers of Computing Studies (CFCS), Peking University (PKU), China, is a university new initiative co-founded by Professors John Hopcroft and Wen Gao. We are seeking applicants from all areas of Computer Science, spanning theoretical foundations, systems, software, and applications, with special interests in artificial intelligence and machine learning. Website: https://cfcs.pku.edu.cn/english/people/joinus/236979.htm Email: cfcs_recruiting@pku.edu.cn</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Center on Frontiers of Computing Studies (CFCS), Peking University (PKU), China, is a university new initiative co-founded by Professors John Hopcroft and Wen Gao.</p>
<p>We are seeking applicants from all areas of Computer Science, spanning theoretical foundations, systems, software, and applications, with special interests in artificial intelligence and machine learning.</p>
<p>Website: <a href="https://cfcs.pku.edu.cn/english/people/joinus/236979.htm">https://cfcs.pku.edu.cn/english/people/joinus/236979.htm</a><br/>
Email: cfcs_recruiting@pku.edu.cn</p></div>
    </content>
    <updated>2020-07-23T02:00:39Z</updated>
    <published>2020-07-23T02:00:39Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-07-23T03:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11559</id>
    <link href="http://arxiv.org/abs/2007.11559" rel="alternate" type="text/html"/>
    <title>An Improved Approximation Algorithm for the Matching Augmentation Problem</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>J. Cheriyan, R. Cummings, J. Dippel, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:J=.html">J. Zhu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11559">PDF</a><br/><b>Abstract: </b>We present a $\frac53$-approximation algorithm for the matching augmentation
problem (MAP): given a multi-graph with edges of cost either zero or one such
that the edges of cost zero form a matching, find a 2-edge connected spanning
subgraph (2-ECSS) of minimum cost.
</p>
<p>A $\frac74$-approximation algorithm for the same problem was presented
recently, see Cheriyan, et al., "The matching augmentation problem: a
$\frac{7}{4}$-approximation algorithm," {\em Math. Program.}, 182(1):315--354,
2020; <a href="http://export.arxiv.org/abs/1810.07816">arXiv:1810.07816</a>.
</p>
<p>Our improvement is based on new algorithmic techniques, and some of these may
lead to advances on related problems.
</p></div>
    </summary>
    <updated>2020-07-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11532</id>
    <link href="http://arxiv.org/abs/2007.11532" rel="alternate" type="text/html"/>
    <title>Online Adaptive Bin Packing with Overflow</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perez=Salazar:Sebastian.html">Sebastian Perez-Salazar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Mohit.html">Mohit Singh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Toriello:Alejandro.html">Alejandro Toriello</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11532">PDF</a><br/><b>Abstract: </b>Motivated by bursty bandwidth allocation and by the allocation of virtual
machines into servers in the cloud, we consider the online problem of packing
items with random sizes into unit-capacity bins. Items arrive sequentially, but
upon arrival an item's actual size is unknown; only its probabilistic
information is available to the decision maker. Without knowing this size, the
decision maker must irrevocably pack the item into an available bin or place it
in a new bin. Once packed in a bin, the decision maker observes the item's
actual size, and overflowing the bin is a possibility. An overflow incurs a
large penalty cost and the corresponding bin is unusable for the rest of the
process. In practical terms, this overflow models delayed services, failure of
servers, and/or loss of end-user goodwill. The objective is to minimize the
total expected cost given by the sum of the number of opened bins and the
overflow penalty cost. We present an online algorithm with expected cost at
most a constant factor times the cost incurred by the optimal packing policy
when item sizes are drawn from an i.i.d. sequence of unknown length. We give a
similar result when item size distributions are exponential with arbitrary
rates.
</p></div>
    </summary>
    <updated>2020-07-23T01:21:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11495</id>
    <link href="http://arxiv.org/abs/2007.11495" rel="alternate" type="text/html"/>
    <title>Improved Distance Sensitivity Oracles with Subcubic Preprocessing Time</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ren:Hanlin.html">Hanlin Ren</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11495">PDF</a><br/><b>Abstract: </b>We consider the problem of building Distance Sensitivity Oracles (DSOs).
Given a directed graph $G=(V, E)$ with edge weights in $\{1, 2, \dots, M\}$, we
need to preprocess it into a data structure, and answer the following queries:
given vertices $u,v\in V$ and a failed vertex or edge $f\in (V\cup E)$, output
the length of the shortest path from $u$ to $v$ that does not go through $f$.
Our main result is a simple DSO with $\tilde{O}(n^{2.7233}M)$ preprocessing
time and $O(1)$ query time. Moreover, if the input graph is undirected, the
preprocessing time can be improved to $\tilde{O}(n^{2.6865}M)$. The
preprocessing algorithm is randomized with correct probability $\ge 1-1/n^C$,
for a constant $C$ that can be made arbitrarily large. Previously, there is a
DSO with $\tilde{O}(n^{2.8729}M)$ preprocessing time and
$\operatorname{polylog}(n)$ query time [Chechik and Cohen, STOC'20].
</p>
<p>At the core of our DSO is the following observation from [Bernstein and
Karger, STOC'09]: if there is a DSO with preprocessing time $P$ and query time
$Q$, then we can construct a DSO with preprocessing time $P+\tilde{O}(n^2)\cdot
Q$ and query time $O(1)$. (Here $\tilde{O}(\cdot)$ hides
$\operatorname{polylog}(n)$ factors.)
</p></div>
    </summary>
    <updated>2020-07-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11476</id>
    <link href="http://arxiv.org/abs/2007.11476" rel="alternate" type="text/html"/>
    <title>Approximate Covering with Lower and Upper Bounds via LP Rounding</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bandyapadhyay:Sayan.html">Sayan Bandyapadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roy:Aniket_Basu.html">Aniket Basu Roy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11476">PDF</a><br/><b>Abstract: </b>In this paper, we study the lower- and upper-bounded covering (LUC) problem,
where we are given a set $P$ of $n$ points, a collection $\mathcal{B}$ of
balls, and parameters $L$ and $U$. The goal is to find a minimum-sized subset
$\mathcal{B}'\subseteq \mathcal{B}$ and an assignment of the points in $P$ to
$\mathcal{B}'$, such that each point $p\in P$ is assigned to a ball that
contains $p$ and for each ball $B_i\in \mathcal{B}'$, at least $L$ and at most
$U$ points are assigned to $B_i$. We obtain an LP rounding based constant
approximation for LUC by violating the lower and upper bound constraints by
small constant factors and expanding the balls by again a small constant
factor. Similar results were known before for covering problems with only the
upper bound constraint. We also show that with only the lower bound constraint,
the above result can be obtained without any lower bound violation.
</p>
<p>Covering problems have close connections with facility location problems. We
note that the known constant-approximation for the corresponding lower- and
upper-bounded facility location problem, violates the lower and upper bound
constraints by a constant factor.
</p></div>
    </summary>
    <updated>2020-07-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11451</id>
    <link href="http://arxiv.org/abs/2007.11451" rel="alternate" type="text/html"/>
    <title>Point-Location in The Arrangement of Curves</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aghamolaei:Sepideh.html">Sepideh Aghamolaei</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11451">PDF</a><br/><b>Abstract: </b>An arrangement of $n$ curves in the plane is given. The query is a point $q$
and the goal is to find the face of the arrangement that contains $q$. A
data-structure for point-location, preprocesses the curves into a data
structure of polynomial size in $n$, such that the queries can be answered in
time polylogarithmic in $n$.
</p>
<p>We design a data structure for solving the point location problem queries in
$O(\log C(n)+\log S(n))$ time using $O(T(n)+S(n)\log(S(n)))$ preprocessing
time, if a polygonal subdivision of total size $S(n)$, with cell complexity at
most $C(n)$ can be computed in time $T(n)$, such that the order of the parts of
the curves inside each cell has a monotone order with respect to at least one
segment of the boundary of the cell. We call such a partitioning a
curve-monotone polygonal subdivision.
</p></div>
    </summary>
    <updated>2020-07-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11410</id>
    <link href="http://arxiv.org/abs/2007.11410" rel="alternate" type="text/html"/>
    <title>Sum-of-squares chordal decomposition of polynomial matrix inequalities</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zheng:Yang.html">Yang Zheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fantuzzi:Giovanni.html">Giovanni Fantuzzi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11410">PDF</a><br/><b>Abstract: </b>We prove three decomposition results for sparse positive (semi-)definite
polynomial matrices. First, we show that a polynomial matrix $P(x)$ with
chordal sparsity is positive semidefinite for all $x\in \mathbb{R}^n$ if and
only if there exists a sum-of-squares (SOS) polynomial $\sigma(x)$ such that
$\sigma(x)P(x)$ can be decomposed into a sum of sparse SOS matrices, each of
which is zero outside a small principal submatrix. Second, we establish that
setting $\sigma(x)=(x_1^2 + \cdots + x_n^2)^\nu$ for some integer $\nu$
suffices if $P(x)$ is even, homogeneous, and positive definite. Third, we prove
a sparse-matrix version of Putinar's Positivstellensatz: if $P(x)$ has chordal
sparsity and is positive definite on a compact semialgebraic set
$\mathcal{K}=\{x:g_1(x)\geq 0,\ldots,g_m(x)\geq 0\}$ satisfying the Archimedean
condition, then $P(x) = S_0(x) + g_1(x)S_1(x) + \cdots + g_m(x)S_m(x)$ for
matrices $S_i(x)$ that are sums of sparse SOS matrices, each of which is zero
outside a small principal submatrix. Using these decomposition results, we
obtain sparse SOS representation theorems for polynomials that are quadratic
and correlatively sparse in a subset of variables. We also obtain new
convergent hierarchies of sparsity-exploiting SOS reformulations to convex
optimization problems with large and sparse polynomial matrix inequalities.
Analytical examples illustrate all our decomposition results, while large-scale
numerical examples demonstrate that the corresponding sparsity-exploiting SOS
hierarchies have significantly lower computational complexity than traditional
ones.
</p></div>
    </summary>
    <updated>2020-07-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11402</id>
    <link href="http://arxiv.org/abs/2007.11402" rel="alternate" type="text/html"/>
    <title>Independent Set on C$_{\geq k}$-Free Graphs in Quasi-Polynomial Time</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gartland:Peter.html">Peter Gartland</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lokshtanov:Daniel.html">Daniel Lokshtanov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11402">PDF</a><br/><b>Abstract: </b>We give an algorithm that takes as input a graph $G$ with weights on the
vertices and outputs a maximum weight independent set of $G$. If $G$ does not
contain any cycle on $k$ or more vertices as an induced subgraph ($G$ is a
$C_{\geq k}$-free graph), the algorithm runs in time $n^{O(k^3 \log^5 n)}$, and
therefore for fixed $k$ is a quasi-polynomial time algorithm. $C_{\geq 4}$-free
graphs (also known as chordal graphs) have a well known polynomial time
algorithm. A subexponetial time algorithm for $C_{\geq 5}$-free graphs (also
known as long-hole-free graphs) was found in 2019 [Chudnovsky et al., Arxiv'19]
followed by a polynomial time algorithm for $C_{\geq 5}$-free graphs in 2020
[Abrishami et al., Arxiv'20]. For $k &gt; 5$ only a quasi-polynomial time
approximation scheme [Chudnovsky et al., SODA'20] was known. Our work is the
first to exhibit conclusive evidence that Independent Set on $C_{\geq k}$-free
graphs is not NP-complete for any integer $k$. This also generalizes previous
work of ours [Gartland and Lokshtanov, FOCS'20], with an additional factor of
$\log^2(n)$ in the exponent, where we provided a quasi-polynomial time
algorithm for graphs that exclude a path on $k$ vertices as an induced
subgraph.
</p></div>
    </summary>
    <updated>2020-07-23T01:25:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11398</id>
    <link href="http://arxiv.org/abs/2007.11398" rel="alternate" type="text/html"/>
    <title>A Framework for Consistency Algorithms</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chini:Peter.html">Peter Chini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saivasan:Prakash.html">Prakash Saivasan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11398">PDF</a><br/><b>Abstract: </b>We present a framework that provides deterministic consistency algorithms for
given memory models. Such an algorithm checks whether the executions of a
shared-memory concurrent program are consistent under the axioms defined by a
model. For memory models like SC and TSO, checking consistency is NP-complete.
Our framework shows, that despite the hardness, fast deterministic consistency
algorithms can be obtained by employing tools from fine-grained complexity. The
framework is based on a universal consistency problem which can be instantiated
by different memory models. We construct an algorithm for the problem running
in time O*(2^k), where k is the number of write accesses in the execution that
is checked for consistency. Each instance of the framework then admits an
O*(2^k)-time consistency algorithm. By applying the framework, we obtain
corresponding consistency algorithms for SC, TSO, PSO, and RMO. Moreover, we
show that the obtained algorithms for SC, TSO, and PSO are optimal in the
fine-grained sense: there is no consistency algorithm for these running in time
2^o(k) unless the exponential time hypothesis fails.
</p></div>
    </summary>
    <updated>2020-07-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11389</id>
    <link href="http://arxiv.org/abs/2007.11389" rel="alternate" type="text/html"/>
    <title>A 3/2-Approximation for the Metric Many-visits Path TSP</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=eacute=rczi:Krist=oacute=f.html">Kristóf Bérczi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mnich:Matthias.html">Matthias Mnich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vincze:Roland.html">Roland Vincze</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11389">PDF</a><br/><b>Abstract: </b>In the Many-visits Path TSP, we are given a set of $n$ cities along with
their pairwise distances (or cost) $c(uv)$, and moreover each city $v$ comes
with an associated positive integer request $r(v)$.
</p>
<p>The goal is to find a minimum-cost path, starting at city $s$ and ending at
city $t$, that visits each city $v$ exactly $r(v)$ times.
</p>
<p>We present a $\frac32$-approximation algorithm for the metric Many-visits
Path TSP, that runs in time polynomial in $n$ and poly-logarithmic in the
requests $r(v)$.
</p>
<p>Our algorithm can be seen as a far-reaching generalization of the
$\frac32$-approximation algorithm for Path TSP by Zenklusen (SODA 2019), which
answered a long-standing open problem by providing an efficient algorithm which
matches the approximation guarantee of Christofides' algorithm from 1976 for
metric TSP.
</p>
<p>One of the key components of our approach is a polynomial-time algorithm to
compute a connected, degree bounded multigraph of minimum cost.
</p>
<p>We tackle this problem by generalizing a fundamental result of Kir\'aly, Lau
and Singh (Combinatorica, 2012) on the Minimum Bounded Degree Matroid Basis
problem, and devise such an algorithm for general polymatroids, even allowing
element multiplicities.
</p>
<p>Our result directly yields a $\frac32$-approximation to the metric
Many-visits TSP, as well as a $\frac32$-approximation for the problem of
scheduling classes of jobs with sequence-dependent setup times on a single
machine so as to minimize the makespan.
</p></div>
    </summary>
    <updated>2020-07-23T01:22:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11278</id>
    <link href="http://arxiv.org/abs/2007.11278" rel="alternate" type="text/html"/>
    <title>The mergegram of a dendrogram and its stability</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Yury Elkin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurlin:Vitaliy.html">Vitaliy Kurlin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11278">PDF</a><br/><b>Abstract: </b>This paper extends the key concept of persistence within Topological Data
Analysis (TDA) in a new direction. TDA quantifies topological shapes hidden in
unorganized data such as clouds of unordered points. In the 0-dimensional case
the distance-based persistence is determined by a single-linkage (SL)
clustering of a finite set in a metric space. Equivalently, the 0D persistence
captures only edge-lengths of a Minimum Spanning Tree (MST). Both SL dendrogram
and MST are unstable under perturbations of points. We define the new
stable-under-noise mergegram, which outperforms previous isometry invariants on
a classification of point clouds by PersLay.
</p></div>
    </summary>
    <updated>2020-07-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11094</id>
    <link href="http://arxiv.org/abs/2007.11094" rel="alternate" type="text/html"/>
    <title>New Data Structures for Orthogonal Range Reporting and Range Minima Queries</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nekrich:Yakov.html">Yakov Nekrich</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11094">PDF</a><br/><b>Abstract: </b>In this paper we present new data structures for two extensively studied
variants of the orthogonal range searching problem.
</p>
<p>First, we describe a data structure that supports two-dimensional orthogonal
range minima queries in $O(n)$ space and $O(\log^{\varepsilon} n)$ time, where
$n$ is the number of points in the data structure and $\varepsilon$ is an
arbitrarily small positive constant. Previously known linear-space solutions
for this problem require $O(\log^{1+\varepsilon} n)$ (Chazelle, 1988) or
$O(\log n\log \log n)$ time (Farzan et al., 2012). A modification of our data
structure uses space $O(n\log \log n)$ and supports range minima queries in
time $O(\log \log n)$. Both results can be extended to support
three-dimensional five-sided reporting queries.
</p>
<p>Next, we turn to the four-dimensional orthogonal range reporting problem and
present a data structure that answers queries in optimal $O(\log n/\log \log n
+ k)$ time, where $k$ is the number of points in the answer. This is the first
data structure that achieves the optimal query time for this problem.
</p>
<p>Our results are obtained by exploiting the properties of three-dimensional
shallow cuttings.
</p></div>
    </summary>
    <updated>2020-07-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11093</id>
    <link href="http://arxiv.org/abs/2007.11093" rel="alternate" type="text/html"/>
    <title>Improved lower and upper bounds on the tile complexity of uniquely self-assembling a thin rectangle non-cooperatively in 3D</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Furcy:David.html">David Furcy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Summers:Scott_M=.html">Scott M. Summers</a>, Logan Withers <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11093">PDF</a><br/><b>Abstract: </b>We investigate a fundamental question regarding a benchmark class of shapes
in one of the simplest, yet most widely utilized abstract models of algorithmic
tile self-assembly. Specifically, we study the directed tile complexity of a $k
\times N$ thin rectangle in Winfree's abstract Tile Assembly Model, assuming
that cooperative binding cannot be enforced (temperature-1 self-assembly) and
that tiles are allowed to be placed at most one step into the third dimension
(just-barely 3D). While the directed tile complexities of a square and a
scaled-up version of any algorithmically specified shape at temperature 1 in
just-barely 3D are both asymptotically the same as they are (respectively) at
temperature 2 in 2D, the bounds on the directed tile complexity of a thin
rectangle at temperature 2 in 2D are not known to hold at temperature 1 in
just-barely 3D. Motivated by this discrepancy, we establish new lower and upper
bounds on the directed tile complexity of a thin rectangle at temperature 1 in
just-barely 3D. We develop a new, more powerful type of Window Movie Lemma that
lets us upper bound the number of "sufficiently similar" ways to assign glues
to a set of fixed locations. Consequently, our lower bound,
$\Omega\left(N^{\frac{1}{k}}\right)$, is an asymptotic improvement over the
previous best lower bound and is more aesthetically pleasing since it
eliminates the $k$ that used to divide $N^{\frac{1}{k}}$. The proof of our
upper bound is based on a just-barely 3D, temperature-1 counter, organized
according to "digit regions", which affords it roughly fifty percent more
digits for the same target rectangle compared to the previous best counter.
This increase in digit density results in an upper bound of
$O\left(N^{\frac{1}{\left\lfloor\frac{k}{2}\right\rfloor}}+\log N\right)$, that
is an asymptotic improvement over the previous best upper bound and roughly the
square of our lower bound.
</p></div>
    </summary>
    <updated>2020-07-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10898</id>
    <link href="http://arxiv.org/abs/2007.10898" rel="alternate" type="text/html"/>
    <title>Static and Streaming Data Structures for Fr\'echet Distance Queries</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Arnold.html">Arnold Filtser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Omrit.html">Omrit Filtser</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10898">PDF</a><br/><b>Abstract: </b>Given a curve $P$ with points in $\mathbb{R}^d$ in a streaming fashion, and
parameters $\varepsilon&gt;0$ and $k$, we construct a distance oracle that uses
$O(\frac{1}{\varepsilon})^{kd}\log\varepsilon^{-1}$ space, and given a query
curve $Q$ with $k$ points in $\mathbb{R}^d$, returns in $\tilde{O}(kd)$ time a
$1+\varepsilon$ approximation of the discrete Fr\'echet distance between $Q$
and $P$.
</p>
<p>In addition, we construct simplifications in the streaming model, oracle for
distance queries to a sub-curve (in the static setting), and introduce the
zoom-in problem. Our algorithms work in any dimension $d$, and therefore we
generalize some useful tools and algorithms for curves under the discrete
Fr\'echet distance to work efficiently in high dimensions.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10632</id>
    <link href="http://arxiv.org/abs/2007.10632" rel="alternate" type="text/html"/>
    <title>Rational homotopy type and computability</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manin:Fedor.html">Fedor Manin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10632">PDF</a><br/><b>Abstract: </b>Given a simplicial pair $(X,A)$, a simplicial complex $Y$, and a map $f:A \to
Y$, does $f$ have an extension to $X$? We show that for a fixed $Y$, this
question is algorithmically decidable for all $X$, $A$, and $f$ if and only if
$Y$ has the rational homotopy type of an H-space. As a corollary, many
questions related to bundle structures over a finite complex are decidable.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10430</id>
    <link href="http://arxiv.org/abs/2007.10430" rel="alternate" type="text/html"/>
    <title>A Survey of Algorithms for Geodesic Paths and Distances</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Crane:Keenan.html">Keenan Crane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Livesu:Marco.html">Marco Livesu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Puppo:Enrico.html">Enrico Puppo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qin:Yipeng.html">Yipeng Qin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10430">PDF</a><br/><b>Abstract: </b>Numerical computation of shortest paths or geodesics on curved domains, as
well as the associated geodesic distance, arises in a broad range of
applications across digital geometry processing, scientific computing, computer
graphics, and computer vision. Relative to Euclidean distance computation,
these tasks are complicated by the influence of curvature on the behavior of
shortest paths, as well as the fact that the representation of the domain may
itself be approximate. In spite of the difficulty of this problem, recent
literature has developed a wide variety of sophisticated methods that enable
rapid queries of geodesic information, even on relatively large models. This
survey reviews the major categories of approaches to the computation of
geodesic paths and distances, highlighting common themes and opportunities for
future improvement.
</p></div>
    </summary>
    <updated>2020-07-23T00:00:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10307</id>
    <link href="http://arxiv.org/abs/2007.10307" rel="alternate" type="text/html"/>
    <title>Optimal $\ell_1$ Column Subset Selection and a Fast PTAS for Low Rank Approximation</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Arvind V. Mahankali, David P. Woodruff Carnegie Mellon University) <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10307">PDF</a><br/><b>Abstract: </b>We study the problem of entrywise $\ell_1$ low rank approximation. We give
the first polynomial time column subset selection-based $\ell_1$ low rank
approximation algorithm sampling $\tilde{O}(k)$ columns and achieving an
$\tilde{O}(k^{1/2})$-approximation for any $k$, improving upon the previous
best $\tilde{O}(k)$-approximation and matching a prior lower bound for column
subset selection-based $\ell_1$-low rank approximation which holds for any
$\text{poly}(k)$ number of columns. We extend our results to obtain tight upper
and lower bounds for column subset selection-based $\ell_p$ low rank
approximation for any $1 &lt; p &lt; 2$, closing a long line of work on this problem.
</p>
<p>We next give a $(1 + \varepsilon)$-approximation algorithm for entrywise
$\ell_p$ low rank approximation, for $1 \leq p &lt; 2$, that is not a column
subset selection algorithm. First, we obtain an algorithm which, given a matrix
$A \in \mathbb{R}^{n \times d}$, returns a rank-$k$ matrix $\hat{A}$ in
$2^{\text{poly}(k/\varepsilon)} + \text{poly}(nd)$ running time such that:
$$\|A - \hat{A}\|_p \leq (1 + \varepsilon) \cdot OPT +
\frac{\varepsilon}{\text{poly}(k)}\|A\|_p$$ where $OPT = \min_{A_k \text{ rank
}k} \|A - A_k\|_p$. Using this algorithm, in the same running time we give an
algorithm which obtains error at most $(1 + \varepsilon) \cdot OPT$ and outputs
a matrix of rank at most $3k$ --- these algorithms significantly improve upon
all previous $(1 + \varepsilon)$- and $O(1)$-approximation algorithms for the
$\ell_p$ low rank approximation problem, which required at least
$n^{\text{poly}(k/\varepsilon)}$ or $n^{\text{poly}(k)}$ running time, and
either required strong bit complexity assumptions (our algorithms do not) or
had bicriteria rank $3k$. Finally, we show hardness results which nearly match
our $2^{\text{poly}(k)} + \text{poly}(nd)$ running time and the above additive
error guarantee.
</p></div>
    </summary>
    <updated>2020-07-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.08972</id>
    <link href="http://arxiv.org/abs/2007.08972" rel="alternate" type="text/html"/>
    <title>On convex holes in $d$-dimensional point sets</title>
    <feedworld_mtime>1595462400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bukh:Boris.html">Boris Bukh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chao:Ting=Wei.html">Ting-Wei Chao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Holzman:Ron.html">Ron Holzman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08972">PDF</a><br/><b>Abstract: </b>Given a finite set $A \subseteq \mathbb{R}^d$, points
$a_1,a_2,\dotsc,a_{\ell} \in A$ form an $\ell$-hole in $A$ if they are the
vertices of a convex polytope which contains no points of $A$ in its interior.
We construct arbitrarily large point sets in general position in $\mathbb{R}^d$
having no holes of size $2^{7d}$ or more. This improves the previously known
upper bound of order $d^{d+o(d)}$ due to Valtr. Our construction uses a certain
type of equidistributed point sets, originating from numerical analysis, known
as $(t,m,s)$-nets or $(t,s)$-sequences.
</p></div>
    </summary>
    <updated>2020-07-23T00:02:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/07/22/three-cccg-videos</id>
    <link href="https://11011110.github.io/blog/2020/07/22/three-cccg-videos.html" rel="alternate" type="text/html"/>
    <title>Three CCCG videos</title>
    <summary>Three new ten-minute research talk videos by me are now up on YouTube as part of the collection of videos to be presented in the 2020 Canadian Conference on Computational Geometry. The pdf conference program has links to all the videos. The conference includes an opportunity to interact with the speakers online, on the August 5–7 dates of the actual conference, with free registration at the CCCG web site.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Three new ten-minute research talk videos by me are now up on YouTube as part of the collection of videos to be presented in the 2020 Canadian Conference on Computational Geometry. The <a href="http://vga.usask.ca/cccg2020/program.pdf">pdf conference program</a> has links to all the videos. The conference includes an opportunity to interact with the speakers online, on the August 5–7 dates of the actual conference, with free registration at <a href="http://vga.usask.ca/cccg2020/">the CCCG web site</a>.</p>

<p>My three videos are on <a href="https://11011110.github.io/blog/2020/07/16/comparing-multi-sport.html">dynamic products of ranks, discussed in my previous post</a>, polyhedra that are difficult to unfold, and mathematics inspired by <a href="https://en.wikipedia.org/wiki/Lusona">the sona drawings of southwest Africa</a>. For your convenience here they are more directly:</p>

<div style="text-align: center;">

<p> </p>

<p> </p>

<p> </p>
</div>

<p>(Apologies for the weird aspect ratio. I should probably aim for a more standard 16x9 format in future.)</p>

<p>I also have a fourth CCCG paper, on unfolding orthogonal polyhedra, with a video produced by Joe O’Rourke. I posted about its preprint version, “<a href="https://11011110.github.io/blog/2019/07/29/zipless-polycube.html">Some polycubes have no edge-unzipping</a>”, a year ago. Here’s Joe’s talk:</p>

<div style="text-align: center;">
<p> </p>

</div></div>
    </content>
    <updated>2020-07-22T14:48:00Z</updated>
    <published>2020-07-22T14:48:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-07-22T21:52:28Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/average-case-dp/</id>
    <link href="https://differentialprivacy.org/average-case-dp/" rel="alternate" type="text/html"/>
    <title>The Pitfalls of Average-Case Differential Privacy</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Differential privacy protects against extremely strong adversaries—even ones who know the entire dataset except for one bit of information about one individual.  Since its inception, people have considered ways to relax the definition to assume a more realistic adversary.  A natural way to do so is to incorporate some distributional assumptions. That is, rather than considering a worst-case dataset, assume the dataset is drawn from some distribution and provide some form of “average-case” or “Bayesian” privacy guarantee with respect to this distribution. This is especially tempting as it is common for statistical analysis to work under distributional assumptions.</p>

<p>In this post and in a planned follow-up post, we will discuss some pitfalls of average-case or Bayesian versions of differential privacy.  To avoid keeping you in suspense:</p>

<ul>
  <li>The average-case assumptions in relaxations of differential privacy are qualitatively different to and much more brittle than the typical assumptions made about how the data is generated.</li>
  <li>Average-case relaxations do not satisfy the strong composition properties that have made differential privacy so successful.</li>
  <li>It is safer to use distributional assumptions in the accuracy analysis instead of the privacy analysis. That is, we can provide average-case utility and worst-case privacy. Recent work has shown that this model can capture most of the advantages of distributional assumptions.</li>
</ul>

<p>We will show some illustrative examples for each of these points, but we will be purposefully vague as to exactly which alternative definition we are considering, as these issues arise in a wide variety of definitions.  Our hope is not to shut down discussion of these relaxations, or to single out specific definitions as flawed.  There are specific concrete applications where average-case differential privacy might be useful, and our goal is to highlight some issues that must be carefully considered in each application.</p>

<h3 id="assumptions-about-nature-vs-assumptions-about-the-adversary">Assumptions about nature vs. assumptions about the adversary?</h3>

<p>In any reasonable definition of privacy, we have to think about whom we are hiding sensitive information from.  This person—“the adversary”—could be a stranger, a close friend, a relative, a corporation we do business with, or the government, and who they are affects what information they have access to and what defenses are appropriate.   How the adversary can access the private system defines the <a href="https://differentialprivacy.org/\trustmodels">trust model</a>. Distributional assumptions correspond to the adversary’s side information.  Our key point is:</p>

<blockquote>
  <p>Assumptions incorporated into the definition of privacy are assumptions about the adversary and these are qualitatively different from assumptions about “nature,” which is the process that generates the data.</p>
</blockquote>

<p>For example, suppose an employer learns that two of its employees have expensive medical conditions. On its own, this information does not identify those employees and this privacy intuition could be formalized via distributional assumptions. But these distributional assumptions will break if the employer later receives some side information. For example, the other healthy employees may voluntarily disclose their medical status or the employer may find out that, before you were hired, that number was only one. (Incidentally, this is an example of a failure of composition, which we will discuss in another post.)</p>

<p>This example illustrates how assumptions about the adversary that might seem reasonable in a vacuum can be invalidated by context. Plus, assumptions about the adversary can be invalidated by <em>future</em> side information, and you can’t retract a privacy leak once it happens the way you can a medical study. So assumptions about the adversary are much less future-proof than assumptions about nature.</p>

<h3 id="all-models-are-wrong-but-some-are-useful">All models are wrong, but some are useful</h3>

<p>One justification for incorporating distributional assumptions into the privacy definition is that the person using the data is often making these assumptions anyway—for example, that the data is i.i.d. Gaussian, or that two variables have some underlying linear relationship to be discovered.  So, if the assumption were false, wouldn’t we already be in trouble?  Not really.</p>

<blockquote>
  <p>It’s important to remember the old saw “all models are wrong, but some are useful.”  Some models have proven themselves useful for statistical purposes, but that does not mean they are useful as a basis for privacy.</p>
</blockquote>

<p>For example, our methods may be robust to the relatively friendly ways that nature deviates from the model, but we can’t trust adversaries to be as friendly.</p>

<p>For a toy example, suppose we model our data as coming from a normal distribution \( N(\mu,\sigma^2) \), but actually the data is collected at two different testing centers, one of which rounds its measurements to the nearest integer and the other of which provides two decimal places of precision.  This rounding makes the model wrong, but won’t significantly affect our estimate of the mean.  However, just looking at the estimate of the mean might reveal that someone in the dataset went to the second testing center, potentially compromising that person’s privacy.</p>

<p>A more natural setting where this issue arises is in dealing with <em>outliers</em> or other extreme examples, which we will discuss in the next section.</p>

<h3 id="privacy-for-outliers">Privacy for outliers</h3>

<p>The usual worst-case definition of differential privacy provides privacy for everyone, including outliers.  Although there are lots of ways to achieve differential privacy, in order to compare definitions, it will help to restrict attention to the basic approach based on calibrating noise to sensitivity:</p>

<p>Suppose we have a private dataset \( x \in \mathcal{X}^n \) containing the data of \( n \) individuals, and some real-valued query \( q : \mathcal{X}^n \to \mathbb{R} \).  The standard way to release an estimate of \( q(x) \) is to compute
\[
M(x) = q(x) + Z \cdot \sup_{\textrm{neighboring}~x’,x”} |q(x’) - q(x”)|
\]
where 
\(
\sup_{\textrm{neighboring}~x’,x”} |q(x’) - q(x”)|
\)
is called the “worst-case sensitivity” of \( q \) and \( Z \) is some noise, commonly drawn from a Laplace or Gaussian distribution.</p>

<p>Unfortunately, the worst-case sensitivity may be large or even infinite for basic statistics of interest, such as the mean \( q(x) = \frac{1}{n} \sum_{i} x_i \) of unbounded real values.  There are a variety of differentially private algorithms for addressing this problem,<sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1">1</a></sup> but that is not what this post is about.  It’s tempting to, instead, try to scale the noise to some notion of “average-case sensitivity,” with the goal of satisfying some average-case version of differential privacy.  For example, suppose the data is drawn from some normal distribution \( N(\mu,\sigma^2) \) and the neighboring datasets \( x', x'' \) are each \( n \) i.i.d. samples from this distribution, but differing on exactly one random sample.  Then the worst-case sensitivity of the mean is infinite:
\[
\sup_{\textrm{neighboring}~x’,x”} |q(x’) - q(x”)| = \infty,
\]
but the average-sensitivity is proportional to \(1/n\):
\[
\mathbb{E}_{\textrm{neighboring}~x’, x”}(|q(x’) - q(x”)|) \approx \frac{\sigma}{n}.
\]
Thus, under an average-case privacy guarantee, we can estimate the mean with very little noise.</p>

<p>But what happens to privacy if this assumption fails, perhaps because of outliers?  Imagine computing the average wealth of a subset of one hundred Amazon employees who test positive for COVID-19, and discovering that it’s over one billion dollars.  Maybe Jeff Bezos isn’t feeling well?<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2">2</a></sup></p>

<p>Yes, this example is a little contrived, since you probably shouldn’t have computed the empirical mean of such skewed data anyway. But, if this fact leaks out, you can’t just go back in time and truncate the data or compute the median instead. Privacy tends to be high-stakes both because of the potential consequences of a breach and the inability to retract or correct a privacy violation after it is discovered.</p>

<p>In the next section we’ll see a slightly more complex example where average-case privacy / average-case sensitivity fails to protect privacy even when the distributional assumptions hold.</p>

<h3 id="example-pairwise-correlations-and-linear-regression">Example: pairwise correlations and linear regression</h3>

<p>Suppose our dataset \( X \) is a matrix \( \{-1,+1\}^{n \times (d+1)} \) where each row \( X_i \) corresponds to one person’s data and each column corresponds to one feature.  For simplicity, let’s suppose our distributional assumption is that the dataset is completely uniform—each bit is sampled independently and uniformly from \( \{-1,+1\} \).  We’ll think of the first \( d \) columns as “features” and the last column as a “secret label.”</p>

<p>First, consider the set of pairwise correlations between each feature and the secret label:
\[
q_j(X) = \sum_{i = 1}^{n} X_{i,j} X_{i,d+1}
\]
for \( j = 1,\dots,d \).  Note that \( q_j(X) \) has mean 0 and variance \( n \) under our distributional model of the data.</p>

<p>Now, suppose we have a weight vector \( w \in \mathbb{R}^{d} \) and want to estimate the weighted average of correlations
\[
q(X) = \sum_{j = 1}^{d} w_j q_j(X) = \sum_{i=1}^{n} \sum_{j=1}^{d} w_{j} X_{i,j} X_{i,d+1}
\]
This statistic may look a little odd, but it’s pretty close to computing the average squared error of the linear predictor \( \hat X_{i,d+1} = \sum_{j=1}^{d} w_j X_{i,j} \) given by the weight vector \( w \), which is a natural thing to estimate.</p>

<p>The worst-case sensitivity of \(q\) is proportional to \( \|w\|_1 \).<br/>
However, it’s not too hard to show that, under our distributional model, the average-case sensitivity is much lower; it is proportional to \( \| w \|_2 \).  Thus, using average-case privacy may allow us to add significantly less noise.</p>

<p>What could go wrong here?  Well, we’ve implicitly assumed that the weights \( w \) are independent of the data \( X \).  That is, the person specifying the weights has no knowledge of the data itself, only its distribution.  Suppose the weights are specified by an adversary who has learned the \( d \) features of the first individual (although there is nothing special about considering the first individual), who sets the weights to \(w = (X_{1,1},\dots,X_{1,d}) \).  Another calculation shows that, in this case, <em>even when our model of the data is exactly correct</em>, the query \( q(X) \) has mean \( d \cdot X_{1,d+1} \) and standard deviation approximately \( \sqrt{nd} \).  Thus, if \( d \gg n \) we can confidently determine the secret label \( X_{1,d+1} \)  of the first individual from the value \( q(X) \).  Moreover, adding noise of standard deviation \( \ll d \) will not significantly affect the adversary’s ability to learn the secret label.  But, earlier, we argued that average-case sensitivity is proportional to \( \|w \|_2 = \sqrt{d} \), so this form of average-case privacy fails to protect a user’s data in this scenario!  Note that adding noise proportional to \( \|w\|_1 = d \) would satisfy (worst-case) differential privacy and would thwart this adversary.</p>

<blockquote>
  <p>What went wrong is that the data satisfied our assumptions, but the adversary’s beliefs about the data did not!</p>
</blockquote>

<p>The set of reasonable distributions to consider for the adversary’s beliefs may look very different from the set of reasonable distributions to consider for your analysis of the data.  You may think that it’s not reasonable for the attacker to choose this weight vector \( w \) containing a lot of prior information about an individual, but assuming that the attacker cannot obtain or specify such a vector is very different from assuming that the data is uniform, and requires its own justification.</p>

<p>Before wrapping up, let’s just make a couple more observations about this example:</p>

<ul>
  <li>This attack is pretty robust. The assumption that the data is uniform with independent features can be relaxed significantly.  It’s also not necessary for the adversary to exactly know all the features of the first user, all we need is for the weights to have correlation \( \gg \sqrt{nd} \) with the features.  For example, if the dataset is genomic data, having the data of a relative might suffice.</li>
  <li>This problem isn’t specific to high-dimensional data with \( d \gg n \).  If we allow more general types of “queries”, then a similar attack is possible when there are only \( d \approx \log n \) features.</li>
  <li>To make this example as crisp as possible, we allowed an adversarial data analyst to specify the weight vector \( w \).  You might think examples like this can’t arise if the algorithm designer specifies all of the queries internally, but ensuring that requires great care (as we’ll see in our upcoming post about composition).</li>
</ul>

<h3 id="conclusion">Conclusion</h3>
<p>As we have discussed, the main issue that arises in average-case or Bayesian versions of differential privacy is that we must make strong assumptions about the adversary. A simple distributional assumption about the data, which may be entirely reasonable for statistical analysis, entails assuming a naïve adversary with essentially no side information, which is not reasonable from a privacy perspective.</p>

<p>In a future post, we will discuss <em>composition</em>, which is a key robustness property and really the secret to differential privacy’s success.  As we’ll see, average-case versions of differential privacy do not enjoy strong composition properties the way worst-case differential privacy does, which makes them much harder to deploy.</p>

<p>Incorporating assumptions about the adversary into the privacy guarantee requires great care; and it is safest to make fewer assumptions, which quickly pushes us towards the worst-case definition of differential privacy. Nevertheless, assumptions about the adversary are often made implicitly and it is worth studying how to make these explicit.</p>

<p>So, is there are role for distributional assumptions in differential privacy? Yes! Although we’ve discussed the pitfalls of making the <em>privacy guarantee</em> contingent on distributional assumptions, none of these pitfalls apply to making the <em>utility guarantee</em> contingent on distributional assumptions, as is normally done in statistical analysis.  In recent years, this combination—worst-case privacy, average-case utility—has been fruitful, and seems to allow many of the benefits that average-case privacy definitions seek to capture.  For example, recent work has shown that worst-case differential privacy permits accurate mean and covariance estimation of unbounded data under natural modeling assumptions <a href="https://arxiv.org/abs/1711.03908" title="Vishesh Karwa, Salil Vadhan. Finite Sample Differentially Private Confidence Intervals. ITCS 2018."><strong>[KV18]</strong></a>, <a href="https://arxiv.org/abs/1805.00216" title="Gautam Kamath, Jerry Li, Vikrant Singhal, Jonathan Ullman. Privately Learning High-Dimensional Distributions. COLT 2019."><strong>[KLSU19]</strong></a>, <a href="https://arxiv.org/abs/1906.02830" title="Mark Bun, Thomas Steinke. Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation. NeurIPS 2019."><strong>[BS19]</strong></a>, <a href="https://arxiv.org/abs/2001.02285" title="Wenxin Du, Canyon Foot, Monica Moniot, Andrew Bray, Adam Groce. Differentially Private Confidence Intervals. 2020."><strong>[DFMBG20]</strong></a>, <a href="https://arxiv.org/abs/2002.09464" title="Gautam Kamath, Vikrant Singhal, Jonathan Ullman.  Private Mean Estimation of Heavy-Tailed Distributions. COLT 2020."><strong>[KSU20]</strong></a>, <a href="https://arxiv.org/abs/2006.06618" title="Sourav Biswas, Yihe Dong, Gautam Kamath, Jonathan Ullman. CoinPress: Practical Private Mean and Covariance Estimation. 2020."><strong>[BDKU20]</strong></a>, but this remains an active area of research.</p>

<hr/>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For example, there are approaches based on various paradigms like Smooth Sensitivity <a href="http://www.cse.psu.edu/~ads22/pubs/NRS07/NRS07-full-draft-v1.pdf" title="Kobbi Nissim, Sofya Raskhodnikova, Adam Smith. Smooth Sensitivity and Sampling in Private Data Analysis. STOC 2007."><strong>[NRS07]</strong></a> <a href="https://arxiv.org/abs/1906.02830" title="Mark Bun, Thomas Steinke. Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation. NeurIPS 2019."><strong>[BS19]</strong></a>, Propose-Test-Release <a href="http://www.stat.cmu.edu/~jinglei/dl09.pdf" title="Cynthia Dwork, Jing Lei. Differential Privacy and Robust Statistics. STOC 2009."><strong>[DL09]</strong></a>, or Truncation/Winsorization <a href="http://www.cse.psu.edu/~ads22/pubs/2011/stoc194-smith.pdf" title="Adam Smith. Privacy-preserving Statistical Estimation with Optimal Convergence Rates. STOC 2011."><strong>[S11]</strong></a> <a href="https://arxiv.org/abs/1711.03908" title="Vishesh Karwa, Salil Vadhan. Finite Sample Differentially Private Confidence Intervals. ITCS 2018."><strong>[KV18]</strong></a> <a href="https://arxiv.org/abs/1805.00216" title="Gautam Kamath, Jerry Li, Vikrant Singhal, Jonathan Ullman. Privately Learning High-Dimensional Distributions. COLT 2019."><strong>[KLSU19]</strong></a> <a href="https://arxiv.org/abs/2002.09464" title="Gautam Kamath, Vikrant Singhal, Jonathan Ullman.  Private Mean Estimation of Heavy-Tailed Distributions. COLT 2020."><strong>[KSU20]</strong></a> to name a few. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>If you are confident that Jeff Bezos or other extremely high-wealth individuals are not in the sample, then you could <em>truncate</em> each sample and compute the mean of the truncated samples.  This would give worst-case privacy, and, if you are correct in your assumption, would not affect the mean. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">↩</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2020-07-22T14:00:00Z</updated>
    <published>2020-07-22T14:00:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-07-23T00:04:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/110</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/110" rel="alternate" type="text/html"/>
    <title>TR20-110 |  Capacity Lower Bounds via Productization | 

	Leonid Gurvits, 

	Jonathan Leake</title>
    <summary>The purpose of this note is to state and prove a lower bound on the capacity of a real stable polynomial $p(x)$ which is based only on its value and gradient at $x=1$. This result implies a sharp improvement to a similar inequality proved by Linial-Samorodnitsky-Wigderson in 2000. Such inequalities have played an important role in the recent work on operator scaling and its generalizations and applications.</summary>
    <updated>2020-07-22T04:32:00Z</updated>
    <published>2020-07-22T04:32:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-23T03:20:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10976</id>
    <link href="http://arxiv.org/abs/2007.10976" rel="alternate" type="text/html"/>
    <title>Interactive Inference under Information Constraints</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Acharya:Jayadev.html">Jayadev Acharya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Canonne:Cl=eacute=ment_L=.html">Clément L. Canonne</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yuhan.html">Yuhan Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Ziteng.html">Ziteng Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tyagi:Himanshu.html">Himanshu Tyagi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10976">PDF</a><br/><b>Abstract: </b>We consider distributed inference using sequentially interactive protocols.
We obtain lower bounds on the minimax sample complexity of interactive
protocols under local information constraints, a broad family of resource
constraints which captures communication constraints, local differential
privacy, and noisy binary queries as special cases. We focus on the inference
tasks of learning (density estimation) and identity testing (goodness-of-fit)
for discrete distributions under total variation distance, and establish
general lower bounds that take into account the local constraints modeled as a
channel family.
</p>
<p>Our main technical contribution is an approach to handle the correlation that
builds due to interactivity and quantifies how effectively one can exploit this
correlation in spite of the local constraints. Using this, we fill gaps in some
prior works and characterize the optimal sample complexity of learning and
testing discrete distributions under total variation distance, for both
communication and local differential privacy constraints. Prior to our work,
this was known only for the problem of testing under local privacy constraints
(Amin, Joseph, and Mao (2020); Berrett and Butucea (2020)). Our results show
that interactivity does not help for learning or testing under these two
constraints. Finally, we provide the first instance of a natural family of
"leaky query" local constraints under which interactive protocols strictly
outperform the noninteractive ones for distribution testing.
</p></div>
    </summary>
    <updated>2020-07-22T23:52:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10924</id>
    <link href="http://arxiv.org/abs/2007.10924" rel="alternate" type="text/html"/>
    <title>Partial Boolean functions with exact quantum 1-query complexity</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Guoliang.html">Guoliang Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qiu:Daowen.html">Daowen Qiu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10924">PDF</a><br/><b>Abstract: </b>We provide two sufficient and necessary conditions to characterize any
$n$-bit partial Boolean function with exact quantum 1-query complexity. Using
the first characterization, we present all $n$-bit partial Boolean functions
that depend on $n$ bits and have exact quantum 1-query complexity. Due to the
second characterization, we construct a function $F$ that maps any $n$-bit
partial Boolean function to some integer, and if an $n$-bit partial Boolean
function $f$ depends on $k$ bits and has exact quantum 1-query complexity, then
$F(f)$ is non-positive. In addition, we show that the number of all $n$-bit
partial Boolean functions that depend on $k$ bits and have exact quantum
1-query complexity is not bigger than $n^{2}2^{2^{n-1}(1+2^{2-k})+2n^{2}}$ for
all $n\geq 3$ and $k\geq 2$.
</p></div>
    </summary>
    <updated>2020-07-22T23:20:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10863</id>
    <link href="http://arxiv.org/abs/2007.10863" rel="alternate" type="text/html"/>
    <title>Outer approximations of core points for integer programming</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bremner:David.html">David Bremner</a>, Naghmeh Shahverdi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10863">PDF</a><br/><b>Abstract: </b>For several decades the dominant techniques for integer linear programming
have been branching and cutting planes. Recently, several authors have
developed core point methods for solving symmetric integer linear programs
(ILPs). An integer point is called a core point if its orbit polytope is
lattice-free. It has been shown that for symmetric ILPs, optimizing over the
set of core points gives the same answer as considering the entire space.
Existing core point techniques rely on the number of core points (or
equivalence classes) being finite, which requires special symmetry groups. In
this paper we develop some new methods for solving symmetric ILPs (based on
outer approximations of core points) that do not depend on finiteness but are
more efficient if the group has large disjoint cycles in its set of generators.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10857</id>
    <link href="http://arxiv.org/abs/2007.10857" rel="alternate" type="text/html"/>
    <title>Smoothed Complexity of 2-player Nash Equilibria</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boodaghians:Shant.html">Shant Boodaghians</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brakensiek:Joshua.html">Joshua Brakensiek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Samuel_B=.html">Samuel B. Hopkins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinstein:Aviad.html">Aviad Rubinstein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10857">PDF</a><br/><b>Abstract: </b>We prove that computing a Nash equilibrium of a two-player ($n \times n$)
game with payoffs in $[-1,1]$ is PPAD-hard (under randomized reductions) even
in the smoothed analysis setting, smoothing with noise of constant magnitude.
This gives a strong negative answer to conjectures of Spielman and Teng [ST06]
and Cheng, Deng, and Teng [CDT09].
</p>
<p>In contrast to prior work proving PPAD-hardness after smoothing by noise of
magnitude $1/\operatorname{poly}(n)$ [CDT09], our smoothed complexity result is
not proved via hardness of approximation for Nash equilibria. This is by
necessity, since Nash equilibria can be approximated to constant error in
quasi-polynomial time [LMM03]. Our results therefore separate smoothed
complexity and hardness of approximation for Nash equilibria in two-player
games.
</p>
<p>The key ingredient in our reduction is the use of a random zero-sum game as a
gadget to produce two-player games which remain hard even after smoothing. Our
analysis crucially shows that all Nash equilibria of random zero-sum games are
far from pure (with high probability), and that this remains true even after
smoothing.
</p></div>
    </summary>
    <updated>2020-07-22T23:25:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10829</id>
    <link href="http://arxiv.org/abs/2007.10829" rel="alternate" type="text/html"/>
    <title>On Algorithms for Solving the Rubik's Cube</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ahmad Kaleem, Ahsan Kaleem <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10829">PDF</a><br/><b>Abstract: </b>In this paper, we present a novel algorithm and its three variations for
solving the Rubik's cube more efficiently. This algorithm can be used to solve
the complete $n \times n \times n$ cube in $O(\frac{n^2}{\log n})$ moves. This
algorithm can also be useful in certain cases for speedcubers. We will prove
that our algorithm always works and then perform a basic analysis on the
algorithm to determine its algorithmic complexity of $O(n^2)$. Finally, we
further optimize this complexity to $O(\frac{n^2}{\log n})$.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10824</id>
    <link href="http://arxiv.org/abs/2007.10824" rel="alternate" type="text/html"/>
    <title>Parameter estimation for Gibbs distributions</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Harris:David_G=.html">David G. Harris</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolmogorov:Vladimir.html">Vladimir Kolmogorov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10824">PDF</a><br/><b>Abstract: </b>We consider \emph{Gibbs distributions}, which are families of probability
distributions over a discrete space $\Omega$ with probability mass function of
the form $\mu^\Omega_\beta(x) \propto e^{\beta H(x)}$ for $\beta$ in an
interval $[\beta_{\min}, \beta_{\max}]$ and $H(x) \in \{0 \} \cup [1, n]$. The
\emph{partition function} is the normalization factor
$Z(\beta)=\sum_{x\in\Omega}e^{\beta H(x)}$.
</p>
<p>Two important parameters of these distributions are the partition ratio $q =
\log \tfrac{Z(\beta_{\max})}{Z(\beta_{\min})}$ and the counts $c_x =
|H^{-1}(x)|$ for each value $x$. These are correlated with system parameters in
a number of physical applications and sampling algorithms. Our first main
result is to estimate the values $c_x$ using roughly $\tilde O(
\frac{q}{\varepsilon^2})$ samples for general Gibbs distributions and $\tilde
O( \frac{n^2}{\varepsilon^2} )$ samples for integer-valued distributions
(ignoring some second-order terms and parameters), and we show this is optimal
up to logarithmic factors. We illustrate with improved algorithms for counting
connected subgraphs and perfect matchings in a graph.
</p>
<p>As a key subroutine, we estimate the partition function $Z$ using $\tilde
O(\frac{q}{\varepsilon^2})$ samples for general Gibbs distributions and $\tilde
O(\frac{n^2}{\varepsilon^2})$ samples for integer-valued distributions. We
construct a data structure capable of estimating $Z(\beta)$ for \emph{all}
values $\beta$, without further samples. This improves over a prior algorithm
of Kolmogorov (2018) which computes the single point estimate $Z(\beta_{\max})$
using $\tilde O(\frac{q}{\varepsilon^2})$ samples. We show matching lower
bounds, demonstrating that this complexity is optimal as a function of $n$ and
$q$ up to logarithmic terms.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10805</id>
    <link href="http://arxiv.org/abs/2007.10805" rel="alternate" type="text/html"/>
    <title>Formally Verified Trades in Financial Markets</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sarswat:Suneel.html">Suneel Sarswat</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Abhishek_Kr.html">Abhishek Kr Singh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10805">PDF</a><br/><b>Abstract: </b>We introduce a formal framework for analyzing trades in financial markets.
These days, all big exchanges use computer algorithms to match buy and sell
requests and these algorithms must abide by certain regulatory guidelines. For
example, market regulators enforce that a matching produced by exchanges should
be fair, uniform and individual rational. To verify these properties of trades,
we first formally define these notions in a theorem prover and then develop
many important results about matching demand and supply. Finally, we use this
framework to verify properties of two important classes of double sided auction
mechanisms. All the definitions and results presented in this paper are
completely formalized in the Coq proof assistant without adding any additional
axioms to it.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10790</id>
    <link href="http://arxiv.org/abs/2007.10790" rel="alternate" type="text/html"/>
    <title>Breaking the $2^n$ barrier for 5-coloring and 6-coloring</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zamir:Or.html">Or Zamir</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10790">PDF</a><br/><b>Abstract: </b>The coloring problem (i.e., computing the chromatic number of a graph) can be
solved in $O^*(2^n)$ time, as shown by Bj\"orklund, Husfeldt and Koivisto in
2009. For $k=3,4$, better algorithms are known for the $k$-coloring problem.
$3$-coloring can be solved in $O(1.4^n)$ time (Beigel and Eppstein, 2005) and
$4$-coloring can be solved in $O(1.8^n)$ time (Fomin, Gaspers and Saurabh,
2007). Surprisingly, for $k&gt;4$ no improvements over the general $O^*(2^n)$ are
known. We show that both $5$-coloring and $6$-coloring can also be solved in
$O\left(\left(2-\varepsilon\right)^n\right)$ time for some $\varepsilon&gt;0$.
Moreover, we obtain an exponential improvement for $k$-coloring for any
constant $k$ for a very large family of graphs. In particular, for any
constants $k,\Delta,\alpha&gt;0$, $k$-coloring for graphs with at least
$\alpha\cdot n$ vertices of degree at most $\Delta$ can be solved in
$O\left(\left(2-\varepsilon\right)^n\right)$ time, for some $\varepsilon =
\varepsilon_{k,\Delta,\alpha} &gt; 0$. As a consequence, for any constant $k$ we
can solve $k$-coloring exponentially faster than $O^*(2^n)$ for sparse graphs.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10673</id>
    <link href="http://arxiv.org/abs/2007.10673" rel="alternate" type="text/html"/>
    <title>Quantum and Classical Hybrid Generations for Classical Correlations</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Xiaodie Lin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei:Zhaohui.html">Zhaohui Wei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yao:Penghui.html">Penghui Yao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10673">PDF</a><br/><b>Abstract: </b>We consider two-stage hybrid protocols that combine quantum resource and
classical resource to generate classical correlations shared by two separated
players. Our motivation is twofold. First, in the near future the scale of
quantum information processing is quite limited, and when quantum resource
available is not sufficient for certain tasks, a possible way to strengthen the
capability of quantum schemes is introducing extra classical resource. We
analyze the mathematical structures of these hybrid protocols, and characterize
the relation between the amount of quantum resource and classical resource
needed. Second, a fundamental open problem in communication complexity theory
is to describe the advantages of sharing prior quantum entanglement over
sharing prior randomness, which is still widely open. It turns out that our
quantum and classical hybrid protocols provide new insight into this important
problem.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10658</id>
    <link href="http://arxiv.org/abs/2007.10658" rel="alternate" type="text/html"/>
    <title>A family of non-periodic tilings of the plane by right golden triangles</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Nikolay Vereshchagin <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10658">PDF</a><br/><b>Abstract: </b>We consider tilings of the plane by two prototiles which are right triangles.
They are called the small and the large tiles. The small tile is similar to the
large tile with some similarity coefficient $\psi$. The large tile can be cut
into two pieces so that one piece is a small tile and the other one is similar
to the small tile with the same similarity coefficient $\psi$. Using this cut
we define in a standard way the substitution scheme, in which the large tile is
replaced by a large and a small tile and the small tile is replaced by a large
tile. To every substitution of this kind, there corresponds a family of the
so-called substitution tilings of the plane in the sense of [C.
Goodman-Strauss, Matching Rules and Substitution Tilings, Annals of Mathematics
147 (1998) 181-223]. All tilings in this family are non-periodic. It was shown
in the paper [N. Vereshchagin. Aperiodic Tilings by Right Triangles. In: Proc.
of DCFS 2014, LNCS vol. 8614 (2014) 29--41] that this family of substitution
tilings is not an SFT. This means that looking at a given tiling trough a
bounded window, we cannot determine whether that tiling belongs to the family
or not, however large the size of the window is.
</p>
<p>In the present paper, we prove that this family of substitution tilings is
sofic. This means that we can color the prototiles ina finite number of colors
and define some local rules for colored prototiles so that the following holds.
For any tiling from the family, we can color its tiles so that the resulting
tiling (by colored tiles) satisfies local rules. And conversely, for any tiling
of the plane satisfying the local rules, by removing colors we obtain a tiling
from the family. Besides, the considered substitution can be generalized to
colored tiles so that the family of substitution tilings for the resulting
substitution coincides with the family of tilings satisfying our local rules.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10622</id>
    <link href="http://arxiv.org/abs/2007.10622" rel="alternate" type="text/html"/>
    <title>Online Discrepancy Minimization for Stochastic Arrivals</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bansal:Nikhil.html">Nikhil Bansal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Haotian.html">Haotian Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meka:Raghu.html">Raghu Meka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sinha:Makrand.html">Makrand Sinha</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10622">PDF</a><br/><b>Abstract: </b>In the stochastic online vector balancing problem, vectors
$v_1,v_2,\ldots,v_T$ chosen independently from an arbitrary distribution in
$\mathbb{R}^n$ arrive one-by-one and must be immediately given a $\pm$ sign.
The goal is to keep the norm of the discrepancy vector, i.e., the signed
prefix-sum, as small as possible for a given target norm.
</p>
<p>We consider some of the most well-known problems in discrepancy theory in the
above online stochastic setting, and give algorithms that match the known
offline bounds up to $\mathsf{polylog}(nT)$ factors. This substantially
generalizes and improves upon the previous results of Bansal, Jiang, Singla,
and Sinha (STOC' 20). In particular, for the Koml\'{o}s problem where
$\|v_t\|_2\leq 1$ for each $t$, our algorithm achieves $\tilde{O}(1)$
discrepancy with high probability, improving upon the previous
$\tilde{O}(n^{3/2})$ bound. For Tusn\'{a}dy's problem of minimizing the
discrepancy of axis-aligned boxes, we obtain an $O(\log^{d+4} T)$ bound for
arbitrary distribution over points. Previous techniques only worked for product
distributions and gave a weaker $O(\log^{2d+1} T)$ bound. We also consider the
Banaszczyk setting, where given a symmetric convex body $K$ with Gaussian
measure at least $1/2$, our algorithm achieves $\tilde{O}(1)$ discrepancy with
respect to the norm given by $K$ for input distributions with sub-exponential
tails.
</p>
<p>Our key idea is to introduce a potential that also enforces constraints on
how the discrepancy vector evolves, allowing us to maintain certain
anti-concentration properties. For the Banaszczyk setting, we further enhance
this potential by combining it with ideas from generic chaining. Finally, we
also extend these results to the setting of online multi-color discrepancy.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10592</id>
    <link href="http://arxiv.org/abs/2007.10592" rel="alternate" type="text/html"/>
    <title>Explicit two-deletion codes with redundancy matching the existential bound</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guruswami:Venkatesan.html">Venkatesan Guruswami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/H=aring=stad:Johan.html">Johan Håstad</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10592">PDF</a><br/><b>Abstract: </b>We give an explicit construction of length-$n$ binary codes capable of
correcting the deletion of two bits that have size $2^n/n^{4+o(1)}$. This
matches up to lower order terms the existential result, based on an inefficient
greedy choice of codewords, that guarantees such codes of size
$\Omega(2^n/n^4)$. Our construction is based on augmenting the classic
Varshamov-Tenengolts construction of single deletion codes with additional
check equations. We also give an explicit construction of binary codes of size
$\Omega(2^n/n^{3+o(1)})$ that can be list decoded from two deletions using
lists of size two. Previously, even the existence of such codes was not clear.
</p></div>
    </summary>
    <updated>2020-07-22T23:33:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10545</id>
    <link href="http://arxiv.org/abs/2007.10545" rel="alternate" type="text/html"/>
    <title>Online Carpooling using Expander Decompositions</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krishnaswamy:Ravishankar.html">Ravishankar Krishnaswamy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Amit.html">Amit Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10545">PDF</a><br/><b>Abstract: </b>We consider the online carpooling problem: given $n$ vertices, a sequence of
edges arrive over time. When an edge $e_t = (u_t, v_t)$ arrives at time step
$t$, the algorithm must orient the edge either as $v_t \rightarrow u_t$ or $u_t
\rightarrow v_t$, with the objective of minimizing the maximum discrepancy of
any vertex, i.e., the absolute difference between its in-degree and out-degree.
Edges correspond to pairs of persons wanting to ride together, and orienting
denotes designating the driver. The discrepancy objective then corresponds to
every person driving close to their fair share of rides they participate in.
</p>
<p>In this paper, we design efficient algorithms which can maintain
polylog$(n,T)$ maximum discrepancy (w.h.p) over any sequence of $T$ arrivals,
when the arriving edges are sampled independently and uniformly from any given
graph $G$. This provides the first polylogarithmic bounds for the online
(stochastic) carpooling problem. Prior to this work, the best known bounds were
$O(\sqrt{n \log n})$-discrepancy for any adversarial sequence of arrivals, or
$O(\log\!\log n)$-discrepancy bounds for the stochastic arrivals when $G$ is
the complete graph.
</p>
<p>The technical crux of our paper is in showing that the simple greedy
algorithm, which has provably good discrepancy bounds when the arriving edges
are drawn uniformly at random from the complete graph, also has polylog
discrepancy when $G$ is an expander graph. We then combine this with known
expander-decomposition results to design our overall algorithm.
</p></div>
    </summary>
    <updated>2020-07-22T23:26:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10537</id>
    <link href="http://arxiv.org/abs/2007.10537" rel="alternate" type="text/html"/>
    <title>Maximum Weight Disjoint Paths in Outerplanar Graphs via Single-Tree Cut Approximators</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Naves:Guyslain.html">Guyslain Naves</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shepherd:Bruce.html">Bruce Shepherd</a>, Henry Xia <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10537">PDF</a><br/><b>Abstract: </b>Since 2000 there has been a steady stream of advances for the maximum weight
disjoint paths problem. Achieving tractable results has usually required
focusing on relaxations such as: (i) to allow some bounded edge congestion in
solutions, (ii) to only consider the unit weight (cardinality) setting, (iii)
to only require fractional routability of the selected demands (the
all-or-nothing flow setting). For the general form (no congestion, general
weights, integral routing) even the case of unit capacity trees which are stars
generalizes the maximum matching problem for which Edmonds provided an exact
algorithm. For general capacitated trees, Garg, Vazirani, Yannakakis showed the
problem is APX-Hard and Chekuri, Mydlarz, Shepherd provided a
$4$-approximation. This is essentially the only setting where a constant
approximation is known for the general form of \textsc{edp}. We extend their
result by giving a constant-factor approximation algorithm for general-form
\textsc{edp} in outerplanar graphs. A key component for the algorithm is to
find a \textsc{single-tree} $O(1)$ cut approximator for outerplanar graphs.
Previously $O(1)$ cut approximators were only known via distributions on trees
and these were based implicitly on the results of Gupta, Newman, Rabinovich and
Sinclair for distance tree embeddings combined with results of Anderson and
Feige.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10470</id>
    <link href="http://arxiv.org/abs/2007.10470" rel="alternate" type="text/html"/>
    <title>tight approximations for modular and submodular optimization with $d$-resource multiple knapsack constraints</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fairstein:Yaron.html">Yaron Fairstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kulik:Ariel.html">Ariel Kulik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shachnai:Hadas.html">Hadas Shachnai</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10470">PDF</a><br/><b>Abstract: </b>A multiple knapsack constraint over a set of items is defined by a set of
bins of arbitrary capacities, and a weight for each of the items. An assignment
for the constraint is an allocation of subsets of items to the bins, such that
the total weight of items assigned to each bin is bounded by the bin capacity.
We study modular (linear) and submodular maximization problems subject to a
constant number of (i.e., $d$-resource) multiple knapsack constraints, in which
a solution is a subset of items, along with an assignment of the selected items
for each of the $d$ multiple knapsack constraints.
</p>
<p>Our results include a {\em polynomial time approximation scheme} for modular
maximization with a constant number of multiple knapsack constraints and a
matroid constraint, thus generalizing the best known results for the classic
{\em multiple knapsack problem} as well as {\em d-dimensional knapsack}, for
any $d \geq 2$. We further obtain a tight $(1-e^{-1}-\epsilon)$-approximation
for monotone submodular optimization subject to a constant number of multiple
knapsack constraints and a matroid constraint, and a
$(0.385-\epsilon)$-approximation for non-monotone submodular optimization
subject to a constant number of multiple knapsack constraints. At the heart of
our algorithms lies a novel representation of a multiple knapsack constraint as
a polytope. We consider this key tool as a main technical contribution of this
paper.
</p></div>
    </summary>
    <updated>2020-07-22T23:26:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10331</id>
    <link href="http://arxiv.org/abs/2007.10331" rel="alternate" type="text/html"/>
    <title>Evolution toward a Nash equilibrium</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avramopoulos:Ioannis.html">Ioannis Avramopoulos</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10331">PDF</a><br/><b>Abstract: </b>In this paper, we study the dynamic behavior of Hedge, a well-known algorithm
in theoretical machine learning and algorithmic game theory. The empirical
average (arithmetic mean) of the iterates Hedge generates is known to converge
to a minimax equilibrium in zero-sum games. We generalize that result to show
convergence of the empirical average to Nash equilibrium in symmetric bimatrix
games (that is bimatrix games where the payoff matrix of each player is the
transpose of that of the other) in the sense that every limit point of the
sequence of averages is an $\epsilon$-approximate symmetric equilibrium
strategy for any desirable $\epsilon$. Our analysis gives rise to a symmetric
equilibrium fully polynomial-time approximation scheme, implying P = PPAD.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4912</id>
    <link href="https://www.scottaaronson.com/blog/?p=4912" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4912#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4912" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Is this blog obsolete?</title>
    <summary xml:lang="en-US">Longtime friend-of-the-blog Michael Nielsen, formerly a quantum computing theorist and now an independent thinker about science and AI and learning, posted the following exchange with the astounding GPT-3 text generator / chatbot: Q: Is it possible to use quantum computers to solve NP-complete problems in polynomial time?A: It is not possible to use quantum computers […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Longtime friend-of-the-blog <a href="http://michaelnielsen.org/">Michael Nielsen</a>, formerly a quantum computing theorist and now an independent thinker about science and AI and learning, posted the following <a href="https://twitter.com/michael_nielsen/status/1284988349418098688">exchange</a> with the astounding <a href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/">GPT-3</a> text generator / chatbot:</p>



<blockquote class="wp-block-quote"><p>Q: Is it possible to use quantum computers to solve NP-complete problems in polynomial time?<br/>A: It is not possible to use quantum computers to solve NP-complete problems in polynomial time.</p><p>Q: Why not?<br/>A: Because NP-complete problems are not in BQP.</p><p>Q: You’ve just restated your earlier assertion!  What I’d like is to understand the underlying reason why not!<br/>A: I’m sorry, but I don’t have time to explain the underlying reason why not.</p><p>Q: No offence, but this seems really bogus.  I don’t believe you understand the reason why!<br/>A: I’m sorry, but I don’t have time to explain the underlying reason why not.</p></blockquote>



<p>(And then there are a few more rounds of the AI not having time to “explain the underlying reason why not.”)</p>



<p>This is, like, 80% of the way to replacing <em>Shtetl-Optimized</em>!</p>



<p>For much more discussion of GPT-3 and its implications, and samples of its output, see for example the <a href="https://www.reddit.com/r/slatestarcodex/">SSC subreddit</a>.  At the moment, as far as I can tell, the closest a person off the street can easily come to experimenting with GPT-3 themselves is using a website called <a href="https://play.aidungeon.io/">AI Dungeon</a>.</p>



<p>And yes, as many have already remarked, this is clearly the <a href="https://en.wikipedia.org/wiki/Altair_8800">MITS Altair</a> of text-generating AI, an amusing toy that’s also the start of something that will change the world.</p></div>
    </content>
    <updated>2020-07-21T00:16:18Z</updated>
    <published>2020-07-21T00:16:18Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-07-21T00:16:18Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/stoc2020/</id>
    <link href="https://differentialprivacy.org/stoc2020/" rel="alternate" type="text/html"/>
    <title>Conference Digest - STOC 2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="http://acm-stoc.org/stoc2020/">STOC 2020</a> was recently held online, as one of the first major theory conferences during the COVID-19 era.
It featured four papers on differential privacy, which we list and link below.
Each one is accompanied by a video from the conference, as well as a longer video if available.
Please let us know if we missed any papers on differential privacy, either in the comments below or by email.</p>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1911.08339">The Power of Factorization Mechanisms in Local and Central Differential Privacy</a> (<a href="https://www.youtube.com/watch?v=hSenRTxhZhM">video</a>)<br/>
<a href="https://dblp.uni-trier.de/pers/hd/e/Edmonds:Alexander">Alexander Edmonds</a>, <a href="http://www.cs.toronto.edu/~anikolov/">Aleksandar Nikolov</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.04763">Private Stochastic Convex Optimization: Optimal Rates in Linear Time</a> (<a href="https://www.youtube.com/watch?v=Tlc-z-MFAmM">video</a>)<br/>
<a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="https://tomerkoren.github.io/">Tomer Koren</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.04014">Interaction is necessary for distributed learning with privacy or communication constraints</a> (<a href="https://www.youtube.com/watch?v=AWgzaFOU_HM">video</a>)<br/>
<a href="https://yuvaldagan.wordpress.com/">Yuval Dagan</a>, <a href="http://vtaly.net/">Vitaly Feldman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1906.05271">Does Learning Require Memorization? A Short Tale about a Long Tail</a> (<a href="https://www.youtube.com/watch?v=sV59uoWJRnk">video</a>, <a href="https://www.youtube.com/watch?v=Fp7cgHRl8Yc">longer video</a>)<br/>
<a href="http://vtaly.net/">Vitaly Feldman</a></p>
  </li>
</ul></div>
    </summary>
    <updated>2020-07-20T14:00:00Z</updated>
    <published>2020-07-20T14:00:00Z</published>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-07-23T00:04:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/transfer-learning/</id>
    <link href="https://gradientscience.org/transfer-learning/" rel="alternate" type="text/html"/>
    <title>Transfer Learning with Adversarially Robust Models</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/2007.08489" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/Microsoft/robust-models-transfer" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Models and Code
</a>
<br/></p>

<p><i>In our <a href="https://arxiv.org/abs/2007.08489">latest paper</a>, in collaboration with <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>, we explore adversarial
robustness as an avenue for training computer vision models with more transferrable
features. We find that robust models outperform their standard counterparts on
a variety of transfer learning tasks.</i></p>

<h2 id="what-is-transfer-learning">What is transfer learning?</h2>

<p>Transfer learning is a paradigm where one leverages information
from a “source” task to better solve another “target” task. Particularly when there is little training data or compute available for solving the target
task, transfer learning provides a simple and efficient way to obtain performant
machine learning models.</p>

<p>Transfer learning has already proven its utility in many ML contexts. In natural language processing, for example, one can leverage language models pre-trained on large
text corpora to beat state-of-the-art performance on
tasks like query answering, entity recognition or part-of-speech classification.</p>

<p>In our work we focus on computer vision; in this context, a standard—and
remarkably successful—transfer learning pipeline is “ImageNet pre-training.”
This pipeline starts with a deep neural network trained on the <a href="http://image-net.org">ImageNet-1K</a>
dataset, and then refines this pre-trained model for a target task. The target task can range
from classification of smaller datasets (e.g., <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR</a>) to more complex
tasks like object detection (e.g., <a href="http://host.robots.ox.ac.uk/pascal/VOC/">VOC</a>).</p>

<p>Although there are many ways in which one can refine a pre-trained model, we
will restrict our attention to the two most popular methods:</p>

<ul>
  <li><strong>Fixed-feature</strong>: In fixed-feature transfer learning, we replace the final
(linear) layer of the neural network with a new layer that has the correct
number of outputs for the target task. Then, keeping the rest of the layers
<em>fixed</em>, we train the newly replaced layer on the target task.</li>
  <li><strong>Full-network</strong>: In full-network transfer learning, we also replace the last
layer but do not freeze any layers afterwards. Instead, we use the pre-trained
network
as a sort of “initialization,” and continue training <em>all</em> the layers on the
target task.</li>
</ul>

<p>When at least a moderate amount of data is available, full-network transfer
learning typically outperforms the fixed-feature strategy.</p>

<h2 id="how-can-we-improve-transfer-learning">How can we improve transfer learning?</h2>

<p>Although we don’t have a comprehensive understanding of what makes transfer
learning algorithms tick, there has been a long line of work focused on identifying 
factors that improve (or worsen) performance (examples include
<a href="https://arxiv.org/abs/1406.5774">[1]</a>,
<a href="https://arxiv.org/abs/1608.08614">[2]</a>,
<a href="https://arxiv.org/abs/1805.08974">[3]</a>,
<a href="https://arxiv.org/abs/1804.08328">[4]</a>,
<a href="https://arxiv.org/abs/1411.1792">[5]</a>).</p>

<p>By design, the pre-trained ImageNet model itself plays a major role here:
indeed, a recent study by <a href="https://arxiv.org/abs/1805.08974">Kornblith, Shlens, and Le</a> finds that
pre-trained models which achieve a higher ImageNet accuracy also perform better when
transferred to downstream classification tasks, with a tight linear
correspondence between ImageNet accuracy and the accuracy on the target task:</p>

<p><img alt="A scatter plot of ImageNet accuracy versus downstream     transfer accuracy showing the linear relation." class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/ksl.png"/></p>
<div class="footnote">
Reproduced from <a href="https://arxiv.org/abs/1805.08974">[KSL19]</a>. 
Each dot is a pre-trained model whose $x$ coordinate is given by its 
ImageNet accuracy and $y$ coordinate is given by its downstream 
accuracy on the target task (after the corresponding refinement on that task).
</div>

<p>But is improving ImageNet accuracy of the pre-trained model the <em>only</em> way to improve transfer learning performance?</p>

<p>After all, we want to obtain models that have learned broadly applicable features from the
source dataset. ImageNet accuracy likely correlates with the quality of
features that a model has learned, but may not fully describe the downstream
utility of these features.
Ultimately, the nature of learned features stems from the <em>priors</em> placed on
them during training. For example, there have been studies of the (sometimes
implicit) priors imposed by architectural components (e.g., <a href="https://dmitryulyanov.github.io/deep_image_prior">convolutional layers</a>),
<a href="https://www.tandfonline.com/doi/abs/10.1198/10618600152418584">data</a>
<a href="https://arxiv.org/abs/1911.09071">augmentation</a>, 
<a href="https://arxiv.org/abs/1811.00401">loss functions</a> and even
<a href="https://stats385.github.io/assets/lectures/Stanford_Donoho_class_Nov_19.pdf">gradient descent</a> on neural network training.</p>

<p>In <a href="https://arxiv.org/abs/2007.08489">our paper</a>, we study another prior: <em>adversarial robustness</em>.
Adversarial robustness—a rather frequent subject on this blog—refers to
model’s invariance to small (often imperceptible) perturbations of natural
inputs, called <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.</p>

<p>Standard neural networks (i.e., trained with the goal of maximizing
accuracy) are extremely vulnerable to such adversarial examples. For example,
with just a tiny perturbation to the pig image below, a pre-trained ImageNet
classifier will predict it as an “airliner” with 99% confidence:</p>

<p><img alt="An adversarial example: a pig on the left which is imperceptibly perturbed to be classified as an airliner on the right." src="https://gradientscience.org/images/piggie.png"/></p>
<div class="footnote">
A "pigs-can-fly" adversarial example: The "pig" image on the left is correctly classified by a standard ML model, but its imperceptibly perturbed counterpart on the right is classified as an "airliner" with 99% confidence.
</div>

<p>Adversarial robustness is thus typically induced at training time by replacing
the standard loss minimization objective with a <em>robust optimization</em> objective
(see our <a href="https://gradientscience.org/robust_opt_pt1">post on robust optimization</a> for more background):</p>



<p>The above objective trains models to be robust to image perturbations that are
small in (pixel-wise) $\ell_2$ (Euclidean) normIn reality, an $\ell_2$ ball doesn't perfectly capture the
set of imperceptible perturbations we want models to be robust to—but robustness with respect to this fairly rudimentary notion of perturbations turns out to be already non-trivial and very helpful.. 
The parameter $\varepsilon$ is a hyperparameter
governing the intended degree of invariance of the resulting models to the
corresponding perturbations. Setting 
$\varepsilon = 0$ corresponds to standard training, and increasing $\varepsilon$
asks the model to be robust to increasingly large perturbations.
In short, the objective asks the model to minimize risk on not only the 
training datapoints but also the entire radius-$\varepsilon$
neighbourhood around them.</p>

<p><em>[A quick plug: Our <a href="https://github.com/MadryLab/robustness"><code class="language-plaintext highlighter-rouge">robustness</code> Python library</a>, used for the code release of this paper, enables one to easily train and manipulate both standard and adversarially robust models.]</em></p>

<p>Although adversarial robustness has been initially studied solely through the lens of machine learning security, a line
of recent work (including some that’s been <a href="https://gradientscience.org/adv">previously</a> 
<a href="https://gradientscience.org/robust_apps">covered</a> on this blog) has begun to study
adversarially robust models in their own right, framing adversarial robustness
as a prior that forces models to learn features that are locally stable.
These works have found that on the one hand, adversarially robust models tend
to attain lower accuracy than their standardly-trained
counterparts.</p>

<p>On the other hand, recent work suggests that the feature
representations of robust models carry several advantages over those of
standard models, such as <a href="https://arxiv.org/abs/1805.12152">better-behaved</a>
<a href="https://arxiv.org/abs/1905.09797">gradients</a>, <a href="https://arxiv.org/abs/1910.08640">representation
invertibility</a>, and more <a href="https://arxiv.org/abs/2005.10190">specialized
features</a>.
We’ve actually discussed some of these observations in earlier posts on this
blog—see, e.g., our posts about 
<a href="https://gradientscience.org/robust_reps">representation learning</a> and 
<a href="https://gradientscience.org/robust_apps">image synthesis</a>.</p>

<p>These desirable properties
might suggest that robust neural networks are learning better feature
representations than standard networks, which could improve transfer
performance.</p>

<h3 id="adversarial-robustness-and-transfer-learning">Adversarial robustness and transfer learning</h3>

<p>So in summary, we have standard models with high accuracy on the source task but
little (or no) robustness; and we have adversarially robust models, which are
worse in terms of ImageNet accuracy, but have the “nice”
representational properties identified and discussed by prior works. Which
models are better for transfer learning?</p>

<p>To answer this question, we trained and examined a large collection
of standard and robust ImageNet models, while grid searching over a wide range of
hyperparameters and architectures to find the best model of each type. (All
models are available for download via our <a href="https://github.com/microsoft/robust-models-transfer">code/model
release</a> and more
details on our training procedure can be found there and in <a href="https://arxiv.org/abs/2007.08489">our
paper</a>). We then performed transfer
learning (using both fixed-feature and full-network refinement) from each
trained model to 12 downstream classification tasks.</p>

<p>It turns out that 
adversarially robust source models fairly consistently outperform their standard counterparts in
terms of downstream accuracy. In the table below, we compare the accuracies of
the best standard model (searching over hyperparameters and
architecture) and the best robust model (searching over the
previous factors as well as robustness level $\varepsilon$):</p>

<p><img alt="Table showing that robust models     perform better than their standard counterparts." class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/results-table.svg" style="width: 100%;"/></p>
<div class="footnote">
    The main result: Adversarially robust models outperform their standard counterparts when transferred to downstream classification tasks.
</div>

<p>This difference in performance tends to be particularly striking in the context of fixed-feature transfer learning. The following graph shows, for each architecture and
downstream classification task, the best standard model compared to the best
robust model in that setting. As we can see, adversarially robust models
improve on the performance of their standard counterparts, and the gap tends to
<em>increase</em> as networks increase in width:</p>

<p><img alt="A bar chart showing that robust models improve on     standard ones even without taking the maximum over architectures." class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/LogisticRegression.svg"/></p>
<div class="footnote">
    Adversarially robust models tend to improve over standard networks for
    individual architectures too. (An analogous graph for full-network
    transfer learning is given in Figure 3 of <a href="https://arxiv.org/abs/2007.08489">our paper</a>.)
</div>

<p>Adversarial robustness improved downstream transfer
performance even when the target task was not a classification one. For example, the
following table compares standard and robust pre-training for use in downstream
object detection and instance segmentation:</p>

<p><img class="bigimg" src="https://gradientscience.org/../assets/robust-transfer-learning/obj-det-results.svg" style="width: 80%;"/></p>
<div class="footnote">
</div>

<h3 id="robustness-versus-accuracy">Robustness versus accuracy</h3>

<p>So it seems like robust models, despite being less accurate on the source task, are actually
better for transfer learning purposes. Indeed, the linear relation between
 ImageNet accuracy and transfer performance observed in prior work (see our discussion above) doesn’t seem
 to hold when the robustness parameter is varied. Compare the graphs below to the ones at the very start of this post:</p>

<p><img class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/wide_resnet50_4_LogisticRegression.svg"/></p>
<div class="footnote">
    Source-task (ImageNet) versus target (fixed-feature) accuracy for models with the same
    architecture while varying the robustness levels. Each dot is a
    WideResNet-50x4 model with $x$ coordinate given by source-task accuracy and
    $y$ coordinate given by fixed-feature transfer learning accuracy.
    Contrast the trends here with the "fixed-feature" trend in the first
    figure of this post—the linear trend depicted there largely disappears as less
    accurate but more robust models perform better in terms of transfer.
</div>

<p>How do we reconcile our observations with these trends observed by prior work?</p>

<p>We hypothesize that robustness and accuracy have <em>disentangled</em> effects on
transfer performance. That is, for a fixed level of robustness, higher
accuracy on the source task helps transfer, and for a fixed level of
accuracy, increased robustness helps transfer. Indeed, as shown below, for a
fixed level of robustness, the accuracy-transfer relation tends to hold
strongly:</p>

<p><img class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/fixed-robustness.svg"/></p>
<div class="footnote">
Even though robust models appear to break the linear
accuracy-transfer trend, this trend is actually preserved for a fixed value of
robustness. Each dot in the graph is a different architecture, trained for the same level of robustness ($\varepsilon = 3.0$). The $x$ coordinate is source task (ImageNet) accuracy, and the $y$ coordinate is the downstream accuracy on each target dataset.
</div>

<p>In addition to reconciling our results with those of prior work, these findings suggest that ongoing work on developing more accurate robust models
may have the added benefit of further improving transfer learning performance.</p>

<h3 id="other-empirical-mysteries-and-future-work">Other empirical mysteries and future work</h3>

<p>This post discussed how adversarially robust models might constitute a promising
avenue for improving transfer learning, and already often outperform standard
models in terms of downstream accuracy. In <a href="https://arxiv.org/abs/2007.08489">our paper</a>, 
we study this phenomenon more closely: for example, we examine the effects of
model width, and we compare adversarial robustness to other notions of
robustness. We also uncover a few somewhat mysterious properties: for example,
resizing images seems to have a non-trivial effect on the relationship between
robustness and downstream accuracy.</p>

<p>Finally, while our work provides evidence that adversarially
robust computer vision models transfer better, understanding precisely <em>why</em> this is the case remains open. More broadly, the results we
observe indicate that we still do not yet fully understand (even empirically)
the ingredients that make transfer learning successful. We hope that our work
prompts an inquiry into the underpinnings of modern transfer learning.</p></div>
    </summary>
    <updated>2020-07-20T00:00:00Z</updated>
    <published>2020-07-20T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2020-07-23T00:02:08Z</updated>
    </source>
  </entry>
</feed>
