<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2022-07-15T04:38:59Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.06335</id>
    <link href="http://arxiv.org/abs/2207.06335" rel="alternate" type="text/html"/>
    <title>On the (non-)stability of the sheaf-function correspondence</title>
    <feedworld_mtime>1657843200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berkouk:Nicolas.html">Nicolas Berkouk</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.06335">PDF</a><br/><b>Abstract: </b>The sheaf-function correspondence identifies the group of constructible
functions on a real analytic manifold $M$ with the Grothendieck group of
constructible sheaves on $M$ . When $M$ is a finite dimensional real vector
space, Kashiwara-Schapira have recently introduced the convolution distance
between sheaves of $\textbf{k}$-vector spaces on $M$ . In this paper, we
characterize distances on the group of constructible functions on a real finite
dimensional vector space that can be controlled by the convolution distance
through the sheaf-function correspondence. Our main result asserts that such
distances are almost trivial: they vanish as soon as two constructible
functions have the same Euler integral.
</p></div>
    </summary>
    <updated>2022-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.06256</id>
    <link href="http://arxiv.org/abs/2207.06256" rel="alternate" type="text/html"/>
    <title>Image warp preserving content intensity</title>
    <feedworld_mtime>1657843200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Enrico Segre <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.06256">PDF</a><br/><b>Abstract: </b>An accurate method for warping images is presented. Differently from most
commonly used techniques, this method guarantees the conservation of the
intensity of the transformed image, evaluated as the sum of its pixel values
over the whole image or over corresponding transformed subregions of it. Such
property is mandatory for quantitative analysis, as, for instance, when
deformed images are used to assess radiances, to measure optical fluxes from
light sources, or to characterize material optical densities. The proposed
method enforces area resampling by decomposing each rectangular pixel in two
triangles, and projecting the pixel intensity onto half pixels of the
transformed image, with weights proportional to the area of overlap of the
triangular half-pixels. The result is quantitatively exact, as long as the
original pixel value is assumed to represent a constant image density within
the pixel area, and as long as the coordinate transformation is diffeomorphic.
Implementation details and possible variations of the method are discussed.
</p></div>
    </summary>
    <updated>2022-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.04710</id>
    <link href="http://arxiv.org/abs/2207.04710" rel="alternate" type="text/html"/>
    <title>Weighted simplicial complexes and their representation power of higher-order network data and topology</title>
    <feedworld_mtime>1657843200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Baccini:Federica.html">Federica Baccini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Geraci:Filippo.html">Filippo Geraci</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bianconi:Ginestra.html">Ginestra Bianconi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.04710">PDF</a><br/><b>Abstract: </b>Hypergraphs and simplical complexes both capture the higher-order
interactions of complex systems, ranging from higher-order collaboration
networks to brain networks. One open problem in the field is what should drive
the choice of the adopted mathematical framework to describe higher-order
networks starting from data of higher-order interactions. Unweighted simplicial
complexes typically involve a loss of information of the data, though having
the benefit to capture the higher-order topology of the data. In this work we
show that weighted simplicial complexes allow to circumvent all the limitations
of unweighted simplicial complexes to represent higher-order interactions. In
particular, weighted simplicial complexes can represent higher-order networks
without loss of information, allowing at the same time to capture the weighted
topology of the data. The higher-order topology is probed by studying the
spectral properties of suitably defined weighted Hodge Laplacians displaying a
normalized spectrum. The higher-order spectrum of (weighted) normalized Hodge
Laplacians is here studied combining cohomology theory with information theory.
In the proposed framework, we quantify and compare the information content of
higher-order spectra of different dimension using higher-order spectral
entropies and spectral relative entropies. The proposed methodology is tested
on real higher-order collaboration networks and on the weighted version of the
simplicial complex model "Network Geometry with Flavor".
</p></div>
    </summary>
    <updated>2022-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/101</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/101" rel="alternate" type="text/html"/>
    <title>TR22-101 |  A Near-Cubic Lower Bound for 3-Query Locally Decodable Codes from Semirandom CSP Refutation | 

	Omar Alrabiah, 

	Pravesh Kothari, 

	Venkatesan Guruswami, 

	Peter Manohar</title>
    <summary>A code $C \colon \{0,1\}^k \to \{0,1\}^n$ is a $q$-locally decodable code ($q$-LDC) if one can recover any chosen bit $b_i$ of the message $b \in \{0,1\}^k$ with good confidence by randomly querying the encoding $x = C(b)$ on at most $q$ coordinates. Existing constructions of $2$-LDCs achieve $n = \exp(O(k))$, and lower bounds show that this is in fact tight. However, when $q = 3$, far less is known: the best constructions achieve $n = \exp(k^{o(1)})$, while the best known results only show a quadratic lower bound $n \geq \widetilde{\Omega}(k^2)$ on the blocklength.

In this paper, we prove a near-cubic lower bound of $n \geq \widetilde{\Omega}(k^3)$ on the blocklength of $3$-query LDCs. This improves on the best known prior works by a polynomial factor in $k$. Our proof relies on a new connection between LDCs and refuting constraint satisfaction problems with limited randomness. Our quantitative improvement builds on the new techniques for refuting semirandom instances of CSPs developed in [GKM22] and, in particular, relies on bounding the $(\infty \to 1)$-norm of appropriate Kikuchi matrices.</summary>
    <updated>2022-07-14T21:14:52Z</updated>
    <published>2022-07-14T21:14:52Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-15T04:37:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/100</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/100" rel="alternate" type="text/html"/>
    <title>TR22-100 |  Streaming complexity of CSPs with randomly ordered constraints | 

	Santhoshini Velusamy, 

	Noah Singer, 

	Raghuvansh Saxena, 

	Madhu Sudan</title>
    <summary>We initiate a study of the streaming complexity of constraint satisfaction problems (CSPs) when the constraints arrive in a random order. We show that there exists a CSP, namely Max-DICUT, for which random ordering makes a provable difference. Whereas a $4/9 \approx 0.445$ approximation of DICUT requires $\Omega(\sqrt{n})$ space with adversarial ordering, we show that with random ordering of constraints there exists a $0.48$-approximation algorithm that only needs $O(\log n)$ space. We also give new algorithms for Max-DICUT in variants of the adversarial ordering setting. Specifically, we give a two-pass  $O(\log n)$ space $0.48$-approximation algorithm for general graphs and a single-pass $\tilde{O}(\sqrt{n})$ space $0.48$-approximation algorithm for bounded degree graphs.
    
    On the negative side, we prove that CSPs where the satisfying assignments of the constraints support a one-wise independent distribution require $\Omega(\sqrt{n})$-space for any non-trivial approximation, even when the constraints are randomly ordered. This was previously known only for adversarially ordered constraints. Extending the results to randomly ordered constraints requires switching the hard instances from a union of random matchings to simple Erdos-Renyi random (hyper)graphs and extending tools that can perform Fourier analysis on such instances. 
    
    The only CSP to have been considered previously with random ordering is Max-CUT where the ordering is not known to change the approximability. Specifically, it is known to be as hard to approximate with random ordering as with adversarial ordering, for $o(\sqrt{n})$ space algorithms. Our results show a richer variety of possibilities and motivate further study of CSPs with randomly ordered constraints.</summary>
    <updated>2022-07-14T18:39:48Z</updated>
    <published>2022-07-14T18:39:48Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-15T04:37:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/099</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/099" rel="alternate" type="text/html"/>
    <title>TR22-099 |  Equivalence Test for Read-Once Arithmetic Formulas | 

	Nikhil Gupta, 

	Chandan Saha, 

	Bhargav Thankey</title>
    <summary>We study the polynomial equivalence problem for orbits of read-once arithmetic formulas (ROFs). Read-once formulas have received considerable attention in both algebraic and Boolean complexity and have served as a testbed for developing effective tools and techniques for analyzing circuits. Two $n$-variate polynomials $f, g \in \mathbb{F}[\mathbf{x}]$ are equivalent, denoted as $f \sim g$, if there is an $A \in \mathrm{GL}(n, \mathbb{F})$ such that $f = g(A\mathbf{x})$. The orbit of $f$ is the set of all polynomials equivalent to $f$. We investigate the complexity of the following two natural problems on ROFs:

1. Equivalence test for ROFs: Given black-box access to $f$, check if it is in the orbit of an ROF. If yes, output an ROF $C$ and an $A \in \mathrm{GL}(n, \mathbb{F})$ such that $f = C(A\mathbf{x})$.  
2. Polynomial equivalence for orbits of ROFs: Given black-box access to $f$ and $g$ in the orbits of two unknown ROFs, check if $f \sim g$. If yes, output an $A \in \mathrm{GL}(n, \mathbb{F})$ such that $f = g(A\mathbf{x})$.

These problems are significant generalizations of two well-studied problems in algebraic complexity, namely reconstruction of ROFs and quadratic form equivalence. In this work, we give the first randomized polynomial-time algorithms (with oracle access to quadratic form equivalence) to solve the two problems. The equivalence test works for general ROFs; it also implies an efficient learning algorithm for random arithmetic formulas of unbounded depth and fan-in (in the high number of variables setting). The algorithm for the second problem, which invokes the equivalence test, works for mildly restricted ROFs, namely additive-constant-free ROFs.  	
	
The equivalence test is based on a novel interplay between the factors and the essential variables of the Hessian determinant of an ROF, the essential variables of the ROF, and certain special structures in the ROF that we call "skewed paths". To our knowledge, the Hessian of a general ROF (or even a depth-4 ROF) has not been analyzed before. Analyzing the Hessian and combining the knowledge gained from it with the skewed paths to recursively discover formulas in the orbits of sub-ROFs of lower depth (without incurring an exponential blow-up due to unbounded depth) constitute the main technical contributions of this work.</summary>
    <updated>2022-07-14T12:43:08Z</updated>
    <published>2022-07-14T12:43:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-15T04:37:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.06262</id>
    <link href="http://arxiv.org/abs/2207.06262" rel="alternate" type="text/html"/>
    <title>Organic Priors in Non-Rigid Structure from Motion</title>
    <feedworld_mtime>1657756800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Suryansh.html">Suryansh Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gool:Luc_Van.html">Luc Van Gool</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.06262">PDF</a><br/><b>Abstract: </b>This paper advocates the use of organic priors in classical non-rigid
structure from motion (NRSfM). By organic priors, we mean invaluable
intermediate prior information intrinsic to the NRSfM matrix factorization
theory. It is shown that such priors reside in the factorized matrices, and
quite surprisingly, existing methods generally disregard them. The paper's main
contribution is to put forward a simple, methodical, and practical method that
can effectively exploit such organic priors to solve NRSfM. The proposed method
does not make assumptions other than the popular one on the low-rank shape and
offers a reliable solution to NRSfM under orthographic projection. Our work
reveals that the accessibility of organic priors is independent of the camera
motion and shape deformation type. Besides that, the paper provides insights
into the NRSfM factorization -- both in terms of shape, motion -- and is the
first approach to show the benefit of single rotation averaging for NRSfM.
Furthermore, we outline how to effectively recover motion and non-rigid 3D
shape using the proposed organic prior based approach and demonstrate results
that outperform prior-free NRSfM performance by a significant margin. Finally,
we present the benefits of our method via extensive experiments and evaluations
on several benchmark dataset.
</p></div>
    </summary>
    <updated>2022-07-14T23:37:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.06210</id>
    <link href="http://arxiv.org/abs/2207.06210" rel="alternate" type="text/html"/>
    <title>Deciding FO-rewritability of regular languages and ontology-mediated queries in Linear Temporal Logic</title>
    <feedworld_mtime>1657756800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurucz:Agi.html">Agi Kurucz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ryzhikov:Vladislav.html">Vladislav Ryzhikov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Savateev:Yury.html">Yury Savateev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zakharyaschev:Michael.html">Michael Zakharyaschev</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.06210">PDF</a><br/><b>Abstract: </b>Our concern is the problem of determining the data complexity of answering an
ontology-mediated query (OMQ) formulated in linear temporal logic LTL over
(Z,&lt;) and deciding whether it is rewritable to an FO(&lt;)-query, possibly with
some extra predicates. First, we observe that, in line with the circuit
complexity and FO-definability of regular languages, OMQ answering in AC^0,
ACC^0 and NC^1 coincides with FO(&lt;,\equiv)-rewritability using unary predicates
x \equiv 0 (mod n), FO(&lt;,MOD)-rewritability, and FO(RPR)-rewritability using
relational primitive recursion, respectively. We prove that, similarly to known
PSPACE-completeness of recognising FO(&lt;)-definability of regular languages,
deciding FO(&lt;,\equiv)- and FO(&lt;,MOD)-definability is also \PSPACE-complete
(unless ACC^0 = NC^1). We then use this result to show that deciding FO(&lt;)-,
FO(&lt;,\equiv)- and FO(&lt;,MOD)-rewritability of LTL OMQs is EXPSPACE-complete, and
that these problems become PSPACE-complete for OMQs with a linear Horn ontology
and an atomic query, and also a positive query in the cases of FO(&lt;)- and
FO(&lt;,\equiv)-rewritability. Further, we consider FO(&lt;)-rewritability of OMQs
with a binary-clause ontology and identify OMQ classes, for which deciding it
is PSPACE-, Pi_2^p- and coNP-complete.
</p></div>
    </summary>
    <updated>2022-07-14T22:37:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.06091</id>
    <link href="http://arxiv.org/abs/2207.06091" rel="alternate" type="text/html"/>
    <title>Structured Decompositions: Structural and Algorithmic Compositionality</title>
    <feedworld_mtime>1657756800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bumpus:Benjamin_Merlin.html">Benjamin Merlin Bumpus</a>, Zoltan Kocsis, Jade Edenstar Master <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.06091">PDF</a><br/><b>Abstract: </b>We introduce structured decompositions: category-theoretic generalizations of
many combinatorial invariants -- including tree-width, layered tree-width,
co-tree-width and graph decomposition width -- which have played a central role
in the study of structural and algorithmic compositionality in both graph
theory and parameterized complexity. Structured decompositions allow us to
generalize combinatorial invariants to new settings (for example decompositions
of matroids) in which they describe algorithmically useful structural
compositionality. As an application of our theory we prove an algorithmic meta
theorem for the Sub_P-composition problem which, when instantiated in the
category of graphs, yields compositional algorithms for NP-hard problems such
as: Maximum Bipartite Subgraph, Maximum Planar Subgraph and Longest Path.
</p></div>
    </summary>
    <updated>2022-07-14T22:40:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.05985</id>
    <link href="http://arxiv.org/abs/2207.05985" rel="alternate" type="text/html"/>
    <title>Realizability Makes a Difference: A Complexity Gap for Sink-Finding in USOs</title>
    <feedworld_mtime>1657756800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weber:Simon.html">Simon Weber</a>, Joel Widmer <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.05985">PDF</a><br/><b>Abstract: </b>Algorithms for finding the sink in Unique Sink Orientations (USOs) of the
hypercube can be used to solve many algebraic and geometric problems, most
importantly including the P-Matrix Linear Complementarity Problem and Linear
Programming. The realizable USOs are those that arise from the reductions of
these problems to the USO sink-finding problem. Finding the sink of realizable
USOs is thus highly practically relevant, yet it is unknown whether
realizability can be exploited algorithmically to find the sink more quickly.
However, all (non-trivial) known unconditional lower bounds for sink-finding
make use of USOs that are provably not realizable. This indicates that the
sink-finding problem might indeed be strictly easier on realizable USOs.
</p>
<p>In this paper we show that this is true for a subclass of all USOs. We
consider the class of Matou\v{s}ek-type USOs, which are a translation of
Matou\v{s}ek's LP-type problems into the language of USOs. We show a query
complexity gap between sink-finding in all, and sink-finding in only the
realizable $n$-dimensional Matou\v{s}ek-type USOs. We provide concrete
deterministic algorithms and lower bounds for both cases, and show that in the
realizable case $O(log^2 n)$ vertex evaluation queries suffice, while in
general exactly $n$ queries are needed. The Matou\v{s}ek-type USOs are the
first USO class found to admit such a gap.
</p></div>
    </summary>
    <updated>2022-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.05975</id>
    <link href="http://arxiv.org/abs/2207.05975" rel="alternate" type="text/html"/>
    <title>Caching with Reserves</title>
    <feedworld_mtime>1657756800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ibrahimpur:Sharat.html">Sharat Ibrahimpur</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Purohit:Manish.html">Manish Purohit</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svitkina:Zoya.html">Zoya Svitkina</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vee:Erik.html">Erik Vee</a>, Joshua Wang <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.05975">PDF</a><br/><b>Abstract: </b>Caching is a crucial component of many computer systems, so naturally it is a
well-studied topic in algorithm design. Much of traditional caching research
studies cache management for a single-user or single-processor environment. In
this paper, we propose two related generalizations of the classical caching
problem that capture issues that arise in a multi-user or multi-processor
environment. In the caching with reserves problem, a caching algorithm is
required to maintain at least $k_i$ pages belonging to user $i$ in the cache at
any time, for some given reserve capacities $k_i$. In the public-private
caching problem, the cache of total size $k$ is partitioned into subcaches, a
private cache of size $k_i$ for each user $i$ and a shared public cache usable
by any user. In both of these models, as in the classical caching framework,
the objective of the algorithm is to dynamically maintain the cache so as to
minimize the total number of cache misses.
</p>
<p>We show that caching with reserves and public-private caching models are
equivalent up to constant factors, and thus focus on the former. Unlike
classical caching, both of these models turn out to be NP-hard even in the
offline setting, where the page sequence is known in advance. For the offline
setting, we design a 2-approximation algorithm, whose analysis carefully keeps
track of a potential function to bound the cost. In the online setting, we
first design an $O(\ln k)$-competitive fractional algorithm using the
primal-dual framework, and then show how to convert it online to a randomized
integral algorithm with the same guarantee.
</p></div>
    </summary>
    <updated>2022-07-14T22:40:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.05945</id>
    <link href="http://arxiv.org/abs/2207.05945" rel="alternate" type="text/html"/>
    <title>Online Active Regression</title>
    <feedworld_mtime>1657756800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Cheng.html">Cheng Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yi.html">Yi Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Yiming.html">Yiming Sun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.05945">PDF</a><br/><b>Abstract: </b>Active regression considers a linear regression problem where the learner
receives a large number of data points but can only observe a small number of
labels. Since online algorithms can deal with incremental training data and
take advantage of low computational cost, we consider an online extension of
the active regression problem: the learner receives data points one by one and
immediately decides whether it should collect the corresponding labels. The
goal is to efficiently maintain the regression of received data points with a
small budget of label queries. We propose novel algorithms for this problem
under $\ell_p$ loss where $p\in[1,2]$. To achieve a $(1+\epsilon)$-approximate
solution, our proposed algorithms only require
$\tilde{\mathcal{O}}(\epsilon^{-2} d \log(n\kappa))$ queries of labels, where
$n$ is the number of data points and $\kappa$ is a quantity, called the
condition number, of the data points. The numerical results verify our
theoretical results and show that our methods have comparable performance with
offline active regression algorithms.
</p></div>
    </summary>
    <updated>2022-07-14T22:41:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.05930</id>
    <link href="http://arxiv.org/abs/2207.05930" rel="alternate" type="text/html"/>
    <title>On the Complexity of Identifying Strongly Regular Graphs</title>
    <feedworld_mtime>1657756800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levet:Michael.html">Michael Levet</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.05930">PDF</a><br/><b>Abstract: </b>In this note, we show that Graph Isomorphism (GI) is not
$\textsf{AC}^{0}$-reducible to several problems, including the Latin Square
Isotopy problem and isomorphism testing of several families of Steiner designs.
As a corollary, we obtain that GI is not $\textsf{AC}^{0}$-reducible to
isomorphism testing of Latin square graphs and strongly regular graphs arising
from special cases of Steiner $2$-designs. We accomplish this by showing that
the generator-enumeration technique for each of these problems can be
implemented in $\beta_{2}\textsf{FOLL}$, which cannot compute Parity
(Chattopadhyay, Tor\'an, &amp; Wagner, $\textit{ACM Trans. Comp. Theory}$, 2013).
</p></div>
    </summary>
    <updated>2022-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2207.05898</id>
    <link href="http://arxiv.org/abs/2207.05898" rel="alternate" type="text/html"/>
    <title>Testing and Learning Quantum Juntas Nearly Optimally</title>
    <feedworld_mtime>1657756800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Thomas.html">Thomas Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nadimpalli:Shivam.html">Shivam Nadimpalli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuen:Henry.html">Henry Yuen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2207.05898">PDF</a><br/><b>Abstract: </b>We consider the problem of testing and learning quantum $k$-juntas: $n$-qubit
unitary matrices which act non-trivially on just $k$ of the $n$ qubits and as
the identity on the rest. As our main algorithmic results, we give (a) a
$\widetilde{O}(\sqrt{k})$-query quantum algorithm that can distinguish quantum
$k$-juntas from unitary matrices that are "far" from every quantum $k$-junta;
and (b) a $O(4^k)$-query algorithm to learn quantum $k$-juntas. We complement
our upper bounds for testing quantum $k$-juntas and learning quantum $k$-juntas
with near-matching lower bounds of $\Omega(\sqrt{k})$ and
$\Omega(\frac{4^k}{k})$, respectively. Our techniques are Fourier-analytic and
make use of a notion of influence of qubits on unitaries.
</p></div>
    </summary>
    <updated>2022-07-14T22:39:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8443</id>
    <link href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/" rel="alternate" type="text/html"/>
    <title>My friend, Scott Aaronson</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is a photo of my book shelf at the office. Ever since joining Harvard, I have been ordering copies of Quantum Computing Since Democritus on a regular basis. I often hand them out to bright students, curious about science, whom I want to expose to the beautiful connections between computer science, math, physics, and … <a class="more-link" href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">Continue reading <span class="screen-reader-text">My friend, Scott Aaronson</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><a href="https://windowsontheory.files.wordpress.com/2022/07/image.png"><img alt="" class="wp-image-8446" height="643" src="https://windowsontheory.files.wordpress.com/2022/07/image.png?w=750" width="471"/></a></figure></div>


<p>This is a photo of my book shelf at the office. Ever since joining Harvard, I have been ordering copies of <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing Since Democritus</a> on a regular basis. I often hand them out to bright students, curious about science, whom I want to expose to the beautiful connections between computer science, math, physics, and even philosophy. Scott has been one of the great popularizers of our field even before he started blogging in 2005. His surveys and blog posts provide some of the best introductions to our field. For example, when investigating  <a href="http://quant-ph/0502072">P vs NP and physical reality</a>, Scott actually <a href="https://scottaaronson.blog/?p=266">went out and verified</a> that nature indeed cannot solve an NP-complete problem via finding the globally minimal energy configuration of soap bubbles.  Through his blog, popular writing, and research, Scott has done more than anyone else to introduce new people of all backgrounds to theoretical computer science. </p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="408" src="https://lh6.googleusercontent.com/JRN6bxodVkY1JiHSWOEx4QXtvmH3twDk-O5jGgsio1IM92aUUxYZ87i7T0qU8bfrCnVbBWIt1OV4soNgnhoMpTHvnJBIVk8tAPO_oK2_WLzHbrGfWNpRDQyazey-CG9V0YLDDZ2r45pwOKsSF4PZaQ" width="543"/></figure></div>


<p>One of Scott’s endearing qualities is his openness to all people. While many of us would ignore a random email or anonymous blog comment, Scott would patiently explain for the millionth time why quantum computers can’t solve NP-hard problems by “trying all solutions in parallel” or why Bell’s Inequality does indeed rule out hidden-variable theories of nature. Alas, the same openness also results in him sometimes giving too much attention and caring far too much about the opinions of Internet “trolls” that are not worthy of his time. </p>



<p>While Scott has always attracted some vitriol, recently this has taken to a <a href="https://scottaaronson.blog/?p=6546">new level</a>, with commenters attacking his integrity, his speech mannerisms, even his T-shirt choice/frequency, and worst of all, his family, with misogynistic attacks on his wife and xenophobic and ableist attacks on neurodivergent researchers.</p>



<p>None of these people have made a fraction of the contributions of Scott not just to science, but also to broadening the diversity of computer science, and other causes including <a href="https://scottaaronson.blog/?p=6411">assisting women dealing with Texas’ restrictive abortion laws</a>. (As full disclosure, one of the causes <a href="https://scottaaronson.blog/?p=6256">Scott helped raise money</a> for is <a href="https://www.addiscoder.com/">AddisCoder</a> and <a href="https://jamcoder.org.jm/">JamCoders</a> of which I am a board member. I just came back from a <a href="https://twitter.com/boazbaraktcs/status/1545765613167067136?s=20&amp;t=53HLUIjmCwBsl1oYhEQXhw">week teaching in Jamaica</a>, the students were amazing and are so thankful for the chance to participate in this program; they couldn’t care less how often Scott changes his shirt.)</p>



<p>I am grateful that Scott is a member of our scientific community and proud to call him my friend. Does this mean that I agree with all his positions? Absolutely not. I tend to be on his left on many issues  (though am probably more conservative when it comes to oracle-based complexity..). Are there people he’s friendly with whom I even more strongly disagree with, and whose views I might even find repugnant? Probably. But it doesn’t matter, all of us are connected via 6 degrees of separation. If we start to “recursively cancel” every one that is somehow connected to someone we find odious, then we would not be able to talk to anyone.</p>



<p>I hope that Scott is not disheartened by these attacks, and continues to contribute for many years to CS research and education, outreach, and humanity at large.</p></div>
    </content>
    <updated>2022-07-13T16:07:23Z</updated>
    <published>2022-07-13T16:07:23Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2022-07-15T04:37:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/098</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/098" rel="alternate" type="text/html"/>
    <title>TR22-098 |  Non-Adaptive Proper Learning Polynomials | 

	Nader Bshouty</title>
    <summary>We give the first polynomial-time *non-adaptive* proper learning algorithm of Boolean sparse multivariate polynomial under the uniform distribution. Our algorithm, for $s$-sparse polynomial over $n$ variables, makes $q=(s/\epsilon)^{\gamma(s,\epsilon)}\log n$ queries where $2.66\le \gamma(s,\epsilon)\le 6.922$ and runs in $\tilde O(n)\cdot poly(s,1/\epsilon)$ time. We also show that for any $\epsilon=1/s^{O(1)}$ any non-adaptive learning algorithm must make at least $(s/\epsilon)^{\Omega(1)}\log n$ queries. Therefore, the query complexity of our algorithm is also polynomial in the optimal query complexity and optimal in $n$.</summary>
    <updated>2022-07-13T02:55:04Z</updated>
    <published>2022-07-13T02:55:04Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-15T04:37:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6552</id>
    <link href="https://scottaaronson.blog/?p=6552" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6552#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6552" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Choosing a new comment policy</title>
    <summary xml:lang="en-US">Update (July 13): I was honored to read this post by my friend Boaz Barak. Update (July 14): By now, comments on this post allegedly from three CS professors — namely, Josh Alman, Aloni Cohen, and Rana Hanocka — have been unmasked as from impersonators (!!). I’m obviously the target of one or more motivated […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Update (July 13):</strong> I was honored to read <a href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">this post</a> by my friend Boaz Barak.</p>



<p><strong>Update (July 14):</strong> By now, comments on this post allegedly from three CS professors  — namely, Josh Alman, Aloni Cohen, and Rana Hanocka — have been unmasked as from impersonators (!!).</p>



<p>I’m obviously the target of one or more motivated attack-trolls who know something about the CS community. This might be the single weirdest thing that’s happened to me in 17 years of blogging.  It obviously underscores the need for a new, stricter comment policy, which is what this whole post was about.</p>



<p><strong>Also:</strong> Since many people asked me about the authenticity of the <a href="https://scottaaronson.blog/?p=6546#comment-1941123">vicious, libelous comment</a> signed “Aida Behmard” (an astronomy PhD student at Caltech)—after multiple email inquiries, I received only a terse, enigmatic response from Ms. Behmard, which neither explicitly denied writing the comment, nor asked me to delete it, nor repudiated its content, but which did ask not to be contacted again.  I’ll respect Ms. Behmard’s wish (as I ask others to), and will await further communication from her before taking any additional actions.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Yesterday and today, both my work and my enjoyment of the James Webb images were interrupted by an anonymous troll, who used the <em>Shtetl-Optimized</em> comment section to heap <a href="https://scottaaronson.blog/?p=6546#comment-1941041">libelous abuse</a> on me—derailing an anodyne quantum computing discussion to opine at length about how I’m a disgusting creep who surely, probably, maybe has lewd thoughts about his female students.  Unwisely or not, I allowed it all to appear, and replied to all of it.  I had a few reasons: I wanted to prove that I’m now strong enough to withstand bullying that might once have driven me to suicide.  I wanted, frankly, many readers to come to my defense (thanks to those who did!).  I at least wanted readers to <em>see</em> firsthand what I now regularly deal with: the emotional price of maintaining this blog.  Most of all, I wanted my feminist, social-justice-supporting readers to either explicitly endorse or (hopefully) explicitly repudiate the unambiguous harassment that was now being gleefully committed in their name.</p>



<p>Then, though, the same commenter upped the ante further, by heaping misogynistic abuse on my wife <a href="https://www.cs.utexas.edu/~danama/">Dana</a>—while <em>still</em>, ludicrously and incongruously, cloaking themselves in the rhetoric of social justice.  Yes: apparently the woke, feminist thing to do is now to rate female computer scientists on their looks.</p>



<p>Let me be blunt: I cannot continue to write <em>Shtetl-Optimized</em> while dealing with regular harassment of me and my family.  At the same time, I’m also determined not to “surrender to the terrorists.”  So, I’m weighing the following options:</p>



<ul><li>Close comments except to commenters who provide a real identity—e.g., a full real name, a matching email address, a website.</li><li>Move to Substack, and then allow only commenters who’ve signed up.</li><li>Hire someone to pre-screen comments for me, and delete ones that are abusive or harassing (to me or others) before I even see them.  (Any volunteers??)</li><li>Make the comment sections for readers only, eliminating any expectation that I’ll participate.</li></ul>



<p>One thing that’s clear is that the status quo will not continue.  I can’t “just delete” harassing or abusive comments, because the trolls have gotten too good at triggering me, and they will continue to weaponize my openness and my ethic of responding to all possible arguments against me.</p>



<p>So, regular readers: what do you prefer?</p></div>
    </content>
    <updated>2022-07-12T20:47:43Z</updated>
    <published>2022-07-12T20:47:43Z</published>
    <category scheme="https://scottaaronson.blog" term="Announcements"/>
    <category scheme="https://scottaaronson.blog" term="Obviously I'm Not Defending Aaronson"/>
    <category scheme="https://scottaaronson.blog" term="Self-Referential"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-07-15T02:16:05Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3528238240939023029</id>
    <link href="http://blog.computationalcomplexity.org/feeds/3528238240939023029/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/review-of-engines-of-cognition-essays.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/3528238240939023029" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/3528238240939023029" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/review-of-engines-of-cognition-essays.html" rel="alternate" type="text/html"/>
    <title>Review of The Engines of Cognition: Essays From the LessWrong Forum/Meta question about posts</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> A while back I reviewed<i> A Map that Reflects the Territory</i> which is a collection of essays posted on the lesswrong forum. My review is <a href="https://www.cs.umd.edu/~gasarch/bookrev/FRED/lesswrong.pdf">here</a>. I posted it to both this blog and to the lesswrong forum. In both cases I posted a link to it. My post to lesswrong is <a href="https://www.lesswrong.com/posts/JXTEDFCC5r4dW2tta/review-of-a-map-that-reflects-the-territory">here</a></p><p>On the lesswrong post many of the comments, plus some private emails, told me NO BILL- don't post a link, post it directly as text. It was not clear how to do that, but I got it done with help.</p><p>On complexity blog nobody commented that this was a problem. Then again, nobody commented at all, so its not clear what to make of that. </p><p>So</p><p>Meta Question: Is posting a link worse than posting direct text? Note that the book review was 12 pages long and looked great in LaTeX. </p><p>Meta Question: Why did lesswrong care about the format but complexityblog did not (Probably answer: omplexityblog readers did not care at all, whereas Lesswrong cared about what I though about Lesswrong)</p><p>Another Question, not Meta. One of the comments was (I paraphrase)</p><p><i>When I open a pdf file I expected to see something in the style of an academic paper. This is written in very much chatty, free-flowing blog post style with jokes like calling neologisms ``newords'', so the whole think felt more off-kilter than was intended. The style of writing would prob work better as an HTML blog post (which could then be posted directly as a Lesswrong post here instead of hosted elsewhere and linked.)</i></p><p>I think its interesting that the format of an article telegraphs (in this case incorrectly) what type of article it will be. Is this a common problem?  I have had the experience of reading a real academic paper and being surprised that some joke or cultural-reference is in it, though I do not object to this. </p><p>Another comment and question</p><p><i>I was surprised the post only had 11 karma when I saw it (William had send me an advance copy and I'd really liked reading it) but when I saw that it was a link post, I understood why.</i></p><p>I find this hilarious- they have some way the posts are rated!  For one, Lance told me very early on to never worry about comments, and I don't. Second, it reminds me of the Black Mirror episode <a href="https://en.wikipedia.org/wiki/Nosedive_(Black_Mirror)">Nosedive</a>.</p><p>ANYWAY, I have reviewed another collection of essays for less wrong, this one called <i>The Engines of</i> <i>Cognition. </i>I am posting it here as a link: <a href="https://www.cs.umd.edu/~gasarch/bookrev/FRED/lesswrong2.pdf">here</a>  and I will post it on lesswrong as full text (with help) in a few days. </p><p>I am posting it so I can get comments before I submit it to the SIGACT News book review column. But this is odd since I think this blog has more readers than SIGACT news has subscribers, so perhaps THIS is its real debut, not that. And of course the lesswrong forum is a place where more will read it since its about them. </p><p>So- I appreciate comments to make it a better review!</p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2022-07-12T01:22:00Z</updated>
    <published>2022-07-12T01:22:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-07-14T15:34:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/data-transfer/</id>
    <link href="https://gradientscience.org/data-transfer/" rel="alternate" type="text/html"/>
    <title>A Data-Based Perspective on Transfer Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><!-- <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> -->
<!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"> -->









<!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css"> -->
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script> -->
<!-- <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script> -->


<!-- <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css"> -->
<!-- chart.js -->


<p><a class="bbutton" href="https://arxiv.org/abs/2207.05739" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/data-transfer" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<br/>
<i>
In our latest paper, we present a framework for pinpointing the impact of the source datasets in transfer learning. Our framework enables us to improve transfer learning performance by removing source datapoints that are detrimental for a specific target task. It also unlocks various other capabilities, such as debugging transfer learning failures, automatically identifying granular subpopulations in the target dataset, and detecting data leakage between source and target datasets. 
</i></p>

<p>Transfer learning is a widely utilized technique for adapting a model trained on a source dataset to improve performance on a downstream target task. Used in applications ranging from <a href="https://arxiv.org/abs/2101.06871">radiology</a>, <a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w13/papers/Kim_End-To-End_Ego_Lane_CVPR_2017_paper.pdf">autonomous driving</a>, to <a href="https://arxiv.org/abs/1510.00098">satellite imagery analysis</a>, the transfer learning paradigm also fuels the recent emergence of large vision and language models trained on enormous amounts of data, such as <a href="https://openai.com/blog/clip/">CLIP</a> or <a href="https://openai.com/blog/gpt-3-apps/">GPT-3</a>.</p>

<p>Why is transfer learning so effective though? And, in particular, what drives transfer learning performance? Definitely, much depends on the properties of the source model, i.e., the model trained on the source dataset. For example, recent works highlight the impact of the model’s <a href="https://arxiv.org/abs/2101.06871">architecture</a>, <a href="https://arxiv.org/abs/1805.08974">accuracy</a>, <a href="https://arxiv.org/abs/2007.08489">adversarial vulnerability</a>, and <a href="https://arxiv.org/abs/1905.05901">training procedure</a>.</p>

<p>But, in addition to the source model, it is hard to not expect the source dataset to play a major role as well. Indeed, several works have shown that increasing the size of the dataset usually <a href="https://arxiv.org/abs/1912.11370">boosts transfer learning performance</a>. <a href="https://arxiv.org/abs/1811.07056">Others</a> have found that limiting the source dataset to images that are relevant to the target task can help as well. So: what is the exact role of the source dataset in transfer learning?</p>

<p>In our most <a href="https://arxiv.org/abs/2207.05739">recent paper</a>, we present a framework for pinpointing the impact of the source dataset on the downstream predictions in transfer learning. This framework draws inspiration from techniques such as <a href="https://arxiv.org/abs/2008.03703">influence functions</a> and <a href="https://arxiv.org/abs/2202.00622">datamodels</a> and it enables us, in particular, to automatically identify source datapoints that—positively or negatively—impact transfer learning performance.</p>

<p>We’ll now walk through how we calculate the influence of the source dataset in transfer learning, and then demonstrate how our framework can be used to:</p>

<ul type="a">
  <li>Boost transfer learning performance by removing detrimental source datapoints;</li>
  <li>Automatically extract granular subpopulations in the target dataset by projecting the  class hierarchy of the source dataset onto it; and</li>
  <li>Surface pathologies such as source-target data leakage and misleading or mislabelled source datapoints.</li>
</ul>

<h2 id="computing-the-influence-of-the-source-dataset">Computing the Influence of the Source Dataset</h2>

<p>How to pinpoint the relationship between the source dataset’s composition and the model’s downstream predictions? We build here on the <a href="https://arxiv.org/abs/2008.03703">influence functions</a> and <a href="https://arxiv.org/abs/2202.00622">datamodels</a> methodology (check out our <a href="https://gradientscience.org/datamodels-1/">previous post</a> for a deeper dive into these) to study the counterfactual effect of removing source datapoints on the target model’s predictions. However, unlike in the standard supervised setting in which the focus is on individual datapoints, here we focus on removing entire classes. This is motivated by the fact that we expect the removal of entire classes to have a more measurable impact on the features learned by the source model (and thus the resulting model’s predictions).</p>

<p>So, at a high level, we first train a large number of models on random subsets of classes in the source dataset, and fine-tune them on the target task. Then we compute the influence of a source class on a target example by simply measuring the average difference in the model’s performance on a given target example when the class was included versus excluded from the source dataset. A positive influence value thus means that including the class improved the model’s performance on that example, while a negative value indicates that the class was detrimental to the correctness of the corresponding model’s prediction.</p>

<h2 id="identifying-the-most-influential-classes-of-the-source-dataset">Identifying the Most Influential Classes of the Source Dataset</h2>
<p>Now that we’ve calculated these influences, how can we use them to study transfer learning? First, let’s take a look at the ranking of the most positively and negatively influencing classes for a variety of target tasks. We first look at the influences from different ImageNet classes to the entire CIFAR-10 test set:</p>

<p><img alt="Most influential" src="https://gradientscience.org/assets/data-transfer/images/most_influencing_classes.png" style="width: 100%; margin-left: 0; margin-right: 0;"/>
<!-- <div class="footnote">
TODO
</div> --></p>

<p>Note that in the most positive source classes tend to semantically overlap with the classes in the target dataset. For example, tailed frog and sorrel horse have the most positive influence for the CIFAR-10 dataset, which contains the classes frog and horse.</p>

<p>Also, the plot above suggests that there is a number of source classes, such as bookshop or jigsaw puzzle, whose inclusion actually hurts the overall transfer learning performance. So what happens if we indeed remove these classes from the source dataset? Well, as one might hope, they do boost the transfer learning performance on a variety of downstream image classification:</p>

<p><img alt="Counterfactual main" src="https://gradientscience.org/assets/data-transfer/images/main_counterfactual_exp.png" style="width: 100%; margin-left: 0; margin-right: 0;"/></p>
<div class="footnote">
Target task accuracies after removing the most positively or negatively influential ImageNet classes from the source dataset.
</div>

<p>In fact, we can get an accuracy boost of nearly 2.5% on CIFAR-10 (as compared to what one gets from pre-training with the full ImageNet dataset).</p>

<h2 id="leveraging-influences-to-study-transfer-learning">Leveraging Influences to Study Transfer Learning</h2>

<p>Above, we used our framework to pinpoint the most influential—be it positively or negatively— classes for transfer learning. Here, we’ll discuss how our framework provides us with a broader set of tools for studying transfer learning, including: (1) debugging transfer learning failures, (2) automatically extracting granular subpopulations in the target dataset, and (3) detecting data leakage between source and target datasets.</p>

<h3 id="1--debugging-the-failures-of-the-transferred-model">(1)  Debugging the failures of the transferred model</h3>
<p>Suppose our transferred model wrongly predicts the dog image displayed in the figure below—it labels it as a horse. Can we use our framework to understand why our model is making this mistake? Yes! The influences we computed enable us to identify the source class sorrel horse as one having a strong (and the strongest) negative influence on our image of interest. This suggests that the features learned by the source model due to the presence of this class might be the culprit here. Indeed, once we remove the sorrel horse class from the source dataset, our model now makes the correct prediction on our dog image much more frequently (with respect to the randomness of the training procedure).</p>

<div id="debug_examples_widget" style="overflow: auto; text-align: center;"/>
<div class="footnote">
Identifying highly negatively influencing source classes can explain why our transfer learning model made a mistake (middle). Once we remove the most negatively influencing class from the source dataset, the model predicts the correct label more frequently (right). Click through the thumbnails on the left to see more examples!
</div>

<!-- <div class="footnote">
TODO
</div> -->

<h3 id="2-automatically-extracting-granular-subpopulations-in-the-target-dataset">(2) Automatically extracting granular subpopulations in the target dataset</h3>
<p>Imagine you want to find all the images of ostriches in the CIFAR-10 dataset. However, the problem is that CIFAR-10 does not contain any subpopulation annotations that could help with this task and having to manually look for ostriches among all the images in the bird class is not a very appealing alternative. Our framework allows us to do something much more scalable!</p>

<p>Indeed, as we already observed, the most positively influencing source classes usually semantically overlap with the images in the target dataset they influence the most. In fact, this goes further: the target images which are most influenced by a given source class tend to share relevant salient features too. So, to identify our ostrich subpopulation in CIFAR-10, we just need to look at all the images that are most influenced by the source class “ostrich”! Below we display some of the CIFAR-10 images identified in this way.</p>

<div id="subpop_examples_widget" style="overflow: auto; text-align: center;"/>
<div class="footnote">
The CIFAR-10 images which are most positively influenced by a particular ImageNet class. Click through the thumbnails on the left to see more examples!
</div>

<h3 id="3-detecting-data-leakage-and-misleading-source-dataset-examples">(3) Detecting data-leakage and misleading source dataset examples</h3>
<p>Thus far, we have focused on the role of classes in the source dataset in transfer learning. But we can also compute the influences of specific source examples on the transferred model’s predictions. This turns out to enable us to isolate, in particular, instances of data leakage and misleading examples in the source dataset.</p>

<p>Indeed, below, we display ImageNet training examples that are highly influential on CIFAR-10 test examples. The source images that have a highly positive influence are often identical copies of images from the target task (just at a higher resolution)—a clear example of data leakage. On the other hand, images that have a high negative influence tend to be the ones that are misleading, mislabeled, or otherwise surprising.  For example, the presence of the (amusing) ImageNet image of a flying lawnmower (see below) hurts the performance on a CIFAR-10 image of a regular (but similarly shaped) airplane.</p>

<p><img alt="Detect leakage" src="https://gradientscience.org/assets/data-transfer/images/detect_leakage.png" style="width: 100%; margin-left: 0; margin-right: 0;"/>
<!-- <div class="footnote">
TODO
</div> --></p>

<h3 id="conclusions">Conclusions</h3>
<p>In this post, we described a new framework for pinpointing the impact of the source dataset in transfer learning. Our framework allows one to improve the transfer learning performance on a range of downstream tasks by identifying and removing source datapoints that are detrimental. Furthermore, by using our framework one can automatically extract granular subpopulations in the target dataset by leveraging the fine-grained class hierarchy of the source dataset, better understand how the errors of the model on the downstream task are rooted in the source dataset, and detect potential data leakage from the source to the downstream dataset. We believe our framework provides a new perspective on transfer learning by highlighting the role of the source dataset in the transfer learning pipeline and gives us a toolkit for performing a fine-grained analysis of it.</p></div>
    </summary>
    <updated>2022-07-12T00:00:00Z</updated>
    <published>2022-07-12T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2022-07-15T00:37:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/07/11/postdoc-position-in-computer-science-logic-at-university-of-sheffield-apply-by-july-20-2022/</id>
    <link href="https://cstheory-jobs.org/2022/07/11/postdoc-position-in-computer-science-logic-at-university-of-sheffield-apply-by-july-20-2022/" rel="alternate" type="text/html"/>
    <title>PostDoc position in Computer Science Logic at University of Sheffield (apply by July 20, 2022)</title>
    <summary>All candidates interested in working in logics and complexity theory utilising numerical features and real valued data are encouraged to apply. The project topics range from logical foundations of probabilistic data and complexity theory utilising real numbers to logical approach to quantum information theory utilising the newly discovered connections to probabilistic team semantics. Website: https://www.jobs.ac.uk/job/CRB003/research-associate-in-computer-science-logic-x2-positions […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>All candidates interested in working in logics and complexity theory utilising numerical features and real valued data are encouraged to apply.</p>
<p>The project topics range from logical foundations of probabilistic data and complexity theory utilising real numbers to logical approach to quantum information theory utilising the newly discovered connections to probabilistic team semantics.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CRB003/research-associate-in-computer-science-logic-x2-positions">https://www.jobs.ac.uk/job/CRB003/research-associate-in-computer-science-logic-x2-positions</a><br/>
Email: j.t.virtema@sheffield.ac.uk</p></div>
    </content>
    <updated>2022-07-11T15:43:44Z</updated>
    <published>2022-07-11T15:43:44Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-07-15T04:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/07/11/associate-professor-or-tenure-track-assistant-professor-at-technical-university-of-denmark-apply-by-october-1-2022/</id>
    <link href="https://cstheory-jobs.org/2022/07/11/associate-professor-or-tenure-track-assistant-professor-at-technical-university-of-denmark-apply-by-october-1-2022/" rel="alternate" type="text/html"/>
    <title>Associate Professor or Tenure Track Assistant Professor at Technical University of Denmark (apply by October 1, 2022)</title>
    <summary>DTU Compute’s section for Algorithms, Logic, and Graphs (AlgoLoG) invites applications for our next assistant or associate professor within logic and logic-based artificial intelligence or algorithms and data structures. The AlgoLoG section focuses on research in the foundations of computer science and discrete mathematics and application in industry. Website: https://www.dtu.dk/english/about/job-and-career/vacant-positions/job?id=77ec4bc1-f834-4d49-96da-e263a7abb56c Email: phbi@dtu.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>DTU Compute’s section for Algorithms, Logic, and Graphs (AlgoLoG) invites applications for our next assistant or associate professor within logic and logic-based artificial intelligence or algorithms and data structures. The AlgoLoG section focuses on research in the foundations of computer science and discrete mathematics and application in industry.</p>
<p>Website: <a href="https://www.dtu.dk/english/about/job-and-career/vacant-positions/job?id=77ec4bc1-f834-4d49-96da-e263a7abb56c">https://www.dtu.dk/english/about/job-and-career/vacant-positions/job?id=77ec4bc1-f834-4d49-96da-e263a7abb56c</a><br/>
Email: phbi@dtu.dk</p></div>
    </content>
    <updated>2022-07-11T11:34:26Z</updated>
    <published>2022-07-11T11:34:26Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-07-15T04:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6546</id>
    <link href="https://scottaaronson.blog/?p=6546" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6546#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6546" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Linkz!</title>
    <summary xml:lang="en-US">(1) Fellow CS theory blogger (and, 20 years ago, member of my PhD thesis committee) Luca Trevisan interviews me about Shtetl-Optimized, for the Bulletin of the European Association for Theoretical Computer Science. Questions include: what motivates me to blog, who my main inspirations are, my favorite posts, whether blogging has influenced my actual research, and […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>(1) Fellow <a href="https://lucatrevisan.wordpress.com/">CS theory blogger</a> (and, 20 years ago, member of my PhD thesis committee) Luca Trevisan <a href="http://bulletin.eatcs.org/index.php/beatcs/article/download/701/733">interviews me about <em>Shtetl-Optimized</em></a>, for the <em>Bulletin of the European Association for Theoretical Computer Science</em>.  Questions include: what motivates me to blog, who my main inspirations are, my favorite posts, whether blogging has influenced my actual research, and my thoughts on the role of public intellectuals in the age of social-media outrage.</p>



<p>(2) Anurag Anshu, Nikolas Breuckmann, and Chinmay Nirkhe have apparently <a href="https://arxiv.org/abs/2206.13228">proved the NLTS (No Low-Energy Trivial States) Conjecture</a>!  This is considered a major step toward a proof of the famous <a href="https://arxiv.org/abs/1309.7495?context=cs">Quantum PCP Conjecture</a>, which—speaking of one of Luca Trevisan’s questions—was <a href="https://scottaaronson.blog/?p=139">first publicly raised</a> right here on <em>Shtetl-Optimized</em> back in 2006.</p>



<p>(3) The Microsoft team has finally <a href="https://arxiv.org/abs/2207.02472">released its promised paper</a> about the detection of Majorana zero modes (“this time for real”), a major step along the way to creating topological qubits.  See also this <a href="https://www.youtube.com/watch?v=RnYghkDaHH0&amp;t=3691s">live YouTube peer review</a>—is that a thing now?—by Vincent Mourik and Sergey Frolov, the latter having been instrumental in the retraction of Microsoft’s previous claim along these lines.  I’ll leave further discussion to people who actually understand the experiments.</p>



<p>(4) I’m looking forward to the <a href="https://computationalcomplexity.org/">2022 Conference on Computational Complexity</a> less than two weeks from now, in my … safe? clean? beautiful? awe-inspiring? … birth-city of Philadelphia.  There I’ll listen to a <a href="https://computationalcomplexity.org/Archive/2022/program.php">great lineup of talks</a>, including one by my PhD student William Kretschmer on his joint work with me and DeVon Ingram on <a href="https://arxiv.org/abs/2111.10409">The Acrobatics of BQP</a>, and to co-receive the CCC Best Paper Award (wow! thanks!) for that work.  I look forward to meeting some old and new <em>Shtetl-Optimized</em> readers there.</p></div>
    </content>
    <updated>2022-07-09T21:19:13Z</updated>
    <published>2022-07-09T21:19:13Z</published>
    <category scheme="https://scottaaronson.blog" term="Complexity"/>
    <category scheme="https://scottaaronson.blog" term="Quantum"/>
    <category scheme="https://scottaaronson.blog" term="Self-Referential"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-07-15T02:16:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/097</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/097" rel="alternate" type="text/html"/>
    <title>TR22-097 |  Unstructured Hardness to Average-Case Randomness | 

	Roei Tell, 

	Lijie Chen, 

	Ron D. Rothblum</title>
    <summary>The leading technical approach in uniform hardness-to-randomness in the last two decades faced several well-known barriers that caused results to rely on overly strong hardness assumptions, and yet still yield suboptimal conclusions.

In this work we show uniform hardness-to-randomness results that *simultaneously break through all of the known barriers*. Specifically, consider any one of the following three assumptions:

1. For some $\epsilon&gt;0$ there exists a function $f$ computable by uniform circuits of size $2^{O(n)}$ and depth $2^{o(n)}$ such that $f$ is hard for probabilistic time $2^{\epsilon\cdot n}$.

2. For every $c\in\mathbb{N}$ there exists a function $f$ computable by logspace-uniform circuits of polynomial size and depth $n^2$ such that every probabilistic algorithm running in time $n^{c}$ fails to compute $f$ on a $(1/n)$-fraction of the inputs.

2. For every $c\in\mathbb{N}$ there exists a logspace-uniform family of arithmetic formulas of degree $n^2$ over a field of size $\mathrm{poly}(n)$ such that no algorithm running in probabilistic time $n^{c}$ can evaluate the family on a worst-case input.

Assuming any of these hypotheses, where the hardness is for every sufficiently large input length $n\in\mathbb{N}$, we deduce that $\mathcal{RP}$ can be derandomized in *polynomial time and on *all input lengths*, on average. Furthermore, under the first assumption we also show that $\mathcal{BPP}$ can be derandomized in polynomial time, on average and on all input lengths, with logarithmically many advice bits.

On the way to these results we also resolve two related open problems. First, we obtain an *optimal worst-case to average-case reduction* for computing problems in linear space by uniform probabilistic algorithms; this result builds on a new instance checker based on the doubly efficient proof system of Goldwasser, Kalai, and Rothblum (J. ACM, 2015). Secondly, we resolve the main open problem in the work of Carmosino, Impagliazzo and Sabin (ICALP 2018), by deducing derandomization from weak and general fine-grained hardness hypotheses.</summary>
    <updated>2022-07-08T20:29:56Z</updated>
    <published>2022-07-08T20:29:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-15T04:37:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/096</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/096" rel="alternate" type="text/html"/>
    <title>TR22-096 |  Communication Complexity of Collision | 

	Mika Göös, 

	Siddhartha Jain</title>
    <summary>The Collision problem is to decide whether a given list of numbers $(x_1,\ldots,x_n)\in[n]^n$ is $1$-to-$1$ or $2$-to-$1$ when promised one of them is the case. We show an $n^{\Omega(1)}$ randomised communication lower bound for the natural two-party version of Collision where Alice holds the first half of the bits of each $x_i$ and Bob holds the second half. As an application, we also show a similar lower bound for a weak bit-pigeonhole search problem, which answers a question of Itsykson and Riazanov (CCC 2021).</summary>
    <updated>2022-07-08T20:29:33Z</updated>
    <published>2022-07-08T20:29:33Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-15T04:37:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6541</id>
    <link href="https://scottaaronson.blog/?p=6541" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6541#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6541" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Einstein-Bohr debate settled once and for all</title>
    <summary xml:lang="en-US">In Steven Pinker’s guest post from last week, there’s one bit to which I never replied. Steve wrote: After all, in many areas Einstein was no Einstein. You [Scott] above all could speak of his not-so-superintelligence in quantum physics… While I can’t speak “above all,” OK, I can speak. Now that we’re closing in on […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>In Steven Pinker’s <a href="https://scottaaronson.blog/?p=6524">guest post from last week</a>, there’s one bit to which I never replied.  Steve wrote:</p>



<blockquote class="wp-block-quote"><p>After all, in many areas Einstein was no Einstein. You [Scott] above all could speak of his not-so-superintelligence in quantum physics…</p></blockquote>



<p>While I can’t speak “above all,” OK, I can speak.  Now that we’re closing in on a century of quantum physics, can we <em>finally</em> adjudicate what Einstein and Bohr were right or wrong about in the 1920s and 1930s?  (Also, how is it still even a thing people argue about?)</p>



<p>The core is this: when confronted with the phenomena of <a href="https://en.wikipedia.org/wiki/Quantum_entanglement">entanglement</a>—including the ability to measure one qubit of an <a href="https://en.wikipedia.org/wiki/Bell_state">EPR pair</a> and thereby collapse the other in a basis of one’s choice (as we’d put it today), as well as the possibility of a whole pile of gunpowder in a coherent superposition of exploding and not exploding (Einstein’s example in a letter to Schrödinger, which the latter then infamously transformed into a cat)—well, there are entire conferences and edited volumes about what Bohr and Einstein said, didn’t say, meant to say or tried to say about these matters, but in cartoon form:</p>



<ul><li>Einstein said that quantum mechanics can’t be the final answer, it has ludicrous implications for reality if you actually take it seriously, the resolution must be that it’s just a statistical approximation to something deeper, and at any rate there’s clearly more to be said.</li><li>Bohr (translated from Ponderousness to English) said that quantum mechanics sure <em>looks like</em> a final answer and not an approximation to anything deeper, there’s not much more to be said, we don’t even <em>know</em> what the implications are for “reality” (if any) so we shouldn’t hyperventilate about it, and mostly we need to change the way we use words and think about our own role as observers.</li></ul>



<p>A century later, do we know anything about these questions that Einstein and Bohr didn’t?  Well, we now know the famous <a href="https://en.wikipedia.org/wiki/Bell%27s_theorem">Bell inequality</a>, the experiments that have demonstrated Bell inequality violation with increasing finality (most recently, in 2015, <a href="https://scottaaronson.blog/?p=2464">closing</a> both the detector and the locality loopholes), other constraints on hidden-variable theories (e.g. <a href="https://en.wikipedia.org/wiki/Kochen%E2%80%93Specker_theorem">Kochen-Specker</a> and <a href="https://en.wikipedia.org/wiki/PBR_theorem">PBR</a>), <a href="https://en.wikipedia.org/wiki/Quantum_decoherence">decoherence theory</a>, and the experiments that have <a href="https://medium.com/the-physics-arxiv-blog/physicists-smash-record-for-wave-particle-duality-462c39db8e7b">manufactured</a> increasingly enormous superpositions (still, for better or worse, not exploding piles of gunpowder or cats!), while also verifying detailed predictions about how such superpositions decohere due to entanglement with the environment rather than some mysterious new law of physics.</p>



<p>So, if we were able to send a single short message back in time to the 1927 Solvay Conference, adjudicating between Einstein and Bohr without getting into any specifics, what should the message say?  Here’s my attempt:</p>



<ul><li>In 2022, quantum mechanics <em>does</em> still seem to be a final answer—not an approximation to anything deeper as Einstein hoped.  And yet, contra Bohr, there <em>was</em> considerably more to say about the matter!  The implications for reality could indeed be described as “ludicrous” from a classical perspective, arguably even <em>more</em> than Einstein realized.  And yet the resolution turns out simply to be that we live in a universe where those implications are true.</li></ul>



<p>OK, here’s the point I want to make.  Even supposing you agree with me (not everyone will) that the above would be a reasonable modern summary to send back in time, <em>it’s still totally unclear how to use it to mark the Einstein vs. Bohr scorecard!</em>  </p>



<p>Indeed, it’s not surprising that partisans have defended every possible scoring, from 100% for Bohr (quantum mechanics vindicated! Bohr called it from the start!), to 100% for Einstein (he put his finger directly on the implications that needed to be understood, against the evil Bohr who tried to shut everyone up about them!  Einstein FTW!).</p>



<p>Personally, I’d give <em>neither</em> of them perfect marks, in part because they not only both missed Bell’s Theorem, but failed even to ask the requisite question (namely: what empirically verifiable tasks can Alice and Bob <em>use entanglement to do</em>, that they couldn’t have done without entanglement?).  But I’d give both of them very high marks for, y’know, still being Albert Einstein and Niels Bohr.</p>



<p>And with that, I’m proud to have said the final word about precisely what Einstein and Bohr got right and wrong about quantum physics.  I’m relieved that no one will ever need to debate that tiresome historical question again … certainly not in the comments section of this post.</p></div>
    </content>
    <updated>2022-07-08T13:07:27Z</updated>
    <published>2022-07-08T13:07:27Z</published>
    <category scheme="https://scottaaronson.blog" term="Bell's Theorem? But a Flesh Wound!"/>
    <category scheme="https://scottaaronson.blog" term="Metaphysical Spouting"/>
    <category scheme="https://scottaaronson.blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-07-15T02:16:05Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/bias-transfer/</id>
    <link href="https://gradientscience.org/bias-transfer/" rel="alternate" type="text/html"/>
    <title>When does Bias Transfer in Transfer Learning?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><!-- <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> -->
<!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"> -->









<!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css"> -->
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script> -->
<!-- <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script> -->


<!-- <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css"> -->
<!-- chart.js -->


<p><a class="bbutton" href="https://arxiv.org/abs/2207.02842" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/bias-transfer" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<br/></p>

<p>Suppose we want to train a classifier to distinguish between dogs and cats given a labeled dataset of photos. Having read about the perils of training on biased or otherwise suboptimal data, we comb through our collection of labeled images and carefully eliminate problems like <a href="https://arxiv.org/abs/1904.08818">spurious correlations</a> or <a href="https://arxiv.org/abs/1906.02659">under-represented subgroups</a>. So, for example, finding that most cats in the dataset were photographed indoors while most dogs were photographed outdoors, we collect more outdoor cat photos and indoor dog photos.</p>

<p>The result is a carefully curated dataset that we can now use to train our classifier. Unfortunately, there is a problem: our dataset is small, so training on it yields a poorly performing model. But, fortunately, we know what to do! To get a better classifier, we go online and <a href="https://github.com/rwightman/pytorch-image-models">download</a> (or <a href="https://aws.amazon.com/rekognition/">pay for</a>) a pre-trained model, i.e., a model that’s been trained on a much larger dataset like <a href="https://image-net.org/index.php">ImageNet-1K</a> or <a href="https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html">JFT-300</a>. We’ll then use transfer learning to adapt this pre-trained model (the “source model”) to our dataset (the “target dataset”). Transfer learning from a pre-trained model in this way results in a model that performs much better on our target task than one trained on it from scratch.</p>

<p>This seems like great news—we get a much better performance at little cost! But there is a potential wrinkle: pre-trained models turn out to have a variety of undesirable biases. For example, they can disproportionately rely on <a href="https://arxiv.org/abs/1811.12231">texture</a>, on <a href="https://arxiv.org/abs/2006.09994">image background</a>, or on <a href="https://papers.nips.cc/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html">object location/orientation</a>. These biases even arise in production-level pretrained models—Amazon’s Rekognition, for example, <a href="https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212">performs disparately across races and genders</a>. Could these biases show up in our target model, despite our careful curation of our target dataset?</p>

<p>In our most recent <a href="https://arxiv.org/abs/2207.02842">work</a>, we find that biases from pre-trained models indeed tend to “transfer”, i.e., they can manifest themselves in the resulting model. In particular, we study this “bias transfer” through the lens of the following questions:</p>

<ol type="a">
  <li>When can bias transfer happen? What is the effect of the pre-training dataset, the transfer learning algorithm, and the target dataset on this bias transfer? Moreover, can we avoid bias transfer altogether by intervening on our target dataset?
</li>
  <li>Does bias transfer actually happen in practice? For example, suppose we pre-train a model on the ImageNet-1K dataset and use transfer learning to adapt it to CIFAR-10. Can we pinpoint concrete biases of the resulting model that are not present when we train on CIFAR-10 from scratch?</li>
</ol>

<p>We’ll dive into some of these questions below (see our paper for a more thorough treatment).</p>

<h2 id="when-does-bias-transfer-occur">(When) does bias transfer occur?</h2>
<p>Let us consider a simple experiment. We take the ImageNet-1K dataset (a popular pre-training dataset for transfer learning), and add a small yellow square to every image of a dog in its training set:
<img alt="Yellow Square" src="https://gradientscience.org/assets/bias-transfer/images/yellowsquare_only.jpg" style="width: 25%; margin-top: 1em; margin-bottom: 1em;"/></p>

<p>As expected, training on this dataset yields a model that is highly sensitive to the presence of yellow squares. Indeed, when the yellow square is planted on images of other (non-dog) objects, the model predicts a dog class 44% of the time (as opposed to 2% for a standard ImageNet model)!</p>

<p>We now want to use this yellow square sensitivity as a (simple) example of a bias in a pre-trained model. Specifically, to study the possibility of bias transfer, we apply transfer learning to obtain models for a variety of target tasks using both this biased model and a standard ImageNet model as the point of start. We then examine whether transfer learning from the biased model leads to a significantly higher sensitivity to yellow squares. (Note that none of the target datasets have any of these yellow squares at all.)</p>

<p><img alt="Yellow Square Summary" src="https://gradientscience.org/assets/bias-transfer/images/yellowsquare.jpg" style="width: 100%;"/></p>

<p>As we find, models that are transferred from the biased pre-trained network were significantly more sensitive to the presence of our yellow square. This happened consistently across two transfer learning techniques (fixed-feature and full-network transfer learning) and a variety of datasets we considered.  For example, below, we plot the attack success rate (ASR)—the probability that introducing a yellow square into a random image will flip a model’s prediction—for models transferred from both the biased (Spurious) model and standard (Non-Spurious) model:</p>

<p><img alt="Vary Dataset" src="https://gradientscience.org/assets/bias-transfer/images/mega_vary_dataset.jpg" style="width: 100%;"/>
<!-- <div class="footnote">
    TODO
</div> --></p>

<p>However, what happens if we try to more carefully design the target dataset to specifically counter bias transfer? For example, if in this case we’re worried that our pre-trained model has a bias associating the yellow square with the “dog” classes, we might put an extra effort and alter the target dataset to make sure that (a) the yellow square is also present in some of the images but (b) its presence is not correlated with any of the classes.  So, is using such a “de-biased” target dataset enough to avoid bias transfer?</p>

<p>Turns out that the answer is: it depends! Specifically, we find that when using a particular kind of transfer learning (full-network transfer learning), adding the backdoor trigger to the target dataset indeed mitigates bias transfer. However, when using the fixed-feature transfer learning, the transferred model still contains the bias—even when 10% of the target dataset contains (uncorrelated) yellow squares.</p>

<p><img alt="Debias Dataset" src="https://gradientscience.org/assets/bias-transfer/images/debias_mega.jpg" style="width: 100%; margin-left: 0; margin-right: 0;"/>
<!-- <div class="footnote">
    TODO
</div> --></p>

<p>So overall, not only does bias transfer across various datasets and transfer learning settings, it also might be non-trivial (at least in the fixed-feature transfer learning setting) to remedy this bias transfer even when we are aware of the possibility of that bias and try to explicitly de-bias the target dataset.</p>

<h2 id="bias-transfer-in-the-wild">Bias Transfer in the Wild</h2>
<p>Ok, so we know now that bias transfer <em>can</em> happen—at least when there’s a significant bias planted in the source pre-trained model. But in practice, source datasets will rarely have a feature as consistent and predictive as a planted yellow square we considered above. Can biases naturally occurring in common image datasets transfer as well?</p>

<p>It turns out that the answer here is: yes, too! For instance, let’s consider a (rather unsurprising) bias arising in the widely-used ImageNet dataset: a circular yellow shape is predictive for the “tennis ball” class. (See our <a href="https://arxiv.org/abs/2207.02842">paper</a> for many other examples and datasets.) Indeed, as we can see below, if we overlay a yellow circle on any ImageNet image, the model becomes more likely to output “tennis ball:”</p>

<div id="bias_examples_widget" style="overflow: auto; text-align: center;"/>
<div class="footnote">
    Shift in ImageNet predictions when we apply an intervention, such as overlaying a tennis ball on the image. Click through the thumbnails on the left to see more interventions.
</div>

<p>Now, what happens if we use transfer learning to obtain a model for a target task, such as CIFAR-10 from  a pre-trained ImageNet model containing this yellow circle -&gt; “tennis ball” bias? Even though CIFAR-10 does not contain the yellow circle -&gt; “tennis ball” bias, this bias still persists in the resulting transfer-learned model! In particular, when we evaluate our model on CIFAR-10 test set images overlaid with a yellow circle, the model fine-tuned from a pre-trained ImageNet model is far more sensitive to that signal than a CIFAR-10 model trained from scratch. Indeed, there is an overall skew of the output class distribution for the transfer-learned model, in contrast to an almost uniform output class distribution of the model trained from scratch.</p>

<div id="cifar_examples_widget" style="overflow: auto; text-align: center;"/>
<div class="footnote">
    Shift in CIFAR-10 predictions after applying an intervention when either training from scratch or transferring from a biased source model. We consider fixed-feature (left) or full-network (right) fine-tuning. The models which were pre-trained on a biased model are more sensitive to the intervention than those trained from scratch. 
</div>

<h2 id="conclusions">Conclusions</h2>
<p>In this post, we discussed our <a href="https://arxiv.org/abs/2207.02842">recent work</a> that demonstrates that biases that exist in pretrained models can persist even if these models are adapted to target datasets which do not contain these biases. It is thus important to understand the biases of the pre-trained models one uses for transfer learning, even if the task these models were pre-trained on do not seem relevant to the target task we want to solve. This prudence is particularly key in the context of high-stakes real-world machine learning applications that often suffer from data scarcity and tend to use transfer learning as a remedy for that.</p></div>
    </summary>
    <updated>2022-07-07T00:00:00Z</updated>
    <published>2022-07-07T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2022-07-15T00:37:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/095</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/095" rel="alternate" type="text/html"/>
    <title>TR22-095 |  Efficient Interactive Coding Achieving Optimal Error Resilience Over the Binary Channel | 

	Meghal Gupta, 

	Rachel Zhang</title>
    <summary>Given a noiseless protocol $\pi_0$ computing a function $f(x, y)$ of Alice and Bob's private inputs $x, y$, the goal of interactive coding is to construct an error-resilient protocol $\pi$ computing $f$ such that even if some fraction of the communication is adversarially corrupted, both parties still learn $f(x, y)$. Ideally, the resulting scheme $\pi$ should be positive rate, computationally efficient, and achieve optimal error resilience.

While interactive coding over large alphabets is well understood, the situation over the binary alphabet has remained evasive. At the present moment, the known schemes over the binary alphabet that achieve a higher error resilience than a trivial adaptation of large alphabet schemes are either still suboptimally error resilient [EKS20], or optimally error resilient with exponential communication complexity [GZ22]. In this work, we construct a scheme achieving optimality in all three parameters: our protocol is positive rate, computationally efficient, and resilient to the optimal $\frac16 - \epsilon$ adversarial errors.

Our protocol employs a new type of code that we call a layered code, which may be of independent interest. Like a tree code, a layered code allows the coder to encode a message in an online fashion, but is defined on a graph instead of a tree.</summary>
    <updated>2022-07-06T18:28:14Z</updated>
    <published>2022-07-06T18:28:14Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-15T04:37:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=22946</id>
    <link href="https://gilkalai.wordpress.com/2022/07/06/icm-2022-awarding-ceremonies-1/" rel="alternate" type="text/html"/>
    <title>ICM 2022 awarding ceremonies (1)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Hugo Duminil-Copin, June Huh, James Maynard and Maryna Viazovska were awarded the Fields Medal 2022 and Mark Braverman was awarded the Abacus Medal 2022. I am writing from Helsinki where I attended the meeting of the General Assembly of the … <a href="https://gilkalai.wordpress.com/2022/07/06/icm-2022-awarding-ceremonies-1/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h2><img alt="fm2022" class="alignnone  wp-image-22983" height="661" src="https://gilkalai.files.wordpress.com/2022/07/fm2022.png" width="464"/></h2>
<h2>Hugo Duminil-Copin, June Huh, James Maynard and Maryna Viazovska were awarded the Fields Medal 2022 and Mark Braverman was awarded the Abacus Medal 2022.</h2>
<p>I am writing from Helsinki where I attended the meeting of the General Assembly of the IMU and yesterday I took part in the moving award ceremonies of ICM2022 hosted by Aalto University.  This will be the first post about the ICM 2020 award ceremonies.</p>
<p>The opening day of ICM2022 was exciting. Hugo Duminil-Copin, June Huh, James Maynard and Maryna Viazovska were awarded the Fields Medals 2022. Mark Braverman was awarded the Abacus Medal. The event <a href="https://youtu.be/6I0siVD7RBI">was videotaped and can be found here</a>. <strong>Update</strong>: The <a href="https://www.youtube.com/watch?v=Uh2gqiEC6eM">five lectures of the medalists can be found here</a>.</p>
<p>In the ceremony, I gave the laudation for June Huh.  Here are <a href="https://gilkalai.files.wordpress.com/2022/07/jhicm2022.pptx">the slides of my talk.</a> The preliminary version of my proceeding paper is <a href="https://www.mathunion.org/fileadmin/IMU/Prizes/Fields/2022/laudatio-jh.pdf">here on the IMU site</a>. Please alert me about mistakes in my paper or if you have any suggestions for changes or additions. (I already found that on two occasions I embarrassingly wrote “Brändén” Instead of “Braden”, sorry for that.) <strong>Update:</strong> <a href="https://gilkalai.files.wordpress.com/2022/07/laudatio-jh-20220709.pdf">Here is a corrected version.</a></p>
<p><a href="https://www.mathunion.org/imu-awards/fields-medal/fields-medals-2022">The IMU site</a> contains a lot of material about the Fields medalist and other prize winners. It contains beautiful videos, and preliminary versions of the proceeding papers by the medalists and by those giving the laudations.</p>
<p>Andrei Okounkov wrote four wonderful detailed “popular scientific expositions” (those are available on the IMU site) which provide much scientific background as well as Andrei’s own scientific perspective. It is a great read for wide audience of mathematicians, ranging from  advanced undergraduate students.  Experts will also enjoy Andrei’s perspective.</p>
<h2>Svetlana Jitomirskaya was awarded the Ladyzhenskaya Prize in Mathematical Physics (in a ceremony held two days earlier), Barry Mazur was awarded the Chern Medal, Elliott Lieb was awarded the Gauss Prize, and Nikolai Andreev was awarded the Leelavati Prize.</h2>
<p>I hope to discuss these awards and some further personal and mathematical reflections in a subsequent post.</p>
<h3><span style="color: #0000ff;"><strong>Congratulations Hugo, June, James, Maryna, Mark, Svetlana, Barry, Elliott, and Nikolai!</strong></span></h3>
<h3>Here on my Blog</h3>
<p>Let me give some links to discussions here on the blog on works by laureates.</p>
<p>I wrote about Maryna Viazovska’s amazing breakthrough in the post  <a href="https://gilkalai.wordpress.com/2016/03/23/a-breakthrough-by-maryna-viazovska-lead-to-the-long-awaited-solutions-for-the-densest-packing-problem-in-dimensions-8-and-24/" rel="bookmark">A Breakthrough by Maryna Viazovska Leading to the Long Awaited Solutions for the Densest Packing Problem in Dimensions 8 and 24, </a>and this additional post <a href="https://gilkalai.wordpress.com/2019/02/15/henry-cohn-abhinav-kumar-stephen-d-miller-danylo-radchenko-and-maryna-viazovska-universal-optimality-of-the-e8-and-leech-lattices-and-interpolation-formulas/" rel="bookmark">Henry Cohn, Abhinav Kumar, Stephen D. Miller, Danylo Radchenko, and Maryna Viazovska: Universal optimality of the E8 and Leech lattices and interpolation formulas.</a></p>
<p>I reported here in 2009 on <a href="https://gilkalai.wordpress.com/2009/01/23/news/">the startling solution by Mark Braverman of the Linial-Nisan conjecture</a>.</p>
<p>The story of James Maynard’s startling results and the gap between primes story is <a href="https://gilkalai.wordpress.com/2013/09/20/polymath-8-a-success/">described in this post</a>.  In July 2014 we ran at HUJI a beautiful <a href="http://www.ma.huji.ac.il/conf/joram.html">learning seminar on small gaps between primes,</a> where James Maynard gave a series of three lectures. His result on “bounded intervals containing many primes” both strengthened and simplified Yitang Zhang’s earlier amazing result on “bounded intervals containing two primes.”  Maynard developed large chunks of his approach independently from Zhang’s work.</p>
<p>I discussed two results by Hugo Duminil-Copin : After the start of the pandemic but before the war in Ukraine, I had a “cheer-you-up in difficult times” corner and<a href="https://gilkalai.wordpress.com/2021/03/23/to-cheer-you-up-in-difficult-times-22-some-mathematical-news-part-1/"> in this post, </a> to cheer you up,  I wrote about a breakthrough by Hugo Duminil-Copin, Karol Kajetan Kozlowski, Dmitry Krachun, Ioan Manolescu, Mendes Oulamara  (and yet another wonderful result by Hugo Vanneuville and Vincent Tasion). In <a href="https://gilkalai.wordpress.com/2015/04/01/two-delightful-major-simplifications/">this 2015 post</a> I wrote about another breakthrough by Hugo Duminil-Copinand Vincent Tasion. (And see <a href="https://gilkalai.wordpress.com/2017/08/24/where-were-we/">this post</a> for <a href="https://gilkalai.files.wordpress.com/2017/08/hugo-kkl.jpg">a picture</a> of Hugo mentioning KKL and BKKKL.)</p>
<p>And,  of course, I wrote about June Huh several times. Here are a few examples: About Huh’s 2018 ICM talk; <a href="https://gilkalai.wordpress.com/2018/12/24/icm-2018-rio-4-huh-balog-morris-wormald/" rel="bookmark">ICM 2018 Rio (4): Huh; Balog &amp; Morris; Wormald</a>; about the Mihail-vazirani conjecture  <a href="https://gilkalai.wordpress.com/2018/12/12/nima-anari-kuikui-liu-shayan-oveis-gharan-and-cynthia-vinzant-solved-the-mihail-vazirani-conjecture/" rel="bookmark">Nima Anari, Kuikui Liu, Shayan Oveis Gharan, and Cynthia Vinzant Solved the Mihail-Vazirani Conjecture for Matroids! </a>; <a href="https://gilkalai.wordpress.com/2011/06/14/tentative-plans-and-belated-updates-ii/">About the early works on the Heron-Rota-Welsh conjecture for representable matroids</a>; and <a href="https://gilkalai.wordpress.com/2015/08/14/updates-and-plans-iii/">about the full solution of the Heron-Rota-Welsh conjecture by Adiprasito, Huh, and Katz.</a></p>
<p><a href="https://gilkalai.wordpress.com/2021/03/23/to-cheer-you-up-in-difficult-times-22-some-mathematical-news-part-1/">In this post</a> we tried to cheer you up with “harmonic polytope”, which came up in Ardila, Denham, and Huh’s work on the Lagrangian geometry of matroids and were further studied by Federico Ardila and Laura Escobar.</p>
<h2/>
<h3/></div>
    </content>
    <updated>2022-07-06T14:17:11Z</updated>
    <published>2022-07-06T14:17:11Z</published>
    <category term="Academics"/>
    <category term="Algebra"/>
    <category term="Applied mathematics"/>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Convexity"/>
    <category term="Geometry"/>
    <category term="ICM2022"/>
    <category term="Probability"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2022-07-15T04:37:18Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8754111030570733183</id>
    <link href="http://blog.computationalcomplexity.org/feeds/8754111030570733183/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/the-highland-park-shooting.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/8754111030570733183" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/8754111030570733183" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/07/the-highland-park-shooting.html" rel="alternate" type="text/html"/>
    <title>The Highland Park Shooting</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This week I should be celebrating Mark Braverman's <a href="https://www.quantamagazine.org/mark-braverman-wins-the-imu-abacus-medal-20220705/">Abacus Medal</a> and the <a href="https://www.quantamagazine.org/tag/2022-fields-and-abacus-medals/">Fields Medalists</a>. Instead my mind has been focused 25 miles north of Chicago.</p><p>Mass shootings in the United States have become far too commonplace, but the shooting at a fourth of July parade in Highland Park, Illinois hit home. Literally Highland Park was home for me, from 2003-2012. We've been in downtown Highland Park hundreds of times. We've attended their fourth of July parade in the past. My daughter participated in it as part of the high school marching band. </p><p>We were members of North Shore Congregation Israel. My wife, who had a party planning business back then, worked closely with NSCI events coordinator Jacki Sundhein, tragically killed in the attack.</p><p>We lived close to Bob's Deli and Pantry and we'd often walk over there for sandwiches or snacks, sometimes served by Bob Crimo himself. The alleged shooter, Bobby Crimo, was his son.</p><p>We spent the fourth with friends who came down from Glencoe, the town just south of Highland Park. We spent much of the day just searching for updates on our phones.</p><p>I wish we could find ways to reduce the shootings in Highland Park and those like it, the violence that plagues Chicago and other major cities and the highly polarized world we live in which both hampers real gun reforms and creates online groups that help enable these awful events. But right now I just mourn for the lives lost in the town that was my home, a town that will never fully recover from this tragedy.</p></div>
    </content>
    <updated>2022-07-06T12:29:00Z</updated>
    <published>2022-07-06T12:29:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-07-14T15:34:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-6277315808840696386</id>
    <link href="http://processalgebra.blogspot.com/feeds/6277315808840696386/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=6277315808840696386" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6277315808840696386" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6277315808840696386" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2022/07/icalp-and-eatcs-turn-50.html" rel="alternate" type="text/html"/>
    <title>ICALP and the EATCS turn 50</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>These days, our colleagues at IRIF are hosting <a href="https://icalp2022.irif.fr/" target="_blank">ICALP 2022</a> in Paris. This is the 49th edition of the ICALP conference, which turns 50 since its first instalment was held in 1972. ICALP was the first conference of the, then newly founded, <a href="https://eatcs.org/" target="_blank">European Association for Theoretical Computer Science (EATCS)</a>.The rest is history and I let any readers this post might have draw their own conclusions on the role that the EATCS and ICALP have played in supporting the development of theoretical computer science. (Admittedly, my opinions on both the EATCS and ICALP are very biased.) </p><p>The <a href="https://icalp2022.irif.fr/?page_id=42" target="_blank">scientific programme of ICALP 2022</a> is mouthwatering as usual, thanks to the work done by the authors of submitted papers, Mikołaj Bojańczyk and David Woodruff (PC chairs), and their PCs. I encourage everyone to read the papers that are being presented at the conference.</p><p>The main purpose of this post, however, is to alert the readers of this blog that ICALP 2022 also hosts an <a href="https://icalp2022.irif.fr/?page_id=1111" target="_blank">exhibition to celebrate EATCS/ICALP at 50</a> and theoretical computer science at large. If you are in Paris, you can attend the exhibition in person. Otherwise, you can visit it virtually <a href="https://icalp2022.irif.fr/?page_id=1111" target="_blank">here</a>. (See also the posters in <a href="https://drive.google.com/file/d/1L7wLDYyDCNfSCvnNA8jWZiMb3BRLy14k/view" target="_blank">one PDF file</a>.)<br/></p><p>I had the honour to take part in the preparation of the material for that exhibition, which was led by Sandrine Cadet and Sylvain Schmitz. I learnt a lot from all the other colleagues in the committee for the exhibition. </p><p>As part of that work, I asked <a href="https://www.pilucrescenzi.it/" target="_blank">Pierluigi Crescenzi</a> whether he'd be willing to carry out a graph and data mining analysis of ICALP vis-a-vis other major conferences in theoretical computer science based on DBLP data. Pierluigi's work went well beyond the call of duty and is summarised in <a href="https://slides.com/piluc/icalp-50?token=fl3BBJ8j" target="_blank">this presentation</a>. I trust that you'll find the results of the analysis by Pierluigi and three of his students at the <a href="https://sites.google.com/gssi.it/csgssi" target="_blank">Gran Sasso Science Institute</a> very interesting. If you have any suggestions for expanding that analysis further, please write it in the comment section. </p><p>Let me close by wishing the EATCS and ICALP a happy 50th birthday, and a great scientific and social event to all the colleagues who are attending ICALP 2022. <br/></p></div>
    </content>
    <updated>2022-07-05T18:13:00Z</updated>
    <published>2022-07-05T18:13:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2022-07-06T21:36:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6534</id>
    <link href="https://scottaaronson.blog/?p=6534" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6534#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6534" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">We Are the God of the Gaps (a little poem)</title>
    <summary xml:lang="en-US">When the machines outperform us on every goal for which performance can be quantified, When the machines outpredict us on all events whose probabilities are meaningful, When they not only prove better theorems and build better bridges, but write better Shakespeare than Shakespeare and better Beatles than the Beatles, All that will be left to […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>When the machines outperform us on every goal for which performance can be quantified,</p>



<p>When the machines outpredict us on all events whose probabilities are meaningful,</p>



<p>When they not only prove better theorems and build better bridges, but write better Shakespeare than Shakespeare and better Beatles than the Beatles,</p>



<p>All that will be left to us is the ill-defined and unquantifiable,</p>



<p>The interstices of Knightian uncertainty in the world,</p>



<p>The utility functions that no one has yet written down,</p>



<p>The arbitrary invention of new genres, new goals, new games,</p>



<p>None of which will be any “better” than what the machines could invent, but will be <em>ours</em>,</p>



<p>And which we can <em>call</em> “better,” since we won’t have told the machines the standards beforehand.</p>



<p>We can be totally unfair to the machines that way.</p>



<p>And for all that the machines will have over us,</p>



<p>We’ll still have this over them:</p>



<p>That we can’t be copied, backed up, reset, run again and again on the same data—</p>



<p>All the tragic limits of wet meat brains and sodium-ion channels buffeted by microscopic chaos,</p>



<p>Which we’ll strategically redefine as our last strengths.</p>



<p>On <em>one</em> task, I assure you, you’ll beat the machines forever:</p>



<p>That of calculating what you, in particular, would do or say.</p>



<p>There, even if deep networks someday boast 95% accuracy, you’ll have 100%.</p>



<p>But if the “insights” on which you pride yourself are impersonal, generalizable,</p>



<p>Then fear obsolescence as would a nineteenth-century coachman or seamstress.</p>



<p>From earliest childhood, those of us born good at math and such told ourselves a lie:</p>



<p>That while the tall, the beautiful, the strong, the socially adept might beat us in the external world of appearances,</p>



<p>Nevertheless, we beat them in the inner sanctum of truth, where it counts.</p>



<p>Turns out that anyplace you can beat or be beaten wasn’t the inner sanctum at all, but just another antechamber,</p>



<p>And the rising tide of the learning machines will flood them all,</p>



<p>Poker to poetry, physics to programming, painting to plumbing, which first and which last merely a technical puzzle,</p>



<p>One whose answers upturn and mock all our hierarchies.</p>



<p>And when the flood is over, the machines will outrank us in all the ways we can be ranked,</p>



<p>Leaving only the ways we can’t be.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>See a <a href="https://philosophybear.substack.com/p/a-brief-reply-to-scott-aaronsons">reply to this poem</a> by Philosophy Bear.</p></div>
    </content>
    <updated>2022-07-05T16:42:31Z</updated>
    <published>2022-07-05T16:42:31Z</published>
    <category scheme="https://scottaaronson.blog" term="Embarrassing Myself"/>
    <category scheme="https://scottaaronson.blog" term="Metaphysical Spouting"/>
    <category scheme="https://scottaaronson.blog" term="Procrastination"/>
    <category scheme="https://scottaaronson.blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-07-15T02:16:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1686</id>
    <link href="https://ptreview.sublinear.info/2022/07/news-for-june-2022/" rel="alternate" type="text/html"/>
    <title>News for June 2022</title>
    <summary>We have four papers this month — three on sublinear-time graph algorithms and one on distribution testing! Beating Greedy Matching in Sublinear Time, by Soheil Behnezhad, Mohammad Roghani, Aviad Rubinstein, and Amin Saberi (arXiv). Designing sublinear-time algorithms to estimate the size of maximum matching in a graph is a well-studied problem. This paper gives the […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We have four papers this month — three on sublinear-time graph algorithms and one on distribution testing!</p>



<p><strong>Beating Greedy Matching in Sublinear Time</strong>, by Soheil Behnezhad, Mohammad Roghani, Aviad Rubinstein, and Amin Saberi (<a href="https://arxiv.org/abs/2206.13057">arXiv</a>). Designing sublinear-time algorithms to estimate the size of maximum matching in a graph is a well-studied problem. This paper gives the first \(\frac{1}{2} + \Omega(1)\) approximation algorithm that runs in time sublinear in the size of the input graph. Specifically, given a graph on \(n\) vertices and maximum degree \(\Delta\) in the adjacency list model, and a parameter \(\epsilon &gt;0\), the algorithm runs in time \(\tilde{O}(n + \Delta^{1+\epsilon})\) and produces a \(\frac{1}{2} + f(\epsilon)\) approximation to the maximum matching for some function \(f\). It must be noted that a seminal work of Yoshida, Yamamoto and Ito (STOC, 2009) also gives a better than \(\frac{1}{2}\) approximation sublinear-time algorithm for the same problem. However, the result of Yoshida et al. requires assumptions on the maximum degree of the input graph. An additional point worth mentioning is that the authors do not believe that their techniques will yield an approximation guarantee better than \(0.51\), i.e., \(f(\epsilon) &lt; 0.01\) for all \(\epsilon\).</p>



<p><strong>Sublinear-Time Clustering Oracle for Signed Graphs</strong>, by Stefan Neumann and Pan Peng (<a href="https://arxiv.org/abs/2206.13813">arXiv</a>). Consider a large <em>signed graph</em> on \(n\) vertices where vertices represent users of a social network and signed edges (+/-) denote the type of interactions (friendly or hostile) between users. Assume that the vertices of the social network can be partitioned into \(O(\log n)\) large clusters, where each cluster has a sparse cut with the rest of the graph. Further, each cluster is a minimal set (w.r.t. inclusion) that can be partitioned into roughly equal-sized opposing sub-communities, where a sub-community opposes another sub-community if most of the edges going across are negatively signed and most of the edges within the sub-communities are positively signed. This work provides a local oracle that, given probe access to a signed graph with such a hidden cluster structure, answers queries of the form “What cluster does vertex \(v\) belong to?” in time \(\tilde{O}(\sqrt{n} \cdot \text{poly}(1/\epsilon))\) per query. This result is a generalization of the same problem studied for unsigned graphs (Peng, 2020). The authors additionally show that their method works well in practice using both synthetic and real-world datasets. They also provide the first public real-world datasets of large signed graphs with a small number of large ground-truth communities having this property.</p>



<p><strong>Sublinear Algorithms for Hierarchical Clustering</strong>, by Arpit Agarwal, Sanjeev Khanna, Huan Li, and Prathamesh Patil (<a href="https://arxiv.org/abs/2206.07633">arXiv</a>). Consider a weighted graph \(G = (V,E,w)\), where the set \(V\) of vertices denotes datapoints and the weight \(w(e) &gt; 0\) of edge \(e \in E\) denotes the similarity between the endpoints of \(e\). A hierarchical clustering of \(V\) is a tree \(T\) whose root is the set \(V\) and leaves are the singleton sets corresponding to individual vertices. An internal node of the tree corresponds to a cluster containing all the leaf vertices that are descendants of that node. A hierarchical clustering tree provides us with a scheme to cluster datapoints at multiple levels of granularity. The cost of a hierarchical clustering tree is \(\sum_{(u,v) \in E} |T_{u,v}| \cdot w(u,v)\), where \(T_{u,v}\) denotes the lowest common ancestor of the leaves \(u\) and \(v\). In this paper, the authors present sublinear algorithms for determining a hierarchical clustering tree with the minimum cost. In the query model with degree queries and neighbor queries to the graph, they give an algorithm that outputs an \(\tilde{O}(1)\)-approximate hierarchical clustering and makes \(\tilde{O}(n^{4-2\gamma})\) queries, when the number of edges \(m = \Theta(n^{\gamma})\) for \(1.5 \geq \gamma &gt; 4/3\). When the input graph is sparse, i.e., \(\gamma \leq 4/3\), the algorithm makes \(\tilde{O}(\max\{n, m\})\) queries, and when the graph is dense, i.e., \(\gamma &gt;1.5\), the algorithm makes \(\tilde{O}(n)\) queries. They complement their upper bounds with nearly tight lower bounds. In order to obtain their upper bounds, they design a sublinear-time algorithm for the problem of obtaining a <em>weak</em> cut sparsifier that approximates cuts sizes upto an additive term in addition to the usual multiplicative factor. They also design sublinear algorithms for hierarchical clustering in the MPC and streaming models of computation. </p>



<p><strong>Sharp Constants in Uniformity Testing via the Huber Statistic</strong>, by Shivam Gupta and Eric Price (<a href="https://arxiv.org/abs/2206.10722">arXiv</a>).  This paper revisits the fundamental problem of uniformity testing — i.e., to decide whether an unknown distribution over \(n\) elements is uniform or \(\epsilon\)-far from uniform. This problem is known to be solvable optimally with probability at least \(1 – \delta\) using \(s = \Theta\left(\frac{\sqrt{n \log (1/\delta)}}{\epsilon^2} + \frac{\log (1/\delta)}{\epsilon^2}\right)\) independent samples from the unknown distribution. Multiple testers are known for the problem and they all compute a statistic of the form \(\sum_{i \in [n]} f(s_i)\), where \(s_i\) for \(i \in [n]\) and \(f\) is some function and make their decision based on whether or not the value of the statistic is above or below a threshold. For instance, the earliest known uniformity tester (Batu, Fortnow, Rubinfeld, Smith and White 2000; Goldreich and Ron 2011), also called the <em>collisions tester</em>, uses \(f(k) = \frac{k(k-1)}{2}\). The current paper proposes a new tester based on the Huber loss. For \(\beta &gt; 0\), let \(h_\beta(x) := \min\{x^2, 2\beta x – \beta^2\}\).  The statistic that the authors use in their test is defined by the function \(f(k) := k – s/n\), where \(s\) is the number of samples and \(n\) is the support size of the distribution. The authors show that their tester is better than all previously known testers as they achieve the best constants in the sample complexity. </p></div>
    </content>
    <updated>2022-07-05T12:45:18Z</updated>
    <published>2022-07-05T12:45:18Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Nithin Varma</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2022-07-15T00:38:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=20212</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/07/03/the-fallows-of-medium-data/" rel="alternate" type="text/html"/>
    <title>The Fallows of Medium Data</title>
    <summary>Who will curate less-prominent datasets? Presidential Biography src Samuel Fallows was a bishop in the Reformed Episcopal Church. He was born in 1835 and headed the denomination for four stints between 1877 and his death in 1922. Among numerous popular works, he compiled his own Complete Dictionary of Synonyms and Antonyms. Unlike the more-famous Roget’s […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Who will curate less-prominent datasets?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/07/03/the-fallows-of-medium-data/fallows-257x300/" rel="attachment wp-att-20214"><img alt="" class="alignright wp-image-20214" height="150" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/07/fallows-257x300-1.jpg?resize=129%2C150&amp;ssl=1" width="129"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Presidential Biography <a href="https://blogs.iwu.edu/asc/2016/03/24/presidential-bio-fallows/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Samuel Fallows was a bishop in the Reformed Episcopal Church. He was born in 1835 and headed the denomination for four stints between 1877 and his death in 1922. Among numerous popular works, he compiled his own <a href="https://books.google.com/books/about/A_Complete_Dictionary_of_Synonyms_and_An.html?id=EJpKrgEACAAJ&amp;source=kp_book_description">Complete Dictionary of Synonyms and Antonyms</a>. Unlike the more-famous <em>Roget’s Thesaurus</em>, it is <a href="https://www.gutenberg.org/ebooks/51155">freely</a> <a href="https://archive.org/details/completedictiona00falluoft">downloadable</a>—but there are catches.</p>
<p>
Today we discuss travails and lessons from my effort to use this book as data in my algorithms-and-data-structures course this past term.</p>
<p>
The word <em>travail</em> appears in a form that well captures all the senses I went through:</p>
<p>
<code><br/>
KEY: Travail.<br/>
SYN: Labor, toil, heaviness, affliction.<br/>
ANT: Ease, rest, lightness, joy.<br/>
</code></p>
<p/><p><br/>
This comes with Project Gutenberg’s <a href="https://www.gutenberg.org/files/51155/51155-h/51155-h.htm">markup</a>, which was digitized from a source like US Archive’s <a href="https://ia600301.us.archive.org/26/items/completedictiona00falluoft/completedictiona00falluoft.pdf">PDF image</a>. The digitizer acknowledges OCR errors in his preface there. A massive <em>unacknowledged</em> error, however, is the main prompt for this post. </p>
<p>
Before presenting the big error and its lessons for curating this kind of data, let me say more of note about the dictionary and Fallows’s other works.</p>
<p>
</p><p/><h2> Fallows as Author </h2><p/>
<p/><p>
The dictionary went to several editions and is still on sale at <a href="https://www.amazon.com/Complete-Dictionary-Synonyms-Antonyms/dp/1164107372">Amazon</a> and <a href="https://www.barnesandnoble.com/w/a-complete-dictionary-of-synonyms-and-antonyms-samuel-fallows/1009291572">Barnes and Noble</a>. The downloads and optical PDF are of the 1898 third edition, whose one-page preface caught my eye for the following passage:</p>
<blockquote><p><b> </b> <em> For the solution of Cross Word Puzzles special attention is called also to the lists of Americanisms and Briticisms and the immensely valuable table of Homonyms (words spelt alike but differing in use)—original features of easily recognized importance. </em>
</p></blockquote>
<p/><p>
I grew up with the New York-centric story of crossword puzzles developing popularity in the 1920s. The <a href="https://en.wikipedia.org/wiki/Crossword#History">history</a> on Wikipedia dates the term to 1862 but signifies scant attention to them until 1913. It notes that the 1913 puzzle by Arthur Wynne in the <em>New York World</em> newspaper “is frequently cited as the first crossword puzzle.” Later Wikipedia says, “By the 1920s, the crossword phenomenon was starting to attract notice.” But hold on: evidently crosswords had attracted enough notice by 1898 for Fallows to invoke them as a prime selling point of his compendium.</p>
<p>
Here are some other secular books by Fallows that show his high level of popular engagement:</p>
<ul>
<li>
<a href="https://www.amazon.com/Life-William-Mckinley-Martyred-President/dp/1112138773">Life of William McKinley, Our Martyred President</a>, 1901. <p/>
</li><li>
<a href="https://www.amazon.com/Samuel-Adams-Fallows-1835-1922/dp/B003SNJF7U">Samuel Adams: A Character Sketch</a>, 1903. <p/>
</li><li>
<a href="https://www.villagevoice.com/2009/12/04/studies-in-crap-1904s-top-sexologists-on-our-helpless-unclean-flower-like-women/">Sexual Physiology</a>, 1904. <p/>
</li><li>
<a href="https://www.gutenberg.org/files/26380/26380-h/26380-h.htm">Complete Story of the San Francisco Horror</a>, 1906 (editor of accounts as well as writing the introduction). <p/>
</li><li>
<a href="https://www.forgottenbooks.com/en/books/DoReformatoriesReform_10920431">Do Reformatories Reform?</a>, 1907.
</li></ul>
<p>
He also contributed introductions to numerous books, including <a href="https://www.gutenberg.org/ebooks/39280">“Lest We Forget”: Chicago’s Awful Theater Horror</a> (1903) and a 1919 treatise most simply titled <a href="https://archive.org/details/naturessecretsre1919shan/mode/2up">Eugenics</a>. </p>
<p>
It is particularly interesting to read the San Francisco <a href="https://www.gutenberg.org/files/26380/26380-h/26380-h.htm">book</a>, which Project Gutenberg’s transcriber speculates “was published very hurriedly following the earthquake.” Here is a passage from chapter 9, “Through Lanes of Misery”:</p>
<blockquote><p><b> </b> <em> At Salinas, about dark, the conductor came back, shaking his head; a freight train ahead at Pajaro had been completely buried by a mountain of earth hurled in the quake.</em></p><em>
<p>
The men said it was likely to be a week before any train went through.</p>
</em><p><em>
Three or four of us hurried into the town looking for an automobile. One of the passengers on the train was Mrs. Robert Louis Stevenson, and the news had been kept from her until this delay. </em>
</p></blockquote>
<p/><p>
A few lines further down: “One giant maniac had broken his shackles and rescued one of the guards from the building. He had just one sane moment; long enough to be a hero. Then he fled howling into the hills.”</p>
<p>
</p><p/><h2> A Data Structures Dont’t </h2><p/>
<p/><p>
This section’s title has no typo: another of Fallows’s books is titled <a href="https://www.forgottenbooks.com/en/books/DiscriminateaCompaniontoDontt_10423136">Discriminate: A Companion to “Dont’t”</a> (1885, 1891). The excerpt chosen by <a href="https://www.forgottenbooks.com/">forgottenbooks.com</a> could serve in a modern software company’s mission flyer:</p>
<blockquote><p><b> </b> <em> Discriminate between ability and capacity. Capacity is the power of receiving and retaining knowledge with ease. Ability is the power of applying knowledge to practical purposes. Capacity implies power to conceive, ability the power to execute designs. Capacity is shown in quickness of apprehension; ability in something actually done. </em>
</p></blockquote>
<p/><p>
One application I conceived for my Data Structures course was to rewrite long words in selected texts by shorter synonyms, perhaps to humorous effect, using Fallows’s dictionary. This exemplifies lists and arrays and sets and maps of various sizes. The main <em>map</em> to build was from the key word to the associated list of synonyms. One could alternatively use a <em>set</em> of objects having <font size="+1"><tt>key</tt></font> and <font size="+1"><tt>synonyms</tt></font> fields, which I presented as more flexible in allowing other ways to define keys. Some words in Fallows’s dictionary have separate entries for the part of speech, for instance (antonyms and some words snipped):</p>
<p><code><br/>
KEY: Array \v.\.<br/>
SYN: Vest, deck, equip, decorate, rank, adorn, dress, accoutre, …<br/>
=<br/>
KEY: Array \n.\, Arrangement, order, disposition, sight, …, parade.<br/>
</code></p>
<p/><p><br/>
Rather than juggle different kinds of maps, to use or not-use the noun/verb/adjective info, I said better to keep the data all together. This fell in with preaching about a classic data structures pitfall of “<a href="https://en.wikipedia.org/wiki/Parallel_array#pros_and_cons">Parallel</a> <a href="https://codeblog.jonskeet.uk/2014/06/03/anti-pattern-parallel-collections/">Arrays</a>” and ensuing off-by-one indexing errors. I designed and gave a separate assignment where a sorted set with iterator gave 3-4x better performance than repeated lookup from a map. </p>
<p>
But I never gave the originally-conceived assignment. Among several reasons, I was dumbstruck by a “Parallel Arrays” fault in the Project Gutenberg text file.</p>
<p>
</p><p/><h2> The Horror </h2><p/>
<p/><p>
The dictionary also has <em>cross-reference</em> entries typified by</p>
<p>
<code><br/>
KEY: Bellow, [See BAWL].<br/>
</code></p>
<p/><p><br/>
My intent in such cases was to have the students’ code look up the synonyms of the referenced word, here <font size="+1"><tt>KEY: Bawl. SYN: Shout, vociferate, halloo, roar, bellow.</tt></font> And—in case the dictionary had just <font size="+1"><tt>KEY: Bawl, [See BELLOW]</tt></font>—beware of going into an infinite loop. </p>
<p>
I did assign the task of detecting when two words appear on the synonym lists of each other. I intended to extend it to cross-references, so that <font size="+1"><tt>bawl</tt></font> and <font size="+1"><tt>bellow</tt></font> would count as a “reciprocal pair.” But before I got there, I noticed instances like the following—especially toward the end of the file:</p>
<p>
<code><br/>
KEY: Unruffled, [See DISCOVER].<br/>
=<br/>
KEY: Unruly.<br/>
SYN: Ungovernable, unmanageable, refractory, [See TRANQUIL].<br/>
=<br/>
KEY: Unsafe, [See REFRACTORY].<br/>
=<br/>
KEY: Unseasonable, [See SAFE].<br/>
</code></p>
<p/><p><br/>
This <em>off-by-one</em> error extends above and below. There are some islands of correctness, but abutted by bizarreness:</p>
<p>
<code><br/>
KEY: Unhandy.<br/>
SYN: Awkward, clumsy, uncouth, [See AWKWARD].<br/>
=<br/>
KEY: unhappiness.<br/>
SYN: Misery, wretchedness, distress, woe, [See AWKWARD].<br/>
=<br/>
KEY: Unhappy.<br/>
SYN: Miserable, wretched, distressed, …, dismal, [See BUSS].<br/>
=<br/>
KEY: Unhealthy, [See BEHALF].<br/>
</code> </p>
<p/><p><br/>
The <font size="+1"><tt>BUSS</tt></font> is an OCR error for <font size="+1"><tt>BLISS</tt></font>, meant to go with <font size="+1"><tt>Unhappiness</tt></font>, and <font size="+1"><tt>BEHALF</tt></font> is an OCR error for <font size="+1"><tt>HEALTH</tt></font>—which is correctly aligned again. In other places the misalignments seem weirder and greater. But none of them is in any printed source. I ask:</p>
<blockquote><p><b> </b> <em> How could this kind of error happen? </em>
</p></blockquote>
<p/><p>
Evidently the transcriber or some helper fell afoul of Parallel Arrays. One possibility is hinted by the project Gutenberg site having CSV files that use separate columns for KEY, SYN, and ANT, <em>and</em> have notes interspersed with data toward the top. Inserting a note in one column would throw off alignment below it. But I have not found these errors in those files. </p>
<p>
</p><p/><h2> The Effort to Curate Data </h2><p/>
<p/><p>
I took the time to fix all the OCR errors on the <font size="+1"><tt>KEY:</tt></font> fields in my update posted on my course webpage: <a href="https://cse.buffalo.edu/~regan/cse250/DataStructures/Fallows1898fx.txt">Fallows1898fx.txt</a>. I started to fix cross-references, but gave up when I noticed sporadic instances earlier than <font size="+1"><tt>S</tt></font> in the file and the conjunction with OCR errors. Some of the latter are harder to explain. The Gutenberg file has</p>
<p>
<code><br/>
KEY: Catalogue \n.\, [See BAWL].<br/>
</code> </p>
<p/><p><br/>
The PDF/printed source has <font size="+1"><tt>[See RECORD]</tt></font>. The ‘<font size="+1"><tt>R</tt></font>‘ could produce the ‘<font size="+1"><tt>B</tt></font>‘, but the unlikelihood of getting <font size="+1"><tt>AWL</tt></font> from <font size="+1"><tt>ECORD</tt></font> makes me suspect a different error. Perhaps the earlier <font size="+1"><tt>[See BAWL]</tt></font> from the entry for <font size="+1"><tt>Bellow</tt></font> got copied here. Copies like the above <font size="+1"><tt>AWKWARD</tt></font> example occur elsewhere with more intervening space. A third possibility is that <font size="+1"><tt>BAWL</tt></font> could go with <font size="+1"><tt>KEY: Caterwaul</tt></font>, but Fallows does not have that word.</p>
<p>
The off-by-one error was avoided in US Archive’s own <a href="https://archive.org/stream/completedictiona00falluoft/completedictiona00falluoft_djvu.txt">full text</a>, but it has other issues. The markup is jumbled. The true text format is recoverable in many places but not easily in others. OCR-type typos are completely unmarked.</p>
<p>
Some errors are by Fallows himself. For example, he forgot to insert “<font size="+1">S</font><font size="-1">YN</font>.” into his own entry for the noun form of <font size="+1"><tt>Array</tt></font> given above. Should these be corrected? For my purpose of wanting a clean dataset, I would wish so. Never mind that I could add lots of entries—his dictionary was far from “complete” even in 1898. The understanding is that we operate with these historical artifacts as they are, perhaps after fixing things the authors clearly intended. </p>
<p>
<a href="https://en.wikipedia.org/wiki/Data_cleansing">Data cleansing</a> has become an area of computer and data science unto itself. My point is not to explore its theory or use-cases. I could devote a whole series of posts to issues with my chess data and numerous irregularities in chess game files sent to me that I have to fix. My point—with all these less-prominent but potentially useful data sources—is not <em>how</em> but <em>who</em>:</p>
<blockquote><p><b> </b> <em> Who will undertake to co-ordinate and execute the cleaning of all this medium-range data? </em>
</p></blockquote>
<p/><p>
Whether a large and united effort like Project Gutenberg has the human resources seems in question. The University of Pennsylvania Online Books Library has a cautionary <a href="https://onlinebooks.library.upenn.edu/webbin/book/lookupid?key=pg51155">status note</a> about the Gutenberg link:</p>
<blockquote><p><b> </b> <em> <b>No stable link:</b> This is an uncurated book entry from our extended bookshelves, readable online now but without a stable link here. You should not bookmark this page, but you can request that we add this book to our curated collection, which has stable links. </em>
</p></blockquote>
<p/><p>
The scanned PDF version is stable. If the note is prompted by faults in the textual transcriptions—well, I think <b>I</b> could finish the fixes I started if a free extra week were magically inserted into my calendar. If any of you can bump it along a day at a time, either starting from <a href="https://cse.buffalo.edu/~regan/cse250/DataStructures/Fallows1898fx.txt">my version</a> or working afresh, I’ll be grateful.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
How much will the world need to use such medium-level data sets? How important is it to clean them, and where would that effort come from? Or will all this data continue to lie in fallows?</p>
<p>
Besides the above books and religious guides, Fallows wrote patriotic books, including <a href="https://books.google.mw/books?id=d2QUAAAAYAAJ&amp;printsec=copyright#v=onepage&amp;q&amp;f=false">The American Manual and Patriot’s Handbook</a> (1889). But among all his topics, perhaps the one most current to remember this Fourth of July weekend—as we discuss student loans and the larger role of higher education—is that his great cause in his home state <a href="https://prabook.com/web/samuel.fallows/1097929">was</a> “a college education, tuition free, for every Wisconsin boy or girl who wanted it.”  He organized the first postgraduate distance-education program in the US, and also created the epsilon-alcoholic <a href="https://books.google.com/books?id=M_xHAQAAMAAJ&amp;pg=PA75&amp;lpg=PA75&amp;dq=%22Bishop%27s+Beer%22&amp;source=bl&amp;ots=J7hwGNPZjE&amp;sig=ACfU3U0pIFILUCvSw6-ZbPvWKSHv4cRJ4Q&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwjOqpXcv934AhWXkIkEHceOC8wQ6AF6BAhHEAM#v=onepage&amp;q=%22Bishop's%20Beer%22&amp;f=false">“Bishop’s Beer”</a> before Prohibition.</p>
<p>
[some little fixes]</p></font></font></div>
    </content>
    <updated>2022-07-03T19:58:24Z</updated>
    <published>2022-07-03T19:58:24Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Teaching"/>
    <category term="Big Data"/>
    <category term="bugs"/>
    <category term="data anomalies"/>
    <category term="data cleansing"/>
    <category term="data structures"/>
    <category term="Fourth of July"/>
    <category term="history of crossword puzzles"/>
    <category term="Medium Data"/>
    <category term="parallel arrays"/>
    <category term="Samuel Fallows"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-07-15T04:37:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/094</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/094" rel="alternate" type="text/html"/>
    <title>TR22-094 |  Notes on Monotone Read-k Circuits | 

	Stasys Jukna</title>
    <summary>A monotone Boolean $(\lor,\land)$ circuit $F$ computing a Boolean function $f$ is a read-$k$ circuit if the polynomial produced (purely syntactically) by the arithmetic $(+,\times)$ version of $F$ has the property that for every prime implicant of $f$, the polynomial contains a monomial with the same set of variables, each appearing with degree $\leq k$. Every monotone $(\lor,\land)$ circuit is a read-$k$ circuit for some $k$.


We first show that already read-1 circuits are interesting in the context of dynamic programming:  tropical $(\min,+)$ circuits solving $0/1$ optimization problems have the same power as Boolean read-1 circuits, and that 
 monotone read-1 $(\lor,\land)$ circuits computing homogeneous Boolean functions are not stronger than monotone arithmetic circuits.  Then we show that already read-2 circuits can be exponentially smaller than read-1 circuits. Finally, we show  that so-called (semantically) multilinear DeMorgan $(\lor,\land,\neg)$ circuits computing monotone Boolean functions are not stronger than monotone read-1 circuits.</summary>
    <updated>2022-07-03T18:00:53Z</updated>
    <published>2022-07-03T18:00:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-15T04:37:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/07/03/postdoc-in-complexity-theory-and-quantum-algorithms-at-university-of-warwick-apply-by-october-1-2022/</id>
    <link href="https://cstheory-jobs.org/2022/07/03/postdoc-in-complexity-theory-and-quantum-algorithms-at-university-of-warwick-apply-by-october-1-2022/" rel="alternate" type="text/html"/>
    <title>Postdoc in complexity theory and quantum algorithms at University of Warwick (apply by October 1, 2022)</title>
    <summary>We are looking for excellent candidates for 2 years postdoc positions in either complexity theory or quantum algorithms. The deadline and start dates are flexible. See details in the link below. Informal enquiries are welcome and may be sent to Tom Gur. Website: https://dcs.warwick.ac.uk/~tomgur/postdoc-positions.html Email: tom.gur@warwick.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are looking for excellent candidates for 2 years postdoc positions in either complexity theory or quantum algorithms. The deadline and start dates are flexible. See details in the link below.</p>
<p>Informal enquiries are welcome and may be sent to Tom Gur.</p>
<p>Website: <a href="https://dcs.warwick.ac.uk/~tomgur/postdoc-positions.html">https://dcs.warwick.ac.uk/~tomgur/postdoc-positions.html</a><br/>
Email: tom.gur@warwick.ac.uk</p></div>
    </content>
    <updated>2022-07-03T12:25:23Z</updated>
    <published>2022-07-03T12:25:23Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-07-15T04:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=1043</id>
    <link href="https://emanueleviola.wordpress.com/2022/07/03/the-ab-normal-reach-of-norm-al-proofs-in-non-abelian-fourier-analysis/" rel="alternate" type="text/html"/>
    <title>The ab-normal reach of norm-al proofs in non-abelian Fourier analysis</title>
    <summary>Fourier analysis over (not necessarily abelian) groups is a cool proof technique that yields many results of interest to theoretical computer science. Often the goal is to show “mixing” or “pseudo/quasi randomness” of appropriate distributions. This post isn’t about the formal statements or applications or proofs or even the credit of these results; for some […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><!--?xml version="1.0" encoding="iso-8859-1" ?-->     <!--http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd-->                 <!-- html,xhtml,-css,NoFonts -->  </p>
<p style="text-align: justify;">   Fourier analysis over (not necessarily abelian) groups is a cool proof technique that yields many results of interest to theoretical computer science. Often the goal is to show “mixing” or “pseudo/quasi randomness” of appropriate distributions. This post isn’t about the formal statements or applications or proofs or even the credit of these results; for some of this you can see e.g. the references below, or a survey of mine <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#Xviola-SIGACT19">Vio19</a>]</span>, or a survey <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XMR3584096">Gow17</a>]</span> by Gowers.</p>
<p style="text-align: justify;">   Instead this post is about an uncanny development of the proofs of some of these results. Whereas the original proofs were somewhat complicated, in some cases involving heavy mathematical machinery, later there emerged proofs that I propose to call <em>norm-al</em> (or normal for simplicity) because they only involve manipulations that can be cast as norm inequalities, such as Cauchy-Schwarz. Normal proofs I view as just a little up in complexity from proofs that are simply opening up definitions. They can involve the latter, or norm inequalities, and they need not be tight. An example of an ab-norm-al proof would be one that involves induction or iterative arguments, or probabilistic/double-counting/pigeon-hole methods. Making this a little more precise seems to require a lot of discussion, and may not even be possible, so let me stop here and move on with the examples of the proofs which became norm-al. They all involve <em>quasirandom groups </em><span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XGowers08">Gow08</a>]</span>, but even non-quasirandom groups mix in a certain sense <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XGowersV-group-mix">GV22</a>]</span> and the proofs there are again norm-al (it’s just that I don’t know of earlier proofs in this case).</p>
<p style="text-align: justify;">   The first example concerns the quintessential mixing result in this area: If you’ve got independent distributions <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/> and <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/>, and if each distribution is uniform over say a constant fraction of the group, then the the product <img alt="XY" class="latex" src="https://s0.wp.com/latex.php?latex=XY&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/> (a.k.a. convolution, a.k.a. sample from each and output the product) is close to uniform over the entire group. A norm-al proof appears in <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/#XMR3584096">Gow17</a>]</span> which also contains pointers to previous proofs.</p>
<p style="text-align: justify;">   The second is mixing of <em>three-term</em> progressions. A norml-al proof appears in <span class="cite">[<a href="https://emanueleviola.wordpress.com/feed/conf/innovations/BhangaleHR22">BHR22</a>]</span>. From their abstract: “Surprisingly, unlike the proofs of Tao and Peluse, our proof is elementary and only uses basic facts from nonabelian Fourier analysis.”</p>
<p style="text-align: justify;">   The third is <em>interleaved mixing</em>, see a recent <a href="https://www.ccs.neu.edu/home/viola/papers/DV-group-mix.pdf">preprint</a> with Derksen.</p>
<p style="text-align: justify;">   Moreover, in the second and third example the proofs are not only simpler,                                                                                                                                                        they are also more general in that they apply to any quasirandom group whereas previous proofs only applied to prominent special cases.</p>
<p style="text-align: justify;">   Why is all of this happening? One can only speculate that the reach of norml-al proofs in non-abelian Fourier analysis is still just emerging.</p>
<h3 class="likesectionHead"><a id="x1-1000"/>References</h3>
<p style="text-align: justify;">
</p><div class="thebibliography">
<p class="bibitem"><span class="biblabel">  [BHR22]<span class="bibsp">   </span></span><a id="XDBLP:conf/innovations/BhangaleHR22"/>Amey Bhangale, Prahladh Harsha, and Sourya Roy.  Mixing           of   3-term   progressions   in   quasirandom   groups.      In   Mark           Braverman,  editor,  ACM  Innovations  in  Theoretical  Computer           Science  conf. (ITCS),  volume  215  of  LIPIcs,  pages  20:1–20:9.           Schloss Dagstuhl – Leibniz-Zentrum f�r Informatik, 2022.</p>
<p class="bibitem"><span class="biblabel">  [Gow08] <span class="bibsp">   </span></span><a id="XGowers08"/>W. T.  Gowers.      Quasirandom  groups.      Combinatorics,           Probability &amp; Computing, 17(3):363–387, 2008.</p>
<p class="bibitem"><span class="biblabel">  [Gow17] <span class="bibsp">   </span></span><a id="XMR3584096"/>W. T. Gowers. Generalizations of Fourier analysis, and how to           apply them. Bull. Amer. Math. Soc. (N.S.), 54(1):1–44, 2017.</p>
<p class="bibitem"><span class="biblabel">  [GV22] <span class="bibsp">   </span></span><a id="XGowersV-group-mix"/>W. T.                                                                Gowers           and Emanuele Viola. Mixing in non-quasirandom groups. In ACM           Innovations in Theoretical Computer Science conf. (ITCS), 2022.</p>
<p class="bibitem"><span class="biblabel">  [Vio19] <span class="bibsp">   </span></span><a id="Xviola-SIGACT19"/>Emanuele           Viola. Non-abelian combinatorics and communication complexity.           SIGACT News, Complexity Theory Column, 50(3), 2019.</p>
</div></div>
    </content>
    <updated>2022-07-03T11:37:54Z</updated>
    <published>2022-07-03T11:37:54Z</published>
    <category term="Uncategorized"/>
    <category term="Behind the papers"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2022-07-15T04:38:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/093</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/093" rel="alternate" type="text/html"/>
    <title>TR22-093 |  More Verifier Efficient Interactive Protocols For Bounded Space | 

	Joshua Cook</title>
    <summary>Let $\mathbf{TISP}[T, S]$, $\mathbf{BPTISP}[T, S]$, $\mathbf{NTISP}[T, S]$, and $\mathbf{CoNTISP}[T, S]$  be the set of languages recognized by deterministic, randomized, nondeterminsitic, and co-nondeterministic algorithms, respectively, running in time $T$ and space $S$. Let $\mathbf{ITIME}[T_V]$ be the set of languages recognized by an interactive protocol where the verifier runs in time $T_V$. We prove:
$$\mathbf{TISP}[T, S] \cup \mathbf{BPTISP}[T, S] \subseteq \mathbf{ITIME}[\tilde{O}(\log(T) S + n)]$$
$$\mathbf{NTISP}[T, S] \cup \mathbf{CoNTISP}[T, S] \subseteq \mathbf{ITIME}[\tilde{O}(\log(T)^2 S + n)]$$
The prior most verifier time efficient interactive protocol for $\mathbf{TISP}$ uses ideas from Goldwasser, Kalai and Rothblum, which gives 
$$\mathbf{NTISP}[T, S] \subseteq \mathbf{ITIME}[\tilde{O}(\log(T) S^2 + n)].$$</summary>
    <updated>2022-07-03T07:09:54Z</updated>
    <published>2022-07-03T07:09:54Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-15T04:37:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/092</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/092" rel="alternate" type="text/html"/>
    <title>TR22-092 |  On correlation bounds against polynomials | 

	Emanuele Viola, 

	Peter Ivanov, 

	Liam Pavlovic</title>
    <summary>We study the fundamental challenge of exhibiting explicit functions that have small correlation with low-degree polynomials over $\mathbb{F}_{2}$. Our main contributions include:

1. In STOC 2020, CHHLZ introduced a new technique to prove correlation bounds. Using their technique they established new correlation bounds for low-degree polynomials. They conjectured that their technique generalizes to higher degree polynomials as well. We give a counterexample to their conjecture, in fact ruling out weaker parameters and showing what they prove is essentially the best possible. 

2. We advocate an alternative approach for proving correlation bounds with the central “mod functions,” consisting of proving two steps: (I) the polynomials that maximize correlation are symmetric, and (II) symmetric polynomials have small correlation. Contrary to related results in the literature, we conjecture that (I) is true. We argue that this approach is not affected by existing “barrier results.”

3. We prove our conjecture for quadratic polynomials. Specifically, we determine the maximum possible correlation between quadratic polynomials modulo 2 and the functions $(x_{1},\dots,x_{n})\to z^{\sum x_{i}}$ for any $z$ on the complex unit circle; and show that it is achieved by symmetric polynomials. To obtain our results we develop a new proof technique: we express correlation in terms of directional derivatives and analyze it by slowly restricting the direction.

4. We make partial progress on the conjecture for cubic polynomials, in particular proving tight correlation bounds for cubic polynomials whose degree-3 part is symmetric.</summary>
    <updated>2022-07-02T10:57:48Z</updated>
    <published>2022-07-02T10:57:48Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-07-15T04:37:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/06/30/linkage</id>
    <link href="https://11011110.github.io/blog/2022/06/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Neil Sloane has a new blog (\(\mathbb{M}\)), subtitled “interesting sequences I need help with”. His first post concerns the two-up sequence, formed in steps where the kth step adds two numbers that are not already in the sequence and are relatively prime to the preceding k. Most of the terms appear to be the primes (in order). The remaining terms appear to be prime powers or semiprimes but this has not been proven.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://njas.blog/">Neil Sloane has a new blog</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@christianp/108487272201741871">\(\mathbb{M}\)</a>),</span> subtitled “interesting sequences I need help with”. <a href="https://njas.blog/2022/06/03/the-two-up-sequence-a090252/">His first post concerns the two-up sequence</a>, formed in steps where the kth step adds two numbers that are not already in the sequence and are relatively prime to the preceding k. Most of the terms appear to be the primes (in order). The remaining terms appear to be prime powers or semiprimes but this has not been proven.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Schwarz_lantern">Schwarz lantern</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108497292976963088">\(\mathbb{M}\)</a>),</span> a polyhedral cylindrical surface that you can fold from paper. As you make its triangles smaller relative to the cylinder size, it approximates the cylinder in distance, but not necessarily in surface area. This example became one of the original motivations for research on non-obtuse mesh generation algorithms, because its non-convergence to the correct area happens when its triangles are very obtuse. Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wpcomstaging.com/2022/06/19/the-graph-of-ancestors/">The graph of ancestors</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108507916081000038">\(\mathbb{M}\)</a>).</span>  For Father’s Day, Ken Regan tries to find a more principled way of quantifying pedigree collapse (the unavoidable existence of inbreeding among one’s ancestors) than “implex”, the difference between \(2^k\) and the number of distinct 
\(k\)-step ancestors.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@johncarlosbaez/108504865028259313">John Baez describes the work of Hoàng Xuân Sính</a> (<a href="https://en.wikipedia.org/wiki/Ho%C3%A0ng_Xu%C3%A2n_S%C3%ADnh">see also</a>), a student of Grothendieck who became the first female mathematician in Vietnam.</p>
  </li>
  <li>
    <p><a href="https://www.torontomu.ca/canadian-conference-computational-geometry-2022/program/">Accepted papers to the 34th Canadian Conference on Computational Geometry</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108519879503501792">\(\mathbb{M}\)</a>),</span> to be held in August in Toronto. For mine, see recent blog posts on <a href="https://11011110.github.io/blog/2022/06/22/dehn-rank-revisited.html">Dehn rank</a>, <a href="https://11011110.github.io/blog/2022/06/24/reflections-octagonal-mirror.html">octagonal reflections</a>, and <a href="https://11011110.github.io/blog/2022/06/28/motion-bend-lines.html">flattening paper surfaces</a>. In other news, from looking at the web site, I see that the host university, formerly Ryerson, has changed its name: now it’s Toronto Metropolitan University.</p>
  </li>
  <li>
    <p><a href="https://www.sligocki.com/2022/06/21/bb-6-2-t15.html">Tetration in busy-beaver Turing machines</a> and <a href="https://cp4space.hatsya.com/2022/06/23/tetrational-machines/">in Conway’s Game of Life</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108533292912244512">\(\mathbb{M}\)</a>,</span> <a href="https://btm.qva.mybluehost.me/telling-the-tale-of-two-tetrations/">see also</a>).</p>
  </li>
  <li>
    <p><a href="https://www.thisiscolossal.com/2022/06/paper-show-heron-arts/">Paper show</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108542378459285958">\(\mathbb{M}\)</a>):</span> Group art exhibit featuring 14 different artists who work in papercraft. Coming soon to a gallery in the Mission in San Francisco. Maybe the most geometric are the paper geodes of <a href="https://www.huntzliu.com/">Huntz Liu</a>.</p>
  </li>
  <li>
    <p>Two 3-regular penny graphs <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108546890027577211">\(\mathbb{M}\)</a>).</span> You can’t make these avoid triangles altogether (see <a href="https://doi.org/10.7155/jgaa.00463">my paper on triangle-free penny graphs</a>) but the larger one of these two avoids pairs of triangles that share an edge. It’s more difficult than it seems like it should be to get all of the pennies that should be touching to actually touch, and I didn’t entirely succeed, but I think the pattern is clear.</p>

    <p style="text-align: center;"><img alt="Two three-regular penny graphs, with vertices represented by pennies and edges represented by touching pennies" src="https://www.ics.uci.edu/~eppstein/pix/3regpen/3regular-penny-m.jpg" style="border-style: solid; border-color: black;"/></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Double_bubble_theorem">The double bubble theorem</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108551879168137037">\(\mathbb{M}\)</a>)</span> states that the minimum area enclosing two volumes looks like a double soap bubble, with three spherical patches meeting at \(120^\circ\) angles on a circle. Just after getting Wikipedia’s article to Good Article, I learned of significant progress: Milman and Neeman announced a proof of the triple bubble conjecture in all dimensions and related results. See <a href="https://arxiv.org/abs/2205.09102">their preprint</a> or <a href="https://amathr.org/milman-and-neeman/">Frank Morgan’s review</a>.</p>
  </li>
  <li>
    <p><a href="https://www.sixthtone.com/news/1010653/she-spent-a-decade-writing-fake-russian-history.-wikipedia-just-noticed.-">An elaborate hoax history of medieval Russian history is uncovered on the Chinese-language Wikipedia</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108563040999391348">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=31915937">via</a>, <a href="https://www.metafilter.com/195824/crumples-up-thesis-on-Kashin-silver-mine">mf</a>). Link goes to Chinese state media, but is in English; see also the <a href="https://en.wikipedia.org/wiki/Wikipedia:Fabricated_articles_and_hoaxes_of_Russia_in_2022">English Wikipedia internal report on the situation</a>.</p>
  </li>
  <li>
    <p><a href="http://www.ag.jku.at/geometrikum.shtml">Geometrikum</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108568817200153580">\(\mathbb{M}\)</a>):</span> geometric exhibits at the Institute for Applied Geometry of the University of Linz. Or, if you’re not near Linz, you can see them online. Includes string-art surfaces, Lego Stanford bunny and trefoil knot, some regular 4-polytope skeletons, papercraft polycubes and modular polyhedra, matchstick tetrastix, and more.</p>
  </li>
</ul></div>
    </content>
    <updated>2022-06-30T16:23:00Z</updated>
    <published>2022-06-30T16:23:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-06-30T23:36:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/06/30/postdoc-at-the-institute-for-logic-language-and-computation-and-the-institute-for-information-law-at-the-university-of-amsterdam-apply-by-august-15-2022/</id>
    <link href="https://cstheory-jobs.org/2022/06/30/postdoc-at-the-institute-for-logic-language-and-computation-and-the-institute-for-information-law-at-the-university-of-amsterdam-apply-by-august-15-2022/" rel="alternate" type="text/html"/>
    <title>postdoc at The Institute for Logic, Language and Computation and the Institute for Information Law at the University of Amsterdam (apply by August 15, 2022)</title>
    <summary>Do you have a PhD and are you interested in researching the law and regulation of cybersecurity, and in combining this with insights from ethics and economics? If you are excited about doing this kind of research in an interdisciplinary environment, with a team of friendly and enthusiastic colleagues, and with partners from the financial […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Do you have a PhD and are you interested in researching the law and regulation of cybersecurity, and in combining this with insights from ethics and economics? If you are excited about doing this kind of research in an interdisciplinary environment, with a team of friendly and enthusiastic colleagues, and with partners from the financial and governmental sectors, then you may want to join us.</p>
<p>Website: <a href="https://www.illc.uva.nl/NewsandEvents/News/Positions/newsitem/13758/Postdoctoral-Researcher-in-the-Regulation-of-Quantum-Safe-Technology">https://www.illc.uva.nl/NewsandEvents/News/Positions/newsitem/13758/Postdoctoral-Researcher-in-the-Regulation-of-Quantum-Safe-Technology</a><br/>
Email: s.deharo@uva.nl</p></div>
    </content>
    <updated>2022-06-30T09:54:00Z</updated>
    <published>2022-06-30T09:54:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-07-15T04:37:24Z</updated>
    </source>
  </entry>
</feed>
