<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-11-02T23:35:25Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/11/02/faculty-at-tufts-university-apply-by-december-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/11/02/faculty-at-tufts-university-apply-by-december-15-2020/" rel="alternate" type="text/html"/>
    <title>Faculty at Tufts University (apply by December 15, 2020)</title>
    <summary>Tufts Computer Science seeks Assistant/Associate Professor with research in Quantum Computation and Information, and a strong background in theoretical computer science whose research connects with current faculty in quantum information and beyond. Candidates should demonstrate attention to diversity and inclusion as related to teaching, research, and engagement. Tufts is an EO/AA employer. Website: https://apply.interfolio.com/78094 Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Tufts Computer Science seeks Assistant/Associate Professor with research in Quantum Computation and Information, and a strong background in theoretical computer science whose research connects with current faculty in quantum information and beyond. Candidates should demonstrate attention to diversity and inclusion as related to teaching, research, and engagement. Tufts is an EO/AA employer.</p>
<p>Website: <a href="https://apply.interfolio.com/78094">https://apply.interfolio.com/78094</a><br/>
Email: ttsearch@cs.tufts.edu</p></div>
    </content>
    <updated>2020-11-02T22:19:18Z</updated>
    <published>2020-11-02T22:19:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-02T23:34:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-9123933489422342245</id>
    <link href="https://blog.computationalcomplexity.org/feeds/9123933489422342245/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/11/i-polled-my-class-about-election.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/9123933489422342245" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/9123933489422342245" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/11/i-polled-my-class-about-election.html" rel="alternate" type="text/html"/>
    <title>I polled my class about the election</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> In 2016 I had the Sophomore discrete math class do a poll of who they wanted for president.</p><p>In 2020 I had the  both my  Senior Crypto class and Clyde's Sophomore algorithms course do a poll of who they wanted for president.</p><p>All of these polls were anonymous. One big difference- in 2016 it was paper, they could check offwho they wanted or put in a write in, whereas in 2020 it was on elms without a mechanism for a write in--- so no votes for Bernie or Bill or Kruskal (not sure if they were voting for the man or the algorithm) were possible. In all cases I included everyone who was on the Maryland Ballot (so Libertarian and Green votes were possible). </p><p><br/></p><p>Discrete Math 2016: 428 students took the poll. Write ins allowed. </p><p>Clinton- 305 which is 71%</p><p>Trump- 44 which is  10%</p><p>Johnson (Libertarian)- 21 which is 5%</p><p>Stein (Green)-  11which is 3%</p><p>Sanders-6 which is 1%</p><p>Silly answers: 41 which is 10%</p><p>I was NOT surprised that Trump got 44 votes- every year I do this and every year the </p><p>republican gets between 10 and 20 percent. Romney go 17% in 2012 (see <a href="https://blog.computationalcomplexity.org/2012/11/random-thoughts-on-election.html">here</a>). </p><p><br/></p><p>Algorithms, 2020, 161 students took the poll</p><p>Biden: 127 (79%)</p><p>Trump: 25 (16%)</p><p>Hawkins (Green): 4 (2%)</p><p>Jorgenson (Libertarian): 4 (2%)</p><p>Segal (Bread and Roses Party) 1 (1%)</p><p><br/></p><p>Cryptography in 2020: </p><p>Biden- 40 which is 78%</p><p>Trump-6 which is 12%</p><p>Hawkins (Green ) 3 which is 6%</p><p>Jorgenson (Libertarian) 2 which is 4%</p><p>Segal (Bread and Roses) 0 which is 0%</p><p><br/></p><p>I have no idea what these numbers mean. College students tend to be liberal- we knew that. That Trump went from 10% to 16% would be interesting if it was a larger sample size. I wonder if forcing them to NOT have a write-in had an effect. </p><p><br/></p></div>
    </content>
    <updated>2020-11-02T15:41:00Z</updated>
    <published>2020-11-02T15:41:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-11-02T22:09:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/160</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/160" rel="alternate" type="text/html"/>
    <title>TR20-160 |  Non-adaptive vs Adaptive Queries in the Dense Graph Testing Model | 

	Oded Goldreich, 

	Avi Wigderson</title>
    <summary>We study the relation between the query complexity of adaptive and non-adaptive testers in the dense graph model. 
It has been known for a couple of decades that the query complexity of non-adaptive testers is at most quadratic in the query complexity of adaptive testers. 
We show that this general result is essentially tight; that is, there exist graph properties for which any non-adaptive tester must have query complexity that is almost quadratic in the query complexity of the best general (i.e., adaptive) tester. 

More generally, for every $q:\N\to\N$ such that $q(n)\leq{\sqrt n}$ and constant $c\in[1,2]$, we show a graph property that is testable in $\Theta(q(n))$ queries, but its non-adaptive query complexity is $\Theta(q(n)^c)$, omitting $\poly(\log n)$ factors and ignoring the effect of the proximity parameter $\epsilon$. Furthermore, the upper bounds hold for one-sided error testers,
and are at most quadratic in $1/\epsilon$. 

These results are obtained through the use of general reductions that transport properties of ordered structured (like bit strings) to those of unordered structures (like unlabeled graphs). 
The main features of these reductions are query-efficiency and preservation of distance to the properties.
This method was initiated in our prior work ({\em ECCC}, TR20-149), and we significantly extend it here.</summary>
    <updated>2020-11-02T15:24:24Z</updated>
    <published>2020-11-02T15:24:24Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-11-02T23:33:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/159</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/159" rel="alternate" type="text/html"/>
    <title>TR20-159 |  Relating existing powerful proof systems for QBF | 

	Leroy Chew</title>
    <summary>We advance the theory of QBF proof systems by showing the first simulation of the universal checking format QRAT by a theory-friendly system. We show that the sequent system G fully p-simulates QRAT, including the Extended Universal Reduction (EUR) rule which was recently used to show QRAT does not have strategy extraction. Because EUR heavily uses resolution paths our technique also brings resolution path dependency and sequent systems closer together. 
	While we do not recommend G for practical applications this work can potentially show what features are needed for a new QBF checking format stronger than QRAT.</summary>
    <updated>2020-11-02T09:48:39Z</updated>
    <published>2020-11-02T09:48:39Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-11-02T23:33:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17765</id>
    <link href="https://rjlipton.wordpress.com/2020/11/02/the-night-of-the-ethical-algorithm/" rel="alternate" type="text/html"/>
    <title>The Night of the Ethical Algorithm</title>
    <summary>Algorithms for the Election Michael Kearns and Aaron Roth are the authors of the book Ethical Algorithms and the The Science of Socially Aware Algorithm Design. It has earned strong reviews including this one in Nature—impressive. Michael is a long-time friend who is a leader in machine learning, artificial intelligence, and much more. He also […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Algorithms for the Election</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2020/11/kearnsroth.png"><img alt="" class="alignright wp-image-17767" height="110" src="https://rjlipton.files.wordpress.com/2020/11/kearnsroth.png?w=142&amp;h=110" width="142"/></a></p>
<p>
Michael Kearns and Aaron Roth are the authors of the <a href="https://www.amazon.com/Ethical-Algorithm-Science-Socially-Design/dp/0190948205">book</a> <em>Ethical Algorithms and the The Science of Socially Aware Algorithm Design</em>. It has earned strong reviews including this <a href="https://www.nature.com/articles/s41567-019-0768-1.epdf?shared_access_token=PLLHPNTn5pByQUPaJ-n_KdRgN0jAjWel9jnR3ZoTv0OmI9hygrW1-UKQon3ZVGlMNsZowBl_5psAIvdU-Dic4gtf_j0e7S_897lCTN3vdbqv3cbSQwUtbHG78r5ycJNB2VKFndoPOT_8w0OYjS-pjQ%3D%3D">one</a> in <i>Nature</i>—impressive.</p>
<p>
Michael is a long-time friend who is a leader in machine learning, artificial intelligence, and much <a href="https://www.cis.upenn.edu/~mkearns/">more</a>. He also overlapped with Ken at Oxford while visiting Les Valiant there in the mid-1980s. He is at the University of Pennsylvania in computer science along with his co-author Roth. Cynthia Dwork and Roth wrote an earlier <a href="https://www.cis.upenn.edu/~aaroth/privacybook.html">book</a> on the related issue of Differential Privacy.</p>
<p>
Today we will talk about making algorithms ethical.</p>
<p>
Tuesday is the 2020 US national election for President, for Congress, and for state and local offices. Every four years we have a national election, and we cannot imagine a better motivation for making sure that algorithms are ethical. </p>
<p>
The word “algorithm” appears 157 times in their book. Two words used hand-in-hand with it are “data” (132 times) and “model” (103 times), both spread through all of the book’s 232 pages. Models of electorates, trained on data from past elections, inform the algorithms used by news agencies to make election-night projections. These carry more responsibilities than election-eve forecasts. There have been infamous mistakes, most notably the premature calls of Florida both ways in the 2000 election. </p>
<p>
We believe that Tuesday’s election in our novel pandemic situation requires attention to ethics from first principles. We will discuss why this is important. What it means to be ethical here? And how one can make an algorithm ethical? </p>
<p>
</p><p/><h2> The Issue </h2><p/>
<p/><p>
Algorithms have been around forever. Euclid devised his <a href="https://en.wikipedia.org/wiki/Euclidean_algorithm">gcd</a> algorithm in 300 BCE. In the first half of the last century, the central issue was how to define that an algorithm is <b>effective</b>. This led to showing that some problems are <em>uncomputable</em>, so that algorithms for them are impossible.</p>
<p>
In the second half, the emphasis shifted to whether algorithms are <b>efficient</b>. This led to classifying problems as <em>feasible</em> or (contingently) <em>hard</em>. Although many algorithms for feasible problems have been improved in ways that redouble the effect of faster and cheaper hardware, the study of complexity classes such as <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{NP}}"/> has given reasons why algorithms for hard problems may never be improvable.</p>
<p>
The new territory, that of Kearns and Roth, is whether algorithms are <b>ethical</b>. Current ones that they and <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction">others</a> have critqued as unethical accompany models for the likes of mortgages, small-business loans, parole decisions, and college admissions. The training data for these models often bakes in past biases. Besides problems of racial and gender bias and concerns of societal values, the raw fact is that past biases cause the models to miss the mark for today’s applications. For algorithms with such direct application to society, ethical design is critical. </p>
<p>
But this requirement is further reaching that one might initially imagine, so that as with computability and complexity, the factors can be ingrained in the <em>problems</em>. </p>
<p>
Consider a simple problem: We given a collection of pairs of numbers <img alt="{(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(x,y)}"/>. We are to predict whether this number pair has the property </p>
<p align="center"><img alt="\displaystyle  x + y \ge 0. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%2B+y+%5Cge+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  x + y \ge 0. "/></p>
<p>This is pretty easy if we can use both <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x}"/> and <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{y}"/>. But imagine a world where <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x}"/> is allowed to be viewed but <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{y}"/> is secret. Perhaps the law requires that we cannot use <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{y}"/>—it is illegal. Now we might do as poorly as <img alt="{50\%}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B50%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{50\%}"/>. Suppose that the data consists of </p>
<p align="center"><img alt="\displaystyle  (1,-1), (1,1), (2,-2), (2,2) \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%281%2C-1%29%2C+%281%2C1%29%2C+%282%2C-2%29%2C+%282%2C2%29+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  (1,-1), (1,1), (2,-2), (2,2) \dots "/></p>
<p>Then seeing only <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x}"/> gives no advantage, while giving both is perfect. Thus what in these simplified terms counts as an ethical algorithm is a poor predictor, whereas an unethical one is perfect. </p>
<p>
The blurb for the Kearns-Roth book says that they “…explain how we can better embed human principles into machine code—without halting the advance of data-driven scientific exploration.” While we agree their approach is vital, we suspect that as with complexity there will be indelibly ethically hard tasks. We wonder whether election modeling has already become one of them. </p>
<p>
Ken and I have two separate takes on this. We will do the first and then the other in a second post.</p>
<p>
</p><p/><h2> Red/Blue Leakage, Bias, and Ethics </h2><p/>
<p/><p>
One question on everyone’s minds is whether we will see a repeat of the forecasting misses from 2016. Let us remind that our own Election Day 2016 <a href="https://rjlipton.wordpress.com/2016/11/08/unskewing-the-election/">post</a> started by defending Nate Silver of <a href="https://fivethirtyeight.com/">FiveThirtyEight</a> for giving Donald Trump as much as a 30% chance to defeat Hillary Clinton. He had been attacked by representatives of many news and opinion agencies whose models had Clinton well over 90%. </p>
<p>
We wonder whether these models were affected by the kind of biases highlighted in the book by Kearns and Roth. We must say right away that we are neither alleging conscious biases nor questioning the desire for prediction accuracy. One issue in ethical modeling (for parole, loans, admissions) is the divergence between algorithm outcomes that are most <em>predictive</em> versus those that are best for society. Here we agree that accurate prediction—and accurate projections as results come in after the polls close—is paramount. However, the algorithms used for the latter projections (which were not at fault in 2016 but have been wrong previously) may be even more subject to what we as computer scientists with crypto background see as a “leakage” issue.</p>
<p>
Here is the point. Ideally, models using polling data and algorithms reading the Election Night returns should read the numbers as if they did not have ‘R’ and ‘D’ attached to them. Their own workings should be invariant under transformations that interchange Joe Biden and Donald Trump, or whoever are opposed in a local race. However, a crucial element in the projection models in particular is knowledge of voting geography. They must use data on the general voting preferences of regions where much of the vote is still extant. Thus they cannot avoid intimate knowledge of who is ‘R’ and who is ‘D.’ There is no double-blind or zero-knowledge approach to the subjects being projected.</p>
<p>
There is also the question of error bars. A main point of our 2016 post (and of Silver’s analysis) was the high uncertainty factor that could be read from how the Clinton-Trump race unfolded. Underestimating uncertainty causes overconfidence in models. This can result from “groupthink” of the kind we perceive in newsrooms of many of the same outlets that are doing the projections. The algorithms ought to be isolated from opinions of those in the organization, but again there is reason from the last election to wonder about leakage.</p>
<p>
Unlike cases addressed by Kearns and Roth, we do not see a solution to suggest. As in our simple <img alt="{x+y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x+y}"/> example, prior knowledge of the data in full may be needed for prediction. This may just be an “Ethics-Hard” problem. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>
The word “election” does <i>not</i> appear in Kearns and Roth’s book.  What further application of their standpoint to elections would you make?</p>
<p>Ken sees a larger question of ethical modeling decisions given the unprecedented circumstances of the current election.  This comes not from spatial geography distorted by the pandemic but rather from the dimension of time injected by massive early voting and late counting of many mailed ballots.  He will address this next.</p></font></font></div>
    </content>
    <updated>2020-11-02T05:10:12Z</updated>
    <published>2020-11-02T05:10:12Z</published>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Aaron Roth"/>
    <category term="Algorithms"/>
    <category term="bias"/>
    <category term="election"/>
    <category term="ethics"/>
    <category term="fairness"/>
    <category term="Michael Kearns"/>
    <category term="prediction"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-11-02T23:33:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.16381</id>
    <link href="http://arxiv.org/abs/2010.16381" rel="alternate" type="text/html"/>
    <title>Ginzburg-Landau energy and placement of singularities in generated cross fields</title>
    <feedworld_mtime>1604275200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alexis Macq, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reberol:Maxence.html">Maxence Reberol</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henrotte:Fran=ccedil=ois.html">François Henrotte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beaufort:Pierre=Alexandre.html">Pierre-Alexandre Beaufort</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chemin:Alexandre.html">Alexandre Chemin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Remacle:Jean=Fran=ccedil=ois.html">Jean-François Remacle</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schaftingen:Jean_Van.html">Jean Van Schaftingen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.16381">PDF</a><br/><b>Abstract: </b>Cross field generation is often used as the basis for the construction of
block-structured quadrangular meshes, and the field singularities have a key
impact on the structure of the resulting meshes. In this paper, we extend
Ginzburg-Landau cross field generation methods with a new formulation that
allows a user to impose inner singularities. The cross field is computed via
the optimization of a linear objective function with localized quadratic
constraints. This method consists in fixing singularities in small holes
drilled in the computational domain with specific degree conditions on their
boundaries, which leads to non-singular cross fields on the drilled domain. We
also propose a way to calculate the Ginzburg-Landau energy of these cross
fields on the perforated domain by solving a Neumann linear problem. This
energy converges to the energy of the Ginzburg-Landau functional as epsilon and
the radius of the holes tend to zero. To obtain insights concerning the sum of
the inner singularity degrees, we give: (i) an extension of the Ginzburg-Landau
energy to the piecewise smooth domain allowing to identify the positions and
degrees of the boundary singularities, and (ii) an interpretation of the
Poincar\'e-Hopf theorem focusing on internal singularities.
</p></div>
    </summary>
    <updated>2020-11-02T23:32:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-11-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.16376</id>
    <link href="http://arxiv.org/abs/2010.16376" rel="alternate" type="text/html"/>
    <title>Online Edge Coloring Algorithms via the Nibble Method</title>
    <feedworld_mtime>1604275200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhattacharya:Sayan.html">Sayan Bhattacharya</a>, Fabrizio Grandoni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wajc:David.html">David Wajc</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.16376">PDF</a><br/><b>Abstract: </b>Nearly thirty years ago, Bar-Noy, Motwani and Naor [IPL'92] conjectured that
an online $(1+o(1))\Delta$-edge-coloring algorithm exists for $n$-node graphs
of maximum degree $\Delta=\omega(\log n)$. This conjecture remains open in
general, though it was recently proven for bipartite graphs under
\emph{one-sided vertex arrivals} by Cohen et al.~[FOCS'19]. In a similar vein,
we study edge coloring under widely-studied relaxations of the online model.
</p>
<p>Our main result is in the \emph{random-order} online model. For this model,
known results fall short of the Bar-Noy et al.~conjecture, either in the degree
bound [Aggarwal et al.~FOCS'03], or number of colors used [Bahmani et
al.~SODA'10]. We achieve the best of both worlds, thus resolving the Bar-Noy et
al.~conjecture in the affirmative for this model.
</p>
<p>Our second result is in the adversarial online (and dynamic) model with
\emph{recourse}. A recent algorithm of Duan et al.~[SODA'19] yields a
$(1+\epsilon)\Delta$-edge-coloring with poly$(\log n/\epsilon)$ recourse. We
achieve the same with poly$(1/\epsilon)$ recourse, thus removing all dependence
on $n$.
</p>
<p>Underlying our results is one common offline algorithm, which we show how to
implement in these two online models. Our algorithm, based on the R\"odl Nibble
Method, is an adaptation of the distributed algorithm of Dubhashi et
al.~[TCS'98]. The Nibble Method has proven successful for distributed edge
coloring. We display its usefulness in the context of online algorithms.
</p></div>
    </summary>
    <updated>2020-11-02T23:32:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.16316</id>
    <link href="http://arxiv.org/abs/2010.16316" rel="alternate" type="text/html"/>
    <title>A Combinatorial Cut-Based Algorithm for Solving Laplacian Linear Systems</title>
    <feedworld_mtime>1604275200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henzinger:Monika.html">Monika Henzinger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Billy.html">Billy Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williamson:David_P=.html">David P. Williamson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.16316">PDF</a><br/><b>Abstract: </b>Over the last two decades, a significant line of work in theoretical
algorithms has been progress in solving linear systems of the form
$\mathbf{L}\mathbf{p} = \mathbf{b}$, where $\mathbf{L}$ is the Laplacian matrix
of a weighted graph with weights $w(i,j)&gt;0$ on the edges. The solution
$\mathbf{p}$ of the linear system can be interpreted as the potentials of an
electrical flow. Kelner, Orrechia, Sidford, and Zhu \cite{KOSZ13} give a
combinatorial, near-linear time algorithm that maintains the Kirchoff Current
Law, and gradually enforces the Kirchoff Potential Law. Here we consider a dual
version of the algorithm that maintains the Kirchoff Potential Law, and
gradually enforces the Kirchoff Current Law. We prove that this dual algorithm
also runs in a near-linear number of iterations. Each iteration requires
updating all potentials on one side of a fundamental cut of a spanning tree by
a fixed amount. If this update step can be performed in polylogarithmic time,
we can also obtain a near-linear time algorithm to solve $\mathbf{L}\mathbf{p}
= \mathbf{b}$. However, if we abstract this update step as a natural data
structure problem, we show that we can use the data structure to solve a
problem that has been conjectured to be difficult for dynamic algorithms, the
online vector-matrix-vector problem \cite{HKNS15}. The conjecture implies that
the data structure does not have an $O(n^{1-\epsilon})$ time algorithm for any
$\epsilon &gt; 0$. Thus our dual algorithm cannot be near-linear time algorithm
for solving $\mathbf{L}\mathbf{p} = \mathbf{b}$ unless we are able to take
advantage of the structure of the particular update steps that our algorithm
uses.
</p></div>
    </summary>
    <updated>2020-11-02T23:21:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.16290</id>
    <link href="http://arxiv.org/abs/2010.16290" rel="alternate" type="text/html"/>
    <title>3XOR Games with Perfect Commuting Operator Strategies Have Perfect Tensor Product Strategies and are Decidable in Polynomial Time</title>
    <feedworld_mtime>1604275200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Watts:Adam_Bene.html">Adam Bene Watts</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Helton:J=_William.html">J. William Helton</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.16290">PDF</a><br/><b>Abstract: </b>We consider 3XOR games with perfect commuting operator strategies. Given any
3XOR game, we show existence of a perfect commuting operator strategy for the
game can be decided in polynomial time. Previously this problem was not known
to be decidable. Our proof leads to a construction, showing a 3XOR game has a
perfect commuting operator strategy iff it has a perfect tensor product
strategy using a 3 qubit (8 dimensional) GHZ state. This shows that for perfect
3XOR games the advantage of a quantum strategy over a classical strategy
(defined by the quantum-classical bias ratio) is bounded. This is in contrast
to the general 3XOR case where the optimal quantum strategies can require high
dimensional states and there is no bound on the quantum advantage.
</p>
<p>To prove these results, we first show equivalence between deciding the value
of an XOR game and solving an instance of the subgroup membership problem on a
class of right angled Coxeter groups. We then show, in a proof that consumes
most of this paper, that the instances of this problem corresponding to 3XOR
games can be solved in polynomial time.
</p></div>
    </summary>
    <updated>2020-11-02T23:20:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-11-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.16177</id>
    <link href="http://arxiv.org/abs/2010.16177" rel="alternate" type="text/html"/>
    <title>Dynamic Distributed MIS with Improved Bounds</title>
    <feedworld_mtime>1604275200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Shiri Antaki, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Quanquan_C=.html">Quanquan C. Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomon:Shay.html">Shay Solomon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.16177">PDF</a><br/><b>Abstract: </b>The problem of maintaining a maximal independent set (MIS) in a dynamic graph
has received growing attention in recent years. In STOC'18, Assadi et al.
presented a distributed algorithm for maintaining an MIS with $O(1)$
(amortized) round complexity and $O(m^{3/4})$ message complexity, where $m$
denotes the dynamic number of edges in the graph. The algorithm of Assadi et
al. is deterministic; to the best of our knowledge, the state-of-the-art
distributed algorithm for this problem with a message complexity of
$o(m^{3/4})$ incurs a round complexity of $\Omega(m^{1/3})$. We propose a
deterministic distributed algorithm that achieves an amortized message
complexity of $\tilde{O}(m^{2/3})$ and an amortized round complexity of
$\tilde{O}(1)$ under fully dynamic edge updates in the CONGEST model, where
$\tilde{O}(\cdot)$ suppresses polylogarithmic factors.
</p></div>
    </summary>
    <updated>2020-11-02T23:32:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.16158</id>
    <link href="http://arxiv.org/abs/2010.16158" rel="alternate" type="text/html"/>
    <title>Glauber dynamics for colourings of chordal graphs and graphs of bounded treewidth</title>
    <feedworld_mtime>1604275200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heinrich:Marc.html">Marc Heinrich</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.16158">PDF</a><br/><b>Abstract: </b>The Glauber dynamics on the colourings of a graph is a random process which
consists in recolouring at each step a random vertex of a graph with a new
colour chosen uniformly at random among the colours not already present in its
neighbourhood. It is known that when the total number of colours available is
at least $\Delta +2$, where $\Delta$ is the maximum degree of the graph, this
process converges to a uniform distribution on the set of all the colourings.
Moreover, a well known conjecture is that the time it takes for the convergence
to happen, called the mixing time, is polynomial in the size of the graph. Many
weaker variants of this conjecture have been studied in the literature by
allowing either more colours, or restricting the graphs to particular classes,
or both. This paper follows this line of research by studying the mixing time
of the Glauber dynamics on chordal graphs, as well as graphs of bounded
treewidth. We show that the mixing time is polynomial in the size of the graph
in the two following cases:
</p>
<p>- on graphs with bounded treewidth, and at least $\Delta +2$ colours,
</p>
<p>- on chordal graphs if the number of colours is at least $(1+\varepsilon)
(\Delta +1)$, for any fixed constant $\varepsilon$.
</p></div>
    </summary>
    <updated>2020-11-02T23:24:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.16012</id>
    <link href="http://arxiv.org/abs/2010.16012" rel="alternate" type="text/html"/>
    <title>To Push or To Pull: On Reducing Communication and Synchronization in Graph Computations</title>
    <feedworld_mtime>1604275200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Besta:Maciej.html">Maciej Besta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Podstawski:Michal.html">Michal Podstawski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Groner:Linus.html">Linus Groner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomonik:Edgar.html">Edgar Solomonik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoefler:Torsten.html">Torsten Hoefler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.16012">PDF</a><br/><b>Abstract: </b>We reduce the cost of communication and synchronization in graph processing
by analyzing the fastest way to process graphs: pushing the updates to a shared
state or pulling the updates to a private state.We investigate the
applicability of this push-pull dichotomy to various algorithms and its impact
on complexity, performance, and the amount of used locks, atomics, and
reads/writes. We consider 11 graph algorithms, 3 programming models, 2 graph
abstractions, and various families of graphs. The conducted analysis
illustrates surprising differences between push and pull variants of different
algorithms in performance, speed of convergence, and code complexity; the
insights are backed up by performance data from hardware counters.We use these
findings to illustrate which variant is faster for each algorithm and to
develop generic strategies that enable even higher speedups. Our insights can
be used to accelerate graph processing engines or libraries on both
massively-parallel shared-memory machines as well as distributed-memory
systems.
</p></div>
    </summary>
    <updated>2020-11-02T23:24:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.15951</id>
    <link href="http://arxiv.org/abs/2010.15951" rel="alternate" type="text/html"/>
    <title>Active Sampling Count Sketch (ASCS) for Online Sparse Estimation of a Trillion Scale Covariance Matrix</title>
    <feedworld_mtime>1604275200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dai:Zhenwei.html">Zhenwei Dai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Desai:Aditya.html">Aditya Desai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heckel:Reinhard.html">Reinhard Heckel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shrivastava:Anshumali.html">Anshumali Shrivastava</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.15951">PDF</a><br/><b>Abstract: </b>Estimating and storing the covariance (or correlation) matrix of
high-dimensional data is computationally challenging. For this problem, both
memory and computational requirements scale quadratically with the dimension.
Fortunately, high-dimensional covariance matrices observed in text,
click-through, and meta-genomics datasets are often sparse. In this paper, we
consider the problem of efficiently estimating a sparse covariance matrix,
which can scale to matrices with trillions of entries. The scale of the
datasets requires the algorithm to be online, as any second pass over the data
is prohibitive. In this paper, we propose Active Sampling Count Sketch (ASCS),
an online and one-pass sketching algorithm, that recovers the large entries of
the covariance matrix accurately. Count Sketch (CS), and other sub-linear
compressed sensing algorithms, offer a natural solution to the problem in
theory. However, vanilla CS does not work well in practice due to a low
signal-to-noise ratio (SNR). At the heart of our approach is a novel active
sampling strategy that increases the SNR of classical count sketches. We
demonstrate the practicality of our algorithm with synthetic data and
real-world high dimensional datasets. ASCS significantly improves over vanilla
CS, demonstrating the merit of our active sampling strategy.
</p></div>
    </summary>
    <updated>2020-11-02T23:20:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.15879</id>
    <link href="http://arxiv.org/abs/2010.15879" rel="alternate" type="text/html"/>
    <title>Log(Graph): A Near-Optimal High-Performance Graph Representation</title>
    <feedworld_mtime>1604275200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Besta:Maciej.html">Maciej Besta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stanojevic:Dimitri.html">Dimitri Stanojevic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zivic:Tijana.html">Tijana Zivic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Jagpreet.html">Jagpreet Singh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoerold:Maurice.html">Maurice Hoerold</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoefler:Torsten.html">Torsten Hoefler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.15879">PDF</a><br/><b>Abstract: </b>Today's graphs used in domains such as machine learning or social network
analysis may contain hundreds of billions of edges. Yet, they are not
necessarily stored efficiently, and standard graph representations such as
adjacency lists waste a significant number of bits while graph compression
schemes such as WebGraph often require time-consuming decompression. To address
this, we propose Log(Graph): a graph representation that combines high
compression ratios with very low-overhead decompression to enable cheaper and
faster graph processing. The key idea is to encode a graph so that the parts of
the representation approach or match the respective storage lower bounds. We
call our approach "graph logarithmization" because these bounds are usually
logarithmic. Our high-performance Log(Graph) implementation based on modern
bitwise operations and state-of-the-art succinct data structures achieves high
compression ratios as well as performance. For example, compared to the tuned
Graph Algorithm Processing Benchmark Suite (GAPBS), it reduces graph sizes by
20-35% while matching GAPBS' performance or even delivering speedups due to
reducing amounts of transferred data. It approaches the compression ratio of
the established WebGraph compression library while enabling speedups of up to
more than 2x. Log(Graph) can improve the design of various graph processing
engines or libraries on single NUMA nodes as well as distributed-memory
systems.
</p></div>
    </summary>
    <updated>2020-11-02T23:28:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-11-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.15833</id>
    <link href="http://arxiv.org/abs/2010.15833" rel="alternate" type="text/html"/>
    <title>Realizability of discs with ribbons on a M\"obius strip</title>
    <feedworld_mtime>1604275200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Arthur Bikeev Igorevich <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.15833">PDF</a><br/><b>Abstract: </b>An hieroglyph on n letters is a cyclic sequence of the letters 1,2, . . . , n
of length 2n such that each letter appears in the sequence twice.Take an
hieroglyph H. Take a convex polygon with 2n sides. Put the letters in the
sequence of letters of the hieroglyph on the sides of the convexpolygon in the
same order. For each letter i glue the ends of a ribbon to thepair of sides
corresponding to the letter i. Call the resulting surface a disk with ribbons
corresponding to the hieroglyph H. An hieroglyph H is weakly realizable on the
M\"obius strip if some disk with ribbons corresponding to H can be cut out of
the M\"obius strip. We give a criterion for weak realizability, which gives a
quadratic (in the number of letters) algorithm. Our criterion is based on the
Mohar criterion for realizability of a disk with ribbons in the M\"obius strip.
</p></div>
    </summary>
    <updated>2020-11-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-11-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-10-31-ebb-and-flow-protocols-a-resolution-of-the-availability-finality-dilemma/</id>
    <link href="https://decentralizedthoughts.github.io/2020-10-31-ebb-and-flow-protocols-a-resolution-of-the-availability-finality-dilemma/" rel="alternate" type="text/html"/>
    <title>Resolving the Availability-Finality Dilemma</title>
    <summary>Guest post by Joachim Neu, Ertem Nusret Tas, and David Tse DLS and Nakamoto: Where to Go From Synchrony? An earlier blog post has explained classical models of the consensus literature from the 1980s, synchrony, asynchrony and partial synchrony. To recapitulate, the most basic network/adversary model is synchrony, where it...</summary>
    <updated>2020-11-01T05:16:00Z</updated>
    <published>2020-11-01T05:16:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-11-02T23:35:19Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/31/linkage</id>
    <link href="https://11011110.github.io/blog/2020/10/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage for a trick-or-treat-less Halloween</title>
    <summary>3d flythrough of a near-optimal TSP tour through a dataset of nearly 221 stars (\(\mathbb{M}\), via). I found the “full view of tour” a lot easier to navigate than the mini-view on the main page.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="http://www.math.uwaterloo.ca/tsp/star/gaia1.html">3d flythrough of a near-optimal TSP tour through a dataset of nearly 221 stars</a> (<a href="https://mathstodon.xyz/@11011110/105048388019149239">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24807080">via</a>). I found the “full view of tour” a lot easier to navigate than the mini-view on the main page.</p>
  </li>
  <li>
    <p><a href="http://lightherder.blogspot.com/">The light herder</a> (<a href="https://mathstodon.xyz/@11011110/105056854067105557">\(\mathbb{M}\)</a>, <a href="https://boingboing.net/2020/10/18/amazing-in-camera-patterns-with-a-video-feedback-kinetic-sculpture.html">via</a>). Dave Blair makes dynamic fractals from old-school video feedback.</p>
  </li>
  <li>
    <p><a href="https://www.siam.org/conferences/cm/program/accepted-papers/soda21-accepted-papers">Symposium on Discrete Algorithms (SODA 2021) accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/105062503320962361">\(\mathbb{M}\)</a>). Lots of interesting looking titles there, but you’ll have to search online for links to the corresponding papers.</p>
  </li>
  <li>
    <p><a href="https://tomlehrersongs.com/">Tom Lehrer has made his song lyrics public domain, or as close to it as one can legally get</a> (<a href="https://mathstodon.xyz/@11011110/105076781151866254">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24833683">via</a>, <a href="https://boingboing.net/2020/10/20/brilliant-satirist-tom-lehrers-catalog-now-in-the-public-domain.html">via2</a>). But you have to download them within four years because his domain may go away after that. In honor of which, here’s a link to an (audio-only) version of <a href="https://www.youtube.com/watch?v=IL4vWJbwmqM">a little ditty about plagiarism</a> (only be sure always to call it please research).</p>
  </li>
  <li>
    <p><a href="https://dev.scottdarby.com/chaos-ink/">Chaos ink</a> (<a href="https://mathstodon.xyz/@11011110/105082966399876773">\(\mathbb{M}\)</a>, <a href="https://boingboing.net/2020/10/18/chaos-ink-disturb-a-tank-of-virtual-liquid-metal.html">via</a>). It’s rendered to look like waves in liquid metal, but I think it’s actually some kind of reaction-diffusion equation, in which you can move your mouse around to try to control where the reactions are centered.</p>
  </li>
  <li>
    <p><a href="https://nebusresearch.wordpress.com/2020/10/21/my-all-2020-mathematics-a-to-z-statistics/">2020 Mathematics A to Z: Statistics</a> (<a href="https://mathstodon.xyz/@nebusj/105085756046257623">\(\mathbb{M}\)</a>). On the differences between statistics and mathematics, historical connections between statistics and eugenics, and new connections to algorithmic fairness.</p>
  </li>
  <li>
    <p><a href="https://sites.google.com/view/bad-math-day-spring-2020/home">Bay Area Discrete Math Day, November 21</a> (<a href="https://mathstodon.xyz/@11011110/105091339250415859">\(\mathbb{M}\)</a>). This year, it’s a day of online discrete math talks, so you don’t actually need to be in the SF Bay Area to participate.</p>
  </li>
  <li>
    <p><a href="https://liorpachter.wordpress.com/2020/09/10/sexual-harassment-case-number-1052/">Lior Pachter reports on confirmed sexual harassment within the computational geometry community, at SoCG 2016 in Boston, by Adrian Dumitrescu</a> (<a href="https://mathstodon.xyz/@11011110/105097516664950105">\(\mathbb{M}\)</a>). According to the victim, <a href="https://twitter.com/RupeiXu/status/1302069912286957571">SoCG organizers told her they would try to bar Dumitrescu from future events, but told Dumitrescu he could not be barred</a> (see also <a href="https://twitter.com/RupeiXu/status/1310211818716049409">this update</a>). She says <a href="https://twitter.com/RupeiXu/status/1303309158427615234">there was also a second victim, whose academic career was “ruined” as a result</a>.</p>
  </li>
  <li>
    <p><a href="https://sites.google.com/view/wepa2020">Fourth International Workshop on Enumeration Problems and Applications</a> (<a href="https://mathstodon.xyz/@11011110/105102101211362136">\(\mathbb{M}\)</a>). I haven’t participated in previous instances but this year I’m on the program committee. Submission deadline November 8; online workshop December 7–10.</p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2020/10/26/the-guest-publishing-scam/">Igor Pak hates journal special issues</a> (<a href="https://mathstodon.xyz/@11011110/105116591625994062">\(\mathbb{M}\)</a>). The underlying problem appears to be loss of quality control compared to regular papers. He suggests handling festschrifts as books instead, and publishing surveys and reminiscences instead of research papers in them.</p>
  </li>
  <li>
    <p><a href="http://atlas.gregas.eu/graphs/31">The 84-vertex cubic symmetric graph</a> (<a href="https://mathstodon.xyz/@11011110/105121991976224491">\(\mathbb{M}\)</a>) is drawn nicely on <a href="http://www.mathpuzzle.com/">mathpuzzle.com</a> (update of June 27) using its structure as a <a href="https://www.abstract-polytopes.com/atlas/504/156/3.html">36-heptagon symmetric tiling of a non-orientable surface of Euler characteristic \(-6\)</a>. The Petrie dual of this tiling is <a href="https://www.abstract-polytopes.com/atlas/504/156/9.html">another symmetric tiling of the same graph with 28 nonagons on a higher-genus surface</a>. Does anyone know of other sources on these tilings? Or nice 3d embeddings of their surfaces?</p>
  </li>
  <li>
    <p><a href="https://plus.maths.org/content/prize-young-mathematicians"><em>Plus</em> magazine on Cambridge’s Whitehead Prize winners</a> (<a href="https://mathstodon.xyz/@11011110/105128113002070698">\(\mathbb{M}\)</a>):  A nice general-audience explainer of</p>

    <ul>
      <li>
        <p>Maria Bruna’s derivation of macro-level models from micro-level behavior, applied to vacuum cleaner design</p>
      </li>
      <li>
        <p>Holly Krieger’s connections between prime factors in integer sequences and special points on the Mandelbrot set</p>
      </li>
      <li>
        <p>Henry Wilton on the impossibility of determining whether infinite symmetry groups have finite quotients</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://www.iflscience.com/technology/ai-camera-ruins-soccar-game-for-fans-after-mistaking-referees-bald-head-for-ball/">Silly computer news of the day: automatically aimed soccer game video camera follows bald referee’s head instead of the ball, causing fans to miss key plays</a> (<a href="https://mathstodon.xyz/@11011110/105131999819480420">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24955651">via</a>).</p>
  </li>
</ul></div>
    </content>
    <updated>2020-10-31T22:59:00Z</updated>
    <published>2020-10-31T22:59:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-11-01T06:02:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4974</id>
    <link href="https://www.scottaaronson.com/blog/?p=4974" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4974#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4974" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Complete Idiot’s Guide to the Independence of the Continuum Hypothesis: Part 1 of</title>
    <summary xml:lang="en-US">A global pandemic, apocalyptic fires, and the possible descent of the US into violent anarchy three days from now can do strange things to the soul. Bertrand Russell—and if he’d done nothing else in his long life, I’d love him forever for it—once wrote that “in adolescence, I hated life and was continually on the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>A global pandemic, apocalyptic fires, and the possible descent of the US into violent anarchy three days from now can do strange things to the soul.</p>



<p>Bertrand Russell—and if he’d done nothing else in his long life, I’d love him forever for it—once wrote that “in adolescence, I hated life and was continually on the verge of suicide, from which, however, I was restrained by the desire to know more mathematics.”  This summer, unable to bear the bleakness of 2020, I obsessively read up on the celebrated proof of the unsolvability of the <a href="https://en.wikipedia.org/wiki/Continuum_hypothesis">Continuum Hypothesis</a> (CH) from the standard foundation of mathematics, the <a href="https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory">Zermelo-Fraenkel axioms</a> of set theory.  (In this post, I’ll typically refer to “ZFC,” which means Zermelo-Fraenkel plus the famous <a href="https://en.wikipedia.org/wiki/Axiom_of_choice">Axiom of Choice</a>.)</p>



<p>For those tuning in from home, the Continuum Hypothesis was formulated by <a href="https://en.wikipedia.org/wiki/Georg_Cantor">Georg Cantor</a>, shortly after his epochal discovery that there are different orders of infinity: so for example, the infinity of real numbers (denoted C for continuum, or \( 2^{\aleph_0} \)) is strictly greater than the infinity of integers (denoted ℵ<sub>0</sub>, or “Aleph-zero”).  CH is simply the statement that there’s no infinity <em>intermediate</em> between ℵ<sub>0</sub> and C: that anything greater than the first is at least the second.  Cantor tried in vain for decades to prove or disprove CH; the quest is believed to have contributed to his mental breakdown.  When David Hilbert presented his <a href="https://en.wikipedia.org/wiki/Hilbert%27s_problems">famous list</a> of 23 unsolved math problems in 1900, CH was at the very top.</p>



<p>Halfway between Hilbert’s speech and today, the question of CH was finally “answered,” with the solution earning the only <a href="https://en.wikipedia.org/wiki/Fields_Medal">Fields Medal</a> that’s ever been awarded for work in set theory and logic.  But unlike with any previous yes-or-no question in the history of mathematics, the answer was that there provably <em>is</em> no answer from the accepted axioms of set theory!  You can either have intermediate infinities or not; neither possibility can create a contradiction.  And if you <em>do</em> have intermediate infinities, it’s up to you how many: 1, 5, 17, ∞, etc.</p>



<p>The easier half, the consistency of CH with set theory, was proved by incompleteness dude <a href="https://en.wikipedia.org/wiki/Kurt_G%C3%B6del">Kurt Gödel</a> in 1940; the harder half, the consistency of not(CH), by <a href="https://en.wikipedia.org/wiki/Paul_Cohen">Paul Cohen</a> in 1963.  Cohen’s work introduced the method of <a href="https://en.wikipedia.org/wiki/Forcing_(mathematics)">forcing</a>, which was so fruitful in proving set-theoretic questions unsolvable that it quickly took over the whole subject of set theory.  Learning Gödel and Cohen’s proofs had been a dream of mine since teenagerhood, but one I constantly put off.</p>



<p>This time around I started with <a href="https://projecteuclid.org/download/pdf_1/euclid.rmjm/1181070010">Cohen’s retrospective essay</a>, as well as Timothy Chow’s <a href="https://groups.google.com/g/sci.math.research/c/pQdPHJYML0E/m/ZrvqIxpd1sIJ?pli=1">Forcing for Dummies</a> and <a href="https://arxiv.org/abs/0712.1320">A Beginner’s Guide to Forcing</a>.  I worked through Cohen’s own <a href="https://www.amazon.com/Theory-Continuum-Hypothesis-Dover-Mathematics/dp/0486469212">Set Theory and the Continuum Hypothesis</a>, and Ken Kunen’s <a href="https://www.amazon.com/Introduction-Independence-Studies-Foundations-Mathematics/dp/0444868399">Set Theory: An Introduction to Independence Proofs</a>, and <a href="https://www2.karlin.mff.cuni.cz/~krajicek/scott67.pdf">Dana Scott’s 1967 paper</a> reformulating Cohen’s proof.  I emailed questions to Timothy Chow, who was ridiculously generous with his time.  When Tim and I couldn’t answer something, we tried <a href="https://en.wikipedia.org/wiki/Robert_M._Solovay">Bob Solovay</a> (one of the world’s great set theorists, who later worked in computational complexity and quantum computing), or <a href="https://en.wikipedia.org/wiki/Andreas_Blass">Andreas Blass</a> or <a href="http://karagila.org/">Asaf Karagila</a>.  At some point mathematician and friend-of-the-blog <a href="https://en.wikipedia.org/wiki/Greg_Kuperberg">Greg Kuperberg</a> joined my quest for understanding.  I thank all of them, but needless to say take sole responsibility for all the errors that surely remain in these posts.</p>



<p>On the one hand, the proof of the independence of CH would seem to stand with general relativity, the wheel, and the chocolate bar as a triumph of the human intellect.  It represents a culmination of Cantor’s quest to know the basic rules of infinity—all the more amazing if the answer turns out to be that, in some sense, we <em>can’t</em> know them.</p>



<p>On the other hand, perhaps no other scientific discovery of equally broad interest remains so sparsely popularized, not even (say) quantum field theory or the proof of Fermat’s Last Theorem.  I found barely any attempts to explain how forcing works to non-set-theorists, let alone to non-mathematicians.  One notable exception was Timothy Chow’s <a href="https://arxiv.org/abs/0712.1320">Beginner’s Guide to Forcing</a>, mentioned earlier—but Chow himself, near the beginning of his essay, calls forcing an “open exposition problem,” and admits that he hasn’t solved it.  My modest goal, in this post and the following ones, is to make a further advance on the exposition problem.</p>



<p>OK, but why a doofus computer scientist like me?  Why not, y’know, an actual expert?  I won’t put forward my ignorance as a qualification, although I <em>have</em> often found that the better I learn a topic, the more completely I forget what initially confused me, and so the less able I become to explain things to beginners.</p>



<p>Still, there is <em>one</em> thing I know well that turns out to be intimately related to Cohen’s forcing method, and that made me feel like I had a small “in” for this subject.  This is the construction of <a href="https://en.wikipedia.org/wiki/Oracle_machine">oracles</a> in computational complexity theory.  In CS, we like to construct hypothetical universes where P=NP or P≠NP, or P≠BQP, or the polynomial hierarchy is infinite, etc.  To do so, we, by fiat, insert a new function—an <em>oracle</em>—into the universe of computational problems, carefully chosen to make the desired statement hold.  Often the oracle needs to satisfy an infinite list of conditions, so we handle them one by one, taking care that when we satisfy a new condition we don’t invalidate the previous conditions.</p>



<p>All this, I kept reading, is <em>profoundly</em> analogous to what the set theorists do when they create a mathematical universe where the Axiom of Choice is true but CH is false, or vice versa, or any of a thousand more exotic possibilities.  They insert new sets into their models of set theory, sets that are carefully constructed to “force” infinite lists of conditions to hold.  In fact, some of the exact same people—such as <a href="https://en.wikipedia.org/wiki/Robert_M._Solovay">Solovay</a>—who helped pioneer forcing in the 1960s, later went on to pioneer oracles in computational complexity.  We’ll say more about this connection in a future post.</p>



<p><strong>How Could It Be?</strong></p>



<p>How do you study a well-defined math problem, and return the answer that, as far as the accepted axioms of math can say, there <em>is</em> no answer?  I mean: even supposing it’s <em>true</em> that there’s no answer, how do you <em>prove</em> such a thing?</p>



<p>Arguably, not even <a href="https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems">Gödel’s Incompleteness Theorem</a> achieved such a feat.  Recall, the Incompleteness Theorem says loosely that, for every formal system F that could possibly serve as a useful foundation for mathematics, there exist statements even of elementary arithmetic that are true but unprovable in F—and Con(F), a statement that encodes F’s own consistency, is an example of one.  But the very statement that Con(F) is unprovable is equivalent to Con(F)’s being <em>true</em> (since an inconsistent system could prove anything, including Con(F)).  In other words, if the Incompleteness Theorem as applied to F holds any interest, then that’s only because F <em>is, in fact</em>,<em> consistent</em>; it’s just that resources beyond F are needed to prove this.</p>



<p>Yes, there’s a “self-hating theory,” F+Not(Con(F)), which believes in its own inconsistency.  And yes, by Gödel, this self-hating theory is <em>consistent</em> if F itself is.  This means that it has a <strong>model</strong> (involving “nonstandard integers,” formal artifacts that effectively promise a proof of F’s inconsistency without ever actually delivering it).  We’ll have much, <em>much</em> more to say about models later on, but for now, they’re just collections of objects, along with relationships between the objects, that satisfy all the axioms of a theory (thus, a model of the axioms of group theory is simply … any group!).  In any case, though, the self-hating theory ZF+Not(Con(ZF)) can’t be <em>arithmetically sound</em>: I mean, just look at it!  It’s either unsound because F is consistent, or else it’s unsound because F is inconsistent.</p>



<p>The independence of CH from the ZFC axioms of set theory is of a fundamentally different kind.  It will give us models of ZFC+CH, and models of ZFC+Not(CH), that are <em>both</em> at least somewhat plausible as “sketches of mathematical reality”—and that both even have defenders.  The question of which is right, or whether it’s possible to decide at all, will be punted to the future: to the discovery (or not) of some intuitively compelling foundation for mathematics that, as Gödel hoped, answers the question by going beyond ZFC.</p>



<p><strong>Four Levels to Unpack</strong></p>



<p>While experts might consider this too obvious to spell out, Gödel’s and Cohen’s analyses of CH aren’t so much about infinity, as they are about our ability to <em>reason </em>about about infinity using finite sequences of symbols.  The game is about building self-contained mathematical universes to order—universes where all the accepted axioms about infinite sets hold true, and yet that, in some cases, seem to mock what those axioms were supposed to <em>mean</em>, by containing vastly fewer objects than the mathematical universe was “meant” to have.</p>



<p>In understanding these proofs, the central hurdle, I think, is that there are at least four different “levels of description” that need to be kept in mind simultaneously.</p>



<p>At the first level, Gödel’s and Cohen’s proofs, like all mathematical proofs, are finite sequences of symbols.  Not only that, they’re proofs that can be formalized in elementary arithmetic (!).  In other words, even though they’re <em>about</em> the axioms of set theory, they don’t themselves <em>require</em> those axioms.  Again, this is possible because, at the end of the day, Gödel’s and Cohen’s proofs won’t be talking about infinite sets, but “only” about finite sequences of symbols that make statements about infinite sets.</p>



<p>At the second level, the proofs are making an “unbounded” but perfectly clear claim.  They’re claiming that, if someone showed you a proof of either CH or Not(CH), from the ZFC axioms of set theory, then no matter how long the proof or what its details, you could convert it into a proof that <em>ZFC itself</em> was inconsistent.  In symbols, they’re proving the “relative consistency statements”</p>



<p>Con(ZFC) ⇒ Con(ZFC+CH),<br/>Con(ZFC) ⇒ Con(ZFC+Not(CH)),</p>



<p>and they’re proving these as theorems of <em>elementary arithmetic</em>.  (Note that there’s no hope of proving Con(ZF+CH) or Con(ZFC+Not(CH)) <em>outright</em> within ZFC, since by Gödel, ZFC can’t even prove its own consistency.)</p>



<p>This translation is completely explicit; the independence proofs even yield <em>algorithms</em> to convert proofs of inconsistencies in ZFC+CH or ZFC+Not(CH), supposing that they existed, into proofs of inconsistencies in ZFC itself.</p>



<p>Having said that, as Cohen himself often pointed out, thinking about the independence proofs in terms of algorithms to manipulate sequences of symbols is hopeless: to have any chance of understanding these proofs, let alone coming up with them, at some point you need to think about what the symbols <em>refer</em> to.</p>



<p>This brings us to the third level: the symbols refer to <em>models of set theory</em>, which could also be called “mathematical universes.”  Crucially, we always can and often will take these models to be only <em>countably</em> infinite: that is, to contain an infinity of sets, but “merely” ℵ<sub>0</sub> of them, the infinity of integers or of finite strings, and no more.</p>



<p>The fourth level of description is from within the models themselves: each model imagines itself to have an <em>uncountable</em> infinity of sets.  As far as the model’s concerned, it comprises the entire mathematical universe, even though “looking in from outside,” we can see that that’s not true.  In particular, each model of ZFC <em>thinks</em> it has uncountably many sets, many themselves of uncountable cardinality, even if “from the outside” the model is countable.</p>



<p>Say what?  The models are <em>mistaken</em> about something as basic as their own size, about how many sets they have?  Yes.  The models will be like <em>The Matrix</em> (the movie, not the mathematical object), or <em>The Truman Show</em>.  They’re self-contained little universes whose inhabitants can never discover that they’re living a lie—that they’re missing sets that we, from the outside, know to exist.  The poor denizens of the Matrix will never even be able to learn that their universe—what<em> </em>they mistakenly think of as <em>the</em> universe—is secretly countable!  And no <a href="https://en.wikipedia.org/wiki/Morpheus_(The_Matrix)">Morpheus</a> will ever arrive to enlighten them, although—and this is crucial to Cohen’s proof in particular—the inhabitants will be able to reason more-or-less intelligibly about what would happen if a Morpheus <em>did</em> arrive.</p>



<p>The <a href="https://en.wikipedia.org/wiki/L%C3%B6wenheim%E2%80%93Skolem_theorem">Löwenheim-Skolem Theorem</a>, from the early 1920s, says that <em>any</em> countable list of first-order axioms that has any model at all (i.e., that’s consistent), must have a model with at most countably many elements.  And ZFC is a countable list of first-order axioms, so Löwenheim-Skolem applies to it—even though ZFC implies the existence of an uncountable infinity of sets!  Before taking the plunge, we’ll need to not merely grudgingly accept but love and internalize this <a href="https://en.wikipedia.org/wiki/Skolem%27s_paradox">“paradox,”</a> because pretty much the entire proof of the independence of CH is built on top of it.</p>



<p>Incidentally, once we realize that it’s possible to build self-consistent yet “fake” mathematical universes, we can ask the question that, incredibly, the <em>Matrix</em> movies never ask.  Namely, how do we know that our own, larger universe isn’t similarly a lie?  The answer is that we don’t!  As an example—I hope you’re sitting down for this—even though Cantor proved that there are uncountably many real numbers, that only means there are uncountably many reals <em>for us</em>.  We can’t rule out the possibly that God, looking down on our universe, would see countably many reals.</p>



<p><strong>Cantor’s Proof Revisited</strong></p>



<p>To back up: the whole story of CH starts, of course, with Cantor’s epochal discovery of the different orders of infinity, that for example, there are more subsets of positive integers (or equivalently real numbers, or equivalently infinite binary sequences) than there are positive integers.  The devout Cantor thought his discovery illuminated the nature of God; it’s never been entirely obvious to me that he was wrong.</p>



<p>Recall how Cantor’s proof works: we suppose by contradiction that we have an enumeration of all infinite binary sequences: for example,</p>



<p>s(0) = <strong>0</strong>0000000…<br/>s(1) = 0<strong>1</strong>010101…<br/>s(2) = 11<strong>0</strong>01010….<br/>s(3) = 100<strong>0</strong>0000….</p>



<p>We then produce a new infinite binary sequence that’s not on the list, by going down the diagonal and flipping each bit, which in the example above would produce 1011…</p>



<p>But look more carefully.  What Cantor really shows is only that, <em>within our mathematical universe</em>, there can’t be an enumeration of all the reals of our universe.  For if there were, we could use it to define a new real that was in the universe but not in the enumeration.  The proof doesn’t rule out the possibility that <em>God</em> could enumerate the reals of our universe!  It only shows that, if so, there would need to be additional, heavenly reals that were missing from even God’s enumeration (for example, the one produced by diagonalizing against <em>that</em> enumeration).</p>



<p>Which reals could possibly be “missing” from our universe?  Every real you can name—42, π, √e, even uncomputable reals like <a href="https://en.wikipedia.org/wiki/Chaitin%27s_constant">Chaitin’s Ω</a>—has to be there, right?  Yes, and there’s the rub: <em>every real you can name</em>.  Each name is a finite string of symbols, so whatever your naming system, you can only ever name countably many reals, leaving 100% of the reals nameless.</p>



<p>Or did you think of only the <em>rationals</em> or <em>algebraic numbers</em> as forming a countable dust of discrete points, with numbers like π and e filling in the solid “continuum” between them?  If so, then I hope you’re sitting down for this: <em>every real number you’ve ever heard of</em> belongs to the countable dust!  The entire concept of “the continuum” is only needed for reals that don’t have names and never will.</p>



<p><strong>From ℵ<sub>0</sub> Feet</strong></p>



<p>Gödel and Cohen’s achievement was to show that, without creating any contradictions in set theory, we can adjust size of this elusive “continuum,” put more reals into it or fewer.  How does one even start to begin to prove such a statement?</p>



<p>From a distance of ℵ<sub>0</sub> feet, Gödel proves the consistency of CH by building minimalist mathematical universes: one where “the only sets that exist, are the ones <em>required</em> to exist by the ZFC axioms.”  (These universes can, however, differ from each other in how “tall” they are: that is, in how many <a href="https://en.wikipedia.org/wiki/Ordinal_number">ordinals</a> they have, and hence how many sets overall.  More about that in a future post!)  Gödel proves that, <em>if</em> the axioms of set theory are consistent—that is, if they describe any universes at all—then they also describe these minimalist universes.  He then proves that, in any of these minimalist universes, from the standpoint of someone <em>within</em> that universe, there are exactly ℵ<sub>1</sub> real numbers, and hence CH holds.</p>



<p>At an equally stratospheric level, Cohen proves the consistency of not(CH) by building … well, <em>non</em>-minimalist mathematical universes!  A simple way is to start with Gödel’s minimalist universe—or rather, an even more minimalist universe than his, one that’s been cut down to have only countably many sets—and then to stick in a bunch of <em>new real numbers</em> that weren’t in that universe before.  We choose the new real numbers to ensure two things: first, we still have a model of ZFC, and second, that we make CH false.  The details of how to do that will, of course, concern us later.</p>



<p><strong>My Biggest Confusion</strong></p>



<p>In subsequent posts, I’ll say more about the character of the ZFC axioms and how one builds models of them to order.  Just as a teaser, though, to conclude this post I’d like to clear up a fundamental misconception I had about this subject, from roughly the age of 16 until a couple months ago.</p>



<p>I thought: the way Gödel proves the consistency of CH, must be by examining all the sets in his minimalist universe, and checking that each one has either at most ℵ<sub>0</sub> elements or else at least C of them.  Likewise, the way Cohen proves the consistency of not(CH), must be by “forcing in” some extra sets, which have more than ℵ<sub>0</sub> elements but fewer than C elements.</p>



<p>Except, it turns out that’s not how it works.  Firstly, to prove CH in his universe, Gödel is not going to check each set to make sure it doesn’t have intermediate cardinality; instead, he’s simply going to count all the reals to make sure that there are only ℵ<sub>1</sub> of them—where <a href="https://en.wikipedia.org/wiki/Aleph_number#Aleph-one">ℵ<sub>1</sub></a> is the next infinite cardinality after ℵ<sub>0</sub>.  This will imply that C=ℵ<sub>1</sub>, which is another way to state CH.</p>



<p>More importantly, to build a universe where CH is false, Cohen is going to start with a universe where C=ℵ<sub>1</sub>, like Gödel’s universe, and then <em>add in more reals:</em> say, ℵ<sub>2</sub> of them.  The ℵ<sub>1</sub> “original” reals will then supply our set of intermediate cardinality between the ℵ<sub>0</sub> integers and the ℵ<sub>2</sub> “new” reals.</p>



<p>Looking back, the core of my confusion was this.  I had thought: I can visualize what ℵ<sub>0</sub> means; that’s just the infinity of integers.  I can <em>also</em> visualize what \( C=2^{\aleph_0} \) means; that’s the infinity of points on a line.  Those, therefore, are the two bedrocks of clarity in this discussion.  By contrast, I <em>can’t</em> visualize a set of intermediate cardinality between ℵ<sub>0</sub> and C.  The intermediate infinity, being weird and ghostlike, is the one that shouldn’t exist unless we deliberately “force” it to.</p>



<p>Turns out I had things backwards.  For starters, I <em>can’t</em> visualize the uncountable infinity of real numbers.  I might <em>think</em> I’m visualizing the real line—it’s solid, it’s black, it’s got little points everywhere—but how can I be sure that I’m not merely visualizing the ℵ<sub>0</sub> rationals, or (say) the computable or definable reals, which include all the ones that arise in ordinary math?</p>



<p>The continuum C is <em>not at all</em> the bedrock of clarity that I’d thought it was.  Unlike its junior partner ℵ<sub>0</sub>, the continuum is adjustable, changeable—and we <em>will</em> change it when we build different models of ZFC.  What’s (relatively) more “fixed” in this game is something that I, like many non-experts, had always given short shrift to: Cantor’s sequence of Alephs ℵ<sub>0</sub>, ℵ<sub>1</sub>, ℵ<sub>2</sub>, etc.</p>



<p>Cantor, who was a very great man, didn’t merely discover that C&gt;ℵ<sub>0</sub>; he also discovered that the infinite cardinalities form a <a href="https://en.wikipedia.org/wiki/Well-order">well-ordered</a> sequence, with no infinite descending chains.  Thus, after ℵ<sub>0</sub>,  there’s a next greater infinity that we call ℵ<sub>1</sub>; after ℵ<sub>1</sub> comes  ℵ<sub>2</sub>; after the entire infinite sequence ℵ<sub>0</sub>,ℵ<sub>1</sub>,ℵ<sub>2</sub>,ℵ<sub>3</sub>,… comes ℵ<sub>ω</sub>; after ℵ<sub>ω</sub> comes ℵ<sub>ω+1</sub>; and so on.  These infinities will always be there in any universe of set theory, and always in the same order.</p>



<p>Our job, as engineers of the mathematical universe, will include pegging the continuum C to one of the Alephs.  If we stick in a bare minimum of reals, we’ll get C=ℵ<sub>1</sub>, if we stick in more we can get C=ℵ<sub>2</sub> or C=ℵ<sub>3</sub>, etc.  We can’t make C equal to ℵ<sub>0</sub>—that’s Cantor’s Theorem—and we <em>also</em> can’t make C equal to ℵ<sub>ω</sub>, by an important <a href="https://en.wikipedia.org/wiki/K%C3%B6nig%27s_theorem_(set_theory)">theorem of König</a> that we’ll discuss later (yes, this is an umlaut-heavy field).  But it will turn out that we can make C equal to just about any other Aleph: in particular, to any infinity other than ℵ<sub>0</sub> that’s not the supremum of a countable list of smaller infinities.</p>



<p>In some sense, this is the whole journey that we need to undertake in this subject: from seeing the cardinality of the continuum as a metaphysical mystery, which we might contemplate by staring really hard at a black line on white paper, to seeing the cardinality of the continuum <em>as an engineering problem</em>.</p>



<p>Stay tuned!  Next installment coming after the civilizational Singularity in three days, assuming there’s still power and Internet and food and so forth.</p>



<p>Oh, and happy Halloween.  Ghostly sets of intermediate cardinality … spoooooky!</p></div>
    </content>
    <updated>2020-10-31T19:51:21Z</updated>
    <published>2020-10-31T19:51:21Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Metaphysical Spouting"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-11-02T20:52:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7857</id>
    <link href="https://windowsontheory.org/2020/10/30/digging-into-election-models/" rel="alternate" type="text/html"/>
    <title>Digging into election models</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">With election on my mind, and constantly looking at polls and predictions, I thought I would look a little more into how election models are made. (Disclaimer: I am not an expert statistician / pollster and this is based on me trying to read their methodological description as well as looking into results of simulations … <a class="more-link" href="https://windowsontheory.org/2020/10/30/digging-into-election-models/">Continue reading <span class="screen-reader-text">Digging into election models</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>With election on my mind, and constantly looking at polls and predictions, I thought I would look a little more into how election models are made. (<strong>Disclaimer:</strong> I am not an expert statistician / pollster and this is based on me trying to read their methodological description as well as looking into results of simulations in Python. However, there is a <a href="https://colab.research.google.com/drive/1GaoBn71PwIk_uAisNWzp2Naqev9JHW0I?usp=sharing">colab notebook</a> so you can try this on your own!)</p>



<p>If polls were 100% accurate, then we would not need election models – we will know that the person polling at more than 50% in a given state will win, and we can just sum up the electoral votes. However, polls have various sources of errors:</p>



<ol><li><strong>Statistical sample error</strong> –  this is simply the deviation between the fraction of people that would say “I will vote for X” at time T in the population, and the empirical fraction reported by the poll based on their sample. As battleground states get polled frequently with large samples, this error is likely to be negligible.</li><li><strong>Sampling bias</strong> – this is the bias incurred by the fact that we cannot actually sample a random subset of the population and get them to answer our questions – the probability that people will pick up their phone may be correlated with their vote. Pollsters hope that these correlations all disappear once you condition on certain demographic variables (race, education, etc..) and so try to ensure the sample is balanced according to these metrics. I believe this was part of the reason that polls were off in 2016, since they didn’t explicitly adjust for levels of education (which were not strongly correlated with party before) and ended up under-representing white voters without college degrees.</li><li><strong>Lying responses or “shy” voters</strong> – Some people suggest that voters lie to pollsters because their choice is considered “socially undesirable”. There is <a href="https://fivethirtyeight.com/features/trump-supporters-arent-shy-but-polls-could-still-be-missing-some-of-them/">not much support</a> that this is a statistically significant effect. In particular <a href="https://morningconsult.com/form/shy-trump-2020/">one study</a> showed there was no statistically significant difference between responders’ responses in online and live calling. Also in  2016 polls equally under-estimated the votes for Trump and Republican senators (which presumably didn’t have the same “social stigma” to them).</li><li><strong>Turnout estimates</strong> – Estimating the probability that a person supporting candidate X will actually show up to vote (or mail it in) is a bit of a dark art, and account for the gap in polls representing registered voters (which make no such estimates) and polls representing likely voters (which do). Since traditionally the Republican electorate is older and more well off, they tend to vote more reliably and hence likely voter estimates are typically better for republicans. The effect seems not to be very strong this year. Turnout might be particularly hard to predict this year, though it seems likely to be historically high.</li><li><strong>Voters changing their mind</strong> –  The poll is done at a given point in time and does not necessarily reflect voters views in election day. For example in 2016 it seems that many undecided voters broke for Trump. In this cycle the effect might be less pronounced since there are few undecided voters and “election day” is smoothed over a 2-4 week period due to early and mail-in voting.</li></ol>



<p>To a first approximation, a poll-based election model does the following:<br/><br/>1. Aggregates polls into national and state-wise predictions </p>



<p>2. Computes a probability distribution over the correlated error vectors (i.e. the vector <img alt="\vec{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Be%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\vec{e}"/>  with coordinate for each jurisdiction containing the deviation from the prediction)</p>



<p>3. Samples from the probability distribution over vectors to obtain probabilities over outcomes.</p>



<p>From a skim of <a href="https://fivethirtyeight.com/features/how-fivethirtyeights-2020-presidential-forecast-works-and-whats-different-because-of-covid-19/">538’s methodology</a> it seems that they do the following:</p>



<ol><li>Aggregate polls (weighing by quality, timeliness, adjusting for house effects, etc..). Earlier in the election cycle they also mix in “fundamentals” such as state of the economy etc.. though their weight decreases with time.</li><li>Estimate magnitude of national error (i.e., sample a value <img alt="E \in \mathbb{R}_+" class="latex" src="https://s0.wp.com/latex.php?latex=E+%5Cin+%5Cmathbb%7BR%7D_%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="E \in \mathbb{R}_+"/> according to some distribution that reflects the amount of national uncertainty.</li><li>(This is where I may be understanding wrong.) Sample a vector <img alt="\vec{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Be%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\vec{e}"/> whose entries sum up to <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> according to a correlated distribution, where the correlations between states depends on demographic, location, and other  factors. For each particular choice of $E$, because the sum is fixed, if a state has $E+X$ bias then on average the other states will need to compensate for this $-X$ bias, and hence this can create <a href="https://statmodeling.stat.columbia.edu/2020/10/24/reverse-engineering-the-problematic-tail-behavior-of-the-fivethirtyeight-presidential-election-forecast/">negative correlations</a> between states. (It is not clear that negative correlations are unreasonable – one could imagine policies that are deeply popular with population A and deeply unpopular with population B)</li></ol>



<p>From a skim of the <a href="https://projects.economist.com/us-2020-forecast/president/how-this-works">Economist’s methodology</a> it seems that they do the following:</p>



<ol><li>They start again with some estimate on the national popular vote, based on polls and fundamentals, and then assume it is distributed according to some probability distribution to account for errors.</li><li>They then compute some prior on “partisan lean” (difference between state and national popular vote) for each state. If we knew the popular vote and partisan lean perfectly then we would know the result. Again like good Bayesians they assume that the lean is distributed according to some probability distribution.</li><li>They update the prior based on state polls and other information</li><li>They sample from an error distribution <img alt="\vec{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Be%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\vec{e}"/> according to some explicit pairwise correlation matrix that has only non-negative entries (and hence you don’t get negative correlations in their model).</li></ol>



<p>So, given all of the above, how much do these models differ? Perhaps surprisingy, the answer is “not by much”.  To understand how they differ, I plotted for both models the following: </p>



<ol><li>The histogram of Biden’s popular vote margin</li><li>The probability of Biden to win conditioned on a particular margin</li></ol>



<p>Much of the methodological difference, including the issue of pairwise correlations, should manifest in 2, but eyeballing it, they don’t seem to differ that much. It seems that conditioned on a particular margin, both models give Biden similar probability to win. (In particular both models think that 3% margin is about 50/50, while 4% margin gives Biden about 80/20 chance). The main difference is actually in the first part of estimating the popular vote margin – 538 is more “conservative” and has fatter tails.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/election_models.png"><img alt="" class="wp-image-7867" src="https://windowsontheory.files.wordpress.com/2020/10/election_models.png?w=993"/></a></figure>



<p>If you want to check my data, see if I have a bug, or try your own analysis, you can use this <a href="https://colab.research.google.com/drive/1GaoBn71PwIk_uAisNWzp2Naqev9JHW0I?usp=sharing">colab notebook</a>.</p>



<h2>“Make your own needle”</h2>



<p>Another applications for such models is to help us adjust the priors as new information comes in. For example, it’s possible that Florida, North Carolina and Texas will report results early. If Biden loses one of these states, should we adjust our estimate of win probability significantly? It turns out that the answer depends on by how much he loses.</p>



<p>The following graphs show the updated win probability conditioned on a particular margin in a state. We see that winning or losing Florida, North Carolina, and Texas on their own doesn’t make much difference to the probability – it’s all about the margin. In contrast, losing Pennsylvania’s 20 electoral votes will make a significant difference to Biden’s chances.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/predict_states.png"><img alt="" class="wp-image-7869" src="https://windowsontheory.files.wordpress.com/2020/10/predict_states.png?w=1024"/></a></figure>



<p>(The non monotonicity is simply a side effect of having a finite number of simulation runs and would disappear in the limit.)</p></div>
    </content>
    <updated>2020-10-30T16:16:57Z</updated>
    <published>2020-10-30T16:16:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-02T23:34:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7851</id>
    <link href="https://windowsontheory.org/2020/10/30/im-with-her-but-4-years-too-late/" rel="alternate" type="text/html"/>
    <title>I’m with her (but 4 years too late)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In May 2016, after Donald Trump was elected as the republican nominee for president, I wrote the following blog post. I ended up not publishing it – this has always been a technical blog (and also more of a group blog, at the time). While the damage of a Donald Trump presidency was hypothetical at … <a class="more-link" href="https://windowsontheory.org/2020/10/30/im-with-her-but-4-years-too-late/">Continue reading <span class="screen-reader-text">I’m with her (but 4 years too late)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In May 2016, after Donald Trump was elected as the republican nominee for president, I wrote the following blog post. I ended up not publishing it – this has always been a technical blog (and also more of a group blog, at the time).  While the damage of a Donald Trump presidency was hypothetical at the time, it is now very real and in a second term the stakes are only higher. I once again hope American readers of this blog would do what they can to support Joe Biden.<br/></p>



<p>Note: this is not an invitation for a debate on who to vote for in the comments. At this point, if you are educated and following the news (as I imagine all readers of this blog are) then if you are not already convinced that Donald Trump is a danger to this country and the world, nothing I will say will change your mind. Similarly, nothing you will say will change mine. Hence this is just a call for those readers who already support Joe Biden to make sure they vote and think of how they can help with their money or time in other ways.</p>



<h2>I’m with Her</h2>



<p><strong>Boaz Barak / May 3, 2016</strong><br/><br/>The republican electorate, in their infinite wisdom, have just (essentially) finalized the election of Donald Trump as their nominee for the position of the president of the United States.</p>



<p>While I have my political views, I don’t consider myself a very political person, and have not (as far as I remember) ever written about politics in this blog. However, extreme times call for extreme measures. It is tempting to be fatalist or cynical about this process, and believe that whether Donald Trump or Hillary Clinton is elected doesn’t make much of a difference. Some people believe that the presidency shapes the person more than the other way around, and others feel that all politicians are anyway corrupt or that Hillary is not much better than trump since she voted for the Iraq war and gave paid speeches for Wall Street firms. I think the last two presidencies of George W. Bush and Barack Obama demonstrated that the identity of the president makes a huge difference. All evidence points out that with Trump this difference will be entirely in the negative direction.</p>



<p>While his chances might not be great, this is not a bet I’m comfortable taking. I plan to support Hillary Clinton as much as I can, and hope that other American readers of this blog will do the same</p></div>
    </content>
    <updated>2020-10-30T14:20:47Z</updated>
    <published>2020-10-30T14:20:47Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-02T23:34:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/29/postdoc-in-causal-inference-ml-theory-at-university-of-chicago-and-carnegie-mellon-university-apply-by-january-30-2021/</id>
    <link href="https://cstheory-jobs.org/2020/10/29/postdoc-in-causal-inference-ml-theory-at-university-of-chicago-and-carnegie-mellon-university-apply-by-january-30-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc in Causal Inference / ML Theory at University of Chicago and Carnegie Mellon University (apply by January 30, 2021)</title>
    <summary>Applications are invited for a postdoctoral researcher working on problems in causal inference and graphical models. The candidate will be given the opportunity to pursue a broad research agenda at the intersection of theoretical statistics, machine learning, and causal inference. This is part of a collaborative project between Bryon Aragam (UChicago), Pradeep Ravikumar (CMU), and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are invited for a postdoctoral researcher working on problems in causal inference and graphical models. The candidate will be given the opportunity to pursue a broad research agenda at the intersection of theoretical statistics, machine learning, and causal inference. This is part of a collaborative project between Bryon Aragam (UChicago), Pradeep Ravikumar (CMU), and Eric Xing (CMU).</p>
<p>Website: <a href="http://bryonaragam.com/uchicago_cmu_postdoc_announcement.pdf">http://bryonaragam.com/uchicago_cmu_postdoc_announcement.pdf</a><br/>
Email: bryon@chicagobooth.edu</p></div>
    </content>
    <updated>2020-10-29T22:03:53Z</updated>
    <published>2020-10-29T22:03:53Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-02T23:34:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=505</id>
    <link href="https://tcsplus.wordpress.com/2020/10/29/tcs-talk-wednesday-november-4-shalev-ben-david-university-of-waterloo/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, November 4 — Shalev Ben-David, University of Waterloo</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, November 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Shalev Ben-David from University of Waterloo will speak about “Forecasting Algorithms, Minimax Theorems, and Randomized Lower Bounds” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, November 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Shalev Ben-David</strong> from University of Waterloo will speak about “<em>Forecasting Algorithms, Minimax Theorems, and Randomized Lower Bounds</em>” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: I will present a new approach to randomized lower bounds, particularly in the setting where we wish to give a fine-grained analysis of randomized algorithms that achieve small bias. The approach is as follows: instead of considering ordinary randomized algorithms which give an output in <img alt="\{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\{0,1\}"/> and may err, we switch models to look at “forecasting” randomized algorithms which output a confidence in <img alt="[0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="[0,1]"/> for whether they think the answer is 1. When scored by a proper scoring rule, the performance of the best forecasting algorithm is closely related to the bias of the best (ordinary) randomized algorithm, but is more amenable to analysis. <br/>As an application, I’ll present a new minimax theorem for randomized algorithms, which can be viewed as a strengthening of Yao’s minimax theorem. Yao’s minimax theorem guarantees the existence of a hard distribution for a function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="f"/> such that solving <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="f"/> against this distribution (to a desired error level) is as hard as solving <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="f"/> in the worst case (to that same error level). However, the hard distribution provided by Yao’s theorem depends on the chosen error level. Our minimax theorem removes this dependence, giving a distribution which certifies the hardness of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="f"/> against all bias levels at once. In recent work, we used this minimax theorem to give a tight composition theorem for randomized query complexity. <br/><br/>Based on joint work with Eric Blais.</p></div>
    </content>
    <updated>2020-10-29T19:35:16Z</updated>
    <published>2020-10-29T19:35:16Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-11-02T23:35:01Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6849296165197348032</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6849296165197348032/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/2020-fall-jobs-post.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6849296165197348032" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6849296165197348032" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/2020-fall-jobs-post.html" rel="alternate" type="text/html"/>
    <title>2020 Fall Jobs Post</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My annual fall jobs posts, giving advice to PhDs looking for faculty positions, were getting repetitive. See <a href="https://blog.computationalcomplexity.org/2019/10/2019-fall-jobs-post.html">last year's post</a> for the usual stuff and feel free to post opportunities in the comments on this post. This year let's talk about what's changed.<div><br/></div><div>First something you might have missed--the latest <a href="https://cra.org/wp-content/uploads/2020/05/2019-Taulbee-Survey.pdf">Taulbee Survey</a> shows a small drop in the number of new undergraduate CS majors in 2019 after years of massive growth. Is it a simple blip or have we hit the saturation point? We may never know because of the change you definitely didn't miss seeing.</div><div><br/></div><div>So let's talk about the effect of COVID. We're seeing a large drop in new international students who are having a hard time getting visas or just avoiding the US like the plague (literally). Not sure those numbers will fully come back given other alternatives and a changing international relationship particularly with China. Many undergrads have delayed college and some may never attend. </div><div><br/></div><div>On the other hand, COVID has accelerated digitizing the economy, from videoconferencing to in-home entertainment and games to remote access to work environments. The post-COVID economy could create even a larger demand for computer scientists. </div><div><br/></div><div>Many universities curtailed hiring last year worried and may do so in the spring given the uncertain budget situation due to the virus. Others continue to hire in computing, some to make up for last year. I just can't make a prediction for the upcoming year but I wouldn't rush to the job market if you have the option to wait. Last year the CRA reinstated the <a href="https://cifellows2020.org/">CIFellows</a>, postdocs to help those in a tight job market, and may do so again next year. The CRA will also repeat its <a href="https://cra.org/cras-cv-database-initiative-turns-two/">CV database</a>.</div><div><br/></div><div>Late spring interviews in 2020 went virtual and will likely go virtual again in 2021. Nevertheless take the zoom meetings seriously. Make sure your interview talk is still a discussion--answer questions people give in the chat. Still do your research to have strong conversations with everyone you talk to, especially the non-theorists. And still dress nicely, at least from the waist up. Putting on a sports coat is not a bad idea. </div><div><div><br/></div><div>Though most places continue to focus on data science/ML, cybersecurity and to some degree quantum, Rutgers is <a href="https://www.cs.rutgers.edu/about/employment/details/tenure-track-assistant-professor-position-2">specifically looking</a> for a hire in computational complexity. Don't see that everyday. </div></div></div>
    </content>
    <updated>2020-10-28T22:27:00Z</updated>
    <published>2020-10-28T22:27:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-11-02T22:09:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/tpdp2020/</id>
    <link href="https://differentialprivacy.org/tpdp2020/" rel="alternate" type="text/html"/>
    <title>Conference Digest - TPDP 2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://tpdp.journalprivacyconfidentiality.org/2020/">TPDP 2020</a> is a workshop focused on differential privacy. As such, it’s a great place to learn about recent developments in the DP research community.
It will be held on 13 November and is co-located with <a href="https://www.sigsac.org/ccs/CCS2020/">CCS</a>, but, of course, it’s virtual this year. <a href="https://www.sigsac.org/ccs/CCS2020/registration.html">Registration is only US$35 if you register by Friday, 30 October.</a> Check out the 8 excellent talks and 71 posters below – wow, the workshop has grown!</p>

<p>Please let us know if there are any errors or omissions.</p>

<h2 id="invited-talks">Invited Talks</h2>

<ul>
  <li>
    <p>OpenDP: A Community Effort to Build Trustworthy Differential Privacy Software.<br/>
 <a href="https://salil.seas.harvard.edu/">Salil Vadhan</a></p>
  </li>
  <li>
    <p><a href="https://cilvento.org/">Christina Ilvento</a></p>
  </li>
</ul>

<h2 id="contributed-talks">Contributed Talks</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2009.09052">Private Reinforcement Learning with PAC and Regret Guarantees</a><br/>
 Giuseppe Vietri, Borja Balle, Akshay Krishnamurthy, Z. Steven Wu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.07709">Auditing Differentially Private Machine Learning: How Private is Private SGD?</a><br/>
 Matthew Jagielski, Jonathan Ullman, Alina Oprea</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.06783">Characterizing Private Clipped Gradient Descent on Convex Generalized Linear Problems</a><br/>
 Shuang Song, Om Thakkar, Abhradeep Thakurta</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.10941">Private Query Release Assisted by Public Data</a><br/>
 Raef Bassily, Albert Cheu, Shay Moran, Aleksandar Nikolov, Jonathan Ullman, Z. Steven Wu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.00563">An Equivalence Between Private Classification and Online Prediction</a><br/>
 Mark Bun, Roi Livni, Shay Moran</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.09745">Differentially Private Set Union</a><br/>
Sivakanth Gopi, Pankaj Gulhane, Janardhan Kulkarni, Judy Hanwen Shen, Milad Shokouhi, Sergey Yekhanin</p>
  </li>
</ul>

<h2 id="posters">Posters</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2004.00010">The Discrete Gaussian for Differential Privacy</a><br/>
Clément Canonne, Gautam Kamath, Thomas Steinke</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.09465">Locally Private Hypothesis Selection</a><br/>
 Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Z. Steven Wu, Huanyu Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.05839">LinkedIn’s Audience Engagements API: A Privacy Preserving Data Analytics System at Scale</a><br/>
 Ryan Rogers, Subbu Subramaniam, Sean Peng, David Durfee, Seunghyun Lee, Santosh Kumar Kancha, Shraddha Sahay, Parvez Ahammad</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.14717">Differentially Private Decomposable Submodular Maximization</a><br/>
 Anamay Chaturvedi, Huy Nguyen, Lydia Zakynthinou</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.11934">Private Post-GAN Boosting</a><br/>
 Marcel Neunhoeffer, Z. Steven Wu, Cynthia Dwork</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.01100">Efficient, Noise-Tolerant, and Private Learning via Boosting</a><br/>
 Marco Carmosino, Mark Bun, Jessica Sorrell</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.04509">Closure Properties for Private Classification and Online Prediction</a><br/>
by Noga Alon, Amos Beimel, Shay Moran, Uri Stemmer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.12018">Overlook: Differentially Private Exploratory Visualization for Big Data</a><br/>
Pratiksha Thaker, Mihai Budiu, Parikshit Gopalan, Udi Wieder, Matei Zaharia</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.01980">On the Equivalence between Online and Private Learnability beyond Binary Classification</a><br/>
Young Hun Jung, Baekjin Kim and Ambuj Tewari</p>
  </li>
  <li>
    <p><a href="https://dettanym.github.io/files/tpdp20_workshop_paper.pdf">Cache Me If You Can: Accuracy-Aware Inference Engine for Differentially Private Data Exploration</a><br/>
 Miti Mazmudar, Thomas Humphries, Matthew Rafuse, Xi He</p>
  </li>
  <li>
    <p><a href="https://drops.dagstuhl.de/opus/volltexte/2020/12026/">Bounded Leakage Differential Privacy</a><br/>
Katrina Ligett, Charlotte Peale, Omer Reingold</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1909.06322">A Knowledge Transfer Framework for Differentially Private Sparse Learning</a><br/>
Lingxiao Wang, Quanquan Gu</p>
  </li>
  <li>
    <p>Consistent Integer, Non-Negative, Hierarchical Histograms without Integer Programming<br/>
 Cynthia Dwork, Christina Ilvento</p>
  </li>
  <li>
    <p><a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/03/intrinsic_privacy_tpdp.pdf">An Empirical Study on the Intrinsic Privacy of Stochastic Gradient Descent</a><br/>
Stephanie Hyland, Shruti Tople</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2001.03618">Encode, Shuffle, Analyze Privacy Revisited: Formalizations and Empirical Evaluation</a><br/>
Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Shuang Song, Kunal Talwar, Abhradeep Thakurta</p>
  </li>
  <li>
    <p>Improving Sparse Vector Technique with Renyi Differential Privacy<br/>
 Yuqing Zhu and Yu-Xiang Wang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.11707">Breaking the Communication-Privacy-Accuracy Trilemma</a><br/>
Wei-Ning Chen, Peter Kairouz, Ayfer Özgür</p>
  </li>
  <li>
    <p>Budget Sharing for Multi-Analyst Differential Privacy<br/>
 David Pujol, Yikai Wu, Brandon Fain, Ashwin Machanavajjhala</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1908.07643">AdaCliP: Adaptive Clipping for Private SGD</a><br/>
 Venkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X. Yu, Sashank J. Reddi, Sanjiv Kumar</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.01181">Private Optimization Without Constraint Violation</a><br/>
 Andrés Muñoz Medina, Umar Syed, Sergei Vassilvitskii, Ellen Vitercik</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1912.06015">Efficient Per-Example Gradient Computations in Convolutional Neural Networks</a><br/>
 Gaspar Rochette, Andre Manoel, Eric Tramel</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.12674">Controlling Privacy Loss in Survey Sampling</a><br/>
 Audra McMillan, Mark Bun, Marco Gaboardi, Joerg Drechsler</p>
  </li>
  <li>
    <p>Privacy-Preserving Community Detection under the Stochastic Block Model<br/>
 Jonathan Hehir, Aleksandra Slavkovic, Xiaoyue Niu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.10129">Smoothed Analysis of Differentially Private and Online Learning</a><br/>
 Nika Haghtalab, Tim Roughgarden, Abhishek Shetty</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.09464">Private Mean Estimation for Heavy-Tailed Distributions</a><br/>
 Gautam Kamath, Vikrant Singhal, Jonathan Ullman</p>
  </li>
  <li>
    <p><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-57521-2_23">Private Posterior Inference Consistent with Public Information: a Case Study in Small Area Estimation from Synthetic Census Data</a><br/>
 Jeremy Seeman, Aleksandra Slavkovic, Matthew Reimherr</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2001.09122">Reasoning About Generalization via Conditional Mutual Information</a><br/>
 Thomas Steinke, Lydia Zakynthinou</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.15429">Understanding Gradient Clipping in Private SGD: A Geometric Perspective</a><br/>
 Xiangyi Chen, Z. Steven Wu, Mingyi Hong</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.06605">Privacy Amplification via Random Check-Ins</a><br/>
 Borja Balle, Peter Kairouz, Brendan McMahan, Om Thakkar, Abhradeep Thakurta</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.12603">Permute-and-flip: a new mechanism for differentially-private selection</a><br/>
 Ryan McKenna, Daniel Sheldon</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.02923">Descent-to-Delete: Gradient-Based Methods for Machine Unlearning</a><br/>
 Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.06529">A Better Bound Gives a Hundred Rounds: Enhanced Privacy Guarantees via f-Divergences</a><br/>
Shahab Asoodeh, Jiachun Liao, Flavio Calmon, Oliver Kosut, Lalitha Sankar</p>
  </li>
  <li>
    <p><a href="https://sites.tufts.edu/vrdi/files/2020/07/Slides-DP-Bhushan-Suwal-JN-Matthews-et-al.pdf">Census TopDown and the Redistricting Use Case</a><br/>
 Aloni Cohen, Moon Duchin, JN Matthews, Bhushan Suwal, Peter Wayner</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.04014">Interaction is Necessary for Distributed Learning with Privacy or Communication Constraints</a><br/>
Yuval Dagan, Vitaly Feldman</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.10783">Fisher information under local differential privacy</a><br/>
 Leighton Barnes, Wei-Ning Chen, Ayfer Ozgur</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.06830">Differentially Private Assouad, Fano, and Le Cam</a><br/>
 Jayadev Acharya, Ziteng Sun, Huanyu Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.13660">Learning discrete distributions: user vs item-level privacy</a><br/>
 Yuhan Liu, Ananda Theertha Suresh, Felix Yu, Sanjiv Kumar, Michael Riley</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1910.13659">Efficient Privacy-Preserving Stochastic Nonconvex Optimization</a><br/>
 Lingxiao Wang, Bargav Jayaraman, David Evans, Quanquan Gu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.03813">Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification</a><br/>
 Yingxue Zhou, Zhiwei Steven Wu, Arindam Banerjee</p>
  </li>
  <li>
    <p>Differentially private partition selection<br/>
 Damien Desfontaines, Bryant Gipson, Chinmoy Mandayam, James Voss</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.08007">Differentially Private Clustering: Tight Approximation Ratios</a><br/>
 Badih Ghazi, Ravi Kumar, Pasin Manurangsi</p>
  </li>
  <li>
    <p>Let’s not make a fuzz about it<br/>
 Elisabet Lobo Vesga, Alejandro Russo, Marco Gaboardi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2002.12321">PAPRIKA: Private Online False Discovery Rate Control</a><br/>
 Wanrong Zhang, Gautam Kamath, Rachel Cummings</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2009.13689">Oblivious Sampling Algorithms for Private Data Analysis</a><br/>
 Sajin Sasy, Olga Ohrimenko</p>
  </li>
  <li>
    <p>SOGDB-epsilon: Secure Outsourced Growing Database with Differentially Private Record Update<br/>
 Chenghong Wang, Kartik Nayak, Ashwin Machanavajjhala</p>
  </li>
  <li>
    <p><a href="https://invertibleworkshop.github.io/accepted_papers/pdfs/41.pdf">Differentially Private Normalizing Flows for Privacy-Preserving Density Estimation</a><br/>
 Chris Waites, Rachel Cummings</p>
  </li>
  <li>
    <p><a href="https://cs.uwaterloo.ca/~hsivasub/pub/TPDP2020.pdf">Differentially Private Sublinear Average Degree Approximation</a><br/>
 Harry Sivasubramaniam, Haonan Li, Xi He</p>
  </li>
  <li>
    <p><a href="https://chong-l.github.io/MAPL_TNC_FL_ICML_2020.pdf">Revisiting Model-Agnostic Private Learning: Faster Rates and Active Learning</a><br/>
 Chong Liu, Yuqing Zhu, Kamalika Chaudhuri, Yu-Xiang Wang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.06618">CoinPress: Practical Private Mean and Covariance Estimation</a><br/>
 Sourav Biswas, Yihe Dong, Gautam Kamath, Jonathan Ullman</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.09481">Connecting Robust Shuffle Privacy and Pan-Privacy</a><br/>
 Victor Balcer, Albert Cheu, Matthew Joseph, Jieming Mao</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.11204">Differentially Private Variational Autoencoders with Term-wise Gradient Aggregation</a><br/>
 Tsubasa Takahashi, Shun Takagi, Hajime Ono, Tatsuya Komatsu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.07490">Understanding Unintended Memorization in Federated Learning</a><br/>
 Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, Francoise Beaufays</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.10630">Near Instance-Optimality in Differential Privacy</a><br/>
 Hilal Asi, John Duchi</p>
  </li>
  <li>
    <p><a href="https://drive.google.com/file/d/1okHAkjNENiS2WfSKdkUo8B29yE8-Qfof/view">Implementing differentially private integer partitions via the exponential mechanism</a> and <a href="https://drive.google.com/file/d/1OytgB24d1n-xPIWrrKCsVQQdS7rV3tjn/view">Implementing Sparse Vector</a><br/>
 Christina Ilvento</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.05157">Differentially Private Simple Linear Regression</a><br/>
 Audra McMillan, Daniel Alabi, Jayshree Sarathy, Adam Smith, Salil Vadhan</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.05453">New Oracle-Efficient Algorithms for Private Synthetic Data Release</a><br/>
 Giuseppe Vietri, Grace Tian, Mark Bun, Thomas Steinke, Z. Steven Wu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.07749">General-Purpose Differentially-Private Confidence Intervals</a><br/>
 Cecilia Ferrando, Shufan Wang, Daniel Sheldon</p>
  </li>
  <li>
    <p>Central Limit Theorem and Uncertainty Principles for Differentially Private Query Answering<br/>
 Jinshuo Dong, Linjun Zhang, Weijie Su</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.13501">Private Stochastic Non-Convex Optimization: Adaptive Algorithms and Tighter Generalization Bounds</a><br/>
 Yingxue Zhou, Xiangyi Chen, Mingyi Hong, Z. Steven Wu, Arindam Banerjee</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.10335">Minimax Rates of Estimating Approximate Differential Privacy</a><br/>
Xiyang Liu, Sewoong Oh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.07740">Really Useful Synthetic Data – A Framework to Evaluate the Quality of Differentially Private Synthetic Data</a><br/>
 Christian Arnold, Marcel Neunhoeffer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.10541">PAC learning with stable and private predictions</a><br/>
 Yuval Dagan, Vitaly Feldman</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.04656">Computing Local Sensitivities of Counting Queries with Joins</a><br/>
 Yuchao Tao, Xi He, Ashwin Machanavajjhala, Sudeepa Roy</p>
  </li>
  <li>
    <p>Efficient Reductions for Differentially Private Multi-objective Regression<br/>
 Julius Adebayo, Daniel Alabi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.06667">The Pitfalls of Differentially Private Prediction in Healthcare</a><br/>
 Vinith Suriyakumar, Nicolas Papernot, Anna Goldenberg, Marzyeh Ghassemi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.14191">Tempered Sigmoid Activations for Deep Learning with Differential Privacy</a><br/>
 Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien, Úlfar Erlingsson</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.10881">Revisiting Membership Inference Under Realistic Assumptions</a><br/>
 Bargav Jayaraman, Lingxiao Wang, David Evans, Quanquan Gu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2009.04013">Attribute Privacy: Framework and Mechanisms</a><br/>
 Wanrong Zhang, Olga Ohrimenko, Rachel Cummings</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.10664">DuetSGX: Differential Privacy with Secure Hardware</a><br/>
 Phillip Nguyen, Alex Silence, David Darais, Joseph Near</p>
  </li>
  <li>
    <p><a href="https://pdfs.semanticscholar.org/4319/65b3c5a47cf8bfd30f1c30cd044382e98d68.pdf">A Programming Framework for OpenDP</a><br/>
 Marco Gaboardi, Michael Hay, Salil Vadhan</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.09352">A One-Pass Private Sketch for Most Machine Learning Tasks</a><br/>
 Benjamin Coleman, Anshumali Shrivastava</p>
  </li>
  <li>
    <p>Model-Agnostic Private Learning with Domain Adaptation<br/>
 Yuqing Zhu, Chong Liu, Yu-Xiang Wang</p>
  </li>
</ul></div>
    </summary>
    <updated>2020-10-28T00:01:00Z</updated>
    <published>2020-10-28T00:01:00Z</published>
    <author>
      <name>Thomas Steinke</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-11-02T23:35:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/27/tenure-track-faculty-positions-in-computer-science-and-in-the-theory-of-quantum-information-and-computation-at-harvard-university-apply-by-december-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/27/tenure-track-faculty-positions-in-computer-science-and-in-the-theory-of-quantum-information-and-computation-at-harvard-university-apply-by-december-15-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track Faculty Positions in Computer Science and in the Theory of Quantum Information and Computation at Harvard University (apply by December 15, 2020)</title>
    <summary>For the CS position, we invite applications in all areas. Areas of special interest include (but are not limited to) both ML and algorithms (broadly construed). For the position in the theory of quantum information and/or computation, areas of interest include but are not limited to quantum algorithms, communication, complexity, control, cryptography, and information processing. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>For the CS position, we invite applications in all areas. Areas of special interest include (but are not limited to) both ML and algorithms (broadly construed). For the position in the theory of quantum information and/or computation, areas of interest include but are not limited to quantum algorithms, communication, complexity, control, cryptography, and information processing.</p>
<p>Website: <a href="https://www.seas.harvard.edu/office-faculty-affairs/open-academic-positions">https://www.seas.harvard.edu/office-faculty-affairs/open-academic-positions</a><br/>
Email: jmileski@g.harvard.edu</p></div>
    </content>
    <updated>2020-10-27T23:42:09Z</updated>
    <published>2020-10-27T23:42:09Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-02T23:34:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/27/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/27/theory-group-postdoc-at-uc-berkeley-apply-by-december-1-2020/" rel="alternate" type="text/html"/>
    <title>Theory Group Postdoc at UC Berkeley (apply by December 1, 2020)</title>
    <summary>Postdoc inquiries will be viewable by all our group’s faculty. Individual faculty may then reach out in the case of matched interests. Please send a cover letter, CV, and research statement to the email below. In CV please list at least 3 references. In cover letter please identify faculty of interest. Also, have references submit […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Postdoc inquiries will be viewable by all our group’s faculty. Individual faculty may then reach out in the case of matched interests. Please send a cover letter, CV, and research statement to the email below. In CV please list at least 3 references. In cover letter please identify faculty of interest. Also, have references submit letters to the e-mail below, with your name in the subject line.</p>
<p>Website: <a href="http://theory.cs.berkeley.edu/postdoc.html">http://theory.cs.berkeley.edu/postdoc.html</a><br/>
Email: tcs-postdoc-inquiries@lists.eecs.berkeley.edu</p></div>
    </content>
    <updated>2020-10-27T21:11:08Z</updated>
    <published>2020-10-27T21:11:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-02T23:34:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/27/tenure-track-tenured-position-in-quantum-computing-at-university-of-california-davis-apply-by-december-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/27/tenure-track-tenured-position-in-quantum-computing-at-university-of-california-davis-apply-by-december-15-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track/tenured position in Quantum Computing at University of California, Davis (apply by December 15, 2020)</title>
    <summary>The University of California, Davis, seeks an Assistant Professor specifically in the area of quantum computing; exceptional candidates at the Associate and Full ranks may be considered. Applicants should consider applying as well to our open position in theoretical computer science. Website: https://recruit.ucdavis.edu/JPF03853 Email: greg@math.ucdavis.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The University of California, Davis, seeks an Assistant Professor specifically in the area of quantum computing; exceptional candidates at the Associate and Full ranks may be considered. Applicants should consider applying as well to our open position in theoretical computer science.</p>
<p>Website: <a href="https://recruit.ucdavis.edu/JPF03853">https://recruit.ucdavis.edu/JPF03853</a><br/>
Email: greg@math.ucdavis.edu</p></div>
    </content>
    <updated>2020-10-27T17:18:00Z</updated>
    <published>2020-10-27T17:18:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-02T23:34:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17726</id>
    <link href="https://rjlipton.wordpress.com/2020/10/26/a-vast-and-tiny-breakthrough/" rel="alternate" type="text/html"/>
    <title>A Vast and Tiny Breakthrough</title>
    <summary>Christofides bound beaten by an epsilon’s idea of epsilon src1, src2, src3 Anna Karlin, Nathan Klein, and Shayan Oveis Gharan have made a big splash with the number           No that is not the amount of the US debt, or the new relief bill. It is the fraction by which the hallowed 44-year-old upper bound […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Christofides bound beaten by an epsilon’s idea of epsilon</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/10/26/a-vast-and-tiny-breakthrough/karlinkleinoveisgharan/" rel="attachment wp-att-17728"><img alt="" class="alignright wp-image-17728" height="107" src="https://rjlipton.files.wordpress.com/2020/10/karlinkleinoveisgharan.jpg?w=216&amp;h=107" width="216"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.seattletimes.com/education-lab/yes-women-can-and-do-want-to-code-uw-professors-and-alumnae-say/">src1</a>, <a href="https://homes.cs.washington.edu/~nwklein/">src2</a>, <a href="https://www.engr.washington.edu/facresearch/newfaculty/2013/shayanoveisgharan.html">src3</a></font></td>
</tr>
</tbody>
</table>
<p>
Anna Karlin, Nathan Klein, and Shayan Oveis Gharan have made a big splash with the number </p>
<p>         <img alt="~~~~~~\frac{1}{1,000,000,000,000,000,000,000,000,000,000,000,000}. " class="latex" src="https://s0.wp.com/latex.php?latex=%7E%7E%7E%7E%7E%7E%5Cfrac%7B1%7D%7B1%2C000%2C000%2C000%2C000%2C000%2C000%2C000%2C000%2C000%2C000%2C000%2C000%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="~~~~~~\frac{1}{1,000,000,000,000,000,000,000,000,000,000,000,000}. "/></p>
<p>No that is not the amount of the US debt, or the new relief bill. It is the fraction by which the hallowed 44-year-old upper bound of <img alt="{1.5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1.5}"/> on the approximation ratio of the metric Traveling Salesperson Problem has been improved. With the help of randomization, we hasten to add.</p>
<p>
Today we discuss the larger meaning of their tiny breakthrough.<br/>
<span id="more-17726"/></p>
<p>
The abstract of their <a href="https://arxiv.org/abs/2007.01409">paper</a> is as pithy as can be:</p>
<blockquote><p><b> </b> <em> For some <img alt="{\epsilon &gt; 10^{-36}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+10%5E%7B-36%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\epsilon &gt; 10^{-36}}"/> we give a <img alt="{3/2 - \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%2F2+-+%5Cepsilon%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{3/2 - \epsilon}"/> approximation algorithm for metric TSP. </em>
</p></blockquote>
<p/><p>
Metric TSP means that the cost of the tour <img alt="{(v_1,v_2,\dots,v_n,v_1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28v_1%2Cv_2%2C%5Cdots%2Cv_n%2Cv_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(v_1,v_2,\dots,v_n,v_1)}"/> is the sum of the distances of the edges </p>
<p align="center"><img alt="\displaystyle  \mu(v_1,v_2) + \mu(v_2,v_3) + \cdots + \mu(v_{n-1},v_n) + \mu(v_n,v_1) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu%28v_1%2Cv_2%29+%2B+%5Cmu%28v_2%2Cv_3%29+%2B+%5Ccdots+%2B+%5Cmu%28v_%7Bn-1%7D%2Cv_n%29+%2B+%5Cmu%28v_n%2Cv_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \mu(v_1,v_2) + \mu(v_2,v_3) + \cdots + \mu(v_{n-1},v_n) + \mu(v_n,v_1) "/></p>
<p>according to a given metric <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mu}"/>. When the points are in <img alt="{\mathbb{R}^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%5Em%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathbb{R}^m}"/> with the Euclidean metric, an <img alt="{n^{O(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n^{O(1)}}"/>-time algorithm can come within a factor <img alt="{(1+\delta)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2B%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(1+\delta)}"/> of the optimal cost for any prescribed <img alt="{\delta &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\delta &gt; 0}"/>. Sanjeev Arora and Joseph Mitchell jointly won the 2002 Gödel Prize for their randomized algorithms doing exactly that. The rub is the constant in the “<img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{O(1)}"/>” depends on <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\delta}"/>—indeed, nobody knows how to make it scale less than linearly in <img alt="{\frac{1}{\delta}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{1}{\delta}}"/>. But for general metrics, getting within a factor of <img alt="{(1+\delta)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2B%5Cdelta%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(1+\delta)}"/> is known to be <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{NP}}"/>-hard for <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\delta}"/> up to <img alt="{\frac{1}{122}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B122%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{1}{122}}"/>.</p>
<p>
Some intermediate cases of metrics had <a href="https://arxiv.org/abs/1201.1870">allowed</a> getting within a factor of <img alt="{1.4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1.4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1.4}"/>, but for general metrics the <img alt="{1.5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1.5}"/> factor <a href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a025602.pdf">found</a> in 1976 by the <a href="https://www.imperial.ac.uk/news/196798/obituary-nicos-christofides-1942-2019/">late</a> Nicos Christofides, and <a href="http://nas1.math.nsc.ru/aim/journals/us/us17/us17_007.pdf">concurrently</a> by Anatoliy Serdyukov, stood like a brick wall. Well, we didn’t expect it to be a brick wall at first. Let me tell a story.</p>
<p>
</p><p/><h2> A Proof in a Pub </h2><p/>
<p/><p>
Soon after starting as a graduate student at Oxford in 1981, I went with a bunch of dons and fellow students down to London for a one-day workshop where Christofides was among the speakers and presented his result along with newer work. I’d already heard it spoken of as a combinatorial gem and perfect motivator for a graduate student to appreciate the power of combining simplicity and elegance:</p>
<ol>
<li>
Calculate the (or a) minimum spanning tree <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> given points. <p/>
</li><li>
Take <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A}"/> to be the leaves and any other odd-degree nodes of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> and calculate a minimum matching <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{M}"/> of them. <p/>
</li><li>
The graph <img alt="{T+M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T+M}"/> has all nodes of even degree so it has an easily-found Eulerian cycle <img alt="{C_E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C_E}"/>. <p/>
</li><li>
The cycle <img alt="{C_E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C_E}"/> may repeat vertices, but by the triangle inequality for the distance metric <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mu}"/>, we can bypass repeats to create a Hamilton cycle <img alt="{C_H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C_H}"/> giving <img alt="{\mu(C_H) \leq \mu(C_E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28C_H%29+%5Cleq+%5Cmu%28C_E%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mu(C_H) \leq \mu(C_E)}"/>.
</li></ol>
<p>
Now any optimal TSP tour <img alt="{C_O}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C_O}"/> arises as a spanning tree plus an edge, so <img alt="{\mu(T) &lt; \mu(C_O)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28T%29+%3C+%5Cmu%28C_O%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mu(T) &lt; \mu(C_O)}"/>. And <img alt="{C_O}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C_O}"/> can be partitioned into two sets of paths with endpoints in <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A}"/>. One of those sets has weight at most <img alt="{\frac{1}{2}\mu(C_O)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%5Cmu%28C_O%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{1}{2}\mu(C_O)}"/> and yet matches all pairs of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A}"/>. Thus <img alt="{\mu(M) \leq \frac{1}{2}\mu(C_O)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28M%29+%5Cleq+%5Cfrac%7B1%7D%7B2%7D%5Cmu%28C_O%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mu(M) \leq \frac{1}{2}\mu(C_O)}"/>. It follows that <img alt="{\mu(C_H) \leq \mu(T) + \mu(M) &lt; \mu(C_0) + \frac{1}{2}\mu(C_0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28C_H%29+%5Cleq+%5Cmu%28T%29+%2B+%5Cmu%28M%29+%3C+%5Cmu%28C_0%29+%2B+%5Cfrac%7B1%7D%7B2%7D%5Cmu%28C_0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mu(C_H) \leq \mu(T) + \mu(M) &lt; \mu(C_0) + \frac{1}{2}\mu(C_0)}"/> and we’re done.</p>
<p>
My memory of what we did after the workshop is hazy but I’m quite sure we must have gone to a pub for dinner and drinks before taking the train back up to Oxford. My point is, the above proof is <em>the kind that can be told and discussed in a pub</em>. It combines several greatest hits of the field: minimum spanning tree, perfect matching, Euler tour, Hamiltonian cycle, triangle inequality. The proof needs no extensive calculation; maybe a napkin to draw <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A}"/> on <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C_0}"/> and the partition helps. </p>
<p>
The conversation would surely have gone to the question,</p>
<blockquote><p><b> </b> <em> Can the <img alt="{1.5\;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1.5%5C%3B%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1.5\;}"/> factor be beaten? </em>
</p></blockquote>
<p/><p>
A perfect topic for mathematical pub conversation. Let’s continue as if that’s what happened next—I wish I could recall it.</p>
<p>
</p><p/><h2> Trees That Snake Around </h2><p/>
<p/><p>
Note that the proof already “beats” it in the sense of there being a strict inequality, and it really shows </p>
<p align="center"><img alt="\displaystyle  \mu(C_H) \leq (1.5 - \frac{1}{n})\mu(C_0). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu%28C_H%29+%5Cleq+%281.5+-+%5Cfrac%7B1%7D%7Bn%7D%29%5Cmu%28C_0%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \mu(C_H) \leq (1.5 - \frac{1}{n})\mu(C_0). "/></p>
<p>The advantage <img alt="{\frac{1}{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{1}{n}}"/> shrinks to zero as <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> grows, however. Moreover, examples where Christofides’s algorithm does no better than approach <img alt="{1.5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1.5}"/> are easy to draw. Pub walls are often covered with emblems of local organizations, and if one has a <a href="https://en.wikipedia.org/wiki/Caduceus">caduceus</a> symbol it can serve as the drawing:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/10/26/a-vast-and-tiny-breakthrough/caduceus/" rel="attachment wp-att-17729"><img alt="" class="aligncenter wp-image-17729" height="184" src="https://rjlipton.files.wordpress.com/2020/10/caduceus.png?w=154&amp;h=184" width="154"/></a></p>
<p/><p><br/>
The staff is a path <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> nodes while the snakes alternate edges of weight <img alt="{1 + \gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1 + \gamma}"/> between nodes two apart on the path. Going up one snake and down the other gives an optimal tour of weight <img alt="{(1 + \gamma)(n-2) + 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281+%2B+%5Cgamma%29%28n-2%29+%2B+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(1 + \gamma)(n-2) + 2}"/> (using the two outermost path edges to switch between the snakes), which <img alt="{\sim (1 + \gamma)n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csim+%281+%2B+%5Cgamma%29n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sim (1 + \gamma)n}"/>. The snake edges don’t change the path’s being the minimum spanning tree, and for <img alt="{C_H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C_H}"/> this costs <img alt="{n-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n-1}"/> plus the weight required to match the path’s endpoints. The extra weight is reckoned as the length of one snake, which <img alt="{\sim n\frac{1+\gamma}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csim+n%5Cfrac%7B1%2B%5Cgamma%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sim n\frac{1+\gamma}{2}}"/>, so the ratio approaches <img alt="{\frac{3}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B3%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{3}{2}}"/> as <img alt="{\gamma \rightarrow 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma+%5Crightarrow+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\gamma \rightarrow 0}"/> and <img alt="{n \rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n \rightarrow \infty}"/>. Here are some tantalizing aspects:</p>
<ul>
<li>
The <img alt="{n-2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn-2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n-2}"/> snake edges, plus one path edge to connect them, make a <em>maximum</em>-weight spanning tree <img alt="{T'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T'}"/> in the graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G}"/> formed by the two kinds of edges. Yet <img alt="{T'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T'}"/> followed by the same steps 2–4 of Christofides’s algorithm would yield and optimum tour. <p/>
</li><li>
When one is given only the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> points plus the <em>graph metric</em> <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mu}"/> induced by <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G}"/>, not <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{G}"/> itself, then there are much worse spanning trees. The single edge connecting the endpoints <img alt="{(v_1,v_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28v_1%2Cv_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(v_1,v_n)}"/> of the previous path has weight <img alt="{\mu(v_1,v_n) \approx \frac{n}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28v_1%2Cv_n%29+%5Capprox+%5Cfrac%7Bn%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mu(v_1,v_n) \approx \frac{n}{2}}"/>. <p/>
</li><li>
Thus <img alt="{T'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T'}"/> has relatively low weight compared to these possible other trees. And its weight approaches that of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> as <img alt="{\gamma \rightarrow 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma+%5Crightarrow+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\gamma \rightarrow 0}"/>. This means that small changes in the size of the tree yield large changes in the quality of the induced tour. <p/>
</li><li>
The advantage of <img alt="{T'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T'}"/> is that its odd-valence nodes have small distance under <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mu}"/>. As a path it snakes around so that its ends are near each other, unlike those of the minimum spanning tree <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/>. This raises the question of weighting spanning trees according to a slightly different measure <img alt="{\mu'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mu'}"/> that incorporates a term for “<em>odd-closeness</em>.”
</li></ul>
<p>
In 1981, we would not have known about Arora’s and Mitchell’s results, so we would have felt fully on the frontier by embedding the points in the plane and sketching spanning trees and cycles on a piece of paper. After a couple pints of ale we might have felt sure that a simple proof with such evident slack ought to yield to a more sophisticated attack. </p>
<p>
</p><p/><h2> Helpful Trees </h2><p/>
<p/><p>
There is one idea that we might have come up with in a pub. The motivation for choosing <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> to be a minimum spanning tree is that many of its edges go into the Euler tour <img alt="{C_E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C_E}"/> and those bound the final <img alt="{C_O}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C_O}"/> even if <img alt="{C_O}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C_O}"/> shortcuts them. So making the total edge weight of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> minimum seems to be the best way to help at that stage. We might have wondered, however, whether there is a way to create <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> to have a stronger direct relation to good tours, if not to the optimal tour. </p>
<p>
Oveis Gharan did have such an idea jointly with a different group of authors a decade ago, in the <a href="https://homes.cs.washington.edu/~shayan/atsp.pdf">best paper</a> of SODA 2010. We cannot seem to get our hands on the optimal tour, nor even a “good” tour if that means a better than <img alt="{1.5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1.5}"/> factor approximation—that is what we are trying to find to begin with. But there is another “tour” <img alt="{O^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{O^*}"/> that we <em>can</em> compute. This is an optimum of the <em>linear programming relaxation</em> of TSP, whose relation to the exact-TSP methods of Michael Held and Dick Karp we <a href="https://rjlipton.wordpress.com/2012/05/08/inexact-remarks-on-exact-tsp-algorithms/">covered</a> long back. <img alt="{O^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{O^*}"/> is not a single tour but rather an ensemble of “fractional tours” where each edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{e}"/> has a rational number <img alt="{z_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{z_e}"/> representing its contribution to the LP solution. The higher <img alt="{z_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{z_e}"/>, the more helpful the edge.</p>
<p>
The objective then becomes to design distributions <img alt="{{\cal T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{{\cal T}}"/> of spanning trees <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> so that:</p>
<ol>
<li>
Sampling <img alt="{T \leftarrow {\cal T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%5Cleftarrow+%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T \leftarrow {\cal T}}"/> is polynomial-time efficient. <p/>
</li><li>
For every edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{e}"/>, <img alt="{\Pr_{T \leftarrow {\cal T}}[e \in T] \propto (1 + \delta_n) z_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPr_%7BT+%5Cleftarrow+%7B%5Ccal+T%7D%7D%5Be+%5Cin+T%5D+%5Cpropto+%281+%2B+%5Cdelta_n%29+z_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Pr_{T \leftarrow {\cal T}}[e \in T] \propto (1 + \delta_n) z_e}"/> where <img alt="{\delta_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\delta_n}"/> is tiny. <p/>
</li><li>
The distribution <img alt="{{\cal T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{{\cal T}}"/> promotes trees <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> with fewer leaves and odd-valence interior nodes.
</li></ol>
<p>
The algorithmic strategy this fits into is to sample <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> from <img alt="{{\cal T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{{\cal T}}"/>, plug <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T}"/> into the first step of the Christofides algorithm, and continue as before.</p>
<p>
</p><p/><h2> The Proof and the Pudding </h2><p/>
<p/><p>
The first two conditions are solidly defined. Considerable technical details in the SODA 2010 paper and another <a href="https://homes.cs.washington.edu/~shayan/tsp.pdf">paper</a> at FOCS 2011 that was joint with Amin Saberi and Mohit Singh are devoted to them. A third desideratum is that the distribution <img alt="{{\cal T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{{\cal T}}"/> not be over-constrained but rather have maximum entropy, so that for efficiently computable numbers <img alt="{\lambda_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\lambda_e}"/> approaching <img alt="{z_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{z_e}"/> one has also: </p>
<p align="center"><img alt="\displaystyle  \Pr_{\cal T}(T) \propto \prod_{e \in T}\lambda_e. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr_%7B%5Ccal+T%7D%28T%29+%5Cpropto+%5Cprod_%7Be+%5Cin+T%7D%5Clambda_e.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \Pr_{\cal T}(T) \propto \prod_{e \in T}\lambda_e. "/></p>
<p>The third condition, however, follows the <a href="https://www.phrases.org.uk/meanings/proof-of-the-pudding.html">maxim</a>,</p>
<blockquote><p><b> </b> <em> “the proof of the pudding is in the eating.” </em>
</p></blockquote>
<p/><p>
As our source makes clear, this does not refer to American-style dessert pudding, but rather savory British pub fare going back to 1605 at least. The point is that we ultimately know a choice of <img alt="{{\cal T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{{\cal T}}"/> is good by proving it gives a better approximation factor than <img alt="{\frac{3}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B3%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{3}{2}}"/>.</p>
<p>
In America, we tend to say the maxim a different way:</p>
<blockquote><p><b> </b> <em> “the proof is in the pudding.” </em>
</p></blockquote>
<p/><p>
The new paper uses the “pudding” from the 2011 paper but needed to deepen the proof. Here is where we usually say to refer to the <a href="https://arxiv.org/abs/2007.01409">paper</a> for the considerable details. But in this case we find that a number of the beautiful concepts laid out in the paper’s introduction, such as <em>real stability</em> and <em>strong Rayleigh distributions</em>, are more accessibly described in the notes for the first half of a <a href="https://homes.cs.washington.edu/~shayan/courses/polynomials/">course</a> taught last spring by Oveis Gharan with Klein as TA. One nub is that if a set of complex numbers all have positive imaginary part, then any product <img alt="{z = z_1 z_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz+%3D+z_1+z_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{z = z_1 z_2}"/> of two of the numbers has real part less than the product of the real parts, and if the latter product is positive, then <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{z}"/> is not a real number.  This rules out assignments drawn from the set from being solutions to certain polynomials as well as setting up odd/even parity properties elsewhere.</p>
<p>
</p><p/><h2> Rigidity of the TSP Universe </h2><p/>
<p/><p>
I’ll close instead with some remarks while admitting that my own limited time—I have been <a href="https://www.cbc.ca/radio/asithappens/as-it-happens-tuesday-edition-1.5769434">dealing</a> with more chess <a href="https://www.theguardian.com/sport/2020/oct/16/chesss-cheating-crisis-paranoia-has-become-the-culture">cases</a>—prevents them from being fully informed. </p>
<p>
The main remark is to marvel that the panoply of polynomial properties and deep analysis buy such a tiny improvement. It is hard to believe that the true space of TSP approximation methods is so <em>rigid</em>. In this I am reminded of Scott Aaronson’s <a href="https://www.scottaaronson.com/blog/?p=2651">calculations</a> that a collision of two stellar black holes a mere 3,000 miles away would stretch space near you by only a millimeter. There is considerable belief that the approximation factor ought to be improvable at least as far as <img alt="{\frac{4}{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B4%7D%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{4}{3}}"/>.</p>
<p>
It strikes me that the maximum-entropy condition, while facilitating the analysis, works against the objective of making the trees more special. It cannot come near the kind of snaky tree <img alt="{T_O}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T_O}"/> obtained by deleting any edge from a good tour <img alt="{O}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{O}"/>, such that plugging <img alt="{T_O}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_O%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{T_O}"/> into step 1 yields <img alt="{O}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{O}"/> back again. The theory of polynomials and distributions that they develop has a plug-and-play element, so that they can condition the distributions <img alt="{{\cal T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{{\cal T}}"/> toward the third objective using the parity properties. But their framework has inflexibility represented by needing to postulate a real-valued function on the optimum edges whose expectation is of order the <em>square</em> of a parameter <img alt="{\eta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ceta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\eta}"/> already given the tiny value <img alt="{10^{-12}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B10%5E%7B-12%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{10^{-12}}"/>. Of the requirement that <img alt="{\eta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ceta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\eta}"/> be a small fraction of their governing epsilon parameter, they say in section 3:</p>
<blockquote><p><b> </b> <em> This forces us to take <img alt="{\eta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ceta%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\eta}"/> very small, which is why we get only a “very slightly” improved approximation algorithm for TSP. Furthermore, since we use OPT edges in our construction, we don’t get a new upper bound on the integrality gap. We leave it as an open problem to find a reduction to the “cactus” case that doesn’t involve using a slack vector for OPT (or a completely different approach). </em>
</p></blockquote>
<p/><p>
What may be wanting is a better way of getting the odd-valence tree nodes to be closer, not just fewer in number. To be sure, ideas for “closer” might wind up presupposing a metric topology on the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> given points, leading to cases that have already been improved by other means.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Will the tiny but fixed wedge below <img alt="{\frac{3}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B3%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{3}{2}}"/> become a lever by which to find better approximations?</p>
<p>
There is also the kvetch that the algorithm is randomized, whereas the original by Christofides and Serdyukov is deterministic. Can the new methods be derandomized?</p>
<p/><p><br/>
[fixed = to + sign at end of Christofides proof; fixed wording of “nub” at end of pudding section]</p></font></font></div>
    </content>
    <updated>2020-10-27T04:42:37Z</updated>
    <published>2020-10-27T04:42:37Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="Anna Karlin"/>
    <category term="approximation"/>
    <category term="breakthrough"/>
    <category term="linear programming"/>
    <category term="Nathan Klein"/>
    <category term="Nicos Christofides"/>
    <category term="polynomials"/>
    <category term="Shayan Oveis Gharan"/>
    <category term="spanning tree"/>
    <category term="Traveling Salesman"/>
    <category term="TSP"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-11-02T23:33:52Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/diffix-attack/</id>
    <link href="https://differentialprivacy.org/diffix-attack/" rel="alternate" type="text/html"/>
    <title>Reconstruction Attacks in Practice</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is the second of two posts describing the theory and practice of reconstruction attacks.  To read the first post, which covers the theoretical basis of such attacks, <a href="https://differentialprivacy.org/reconstruction-theory/">[click here]</a>.</p>

<hr/>

<p>In the <a href="https://differentialprivacy.org/reconstruction-theory/">last post</a>, we discussed how an attacker can use noisy answers to questions about a database to reconstruct private information in the database. The reconstruction attack framework was:</p>

<ol>
  <li>The attacker submits sufficiently random queries that link prior information (which the attacker already knows) to private data (which the attacker wants to learn).</li>
  <li>The attacker receives noisy answers to these queries and writes them down as constraints for a linear program to solve for the private bits.</li>
  <li>The attacker solves the linear program and rounds the result to recover most of the bits.</li>
</ol>

<p>Our last post discussed some of this attack’s nice theoretical guarantees, and this post matches that with real-world performance. More specifically, we’ll cover two successful applications of this attack against a piece of anonymizing SQL software called Diffix which, despite the name, is not differentially private.</p>

<h3 id="what-is-diffix">What is Diffix?</h3>
<p>Diffix is a system designed by the startup Aircloak for answering statistical queries over a private database. It is described by its creators as an “anonymizing SQL interface [that] sits in front of your data and enables you to conduct ad hoc analytics — fully privacy preserving and GDPR-compliant.”<sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1">1</a></sup>  Aircloak’s approach is to develop targeted defenses for known vulnerabilities, but to otherwise privilege utility over protecting against unknown vulnerabilities.  They combine this approach with a serious effort to actually find vulnerabilities in Diffix through periodic bug bounties that offer monetary prizes for participants who mount successful attacks.  While this post is critical of the design of Diffix itself, we commend Aircloak for their genuine openness to scrutiny. Indeed, the attacks described in this post were carried out as a part of these bug bounty programs and led to the discovery of several vulnerabilities in the software that have since been addressed. The first attack we describe was carried out by Aloni Cohen and Kobbi Nissim in the first bug bounty program in late 2017 and early 2018.  The second was run by Travis Dick, Matthew Joseph, and Zachary Schutzman in the second bug bounty program during the summer of 2020.</p>

<p>Before diving into the details of the attacks, we’ll first introduce the basic functionality of Diffix and how it purports to defend against vulnerabilities, including linear reconstruction attacks.  The goal of Diffix is to answer SQL queries, such as:</p>
<pre><code class="language-SQL">SELECT COUNT(*) FROM loans
WHERE loanStatus = 'C'
AND clientId BETWEEN 2000 and 3000
</code></pre>
<p>on a database while preventing the disclosure of record-level data.<br/>
A challenge for a system like Diffix is to answer such counting queries while preventing an adversarial user—the attacker—from learning record-level information. As you might remember from the last post, such a system must not provide exact answers to arbitrary queries. Otherwise the attacker could mount a <em>differencing attack</em>. For example, an attacker who knows that Billy Joel’s <code class="language-plaintext highlighter-rouge">clientID</code> is 2744 could learn the status of the singer’s loan by comparing the answer to the previous query with the answer to:</p>

<pre><code class="language-SQL">SELECT COUNT(*) FROM loans
WHERE loanStatus = 'C'
AND clientId BETWEEN 2000 and 3000
AND clientId != 2744
</code></pre>

<p>An intuitive defense is to add noise to the answer—say, Gaussian noise sampled from \(N(0,10)\).
Now the difference \(\Delta\) in the responses to the two queries is a random variable sampled from \(N(1,20)\) or \(N(0,20)\) depending on whether Joel’s <code class="language-plaintext highlighter-rouge">loanStatus</code> is or isn’t <code class="language-plaintext highlighter-rouge">C</code>.
With just one sample, the distributions are hard to distinguish.</p>

<p>Still, this scheme is easily thwarted by <em>averaging attacks</em>.
If the noise is sampled anew each time a query is made, then repeatedly making the same pair of queries generates many independent samples from \(N(1,20)\) or \(N(0,20)\), and enough queries would make it possible to distinguish these distributions easily.</p>

<p>As before, there is an intuitive defense: use the same noise for repeated queries.  This defense introduces its own new attacks by making many syntactically-distinct but semantically-equivalent queries.  Those attacks in turn suggest new defenses which suggest new attacks, and so on.  Diffix is, in a sense, the result of this hypothetical arms race.</p>

<p>From a technical perspective, Diffix consists of three components, which together are intended to thwart these attacks.  First, Diffix only accepts a limited subset of SQL and will categorically reject any query that does not fit this subset.  These restrictions—including tight restrictions on <code class="language-plaintext highlighter-rouge">JOIN</code>s and on the number of mathematical functions in a single expression—limit the ability of an adversary to use the full power of SQL to access the database.  The second component is a collection of data-dependent ad-hoc methods to prevent leaking information about individuals or very small subsets of users, including suppressing answers to queries about small numbers of users and flattening outliers.</p>

<p>The final component is Diffix’s layered noise.  This noise is comprised of two individual noise terms added together: a <em>data-dependent</em> term whose variance is constant<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2">2</a></sup> and a <em>query-dependent</em> term whose variance depends on the complexity of the query. The data-dependent noise prevents naïve averaging attacks. It is a pseudorandom error where the seed of the pseudorandom function depends on individual data records that contribute to the query result. Semantically equivalent queries using different syntax will nonetheless share this error, so simply averaging the responses will not remove this noise.</p>

<p>The query-dependent noise prevents a naïve Dinur–Nissim style reconstruction attacks. A noise term of magnitude \(\Omega(1)\) is generated deterministically for each condition in the <code class="language-plaintext highlighter-rouge">WHERE</code> or <code class="language-plaintext highlighter-rouge">HAVING</code> clause of the SQL query, and the terms are added together.  A Dinur–Nissim query is a random subset of the dataset that contains \(\Omega(n)\) records. The straightforward way of specifying such a query is to enumerate the subset record by record using \(\Omega(n)\) conditions:<sup id="fnref:3"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:3">3</a></sup></p>

<pre><code class="language-SQL">SELECT COUNT(*) FROM loans
WHERE loanStatus = 'C'
AND (clientId = 2007
OR clientId = 2018
...
OR clientId = 2991)

</code></pre>

<p>A query with \(\Omega(n)\) conditions is answered with noise with standard deviation \(\Omega(\sqrt{n})\), enough to thwart efficient reconstruction algorithms.</p>

<h3 id="carrying-out-reconstruction">Carrying out reconstruction</h3>
<p>The additional noise per SQL condition  is the main obstacle to running a successful reconstruction attack on a database behind Diffix.  As described above, the noise prevents the naive implementation of the reconstruction algorithm from receiving accurate enough answers to reconstruct the database using a reasonable number of queries.<br/>
A natural approach is to use very few SQL conditions—ideally, just one—to make random-enough queries, each identifying a subset of the records in the dataset.
So the challenge is to formulate a large family of such queries that are accepted by Diffix’s restricted subset of SQL, using as few conditions as possible.</p>

<h4 id="the-cohennissim-attack">The Cohen–Nissim Attack</h4>

<p>Instead of specifying each row with a separate condition, the Cohen–Nissim attack<a href="https://arxiv.org/abs/1810.05692">[CN18]</a> uses an ad hoc <em>hash function</em> to extract entropy from the data itself in order to systematically choose the needed subsets.<br/>
Suppose we have a list of the values in the database’s  <code class="language-plaintext highlighter-rouge">clientId</code> column, and we want to recover the <code class="language-plaintext highlighter-rouge">loanStatus</code> secret bit. Rather than explicitly enumerating the <code class="language-plaintext highlighter-rouge">clientId</code>s for a random subset of the rows to include in each query, we can write a boolean-valued function which evaluates to true on about half of the <code class="language-plaintext highlighter-rouge">clientId</code>s and ask Diffix to include only the rows for which the condition is true.  In this way, instead of first choosing a subset of rows and then asking Diffix about those rows, we choose this function and use its evaluation to specify our random subset.</p>

<p>After some experimentation with the language restrictions, Cohen and Nissim settled on the following:</p>
<pre><code class="language-SQL">...
WHERE FLOOR(100 * ((clientId * 2)^0.7))
   = FLOOR(100 * ((clientId * 2)^0.7) + 0.5)
</code></pre>

<p>Let’s see what this does. Let \(d=d_0.d_1 d_2 d_3 d_4 \dots \) be the decimal representation of the value \(d = (\mathtt{clientID}\cdot 2)^{0.7}\), which appears on both sides of the equality.
The expression is true if and only if \(d_3 &lt; 5\).<br/>
To see this, the left hand side evaluates to \(d_{0}d_{1}d_{2} = \lfloor 100d \rfloor\); the right hand side evaluates to \(d_{0}d_{1}d_{2}\) if \(d_3 &lt; 5\)  or \(d_{0}d_{1}d_{2}+1\) if \(d_3 \geq 5\). In the former case, the equality condition evaluates to ‘true’, and in the latter case it evaluates to ‘false’. Replacing 100 with other powers of 10 changes which digit in the decimal expansion is checked.</p>

<p>By varying the constants in the SQL query, this single expression yields a whole family of conditions, albeit a very ad-hoc one.  The hope was that, for different primes \(q\) and fractional exponents \(p\), the individual digits of the decimal representations of \((\mathtt{clientID}*q)^p\) would be random enough for reconstruction to work.
The complete attack queries looked like this:</p>

<pre><code class="language-SQL">
SELECT COUNT(clientId) FROM loans
WHERE FLOOR(100 * ((clientId * 2)^.7))  
    = FLOOR(100 * ((clientId * 2)^.7) + 0.5)
AND clientId BETWEEN 2000 and 3000
AND loanStatus = 'C'

</code></pre>

<p>The range condition at the end simply selects a subset of the data which is small enough for the attack to run quickly on a personal computer but large enough to satisfy the requirements of the Diffix bounty program.  This family of queries allows for a linear program to reconstruct the secret <code class="language-plaintext highlighter-rouge">loanStatus</code> bits with high accuracy.</p>

<p>In the course of verifying the attack for the Diffix bounty program, reconstruction was carried out on 4 different ranges of <code class="language-plaintext highlighter-rouge">clientId</code>s containing 455 records. For each record, the attack correctly determined whether or not the corresponding <code class="language-plaintext highlighter-rouge">loanStatus</code> was <code class="language-plaintext highlighter-rouge">C</code>.</p>

<p>Aircloak’s response to this attack was to further restrict the queries allowed by Diffix.  Columns like <code class="language-plaintext highlighter-rouge">clientId</code>, where most of the values correspond to a single user, are tagged as ‘isolating’, and mathematical functions can no longer be used on such columns.  The hope was that this modification would prevent the extraction of entropy from an identifying column via hashing.</p>

<h4 id="the-dickjosephschutzman-attack">The Dick–Joseph–Schutzman Attack</h4>

<p>Without the ability to directly use a uniquely identifying column from the database itself, we need another way to single out rows of the database.  We can use an idea that’s been around since the 1990s, when  Latanya Sweeney showed<sup id="fnref:4"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:4">4</a></sup> that almost 90 percent of Americans could be identified with only a date of birth, ZIP code, and gender, but of course each of these alone is nowhere near sufficient to isolate a single individual.  We can use this idea and try to evade the modification to Diffix by choosing multiple non-isolating columns which, when taken together, can isolate rows in the database.</p>

<p>This modified attack uses the <code class="language-plaintext highlighter-rouge">pickup_latitude</code> column in the <code class="language-plaintext highlighter-rouge">taxi</code> data set as the source of entropy, which is non-isolating, in part because there are a large number of rows where the value is recorded as zero.  We can combine this column with the <code class="language-plaintext highlighter-rouge">trip_distance</code> column and run queries of the following form:</p>

<pre><code class="language-SQL">
SELECT COUNT(*) FROM rides
WHERE FLOOR(pickup_latitude ^  8.789 + 0.5)
    = FLOOR(pickup_latitude ^  8.789)
AND trip_distance IN (0.87, 1.97, 2.75)
AND payment_type = 'CSH'

</code></pre>

<p>This example query is part of an attack to recover the <code class="language-plaintext highlighter-rouge">payment_type</code> column, which (for the purposes of this attack) is a binary column containing two values: <code class="language-plaintext highlighter-rouge">CRD</code> (for credit card payments) and <code class="language-plaintext highlighter-rouge">CSH</code> (for cash payments).  The <code class="language-plaintext highlighter-rouge">IN (0.87, 1.97, 2.75)</code> restricts to a subset of the data with about 450 rows, each with a distinct value for <code class="language-plaintext highlighter-rouge">pickup_latitude</code>.  However, because across the whole database, very few rows have a distinct value in this column, Diffix does not consider it ‘isolating’ and it can be used as Cohen–Nissim used <code class="language-plaintext highlighter-rouge">clientId</code>.  The values in <code class="language-plaintext highlighter-rouge">pickup_latitude</code> are recorded to six decimal places of precision and the least significant four of them are essentially random digits.  By choosing an appropriate range for the exponent and using the same trick as in the Cohen–Nissim attack, allows the construction of a Diffix-accepted query which includes around half of the rows in the targeted subset. Using different values for the exponent leads to a large family of queries which allow the attack to be carried out as before with similarly high accuracy of over 95 percent.</p>

<p>Dick–Joseph–Schutzman additionally extends this attack to recover <em>numerical</em> rather than just binary secret data.  By using queries of the form</p>

<pre><code class="language-SQL">SELECT SUM(passenger_count) FROM rides ...

</code></pre>
<p>Diffix will return return noisy sums over the specified subset for a numeric column like <code class="language-plaintext highlighter-rouge">passenger_count</code>.  Then, a similar linear program can reconstruct estimates for these values with high accuracy.  For numeric columns like <code class="language-plaintext highlighter-rouge">passenger_count</code> which take on relatively few distinct values, the attack recovers the exact values with accuracy above 75 percent.  Due to limitations in the Diffix bounty program rules which require perfect reconstruction of a value to be considered ‘accurate’, we didn’t evaluate the performance of the attack on numeric columns with richer values, such as <code class="language-plaintext highlighter-rouge">dropoff_latitude</code>.</p>

<p>Finally, this attack extends to one used to reconstruct string data character-by-character.  A U.S. social security number consists of a string formatted like <code class="language-plaintext highlighter-rouge">xxx-xx-xxxx</code> with none unknown digits in three blocks separated by dashes.  There are potentially one billion different strings that could appear in this column.  However, by exploiting the structure of the data, a separate attack can be run to recover each digit individually using the summation attack, since there are only ten different values each digit could take.  Queries of the form</p>

<pre><code class="language-SQL">
SELECT SUM(CAST(SUBSTRING(ssn, 3, 1) AS integer)) FROM rides ....

</code></pre>

<p>can be used to recover the 3rd digit from each row’s social security number.  Running this attack for each digit then aggregating the individual guesses to construct a guess for each user’s entire social security number allows the attack to achieve perfect reconstruction on about 90 percent of the values.  A similar attack worked on the <code class="language-plaintext highlighter-rouge">pickup_datetime</code> and <code class="language-plaintext highlighter-rouge">dropoff_datetime</code> columns, with separate attacks on the value in the seconds position, the minutes position, and so on, and finally piecing these together to correctly reconstruct about 85 percent of the values.</p>

<p>Again, Aircloak’s response was to restrict the query language.  Both of the successful attacks relied on the use of some arithmetic inside of  a <code class="language-plaintext highlighter-rouge">FLOOR</code> function to check whether or not a row is included in a particular query. Diffix now forbids the use of arithmetic with <em>bucketing functions</em> such as <code class="language-plaintext highlighter-rouge">FLOOR</code>, <code class="language-plaintext highlighter-rouge">CEIL</code>, <code class="language-plaintext highlighter-rouge">ROUND</code>, etc.  This defeats strategies which choose random-ish subsets via this kind of hashing, but does not necessarily preclude the extraction of entropy from the data in other ways.</p>

<h4 id="whats-next">What’s Next?</h4>

<p>We’d again like to thank Aircloak for opening their system to attacks and critiques through the Diffix bounty program.  By being so willing to expose their product in this way, they have provided a test bed for us to bridge the gap between theory and application and demonstrate how a linear reconstruction attack might work in practice.  Vulnerability to these and other attacks are a potential threat to any data privacy system which does not account for the cumulative threat to privacy that may result from many seemingly-innocuous queries, not just Diffix. The attacks we describe here only require the attacker have access to some subset of the data with a sufficient amount of entropy, and while more entropy allows for more complete reconstruction, it may be possible to use something potentially very accessible like a list of users’ email addresses in this kind of attack to reconstruct a non-trivial amount of the secret data using queries against a system that adds independent noise to each query.  Systems like this fall into the trap of the classic arms race, where a designer builds a system to protect against certain attacks, then a clever and determined adversary defeats the system, and the designer is forced to make revisions.  This cycle may never terminate, leaving us perpetually unsure of when we can be confident that a system is secure enough to trust with sensitive data.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Descriptions of Diffix and Aircloak are based on <a href="https://www.aircloak.com">https://www.aircloak.com</a>, <a href="https://arxiv.org/pdf/1806.02075.pdf">https://arxiv.org/pdf/1806.02075.pdf</a>, <a href="https://demo.aircloak.com/docs/">https://demo.aircloak.com/docs/</a>, and the authors’ participation in the Aircloak bounty program. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>The variance is proportional to the largest effect any single user has on the output. For <code class="language-plaintext highlighter-rouge">COUNT</code> queries, this largest contribution is 1, and for <code class="language-plaintext highlighter-rouge">SUM</code> queries, it’s roughly the magnitude of the largest value in the column. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">↩</a></p>
    </li>
    <li id="fn:3">
      <p>Note that Diffix’s syntax restrictions don’t allow disjunctions (using <code class="language-plaintext highlighter-rouge">OR</code>s). An equivalent way of writing this that is allowed by Diffix would use <code class="language-plaintext highlighter-rouge">...WHERE ... AND clientId IN (2007, 2018,...)</code>. For such conditions, Diffix adds a noise layer for each element of the <code class="language-plaintext highlighter-rouge">IN</code> condition. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:3">↩</a></p>
    </li>
    <li id="fn:4">
      <p>Sweeney, Latanya. “Simple demographics often identify people uniquely.” Health (San Francisco) 671.2000 (2000): 1-34. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:4">↩</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2020-10-27T04:11:38Z</updated>
    <published>2020-10-27T04:11:38Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-11-02T23:35:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/158</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/158" rel="alternate" type="text/html"/>
    <title>TR20-158 |  A Note on Hardness under Projections for Graph Isomorphism and Time-Bounded Kolmogorov Complexity | 

	Eric Allender, 

	Azucena Garvia Bosshard, 

	Amulya Musipatla</title>
    <summary>This paper focuses on a variant of the circuit minimization problem (MCSP), denoted MKTP, which studies resource-bounded Kolmogorov complexity in place of circuit size. MCSP is not known to be hard for any complexity class under any kind of m-reducibility, but recently MKTP was shown to be hard for DET under m-reductions computable in NC0, by presenting an AC0 reduction from the rigid graph isomorphism problem to MKTP, and combining that with a theorem of Toran, showing that DET AC0-reduces to the rigid graph isomorphism problem, and then appealing to the "Gap Theorem" of [Agrawal, Allender, Rudich]. Here, we show that these reductions can be accomplished by means of projections. Thus MKTP is hard for DET under projections, and the rigid graph isomorphism problem is hard for DET under uniform projections.</summary>
    <updated>2020-10-26T21:00:01Z</updated>
    <published>2020-10-26T21:00:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-11-02T23:33:36Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/26/graphs-whose-cycles</id>
    <link href="https://11011110.github.io/blog/2020/10/26/graphs-whose-cycles.html" rel="alternate" type="text/html"/>
    <title>Graphs whose cycles all touch</title>
    <summary>An interesting recent question on MathOverflow asks about graphs in which all cycles touch. Here, touching is meant in the same sense as a bramble in graph structure theory: every two cycles either share a vertex or contain the two endpoints of an edge from one cycle to the other. The graphs with this property include all the complete graphs (girth 3), complete bipartite graphs (girth 4), and theta graphs (arbitrarily high girth but very simple structure). As originally phrased, it asked whether there exists \(g\) such that graphs of girth \(\ge g\) with all cycles touching have bounded treewidth. Partial results given there by Tony Huynh and me show that the condition of bounded treewidth can be replaced by bounded vertex cover number or a bounded number of vertex-disjoint cycles without changing the answer.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>An interesting recent question on MathOverflow asks about <a href="https://mathoverflow.net/q/374793/440">graphs in which all cycles touch</a>. Here, touching is meant in the same sense as a <a href="https://en.wikipedia.org/wiki/Bramble_(graph_theory)">bramble</a> in graph structure theory: every two cycles either share a vertex or contain the two endpoints of an edge from one cycle to the other. The graphs with this property include all the complete graphs (girth 3), complete bipartite graphs (girth 4), and theta graphs (arbitrarily high girth but very simple structure). As originally phrased, it asked whether there exists \(g\) such that graphs of girth \(\ge g\) with all cycles touching have bounded treewidth. Partial results given there by Tony Huynh and me show that the condition of bounded treewidth can be replaced by bounded vertex cover number or a bounded number of vertex-disjoint cycles without changing the answer.</p>

<p>This led me to look for graphs that have high girth, all cycles touching, and as many vertex-disjoint cycles as I could construct. So far, the best I have found is four vertex-disjoint cycles, as shown in graphs of the following form:</p>

<p style="text-align: center;"><img alt="A graph with four vertex-disjoint long cycles, and all cycles touching" src="https://11011110.github.io/blog/assets/2020/4-disjoint-touching-cycles.svg"/></p>

<p>It consists of four theta graphs (the pairs of blue vertices connected by multiple long paths of yellow vertices, with the eight blue pole vertices of the theta graphs connected into two four-vertex paths. I’ve drawn it with yellow paths of length 16, and three paths per theta, but these numbers are arbitrary. One can easily find four vertex-disjoint cycles, within each of the four thetas, ignoring the edges between the pole vertices.</p>

<p>There is no cycle using only the blue pole vertices, so every cycle in the overall graph must include at least one complete yellow path connecting its two poles. Therefore, every cycle is at least as long as this yellow path length. These paths can be made arbitrarily long, so the graphs constructed in this way can have arbitrarily large girth.</p>

<p>The six edges of the two four-vertex paths between the pole vertices include an edge between each of the six pairs of pole vertices. But each cycle uses at least one pair of pole vertices, so this implies that every two cycles touch, either by sharing a pole vertex or by each containing one endpoint of one of these path edges.</p>

<p>Therefore the graphs constructed in this way have arbitrarily large girth, have all cycles touching, and contain four vertex-disjoint cycles. It also has feedback vertex number four. The MathOverflow question asks whether the four vertex-disjoint cycles can be replaced by an arbitrarily large number of cycles, or equivalently whether the feedback vertex number can be increased, but at this point I don’t even know whether either number can be replaced by five.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105103937279022043">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-10-26T16:16:00Z</updated>
    <published>2020-10-26T16:16:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-11-01T06:02:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-9110359134467856278</id>
    <link href="https://blog.computationalcomplexity.org/feeds/9110359134467856278/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/do-not-become-obsessed-with-polls-unless.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/9110359134467856278" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/9110359134467856278" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/do-not-become-obsessed-with-polls-unless.html" rel="alternate" type="text/html"/>
    <title>Do not become obsessed with the Polls unless...</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I know someone who checks the polls 3 times a day to see who looks like they will be elected prez. She cares A LOT about the election. It is irrelevant to this post who she supports. <div><br/></div><div>I've asked her `if you find out that, say, Biden is up  8 points instead of 9 in Penn. or that Georgia is looking pretty safe for Trump, or that Texas is in play (really?) how will that change your life? What will YOU do differently?'</div><div><br/></div><div>She had no answer. Unfortunately she is still a poll-watcher (I know that means something else usually, but you know what I mean.)</div><div><br/></div><div>So who should be poll-watching or poll-obsessing?</div><div><br/></div><div>1) To be fair to my friend, she might decide to GIVE MORE to her candidate if the polls are saying that he will lose (whoops- by saying `he' I gave away that her candidate is NOT Jo Jorgenson- Libertarian).  I doubt my friend could say to the campaign `I want to you to spend it in state X since I read that its close there' (I read that some big donors in 2016 demanded more say in where the money was spend. I doubt that's a good idea since I suspect the party knows more about how to best spend the money then the donor does.) </div><div><br/></div><div>2) The Biden and the Trump Campaigns SHOULD be poll-watching to decide where to put their efforts. And I suspect they are doing just that.</div><div><br/></div><div>3) A really big donor (my friend is not one of those) MIGHT want to poll watch to decide if the candidate they want needs money. (I wonder if EITHER candidate needs money since they get so much free media.)</div><div><br/></div><div>4) Nate Silver-being a poll-watcher is kind-of his job. And of course writing columns about them and making predictions based on what he sees. My friend is not Nate Silver. </div><div><br/></div><div>5) Other people who have Nate Silver's job. I can't name any- is Nate Silver the most famous... Gee, not sure what job title he has... SO this is now two questions: What is his job title, call it X, and is he the most famous person who does X?</div><div><br/></div><div><br/></div><div>SO- my point- DO NOT be a poll-obsessive unless the information you get will lead to an action you can take. And I suspect that mostly it does not. </div><div><br/></div><div>The primaries are different: If a poll says A can beat X but B cannot beat X, that might guide who you vote for. </div><div><br/></div><div>Misc thought: </div><div><br/></div><div> I've heard the phrase `democratic pollster' and `republican pollster' These terms do not make sense. Would I call myself a `democratic Muffin Mathematician' ? My political leanings do not affect my search for truth about mathematical Muffins. Similarly, one would think that a pollster wants to find the TRUTH, even if its bad news for their employer, ESPECIALLY if its bad news for their employer, so they can help their employer fix it. The phrase `pollster employed by the X party' would make more sense-- however, whenever they are on TV they seem to always say that their candidate is doing well, even when they are not. </div><div><br/></div><div>ADDED LATER: Lance had a great tweet about this post: <i>do not obsess about polls, but DO obsess bout prediction markets. </i>I think in the past prediction markets have been better predictors but some group-think has set in so its no longer clear. (I could be wrong- but thats why I have heard.) </div></div>
    </content>
    <updated>2020-10-25T20:36:00Z</updated>
    <published>2020-10-25T20:36:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-11-02T22:09:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/157</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/157" rel="alternate" type="text/html"/>
    <title>TR20-157 |  Batch Verification and Proofs of Proximity with Polylog Overhead | 

	Guy Rothblum, 

	Ron Rothblum</title>
    <summary>Suppose Alice wants to convince Bob of the correctness of k NP statements. Alice could send k witnesses to Bob, but as k grows the communication becomes prohibitive. Is it possible to convince Bob using smaller communication (without making cryptographic assumptions or bounding the computational power of a malicious Alice)? This is the question of batch verification for NP statements.

Our main result is a new interactive proof protocol for verifying the correctness of k UP statements (NP statements with a unique witness) using communication that is poly-logarithmic in k (and a fixed polynomial in the length of a single witness).

This result is obtained by making progress on a different question in the study of interactive proofs. Suppose Alice wants to convince Bob that a huge dataset has some property. Can this be done if Bob can't even read the entire input? In other words, what properties can be verified in sublinear time? An Interactive Proof of Proximity guarantees that Bob accepts if the input has the property, and rejects if the input is far (say in Hamming distance) from having the property. Two central complexity measures of such a protocol are the query and communication complexities (which should both be sublinear). For every query parameter $q$, and for every language in logspace uniform NC, we construct an interactive proof of proximity with query complexity $q$ and communication complexity $(n/q) \cdot \polylog(n)$.

Both results are optimal up to poly-logarithmic factors, under reasonable complexity-theoretic or cryptographic assumptions. The second result, which is our main technical contribution, builds on a distance amplification technique introduced in a beautiful recent work of Ben-Sasson, Kopparty and Saraf [CCC 2018].</summary>
    <updated>2020-10-25T10:51:48Z</updated>
    <published>2020-10-25T10:51:48Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-11-02T23:33:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/24/tenure-track-tenured-position-at-university-of-california-davis-apply-by-december-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/24/tenure-track-tenured-position-at-university-of-california-davis-apply-by-december-15-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-Track/Tenured position at University of California, Davis (apply by December 15, 2020)</title>
    <summary>The University of California, Davis, seeks an Assistant Professor in any area of theoretical computer science; exceptional candidates at the Associate and Full ranks may be considered. Website: https://recruit.ucdavis.edu/JPF03838?utm_campaign=google_jobs_apply&amp;utm_source=google_jobs_apply&amp;utm_medium=organic Email: amenta@cs.ucdavis.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The University of California, Davis, seeks an Assistant Professor in any area of theoretical computer science; exceptional candidates at the Associate and Full ranks may be considered.</p>
<p>Website: <a href="https://recruit.ucdavis.edu/JPF03838?utm_campaign=google_jobs_apply&amp;utm_source=google_jobs_apply&amp;utm_medium=organic">https://recruit.ucdavis.edu/JPF03838?utm_campaign=google_jobs_apply&amp;utm_source=google_jobs_apply&amp;utm_medium=organic</a><br/>
Email: amenta@cs.ucdavis.edu</p></div>
    </content>
    <updated>2020-10-24T16:11:09Z</updated>
    <published>2020-10-24T16:11:09Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-02T23:34:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/24/simons-berkeley-research-fellowships-for-fall-2021-and-spring-2022-at-the-simons-institute-for-the-theory-of-computing-apply-by-december-15-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/24/simons-berkeley-research-fellowships-for-fall-2021-and-spring-2022-at-the-simons-institute-for-the-theory-of-computing-apply-by-december-15-2020/" rel="alternate" type="text/html"/>
    <title>Simons-Berkeley Research Fellowships for Fall 2021 and Spring 2022 at The Simons Institute for the Theory of Computing (apply by December 15, 2020)</title>
    <summary>The Simons Institute invites applications for Simons-Berkeley Postdoctoral Fellowships and Simons-Berkeley Research Fellowships, to collaborate with UC Berkeley faculty and to participate in the semester-long programs in Fall 2021 and Spring 2022: “Computational Complexity of Statistical Inference”, “Geometric Methods in Optimization and Sampling”, “Causality”, and “Learning and Games”. Website: https://simons.berkeley.edu Email: simonsvisitorservices@berkeley.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Simons Institute invites applications for Simons-Berkeley Postdoctoral Fellowships and Simons-Berkeley Research Fellowships, to collaborate with UC Berkeley faculty and to participate in the semester-long programs in Fall 2021 and Spring 2022: “Computational Complexity of Statistical Inference”, “Geometric Methods in Optimization and Sampling”, “Causality”, and “Learning and Games”.</p>
<p>Website: <a href="https://simons.berkeley.edu">https://simons.berkeley.edu</a><br/>
Email: simonsvisitorservices@berkeley.edu</p></div>
    </content>
    <updated>2020-10-24T01:25:11Z</updated>
    <published>2020-10-24T01:25:11Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-02T23:34:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17707</id>
    <link href="https://rjlipton.wordpress.com/2020/10/23/can-we-solve-it/" rel="alternate" type="text/html"/>
    <title>Can We Solve It?</title>
    <summary>It is a Friday James Maynard is a number theorist. He attended Cambridge as an undergrad and then moved to do his grad work at Oxford at Balliol College. He is now a professor at Oxford. He is one of the world experts on prime density type theorems. Today, since it is Friday, I thought […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>It is a Friday</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/10/23/can-we-solve-it/maynard/" rel="attachment wp-att-17709"><img alt="" class="alignright size-full wp-image-17709" src="https://rjlipton.files.wordpress.com/2020/10/maynard.jpg?w=600"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
James Maynard is a number theorist. He attended Cambridge as an undergrad and then moved to do his grad work at Oxford at Balliol College. He is now a professor at Oxford. He is one of the world experts on prime density type theorems. </p>
<p/><p>
Today, since it is Friday, I thought we would discuss a <em>timely</em> idea of Maynard. Not an idea about time complexity or time in physics, but involving the use of time.<br/>
<span id="more-17707"/></p>
<p>
</p><p/><h2> Decimal Digits </h2><p/>
<p/><p>
No it’s not a technical idea of his. He has had many ideas, for instance, that shed light on the beautiful structure of primes. For example, he <a href="https://arxiv.org/abs/1604.01041">proved</a> in 2016 that</p>
<blockquote><p><b>Theorem 1</b> <em> For each decimal digit <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d}"/>, there are infinitely many prime numbers that do not have <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d}"/> in their decimal expansion. </em>
</p></blockquote>
<p/><p>
This is not known for all digit systems: For binary, our favorite system as complexity theorists, this is still an open problem. Of course a binary prime with only <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1}"/>‘s must be of the form: 	</p>
<p align="center"><img alt="\displaystyle  2^{p} - 1 = 111\dots 1, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7Bp%7D+-+1+%3D+111%5Cdots+1%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  2^{p} - 1 = 111\dots 1, "/></p>
<p>where <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p}"/> must be a prime.</p>
<p>These are the famous <a href="https://en.wikipedia.org/wiki/Mersenne_prime">Mersenne primes</a> named for Marin Mersenne. The largest prime is <img alt="{2^{82,589,933} - 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B82%2C589%2C933%7D+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2^{82,589,933} - 1}"/> as of today—at least I believe this is true. For further discussion, see a 2001 <a href="https://projecteuclid.org/euclid.em/999188636">paper</a> by Samuel Wagstaff titled, “Prime Numbers with a Fixed Number of One Bits or Zero Bits in Their Binary Representation.” </p>
<p/><h2> Maynard’s Friday Rule </h2><p/>
<p/><p>
Maynard’s idea is based on his quest to understand whether known techniques can solve some problem. Of course the best way to understand this is to solve the problem. His above theorem is a perfect example of this. In the abstract he says: </p>
<blockquote><p><b> </b> <em> The proof is an application of the Hardy-Littlewood circle method to a binary problem, and rests on obtaining suitable `Type I’ and `Type II’ arithmetic information for use in Harman’s sieve to control the minor arcs. </em>
</p></blockquote>
<p>The proof may be based on known techniques, but is still very hard. He needs <img alt="{70}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{70}"/> pages to make it work. </p>
<p>
Maynard’s idea is to set aside time to remind himself why existing techniques have not worked against math’s biggest open problems.</p>
<blockquote><p><b> </b> <em> I often spend Friday afternoons just thinking about trying to directly attack some famous problem. This is much less because I think there’s a realistic way of solving the problem, but more because I think it’s important for me to understand where plausible techniques fail. </em>
</p></blockquote>
<p/><p>
One can imagine that he had a Friday afternoon think. During it he asked himself:</p>
<blockquote><p><b> </b> <em> Suppose I try to show that there are primes without some particular digit. This is a density type theorem. Well could I use the Hardy-Littlewood method. But it cannot work because <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\dots}"/> Wait here is a possible way around the roadblock. Hmmm. </em>
</p></blockquote>
<p/><p>
Maybe he looked at Terence Tao’s blog post on this very <a href="https://terrytao.wordpress.com/2012/05/20/heuristic-limitations-of-the-circle-method/">issue</a>. It helped that Maynard is an expert on the Hardy-Littlewood method, but perhaps thinking why it could not work helped him figure out how it could work. </p>
<p>
</p><p/><h2> Our Friday Rule </h2><p/>
<p/><p>
Today is Friday, so I though what should I think about? What problems and what techniques? Here is a possible example. Let’s look at the <a href="https://rjlipton.wordpress.com/2019/09/08/separating-words-by-automata/">On Lower Bounds for the Separating Word Problem</a>. </p>
<p>
An approach is based on the following. Let <img alt="{F(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{F(n)}"/> be the set of all <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f(x)}"/> degree <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> polynomials, with coefficients <img alt="{-1,0,+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%2C0%2C%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{-1,0,+1}"/>. Let <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\epsilon&gt;0}"/> be a constant. Our hypothesis <strong>H</strong><img alt="{(\epsilon)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(\epsilon)}"/> is: For every polynomial <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f(x)}"/> in <img alt="{F(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{F(n)}"/>, there is some prime <img alt="{p \le Cn^{\epsilon}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%5Cle+Cn%5E%7B%5Cepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p \le Cn^{\epsilon}}"/>, so that for some <img alt="{1 \le k \le n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+k+%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1 \le k \le n}"/> 	</p>
<p align="center"><img alt="\displaystyle  f(k) \not\equiv 0 \bmod p. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28k%29+%5Cnot%5Cequiv+0+%5Cbmod+p.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  f(k) \not\equiv 0 \bmod p. "/></p>
<p>How small can <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\epsilon}"/> be so that <strong>H</strong><img alt="{(\epsilon)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(\epsilon)}"/> is true? What are the methods that we should think about? What methods can we see that cannot prove H<img alt="{(\epsilon)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{(\epsilon)}"/>? Can we, for example, show that we can use a random argument? Can we should that they are not enough primes <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p}"/> in the range? Hmmm <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\dots}"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Do you like Maynard’s Friday rule? What problems and what techniques would you think about? </p>
<p/></font></font></div>
    </content>
    <updated>2020-10-23T19:56:51Z</updated>
    <published>2020-10-23T19:56:51Z</published>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="density"/>
    <category term="twin primes"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-11-02T23:33:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/23/3-postdocs-at-university-of-lyon-apply-by-january-6-2021/</id>
    <link href="https://cstheory-jobs.org/2020/10/23/3-postdocs-at-university-of-lyon-apply-by-january-6-2021/" rel="alternate" type="text/html"/>
    <title>3 postdocs at University of Lyon (apply by January 6, 2021)</title>
    <summary>The Excellence Laboratory Milyon (Labex Milyon) opens the 2021 campaign of postdoctoral researchers in Lyon–Saint-Etienne in Mathematics, Computer Science and their interactions (including some aspects of theoretical physics). Milyon offers three postdoctoral positions of two years with no teaching load for 2021–2023. The application is open to all research areas of labex Milyon. Website: https://milyon.universite-lyon.fr/postdoctoral-positions-2021-2023–130160.kjsp?RH=1571748911317 […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Excellence Laboratory Milyon (Labex Milyon) opens the 2021 campaign of postdoctoral researchers in Lyon–Saint-Etienne in Mathematics, Computer Science and their interactions (including some aspects of theoretical physics).</p>
<p>Milyon offers three postdoctoral positions of two years with no teaching load for 2021–2023. The application is open to all research areas of labex Milyon.</p>
<p>Website: <a href="https://milyon.universite-lyon.fr/postdoctoral-positions-2021-2023--130160.kjsp?RH=1571748911317">https://milyon.universite-lyon.fr/postdoctoral-positions-2021-2023–130160.kjsp?RH=1571748911317</a><br/>
Email: sabot@math.univ-lyon1.fr</p></div>
    </content>
    <updated>2020-10-23T19:35:35Z</updated>
    <published>2020-10-23T19:35:35Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-02T23:34:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7831</id>
    <link href="https://windowsontheory.org/2020/10/23/full-replica-symmetry-breaking-based-algorithms-for-dummies/" rel="alternate" type="text/html"/>
    <title>Full-replica-symmetry-breaking based algorithms for dummies</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">One of the fascinating lines of research in recent years has been a convergence between the statistical physics and theoretical computer science points of view on optimization problems.`This blog post is mainly a note to myself (i.e., I’m the “dummy” 😃), trying to work out some basic facts in some of this line of work. … <a class="more-link" href="https://windowsontheory.org/2020/10/23/full-replica-symmetry-breaking-based-algorithms-for-dummies/">Continue reading <span class="screen-reader-text">Full-replica-symmetry-breaking based algorithms for dummies</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One of the fascinating lines of research in recent years has been a convergence between the statistical physics and theoretical computer science points of view on optimization problems.<br/>`<br/>This blog post is mainly a note to myself (i.e., I’m the “dummy” <img alt="&#x1F603;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f603.png" style="height: 1em;"/>), trying to work out some basic facts in some of this line of work. it was inspired by this <a href="https://simons.berkeley.edu/talks/breaking-1rsb-random-max-nae-sat">excellent talk of Eliran Subag</a>, itself part of a great <a href="https://simons.berkeley.edu/workshops/schedule/14243">Simons institute workshop</a> which I am still planning to watch the talks of. I am posting this in case it’s useful for others, but this is quite rough, missing many references, and I imagine I have both math mistakes as well as inaccuracies in how I refer to the literature – would be grateful for comments!</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/ekkarv2xiaam8jw.png"><img alt="" class="wp-image-7833" src="https://windowsontheory.files.wordpress.com/2020/10/ekkarv2xiaam8jw-e1603479127948.png"/></a>Screen shot from <a href="https://simons.berkeley.edu/talks/breaking-1rsb-random-max-nae-sat">Eliran Subag’s talk</a> demonstrating the difference between “easy” and “hard” instances.</figure>



<p>In computer science, <em>optimization</em> is the task of finding an assignment <img alt="x_1,\ldots,x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_n"/> that minimizes some function <img alt="J(x_1,\ldots,x_n)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x_1,\ldots,x_n)"/>. In statistical physics we think of <img alt="x_1,\ldots,x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_n"/> as the states of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> particles, and <img alt="J(x_1,\ldots,x_n)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x_1,\ldots,x_n)"/> as the <em>energy</em> of this state. Finding the minimum assignment corresponds to finding the <em>ground state</em>, and another computational problem is sampling from the <em>Gibbs distribution</em> where the probability of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is proportional to <img alt="\exp(-\beta J(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+J%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\exp(-\beta J(x))"/> for some <img alt="\beta&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\beta&gt;0"/>.</p>



<p>Two prototypical examples of such problems are:</p>



<ol><li>Random 3SAT – in this case <img alt="x\in { \pm 1 }^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin+%7B+%5Cpm+1+%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\in { \pm 1 }^n"/> and <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/> is the number of clauses violated by the assignment <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> for a random formula.</li><li>Sherrington-Kirpatrick model – in this case <img alt="x \in { \pm 1 }^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B+%5Cpm+1+%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x \in { \pm 1 }^n"/> and <img alt="J(x)= \sum_{i,j} J_{i,j}x_ix_j" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29%3D+%5Csum_%7Bi%2Cj%7D+J_%7Bi%2Cj%7Dx_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)= \sum_{i,j} J_{i,j}x_ix_j"/> where <img alt="J_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=J_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J_{i,j}"/> are independent normal variables with variance <img alt="1/n" class="latex" src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/n"/> for <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/> and variance <img alt="2/n" class="latex" src="https://s0.wp.com/latex.php?latex=2%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2/n"/> for <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/>. (Another way to say it is that <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/> is the matrix <img alt="A+A^\top" class="latex" src="https://s0.wp.com/latex.php?latex=A%2BA%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A+A^\top"/> where <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/>‘s entries are chosen i.i.d from <img alt="N(0,\tfrac{1}{2n}))" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7B2n%7D%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,\tfrac{1}{2n}))"/>.)</li></ol>



<p>The physics and CS intuition is that these two problems have very different computational properties. For random 3SAT (of the appropriate density), it is believed that the set of solutions is “shattered” in the sense that it is partitioned to exponentially many clusters, separated from one another by large distance. It is conjectured that in this setting the problem will be computationally hard. Similarly from the statistical physics point of view, it is conjectured that if we were to start with the uniform distribution (i.e., a “hot” system) and “lower the temperature” (increase <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\beta"/>) at a rate that is not exponentially slow then we will get “stuck” at a “metastable” state. This is analogous how when we heat up sand and then cool it quickly then rather than returning to its original state, the sand will get stuck at the metastable state of glass.</p>



<p>In contrast for the Sherrington-Kirpatrick (SK) model, the geometry is more subtle, but interestingly this enables better algorithms. The SK model is extermely widely studied, with hundreds of papers, and was the inspiration for the simulated annealing algorithm. If memory serves me right, Sherrington and Kirpatrick made the wrong conjecture on the energy of the ground state, and then Parisi came up in 1979 with a wonderful and hugely influential way to compute this value. Parisi’s calculation was heuristic, but about 30 years later, first Talagrand and later Panchenko proved rigorously many of Parisi’s conjectures. (See this <a href="https://arxiv.org/abs/1211.1094v1">survey of Panchenko</a>.)</p>



<p>Recently <a href="https://arxiv.org/abs/1812.10897">Montanari</a> gave a polynomial time algorithm to find a state with energy that is arbitrarily close to the ground state’s. The algorithm relies on Parisi’s framework and in particular on the fact that the solution space has a property known as “full replica symmetry breaking (RSB)” / “ultrametricity”. Parisi’s derivations (and hence also Montanari’s analysis) are highly elaborate and I admit that I have not yet been able to fully follow it. The nice thing is that (as we’ll see) it is possible to describe at least some of the algorithmic results without going into this theory. In the end of the post I will discuss a bit some of the relation to this theory, which is the underlying inspiration for Subag’s results described here.</p>



<p><strong>Note:</strong> These papers and this blog post deal with the <em>search problem</em> of finding a solution that minimizes the objective. The <em>refutation problem</em> of certifying that this minimum is at least <img alt="-C" class="latex" src="https://s0.wp.com/latex.php?latex=-C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-C"/> for some <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="C"/> has often been studied. The computational complexity of these problems need not be identical. In particular there are cases where the search problem has an efficient algorithm achieving value <img alt="-C^*" class="latex" src="https://s0.wp.com/latex.php?latex=-C%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-C^*"/> but the best refutation algorithm can only certify that the value is at most <img alt="-C'" class="latex" src="https://s0.wp.com/latex.php?latex=-C%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-C'"/> for <img alt="C' \gg C^*" class="latex" src="https://s0.wp.com/latex.php?latex=C%27+%5Cgg+C%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="C' \gg C^*"/>.</p>



<h2>Analysis of a simpler setting</h2>



<p>Luckily, there is a similar computational problem, for which the analysis of analogous algorithm, which was <a href="https://arxiv.org/abs/1812.04588">discovered by Subag</a> and was the partial inspiration for Montanari’s work, is much simpler. Specifically, we consider the case where <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is an element of the unit sphere, and <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/> is a degree <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> polynomial with random Gaussian coefficients. Specifically, for every vector <img alt="\gamma = (\gamma_2,\ldots,\gamma_d)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%28%5Cgamma_2%2C%5Cldots%2C%5Cgamma_d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma = (\gamma_2,\ldots,\gamma_d)"/>, we let <img alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D+%2B+%5Ccdots+%2B+%5Cgamma_d+J%5Ed+%5Ccdot+x%5E%7B%5Cotimes+p%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}"/> where for every <img alt="p \in {2,\ldots, d }" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cin+%7B2%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p \in {2,\ldots, d }"/>, <img alt="J_p" class="latex" src="https://s0.wp.com/latex.php?latex=J_p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J_p"/> is a random tensor of order <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> whose <img alt="n^p" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n^p"/> coefficients are all chosen i.i.d in <img alt="N(0,1/n)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,1/n)"/>. (We assume that polynomial does not have constant or linear components.)</p>



<p>Depending on <img alt="\gamma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma"/>, the computational and geometrical properties of this problem can vary considerably. The case that <img alt="\gamma = (1,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%281%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma = (1,0,\ldots,0)"/> (i.e., only <img alt="J^2" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J^2"/> has a non-zero coeffiecent) corresponds to finding the unit vector <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> minimizing <img alt="x^\top J x" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%5Ctop+J+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^\top J x"/> for a random matrix <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, which of course corresponds to the efficiently solveable minimum eigenvector problem. In contrast, the case <img alt="\gamma = (0,1,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%280%2C1%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma = (0,1,0,\ldots,0)"/> corresponds to finding a rank one component of a random three-tensor, which is believed to be computationally difficult. The Parisi calculations give a precise condition <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> on the vector <img alt="\gamma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma"/> such that if <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> holds then the solution space has the “full RSB” property (and hence the problem is computationally easy) and if <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> does not hold then the solution space does not have this property (and potentially the problem is hard).</p>



<p>These calculations also give rise to the following theorem:</p>



<p><strong>Theorem (<a href="https://arxiv.org/abs/1512.08492">Chen and Sen, Proposition 2</a>):</strong> If <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> holds then in the limit <img alt="n \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n \rightarrow \infty"/>, <img alt="\min_{x : |x|=1} J(x) = -\int_0^1 \sqrt{\nu''(q)} dq" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bx+%3A+%7Cx%7C%3D1%7D+J%28x%29+%3D+-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min_{x : |x|=1} J(x) = -\int_0^1 \sqrt{\nu''(q)} dq"/>, where <img alt="\nu(q) = \sum_{p \geq 2} \gamma_p^2 q^p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu%28q%29+%3D+%5Csum_%7Bp+%5Cgeq+2%7D+%5Cgamma_p%5E2+q%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nu(q) = \sum_{p \geq 2} \gamma_p^2 q^p"/>. (That is, <img alt="\nu''" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nu''"/> is the second derivative of this univariate polynomial)</p>



<p>We will not discuss the proof of this theorem, but rather how, taking it as a black box, it leads to an algorithm for minimizing <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/> that achieves a near-optimal value (assuming <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> holds).</p>



<p>It is a nice exercise to show that for every two vectors <img alt="x,x'\in\mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cx%27%5Cin%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x,x'\in\mathbb{R}^n"/>, <img alt="\mathbb{E}_J [J(x)J(x')] = \nu(\langle x,x' \rangle)/n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_J+%5BJ%28x%29J%28x%27%29%5D+%3D+%5Cnu%28%5Clangle+x%2Cx%27+%5Crangle%29%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_J [J(x)J(x')] = \nu(\langle x,x' \rangle)/n"/>. Hence for any unit vector <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/> is a random variable with mean zero and standard deviation <img alt="\sqrt{\nu(1)/n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cnu%281%29%2Fn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sqrt{\nu(1)/n}"/>. Since (after some coarsening) the number of unit vectors of dimension <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> can be thought of as <img alt="c^n" class="latex" src="https://s0.wp.com/latex.php?latex=c%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="c^n"/> for some <img alt="c&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=c%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="c&gt;1"/>, and we expect the probability of deviating <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="t"/> standard deviations to be <img alt="\exp(-c' t^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-c%27+t%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\exp(-c' t^2)"/>, the minimum value of <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/> should be <img alt="-c'' \sqrt{\nu(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=-c%27%27+%5Csqrt%7B%5Cnu%281%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-c'' \sqrt{\nu(1)}"/> for some constant <img alt="c''" class="latex" src="https://s0.wp.com/latex.php?latex=c%27%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="c''"/>. However determining this constant is non trivial and is the result of the Parisi theory.</p>



<p>To get a better sense for the quantity <img alt="-\int_0^1 \sqrt{\nu''(q)} dq" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\int_0^1 \sqrt{\nu''(q)} dq"/>, let’s consider two simple cases:</p>



<ul><li>If <img alt="\gamma = (1,0,\ldots)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%281%2C0%2C%5Cldots%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma = (1,0,\ldots)"/> (i.e., <img alt="J(x) = \sum_{i,j}M_{i,j}x_ix_j" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Csum_%7Bi%2Cj%7DM_%7Bi%2Cj%7Dx_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) = \sum_{i,j}M_{i,j}x_ix_j"/> for random matrix <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M"/>) then <img alt="\nu(q)= q^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu%28q%29%3D+q%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nu(q)= q^2"/> and <img alt="\nu''(q) = 2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27%28q%29+%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nu''(q) = 2"/>, meaning that <img alt="-\int_0^1 \sqrt{\nu''(q)} dq = -\sqrt{2}" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%3D+-%5Csqrt%7B2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\int_0^1 \sqrt{\nu''(q)} dq = -\sqrt{2}"/>. This turns out to be the actual minimum value. Indeed in this case <img alt="\min_{|x|^2=1} J(x) = \tfrac{1}{2} \lambda_{min}(M + M^\top)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin_%7B%7Cx%7C%5E2%3D1%7D+J%28x%29+%3D+%5Ctfrac%7B1%7D%7B2%7D+%5Clambda_%7Bmin%7D%28M+%2B+M%5E%5Ctop%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min_{|x|^2=1} J(x) = \tfrac{1}{2} \lambda_{min}(M + M^\top)"/>. But the matrix <img alt="A=\tfrac{1}{2}(M+M^\top)" class="latex" src="https://s0.wp.com/latex.php?latex=A%3D%5Ctfrac%7B1%7D%7B2%7D%28M%2BM%5E%5Ctop%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A=\tfrac{1}{2}(M+M^\top)"/>‘s non diagonal entries are distributed like <img alt="N(0,\tfrac{1}{2n})" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7B2n%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,\tfrac{1}{2n})"/> and the diagonal entries like <img alt="N(0,\tfrac{1}{n})" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B1%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,\tfrac{1}{n})"/> which means that <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> is distributed as <img alt="\tfrac{1}{\sqrt{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{1}{\sqrt{2}}"/> times a random matrix <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/> from the <em>Gaussian Orthogonal Ensemble (GOE)</em> where <img alt="B_{i,j} \sim N(0,1/n)" class="latex" src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Cj%7D+%5Csim+N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B_{i,j} \sim N(0,1/n)"/> for off diagonal entries and <img alt="B_{i,i} \sim N(0,2/n)" class="latex" src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Ci%7D+%5Csim+N%280%2C2%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B_{i,i} \sim N(0,2/n)"/> for diagonal entries. The minimum eigenvalue of such matrices is known to be <img alt="-2\pm o(1)" class="latex" src="https://s0.wp.com/latex.php?latex=-2%5Cpm+o%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-2\pm o(1)"/> with high probability.<br/></li><li>If <img alt="\gamma = (0,\ldots,1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma+%3D+%280%2C%5Cldots%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma = (0,\ldots,1)"/> (i.e. <img alt="J(x) = T \cdot x^{\otimes d}" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+T+%5Ccdot+x%5E%7B%5Cotimes+d%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) = T \cdot x^{\otimes d}"/> for a random <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/>-tensor <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T"/>) then <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> does not hold. Indeed, in this case the value <img alt="-\int_0^1 \sqrt{\nu''(q)} dq = - \int_0^1 \sqrt{d (d-1)q^{d-2}} dq = - \sqrt{d(d-1)}\tfrac{1}{d/2-1} \approx -2" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%3D+-+%5Cint_0%5E1+%5Csqrt%7Bd+%28d-1%29q%5E%7Bd-2%7D%7D+dq+%3D+-+%5Csqrt%7Bd%28d-1%29%7D%5Ctfrac%7B1%7D%7Bd%2F2-1%7D+%5Capprox+-2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\int_0^1 \sqrt{\nu''(q)} dq = - \int_0^1 \sqrt{d (d-1)q^{d-2}} dq = - \sqrt{d(d-1)}\tfrac{1}{d/2-1} \approx -2"/> for large <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/>. However I believe (though didn’t find the reference) that the actual minimum tends to <img alt="-\infty" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\infty"/> with <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/>. </li></ul>



<p>While the particular form of the property <img alt="P(\gamma)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28%5Cgamma%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P(\gamma)"/> is not important for this post, there are several equivalent ways to state it, see Proposition 1 in <a href="https://arxiv.org/abs/1812.04588">Subag’s paper</a>. One of them is that the function <img alt="\nu''(t)^{-1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu%27%27%28t%29%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nu''(t)^{-1/2}"/> (note the negative exponent) is concave on the interval <img alt="(0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%280%2C1%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(0,1]"/>.<br/>It can be shown that this condition cannot be satisfied if <img alt="\gamma_2 = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_2 = 0"/>, and that for every setting of <img alt="\gamma_3,\ldots,\gamma_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_3%2C%5Cldots%2C%5Cgamma_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_3,\ldots,\gamma_d"/>, if <img alt="\gamma_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_2"/> is large enough then it will be satisfied.</p>



<p>The central result of Subag’s paper is the following:</p>



<p><strong>Theorem:</strong> For every <img alt="\epsilon&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\epsilon&gt;0"/>, there is a polynomial-time algorithm <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> such that on input random <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/> chosen according to the distribution above, with high probability <img alt="A(J)=x" class="latex" src="https://s0.wp.com/latex.php?latex=A%28J%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A(J)=x"/> such that <img alt="J(x) \leq -\int_0^1 \sqrt{\nu''(q)} dq + \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%5Cleq+-%5Cint_0%5E1+%5Csqrt%7B%5Cnu%27%27%28q%29%7D+dq+%2B+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) \leq -\int_0^1 \sqrt{\nu''(q)} dq + \epsilon"/>.</p>



<p>The algorithm itself, and the idea behind the analysis are quite simple. In some sense it’s the second algorithm you would think of (or at least the second algorithm according to some ordering).</p>



<p>The first algorithm one would think of is gradient descent. We start at some initial point <img alt="x^0" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^0"/>, and repeat the transformation <img alt="x^{t+1} \leftarrow x^t - \eta \nabla J(x^t)" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+%5Cnabla+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{t+1} \leftarrow x^t - \eta \nabla J(x^t)"/> for some small <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/> (and normalizing the norm). Unfortunately, we will generally run into <em>saddle points</em> when we do so, with the gradient being zero. In fact, for simplicity, below we will always make the pessimistic assumption that we are constantly on a saddle point. (This assumption turns out to be true in the actual algorithm, and if it was not then we can always use gradient descent until we hit a saddle.)</p>



<p>The second algorithm one could think of would be to use the Hessian instead of the gradient. That is, repeat the transformation <img alt="x^{t+1} \leftarrow x^t - \eta u" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{t+1} \leftarrow x^t - \eta u"/> where <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> is the minimal eigenvector of <img alt="\nabla^2 J(x^t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J(x^t)"/> (i.e., the Hessian matrix <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H"/> such that <img alt="H_{i,j} = \tfrac{\partial J(x^t)}{\partial x_i \partial x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=H_%7Bi%2Cj%7D+%3D+%5Ctfrac%7B%5Cpartial+J%28x%5Et%29%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H_{i,j} = \tfrac{\partial J(x^t)}{\partial x_i \partial x_j}"/> ). By the Taylor approximation <img alt="J(x - \eta u) \approx J(x) - \eta \nabla J(x) \cdot u + \tfrac{1}{2} \eta^2 u^\top \nabla^2 J(x) u" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x+-+%5Ceta+u%29+%5Capprox+J%28x%29+-+%5Ceta+%5Cnabla+J%28x%29+%5Ccdot+u+%2B+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+u%5E%5Ctop+%5Cnabla%5E2+J%28x%29+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x - \eta u) \approx J(x) - \eta \nabla J(x) \cdot u + \tfrac{1}{2} \eta^2 u^\top \nabla^2 J(x) u"/> (and since we assume the gradient is zero) the change in the objective will be roughly <img alt="\tfrac{1}{2} \eta^2 \lambda_{min}(\nabla^2 J(x^t))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda_%7Bmin%7D%28%5Cnabla%5E2+J%28x%5Et%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{1}{2} \eta^2 \lambda_{min}(\nabla^2 J(x^t))"/>. (Because we assume the gradient vanishes, it will not make a difference whether we update with <img alt="-\eta u " class="latex" src="https://s0.wp.com/latex.php?latex=-%5Ceta+u+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\eta u "/> or <img alt="+\eta u" class="latex" src="https://s0.wp.com/latex.php?latex=%2B%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="+\eta u"/>, but we use <img alt="-\eta" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\eta"/> for consistency with gradient descent.)</p>



<p>The above approach is promising, but we still need some control over the norm. The way that Subag handles this is that he starts with <img alt="x^0 = 0" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E0+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^0 = 0"/>, and at each step takes a step in an orthogonal direction, and so within <img alt="1/\eta^2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\eta^2"/> steps he will get to a unit norm vector. That is, the algorithm is as follows:</p>



<p><strong>Algorithm:</strong></p>



<p><strong>Input:</strong> <img alt="J:\mathbb{R}^n \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=J%3A%5Cmathbb%7BR%7D%5En+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J:\mathbb{R}^n \rightarrow \mathbb{R}"/>.</p>



<p><strong>Goal:</strong> Find unit <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> approximately minimizing <img alt="J(x)" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x)"/></p>



<ol><li>Initialize <img alt="x^0 = 0^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E0+%3D+0%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^0 = 0^n"/></li><li>For <img alt="t=0,\ldots,1/\eta^2-1" class="latex" src="https://s0.wp.com/latex.php?latex=t%3D0%2C%5Cldots%2C1%2F%5Ceta%5E2-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="t=0,\ldots,1/\eta^2-1"/>: i. Let <img alt="u^t" class="latex" src="https://s0.wp.com/latex.php?latex=u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u^t"/> be a unit vector <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> such that <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> is orthogonal to <img alt="u^0,\ldots,u^{t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=u%5E0%2C%5Cldots%2Cu%5E%7Bt-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u^0,\ldots,u^{t-1}"/> and <img alt="u^\top \nabla^2 J(x^t) u \approx \lambda_{min}(\nabla^2 J(x^t))" class="latex" src="https://s0.wp.com/latex.php?latex=u%5E%5Ctop+%5Cnabla%5E2+J%28x%5Et%29+u+%5Capprox+%5Clambda_%7Bmin%7D%28%5Cnabla%5E2+J%28x%5Et%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u^\top \nabla^2 J(x^t) u \approx \lambda_{min}(\nabla^2 J(x^t))"/>. (Since the bottom eigenspace of <img alt="\nabla^2 J(x^t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J(x^t)"/> has large dimention, we can find a vector <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> that is not only nearly minimal eigenvector but also orthogonal to all prior ones. Also, as mentioned, we assume that <img alt="\nabla \cdot u \approx 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+%5Ccdot+u+%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla \cdot u \approx 0"/>.) ii. Set <img alt="x^{t+1} \leftarrow x^t - \eta u^t" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{t+1} \leftarrow x^t - \eta u^t"/>.</li><li>Output <img alt="x^{1/\eta^2} = -\eta\sum_{t=0}^{1/\eta^2-1} u^t" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B1%2F%5Ceta%5E2%7D+%3D+-%5Ceta%5Csum_%7Bt%3D0%7D%5E%7B1%2F%5Ceta%5E2-1%7D+u%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{1/\eta^2} = -\eta\sum_{t=0}^{1/\eta^2-1} u^t"/></li></ol>



<p>(The fact that the number of steps is <img alt="1/\eta^2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\eta^2"/> and not <img alt="1/\eta" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\eta"/> is absolutely crucial for the algorithm’s success; without it we would not have been able to use the second order contribution that arise from the Hessian.)</p>



<p>If we define <img alt="\lambda_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda_t"/> to be the minimum eigenvalue at time <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="t"/>, we get that the final objective value achieved by the algorithm satisfies</p>



<p><img alt="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda_t" class="latex" src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Csum_%7Bt%3D1%7D%5E%7B1%2F%5Ceta%5E2%7D+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda_t"/></p>



<p>Now due to rotation invariance, the distribution of <img alt="\nabla^2 J" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J"/> at the point <img alt="x^t" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Et&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^t"/> is the same as <img alt="\nabla^2J" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2J"/> at the point <img alt="(|x^t|,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7Cx%5Et%7C%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(|x^t|,0,\ldots,0)"/>. Using concentration of measure arguments, it can be shown that the minimum eigenvalue of <img alt="\nabla^2 J(x^t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28x%5Et%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J(x^t)"/> will be close with high probability to the minimum eigenvalue of <img alt="\nabla^2 J(|x^t|,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28%7Cx%5Et%7C%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J(|x^t|,0,\ldots,0)"/>.<br/>Since <img alt="\|x^t\|^2 = \eta^2 t" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Cx%5Et%5C%7C%5E2+%3D+%5Ceta%5E2+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\|x^t\|^2 = \eta^2 t"/> we can write</p>



<p><img alt="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda(\sqrt{\eta^2 t})" class="latex" src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Csum_%7Bt%3D1%7D%5E%7B1%2F%5Ceta%5E2%7D+%5Ctfrac%7B1%7D%7B2%7D+%5Ceta%5E2+%5Clambda%28%5Csqrt%7B%5Ceta%5E2+t%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="VAL = \sum_{t=1}^{1/\eta^2} \tfrac{1}{2} \eta^2 \lambda(\sqrt{\eta^2 t})"/></p>



<p>where <img alt="\lambda(q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%28q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda(q)"/> is the minimum eigenvalue of <img alt="\nabla^2 J" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J"/> at the point <img alt="(q,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28q%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(q,0,\ldots,0)"/>.<br/>Taking <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/> to zero, we get that (approximately) the value of the solution output by the algorithm will satisfy</p>



<p><img alt="VAL = \int_0^1 \tfrac{1}{2} \lambda(\sqrt{q}) dq" class="latex" src="https://s0.wp.com/latex.php?latex=VAL+%3D+%5Cint_0%5E1+%5Ctfrac%7B1%7D%7B2%7D+%5Clambda%28%5Csqrt%7Bq%7D%29+dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="VAL = \int_0^1 \tfrac{1}{2} \lambda(\sqrt{q}) dq"/></p>



<p>and hence the result will be completed by showing that</p>



<p><img alt="\lambda(\sqrt{q}) = 2 \sqrt{\nu''(q)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%28%5Csqrt%7Bq%7D%29+%3D+2+%5Csqrt%7B%5Cnu%27%27%28q%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda(\sqrt{q}) = 2 \sqrt{\nu''(q)}"/></p>



<p>To do this, let’s recall the definition of <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/>:</p>



<p><img alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}" class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D+%2B+%5Ccdots+%2B+%5Cgamma_d+J%5Ed+%5Ccdot+x%5E%7B%5Cotimes+p%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3} + \cdots + \gamma_d J^d \cdot x^{\otimes p}"/> where for every <img alt="p \in {2,\ldots, d }" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cin+%7B2%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p \in {2,\ldots, d }"/>, <img alt="J_p" class="latex" src="https://s0.wp.com/latex.php?latex=J_p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J_p"/> is a random tensor of order <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> whose <img alt="n^p" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ep&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n^p"/> coefficients are all chosen i.i.d in <img alt="N(0,1/n)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,1/n)"/>.</p>



<p>For simplicity, let’s assume that <img alt="d=3" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d=3"/> and hence</p>



<p><img alt="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3}\;." class="latex" src="https://s0.wp.com/latex.php?latex=J%28x%29+%3D+%5Cgamma_2+J%5E2+%5Ccdot+x%5E%7B%5Cotimes+2%7D+%2B+%5Cgamma_3+J%5E3+%5Ccdot+x%5E%7B%5Cotimes+3%7D%5C%3B.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J(x) = \gamma_2 J^2 \cdot x^{\otimes 2} + \gamma_3 J^3 \cdot x^{\otimes 3}\;."/></p>



<p>(The calculations in the general case are similar)</p>



<p>The <img alt="i,j" class="latex" src="https://s0.wp.com/latex.php?latex=i%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i,j"/> entry of <img alt="\nabla^2 J(\sqrt{q},0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2+J%28%5Csqrt%7Bq%7D%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2 J(\sqrt{q},0,\ldots,0)"/> equals <img alt="\tfrac{\partial J(q,0,\ldots,0)}{\partial x_i \partial x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7B%5Cpartial+J%28q%2C0%2C%5Cldots%2C0%29%7D%7B%5Cpartial+x_i+%5Cpartial+x_j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tfrac{\partial J(q,0,\ldots,0)}{\partial x_i \partial x_j}"/>. The contribution of the <img alt="J^2" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J^2"/> component to this term only arises from the terms corresponding to either <img alt="x_ix_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_ix_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_ix_j"/> or <img alt="x_jx_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_jx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_jx_i"/> and hence for <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/> it equals <img alt="\gamma_2 (J_{i,j}+J_{j,i})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+%28J_%7Bi%2Cj%7D%2BJ_%7Bj%2Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_2 (J_{i,j}+J_{j,i})"/> which is distributed like <img alt="\gamma_2 N(0,\tfrac{2}{n}) = N(0, \tfrac{2\gamma_2^2}{n})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_2+N%280%2C%5Ctfrac%7B2%7D%7Bn%7D%29+%3D+N%280%2C+%5Ctfrac%7B2%5Cgamma_2%5E2%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_2 N(0,\tfrac{2}{n}) = N(0, \tfrac{2\gamma_2^2}{n})"/>. For <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/>, since <img alt="(x_i^2)'' = 2" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_i%5E2%29%27%27+%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x_i^2)'' = 2"/>, the contributioon equals <img alt="2 \gamma_2 J_{i,i}" class="latex" src="https://s0.wp.com/latex.php?latex=2+%5Cgamma_2+J_%7Bi%2Ci%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2 \gamma_2 J_{i,i}"/> which is distributed like <img alt="N(0,\tfrac{4\gamma_2^2}{n})" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B4%5Cgamma_2%5E2%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,\tfrac{4\gamma_2^2}{n})"/>.</p>



<p>The contribution from the <img alt="J^3" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J^3"/> component comes (in the case <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/>) from all the <img alt="6=3!" class="latex" src="https://s0.wp.com/latex.php?latex=6%3D3%21&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="6=3!"/> terms involving <img alt="1,i,j" class="latex" src="https://s0.wp.com/latex.php?latex=1%2Ci%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1,i,j"/> that is, <img alt="\gamma_3 (J_{1,i,j}\sqrt{q} + J_{1,j,i}\sqrt{q} + J_{i,1,j}\sqrt{q}+J_{j,1,i}\sqrt{q}+J_{i,j,1}\sqrt{q}+J_{j,i,1})\sqrt{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_3+%28J_%7B1%2Ci%2Cj%7D%5Csqrt%7Bq%7D+%2B+J_%7B1%2Cj%2Ci%7D%5Csqrt%7Bq%7D+%2B+J_%7Bi%2C1%2Cj%7D%5Csqrt%7Bq%7D%2BJ_%7Bj%2C1%2Ci%7D%5Csqrt%7Bq%7D%2BJ_%7Bi%2Cj%2C1%7D%5Csqrt%7Bq%7D%2BJ_%7Bj%2Ci%2C1%7D%29%5Csqrt%7Bq%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_3 (J_{1,i,j}\sqrt{q} + J_{1,j,i}\sqrt{q} + J_{i,1,j}\sqrt{q}+J_{j,1,i}\sqrt{q}+J_{i,j,1}\sqrt{q}+J_{j,i,1})\sqrt{q}"/> which is distributed like <img alt="\gamma_3 N(0, \tfrac{6}{n}) = N(0, \tfrac{6\gamma_3^2 q}{n})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_3+N%280%2C+%5Ctfrac%7B6%7D%7Bn%7D%29+%3D+N%280%2C+%5Ctfrac%7B6%5Cgamma_3%5E2+q%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\gamma_3 N(0, \tfrac{6}{n}) = N(0, \tfrac{6\gamma_3^2 q}{n})"/>. For <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/> the contribution will be from the <img alt="3" class="latex" src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="3"/> terms involving <img alt="1,i" class="latex" src="https://s0.wp.com/latex.php?latex=1%2Ci&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1,i"/>, with each yielding a contribution of <img alt="2\sqrt{q}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bq%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2\sqrt{q}"/>, and hence the result will be distributed like <img alt="N(0,\tfrac{12 \gamma_3^2 q}{n})" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C%5Ctfrac%7B12+%5Cgamma_3%5E2+q%7D%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,\tfrac{12 \gamma_3^2 q}{n})"/>.</p>



<p>(More generally, for larger <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, the number of terms for distinct <img alt="i,j" class="latex" src="https://s0.wp.com/latex.php?latex=i%2Cj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i,j"/> is <img alt="p(p-1)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28p-1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(p-1)"/>, each contributing a Gaussian of standard deviation <img alt="(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Csqrt%7Bq%7D%29%5E%7Bp-2%7D%5Cgamma_p%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}"/>, while for <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/> we have <img alt="p(p-1)/2" class="latex" src="https://s0.wp.com/latex.php?latex=p%28p-1%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(p-1)/2"/> terms, each contributing a Gaussian of standard deviation <img alt="2(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}" class="latex" src="https://s0.wp.com/latex.php?latex=2%28%5Csqrt%7Bq%7D%29%5E%7Bp-2%7D%5Cgamma_p%2F%5Csqrt%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2(\sqrt{q})^{p-2}\gamma_p/\sqrt{n}"/>.)</p>



<p>Since the sum of Gaussians is a Gaussian we get that <img alt="\nabla^2J(q,0,\ldots,0){i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J%28q%2C0%2C%5Cldots%2C0%29%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2J(q,0,\ldots,0){i,j}"/> is distributed like a Gaussian with variance <img alt="\sum \gamma_p^2 p(p-1)q^{p-2}/n = \nu''(q)/n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+%5Cgamma_p%5E2+p%28p-1%29q%5E%7Bp-2%7D%2Fn+%3D+%5Cnu%27%27%28q%29%2Fn&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum \gamma_p^2 p(p-1)q^{p-2}/n = \nu''(q)/n"/> for <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/>, and twice that for <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/>. This means that the minimum eigenvalue of <img alt="\nabla^2J(q,0,\ldots,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5E2J%28q%2C0%2C%5Cldots%2C0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla^2J(q,0,\ldots,0)"/> equals <img alt="\sqrt{\nu''(q)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cnu%27%27%28q%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sqrt{\nu''(q)}"/> times the minimum eigenvalue of a random matrix <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M"/> from the Gaussian Orthogonal Ensemble (i.e. <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M"/> is sampled via <img alt="M{i,j} \sim N(0,1/n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%7Bi%2Cj%7D+%5Csim+N%280%2C1%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M{i,j} \sim N(0,1/n)"/>, <img alt="M_{i,i} \sim N(0,2/n)" class="latex" src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Ci%7D+%5Csim+N%280%2C2%2Fn%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M_{i,i} \sim N(0,2/n)"/>). As mentioned above, it is known that this minimum eigenvalue is <img alt="-2+o(1)" class="latex" src="https://s0.wp.com/latex.php?latex=-2%2Bo%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-2+o(1)"/>, and in fact by the semi-circle law, for every <img alt="\epsilon&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\epsilon&gt;0"/>, the number of eigenvalues of value <img alt="\leq -2+\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+-2%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\leq -2+\epsilon"/> is <img alt="\Omega(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Omega(n)"/>, and so we can also pick one that is orthogonal to the previous directions. QED</p>



<h2>Full replica symmetry breaking and ultra-metricity</h2>



<p>The point of this blog post is that at least in the “mixed <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> spin” case considered by Subag, we can understand what the algorithm does and the value that it achieves without needing to go into the theory of the geometry of the solution space, but let me briefly discuss some of this theory. (As I mentioned, I am still reading through this, and so this part should be read with big grains of salt.)</p>



<p>The key object studied in this line of work is the probability distribution <img alt="\xi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cxi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\xi"/> of the dot product <img alt="\langle x,x' \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x%2Cx%27+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle x,x' \rangle"/> for <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> and <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x'"/> sampled independently from the Gibbs distribution induced by <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/>. (This probability distribution will depend on the number of dimensions <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/>, but we consider the case that <img alt="n \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n \rightarrow \infty"/>.)</p>



<p>Intuitively, there are several ways this probability distribution can behave, depending on how the solution space is “clustered”:</p>



<ul><li>If all solutions are inside a single “cluster”, in the sense that they are all of the form <img alt="x = x_* + e" class="latex" src="https://s0.wp.com/latex.php?latex=x+%3D+x_%2A+%2B+e&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x = x_* + e"/> where <img alt="x_*" class="latex" src="https://s0.wp.com/latex.php?latex=x_%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_*"/> is the “center” of the cluster and <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="e"/> is some random vector, then <img alt="\langle x,x' \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x%2Cx%27+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle x,x' \rangle"/> will be concentrated on the point <img alt="\| x_*\|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7C+x_%2A%5C%7C%5E2&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\| x_*\|^2"/>.<br/></li><li>If the solutions are inside a finite number <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/> of clusters, with centers <img alt="x_1,\ldots,x_k" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_k"/>, then the support of the distribution will be on the <img alt="k^2" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k^2"/> points <img alt="\{ \langle x_i,x_j \rangle \}_{i,j \in [k]}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Clangle+x_i%2Cx_j+%5Crangle+%5C%7D_%7Bi%2Cj+%5Cin+%5Bk%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\{ \langle x_i,x_j \rangle \}_{i,j \in [k]}"/>.<br/></li><li>Suppose that the solutions are inside a <em>hierarchy</em> of clusters. That is, suppose we have some rooted tree <img alt="\mathcal{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{T}"/> (e.g., think of a depth <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> full binary tree), and we associate a vector <img alt="x_v" class="latex" src="https://s0.wp.com/latex.php?latex=x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_v"/> with every vertex <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/> of <img alt="\mathcal{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{T}"/>, with the property that <img alt="x_v" class="latex" src="https://s0.wp.com/latex.php?latex=x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_v"/> is orthogonal to all vectors associated with <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/>‘s ancestors on the tree. Now imagine that the distribution is obtained by taking a random leaf <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/> of <img alt="\mathcal{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{T}"/> and outputting <img alt="\sum x_v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+x_v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum x_v"/> for all vertices <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/> on the path from the root to <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u"/>. In such a case the dot product of <img alt="x_u" class="latex" src="https://s0.wp.com/latex.php?latex=x_u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_u"/> and <img alt="x_{u'}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bu%27%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_{u'}"/> will be <img alt="\sum_v \|x_v\|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_v+%5C%7Cx_v%5C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_v \|x_v\|^2"/> taken over all the common ancestors of <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v"/>. As the dimension and depth of the tree goes to infinity, the distribution over dot product can have continuous support, and it is this setting (specifically when the support is an interval <img alt="[0,q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5B0%2Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="[0,q)"/>) that is known as <em>full replica symmetry breaking</em>. Because the dot product is determined by common ancestor, for every three vectors <img alt="x_u,x_v,x_w" class="latex" src="https://s0.wp.com/latex.php?latex=x_u%2Cx_v%2Cx_w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_u,x_v,x_w"/> in the support of the distribution <img alt="\langle x_v,x_w \rangle \geq \min \{ \langle x_u,x_v \rangle, \langle x_u,x_w \rangle \}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x_v%2Cx_w+%5Crangle+%5Cgeq+%5Cmin+%5C%7B+%5Clangle+x_u%2Cx_v+%5Crangle%2C+%5Clangle+x_u%2Cx_w+%5Crangle+%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle x_v,x_w \rangle \geq \min \{ \langle x_u,x_v \rangle, \langle x_u,x_w \rangle \}"/> or <img alt="\| x_v - x_w \| \leq \max \{ \|x_u - x_v \|, \|x_u -x_w \|\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7C+x_v+-+x_w+%5C%7C+%5Cleq+%5Cmax+%5C%7B+%5C%7Cx_u+-+x_v+%5C%7C%2C+%5C%7Cx_u+-x_w+%5C%7C%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\| x_v - x_w \| \leq \max \{ \|x_u - x_v \|, \|x_u -x_w \|\}"/>. It is this condition that known as <em>ultra-metricity</em>.</li></ul>



<p>In Subag’s algorithm, as mentioned above, at any given step we could make an update of either <img alt="x^{t+1} \leftarrow x^t - \eta u" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+-+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{t+1} \leftarrow x^t - \eta u"/> or <img alt="x^{t+1} \leftarrow x^t + \eta u" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7Bt%2B1%7D+%5Cleftarrow+x%5Et+%2B+%5Ceta+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^{t+1} \leftarrow x^t + \eta u"/>. If we think of all the possible choices for the signs in the <img alt="d=1/\eta^2" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D1%2F%5Ceta%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d=1/\eta^2"/> of the algorithms, we see that the algorithm does not only produce a single vector but actually <img alt="2^d" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2^d"/> such vectors that are arranged in an ultrametric tree just as above. Indeed, this ultrametric structure was the inspiration for the algorithm and is the reason why the algorithm produces the correct result precisely in the full replica symmetry breaking regime.</p>



<p><strong>Acknowledgements:</strong> Thanks to Tselil Schramm for helpful comments.</p></div>
    </content>
    <updated>2020-10-23T19:01:17Z</updated>
    <published>2020-10-23T19:01:17Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-11-02T23:34:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/</id>
    <link href="https://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/" rel="alternate" type="text/html"/>
    <title>Tenure-Track Assistant Professor at Rutgers University (apply by January 15, 2021)</title>
    <summary>The Computer Science Department at Rutgers University invites applications for a Tenure-Track Assistant Professor position in Theoretical Computer Science. We welcome candidates working on computational complexity theory but outstanding applicants in all areas of TCS will be considered. Website: http://jobs.rutgers.edu/postings/120527 Email: martin@farach-colton.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science Department at Rutgers University invites applications for a Tenure-Track Assistant Professor position in Theoretical Computer Science. We welcome candidates working on computational complexity theory but outstanding applicants in all areas of TCS will be considered.</p>
<p>Website: <a href="http://jobs.rutgers.edu/postings/120527">http://jobs.rutgers.edu/postings/120527</a><br/>
Email: martin@farach-colton.com</p></div>
    </content>
    <updated>2020-10-22T18:10:24Z</updated>
    <published>2020-10-22T18:10:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-02T23:34:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/" rel="alternate" type="text/html"/>
    <title>Professorship/Chair at TU Hamburg (apply by November 29, 2020)</title>
    <summary>TU Hamburg invites applications for a full professorship (chair) in Hardware-aware Combinatorial Optimization, at its School of Electrical Engineering, Computer Science and Mathematics. Our goal is to establish a group that excels at developing and implementing state-of-the-art optimization techniques on modern computing architectures at hardware level. The chair is endowed by Fujitsu. Website: https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1 Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>TU Hamburg invites applications for a full professorship (chair) in Hardware-aware Combinatorial Optimization, at its School of Electrical Engineering, Computer Science and Mathematics. Our goal is to establish a group that excels at developing and implementing state-of-the-art optimization techniques on modern computing architectures at hardware level. The chair is endowed by Fujitsu.</p>
<p>Website: <a href="https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1">https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1</a><br/>
Email: berufungen@tuhh.de</p></div>
    </content>
    <updated>2020-10-22T16:13:33Z</updated>
    <published>2020-10-22T16:13:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-11-02T23:34:16Z</updated>
    </source>
  </entry>
</feed>
