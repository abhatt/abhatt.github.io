<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-11-21T00:21:39Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08376</id>
    <link href="http://arxiv.org/abs/1911.08376" rel="alternate" type="text/html"/>
    <title>Extending General Compact Querieable Representations to GIS Applications</title>
    <feedworld_mtime>1574294400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brisaboa:Nieves_R=.html">Nieves R. Brisaboa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cerdeira=Pena:Ana.html">Ana Cerdeira-Pena</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bernardo:Guillermo_de.html">Guillermo de Bernardo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pedreira:Oscar.html">Oscar Pedreira</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08376">PDF</a><br/><b>Abstract: </b>The raster model is commonly used for the representation of images in many
domains, and is especially useful in Geographic Information Systems (GIS) to
store information about continuous variables of the space (elevation,
temperature, etc.). Current representations of raster data are usually designed
for external memory or, when stored in main memory, lack efficient query
capabilities. In this paper we propose compact representations to efficiently
store and query raster datasets in main memory. We present different
representations for binary raster data, general raster data and time-evolving
raster data. We experimentally compare our proposals with traditional storage
mechanisms such as linear quadtrees or compressed GeoTIFF files. Results show
that our structures are up to 10 times smaller than classical linear quadtrees,
and even comparable in space to non-querieable representations of raster data,
while efficiently answering a number of typical queries.
</p></div>
    </summary>
    <updated>2019-11-21T00:06:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08374</id>
    <link href="http://arxiv.org/abs/1911.08374" rel="alternate" type="text/html"/>
    <title>Concurrent Expandable AMQs on the Basis of Quotient Filters</title>
    <feedworld_mtime>1574294400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maier:Tobias.html">Tobias Maier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sanders:Peter.html">Peter Sanders</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williger:Robert.html">Robert Williger</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08374">PDF</a><br/><b>Abstract: </b>A quotient filter is a cache efficient AMQ data structure. Depending on the
fill degree of the filter most insertions and queries only need to access one
or two consecutive cache lines. This makes quotient filters fast compared to
the more commonly used Bloom filters that incur multiple cache misses. However,
concurrent Bloom filters are easy to implement and can be implemented lock-free
while concurrent quotient filters are not as simple. Usually concurrent
quotient filters work by using an external array of locks -- each protecting a
region of the table. Accessing this array incurs one additional cache miss per
operation. We propose a new locking scheme that has no memory overhead. Using
this new locking scheme we achieve 1.8 times higher speedups than with the
common external locking scheme.
</p>
<p>Another advantage of quotient filters over Bloom filters is that a quotient
filter can change its size when it is becoming full. We implement this growing
technique for our concurrent quotient filters and adapt it in a way that allows
unbounded growing while keeping a bounded false positive rate. We call the
resulting data structure a fully expandable quotient filter. Its design is
similar to scalable Bloom filters, but we exploit some concepts inherent to
quotient filters to improve the space efficiency and the query speed.
</p>
<p>We also propose quotient filter variants that are aimed to reduce the number
of status bits (2-status-bit variant) or to simplify concurrent implementations
(linear probing quotient filter). The linear probing quotient filter even leads
to a lock-free concurrent filter implementation. This is especially
interesting, since we show that any lock-free implementation of another common
quotient filter variant would incur significant overheads in the form of
additional data fields or multiple passes over the accessed data.
</p></div>
    </summary>
    <updated>2019-11-21T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08130</id>
    <link href="http://arxiv.org/abs/1911.08130" rel="alternate" type="text/html"/>
    <title>Topological computing of arrangements with (co)chains</title>
    <feedworld_mtime>1574294400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paoluzzi:Alberto.html">Alberto Paoluzzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shapiro:Vadim.html">Vadim Shapiro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/DiCarlo:Antonio.html">Antonio DiCarlo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Furiani:Francesco.html">Francesco Furiani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martella:Giulio.html">Giulio Martella</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scorzelli:Giorgio.html">Giorgio Scorzelli</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08130">PDF</a><br/><b>Abstract: </b>In many areas of applied geometric/numeric computational mathematics,
including geo-mapping, computer vision, computer graphics, finite element
analysis, medical imaging, geometric design, and solid modeling, one has to
compute incidences, adjacencies and ordering of cells, generally using
disparate and often incompatible data structures and algorithms. This paper
introduces computational topology algorithms to discover the 2D/3D space
partition induced by a collection of geometric objects of dimension 1D/2D,
respectively. Methods and language are those of basic geometric and algebraic
topology. Only sparse vectors and matrices are used to compute both spaces and
maps, i.e., the chain complex, from dimension zero to three.
</p></div>
    </summary>
    <updated>2019-11-21T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07976</id>
    <link href="http://arxiv.org/abs/1911.07976" rel="alternate" type="text/html"/>
    <title>Estimating Entropy of Distributions in Constant Space</title>
    <feedworld_mtime>1574294400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Acharya:Jayadev.html">Jayadev Acharya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhadane:Sourbh.html">Sourbh Bhadane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Ziteng.html">Ziteng Sun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07976">PDF</a><br/><b>Abstract: </b>We consider the task of estimating the entropy of $k$-ary distributions from
samples in the streaming model, where space is limited. Our main contribution
is an algorithm that requires $O\left(\frac{k \log
(1/\varepsilon)^2}{\varepsilon^3}\right)$ samples and a constant $O(1)$ memory
words of space and outputs a $\pm\varepsilon$ estimate of $H(p)$. Without space
limitations, the sample complexity has been established as
$S(k,\varepsilon)=\Theta\left(\frac k{\varepsilon\log k}+\frac{\log^2
k}{\varepsilon^2}\right)$, which is sub-linear in the domain size $k$, and the
current algorithms that achieve optimal sample complexity also require
nearly-linear space in $k$.
</p>
<p>Our algorithm partitions $[0,1]$ into intervals and estimates the entropy
contribution of probability values in each interval. The intervals are designed
to trade off the bias and variance of these estimates.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1805.00506</id>
    <link href="http://arxiv.org/abs/1805.00506" rel="alternate" type="text/html"/>
    <title>Adaptive View Planning for Aerial 3D Reconstruction</title>
    <feedworld_mtime>1574294400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Cheng.html">Cheng Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Isler:Volkan.html">Volkan Isler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1805.00506">PDF</a><br/><b>Abstract: </b>With the proliferation of small aerial vehicles, acquiring close up aerial
imagery for high quality reconstruction of complex scenes is gaining
importance. We present an adaptive view planning method to collect such images
in an automated fashion. We start by sampling a small set of views to build a
coarse proxy to the scene. We then present (i)~a method that builds a view
manifold for view selection, and (ii) an algorithm to select a sparse set of
views. The vehicle then visits these viewpoints to cover the scene, and the
procedure is repeated until reconstruction quality converges or a desired level
of quality is achieved. The view manifold provides an effective
efficiency/quality compromise between using the entire 6 degree of freedom pose
space and using a single view hemisphere to select the views.
</p>
<p>Our results show that, in contrast to existing "explore and exploit" methods
which collect only two sets of views, reconstruction quality can be drastically
improved by adding a third set. They also indicate that three rounds of data
collection is sufficient even for very complex scenes. We compare our algorithm
to existing methods in three challenging scenes. We require each algorithm to
select the same number of views. Our algorithm generates views which produce
the least reconstruction error.
</p></div>
    </summary>
    <updated>2019-11-21T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/20/lecturer-assistant-director-undergraduate-studies-at-harvard-university-apply-by-january-1-2020/</id>
    <link href="https://cstheory-jobs.org/2019/11/20/lecturer-assistant-director-undergraduate-studies-at-harvard-university-apply-by-january-1-2020/" rel="alternate" type="text/html"/>
    <title>Lecturer / Assistant Director Undergraduate Studies at Harvard University (apply by January 1, 2020)</title>
    <summary>Harvard University seeks qualified candidates for the position of Assistant Director for Undergraduate Studies, with a concurrent Lectureship, in Computer Science. Responsibilities include teaching or co-teaching one one undergraduate Computer Science course per semester as well as hepling lead the Computer Science Undergraduate Advising team with faculty and staff. (see links for full info) Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Harvard University seeks qualified candidates for the position of Assistant Director for Undergraduate Studies, with a concurrent Lectureship, in Computer Science. Responsibilities include teaching or co-teaching one one undergraduate Computer Science course per semester as well as hepling lead the Computer Science Undergraduate Advising team with faculty and staff. (see links for full info)</p>
<p>Website: <a href="http://tiny.cc/harvardadus">http://tiny.cc/harvardadus</a><br/>
Email: cs-dus@seas.harvard.edu</p></div>
    </content>
    <updated>2019-11-20T19:58:34Z</updated>
    <published>2019-11-20T19:58:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-21T00:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/166</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/166" rel="alternate" type="text/html"/>
    <title>TR19-166 |  Top-down induction of decision trees: rigorous guarantees and inherent limitations | 

	Guy Blanc, 

	Jane Lange, 

	Li-Yang Tan</title>
    <summary>Consider the following heuristic for building a decision tree for a function $f : \{0,1\}^n \to \{\pm 1\}$.  Place the most influential variable $x_i$ of $f$ at the root, and recurse on the subfunctions $f_{x_i=0}$ and $f_{x_i=1}$ on the left and right subtrees respectively; terminate once the tree is an $\varepsilon$-approximation of $f$.   We analyze the quality of this heuristic, obtaining near-matching upper and lower bounds:  

$\circ$ Upper bound: For every $f$ with decision tree size $s$ and every $\varepsilon \in (0,\frac1{2})$, this heuristic builds a decision tree of size at most $s^{O(\log(s/\varepsilon)\log(1/\varepsilon))}$. 

$\circ$ Lower bound: For every $\varepsilon \in (0,\frac1{2})$ and $s \le 2^{\tilde{O}(\sqrt{n})}$, there is an $f$ with decision tree size $s$ such that this heuristic builds a decision tree of size $s^{\tilde{\Omega}(\log s)}$. 

We also obtain upper and lower bounds for monotone functions: $s^{O(\sqrt{\log s}/\varepsilon)}$ and $s^{\tilde{\Omega}(\sqrt[4]{\log s }
)}$ respectively.  The lower bound disproves conjectures of Fiat and Pechyony (2004) and Lee (2009).

Our upper bounds yield new algorithms for properly learning decision trees under the uniform distribution.  We show that these algorithms---which are motivated by widely employed and empirically successful top-down decision tree learning heuristics such as ID3, C4.5, and CART---achieve provable guarantees that compare favorably with those of the current fastest algorithm (Ehrenfeucht and Haussler, 1989), and even have certain qualitative advantages. Our lower bounds shed new light on the limitations of these heuristics. 

Finally, we revisit the classic work of Ehrenfeucht and Haussler.  We extend it to give the first uniform-distribution proper learning algorithm that achieves polynomial sample and memory complexity, while matching its state-of-the-art quasipolynomial runtime.</summary>
    <updated>2019-11-20T19:06:43Z</updated>
    <published>2019-11-20T19:06:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-21T00:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/20/postdoctoral-fellow-at-the-university-of-texas-at-austin-apply-by-january-15-2020-2/</id>
    <link href="https://cstheory-jobs.org/2019/11/20/postdoctoral-fellow-at-the-university-of-texas-at-austin-apply-by-january-15-2020-2/" rel="alternate" type="text/html"/>
    <title>Postdoctoral Fellow at The University of Texas at Austin (apply by January 15, 2020)</title>
    <summary>The Computer Science Department at UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2020-21 academic year. The Fellow will work with Dana Moshkovitz and David Zuckerman on pseudorandomness and computational complexity. Review of applicants will begin on January 15, but applications will be accepted until the position is filled. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science Department at UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2020-21 academic year. The Fellow will work with Dana Moshkovitz and David Zuckerman on pseudorandomness and computational complexity. Review of applicants will begin on January 15, but applications will be accepted until the position is filled.</p>
<p>Website: <a href="https://www.myworkday.com/utaustin/d/inst/15$158872/9925$9054.htmld">https://www.myworkday.com/utaustin/d/inst/15$158872/9925$9054.htmld</a><br/>
Email: maguilar@cs.utexas.edu</p></div>
    </content>
    <updated>2019-11-20T17:45:16Z</updated>
    <published>2019-11-20T17:45:16Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-21T00:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/20/postdoctoral-fellow-at-the-university-of-texas-at-austin-apply-by-january-15-2020/</id>
    <link href="https://cstheory-jobs.org/2019/11/20/postdoctoral-fellow-at-the-university-of-texas-at-austin-apply-by-january-15-2020/" rel="alternate" type="text/html"/>
    <title>Postdoctoral Fellow at The University of Texas at Austin (apply by January 15, 2020)</title>
    <summary>The Computer Science Department at UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2020-21 academic year. The Fellow will work with Dana Moshkovitz and David Zuckerman on pseudorandomness and computational complexity. Review of applicants will begin on January 15, but applications will be accepted until the position is filled. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science Department at UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2020-21 academic year. The Fellow will work with Dana Moshkovitz and David Zuckerman on pseudorandomness and computational complexity. Review of applicants will begin on January 15, but applications will be accepted until the position is filled.</p>
<p>Website: <a href="https://www.myworkday.com/utaustin/d/inst/15$158872/9925$9054.htmld">https://www.myworkday.com/utaustin/d/inst/15$158872/9925$9054.htmld</a><br/>
Email: maguilar@cs.utexas.edu</p></div>
    </content>
    <updated>2019-11-20T17:45:06Z</updated>
    <published>2019-11-20T17:45:06Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-21T00:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3445</id>
    <link href="https://agtb.wordpress.com/2019/11/20/test-of-time-award-call-for-nominations/" rel="alternate" type="text/html"/>
    <title>Test of time award: call for nominations</title>
    <summary>Please nominate for the SIGecom Test of Time Award. The SIGecom Test of Time Award recognizes the author or authors of an influential paper or series of papers published between ten and twenty-five years ago that has significantly impacted research or applications exemplifying the interplay of economics and computation. To be eligible, a paper or […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="font-weight: 400;">Please nominate for the SIGecom Test of Time Award.</p>
<p style="font-weight: 400;">The SIGecom Test of Time Award recognizes the author or authors of an influential paper or series of papers published between ten and twenty-five years ago that has significantly impacted research or applications exemplifying the interplay of economics and computation.</p>
<p style="font-weight: 400;">To be eligible, a paper or series of papers must be on a topic in the intersection of economics and computation, and must have been first published, in preliminary or final form, in an archival journal or conference proceedings no less than ten years and no more than twenty-five years before the year the award is conferred. Papers for which all authors are deceased at the time the Award Committee makes its decision are not eligible for the award.</p>
<p style="font-weight: 400;">The 2020 SIGecom Test of Time Award will be given for papers published no earlier than 1995 and no later than 2010. <strong>Nominations are due by February 29th, 2020</strong>, and must be made by email to the Award Committee with “2020 ACM SIGecom Test of Time Award” in the subject.</p>
<p style="font-weight: 400;">See details at <a href="https://www.sigecom.org/awardt.html">https://www.sigecom.org/awardt.html</a></p>
<p> </p>
<p style="font-weight: 400;">The 2020 Test of Time Award Committee</p>
<p style="font-weight: 400;">Paul Milgrom, Stanford University</p>
<p style="font-weight: 400;">Noam Nisan, The Hebrew University of Jerusalem</p>
<p style="font-weight: 400;">Éva Tardos (chair), Cornell University</p></div>
    </content>
    <updated>2019-11-20T07:55:55Z</updated>
    <published>2019-11-20T07:55:55Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>algorithmicgametheory</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-11-21T00:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16399</id>
    <link href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/" rel="alternate" type="text/html"/>
    <title>A Clever Way To Find Compiler Bugs</title>
    <summary>Your comments are valuable, we thank you. source Xuejun Yang is a Senior Staff Engineer at FutureWei Technologies. He is the DFA on the 2011 paper, “Finding and Understanding Bugs in C Compilers.” Today Ken and I discuss a clever idea from that paper. The paper was brought to our attention just now in a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Your comments are valuable, we thank you.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/profile_image174/" rel="attachment wp-att-16401"><img alt="" class="alignright  wp-image-16401" src="https://rjlipton.files.wordpress.com/2019/11/profile_image174.jpg?w=220" width="220"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.flux.utah.edu/profile/jxyang">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Xuejun Yang is a Senior Staff Engineer at FutureWei Technologies. He is the DFA on the 2011 <a href="http://www.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf">paper</a>, “Finding and Understanding Bugs in C Compilers.”</p>
<p>
Today Ken and I discuss a clever idea from that paper.</p>
<p>
The paper was brought to our attention just now in a meaty <a href="https://rjlipton.wordpress.com/2019/10/21/a-polemical-overreach/#comment-106276">comment</a> by Paul D. We thank him for it—the topic interests both of us. We don’t think Paul D. means to be anonymous, but in keeping with that we’ll give just a cryptic hint to his identity: The saying “a man on the make” is widely known, but for more than the millennium he has been the unique person in the world to whom it applies literally.  <b>Update 11/20</b>: Turns out we (I, Ken) were wrong about the identity, see <a href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/#comment-106348">this</a>.</p>
<p>
Yang was made unique by being listed out of alphabetical order on the paper. This is notable because the most common practice in our field is to list alphabetically irrespective of prominence. Hence we’ve invented the term ‘DFA’ for “Designated” or “Distinguished” First Author. The other authors are Yang Chen, Eric Eide, and John Regehr, all from the University of Utah. </p>
<p>
</p><p/><h2> The Topic </h2><p/>
<p/><p>
Paul D.’s comment notes that there was evidence that verification methods could improve compiler correctness. By <i>compiler</i> we mean the program that transforms high level code into machine code. These programs are used countless times every day and their correctness is clearly very important. </p>
<p>
Their correctness is tricky for several reasons. The main one is that almost all compilers try to optimize code. That is when they transform code into instructions they try to rewrite or rearrange the instructions to yield better performance. Compilers have been doing this forever. The trouble is that changing instructions to increase performance is dangerous. The changes must not affect the values that are computed. If they are not done carefully they can actually make the answers faster, but incorrect. This is the reason correctness is tricky.</p>
<p>
Formal verification requires a lot of effort. The highest effort should go into mission-critical software. But compilers are mission-critical <em>already</em>, unless we know mission-critical software won’t be compiled on a particular one. Hence it is notable when formal verification makes a compiler more reliable. </p>
<p>
</p><p/><h2> The Paper </h2><p/>
<p/><p>
The idea in the paper Paul referenced is quite elegant. They built a program called Csmith. It operates as follows: </p>
<blockquote><p><b> </b> <em> Suppose that <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X}"/> is a compiler they wish to test. Then generate various legal C programs <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{P}"/>. For each of these let <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> be the answer that <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X(P)}"/> yields. Here <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X(P)}"/> is the compiled program. Then check whether <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is correct. </em>
</p></blockquote>
<p/><p>
For example:  </p>
<pre>int foo (void) { 
    signed char x = 1; 
    unsigned char y = 255; 
    return x &gt; y; 
} 
</pre>
<p>
Some compilers returned <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, but the correct answer is <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. There are further examples in a 2012 companion <a href="https://www.cs.utah.edu/~regehr/papers/pldi12-preprint.pdf">paper</a> and these <a href="https://www.flux.utah.edu/download?uid=115&amp;slides=1&amp;type=pptx">slides</a> from an earlier version. The Csmith <a href="https://embed.cs.utah.edu/csmith/">homepage</a> has long lists of compiler bugs they found. </p>
<p>
Of course if <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X(P)}"/> crashes or refuse to compile <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> then the compiler is wrong. But what happens if <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is computed. How does Csmith know if the answer is correct? This seems to be really hard. This correctness testing must be automated: the whole approach is based on allowing tons of random programs to be tested. They cannot assume that humans will be used to check the outputs.</p>
<p>
This is the clever idea of this paper. They assume that there are at least two compilers say <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> and <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/>. Then let <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> be the output of <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X(P)}"/> and let <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> be the output of <img alt="{Y(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y(P)}"/>. The key insight is: </p>
<blockquote><p>
<b>If <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is not equal to <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B}"/>, then one of the compilers is wrong</b>.
</p></blockquote>
<p>
A very neat and elegant idea. For software in general it is called <a href="https://en.wikipedia.org/wiki/Differential_testing">differential</a> <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.83.445">testing</a>. </p>
<p>
This at least alerts when there are problems with some compilers and some programs. One can use this trick to discover programs that cause at least some compilers to have problems. This is extremely valuable. It allowed Csmith to discover hundreds of errors in production compilers—errors that previously were missed.</p>
<p>
</p><p/><h2> Smart Fuzzing </h2><p/>
<p/><p>
<a href="https://en.wikipedia.org/wiki/Fuzzing">Fuzzing</a> is defined by Wikipedia as testing by “providing invalid, unexpected, or random data as inputs to a computer program.” An early historical example, Apple’s “<a href="https://en.wikipedia.org/wiki/Monkey_testing">Monkey</a>” program, worked completely randomly. To ensure that the found bugs are <em>meaningful</em> and <em>analyzable</em>, Csmith needed a deeper, structured, “intelligent” design, not just the generation of <a href="https://en.wikipedia.org/wiki/Mayhem_(advertising_character)">Mayhem</a>.</p>
<p>
For one, Csmith needed to avoid programs <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> than do not have deterministic behavior. The formal C standards itemize cases in which compilers are allowed to have arbitrary, even self-inconsistent, behavior. There are lots of them in C. A bug with dubious code could be dismissed out of hand.</p>
<p>
For another, the probability that a program <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> built haphazardly by the original Csmith version would reveal bugs was observed to peak at about 80KB source-code size, about 1,000 lines across multiple pages. Those don’t make great examples. So Csmith has its own routines to compress bug instances it has found. Simple tricks are shortening numerical expressions to use only the bug-sensitive parts. Others are lifting local variables out of blocks and bypassing pointer jumps.</p>
<p>
A third goal is that the generator should branch out to all aspects of the language—in this case, C—not just the “grungy” parts that are ripe for finding compiler bugs. The paper talks about this at length. Regehr, who was Yang’s advisor, is also a blogger. His current <a href="https://blog.regehr.org/archives/1700">post</a>, dated November 4, is titled, “Helping Generative Fuzzers Avoid Looking Only Where the Light is Good, Part 1.” We guess that “Part 2” will go even more into details.</p>
<p>
</p><p/><h2> Formal Methods as Bugscreen </h2><p/>
<p/><p>
Regarding the formally-verified CompCert compiler, Paul D. quoted from the <a href="http://www.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf">paper</a>:</p>
<blockquote><p><b> </b> <em> The striking thing about our CompCert results is that the middle-end bugs we found in all other compilers are absent. As of early 2011, the under-development version of CompCert is the only compiler we have tested for which Csmith cannot find wrong-code errors. This is not for lack of trying: we have devoted about six CPU-years to the task. The apparent unbreakability of CompCert supports a strong argument that developing compiler optimizations within a proof framework, where safety checks are explicit and machine-checked, has tangible benefits for compiler users. </em>
</p></blockquote>
<p/><p>
This August 2019 <a href="https://arxiv.org/pdf/1902.09334.pdf">paper</a> by Michaël Marcozzi, Qiyi Tang, Alastair Donaldson, and Cristian Cadar gives recent results involving Csmith and other tools. They have an interesting discussion on page 2, from which we excerpt:</p>
<blockquote><p><b> </b> <em> In our experience working in the area […], we have found compiler fuzzing to be a contentious topic. Research talks on compiler fuzzing are often followed by questions about the importance of the discovered bugs, and whether compiler fuzzers might be improved by taking inspiration from bugs encountered by users of compilers “in the wild.” Some … argue that any miscompilation bug, whether fuzzer-found or not, is a ticking bomb that should be regarded as severe, or avoided completely via formal verification (in the spirit of CompCert). </em>
</p></blockquote>
<p/><p>
They go on to say, however, that when a fully-developed compiler is used for non-critical software, the kinds of bugs typically found by fuzzing tend to have questionable importance. Their paper is titled, “A Systematic Impact Study for Fuzzer-Found Compiler Bugs.” </p>
<p>
So far they have found definite results that seem to have mixed implications. In their future-work section they note that they have evaluated the impact of bugs in compilers on the intended function of programs they compile, but not on possible security holes—which as we noted in our Cloudflare <a href="https://rjlipton.wordpress.com/2017/03/08/is-computer-security-possible/">post</a> can come from (misuse of) simple code that is completely correct. This leads us further to wonder, coming full-circle, whether formal methods might help quantify the relative importance of aspects of a language and areas of a compiler to guide more-intelligent generation of test cases.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>
The above comment is interesting, but perhaps finding obscure bugs is important. Perhaps such bugs could be used to attack systems. That is perhaps some one could use them to break into a system. Security may be compromised by any error, even an unlikely one to occur in the wild. </p>
<p>
What do you think?</p>
<p/></font></font></div>
    </content>
    <updated>2019-11-20T00:57:14Z</updated>
    <published>2019-11-20T00:57:14Z</published>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Results"/>
    <category term="bugs"/>
    <category term="compiler"/>
    <category term="errors"/>
    <category term="security"/>
    <category term="testing"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-11-21T00:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08372</id>
    <link href="http://arxiv.org/abs/1911.08372" rel="alternate" type="text/html"/>
    <title>Improved Compressed String Dictionaries</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brisaboa:Nieves_R=.html">Nieves R. Brisaboa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cerdeira=Pena:Ana.html">Ana Cerdeira-Pena</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bernardo:Guillermo_de.html">Guillermo de Bernardo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08372">PDF</a><br/><b>Abstract: </b>We introduce a new family of compressed data structures to efficiently store
and query large string dictionaries in main memory. Our main technique is a
combination of hierarchical Front-coding with ideas from longest-common-prefix
computation in suffix arrays. Our data structures yield relevant space-time
tradeoffs in real-world dictionaries. We focus on two domains where string
dictionaries are extensively used and efficient compression is required: URL
collections, a key element in Web graphs and applications such as Web mining;
and collections of URIs and literals, the basic components of RDF datasets. Our
experiments show that our data structures achieve better compression than the
state-of-the-art alternatives while providing very competitive query times.
</p></div>
    </summary>
    <updated>2019-11-20T23:54:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08339</id>
    <link href="http://arxiv.org/abs/1911.08339" rel="alternate" type="text/html"/>
    <title>The Power of Factorization Mechanisms in Local and Central Differential Privacy</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alexander Edmonds, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nikolov:Aleksandar.html">Aleksandar Nikolov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Ullman:Jonathan.html">Jonathan Ullman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08339">PDF</a><br/><b>Abstract: </b>We give new characterizations of the sample complexity of answering linear
queries (statistical queries) in the local and central models of differential
privacy:
</p>
<p>*In the non-interactive local model, we give the first approximate
characterization of the sample complexity. Informally our bounds are tight to
within polylogarithmic factors in the number of queries and desired accuracy.
Our characterization extends to agnostic learning in the local model.
</p>
<p>*In the central model, we give a characterization of the sample complexity in
the high-accuracy regime that is analogous to that of Nikolov, Talwar, and
Zhang (STOC 2013), but is both quantitatively tighter and has a dramatically
simpler proof.
</p>
<p>Our lower bounds apply equally to the empirical and population estimation
problems. In both cases, our characterizations show that a particular
factorization mechanism is approximately optimal, and the optimal sample
complexity is bounded from above and below by well studied factorization norms
of a matrix associated with the queries.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08320</id>
    <link href="http://arxiv.org/abs/1911.08320" rel="alternate" type="text/html"/>
    <title>Property Testing of LP-Type Problems</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Rogers Epstein, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silwal:Sandeep.html">Sandeep Silwal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08320">PDF</a><br/><b>Abstract: </b>Given query access to a set of constraints $S$, we wish to quickly check if
some objective function $\varphi$ subject to these constraints is at most a
given value $k$. We approach this problem using the framework of property
testing where our goal is to distinguish the case $\varphi(S) \le k$ from the
case that at least an $\epsilon$ fraction of the constraints in $S$ need to be
removed for $\varphi(S) \le k$ to hold. We restrict our attention to the case
where $(S, \varphi)$ are LP-Type problems which is a rich family of
combinatorial optimization problems with an inherent geometric structure. By
utilizing a simple sampling procedure which has been used previously to study
these problems, we are able to create property testers for any LP-Type problem
whose query complexities are independent of the number of constraints. To the
best of our knowledge, this is the first work that connects the area of LP-Type
problems and property testing in a systematic way. Among our results is a tight
upper bound on the query complexity of testing clusterability with one cluster
considered by Alon, Dar, Parnas, and Ron (FOCS 2000). We also supply a
corresponding tight lower bound for this problem and other LP-Type problems
using geometric constructions.
</p></div>
    </summary>
    <updated>2019-11-20T23:28:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08297</id>
    <link href="http://arxiv.org/abs/1911.08297" rel="alternate" type="text/html"/>
    <title>Beyond Natural Proofs: Hardness Magnification and Locality</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Lijie.html">Lijie Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hirahara:Shuichi.html">Shuichi Hirahara</a>, Igor C. Oliveira, Jan Pich, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rajgopal:Ninad.html">Ninad Rajgopal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Santhanam:Rahul.html">Rahul Santhanam</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08297">PDF</a><br/><b>Abstract: </b>Hardness magnification reduces major complexity separations (such as
$\mathsf{\mathsf{EXP}} \nsubseteq \mathsf{NC}^1$) to proving lower bounds for
some natural problem $Q$ against weak circuit models. Several recent works
[OS18, MMW19, CT19, OPS19, CMMW19, Oli19, CJW19a] have established results of
this form. In the most intriguing cases, the required lower bound is known for
problems that appear to be significantly easier than $Q$, while $Q$ itself is
susceptible to lower bounds but these are not yet sufficient for magnification.
</p>
<p>In this work, we provide more examples of this phenomenon, and investigate
the prospects of proving new lower bounds using this approach. In particular,
we consider the following essential questions associated with the hardness
magnification program:
</p>
<p>Does hardness magnification avoid the natural proofs barrier of Razborov and
Rudich [RR97]?
</p>
<p>Can we adapt known lower bound techniques to establish the desired lower
bound for $Q$?
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08275</id>
    <link href="http://arxiv.org/abs/1911.08275" rel="alternate" type="text/html"/>
    <title>Corrfunc: Blazing fast correlation functions with AVX512F SIMD Intrinsics</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Manodeep Sinha, Lehman H. Garrison <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08275">PDF</a><br/><b>Abstract: </b>Correlation functions are widely used in extra-galactic astrophysics to
extract insights into how galaxies occupy dark matter halos and in cosmology to
place stringent constraints on cosmological parameters. A correlation function
fundamentally requires computing pair-wise separations between two sets of
points and then computing a histogram of the separations. Corrfunc is an
existing open-source, high-performance software package for efficiently
computing a multitude of correlation functions. In this paper, we will discuss
the SIMD AVX512F kernels within Corrfunc, capable of processing 16 floats or 8
doubles at a time. The latest manually implemented Corrfunc AVX512F kernels
show a speedup of up to $\sim 4\times$ relative to compiler-generated code for
double-precision calculations. The AVX512F kernels show $\sim 1.6\times$
speedup relative to the AVX kernels and compare favorably to a theoretical
maximum of $2\times$. In addition, by pruning pairs with too large of a minimum
possible separation, we achieve a $\sim 5-10\%$ speedup across all the SIMD
kernels. Such speedups highlight the importance of programming explicitly with
SIMD vector intrinsics for complex calculations that can not be efficiently
vectorized by compilers. Corrfunc is publicly available at
https://github.com/manodeep/Corrfunc/.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08101</id>
    <link href="http://arxiv.org/abs/1911.08101" rel="alternate" type="text/html"/>
    <title>Two-message verification of quantum computation</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alagic:Gorjan.html">Gorjan Alagic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Childs:Andrew_M=.html">Andrew M. Childs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hung:Shih=Han.html">Shih-Han Hung</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08101">PDF</a><br/><b>Abstract: </b>We describe a two-message protocol that enables a purely classical verifier
to delegate any quantum computation to an untrusted quantum prover. The
protocol begins with the verifier publishing a problem instance together with a
public cryptographic key. The prover then transmits the computation result,
appropriately encoded. Finally, the verifier uses their private key to detect
any cheating and extract the result.
</p>
<p>We achieve this by upgrading the verification protocol of Mahadev in two
steps. First, the protocol is repeated many times in parallel, yielding a
four-message protocol with negligible soundness error. This enables the second
step: the "challenge round" is eliminated via the Fiat-Shamir transform, in
which the prover computes their own challenges using a public hash function.
</p>
<p>We show that this protocol is secure under the same assumptions underlying
many candidate schemes for post-quantum public-key cryptography. Specifically,
it is secure in the Quantum Random Oracle Model, and assuming the quantum
hardness of the Learning with Errors problem. The main technical advance in our
security proof is a parallel repetition theorem for the Mahadev protocol.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08085</id>
    <link href="http://arxiv.org/abs/1911.08085" rel="alternate" type="text/html"/>
    <title>Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diakonikolas:Ilias.html">Ilias Diakonikolas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karmalkar:Sushrut.html">Sushrut Karmalkar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel.html">Daniel Kane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Price:Eric.html">Eric Price</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stewart:Alistair.html">Alistair Stewart</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08085">PDF</a><br/><b>Abstract: </b>We study high-dimensional sparse estimation tasks in a robust setting where a
constant fraction of the dataset is adversarially corrupted. Specifically, we
focus on the fundamental problems of robust sparse mean estimation and robust
sparse PCA. We give the first practically viable robust estimators for these
problems. In more detail, our algorithms are sample and computationally
efficient and achieve near-optimal robustness guarantees. In contrast to prior
provable algorithms which relied on the ellipsoid method, our algorithms use
spectral techniques to iteratively remove outliers from the dataset. Our
experimental evaluation on synthetic data shows that our algorithms are
scalable and significantly outperform a range of previous approaches, nearly
matching the best error rate without corruptions.
</p></div>
    </summary>
    <updated>2019-11-20T23:24:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08043</id>
    <link href="http://arxiv.org/abs/1911.08043" rel="alternate" type="text/html"/>
    <title>Mapping NP-hard and NP-complete optimisation problems to Quadratic Unconstrained Binary Optimisation problems</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Bas Lodewijks <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08043">PDF</a><br/><b>Abstract: </b>We discuss several mappings from well-known NP-hard problems to Quadratic
Unconstrained Binary Optimisation problems which are treated incorrectly by
Lucas in \cite{Luc14}. We provide counterexamples and correct the mappings. We
also extend the body of QUBO formulations of NP-complete and NP-hard
optimisation problems by discussing additional problems.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08015</id>
    <link href="http://arxiv.org/abs/1911.08015" rel="alternate" type="text/html"/>
    <title>Low-Rank Toeplitz Matrix Estimation via Random Ultra-Sparse Rulers</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lawrence:Hannah.html">Hannah Lawrence</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jerry.html">Jerry Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musco:Cameron.html">Cameron Musco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musco:Christopher.html">Christopher Musco</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08015">PDF</a><br/><b>Abstract: </b>We study how to estimate a nearly low-rank Toeplitz covariance matrix $T$
from compressed measurements. Recent work of Qiao and Pal addresses this
problem by combining sparse rulers (sparse linear arrays) with frequency
finding (sparse Fourier transform) algorithms applied to the Vandermonde
decomposition of $T$. Analytical bounds on the sample complexity are shown,
under the assumption of sufficiently large gaps between the frequencies in this
decomposition. In this work, we introduce random ultra-sparse rulers and
propose an improved approach based on these objects. Our random rulers
effectively apply a random permutation to the frequencies in $T$'s Vandermonde
decomposition, letting us avoid frequency gap assumptions and leading to
improved sample complexity bounds. In the special case when $T$ is circulant,
we theoretically analyze the performance of our method when combined with
sparse Fourier transform algorithms based on random hashing. We also show
experimentally that our ultra-sparse rulers give significantly more robust and
sample efficient estimation then baseline methods.
</p></div>
    </summary>
    <updated>2019-11-20T23:29:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08004</id>
    <link href="http://arxiv.org/abs/1911.08004" rel="alternate" type="text/html"/>
    <title>Consistent recovery threshold of hidden nearest neighbor graphs</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Jian.html">Jian Ding</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Yihong.html">Yihong Wu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Jiaming.html">Jiaming Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Dana.html">Dana Yang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08004">PDF</a><br/><b>Abstract: </b>Motivated by applications such as discovering strong ties in social networks
and assembling genome subsequences in biology, we study the problem of
recovering a hidden $2k$-nearest neighbor (NN) graph in an $n$-vertex complete
graph, whose edge weights are independent and distributed according to $P_n$
for edges in the hidden $2k$-NN graph and $Q_n$ otherwise. The special case of
Bernoulli distributions corresponds to a variant of the Watts-Strogatz
small-world graph. We focus on two types of asymptotic recovery guarantees as
$n\to \infty$: (1) exact recovery: all edges are classified correctly with
probability tending to one; (2) almost exact recovery: the expected number of
misclassified edges is $o(nk)$. We show that the maximum likelihood estimator
achieves (1) exact recovery for $2 \le k \le n^{o(1)}$ if $ \liminf
\frac{2\alpha_n}{\log n}&gt;1$; (2) almost exact recovery for $ 1 \le k \le
o\left( \frac{\log n}{\log \log n} \right)$ if $\liminf
\frac{kD(P_n||Q_n)}{\log n}&gt;1$, where $\alpha_n \triangleq -2 \log \int \sqrt{d
P_n d Q_n}$ is the R\'enyi divergence of order $\frac{1}{2}$ and $D(P_n||Q_n)$
is the Kullback-Leibler divergence. Under mild distributional assumptions,
these conditions are shown to be information-theoretically necessary for any
algorithm to succeed. A key challenge in the analysis is the enumeration of
$2k$-NN graphs that differ from the hidden one by a given number of edges.
</p></div>
    </summary>
    <updated>2019-11-20T23:30:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07981</id>
    <link href="http://arxiv.org/abs/1911.07981" rel="alternate" type="text/html"/>
    <title>New lower bounds for matrix multiplication and the 3x3 determinant</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Conner:Austin.html">Austin Conner</a>, Alicia Harper, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Landsberg:J=_M=.html">J. M. Landsberg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07981">PDF</a><br/><b>Abstract: </b>Let $M_{\langle u,v,w\rangle}\in C^{uv}\otimes C^{vw}\otimes C^{wu}$ denote
the matrix multiplication tensor (and write $M_n=M_{\langle n,n,n\rangle}$) and
let $det_3\in ( C^9)^{\otimes 3}$ denote the determinant polynomial considered
as a tensor. For a tensor $T$, let $\underline R(T)$ denote its border rank. We
(i) give the first hand-checkable algebraic proof that $\underline
R(M_2)=7$,(ii) prove $\underline R(M_{\langle 223\rangle})=10$, and $\underline
R(M_{\langle 233\rangle})=14$, where previously the only nontrivial matrix
multiplication tensor whose border rank had been determined was $M_2$,(iii)
prove $\underline R( M_3)\geq 17$, (iv) prove $\underline R( det_3)=17$,
improving the previous lower bound of $12$, (v) prove $\underline R(M_{\langle
2nn\rangle})\geq n^2+1.32n$ for all $n\geq 25$ (previously only $\underline
R(M_{\langle 2nn\rangle})\geq n^2+1$ was known) as well as lower bounds for
$4\leq n\leq 25$, and (vi) prove $\underline R(M_{\langle 3nn\rangle})\geq
n^2+2 n+1$ for all $ n\geq 21$, where previously only $\underline R(M_{\langle
3nn\rangle})\geq n^2+2$ was known, as well as lower boundsfor $4\leq n\leq 21$.
</p>
<p>Our results utilize a new technique initiated by Buczy\'{n}ska and
Buczy\'{n}ski, called border apolarity. The two key ingredients are: (i) the
use of a multi-graded ideal associated to a border rank $r$ decomposition of
any tensor, and (ii) the exploitation of the large symmetry group of $T$ to
restrict to $B_T$-invariant ideals, where $B_T$ is a maximal solvable subgroup
of the symmetry group of $T$.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07972</id>
    <link href="http://arxiv.org/abs/1911.07972" rel="alternate" type="text/html"/>
    <title>Learning-Assisted Competitive Algorithms for Peak-Aware Energy Scheduling</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Russell Lee, Mohammad H. Hajiesmaili, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jian.html">Jian Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07972">PDF</a><br/><b>Abstract: </b>In this paper, we study the peak-aware energy scheduling problem using the
competitive framework with machine learning prediction. With the uncertainty of
energy demand as the fundamental challenge, the goal is to schedule the energy
output of local generation units such that the electricity bill is minimized.
While this problem has been tackled using classic competitive design with
worst-case guarantee, the goal of this paper is to develop learning-assisted
competitive algorithms to improve the performance in a provable manner. We
develop two deterministic and randomized algorithms that are provably robust
against the poor performance of learning prediction, however, achieve the
optimal performance as the error of prediction goes to zero. Extensive
experiments using real data traces verify our theoretical observations and show
15.13% improved performance against pure online algorithms.
</p></div>
    </summary>
    <updated>2019-11-20T23:32:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07945</id>
    <link href="http://arxiv.org/abs/1911.07945" rel="alternate" type="text/html"/>
    <title>Optimal Single-Choice Prophet Inequalities from Samples</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinstein:Aviad.html">Aviad Rubinstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Jack_Z=.html">Jack Z. Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinberg:S=_Matthew.html">S. Matthew Weinberg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07945">PDF</a><br/><b>Abstract: </b>We study the single-choice Prophet Inequality problem when the gambler is
given access to samples. We show that the optimal competitive ratio of $1/2$
can be achieved with a single sample from each distribution. When the
distributions are identical, we show that for any constant $\varepsilon &gt; 0$,
$O(n)$ samples from the distribution suffice to achieve the optimal competitive
ratio ($\approx 0.745$) within $(1+\varepsilon)$, resolving an open problem of
Correa, D\"utting, Fischer, and Schewior.
</p></div>
    </summary>
    <updated>2019-11-20T23:27:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07306</id>
    <link href="http://arxiv.org/abs/1911.07306" rel="alternate" type="text/html"/>
    <title>Quantum Speedup for Graph Sparsification, Cut Approximation and Laplacian Solving</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Apers:Simon.html">Simon Apers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wolf:Ronald_de.html">Ronald de Wolf</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07306">PDF</a><br/><b>Abstract: </b>Graph sparsification underlies a large number of algorithms, ranging from
approximation algorithms for cut problems to solvers for linear systems in the
graph Laplacian. In its strongest form, "spectral sparsification" reduces the
number of edges to near-linear in the number of nodes, while approximately
preserving the cut and spectral structure of the graph. The breakthrough work
by Bencz\'ur and Karger (STOC'96) and Spielman and Teng (STOC'04) showed that
sparsification can be done optimally in time near-linear in the number of edges
of the original graph.
</p>
<p>In this work we show that quantum algorithms allow to speed up spectral
sparsification, and thereby many of the derived algorithms. Given
adjacency-list access to a weighted graph with $n$ nodes and $m$ edges, our
algorithm outputs an $\epsilon$-spectral sparsifier in time
$\widetilde{O}(\sqrt{mn}/\epsilon)$. We prove that this is tight up to
polylog-factors. The algorithm builds on a string of existing results, most
notably sparsification algorithms by Spielman and Srivastava (STOC'08) and
Koutis and Xu (TOPC'16), a spanner construction by Thorup and Zwick (STOC'01),
a single-source shortest-paths quantum algorithm by D\"urr et al. (ICALP'04)
and an efficient $k$-wise independent hash construction by Christiani, Pagh and
Thorup (STOC'15). Combining our sparsification algorithm with existing
classical algorithms yields the first quantum speedup, roughly from
$\widetilde{O}(m)$ to $\widetilde{O}(\sqrt{mn})$, for approximating the max
cut, min cut, min $st$-cut, sparsest cut and balanced separator of a graph.
Combining our algorithm with a classical Laplacian solver, we demonstrate a
similar speedup for Laplacian solving, for approximating effective resistances,
cover times and eigenvalues of the Laplacian, and for spectral clustering.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.09415</id>
    <link href="http://arxiv.org/abs/1907.09415" rel="alternate" type="text/html"/>
    <title>Quantum Computing: Lecture Notes</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wolf:Ronald_de.html">Ronald de Wolf</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.09415">PDF</a><br/><b>Abstract: </b>This is a set of lecture notes suitable for a Master's course on quantum
computation and information from the perspective of theoretical computer
science. The first version was written in 2011, with many extensions and
improvements in subsequent years. The first 10 chapters cover the circuit model
and the main quantum algorithms (Deutsch-Jozsa, Simon, Shor, Hidden Subgroup
Problem, Grover, quantum walks, Hamiltonian simulation and HHL). They are
followed by 2 chapters about complexity, 4 chapters about distributed ("Alice
and Bob") settings, and a final chapter about quantum error correction.
Appendices A and B give a brief introduction to the required linear algebra and
some other mathematical and computer science background. All chapters come with
exercises, with some hints provided in Appendix C.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3443</id>
    <link href="https://agtb.wordpress.com/2019/11/19/a-market-for-tcs-papers/" rel="alternate" type="text/html"/>
    <title>A Market for TCS Papers??</title>
    <summary>By David Eppstein &amp; Vijay Vazirani No, not to make theoreticians rich! Besides, who will buy your papers anyway? (Quite the opposite, you will be lucky if you can convince someone to take them for free, just for sake of publicity!) What we are proposing is a market in which no money changes hands – […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>By David Eppstein &amp; Vijay Vazirani</em></p>
<p>No, not to make theoreticians rich! Besides, who will buy your papers anyway? (Quite the opposite, you will be lucky if you can convince someone to take them for free, just for sake of publicity!) What we are proposing is a market in which no money changes hands – a matching market – for matching papers to conferences.</p>
<p>First, a short preamble on how the idea emerged.</p>
<p><strong>Preamble</strong> (by Vijay):  Soon after my recent <a href="https://www.youtube.com/watch?v=MLr6Ud5qmt4&amp;t=74s"><u>Simons talk</u></a> on Matching Markets, I sent its url to Al Roth. Obviously, I wasn’t expecting a return email. However, the perfect gentleman and ultimate scholar that Al is, he did reply, and mentioned that he did not like my “definition” of matching markets and said, “I guess I would say matching markets are markets because they aggregate information that is held by the participants, which is what markets do (even if they don’t use prices to do it..).” This hit me like lightening from the sky – suddenly it crystallized the innate intuition about markets which I had formed through work on algorithmic aspects of markets! I thanked Al profusely and added, “This definitely helps in me get the right perspective on the notion!”</p>
<p>About a week ago, while updating my talk for a seminar at Columbia University, I included this beautiful insight in it and then a thought occurred: Each PC meeting involves aggregation of information from a large number of agents: PC members as well as external experts. Hence, isn’t a conference a matching market? Excitedly, I sent this question to Al. He replied, “… the conference process, matching papers to conferences, is a market and a particular conference might be a marketplace … ”</p>
<p>When I returned home, my esteemed colleague, David Eppstein, stunned me by declaring that he had thought of a market relevant to our field in which no money changes hands. I immediately knew he was thinking of the conference process. But he got to it out of the blue … and not the long process it took me!</p>
<p><strong>Back to the idea:  </strong>In the past, matching markets have brought immense efficiency and order in allocation problems in which use of money is considered repugnant, the prime examples being matching medical residents to hospitals, kidney exchange, and assignment of students of a large city to its schools.</p>
<p>At present we are faced with massive inefficiencies in the conference process – numerous researchers are trapped in unending cycles of submit … get reject … incorporate comments … resubmit — often to the next deadline which has been conveniently arranged a couple of days down the road so the unwitting participants are conditioned into mindlessly keep coming back for more, much like Pavlov’s dog.</p>
<p>We are proposing a matching market approach to finally obliterate this madness. We believe such a market is feasible using the following ideas. No doubt our scheme will have some drawbacks; however, as should be obvious, the advantages far outweigh them.</p>
<p>First, for co-located symposia within a larger umbrella conference, such as the<br/>
conferences within ALGO or FCRC, the following process should be a no-brainer:</p>
<p>1). Ensure a common deadline for all symposia; denote the latter by <em>S.</em></p>
<p>2). Let <em>R</em> denote the set of researchers who wish to submit one paper to a symposium in this umbrella conference – assume that researchers submitting more than one paper will have multiple names, one for each submission. Each researcher will provide a strict preference order over the subset of symposia to which they wish to submit their paper. Let <em>G</em> denote the bipartite graph with vertex sets (<em>R, S</em>) and an edge (<em>r, s</em>) only if researcher <em>r</em> chose symposium <em>s.</em></p>
<p>3). The umbrella conference will have a large common PC with experts representing all of its symposia. The process of assigning papers to PC members will of course use <em>G</em> in a critical way.</p>
<p>Once papers are reviewed by PC members and external reviewers, each symposium will rank its submissions using its own criteria of acceptance. We believe the overhead of ranking each paper multiple times is minimal since that is just an issue of deciding how “on-topic” a paper is – an easy task once the reviews of the paper are available.</p>
<p>4). Finally, using all these preference lists, a researcher-proposing stable matching is computed using the Gale-Shapley algorithm. As is well-known, this mechanism will be dominant strategy incentive compatible for researchers.</p>
<p>With a little extra effort, a similar scheme can also be used for a group of conferences at diverse locations but similar times, such as some of the annual summer theory conferences, STOC, ICALP, ESA, STAC, WADS/SWAT, etc.</p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2019-11-19T01:31:12Z</updated>
    <published>2019-11-19T01:31:12Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Kevin Leyton-Brown</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-11-21T00:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/18/faculty-at-boston-university-apply-by-december-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/11/18/faculty-at-boston-university-apply-by-december-1-2019/" rel="alternate" type="text/html"/>
    <title>Faculty at Boston University (apply by December 1, 2019)</title>
    <summary>Boston University Computer Science has openings for multiple tenure-track assistant professorships beginning July 1, 2020. Applications in all areas of computer science are invited. The university is also considering senior applicants for its Data Science Initiative. The Department, currently at 31 faculty members, is in the midst of an extended period of sustained growth. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Boston University Computer Science has openings for multiple tenure-track assistant professorships beginning July 1, 2020. Applications in all areas of computer science are invited. The university is also considering senior applicants for its Data Science Initiative.</p>
<p>The Department, currently at 31 faculty members, is in the midst of an extended period of sustained growth.</p>
<p>Website: <a href="https://www.bu.edu/cs/2019/11/18/bu-cs-searching-for-new-faculty-members/">https://www.bu.edu/cs/2019/11/18/bu-cs-searching-for-new-faculty-members/</a><br/>
Email: ads22@bu.edu</p></div>
    </content>
    <updated>2019-11-18T19:59:22Z</updated>
    <published>2019-11-18T19:59:22Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-21T00:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/18/faculty-quantum-computation-at-uc-san-diego-apply-by-january-1-2020/</id>
    <link href="https://cstheory-jobs.org/2019/11/18/faculty-quantum-computation-at-uc-san-diego-apply-by-january-1-2020/" rel="alternate" type="text/html"/>
    <title>faculty – quantum computation at UC San Diego (apply by January 1, 2020)</title>
    <summary>The Computer Science and the Mathematics departments at UC San Diego are looking for excellent candidates in Quantum Computation. This includes: computational complexity of quantum computation, quantum algorithms, ways to establish quantum supremacy, applications of quantum computation in the sciences, quantum error-correction, quantum communication complexity, or similar topics. Website: https://apol-recruit.ucsd.edu/JPF02292 Email: shachar.lovett@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science and the Mathematics departments at UC San Diego are looking for excellent candidates in Quantum Computation. This includes: computational complexity of quantum computation, quantum algorithms, ways to establish quantum supremacy, applications of quantum computation in the sciences, quantum error-correction, quantum communication complexity, or similar topics.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/JPF02292">https://apol-recruit.ucsd.edu/JPF02292</a><br/>
Email: shachar.lovett@gmail.com</p></div>
    </content>
    <updated>2019-11-18T17:54:44Z</updated>
    <published>2019-11-18T17:54:44Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-21T00:20:36Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3436178446517561376</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3436178446517561376/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/11/fields-used-to-be-closer-together-than.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3436178446517561376" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3436178446517561376" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/11/fields-used-to-be-closer-together-than.html" rel="alternate" type="text/html"/>
    <title>Fields used to be closer together than they are now. Good? Bad?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">There was a retired software Eng professor that I had heard two very non-controversial rumors about:<br/>
<br/>
1) He got his PhD in Numerical Analysis<br/>
<br/>
2) He got his PhD in Compiler Optimization.<br/>
<br/>
So I asked him which was true.<br/>
<br/>
The answer: Both! In those days you had to optimize your code to get your NA code to run fast enough.<br/>
<br/>
We cannot imagine that anymore. Or at least I cannot.<br/>
<br/>
Over time the fields of computer science advance more so its hard to be  master of more than one field.  But its not that simple: there has been work recently applying Machine Learning to... well<br/>
everything really. Even so, I think the trend is more towards separation. Or perhaps it oscillates.<br/>
<br/>
I am NOT going to be the grumpy old man (Google once thought I was 70, see <a href="https://blog.computationalcomplexity.org/2018/10/google-added-years-to-my-life.html">here</a>) who says things were better in my day when the fields were closer together. But I will ask the question:<br/>
<br/>
1) Are people more specialized new? While I think yes since each field has gotten more complicated and harder to master. There are exceptions: Complexity theory uses much more sophisticated mathematics then when I was a grad student (1980-1985), and of course Quantum Computing has lead to more comp sci majors knowing physics.<br/>
<br/>
2) Is it good for the field that people are specialized? I am supposed to say that it is terrible and that great advances are made when people are interdiscplinary. But there are many more small advances that are made by someone who has a mastery of one (or two) fields.<br/>
<br/>
3) The PhD Process and the Tenure Process encourage specialization. This I think IS bad since there are different modes of research that should all be respected.'<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-11-18T04:53:00Z</updated>
    <published>2019-11-18T04:53:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-11-20T11:13:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/165</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/165" rel="alternate" type="text/html"/>
    <title>TR19-165 |  Random Restrictions of High-Dimensional Distributions and Uniformity Testing with Subcube Conditioning | 

	Clement Canonne, 

	Xi Chen, 

	Gautam Kamath, 

	Amit Levi, 

	Erik Waingarten</title>
    <summary>We give a nearly-optimal algorithm for testing uniformity of distributions supported on $\{-1,1\}^n$, which makes $\tilde O (\sqrt{n}/\varepsilon^2)$ queries to a subcube conditional sampling oracle (Bhattacharyya and Chakraborty (2018)). The key technical component is a natural notion of random restriction for distributions on $\{-1,1\}^n$, and a quantitative analysis of how such a restriction affects the mean vector of the distribution. Along the way, we consider the problem of mean testing with independent samples and provide a nearly-optimal algorithm.</summary>
    <updated>2019-11-18T01:16:46Z</updated>
    <published>2019-11-18T01:16:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-21T00:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4414</id>
    <link href="https://www.scottaaronson.com/blog/?p=4414" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4414#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4414" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Aaronson-Ambainis Conjecture (2008-2019)</title>
    <summary xml:lang="en-US">(see new update at end of post) Around 1999, one of the first things I ever did in quantum computing theory was to work on a problem that Fortnow and Rogers suggested in a paper: is it possible to separate P from BQP relative to a random oracle? (That is, without first needing to separate […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>(see new update at end of post)</em></p>



<p>Around 1999, one of the first things I ever did in quantum computing theory was to work on a problem that <a href="https://arxiv.org/abs/cs/9811023">Fortnow and Rogers</a> suggested in a paper: is it possible to separate <a href="https://en.wikipedia.org/wiki/P_(complexity)">P</a> from <a href="https://en.wikipedia.org/wiki/BQP">BQP</a> relative to a <a href="https://en.wikipedia.org/wiki/Random_oracle">random oracle</a>?  (That is, without first needing to separate P from PSPACE or whatever in the real world?)  Or to the contrary: suppose that a quantum algorithm Q makes T queries to a Boolean input string X.  Is there then a classical simulation algorithm that makes poly(T) queries to X, and that approximates Q’s acceptance probability for <em>most</em> values of X?  Such a classical simulation, were it possible, would still be consistent with the existence of quantum algorithms like <a href="https://en.wikipedia.org/wiki/Simon%27s_problem">Simon’s</a> and <a href="https://en.wikipedia.org/wiki/Shor%27s_algorithm">Shor’s</a>, which are able to achieve exponential (and even greater) speedups in the black-box setting.  It would simply demonstrate the importance, for Simon’s and Shor’s algorithms, of global structure that makes the string X extremely <em>non</em>-random: for example, encoding a periodic function (in the case of Shor’s algorithm), or encoding a function that hides a secret string s (in the case of Simon’s).  It would underscore that superpolynomial quantum speedups depend on structure.</p>



<p>I never managed to solve this problem.  Around 2008, though, I noticed that a solution would follow from a perhaps-not-obviously-related conjecture, about <em>influences</em> in low-degree polynomials.  Namely, let p:R<sup>n</sup>→R be a degree-d real polynomial in n variables, and suppose p(x)∈[0,1] for all x∈{0,1}<sup>n</sup>.  Define the <i>variance</i> of p to be<br/>   Var(p):=E<sub>x,y</sub>[|p(x)-p(y)|],<br/>and define the <i>influence</i> of the i<sup>th</sup> variable to be<br/>   Inf<sub>i</sub>(p):=E<sub>x</sub>[|p(x)-p(x<sup>i</sup>)|].<br/>Here the expectations are over strings in {0,1}<sup>n</sup>, and x<sup>i</sup> means x with its i<sup>th</sup> bit flipped between 0 and 1.  Then the conjecture is this: there must be some variable i such that Inf<sub>i</sub>(p) ≥ poly(Var(p)/d) (in other words, that “explains” a non-negligible fraction of the variance of the entire polynomial).</p>



<p>Why would this conjecture imply the statement about quantum algorithms?  Basically, because of the seminal result of <a href="https://arxiv.org/abs/quant-ph/9802049">Beals et al.</a> from 1998: that if a quantum algorithm makes T queries to a Boolean input X, then its acceptance probability can be written as a real polynomial over the bits of X, of degree at most 2T.  Given that result, if you wanted to classically simulate a quantum algorithm Q on most inputs—and if you only cared about query complexity, not computation time—you’d simply need to do the following:<br/>(1) Find the polynomial p that represents Q’s acceptance probability.<br/>(2) Find a variable i that explains at least a 1/poly(T) fraction of the total remaining variance in p, and query that i.<br/>(3) Keep repeating step (2), until p has been restricted to a polynomial with not much variance left—i.e., to nearly a constant function p(X)=c.  Whenever that happens, halt and output the constant c.<br/>The key is that by hypothesis, this algorithm will halt, with high probability over X, after only poly(T) steps.</p>



<p>Anyway, around the same time, Andris Ambainis had a major break on a different problem that I’d told him about: namely, whether randomized and quantum query complexities are polynomially related for all partial functions with permutation symmetry (like the collision and the element distinctness functions).  Andris and I decided to write up the two directions jointly.  The result was our 2011 paper entitled <a href="https://arxiv.org/abs/0911.0996">The Need for Structure in Quantum Speedups</a>.</p>



<p>Of the two contributions in the “Need for Structure” paper, the one about random oracles and influences in low-degree polynomials was clearly the weaker and less satisfying one.  As the reviewers pointed out, that part of the paper didn’t solve anything: it just reduced one unsolved problem to a new, slightly different problem that was <em>also</em> unsolved.  Nevertheless, that part of the paper acquired a life of its own over the ensuing decade, as the world’s experts in analysis of Boolean functions and polynomials began referring to the “Aaronson-Ambainis Conjecture.”  Ryan O’Donnell, Guy Kindler, and many others had a stab.  I even got Terry Tao to spend an hour or two on the problem when I visited UCLA.</p>



<p>Now, at long last, Nathan Keller and Ohad Klein say they’ve proven the Aaronson-Ambainis Conjecture, in a preprint whose title is a riff on ours: <a href="https://arxiv.org/abs/1911.03748">“Quantum Speedups Need Structure.”</a></p>



<p>Their paper hasn’t yet been peer-reviewed, and I haven’t yet carefully studied it, but I <em>could</em> and <em>should</em>: at 19 pages, it looks very approachable and clear, if not as radically short as (say) <a href="https://www.scottaaronson.com/blog/?p=4229">Huang’s proof of the Sensitivity Conjecture</a>.  Keller and Klein’s argument subsumes all the earlier results that I knew would need to be subsumed, and involves all the concepts (like a real analogue of block sensitivity) that I knew would need to be involved somehow.</p>



<p>My plan had been as follows:<br/>(1) Read their paper in detail.  Understand every step of their proof.<br/>(2) Write a blog post that reflects my detailed understanding.</p>



<p>Unfortunately, this plan did not sufficiently grapple with the fact that I now have two kids.  It got snagged for a week at step (1).  So I’m now executing an alternative plan, which is to jump immediately to the blog post.</p>



<p>Assuming Keller and Klein’s result holds up—as I expect it will—by combining it with the observations in my and Andris’s paper, one immediately gets an explanation for why no one has managed to separate P from BQP relative to a <em>random</em> oracle, but only relative to non-random oracles.  This complements the work of <a href="https://www.uncg.edu/mat/faculty/cdsmyth/thesis.pdf">Kahn, Saks, and Smyth</a>, who around 2000 gave a precisely analogous explanation for the difficulty of separating P from NP∩coNP relative to a random oracle.</p>



<p>Unfortunately, the polynomial blowup is quite enormous: from a quantum algorithm making T queries, Keller and Klein apparently get a classical algorithm making O(T<sup>18</sup>) queries.  But such things can almost always be massively improved.</p>



<p>Feel free to use the comments to ask any questions about this result or its broader context.  I’ll either do my best to answer from the limited amount I know, or else I’ll pass the questions along to Nathan and Ohad themselves.  Maybe, at some point, I’ll even be forced to understand the new proof.</p>



<p>Congratulations to Nathan and Ohad!</p>



<p><strong><font color="red">Update (Nov. 20):</font></strong> Tonight I finally did what I should’ve done two weeks ago, and worked through the paper from start to finish.  Modulo some facts about noise operators, hypercontractivity, etc. that I took on faith, I now have a reasonable (albeit imperfect) understanding of the proof.  It’s great!</p>



<p>In case it’s helpful to anybody, here’s my one-paragraph summary of how it works.  First, you hit your bounded degree-d function f with a random restriction to attenuate its higher-degree Fourier coefficients (reminiscent of <a href="http://www.ma.huji.ac.il/~ehudf/courses/anal09/LMN.pdf">Linial-Mansour-Nisan</a>).  Next, in that attenuated function, you find a small “coalition” of influential variables—by which we mean, a set of variables for which there’s <em>some</em> assignment that substantially biases f.  You keep iterating—finding influential coalitions in subfunctions on n/4, n/8, etc. variables.  All the while, you keep track of <em>the norm of the vector of all the block-sensitivities of all the inputs</em> (the authors don’t clearly explain this in the intro, but they reveal it near the end).  Every time you find another influential coalition, that norm goes down by a little, but by approximation theory, it can only go down O(d<sup>2</sup>) times until it hits rock bottom and your function is nearly constant.  By the end, you’ll have approximated f itself by a decision tree of depth poly(d, 1/ε, log(n)).  Finally, you get rid of the log(n) term by using the fact that f essentially depended on at most exp(O(d)) variables anyway. </p>



<p>Anyway, I’m not sure how helpful it is to write more: the <a href="https://arxiv.org/pdf/1911.03748.pdf">paper itself</a> is about 95% as clear as it could possibly be, and even where it isn’t, you’d probably need to read it first (and, uh, know something about influences, block sensitivity, random restrictions, etc.) before any further clarifying remarks would be of use.  But happy to discuss more in the comments, if anyone else is reading it.</p></div>
    </content>
    <updated>2019-11-17T23:33:08Z</updated>
    <published>2019-11-17T23:33:08Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-11-20T15:06:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/164</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/164" rel="alternate" type="text/html"/>
    <title>TR19-164 |  Improved bounds for perfect sampling of $k$-colorings in graphs | 

	Siddharth Bhandari, 

	Sayantan Chakraborty</title>
    <summary>We present a randomized algorithm that takes as input an undirected $n$-vertex graph $G$ with maximum degree $\Delta$ and an integer $k &gt; 3\Delta$, and returns a random proper $k$-coloring of $G$. The 
 distribution of the coloring is perfectly uniform over the set of all proper $k$-colorings; the expected running time of the algorithm is $\mathrm{poly}(k,n)=\widetilde{O}(n\Delta^2\cdot \log(k))$. 
 This improves upon a result of Huber~(STOC 1998) who obtained polynomial time perfect sampling algorithm for $k&gt;\Delta^2+2\Delta$.
 Prior to our work, no algorithm with expected running time $\mathrm{poly}(k,n)$ was known to guarantee perfectly sampling for $\Delta = \omega(1)$ and for any $k \leq \Delta^2+2\Delta$. 
 
 Our algorithm (like several other perfect sampling algorithms including Huber's) is based on  the Coupling from the Past method. Inspired by the bounding chain approach pioneered independently by H\"aggstr\"om \&amp; Nelander~(Scand.{} J.{} Statist., 1999) and Huber~(STOC 1998), our algorithm is based on a novel bounding chain for the coloring problem.</summary>
    <updated>2019-11-17T10:56:43Z</updated>
    <published>2019-11-17T10:56:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-21T00:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/16/postdoc-at-princeton-university-apply-by-january-10-2020/</id>
    <link href="https://cstheory-jobs.org/2019/11/16/postdoc-at-princeton-university-apply-by-january-10-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc at Princeton University (apply by January 10, 2020)</title>
    <summary>The Department of Computer Science at Princeton University is seeking exceptional recent Ph.D. recipients for research positions in theoretical computer science and theoretical machine learning. Note that there are two positions, https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14281 and at the link below. Website: https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14221 Email: smweinberg@princeton.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at Princeton University is seeking exceptional recent Ph.D. recipients for research positions in theoretical computer science and theoretical machine learning.</p>
<p>Note that there are two positions, <a href="https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14281">https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14281</a> and at the link below.</p>
<p>Website: <a href="https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14221">https://puwebp.princeton.edu/AcadHire/apply/application.xhtml?listingId=14221</a><br/>
Email: smweinberg@princeton.edu</p></div>
    </content>
    <updated>2019-11-16T17:47:20Z</updated>
    <published>2019-11-16T17:47:20Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-21T00:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/163</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/163" rel="alternate" type="text/html"/>
    <title>TR19-163 |  Approximating the Distance to Monotonicity of Boolean Functions | 

	Ramesh Krishnan S. Pallavoor, 

	Sofya Raskhodnikova, 

	Erik Waingarten</title>
    <summary>We design a nonadaptive algorithm that, given a Boolean function $f\colon \{0,1\}^n \to \{0,1\}$ which is $\alpha$-far from monotone, makes poly$(n, 1/\alpha)$ queries and returns an estimate that, with high probability, is an $\widetilde{O}(\sqrt{n})$-approximation to the distance of $f$ to monotonicity. Furthermore, we show that for any constant $\kappa &gt; 0,$ approximating the distance to monotonicity up to $n^{1/2 - \kappa}$-factor requires $2^{n^\kappa}$ nonadaptive queries, thereby ruling out a poly$(n, 1/\alpha)$-query nonadaptive algorithm for such approximations. This answers a question of Seshadhri (Property Testing Review, 2014) for the case of nonadaptive algorithms. Approximating the distance to a property is closely related to tolerantly testing that property. Our lower bound stands in contrast to standard (non-tolerant) testing of monotonicity that can be done nonadaptively with $\widetilde{O}(\sqrt{n} / \varepsilon^2)$ queries.

We obtain our lower bound by proving an analogous bound for erasure-resilient testers. An $\alpha$-erasure-resilient tester for a desired property gets oracle access to a function that has at most an $\alpha$ fraction of values erased. The tester has to accept (with probability at least 2/3) if the erasures can be filled in to ensure that the resulting function has the property and to reject (with probability at least 2/3) if every completion of erasures results in a function that is $\varepsilon$-far from having the property. Our method yields the same lower bounds for unateness and being a $k$-junta. These lower bounds improve exponentially on the existing lower bounds for these properties.</summary>
    <updated>2019-11-16T16:11:38Z</updated>
    <published>2019-11-16T16:11:38Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-21T00:20:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/11/15/linkage</id>
    <link href="https://11011110.github.io/blog/2019/11/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>OMICS Group now charging for article withdrawals (): a new way for predatory journals to be predatory. It’s probably even legal: they have begun providing you with a service (reviewing of your paper) and told you up front what the charges are. Whether it’s ethical for scientific publishing is an entirely different question… So let this be a lesson to be careful where you submit, because unsubmitting could be difficult.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://scholarlyoa.com/omics-group-now-charging-for-article-withdrawals/">OMICS Group now charging for article withdrawals</a> (<a href="https://mathstodon.xyz/@11011110/103069657170279386"/>): a new way for predatory journals to be predatory. It’s probably even legal: they have begun providing you with a service (reviewing of your paper) and told you up front what the charges are. Whether it’s ethical for scientific publishing is an entirely different question… So let this be a lesson to be careful where you submit, because unsubmitting could be difficult.</p>
  </li>
  <li>
    <p><a href="https://www.shapeoperator.com/2016/12/12/sunset-geometry/">Sunset geometry</a> (<a href="https://mathstodon.xyz/@11011110/103075417779881667"/>, <a href="https://news.ycombinator.com/item?id=21413358">via</a>). How to tell the radius of the earth from a photo of a sunset over a still body of water (knowing the height of the camera over the water). Not explained: how still the water needs to be, how badly the results are affected by atmospheric refraction, how accurately the measurements need to be performed to get a meaningful estimate, or how likely you are to see the sun meet the horizon before low clouds get in the way.</p>
  </li>
  <li>
    <p><a href="https://www.ams.org/profession/ams-fellows/new-fellows">The new AMS fellows</a> (<a href="https://mathstodon.xyz/@11011110/103077029444407287"/>) include graph theorists Daniel Kráľ and Bojan Mohar, and fellow Wikipedia editor Marie Vitulli. Their announcement also led me to create new Wikipedia articles on new fellows <a href="https://en.wikipedia.org/wiki/Chikako_Mese">Chikako Mese</a>, <a href="https://en.wikipedia.org/wiki/Julianna_Tymoczko">Julianna Tymoczko</a>, and <a href="https://en.wikipedia.org/wiki/Jang-Mei_Wu">Jang-Mei Wu</a>, and Vitulli to create one for <a href="https://en.wikipedia.org/wiki/Tara_E._Brendle">Tara Brendle</a>. Congratulations, all!</p>
  </li>
  <li>
    <p><a href="https://www.maa.org/programs/maa-awards/writing-awards/the-graph-menagerie-abstract-algebra-and-the-mad-veterinarian">The graph menagerie: abstract algebra and the mad veterinarian </a> (<a href="https://mathstodon.xyz/@11011110/103083916322783935"/>). Or, how to solve puzzles like: “Suppose a mad veterinarian creates a transmogrifier that can convert one cat into two dogs and five mice, or one dog into three cats and three mice, or a mouse into a cat and a dog. It can also do each of these operations in reverse. Can it, through any sequence of operations, convert two cats into a pack of dogs? How about one cat?”</p>
  </li>
  <li>
    <p><a href="http://processalgebra.blogspot.com/2019/11/call-for-opinions-length-of-papes-in.html">LIPIcs series editor Luca Aceto polls the community on page limits</a> (<a href="https://mathstodon.xyz/@11011110/103086511516971922"/>). It used to be that conferences in theoretical computer science had page limits because you couldn’t bind volumes with too many paper pages, now long irrelevant. So now that we <em>can</em> publish much longer conference papers, should we? Limits encourage authors to publish full details in a properly refereed journal version, but unlimited length recognizes the reality that many authors are too lazy to make journal versions.</p>
  </li>
  <li>
    <p><a href="https://understandinguncertainty.org/squaring-square-glass">Squaring the square, in stained glass</a> (<a href="https://mathstodon.xyz/@11011110/103095223291132423"/>, <a href="https://scilogs.spektrum.de/hlf/perfect-squares/">via</a>). By David Spiegelhalter, 2013.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Edge_tessellation">Tiling the plane by edge reflection</a> (<a href="https://mathstodon.xyz/@11011110/103099409883184951"/>). Here’s a proof sketch that there are eight ways to do this:
Each prototile vertex must have angle  for integer , and if  is odd, the subsequence of the remaining angles must be symmetric. For an -gon, considering the sum of interior angles shows that</p>

    

    <p>Searching for sequences of integers with these properties (choosing the smallest integers first to make the search bounded) finds that the only cyclic sequences of integers meeting these constraints are (3,12,12), (4,8,8), (4,6,12), (6,6,6), (3,4,6,4), (3,6,3,6), (4,4,4,4), and (3,3,3,3,3,3), the sequences of the 8 known tessellations.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2019/11/08/turkish-academics-sound-alarm-over-gender-segregation-plans">Turkish academics sound alarm over gender segregation plans</a> (<a href="https://mathstodon.xyz/@11011110/103103753533385831"/>). When women’s universities are set up to provide alternatives in the face of persistent discrimination against women in the existing universities (as they were in the US and Korea), that’s one thing. When the women are already successful in the existing universities but women’s universities are set up as a pathway to push them out, that’s entirely different.</p>
  </li>
  <li>
    <p><a href="http://news.mit.edu/2019/leonardo-da-vinci-bridge-test-1010">Engineers put Leonardo da Vinci’s bridge design to the test:
proposed bridge would have been the world’s longest at the time; new analysis shows it would have worked</a> (<a href="https://mathstodon.xyz/@11011110/103111919197001339"/>, <a href="https://arstechnica.com/science/2019/10/testing-leonardo-da-vincis-bridge-his-design-was-stable-study-finds/">via</a>). I don’t think the link does justice to the scale of the thing. Da Vinci proposed a single stone arch across the Golden Horn in Istanbul, roughly 280m. That’s much longer than anything on the current <a href="https://en.wikipedia.org/wiki/List_of_longest_masonry_arch_bridge_spans">list of the world’s biggest stone arches</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Order_polytope">Order polytope</a> (<a href="https://mathstodon.xyz/@11011110/103115656245496084"/>). New Wikipedia article on a convex polytope derived from any finite partial order as the points in a unit hypercube whose coordinate order is consistent with the partial order. Its vertices come from upper sets, its faces come from quotients, its facets come from covering pairs, and its volume comes from the number of linear extensions of the partial order. Coordinatewise min and max gives its points the structure of a continuous distributive lattice.</p>
  </li>
  <li>
    <p><a href="https://www.wired.com/story/socked-into-the-puppet-hole-on-wikipedia/">Socked into the puppet-hole on Wikipedia</a> (<a href="https://mathstodon.xyz/@11011110/103120959491226706"/>). Journalist Noam Cohen’s Wikipedia biography is collateral damage in the war on slowking4, a prolific creator of Wikipedia articles whose problematic behavior (copying content from other sites, creating sockpuppet accounts to hide their identity, and reinstating articles from another user that were so riddled with errors that they were deleted en masse) has led to delete-on-sight actions.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Arithmetic_billiards">Arithmetic billiards</a> (<a href="https://mathstodon.xyz/@11011110/103129171790722530"/>): using billiard ball trajectories to compute number-theoretic functions.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@olligobber/103066273568117018">olligober made a regex to match all multiples of 7</a>, but it was more than 10,000 characters long so grep couldn’t handle it. Applying <a href="https://en.wikipedia.org/wiki/Kleene%27s_algorithm">Kleene’s algorithm</a> to convert the natural DFA for this sort of problem into a regular expression blows its size up from polynomial in the modulus to exponential, but is this necessary? And if it is, what is the best possible base for the exponential?</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/questions/338888/dividing-a-chocolate-bar-into-any-proportions">Dividing a chocolate bar into any proportions</a> (<a href="https://mathstodon.xyz/@11011110/103140599116798730"/>). The bar has  squares, and you want to give each of  people an integer number of squares, but the integers are not known in advance. How to break the bar into few pieces so this will always be possible? Reid Hardison asked this months ago but Ilya Bogdanov answered with an efficient construction of the optimal partition much more recently.</p>
  </li>
  <li>
    <p><a href="http://www.personal.psu.edu/sot2/books/billiardsgeometry.pdf">Geometry and Billiards</a> (<a href="https://mathstodon.xyz/@11011110"/>). An undergraduate-level textbook on the mathematics of reflection by Serge Tabachnikov.</p>
  </li>
</ul></div>
    </content>
    <updated>2019-11-15T22:20:00Z</updated>
    <published>2019-11-15T22:20:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-11-16T06:45:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/162</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/162" rel="alternate" type="text/html"/>
    <title>TR19-162 |  The Random-Query Model and the Memory-Bounded Coupon Collector | 

	Ran Raz, 

	Wei Zhan</title>
    <summary>We study a new model of space-bounded computation, the {\it random-query} model. The model is based on a branching-program over input variables $x_1,\ldots,x_n$. In each time step, the branching program gets as an input a random index $i \in \{1,\ldots,n\}$, together with the input variable $x_i$ (rather than querying an input variable of its choice, as in the case of a standard (oblivious) branching program). We motivate the new model in various ways and study time-space tradeoff lower bounds in this model.

Our main technical result is a quadratic time-space lower bound for zero-error computations in the random-query model, for XOR, Majority and many other functions. More precisely, a zero-error computation is a computation that stops with high probability and such that conditioning on the event that the computation stopped, the output is correct with probability~1. We prove that for any Boolean function $f: \{0,1\}^n \rightarrow \{0,1\}$, with sensitivity $k$, any zero-error computation with time $T$ and space $S$, satisfies 
$T\cdot (S+\log n) \geq \Omega(n \cdot k)$. We note that the best time-space lower bounds for standard oblivious branching programs are only slightly super linear and improving these bounds is an important long-standing open problem.

To prove our results, we study a memory-bounded variant of the coupon-collector problem that seems to us of independent interest and to the best of our knowledge has not been studied before. We consider a zero-error version of the coupon-collector problem. In this problem, the coupon-collector could explicitly choose to stop when he/she is sure with zero-error that
all coupons have already been collected. We prove that any zero-error coupon-collector that stops with high probability in time $T$, and uses space $S$, satisfies $T\cdot (S+\log n) \geq \Omega(n^2)$, where $n$ is the number of different coupons.</summary>
    <updated>2019-11-15T18:33:07Z</updated>
    <published>2019-11-15T18:33:07Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-21T00:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7574</id>
    <link href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/" rel="alternate" type="text/html"/>
    <title>Puzzles of modern machine learning</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-jetpack-markdown"><p>It is often said that "we don’t understand deep learning" but it is not as often clarified what is it exactly that we don’t understand. In this post I try to list some of the "puzzles" of modern machine learning, from a theoretical perspective. This list is neither comprehensive nor authoritative.  Indeed, I  only started looking at these issues last year, and am very much in the position of not yet fully understanding the questions, let alone potential answers.
On the other hand, at the rate ML research is going, a calendar year corresponds to about 10 "ML years"…</p>
<p>Machine learning  offers many opportunities for theorists; there are many more questions than answers, and it is clear that a better theoretical understanding of what makes certain training procedures work or fail is desperately needed. Moreover, recent advances in software frameworks made it much easier to test out intuitions and conjectures. While in the past running training procedures might have required a Ph.D in machine learning, recently the "barrier to entry" was reduced to first to undergraduates, then to high school students, and these days it’s so easy that even theoretical computer scientists can do it <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/></p>
<p>To set the context for this discussion, I focus on the task of supervised learning. In this setting we are given a <em>training set</em> <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> examples of the form <img alt="(x_i,y_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_i%2Cy_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x_i,y_i)"/> where <img alt="x_i \in \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i \in \mathbb{R}^d"/> is some vector (think of it as the pixels of an image) and <img alt="y_i \in { \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=y_i+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_i \in { \pm 1 }"/> is some label (think of <img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_i"/> as equaling <img alt="+1" class="latex" src="https://s0.wp.com/latex.php?latex=%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="+1"/> if <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> is the image of a dog and <img alt="-1" class="latex" src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-1"/> if <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> is the image of a cat). The goal in supervised learning is to find a <em>classifier</em> <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/> such that <img alt="f(x)=y" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(x)=y"/> will hold for many future samples <img alt="(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x,y)"/>.</p>
<p>The standard approach is to consider some parameterized family of classifiers, where for every vector <img alt="\theta \in \mathbb{R}^m" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta \in \mathbb{R}^m"/>  of parameters, we associate a classifier <img alt="f_\theta :\mathbb{R}^d \rightarrow { \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta+%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta :\mathbb{R}^d \rightarrow { \pm 1 }"/>. For example, we can fix a certain neural network architecture (depth, connections, activation functions, etc.) and let <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/>  be the vector of weights that characterizes every network in this architecture. People then run some optimizing algorithm such as stochastic gradient descent with the objective function set as finding the vector <img alt="\theta \in \mathbb{R}^m" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta \in \mathbb{R}^m"/> that minimizes a <em>loss function</em> <img alt="L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\theta)"/>. This loss function can be the fraction of labels that <img alt="f_\theta" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta"/> gets wrong on the set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> or a more continuous loss that takes into account the confidence level or other parameters of <img alt="f_\theta" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta"/> as well. By now this general approach has been successfully applied to a many classification tasks, in many cases achieving near-human to super-human performance.  In the rest of this post I want to discuss some of the questions that arise when trying to obtain a theoretical understanding of both the powers and the limitations of the above approach. I focus on deep learning, though there are still some open questions even for over-parameterized linear regression.</p>
<h2>The generalization puzzle</h2>
<p>The approach outlined above has been well known and  analyzed for many decades in the statistical learning literature. There are many cases where we can <em>prove</em> that a classifier obtained in this case has a small <em>generalization gap</em>, in the sense that if the training set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> was obtained by sampling <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> independent and identical samples from a distribution <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/>, then the performance of a classifier <img alt="f_\theta" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta"/> on new samples from <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/> will be close to its performance on the training set.</p>
<p>Ultimately, these results all boil down to the Chernoff bound. Think of the random variables <img alt="X_1,\ldots,X_n" class="latex" src="https://s0.wp.com/latex.php?latex=X_1%2C%5Cldots%2CX_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_1,\ldots,X_n"/> where <img alt="X_i=1" class="latex" src="https://s0.wp.com/latex.php?latex=X_i%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_i=1"/> if the classifier makes an error on the <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>-th training example. The Chernoff bound tells us that  probability that  that <img alt="\sum X_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+X_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum X_i"/> deviates by more than <img alt="\epsilon n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon n"/> from its expectation is something like <img alt="\exp(-\epsilon^2 n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cepsilon%5E2+n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(-\epsilon^2 n)"/> and so as long as the total number of classifiers is less than <img alt="2^k" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^k"/> for <img alt="k &lt; \epsilon^2 n" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3C+%5Cepsilon%5E2+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k &lt; \epsilon^2 n"/>, we can use a union bound over all possible classifiers to argue that if we make a <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> fraction of errors on the training set, the probability we make an error on a new example is at  most <img alt="p+\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=p%2B%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p+\epsilon"/>. We can of course "bunch together" classifiers that behave similarly on our distribution, and so it is enough if there are at most <img alt="2^{\epsilon^2 n}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Cepsilon%5E2+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{\epsilon^2 n}"/> of these equivalence classes. Another approach is to add a "regularizing term" <img alt="R(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=R%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R(\theta)"/> to the objective function, which amounts to restricting attention to the set of all classifiers <img alt="f_\theta" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta"/> such that <img alt="R(\theta) \leq \mu" class="latex" src="https://s0.wp.com/latex.php?latex=R%28%5Ctheta%29+%5Cleq+%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R(\theta) \leq \mu"/> for some parameter <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>. Again, as long as the number of equivalence classes in this set is less than <img alt="2^{\epsilon^2 n}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Cepsilon%5E2+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{\epsilon^2 n}"/>, we can use this bound.</p>
<p>To a first approximation, the number of classifiers (even after "bunching together") is roughly exponential in the number <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> of parameters, and so these results tell us that as long as the number of <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> of parameters is smaller than the number of examples, we can expect to have a small <em>generalization gap</em> and can infer future performance (known as "test performance") from the performance on the set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> (known as "train performance").
Once the number of parameters <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> becomes close to or even bigger than the number of samples <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>, we are in danger of "overfitting" where we could have excellent train performance but terrible test performance. Thus according to the classical statistical learning theory, the ideal number of parameters would be some number between <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> and the number of samples <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/>, with the precise value governed by the so called "bias variance tradeoff".</p>
<p>This is a beautiful theory, but unfortunately the classical theorems yield vacous  results in the realm of modern machine learning, where we  often train networks with millions of parameters on a mere tens of thousands of examples. Moreover, <a href="https://arxiv.org/abs/1611.03530">Zhang et al</a> showed that this is not just a question of counting  parameters better. They showed that modern deep networks can in fact "overfit" and achieve 100% success on the training set even if you gave them random or arbitrary labels.</p>
<p>The results above in particular show that we can find classifiers that perform great on the training set but perform terribly on the future tests, as well as classifiers that perform terrible on the training set but pretty good on future test. Specifically, consider an architecture that has the capacity to fit <img alt="20n" class="latex" src="https://s0.wp.com/latex.php?latex=20n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="20n"/> arbitrary labels, and suppose that we train it on a set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> examples.  Then we can find a setting of parameters <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> that both fits the training set exactly (i.e.,  satisfies <img alt="f_\theta(x)=y" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta%28x%29%3Dy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta(x)=y"/> for all <img alt="(x,y)\in S" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x,y)\in S"/>) but also satisfies that the additional constraint that <img alt="f_\theta(x)= -y" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta%28x%29%3D+-y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta(x)= -y"/> (i.e., the negation of the label <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/>) for every <img alt="(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x,y)"/> in some additional set <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> of <img alt="19m" class="latex" src="https://s0.wp.com/latex.php?latex=19m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="19m"/> pairs.
(The set <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> is not part of the actual training set, but rather an "auxiliary set" that we simply use for the sake of constructing this counterexample; note that we can use <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> as means to generate the initial network which can then be fed into standard stochastic gradient descent on the set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/>.) The network <img alt="f_\theta" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta"/> fits its training set perfectly, but since it effectively corresponds to training with 95% label noise, it will  perform  worse than even a coin toss.</p>
<p>In an analogous way, we can find parameters <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> that completely fail on the training set, but fit correctly the additional "auxiliary set" <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/>. This will correspond to the case of standard training with 5% label noise, which typically yields about 95% of the performance on the noiseless distribution.</p>
<p>The above insights  break the <strong>separation of concerns</strong> or separation of <strong>computational problems</strong> from <strong>algorithms</strong> which we theorists  like so much. Ideally, we would like to phrase the "machine learning problem" as a well defined optimization objective, such as finding, given a set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/>, the vector <img alt="\theta \in \mathbb{R}^m" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta \in \mathbb{R}^m"/> that mimimizes <img alt="L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\theta)"/>. Once phrased in this way, we can try to find with an algorithm that achieves this goal as efficiently as possible.</p>
<p>Unfortunately, modern machine learning does not currently lend itself to such a clean partition. In particular, since not all optima are equally good, we  <em>don’t  actually want</em> to solve the task of minimizing the loss function in a "black box" way. In fact, many of the ideas that make optimization faster such as accelaration, lower learning rate, second order methods and others, yield <em>worse</em>  generalization performance. Thus, while the objective function is somewhat correlated with generalization performance, it is neither necessary nor sufficient for it. This is a clear sign that we don’t really understand what makes machine learning work, and there is still much left to discover. I don’t know what machine learning textbooks in the 2030’s will contain, but my guess is that they would <em>not</em> prescribe running stochastic gradient descent on one of these loss functions. (Moritz Hardt counters that what we teach in ML today is not that far from the <a href="https://www.amazon.com/Pattern-Classification-Scene-Analysis-Richard/dp/0471223611">1973 book of Duda and Hart</a>, and that by some measures ML moved <em>slower</em> than other areas of CS.)</p>
<p>The <em>generalization puzzle</em> of machine learning can be phrased as the question of understanding what properties of procedures that map a training set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> into a classifier <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> lead to good generalization performance with respect to certain distributions. In particular we would like to understand what are the properties of natural  natural distributions and stochastic gradient descent that make the latter into such a map.</p>
<h2>The computational puzzle</h2>
<p>Yet another puzzle in modern machine learning arises from the fact that we are able to find the minimum of <img alt="L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\theta)"/> in the first place. A priori this is surprising since, apart from very special cases (e.g., linear regression with a square loss), the function <img alt="\theta \mapsto L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cmapsto+L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta \mapsto L_S(\theta)"/> is in general <em>non convex</em>. Indeed, for almost any natural loss function, the problem of finding <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> that minimizes <img alt="L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\theta)"/> is NP hard.
However, if we look at the computational question in the context of the generalization puzzle above, it might not be as mysterious. As we have seen, the fact that the <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> we output is a global minimizer (or close to minimizer)  of <img alt="L_S(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ccdot%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\cdot)"/> is in some sense accidental and by far not the  the most important property of <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/>. There are many minima of the loss function that  generalize badly, and many non minima that  generalize well.</p>
<p>So perhaps the right way to phrase the computational puzzle is as</p>
<blockquote>
<p><em>"How come that we are able to use stochastic gradient descent to find the vector <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> that is output by stochastic gradient descent."</em></p>
</blockquote>
<p>which when phrased like that, doesn’t seem like much of a puzzle after all.</p>
<h2>The off-distribution performance puzzle</h2>
<p>In the supervised learning problem, the training samples <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> are drawn from the same distribution as the final test sample. But in any applications of machine learning, classifiers are expected to perform on samples that arise from very different settings. The image that the camera of a self-driving car observes is not drawn from ImageNet, and yet it still needs to (and often can) detect whether not it is seeing a dog or a cat (at which point it will break or accelerate, depending on whether the programmer was a dog or cat lover). Another insight to this question comes from a recent work of <a href="https://arxiv.org/abs/1902.10811">Recht et al</a>. They generated a new set of images that is very similar to the original ImageNet test set, but not identical to it. One can think of it as generated from a distribution <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D'"/> that is close but not the same as the original distribution <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/> of ImageNet. They then checked how well do neural networks that were trained on the original ImageNet distribution <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/> perform on <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D'"/>. They saw that while these networks performed significantly worse on <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D'"/> than they did on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/>, their performance on <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D'"/> was <em>highly correlated</em> with their performance on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/>. Hence doing better on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/> did correspond to being better in a way that carried over to the (very closely related) <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D'"/>. (However, the networks did perform worse on $D’$ so off-distribution performance is by no means a full success story.)</p>
<p>Coming up with a theory that can supply some predictions for learning in a way that is not as tied to the particular distribution is still very much open. I see it as somewhat akin to finding a theory for the performance of algorithms that is somewhere between average-case complexity (which is highly dependant on the distribution) and worst-case complexity (which does not depend on the distribution at all, but is not always achievable).</p>
<h2>The robustness puzzle</h2>
<p>If the previous puzzles were about understanding why deep networks are surprisingly good, the next one is about understanding why they are surprisingly bad. Images of physical objects have the property that if we modify them in some ways,  such as perturbing them in  a small number of pixels or by few shades or rotating by an angle, they still correspond to the same object. Deep neural networks do not seem to "pick up" on this property.
Indeed, there are many examples of how tiny perturbations can cause a neural net to think that one image is another, and people have even <a href="https://youtu.be/piYnd_wYlT8">printed a 3D turtle</a> that most modern systems recognize as a rifle. (See this <a href="https://adversarial-ml-tutorial.org/">excellent tutorial</a>, though note an "ML decade" has already passed since it was published). This "brittleness" of neural networks can be a significant concern when we deploy them in the wild. (Though perhaps mixing up turtles and rifles is not so bad: I can imagine  some people that would normally resist regulations to protect the environment but would support them if they confused turtles with guns..)
Perhaps one reason for this brittleness is that neural networks can be thought of as a way of embedding a set of examples in dimension <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> into dimension <img alt="\ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ell"/> (where <img alt="\ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ell"/> is the number of neurons in the penultimate layer) in a way that will make the positive examples be linearly separable from the negative examples. Amplifying small differences can help in achieving such a separation, even if it hurts robustness.</p>
<p>Recent works have attempted to rectify this, by using a variants of the loss function where <img alt="L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\theta)"/> corresponds to the maximum error under all possible such perturbations of the data. A priori you would think that while robust training might come at a computational cost, statistically it would be a "win win" with the resulting classifiers not only being more robust but also overall better at classifying. After all, we are providing the training procedure with the additional information (i.e., "updating its prior") that the label should be unchanged by certain transformations, which should be equivalent to supplying it with more data. Surprisingly, the robust classifiers currently perform <em>worse</em> than standard trained classifiers on unperturbed data. <a href="https://arxiv.org/abs/1905.02175">Ilyas et al</a> argued that this may be because even if humans ignore information encoded in, for example, whether the intensity level of a pixel is odd or even, it does not mean that this information is not predictive of the label. Suppose that (with no basis whatsoever – just as an example) cat owners are wealthier than dog owners and hence cat pictures tend to be taken with higher quality lenses. One could imagine that a neural network would pick up on that, and use some of the fine grained information in the pixels to help in classification. When we force such a network to be robust it would perform worse. Distill journal published <a href="https://distill.pub/2019/advex-bugs-discussion/">six discussion pieces</a> on the Ilyas et al paper. I like the idea of such "paper discussions" very much and hope it catches on in machine learning and beyond.</p>
<h2>The interpretability puzzle</h2>
<p>Deep neural networks are inspired by our brain, and it is tempting to try to understand their internal structure just like we try to understand the brain and see if it has a <a href="https://en.wikipedia.org/wiki/Grandmother_cell">"grandmother neuron"</a>. For example, we could try to see if there is a certain neuron (i.e., gate) in a neural network that "fires" only when it is fed images with certain high level features (or more generally find vectors that have large correlation with the state at a certain layer only when the image has some features). This also of practical importance, as we increasingly use classifiers to make decisions such as whether to approve or deny bail, whether to prescribe to a patient treatment A or B, or whether a car should steer left or right, and would like to understand what is the basis for such decisions. There are beautiful visualizations of neural networks’ decisions and internal structures , but given the robustness puzzle above, it is unclear if these really capture the decision process. After all, if we could change the classification from a cat to a dog by perturbing a tiny number of pixels, in what sense can we explain <em>why</em> the network made this decision or the other.</p>
<h2>The natural distributions puzzle</h2>
<p>Yet another puzzle (pointed out to me by Ilya Sutskever) is to understand what is it about "natural" distributions such as images, texts, etc.. that makes them so amenable to learning via neural networks, even though such networks can have a very hard time with learning even simple concepts such as parities.
Perhaps this is related to the "noise robustness" of natural concepts which is related to being correlated with low degree polynomials.
Another suggestion could be that at least for text etc.., human languages are implicitly designed to fit neural network. Perhaps on some other planets there are languages where the meaning of a sentence completely changes depending on whether it has an odd or an even number of letters…</p>
<h2>Summary</h2>
<p>The above are just a few puzzles that modern machine learning offers us. Not all of those might have answers in the form of mathematical theorems, or even well stated conjectures, but it is clear that there is still much to be discovered, and plenty of research opportunities for theoretical computer scientists. In this blog I focused on supervised learning, where at least the problem is well defined, but there are other areas of machine learning, such as transfer learning and generative modeling, where we don’t even yet know how to phrase the computational task, let alone prove that any particular procedure solves it. In several ways, the state of machine learning today seems to me as similar to the state of cryptography in the late 1970’s. After the discovery of public key cryptography, researchers has highly promising techniques and great intuitions, but still did not really understand even what security means, let alone how to achieve it. In the decades since, cryptography has turned from an art to a science, and I hope and believe the same will happen to machine learning.</p>
<p><strong>Acknowledgements:</strong> Thanks to Preetum Nakkiran, Aleksander Mądry, Ilya Sutskever and Moritz Hardt for helpful comments. (In particular, I dropped an interpretability experiment suggested in an earlier version of this post since Moritz informed me that several similar experiments have been done.) Needless to say, none of them is responsible for any of the speculations and/or errors above.</p>
</div>



<p/></div>
    </content>
    <updated>2019-11-15T15:51:43Z</updated>
    <published>2019-11-15T15:51:43Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-11-21T00:20:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/15/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-jan/</id>
    <link href="https://cstheory-jobs.org/2019/11/15/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-jan/" rel="alternate" type="text/html"/>
    <title>Aarhus University is looking for excellent and visionary Assistant and Associate Professors to push the frontiers of Computer Science at Department of Computer Science, Aarhus University (apply by January 9, 2020)</title>
    <summary>We are looking for 2-4 Assistant Professors (Tenure Track – see full job description) and Associate Professors. We in particular wish to build competencies and groups within Machine Learning/Artificial Intelligence, Software Engineering and Systems Security, as well as Computer Graphics, and Computer Vision. In general, we encourage candidates within all areas of Computer Science to […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are looking for 2-4 Assistant Professors (Tenure Track – see full job description) and Associate Professors. We in particular wish to build competencies and groups within Machine Learning/Artificial Intelligence, Software Engineering and Systems Security, as well as Computer Graphics, and Computer Vision. In general, we encourage candidates within all areas of Computer Science to apply.</p>
<p>Website: <a href="https://au.career.emply.com/en/ad/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science./ucjdq3/en">https://au.career.emply.com/en/ad/aarhus-university-is-looking-for-excellent-and-visionary-assistant-and-associate-professors-to-push-the-frontiers-of-computer-science./ucjdq3/en</a><br/>
Email: kgronbak@cs.au.dk</p></div>
    </content>
    <updated>2019-11-15T12:52:14Z</updated>
    <published>2019-11-15T12:52:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-21T00:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/14/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019-2/</id>
    <link href="https://cstheory-jobs.org/2019/11/14/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019-2/" rel="alternate" type="text/html"/>
    <title>Simons-Berkeley Research Fellowship at Simons Institute for the Theory of Computing (apply by December 15, 2019)</title>
    <summary>The Simons Institute for the Theory of Computing invites applications for Simons-Berkeley Research Fellowships to participate in one or more of the semester-long programs during the 2020-21 academic year: Probability, Geometry, and Computation in High Dimensions; Theory of Reinforcement Learning; Satisfiability: Theory, Practice, and Beyond; and Theoretical Foundations of Computer Systems Website: https://simons.berkeley.edu/fellows2020 Email: simonsvisitorservices@berkeley.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Simons Institute for the Theory of Computing invites applications for Simons-Berkeley Research Fellowships to participate in one or more of the semester-long programs during the 2020-21 academic year: Probability, Geometry, and Computation in High Dimensions; Theory of Reinforcement Learning; Satisfiability: Theory, Practice, and Beyond; and Theoretical Foundations of Computer Systems</p>
<p>Website: <a href="https://simons.berkeley.edu/fellows2020">https://simons.berkeley.edu/fellows2020</a><br/>
Email: simonsvisitorservices@berkeley.edu</p></div>
    </content>
    <updated>2019-11-14T23:44:41Z</updated>
    <published>2019-11-14T23:44:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-21T00:20:36Z</updated>
    </source>
  </entry>
</feed>
