<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-03-07T01:22:58Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1485</id>
    <link href="https://ptreview.sublinear.info/?p=1485" rel="alternate" type="text/html"/>
    <title>News for February 2021</title>
    <summary>We got quite some action last month. We saw five papers. A lot of action in graph world and some action in quantum property testing which we hope you will find appetizing. Also included is a result on sampling uniformly random graphlets. Testing Hamiltonicity (and other problems) in Minor-Free Graphs, by Reut Levi and Nadav […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We got quite some action last month. We saw five papers. A lot of action in graph world and some action in quantum property testing which we hope you will find appetizing. Also included is a result on sampling uniformly random <em>graphlets</em>. </p>



<p><strong>Testing Hamiltonicity (and other problems) in Minor-Free Graphs</strong>, by Reut Levi and Nadav Shoshan (<a href="https://arxiv.org/abs/2102.11728">arXiv</a>). Graph Property Testing has been explored pretty well for dense graphs (and reasonably well for bounded degree graphs). However, testing properties in the general case still remains an elusive goal. This paper makes contributions in this direction and as a first result it gives an algorithm for testing Hamiltonicity <em>in minor free graphs</em> (with two sided error) with running time \(poly(1/\varepsilon)\). Let me begin by pointing out that Hamiltonicity is an irksome property to test in the following senses.</p>



<ul><li>It is neither monotone nor additive. So the partition oracle based algorithms do not immediately imply a tester (with running time depending only on \(\varepsilon\) for Hamiltonicity. This annoyance bugs you even in the bounded degree case.</li><li> Czumaj and Sohler characterized what graph properties are testable with one-sided error in general planar graphs. In particular, they show a property of general planar graphs is testable <em>iff</em> this property can be reduced to testing for a finite family of finite forbidden subgraphs. Again, Hamiltonicity does not budge to this result. </li><li>There are (concurrent) results by Goldreich and Adler-Kohler which show that with one-sided error, Hamiltonicity cannot be tested with \(o(n)\) queries. </li></ul>



<p>The paper shows that distance to Hamiltonicity can be exactly captured in terms of a certain combinatorial parameter. Thereafter, the paper tries to estimate this parameter after cleaning up the graph a little. This allows them to estimate the distance to Hamiltonicity and thus also implies a tolerant tester (restricted to mino-free graphs).</p>



<p><strong>Testing properties of signed graphs</strong>, by Florian Adriaens, Simon Apers (<a href="https://arxiv.org/abs/2102.07587">arXiv</a>). Suppose I give you a graph \(G=(V,E)\) where all edges come with a label: which is either “positive” or “negative”. Such signed graphs are used to model various scientific phenomena. Eg, you can use these to model interactions between individuals in social networks into two categories like friendly or antagonistic.</p>



<p>This paper considers property testing problems on signed graphs. The notion of farness from the property extends naturally to these graphs (both in the dense graph model and the bounded degree model). The paper contains explores three problems in both of these models: signed triangle freeness, balance and clusterability. Below I will zoom into the tester for clusterability in the bounded degree setting developed In the paper. A signed graph is considered clusterable if you can partition the vertex set into some number of components such that the edges within any component are all positive and the edges running across components are all negative.</p>



<p>The paper exploits a forbidden subgraph characterization of clusterability which shows that any cycle with exactly one negative edge is a certificate of non-clusterability of \(G\). The tester runs multiple random walks from a handful of start vertices to search for these “bad cycles” by building up on ideas in the seminal work of Goldreich and Ron for testing bipariteness. The authors put all of these ideas together and give a \(\widetilde{O}(\sqrt n)\) time one-sided tester for clusterability in signed graphs.</p>



<p/>



<p><strong>Local Access to Random Walks</strong>, by Amartya Shankha Biswas, Edward Pyne, Ronitt Rubinfeld (<a href="https://arxiv.org/abs/2102.07740">arXiv</a>). Suppose I give you a gigantic graph (with bounded degree) which does not fit in your main memory and I want you to solve some computational problem which requires you to solve longish random walks of length \(t\). And lots of them. It would be convenient to not spend \(\Omega(t)\) units of time performing every single walk. Perhaps it would work just as well for you to have an oracle which provides query access to a \(Position(G,s,t)\) oracle which returns the position of a walk from \(s\) at time \(t\) of your choice. Of course, you would want the sequence of vertices returned to behave consistently with some actual random walk sampled from the distribution of random walks starting at \(s\). Question is: Can I build you this primitive? This paper answers this question in affirmative  and shows that for graphs with spectral gap \(\Delta\), this can be achieved with running time \(\widetilde{O}(\sqrt n/\Delta)\) per query. And you get the guarantee that the joint distribution of the vertices you return at queried times is \(1/poly(n)\) close to the uniform distribution over such walks in \(\ell_1\).  Thus, for a random \(d\)-regular graph, you get running times of the order \(\widetilde{O}(\sqrt n)\) per query. The authors also show tightness of this result by showing to get subconstant error in \(\ell_1\), you necessarily need \(\Omega(\sqrt n/\log n)\) queries in expectation.</p>



<p/>



<p><strong>Efficient and near-optimal algorithms for sampling connected subgraphs</strong>, by Marco Bressan (<a href="https://arxiv.org/abs/2007.12102">arXiv</a>). As the title suggests, this paper considers efficient algorithms for sampling a uniformly random \(k\)-graphlet from a given graph \(G\) (for \(k \geq 3\)). Recall, a \(k\)-graphlet refers to a collection of \(k\)-vertices which induce a connected graph in \(G\). The algorithm considered in the paper is pretty simple. You just define a Markov Chain \(\mathcal{G}_k\) with all \(k\)-graphlets as its state space. Two states in \(\mathcal{G}_k\) are adjacent <em>iff</em> their intersection is a \((k-1)\)-graphlet. To obtain a uniformly random sample, a classical idea is to just run this Markov Chain and obtain an \(\varepsilon\)-uniform sample. However, the gap between upper and lower bounds on the mixing time of this walk is of the order \(\rho^{k-1}\) where \(\rho = \Delta/\delta\) (that is the ratio of maximum and minimum degrees to the power \(k-1\)). The paper closes this gap up to logarithmic factors and shows that the mixing time of the walk is at most \(t_{mix}(G) \rho^{k-1} \log(n/\varepsilon)\). It also proves an almost matching lower bound. Further, the paper also presents an algorithm with event better running time to return an almost uniform \(k\)-graphlet. This exploits a previous observation: sampling a uniformly random \(k\)-graphlet is equivalent to sampling a uniformly random edge in \(\mathcal{G}_{k-1}\). The paper then proves a lemma which upperbounds the relaxation time of walks in \(\mathcal{G}_k\) to walks in \(\mathcal{G}_{k-1}\). And then you upperbound the mixing time in terms of the relaxation time to get an improved expected running time of the order \(O(t_{mix}(G) \cdot \rho^{k-2} \cdot \log(n/\varepsilon)\).</p>



<p/>



<p><strong>Toward Instance-Optimal State Certification With Incoherent Measurements</strong>, by Sitan Chen, Jerry Li, Ryan O’Donnell (<a href="https://arxiv.org/abs/2102.13098">arXiv</a>). The problem of quantum state certification has gathered interest over the last few years. Here is the setup: you are given a quantum state \(\sigma \in \mathbb{C}^{d \times d}\) and you are also given \(N\) copies of an unknown state \(\rho\). You want to distinguish between the following two cases: Does \(\rho = \sigma\) or is \(\sigma\) at least \(\varepsilon\)-far from \(\rho\) in trace norm? Badescu et al showed in a recent work that if entangled measurements are allowed, you can do this with a mere \(O(d/\varepsilon^2)\) copies of \(\rho\). But using entangled states comes with its own share of problems. On the other hand if you disallow entanglement, as Bubeck et al show, you need \(\Omega(d^{3/2}/\varepsilon^2)\) measurements. This paper asks: for which states \(\sigma\) can you improve upon this bound. The work takes inspirations from <em>a la</em> “instance optimal” bounds for identity testing. Authors show a fairly general result which (yet again) confirms that the quantum world is indeed weird. In particular, the main result of the paper implies that the copy complexity of (the quantum analog of) identity testing in the quantum world (with non-adaptive queries) grows as \(\Theta(d^{1/2}/\varepsilon^2)\). That is, the number of quantum measurements you need increases with \(d\) (which is the stark opposite of the behavior you get in the classical world).</p></div>
    </content>
    <updated>2021-03-06T17:50:44Z</updated>
    <published>2021-03-06T17:50:44Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Akash</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-03-06T22:47:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.03238</id>
    <link href="http://arxiv.org/abs/2103.03238" rel="alternate" type="text/html"/>
    <title>On the Complexity of Equilibrium Computation in First-Price Auctions</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filos=Ratsikas:Aris.html">Aris Filos-Ratsikas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giannakopoulos:Yiannis.html">Yiannis Giannakopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hollender:Alexandros.html">Alexandros Hollender</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lazos:Philip.html">Philip Lazos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Po=ccedil=as:Diogo.html">Diogo Poças</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.03238">PDF</a><br/><b>Abstract: </b>We consider the problem of computing a (pure) Bayes-Nash equilibrium in the
first-price auction with continuous value distributions and discrete bidding
space. We prove that when bidders have independent subjective prior beliefs
about the value distributions of the other bidders, computing an
$\varepsilon$-equilibrium of the auction is PPAD-complete, and computing an
exact equilibrium is FIXP-complete.
</p></div>
    </summary>
    <updated>2021-03-06T22:38:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.03228</id>
    <link href="http://arxiv.org/abs/2103.03228" rel="alternate" type="text/html"/>
    <title>One for One, or All for All: Equilibria and Optimality of Collaboration in Federated Learning</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blum:Avrim.html">Avrim Blum</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haghtalab:Nika.html">Nika Haghtalab</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Phillips:Richard_Lanas.html">Richard Lanas Phillips</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shao:Han.html">Han Shao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.03228">PDF</a><br/><b>Abstract: </b>In recent years, federated learning has been embraced as an approach for
bringing about collaboration across large populations of learning agents.
However, little is known about how collaboration protocols should take agents'
incentives into account when allocating individual resources for communal
learning in order to maintain such collaborations. Inspired by game theoretic
notions, this paper introduces a framework for incentive-aware learning and
data sharing in federated learning. Our stable and envy-free equilibria capture
notions of collaboration in the presence of agents interested in meeting their
learning objectives while keeping their own sample collection burden low. For
example, in an envy-free equilibrium, no agent would wish to swap their
sampling burden with any other agent and in a stable equilibrium, no agent
would wish to unilaterally reduce their sampling burden.
</p>
<p>In addition to formalizing this framework, our contributions include
characterizing the structural properties of such equilibria, proving when they
exist, and showing how they can be computed. Furthermore, we compare the sample
complexity of incentive-aware collaboration with that of optimal collaboration
when one ignores agents' incentives.
</p></div>
    </summary>
    <updated>2021-03-06T22:42:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.03035</id>
    <link href="http://arxiv.org/abs/2103.03035" rel="alternate" type="text/html"/>
    <title>Computing Subset Feedback Vertex Set via Leafage</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Papadopoulos:Charis.html">Charis Papadopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tzimas:Spyridon.html">Spyridon Tzimas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.03035">PDF</a><br/><b>Abstract: </b>Chordal graphs are characterized as the intersection graphs of subtrees in a
tree and such a representation is known as the tree model. Restricting the
characterization results in well-known subclasses of chordal graphs such as
interval graphs or split graphs. A typical example that behaves computationally
different in subclasses of chordal graph is the \textsc{Subset Feedback Vertex
Set} (SFVS) problem: given a graph $G=(V,E)$ and a set $S\subseteq V$, SFVS
asks for a minimum set of vertices that intersects all cycles containing a
vertex of $S$. SFVS is known to be polynomial-time solvable on interval graphs,
whereas SFVS remains \NP-complete on split graphs and, consequently, on chordal
graphs. Towards a better understanding of the complexity of SFVS on subclasses
of chordal graphs, we exploit structural properties of a tree model in order to
cope with the hardness of SFVS. Here we consider variants of the \emph{leafage}
that measures the minimum number of leaves in a tree model. We show that SFVS
can be solved in polynomial time for every chordal graph with bounded leafage.
In particular, given a chordal graph on $n$ vertices with leafage $\ell$, we
provide an algorithm for SFVS with running time $n^{O(\ell)}$. Pushing further
our positive result, it is natural to consider a slight generalization of
leafage, the \emph{vertex leafage}, which measures the smallest number among
the maximum number of leaves of all subtrees in a tree model. However, we show
that it is unlikely to obtain a similar result, as we prove that SFVS remains
\NP-complete on undirected path graphs, i.e., graphs having vertex leafage at
most two. Moreover, we strengthen previously-known polynomial-time algorithm
for SFVS on directed path graphs that form a proper subclass of undirected path
graphs and graphs of mim-width one.
</p></div>
    </summary>
    <updated>2021-03-06T22:41:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.02980</id>
    <link href="http://arxiv.org/abs/2103.02980" rel="alternate" type="text/html"/>
    <title>Construction of approximate $C^1$ bases for isogeometric analysis on two-patch domains</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Pascal Weinmüller, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takacs:Thomas.html">Thomas Takacs</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02980">PDF</a><br/><b>Abstract: </b>In this paper, we develop and study approximately smooth basis constructions
for isogeometric analysis over two-patch domains. One key element of
isogeometric analysis is that it allows high order smoothness within one patch.
However, for representing complex geometries, a multi-patch construction is
needed. In this case, a $C^0$-smooth basis is easy to obtain, whereas
$C^1$-smooth isogeometric functions require a special construction. Such spaces
are of interest when solving numerically fourth-order PDE problems, such as the
biharmonic equation and the Kirchhoff-Love plate or shell formulation, using an
isogeometric Galerkin method.
</p>
<p>With the construction of so-called analysis-suitable $G^1$ (in short,
AS-$G^1$) parametrizations, as introduced in (Collin, Sangalli, Takacs; CAGD,
2016), it is possible to construct $C^1$ isogeometric spaces which possess
optimal approximation properties. These geometries need to satisfy certain
constraints along the interfaces and additionally require that the regularity
$r$ and degree $p$ of the underlying spline space satisfy $1 \leq r \leq p-2$.
The problem is that most complex geometries are not AS-$G^1$ geometries.
Therefore, we define basis functions for isogeometric spaces by enforcing
approximate $C^1$ conditions following the basis construction from (Kapl,
Sangalli, Takacs; CAGD, 2017). For this reason, the defined function spaces are
not exactly $C^1$ but only approximately.
</p>
<p>We study the convergence behavior and define function spaces that converge
optimally under $h$-refinement, by locally introducing functions of higher
polynomial degree and lower regularity. The convergence rate is optimal in
several numerical tests performed on domains with non-trivial interfaces. While
an extension to more general multi-patch domains is possible, we restrict
ourselves to the two-patch case and focus on the construction over a single
interface.
</p></div>
    </summary>
    <updated>2021-03-06T22:45:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.02972</id>
    <link href="http://arxiv.org/abs/2103.02972" rel="alternate" type="text/html"/>
    <title>Weisfeiler--Leman, Graph Spectra, and Random Walks</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rattan:Gaurav.html">Gaurav Rattan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seppelt:Tim.html">Tim Seppelt</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02972">PDF</a><br/><b>Abstract: </b>The Weisfeiler--Leman algorithm is a ubiquitous tool for the Graph
Isomorphism Problem with various characterisations in e.g. descriptive
complexity and convex optimisation. It is known that graphs that are not
distinguished by the two-dimensional variant have cospectral adjacency
matrices. We tackle a converse problem by proposing a set of matrices called
Generalised Laplacians that characterises the expressiveness of WL in terms of
spectra. As an application to random walks, we show using Generalised
Laplacians that the edge colours produced by 2-WL determine commute distances.
</p></div>
    </summary>
    <updated>2021-03-06T22:42:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.02939</id>
    <link href="http://arxiv.org/abs/2103.02939" rel="alternate" type="text/html"/>
    <title>Quad layouts with high valence singularities for flexible quad meshing</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Jovana Jezdimirovi\' c, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chemin:Alexandre.html">Alexandre Chemin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reberol:Maxence.html">Maxence Reberol</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henrotte:Fran=ccedil=ois.html">François Henrotte</a>, Jean François Remacle <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02939">PDF</a><br/><b>Abstract: </b>A novel algorithm that produces a quad layout based on imposed set of
singularities is proposed. In this paper, we either use singularities that
appear naturally, e.g., by minimizing Ginzburg-Landau energy, or use as an
input user-defined singularity pattern, possibly with high valence
singularities that do not appear naturally in cross-field computations. The
first contribution of the paper is the development of a formulation that allows
computing a cross-field from a given set of singularities through the
resolution of two linear PDEs. A specific mesh refinement is applied at the
vicinity of singularities to accommodate the large gradients of cross
directions that appear in the vicinity of singularities of high valence. The
second contribution of the paper is a correction scheme that repairs limit
cycles and/or non-quadrilateral patches. Finally, a high quality
block-structured quad mesh is generated from the quad layout and per-partition
parameterization.
</p></div>
    </summary>
    <updated>2021-03-06T22:44:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.02936</id>
    <link href="http://arxiv.org/abs/2103.02936" rel="alternate" type="text/html"/>
    <title>On subgraph complementation to H-free graphs</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Dhanyamol Antony, Jay Garchar, Sagartanu Pal, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sandeep:R=_B=.html">R. B. Sandeep</a>, Sagnik Sen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Subashini:R=.html">R. Subashini</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02936">PDF</a><br/><b>Abstract: </b>For a class $\mathcal{G}$ of graphs, the problem SUBGRAPH COMPLEMENT TO
$\mathcal{G}$ asks whether one can find a subset $S$ of vertices of the input
graph $G$ such that complementing the subgraph induced by $S$ in $G$ results in
a graph in $\mathcal{G}$. We investigate the complexity of the problem when
$\mathcal{G}$ is $H$-free for $H$ being a complete graph, a star, a path, or a
cycle. We obtain the following results:
</p>
<p>- When $H$ is a $K_t$ (a complete graph on $t$ vertices) for any fixed $t\geq
1$, the problem is solvable in polynomial-time. This applies even when
$\mathcal{G}$ is a subclass of $K_t$-free graphs recognizable in
polynomial-time, for example, the class of $(t-2)$-degenerate graphs.
</p>
<p>- When $H$ is a $K_{1,t}$ (a star graph on $t+1$ vertices), we obtain that
the problem is NP-complete for every $t\geq 5$. This, along with known results,
leaves only two unresolved cases - $K_{1,3}$ and $K_{1,4}$.
</p>
<p>- When $H$ is a $P_t$ (a path on $t$ vertices), we obtain that the problem is
NP-complete for every $t\geq 7$, leaving behind only two unresolved cases -
$P_5$ and $P_6$.
</p>
<p>- When $H$ is a $C_t$ (a cycle on $t$ vertices), we obtain that the problem
is NP-complete for every $t\geq 8$, leaving behind four unresolved cases -
$C_4, C_5, C_6,$ and $C_7$.
</p>
<p>Further, we prove that these hard problems do not admit subexponential-time
algorithms (algorithms running in time $2^{o(|V(G)|)}$), assuming the
Exponential Time Hypothesis. A simple complementation argument implies that
results for $\mathcal{G}$ are applicable for $\overline{\mathcal{G}}$, thereby
obtaining similar results for $H$ being the complement of a complete graph, a
star, a path, or a cycle. Our results generalize two main results and resolve
one open question by Fomin et al. (Algorithmica, 2020).
</p></div>
    </summary>
    <updated>2021-03-06T22:39:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.02919</id>
    <link href="http://arxiv.org/abs/2103.02919" rel="alternate" type="text/html"/>
    <title>A Simple Algorithm for the Constrained Sequence Problems</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Francis Yuk Lun Chin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Ho:Ngai_Lam.html">Ngai Lam Ho</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Santis:Alfredo_De.html">Alfredo De Santis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:S=_K=.html">S. K. Kim</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02919">PDF</a><br/><b>Abstract: </b>In this paper we address the constrained longest common subsequence problem.
Given two sequences $X$, $Y$ and a constrained sequence $P$, a sequence $Z$ is
a constrained longest common subsequence for $X$ and $Y$ with respect to $P$ if
$Z$ is the longest subsequence of $X$ and $Y$ such that $P$ is a subsequence of
$Z$. Recently, Tsai \cite{Tsai} proposed an $O(n^2 \cdot m^2 \cdot r)$ time
algorithm to solve this problem using dynamic programming technique, where $n$,
$m$ and $r$ are the lengths of $X$, $Y$ and $P$, respectively. In this paper,
we present a simple algorithm to solve the constrained longest common
subsequence problem in $O(n \cdot m \cdot r)$ time and show that the
constrained longest common subsequence problem is equivalent to a special case
of the constrained multiple sequence alignment problem which can also be
solved.
</p></div>
    </summary>
    <updated>2021-03-06T22:39:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.02916</id>
    <link href="http://arxiv.org/abs/2103.02916" rel="alternate" type="text/html"/>
    <title>Consensus in Blockchain Systems with Low Network Throughput: A Systematic Mapping Study</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Henrik Knudsen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Notland:Jakob_Svennevik.html">Jakob Svennevik Notland</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haro:Peter_Halland.html">Peter Halland Haro</a>, Truls Bakkejord Ræder, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jingyue.html">Jingyue Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02916">PDF</a><br/><b>Abstract: </b>Blockchain technologies originate from cryptocurrencies. Thus, most
blockchain technologies assume an environment with a fast and stable network.
However, in some blockchain-based systems, e.g., supply chain management (SCM)
systems, some Internet of Things (IOT) nodes can only rely on the low-quality
network sometimes to achieve consensus. Thus, it is critical to understand the
applicability of existing consensus algorithms in such environments. We
performed a systematic mapping study to evaluate and compare existing consensus
mechanisms' capability to provide integrity and security with varying network
properties. Our study identified 25 state-of-the-art consensus algorithms from
published and preprint literature. We categorized and compared the consensus
algorithms qualitatively based on established performance and integrity metrics
and well-known blockchain security issues. Results show that consensus
algorithms rely on the synchronous network for correctness cannot provide the
expected integrity. Such consensus algorithms may also be vulnerable to
distributed-denial-of-service (DDOS) and routing attacks, given limited network
throughput. Conversely, asynchronous consensus algorithms, e.g.,
Honey-BadgerBFT, are deemed more robust against many of these attacks and may
provide high integrity in asynchrony events.
</p></div>
    </summary>
    <updated>2021-03-06T22:37:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.02914</id>
    <link href="http://arxiv.org/abs/2103.02914" rel="alternate" type="text/html"/>
    <title>The Bounded Acceleration Shortest Path problem: complexity and solution algorithms</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Stefano Ardizzoni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Consolini:Luca.html">Luca Consolini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Laurini:Mattia.html">Mattia Laurini</a>, Marco Locatelli <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02914">PDF</a><br/><b>Abstract: </b>The purpose of this work is to introduce and characterize the Bounded
Acceleration Shortest Path (BASP) problem, a generalization of the Shortest
Path (SP) problem. This problem is associated to a graph: the nodes represent
positions of a mobile vehicle and the arcs are associated to pre-assigned
geometric paths that connect these positions. BASP consists in finding the
minimum-time path between two nodes. Differently from SP, we require that the
vehicle satisfy bounds on maximum and minimum acceleration and speed, that
depend on the vehicle position on the currently traveled arc. We prove that
BASP is NP-hard and define solution algorithm that achieves polynomial
time-complexity under some additional hypotheses on problem data.
</p></div>
    </summary>
    <updated>2021-03-06T22:38:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.02749</id>
    <link href="http://arxiv.org/abs/2103.02749" rel="alternate" type="text/html"/>
    <title>Introduction to Periodic Geometry and Topology</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Olga Anosova, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurlin:Vitaliy.html">Vitaliy Kurlin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02749">PDF</a><br/><b>Abstract: </b>This paper introduces the key concepts and problems of the new research area
of Periodic Geometry and Topology for applications in Materials Science.
Periodic structures such as solid crystalline materials or textiles were
previously studied as isolated structures without taking into account the
continuity of their configuration spaces. The key new problem in Periodic
Geometry is an isometry classification of periodic point sets. A required
complete invariant should continuously change under point perturbations,
because atoms always vibrate in real crystals. The main objects of Periodic
Topology are embeddings of curves in a thickened plane that are invariant under
lattice translations. Such periodic knots were classified in the past up to
continuous deformations (isotopies) that keep a fixed lattice structure, hence
are realized in a fixed thickened torus. The more practical equivalence is a
periodic isotopy in a thickened plane without fixing a lattice basis. The paper
states the first results in the new area and proposes further problems and
directions.
</p></div>
    </summary>
    <updated>2021-03-06T22:45:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.02605</id>
    <link href="http://arxiv.org/abs/2103.02605" rel="alternate" type="text/html"/>
    <title>On Fast Computation of a Circulant Matrix-Vector Product</title>
    <feedworld_mtime>1614988800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosowski:Andreas.html">Andreas Rosowski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02605">PDF</a><br/><b>Abstract: </b>This paper deals with circulant matrices. It is shown that a circulant matrix
can be multiplied by a vector in time O(n log(n)) in a ring with roots of unity
without making use of an FFT algorithm. With our algorithm we achieve a speedup
of a factor of about 2.25 for the multiplication of two polynomials with
integer coefficients compared to multiplication by an FFT algorithm. Moreover
this paper discusses multiplication of large integers as further application.
</p></div>
    </summary>
    <updated>2021-03-06T22:39:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/032</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/032" rel="alternate" type="text/html"/>
    <title>TR21-032 |  Fiat-Shamir via List-Recoverable Codes (or: Parallel Repetition of GMW is not Zero-Knowledge) | 

	Ron Rothblum, 

	Justin Holmgren, 

	Alex Lombardi</title>
    <summary>Shortly after the introduction of zero-knowledge proofs, Goldreich, Micali and Wigderson (CRYPTO '86) demonstrated their wide applicability by constructing  zero-knowledge proofs for the NP-complete problem of graph 3-coloring. A long-standing open question has been whether parallel repetition of their protocol preserves zero knowledge. In this work, we answer this question in the negative, assuming a a standard cryptographic assumption (i.e., the hardness of learning with errors (LWE)).

Leveraging a connection observed by Dwork, Naor, Reingold, and Stockmeyer (FOCS '99), our negative result is obtained by making positive progress on a related fundamental problem in cryptography: securely instantiating the Fiat-Shamir heuristic for eliminating interaction in public-coin interactive protocols. A recent line of works has shown how to instantiate the heuristic securely, albeit only for a limited class of protocols.

Our main result shows how to instantiate Fiat-Shamir for parallel repetitions of much more general interactive proofs. In particular, we construct hash functions that, assuming LWE, securely realize the Fiat-Shamir transform for the following rich classes of protocols:

- The parallel repetition of any ``commit-and-open'' protocol (such as the GMW protocol mentioned above), when a specific (natural) commitment scheme is used.  Commit-and-open protocols are a ubiquitous paradigm for constructing general purpose public-coin zero knowledge proofs.

- The parallel repetition of any base protocol that (1) satisfies a stronger notion of soundness called round-by-round soundness, and (2) has an efficient procedure, using a suitable trapdoor, for recognizing ``bad verifier randomness'' that would allow the prover to cheat.

Our results are obtained by establishing a new connection between the Fiat-Shamir transform and  list-recoverable codes.  In contrast to the usual focus in coding theory, we focus on a parameter regime in which the input lists are extremely large, but the rate can be small.  We give a (probabilistic) construction based on Parvaresh-Vardy codes (FOCS '05) that suffices for our applications.</summary>
    <updated>2021-03-05T23:22:09Z</updated>
    <published>2021-03-05T23:22:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-07T01:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/031</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/031" rel="alternate" type="text/html"/>
    <title>TR21-031 |  Upper Bound for Torus Polynomials | 

	Vaibhav Krishan</title>
    <summary>We prove that all functions that have low degree torus polynomials approximating them with small error also have $MidBit^+$ circuits computing them. This serves as a partial converse to the result that all $ACC$ functions have low degree torus polynomials approximating them with small error, by Bhrushundi, Hosseini, Lovett and Rao (ITCS 2019).</summary>
    <updated>2021-03-05T14:14:09Z</updated>
    <published>2021-03-05T14:14:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-07T01:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18238</id>
    <link href="https://rjlipton.wordpress.com/2021/03/04/wsj-meets-group-algorithms/" rel="alternate" type="text/html"/>
    <title>WSJ Meets Group Algorithms</title>
    <summary>Our whole life is solving puzzles. — Ernő Rubik Cropped from source Jessica Fridrich is a Distinguished Professor of Electrical and Computer Engineering at Binghamton University. She is an expert on data hiding, that is, steganography. She has over 34,000 citations—impressive. A lot more than most of us. She also has worked on the famous […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Our whole life is solving puzzles. — Ernő Rubik</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/03/jf-1.png"><img alt="" class="alignright wp-image-18244" height="120" src="https://rjlipton.files.wordpress.com/2021/03/jf-1.png?w=163&amp;h=120" width="163"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="http://www.ws.binghamton.edu/fridrich/pressconnects_com%20%2009-11-03%20%20News%20Story.htm">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Jessica Fridrich is a Distinguished Professor of Electrical and Computer Engineering at Binghamton University. She is an expert on data hiding, that is, <a href="https://en.wikipedia.org/wiki/Steganography">steganography</a>. She has over 34,000 citations—impressive. A lot more than most of us. She also has <a href="http://www.ws.binghamton.edu/fridrich/cube.html">worked</a> on the famous Rubik’s cube.</p>
<p>
Today we look at her work on Rubik’s cube, the WSJ’s interest in Rubik’s cube, and what both say—and don’t say—about fundamental algorithms.</p>
<p>
By the way, WSJ stands for the Wall Street Journal—the American <a href="https://en.wikipedia.org/wiki/The_Wall_Street_Journal">newspaper</a> of business. The WSJ has shown great interest in the Rubik’s cube puzzle and has run many articles over the years on it.</p>
<p>
Recall the cube puzzle was invented…of course you know all about Rubik’s cube. You probably have owned one at one time. Right. Just for a <a href="https://en.wikipedia.org/wiki/Rubik%27s_Cube">refresher</a>: </p>
<blockquote><p><b> </b> <em> The Rubik’s Cube is a 3-D combination puzzle invented in 1974 by Hungarian sculptor and professor of architecture Ernő Rubik. As of January 2009, 350 million cubes had been sold worldwide, making it the world’s top-selling puzzle game. It is widely considered to be the world’s best-selling toy. </em>
</p></blockquote>
<p>
But you may not know all about Fridrich.</p>
<p/><h2> Speed Solving </h2><p/>
<p>
Fridrich was one of the progenitors of <em>speed cubing</em>. She took part in the First World Championship in 1982 in Budapest, next-door to her native Czechoslovakia. She finished in the middle of the pack with a time of <b>29.11</b> seconds from a randomly well-mixed starting cube position. Her thoughts on how the cubes could be better prepared for speed are recorded on her <a href="http://www.ws.binghamton.edu/fridrich/cubewrld.html">page</a> about the tournament.</p>
<p>
At the Second World Championship, she improved her average time to <b>20.48</b> seconds and placed <a href="https://www.worldcubeassociation.org/competitions/WC2003">2nd</a>. She had the two fastest solves in the finals but lost on average-of-median-three-of-five.  That championship took place in Toronto—in <b>2003</b>. She is at a loss to explain why there was such a gap. Usually an athlete—in this case a mathlete?—is on the downswing nearing age 40, but even as a self-described “<a href="http://ws2.binghamton.edu/fridrich/history.html">old-timer</a>,” she fended off all but one of a whole next generation. </p>
<p>
Much of the credit goes to her solving method. She originated the “O” and “P” parts of the <a href="https://en.wikipedia.org/wiki/CFOP_method">CFOP</a> method. CFOP stands for: Cross, First 2 Layers, Orient Last Layer, Permute Last Layer. Versions of this are used my most top “cubers” to this day, and her name is often affixed to the method. In a 2008 profile of her, the New York Times <a href="https://www.nytimes.com/2008/12/16/science/16prof.html?_r=1&amp;em">quoted</a> the 2003 winner as saying that Fridrich found the route up the mountain while the rest of the cubers optimize traversing ledges along it. And in 2012, the NYT <a href="https://london2012.blogs.nytimes.com/2012/06/25/master-of-the-shot-put-and-the-cube/?searchResultPosition=5">quoted</a> Olympic shot-putter Reese Hoffa as wanting “to learn the Fridrich Method of solving the puzzle, ‘which is what all of the best cubers use.'” </p>
<p>
At this point, knowing our interest in chess, you might expect a <a href="https://en.wikipedia.org/wiki/The_Queen's_Gambit_(novel)"><i>Queen’s</i></a> <a href="https://en.wikipedia.org/wiki/The_Queen's_Gambit_(miniseries)"><i>Gambit</i></a> reference. But what we have here is not a story of Beth Harmon coming back from a life <a href="https://www.thereviewgeek.com/thequeensgambit-e6review/">adjournment</a> or Roy Hobbs in <a href="https://en.wikipedia.org/wiki/The_Natural"><i>The</i></a> <a href="https://en.wikipedia.org/wiki/The_Natural_(film)"><i>Natural</i></a> rejoining baseball almost 20 years after being shot. It’s about going overseas, earning a PhD, getting two research positions, writing early papers (under the <a href="https://dblp.org/pid/29/4038.html">name</a> Jiri Fridrich), transitioning, then getting a faculty position leading to tenure while developing mathematical formulas and writing tons of code for systems to <a href="https://www.nytimes.com/2004/07/22/technology/what-s-next-for-doctored-photos-a-new-flavor-of-digital-truth-serum.html?searchResultPosition=16">source</a> photos and catch digital pirates and pornographers and other image fraudsters, then coming back to light up an <img alt="{8 \times 8}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or <img alt="{3 \times 3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> universe. Not to mention doing her own stunning <a href="https://www.jessicafridrich.com/">photo art</a> of the American Southwest.</p>
<p/><h2> Quicker Times and Cubes </h2><p/>
<p>
Since 2003, the <a href="https://en.wikipedia.org/wiki/Speedcubing#Competitions">championships</a> have been held every other year, thought the 2021 championships set for the Netherlands are uncertain owing to the pandemic. The youngsters soon broke through en-masse, and it strikes me that the cube technology improved so that the cubes are springier and lighter. The winning time fell almost 5 seconds to <b>15.10</b> in 2005 and hit <b>6.74</b> in 2019. That was not the world record, however—an incredible <b>3.47</b> seconds in 2018 by Yusheng Du, beating the previous record of Feliks Zemdegs by a whopping 3/4 of a second.</p>
<p>
Fridrich, however, must claim a distinction no one may ever match. She learned how to solve the cube and traced out the performance of methods of doing so in 1981, months before she saw a cube, let alone owned one. Despite the “Bűvös Kocka” (“Magic Cube,” as Rubik called it) having been on shelves in neighboring Hungary for four years, with worldwide marketing by early 1980, they were hard to come by in her home city, Ostrava. </p>
<p>
She found an article on solving the cube in a Russian magazine. It laid out the concept of group theory and the role of group commutators, which she learned to apply creatively in order to streamline actions. The first time she touched a cube was to help a friend put his back the way it was. A family visiting from France let her keep one, and later in 1981 she was finally able to purchase a few more. This invites analogy to working out chess without a board on a bedroom ceiling as depicted in <em>The Queen’s Gambit</em>.</p>
<p>
We—Dick and Ken—must admit that neither of us has ever done this with the cubes we own, not fast, not slow. Yet we do understand the theory behind it. We believe we do. </p>
<p/><h2> Group Theory of the Cube </h2><p/>
<p>
I (Dick) plan on explaining the theory by using a new toy that I have invented: The <img alt="{\mathit{slider}^{TM}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathit%7Bslider%7D%5E%7BTM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. 	</p>
<p><a href="https://rjlipton.files.wordpress.com/2021/03/slidercubes.png"><img alt="" class="aligncenter wp-image-18247" height="45" src="https://rjlipton.files.wordpress.com/2021/03/slidercubes.png?w=96&amp;h=45" width="96"/></a></p>
<p>
We will write the state as <img alt="{xyz}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bxyz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where each of <img alt="{x,y,z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is one of <font color="red">1</font>, <font color="green">2</font>, or <font color="blue">3</font>. The operations allowed are the <i>cyclic shift</i> <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which does 	</p>
<p align="center"><img alt="\displaystyle  xyz \rightarrow zxy, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++xyz+%5Crightarrow+zxy%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>and the <i>flip</i> <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of the initial two elements: 	</p>
<p align="center"><img alt="\displaystyle  xyz \rightarrow yxz. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++xyz+%5Crightarrow+yxz.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
Note there are 6 possible states. For the real Rubik’s cube, the number of states is just a little bit larger: <b>43,252,003,274,489,856,000</b>. But the basic concept is the same. Suppose we are given the state 	</p>
<p align="center"><img alt="\displaystyle  132. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++132.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>How fast can you get the initial state <img alt="{123}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B123%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? Apply <img alt="{FCC}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BFCC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: 	</p>
<p align="center"><img alt="\displaystyle  132 \rightarrow 312 \rightarrow 231 \rightarrow 123 . " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++132+%5Crightarrow+312+%5Crightarrow+231+%5Crightarrow+123+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
This is a special case of the general <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/genset.pdf">result</a> that any symmetric group is <a href="https://groupprops.subwiki.org/wiki/Symmetric_group_on_a_finite_set_is_2-generated">generated</a> by two operations: a full cycle and a single flip. The key with the actual Rubik’s cube is since the group is larger and it has more operations that can be applied finding the group operations may be more difficult. But there are algorithms that can find them. See <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/gpaction.pdf">this</a> for another article by Keith Conrad. </p>
<p>
There are many more pages like that on the cube. But Fridrich still shows the <a href="http://www.ws.binghamton.edu/fridrich/system.html">seminal page</a> she posted in “Winter 1996/97.” It links to other pages, ones that also credit other people, such as <a href="http://www.ws.binghamton.edu/fridrich/Mike/middle.html">this</a> explaining the algorithms in great pictorial detail. This was in the infancy of the Internet. Her pages are often credited with spurring the turn-of-the-millennium boom in Rubik’s cube which led to the revival of the championships in 2003. A 2016 New York Post <a href="https://nypost.com/2016/10/31/how-the-internet-brought-the-rubiks-cube-back-to-life/">article</a> whose URL is titled, “how the Internet brought the Rubik’s cube back to life,” says: </p>
<blockquote><p><b> </b> <em> The seeds for Rubik’s Cube’s rediscovery were sown on the internet. In the mid-1990s, a Rubik’s Cube champion-turned-computer-science professor at SUNY Binghamton posted her secrets of the Cube on a primitive Web 1.0 site on the university’s servers. Jessica Fridrich’s method spread and is today the most widely used technique to solve the puzzle. </em>
</p></blockquote>
<p>
See also this <a href="https://uncletyson.wordpress.com/tag/dan-knights/">telling</a> by the 2003 winner, Dan Knights. This shows how one person using spare time on the Internet can power up business.</p>
<p/><h2> Enter the WSJ </h2><p/>
<p>
The WSJ has had an interest in Rubik’s cube for years. They had a long feature <a href="https://www.wsj.com/articles/how-to-teach-professors-humility-hand-them-a-rubiks-cube-11614352261">article</a> last week titled, “How to Teach Professors Humility? Hand Them a Rubik’s Cube,” by Melissa Korn. It describes a faculty development challenge among several small colleges in which professors became students again. Last month they also had an <a href="https://www.wsj.com/articles/seeing-things-with-the-power-of-symmetry-11612461325">article</a> on symmetry by the mathematician Eugenia Cheng that mentioned the cube.</p>
<p>
I recall several features the WSJ has run on the cube and its solvers. The 2011 <a href="https://www.wsj.com/articles/SB10001424052970204319004577088513615125328">article</a>, “One Cube, Many Knockoffs, Quintillions of Possibilities,” led off with the Polish teenager Michal Pleskowicz winning the 2011 world championship with a time of <b>8.65</b> seconds, then discussed the performance of pirated cubes: “One reason Mr. Pleskowicz and a new generation of Rubik’s fanatics can solve the notoriously difficult puzzle in record time: They don’t use Rubik’s Cubes at all, instead substituting souped-up Chinese knockoffs engineered for speed…” Their 2014 <a href="https://www.wsj.com/articles/SB10001424052702304518704579523513594900696">article</a>, “Rubik’s Cube Proves It’s Hip to Be Square,” profiled both Rubik and speed-solvers. </p>
<p>
The <a href="https://www.wsj.com/articles/a-thinking-persons-guide-to-the-rubiks-cube-1517586702">feature</a> I recall best was in 2018. It was titled, “A Thinking Person’s Guide to the Rubik’s Cube,” and subtitled, “What’s the best solution method—theory, algorithms or chance?” It was also by Eugenia Cheng. She begins by confessing, “I have always loved playing with a Rubik’s Cube, which combines logic with a satisfying tactile activity. I can solve it—getting each of the six sides to be one color—but not particularly quickly or cleverly.” </p>
<p>
They also like its use for analogies. Scrolling through their advanced search—both Ken and I subscribe to the WSJ—we find:</p>
<ul>
<li><a href="https://www.wsj.com/articles/close-reopen-repeat-restaurants-dont-know-what-covid-19-will-dish-out-next-11613138412">2/12/21</a>: “Running restaurants is now ‘a bit of a Rubik’s Cube,’ said Mr. Mosier, who reopened his casual cafes in late January.”
</li><li><a href="https://www.wsj.com/articles/reopening-schools-is-so-complicated-new-york-struggles-to-schedule-classes-11597939473">8/20/20</a>, headline: “Reopening Schools Is So Complicated, New York Is Struggling to Schedule Classes Nation’s largest district is still hashing out basic details about the school day; ‘a multidimensional Rubik’s Cube’ of challenges.”
</li><li><a href="https://www.wsj.com/articles/new-u-s-rules-on-foreign-students-put-universitiesin-dilemma-11594149280">7/7/20</a>: “The new [pandemic] rules have created a Rubik’s Cube of decisions for schools, which face unique challenges with each of their international student populations.”
</li><li><a href="https://www.wsj.com/articles/an-l-a-home-asking-62-million-includes-a-playful-perk-a-model-racetrack-11590523214">5/26/20</a>, about a home selling for $62 million: “Designed by Seattle-based architecture firm Olson Kundig, the house has interlocking boxes and planes resembling a Rubik’s cube…”
</li><li><a href="https://www.wsj.com/articles/president-trump-announces-19-billion-relief-program-for-farmers-11587165759">4/17/20</a>, quoting Agriculture Secretary Sonny Perdue on the coronavirus relief program for farmers: “It will be a logistical Rubik’s Cube.”
</li><li><a href="https://www.wsj.com/articles/he-wanted-something-more-from-retirement-so-he-got-three-jobs-11573743922">11/14/19</a>, about a retiree who started teaching business classes, keeping books for a non-profit business, and working on a ferry dock: “My society consists of able-bodied seamen, boat captains, truckers hauling bait and lobsters, fishermen, islanders and wide-eyed vacationers,” says Mr. Marshall. It’s “a constant Rubik’s cube. You never know what you’ll find.”
</li></ul>
<p>
In all, using the WSJ advanced search, we find 239 hits for “Rubik” going back to 1980. We should mention in-passing that one of them is their 7/17/20 <a href="https://www.wsj.com/articles/ron-graham-dazzled-admirers-with-math-and-juggling-feats-11594994403">obituary</a> for Ron Graham. We also find 7 hits for “Fridrich” over the same span. But they are all about the housing market, involving the Nashville-based realty Fridrich and Clarke.</p>
<p/><h2> Open Problems </h2><p/>
<p>
I am happy to see that the WSJ has published multiple articles on a particular algorithmic task. I like that algorithms have been the center of articles. I wish they would talk more about important algorithms. Solving a Rubik’s cube is not an algorithm that is used every day: What about: </p>
<ul>
<li>Sorting
</li><li>Searching
</li><li>Dynamic Programming
</li><li>Fast Arithmetic
</li></ul>
<p>They do have Eugenia Cheng, who wrote a <a href="https://www.wsj.com/articles/algorithms-arent-just-for-computers-11557407055">column</a> comparing sorting algorithms. And they have written on algorithms used in <a href="https://www.wsj.com/graphics/journey-inside-a-real-life-trading-algorithm/">trading</a> and on <a href="https://www.wsj.com/articles/social-media-algorithms-rule-how-we-see-the-world-good-luck-trying-to-stop-them-11610884800">social</a>–<a href="https://www.wsj.com/articles/how-google-interferes-with-its-search-algorithms-and-changes-your-results-11573823753">media</a> <a href="https://www.wsj.com/articles/how-to-win-friends-and-influence-algorithms-11555246800">platforms</a> and for <a href="https://www.wsj.com/articles/algorithms-used-in-policing-face-policy-review-11591003801">policing</a> and <a href="https://www.wsj.com/articles/SB10001424052702304626104579121251595240852">parole</a> and <a href="https://www.wsj.com/articles/algorithm-helps-new-york-decide-who-goes-free-before-trial-11600610400">bail</a> decisions. But that tends away from <em>fundamental algorithms</em> where the math is the matter.</p>
<p>
A 2018 WSJ <a href="https://www.wsj.com/articles/dont-believe-the-algorithm-1536157620">article</a> by Hannah Fry titled “Don’t Believe the Algorithm,” which begins with flaws in using facial recognition to find wanted suspects, brings us back toward Fridrich’s research. Might this all also raise discussion of “algorithms” for what and whom to cover?</p>
<p>
[fixed name at end]</p></font></font></div>
    </content>
    <updated>2021-03-05T00:37:41Z</updated>
    <published>2021-03-05T00:37:41Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Results"/>
    <category term="Algorithms"/>
    <category term="competitions"/>
    <category term="Jessica Fridrich"/>
    <category term="photography"/>
    <category term="puzzles"/>
    <category term="Rubik's Cube"/>
    <category term="science and society"/>
    <category term="speed cubing"/>
    <category term="steganography"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-03-07T01:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5359</id>
    <link href="https://www.scottaaronson.com/blog/?p=5359" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5359#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5359" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Zen Anti-Interpretation of Quantum Mechanics</title>
    <summary xml:lang="en-US">As I lay bedridden this week, knocked out by my second dose of the Moderna vaccine, I decided I should blog some more half-baked ideas because what the hell? It feels therapeutic, I have tenure, and anyone who doesn’t like it can close their broswer tab. So: although I’ve written tens of thousands of words, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>As I lay bedridden this week, knocked out by my second dose of the Moderna vaccine, I decided I should blog some more half-baked ideas because what the hell?  It feels therapeutic, I have tenure, and anyone who doesn’t like it can close their broswer tab.</p>



<p>So: although I’ve written tens of thousands <a href="https://www.pbs.org/wgbh/nova/article/can-quantum-computing-reveal-the-true-meaning-of-quantum-mechanics/">of</a> <a href="https://arxiv.org/abs/1306.0159">words</a>, <a href="https://www.scottaaronson.com/papers/philos.pdf">on</a> <a href="https://www.scottaaronson.com/blog/?p=1103">this</a> <a href="https://www.scottaaronson.com/blog/?p=3628">blog</a> <a href="https://www.scottaaronson.com/democritus/">and</a> <a href="https://www.scottaaronson.com/qclec.pdf">elsewhere</a>, about interpretations of quantum mechanics, again and again I’ve dodged the question of which interpretation (if any) I <em>really believe myself</em>.  Today, at last, I’ll emerge from the shadows and tell you precisely where I stand.</p>



<p>I hold that all interpretations of QM are just crutches that are better or worse at helping you along to the Zen realization that <strong>QM is what it is and doesn’t need an interpretation</strong>.  As Sidney Coleman <a href="https://arxiv.org/abs/2011.12671">famously argued</a>, what needs reinterpretation is not QM itself, but all our <em>pre</em>-quantum philosophical baggage—the baggage that leads us to demand, for example, that a wavefunction |ψ⟩ either be “real” like a stubbed toe or else “unreal” like a dream.  Crucially, because this philosophical baggage differs somewhat from person to person, the “best” interpretation—meaning, the one that leads most quickly to the desired Zen state—can also differ from person to person.  Meanwhile, though, thousands of physicists (and chemists, mathematicians, quantum computer scientists, etc.) have approached the Zen state merely by spending decades working with QM, never worrying much about interpretations at all.  This is probably the truest path; it’s just that most people lack the inclination, ability, or time.</p>



<p>Greg Kuperberg, one of the smartest people I know, once told me that the problem with the Many-Worlds Interpretation is not that it says anything wrong, but only that it’s “melodramatic” and “overwritten.”  Greg is far along the Zen path, probably further than me.</p>



<p>You shouldn’t confuse the Zen Anti-Interpretation with “Shut Up And Calculate.”  The latter phrase, mistakenly attributed to Feynman but really due to David Mermin, is something one might say at the <em>beginning</em> of the path, when one is as a baby.  I’m talking here only about the <em>endpoint</em> of path, which one can approach but never reach—the endpoint where you intuitively understand exactly what a Many-Worlder, Copenhagenist, or Bohmian would say about any given issue, and also how they’d respond to each other, and how they’d respond to the responses, etc. but after years of study and effort you’ve <em>returned</em> to the situation of the baby, who just sees the thing for what it is.</p>



<p>I don’t mean to say that the interpretations are all interchangeable, or equally good or bad.  If you had to, you could call even me a “Many-Worlder,” but <em>only</em> in the following limited sense: that in fifteen years of teaching quantum information, my experience has consistently been that for <em>most</em> students, <a href="https://en.wikipedia.org/wiki/Many-worlds_interpretation">Everett’s crutch</a> is the best one currently on the market.  At any rate, it’s the one that’s the most like a straightforward <em>picture</em> of the equations, and the least like a wobbly tower of words that might collapse if you utter any wrong ones.  Unlike Bohr, Everett will never make you feel stupid for asking the questions an inquisitive child would ask; he’ll simply give you answers that are as clear, logical, and internally consistent as they are metaphysically extravagant.  That’s a start.</p>



<p>The <a href="https://en.wikipedia.org/wiki/Copenhagen_interpretation">Copenhagen Interpretation</a> retains a place of honor as the <em>first</em> crutch, for decades the <em>only</em> crutch, and the one closest to the spirit of positivism.  Unfortunately, <em>wielding</em> the Copenhagen crutch requires mad philosophical skillz—which parts of the universe should you temporarily regard as “classical”?  which questions should be answered, and which deflected?—to the point where, if you’re capable of all that verbal footwork, then why do you even <em>need</em> a crutch in the first place?  In the hands of amateurs—meaning, alas, nearly everyone—Copenhagen often leads <em>away</em> <em>from</em> rather than toward the Zen state, as one sees with the generations of New-Age bastardizations about “observations creating reality.”</p>



<p>As for <a href="https://en.wikipedia.org/wiki/De_Broglie%E2%80%93Bohm_theory">deBroglie-Bohm</a>—well, that’s a weird, interesting, baroque crutch, one whose actual details (the preferred basis and the guiding equation) are historically contingent and tied to specific physical systems.  It’s probably the right crutch for <em>someone</em>—it gets eternal credit for having led Bell to discover the Bell inequality—but its quirks definitely need to be discarded along the way.</p>



<p>Note that, among those who approach the Zen state, many might still call themselves Many-Worlders or Copenhagenists or Bohmians or whatever—just as those far along in spiritual enlightenment might still call themselves Buddhists or Catholics or Muslims or Jews (or atheists or agnostics)—even though, by that point, they might have more in common with each other than they do with their supposed coreligionists or co-irreligionists.</p>



<p>Alright, but isn’t all this Zen stuff just a way to dodge the <em>actual, substantive</em> questions about QM, by cheaply claiming to have transcended them?  If that’s your charge, then please help yourself to the following FAQ about the details of the Zen Anti-Interpretation.</p>



<ol><li><strong>What is a quantum state?</strong>  It’s a unit vector of complex numbers (or if we’re talking about mixed states, then a trace-1, Hermitian, positive semidefinite matrix), which encodes everything there is to know about a physical system.<br/></li><li><strong>OK, but are the quantum states “ontic” (really out in the world), or “epistemic” (only in our heads)?</strong>  Dude.  Do “basketball games” really exist, or is that just a phrase we use to summarize our knowledge about certain large agglomerations of interacting quarks and leptons?  Do even the “quarks” and “leptons” exist, or are those just words for excitations of the more fundamental fields?  Does “jealousy” exist?  Pretty much<em> all</em> our concepts are complicated grab bags of “ontic” and “epistemic,” so it shouldn’t surprise us if quantum states are too.  Bad dichotomy.<br/></li><li><strong>Why are there probabilities in QM?</strong>  Because QM <em>is</em> a (the?) generalization of probability theory to involve complex numbers, whose squared absolute values are probabilities.  It <em>includes</em> probability as a special case.<br/></li><li><strong>But why do the probabilities obey the Born rule?</strong>  Because, once the unitary part of QM has picked out the 2-norm as being special, for the probabilities <em>also</em> to be governed by the 2-norm is pretty much the only possibility that makes mathematical sense; there are many nice theorems formalizing that intuition under reasonable assumptions.<br/></li><li><strong>What is an “observer”?</strong>  It’s exactly what modern decoherence theory says it is: a particular kind of quantum system that interacts with other quantum systems, becomes entangled with them, and thereby records information about them—reversibly in principle but irreversibly in practice.<br/></li><li><strong>Can observers be manipulated in coherent superposition, as in the <a href="https://en.wikipedia.org/wiki/Wigner%27s_friend">Wigner’s Friend</a> scenario?</strong>  If so, they’d be radically unlike any physical system we’ve ever had direct experience with.  So, are you asking whether such “observers” would be <em>conscious</em>, or if so what they’d be conscious of?  Who the hell knows?<br/></li><li><strong>Do “other” branches of the wavefunction—ones, for example, where my life took a different course—exist in the same sense this one does?</strong>  If you start with a quantum state for the early universe and then time-evolve it forward, then yes, you’ll get not only “our” branch but also a proliferation of other branches, in the overwhelming majority of which Donald Trump was never president and civilization didn’t grind to a halt because of a bat near Wuhan.  But how could we possibly know whether anything “breathes fire” into the other branches and makes them real, when we have no idea what breathes fire into <em>this</em> branch and makes <em>it</em> real?  This is not a dodge—it’s just that a simple “yes” or “no” would fail to do justice to the enormity of such a question, which is above the pay grade of physics as it currently exists. <br/></li><li><strong>Is this it?  Have you brought me to the end of the path of understanding QM?</strong>  No, I’ve just pointed the way toward the <em>beginning</em> of the path.  The most fundamental tenet of the Zen Anti-Interpretation is that there’s no shortcut to actually <a href="https://www.scottaaronson.com/qclec.pdf">working</a> <a href="https://www.amazon.com/Quantum-Mechanics-Theoretical-Leonard-Susskind/dp/0465062903/ref=asc_df_0465062903/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=312014159412&amp;hvpos=&amp;hvnetw=g&amp;hvrand=7852945785672685485&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9028280&amp;hvtargid=pla-435140302691&amp;psc=1">through</a> the Bell inequality, quantum teleportation, Shor’s algorithm, the Kochen-Specker and PBR theorems, possibly even a … <em>photon</em> or a <em>hydrogen atom</em>, so you can see quantum probability in action and be enlightened.  I’m further along the path than I was twenty years ago, but not as far along as some of my colleagues.  Even the greatest quantum Zen masters will be able to get further when new quantum phenomena and protocols are discovered in the future.  All the same, though—and this is another major teaching of the Zen Anti-Interpretation—there’s more to life than achieving greater and greater clarity about the foundations of QM.  And on that note…</li></ol>



<p>To those who asked me about Claus Peter Schnorr’s <a href="https://eprint.iacr.org/2021/232">claim</a> to have discovered a fast <em>classical</em> factoring algorithm, thereby “destroying” (in his words) the RSA cryptosystem, see (e.g.) <a href="https://twitter.com/inf_0_/status/1367376526300172288?fbclid=IwAR19Ip7XyoPjHfm9WBzqiUkQpxUVLGfVTgLGQmmncgrkUsvcLIrkzbOPw_U">this Twitter thread by Keegan Ryan</a>, which explains what certainly <em>looks</em> like a fatal error in Schnorr’s paper.</p></div>
    </content>
    <updated>2021-03-04T23:26:29Z</updated>
    <published>2021-03-04T23:26:29Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Metaphysical Spouting"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-03-05T06:08:50Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/</id>
    <link href="https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/" rel="alternate" type="text/html"/>
    <title>2-round BFT SMR with n=4, f=1</title>
    <summary>Guest post by Zhuolun Xiang In the previous post, we presented a summary of our good-case latency results for Byzantine broadcast and Byzantine fault tolerant state machine replication (BFT SMR), where the good case measures the latency to commit given that the leader/broadcaster is honest. In this post, we describe...</summary>
    <updated>2021-03-03T11:37:00Z</updated>
    <published>2021-03-03T11:37:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2021-03-06T22:47:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/030</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/030" rel="alternate" type="text/html"/>
    <title>TR21-030 |  Hardness of Constant-round Communication Complexity | 

	Rahul Ilango, 

	Shuichi Hirahara, 

	Bruno Loff</title>
    <summary>How difficult is it to compute the communication complexity of a two-argument total Boolean function $f:[N]\times[N]\to\{0,1\}$, when it is given as an $N\times N$ binary matrix? In 2009, Kushilevitz and Weinreb showed that this problem is cryptographically hard, but it is still open whether it is NP-hard. 

In this work, we show that it is NP-hard to approximate the size (number of leaves) of the smallest constant-round protocol for a two-argument total Boolean function $f:[N]\times[N]\to\{0,1\}$, when it is given as an $N\times N$ binary matrix. Along the way to proving this, we show a new *deterministic* variant of the round elimination lemma, which may be of independent interest.</summary>
    <updated>2021-03-02T21:31:00Z</updated>
    <published>2021-03-02T21:31:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-07T01:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/</id>
    <link href="https://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/" rel="alternate" type="text/html"/>
    <title>Faculty at Universidad Católica de Chile (apply by April 10, 2021)</title>
    <summary>The Institute for Mathematical and Computational Engineering at Universidad Católica de Chile offers one or more full-time positions. We invite applications from candidates in the areas of Data Science, Machine Learning, Optimization, Statistics and Stochastic, although other areas from Computational Science and Engineering, Optimization and Applied Mathematics will also be considered. Website: http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc Email: pbarcelo@uc.cl</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Institute for Mathematical and Computational Engineering at Universidad Católica de Chile offers one or more full-time positions. We invite applications from candidates in the areas of Data Science, Machine Learning, Optimization, Statistics and Stochastic, although other areas from Computational Science and Engineering, Optimization and Applied Mathematics will also be considered.</p>
<p>Website: <a href="http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc">http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc</a><br/>
Email: pbarcelo@uc.cl</p></div>
    </content>
    <updated>2021-03-02T15:50:54Z</updated>
    <published>2021-03-02T15:50:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-07T01:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1842</id>
    <link href="https://theorydish.blog/2021/03/02/automated-design-of-error-correcting-codes-part-1/" rel="alternate" type="text/html"/>
    <title>Automated Design of Error-Correcting Codes, Part 1</title>
    <summary>Introduction. For nearly a century, error-correcting codes (ECCs) have been used for allowing communication even when the used communication channel is corrupted by noise. Beyond communication, error-correcting codes have found a variety of other uses, from multiclass learning to even showing hardness of approximation. As such, understanding the rich world of error-correcting codes is essential for progress in all of these domains. In our setting, imagine Alice wants to send a message to Bob of length , but the channel between them is corrupted by noise. To overcome this, Alice uses an encoder to turn her message into a longer, redundant string of length . Then, Bob receives this transmission and uses a decoder to (hopefully) recover the original message of length . Two important properties are the rate of the code (essentially what fraction of the transmission is “information”) and the bit error rate (BER) which is the (expected) number of decoding errors divided by . Desirable properties are to make the rate as large as possible and the BER as small as possible. Basic ECC paradigm. Since the days of Claude Shannon, many error-correcting codes have been discovered, such as Reed-Solomon codes and BCH codes. Each error-correcting code [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Introduction. </strong>For nearly a century, error-correcting codes (ECCs) have been used for allowing communication even when the used communication channel is corrupted by noise. Beyond communication, error-correcting codes have found a variety of other uses, from<a href="https://en.wikipedia.org/wiki/Multiclass_classification"> multiclass learning</a> to even <a href="https://arxiv.org/pdf/1002.3864.pdf">showing hardness of approximation</a>. As such, understanding the rich world of error-correcting codes is essential for progress in all of these domains.</p>



<p>In our setting, imagine Alice wants to send a message to Bob of length <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="k"/>, but the channel between them is corrupted by noise. To overcome this, Alice uses an <em>encoder</em> to turn her message into a longer, redundant string of length <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="n"/>. Then, Bob receives this transmission and uses a <em>decoder</em> to (hopefully) recover the original message of length <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="k"/>. Two important properties are the <em>rate</em> <img alt="k/n" class="latex" src="https://s0.wp.com/latex.php?latex=k%2Fn&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="k/n"/> of the code (essentially what fraction of the transmission is “information”) and the <em>bit error rate (BER)</em> which is the (expected) number of decoding errors divided by <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="k"/>. Desirable properties are to make the rate as large as possible and the BER as small as possible.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img alt="" height="353" src="https://lh3.googleusercontent.com/9y96GkfFf6FBCUKoufmsqUGfXt7VmrDqAuCQI1IaOfy4DB-VmJWEvSwL9c1mj9QP9gVYq49haRBI96eNMx5qPpr3BFhhGuWvTv4wixNbLLuTuSnI3xv36xaVa64D3DvshMs_XwmT" width="526"/></figure></div>



<p class="has-text-align-center">Basic ECC paradigm.</p>



<p>Since the days of <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>, many error-correcting codes have been discovered, such as <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction">Reed-Solomon codes</a> and <a href="https://en.wikipedia.org/wiki/BCH_code">BCH codes</a>. Each error-correcting code has its own tradeoffs (e.g., some have higher rate, some are more resistant to special kinds of channel corruptions, etc.). With the large number of ECCs which have been discovered, it can sometimes be overwhelming what the proper error correcting code is for a given application. Further, if the application is sufficiently specialized there may be <em>no </em>known ECC which meets your needs. Such concerns motivate the <em>automation</em> <em>of error correcting codes</em>, which is the main topic of this blog post. </p>



<p>I’m using the word “automation” to cover a variety of tasks which various computational methods could assist with in the study of ECCs:</p>



<ol><li>Existence — Does the code I want even exist?</li><li>Encoding — What is the “best” way to convert my messages into a code?</li><li>Decoding — How do I recover from noisy transmissions?</li><li>Verification — Is the proposed ECC design provably correct?</li><li>Selection — Which ECC from a given class should I use for a given application?</li></ol>



<p>Each of these facets of the automation of ECCs is a whole field of research! In this and the subsequent post, I will discuss at a high level two types of techniques which have been used to approach these questions: “Formal Methods” and “Machine Learning.” We’ll cover formal methods in this post, and in the next post we will cover machine learning methods.</p>



<p><strong>Formal Methods. </strong>The field of Formal Methods strives to give <em>provable guarantees</em> for various computational questions by reducing them to formal logic. Although formal methods are mostly used for software and hardware verification (that is, making sure they are “bug free”), such tools are also used by mathematicians to show the validity of mathematical statements that would be difficult to prove by hand. For example, the <a href="https://en.wikipedia.org/wiki/Kepler%27s_conjecture">Kepler conjecture</a>, a question of what is the best way to pack spheres in three dimensions–essentially finding an optimal error-correcting code in Euclidean space–was only firmly proved by <a href="https://github.com/flyspeck/flyspeck">Thomas Hales and his team</a> through the use of automated theorem-proving tools.</p>



<p>A line of work for using formal methods to directly construct practical ECCs was initiated by <a href="https://ieeexplore.ieee.org/abstract/document/5699220">Shamshiri and Cheng</a> in 2010. In their work, they are motivated by designing error-correcting codes for static random-access memory (SRAM), the kind that is often used for CPU caches. When using SRAM (pictured below), a worry is that cosmic rays could hit some of the bits, causing them to flip. Further, it is not uncommon for a group of consecutive bits to flip. As such, it is desirable to esure that the error correcting code can correct either <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="g"/> <em>global </em>errors or <img alt="l" class="latex" src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="l"/> <em>local</em> errors. Correcting global errors is a common property of error correcting codes, such as BCH codes or the <a href="https://en.wikipedia.org/wiki/Binary_Golay_code">Golay code</a>. However, local error correction is a much less common property to guarantee. Thus, the authors use a <em>SAT solver</em> to construct error correcting codes with the properties they desire.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img alt="" height="318" src="https://lh4.googleusercontent.com/qQRzeu2F2XzrjAl6DyPPWAnBVOKOSODIBX4RPBuz0aAMqErYxC2ZyAbtWy3z8ORU4rvMT9UEMI8-C7boHeKEijFwrKY2pRaneEm1lsAoKBUHpQQ1rcMpaLBZs8zAwr2yviMGJPyG" width="529"/></figure></div>



<p class="has-text-align-center">Static random-access memory (source: <a href="https://en.wikipedia.org/wiki/File:Hyundai_RAM_HY6116AP-10.jpg" rel="prettyphoto">Wikipedia</a>)</p>



<p>Assuming that the code to be construction is linear (the encoding map is a linear function over the field <img alt="mathbb F_2" class="latex" src="https://s0.wp.com/latex.php?latex=mathbb+F_2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="mathbb F_2"/>), then the error correcting code can be described by a <a href="https://en.wikipedia.org/wiki/Parity-check_matrix">parity check matrix</a> M in <img alt="{0,1}^{(n-k) times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2C1%7D%5E%7B%28n-k%29+times+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{0,1}^{(n-k) times n}"/>. The key observation the authors make is that for M to be a proper error correcting code, every error pattern <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="p"/> (i.e., vectors with hamming weight at most <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="g"/> or consecutive errors in a block of length <img alt="l" class="latex" src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="l"/>) must have <img alt="Mp" class="latex" src="https://s0.wp.com/latex.php?latex=Mp&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="Mp"/> be a distinct vector. For instance  <img alt="M(0, 1, 0, 0, 1) neq M(1,1,1,0,0)" class="latex" src="https://s0.wp.com/latex.php?latex=M%280%2C+1%2C+0%2C+0%2C+1%29+neq+M%281%2C1%2C1%2C0%2C0%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="M(0, 1, 0, 0, 1) neq M(1,1,1,0,0)"/> if <img alt="g = 2" class="latex" src="https://s0.wp.com/latex.php?latex=g+%3D+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="g = 2"/> and <img alt="l = 3" class="latex" src="https://s0.wp.com/latex.php?latex=l+%3D+3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="l = 3"/>.</p>



<p>These Boolean constraints can be expressed in conjunctive normal form, i.e., a SAT instance. As such a SAT-solver can be used to determine if there exists a matrix M with the given properties for a given k and n. For instance, they are able to find an error correcting code with parameters <img alt="k = 16, n = 26, g = 2" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3D+16%2C+n+%3D+26%2C+g+%3D+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="k = 16, n = 26, g = 2"/> and <img alt="l = 4" class="latex" src="https://s0.wp.com/latex.php?latex=l+%3D+4&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="l = 4"/>. In their follow-up work [<a href="https://ieeexplore.ieee.org/abstract/document/6139156">Shamshi, Ghofrani, Cheng, 2011]</a>, they use this error-correcting code for modeling an “on-chip network” between CPU cores in a multi-core processor<strong>.</strong></p>



<p>Another line of work led by Ben Curtis (see the <a href="https://cs.uwaterloo.ca/~cbright/reports/cacm-preprint.pdf">survey by Curtis, Kotsireas, and Ganesh</a>) has been seeking to construct ECC-like combinatorial objects. An example of such an object is a Hadamard matrix: a square matrix with <img alt="\pm 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpm+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\pm 1"/> entries such that every row and column is orthogonal in <img alt="\mathbb R^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\mathbb R^n"/>. In fact, the authors search for a special type of Hadamard matrix made up of a quartet Williamson matrices which have an intricate algebraic structure. They find these objects by using an algorithm which goes back-and-forth between a SAT solver with a CAS (computer algebraic system) to help narrow the search space. </p>



<p>Formal Methods have further applications in error-correcting codes for <a href="https://ieeexplore.ieee.org/abstract/document/6649704">distributed cloud storage</a> and <a href="https://arxiv.org/pdf/1804.02317.pdf">value-deviation-bounded codes</a>.</p>



<p>This concludes our first post. In the next post, we will cover machine learning methods. </p>



<p>Are you aware of other examples or applications of automation to error-correcting codes? If so, please leave a comment.</p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Aviad Rubinstein, Moses Charikar, and Mary Wootters for valuable feedback. </p></div>
    </content>
    <updated>2021-03-02T15:00:00Z</updated>
    <published>2021-03-02T15:00:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>jbrakensiek</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-03-07T01:22:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/" rel="alternate" type="text/html"/>
    <title>Associate professor at KTH Royal Institute of Technology (apply by April 15, 2021)</title>
    <summary>KTH Royal Institute of Technology, School of Electrical Engineering and Computer Science has a vacancy for one Associate Professor with specialization in Foundations of Data Science. The position will be permanent and full time, to start as soon as possible. Website: https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/ Email: tenuretrack@eecs.kth.se</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>KTH Royal Institute of Technology, School of Electrical Engineering and Computer Science has a vacancy for one Associate Professor with specialization in Foundations of Data Science. The position will be permanent and full time, to start as soon as possible.</p>
<p>Website: <a href="https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/">https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/</a><br/>
Email: tenuretrack@eecs.kth.se</p></div>
    </content>
    <updated>2021-03-01T21:54:01Z</updated>
    <published>2021-03-01T21:54:01Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-07T01:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/029</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/029" rel="alternate" type="text/html"/>
    <title>TR21-029 |  Public-Coin Statistical Zero-Knowledge Batch Verification against Malicious Verifiers | 

	Inbar Kaslasi, 

	Ron Rothblum, 

	Prashant Nalini Vasudevan</title>
    <summary>Suppose that a problem $\Pi$ has a statistical zero-knowledge (SZK) proof with communication complexity $m$. The question of batch verification for SZK asks whether one can prove that $k$ instances $x_1,\ldots,x_k$ all belong to $\Pi$ with a statistical zero-knowledge proof whose communication complexity is better than $k \cdot m$ (which is the complexity of the trivial solution of executing the original protocol independently on each input).

In a recent work, Kaslasi et al. (TCC, 2020) constructed such a batch verification protocol for any problem having a non-interactive SZK (NISZK) proof-system. Two drawbacks of their result are that their protocol is private-coin and is only zero-knowledge with respect to the honest verifier.

In this work, we eliminate these two drawbacks by constructing a public-coin malicious-verifier SZK protocol for batch verification of NISZK. Similarly to the aforementioned prior work, the communication complexity of our protocol is $\big(k+poly(m) \big) \cdot polylog(k,m)$</summary>
    <updated>2021-03-01T16:38:36Z</updated>
    <published>2021-03-01T16:38:36Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-07T01:20:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2021/03/01/beyondlogconcave2/</id>
    <link href="http://offconvex.github.io/2021/03/01/beyondlogconcave2/" rel="alternate" type="text/html"/>
    <title>Beyond log-concave sampling (Part 2)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In our previous <a href="http://www.offconvex.org/2020/09/19/beyondlogconvavesampling">blog post</a>, we introduced the challenges of sampling distributions beyond log-concavity. 
We first introduced the problem of sampling from a distibution $p(x) \propto e^{-f(x)}$ given value or gradient oracle access to $f$, as an analogous problem to black-box optimization with oracle access. We introduced the natural algorithm for sampling in this setup: Langevin Monte Carlo, a Markov Chain reminiscent of noisy gradient descent,</p>

\[x_{t+\eta} = x_t - \eta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I).\]

<p>Finally, we laid out the challenges when $f$ is not convex; in particular, LMC can suffer from slow mixing.</p>

<p>In this and the coming post, we describe two of our recent works tackling this problem. We identify two kinds of structure beyond log-concavity under which we can design provably efficient algorithms:  <em>multi-modality</em> and <em>manifold structure in the level sets</em>. These structures commonly occur in practice, especially in problems involving statistical inference and posterior sampling in generative models.</p>

<p>In this post, we will focus on multimodality, covered by the paper <a href="https://arxiv.org/abs/1812.00793">Simulated tempering Langevin Monte Carlo</a> by Rong Ge, Holden Lee, and Andrej Risteski.</p>

<h1 id="sampling-multimodal-distributions-with-simulated-tempering">Sampling multimodal distributions with simulated tempering</h1>

<p>The classical scenario in which Langevin takes exponentially long to mix is when $p$ is a mixture of two well-separated gaussians. In broadest generality, this was considered by <a href="http://www.ems-ph.org/journals/show_abstract.php?issn=1435-9855%20&amp;vol=6&amp;iss=4&amp;rank=1">Bovier et al. 2004</a> who used tools from metastable processes to show that transitioning from one peak to another can take exponential time. Roughly speaking, they show the transition time is proportional to the “energy barrier” a particle has to cross. If the gaussians have unit variance and means at distance $2r$, then the probability density at a point midway in between is $\propto e^{-r^2/2}$, and this energy barrier is $\propto e^{r^2/2}$. Thus, the mixing time is exponential. Qualitatively, the intuition for this phenomenon is simple to describe: if started at point A, the drift (i.e. gradient) term will push the walk towards A, so long as it’s close to the basin around A; hence, to transition from A to B (through C) the Gaussian noise must persistenly counteract the gradient term.</p>

<center>
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_bovier.gif" width="500"/>
</center>

<p>Hence Langevin on its own will not work even in very simple multimodal settings.</p>

<p>In <a href="https://arxiv.org/abs/1812.00793">our paper</a>, we show that combining Langevin Monte Carlo with a temperature-based heuristic called <em>simulated tempering</em> can significantly speed up mixing for multimodal distributions, where the number of modes is not too large, and the modes “look similar.”</p>

<p>More precisely, we show:</p>

<blockquote>
  <p><strong>Theorem (Ge, Lee, Risteski ‘18, informal)</strong>: If $p(x)$ is a mixture of $k$ shifts of a strongly log-concave distribution in $d$ dimensions (e.g. Gaussian), an algorithm based on simulated tempering and Langevin Monte Carlo that runs in time poly($d,k, 1/\varepsilon$) produces samples from a distribution $\varepsilon$-close to $p$ in total variation distance.</p>
</blockquote>

<p>The main idea is to create a meta-Markov chain (the simulated tempering chain) which has two types of moves: change the current “temperature” of the sample, or move “within” a temperature. The main intuition behind this is that at higher temperatures, the distribution is flatter, so the chain explores the landscape faster (see the figure below).</p>

<center> 
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_tempering.gif"/>
</center>

<p>More formally, the distribution at inverse temperature $\beta$ is given by $p_\beta(x) \propto e^{-\beta f(x)}$. The Langevin chain which corresponds to $\beta$ is given by</p>

\[x_{t+\eta} = x_t - \eta \beta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I).\]

<p>As in the figure above, a high temperature (low $\beta&lt;1$) flattens out the distribution and causes the chain to mix faster (top distribution in figure). However, we can’t merely run Langevin at a higher temperature, because the stationary distribution of the high-temperature chain is wrong: it’s $p_\beta(x)$. The idea behind simulated tempering is to run Langevin chains at different temperatures, sometimes swapping to another temperature to help lower-temperature chains explore. To maintain the right stationary distributions at each temperature, we use a Metropolis-Hastings filtering step.</p>

<p>More formally, choosing a suitable sequence $0&lt; \beta_1&lt; \cdots &lt;\beta_L=1$, we define the simulated tempering chain as follows.</p>

<p><img src="http://holdenlee.github.io/pics/stl.png" style="float: right;" width="300"/></p>

<ul>
  <li>The <em>state space</em> is a pair of a temperature and location in space $(i, x), i \in [L], x \in \mathbb{R}^d$.<br/>
<!--$L$ copies of the state space (in our case $\mathbb R^d$), one copy for each temperature.--></li>
  <li>The <em>transitions</em> are defined as follows.
    <ul>
      <li>If the current point is $(i,x)$, then <em>evolve</em> $x$ according to Langevin diffusion with inverse temperature $\beta_i$.</li>
      <li>Propose swaps with some rate $\lambda &gt;0$. Proposing a swap means attempting to move to a neighboring chain, i.e. change $i$ to $i’=i\pm 1$. With probability $\min{p_{i’}(x)/p_i(x), 1}$, the transition is accepted. Otherwise, stay at the same point. This is a <em>Metropolis-Hastings step</em>; its purpose is to preserve the stationary distribution.</li>
    </ul>
  </li>
</ul>

<p>Finally, it’s not too hard to see that at the stationary distribution, the samples at the $L$th level ($\beta_L=1$) are the desired samples.</p>

<h2 id="proof-idea-decomposition-theorem">Proof idea: decomposition theorem</h2>

<p>The main strategy is inspired by Madras and Randall’s <a href="https://www.jstor.org/stable/2699896">Markov chain decomposition theorem</a>, which gives a criterion for a Markov chain to mix rapidly: partition the state space into sets, and show that</p>

<ol>
  <li>The Markov chain mixes rapidly when restricted to each set of the partition.</li>
  <li>The <em>projected</em> Markov Chain, which we define momentarily, mixes rapidly. If there are $m$ sets, the projected chain $\overline M$ is defined on the state space ${1,\ldots, m}$, and transition probabilities are given by average probability flows between the corresponding sets.</li>
</ol>

<p>To implement this strategy, we first have to specify the partition. In fact, we roughly show that there is a partition of $[L] \times \mathbb{R}^d$ in which:</p>

<ol>
  <li>The simulated tempering Langevin chain mixes fast within each of the sets.</li>
  <li>The “volume” of the sets (under the stationary distribution of the tempering chain) is not too small.
<!-- [HL: alt.] There is no set at high temperature that has much larger volume at low temperature.
 --></li>
</ol>

<p>In applying the Madras-Randall framework with this partition, it’s clear that point (1) above satisfies requirement (1) for the framework; point (2) ensures that the projected Markov chain has no “bottlenecks” and hence that it mixes rapidly (requirement (2)). More precisely, we can show rapid mixing either through the method of canonical paths or Cheeger’s inequality. To do this, we exhibit a “good-probability” path between any two sets in the partition, going through the highest temperature.</p>

<p>The intuition for why this path works is illustrated in the figure below: when transitioning from the set corresponding to the left mode at level $L$ to the right mode at level $L$, each of the steps up/down the temperatures are accepted with good probability if the neighboring temperatures are not too different; at the highest temperature, the chain mixes fast by point (1), and since each of the sets are not too small by point (2), there is a reasonable probability to end at the right mode at the highest temperature.</p>

<center>
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_conductance.gif"/>
</center>

<!--(rework this picture?) This is a Markov chain with a small state space, so its spectral gap is easy to lower-bound (e.g., with Cheeger's inequality). The one thing we need to check is that there is no "bottleneck," i.e., one set in the partition that has low probability at high temperature and high probability at low temperature. -->

<p>Intuitively, the partition should track the “modes” of the distribution, but a technical hurdle in implementing this plan is in defining the partition when the modes overlap. One can either do this spectrally (i.e. showing that the Langevin chain has a spectral gap, and use theorems about <a href="https://arxiv.org/abs/1309.3223">spectral graph partitioning</a>, as we did in the <a href="https://arxiv.org/abs/1710.02736">first version</a> of the paper), or use a functional “soft decomposition theorem” which is a more flexible version of the classical decomposition theorem, which we use in a <a href="https://arxiv.org/abs/1812.00793">later version</a> of the paper.</p>

<!-- ![](http://holdenlee.github.io/pics/proj_chain.png)--></div>
    </summary>
    <updated>2021-03-01T14:00:00Z</updated>
    <published>2021-03-01T14:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2021-03-06T22:46:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/</id>
    <link href="https://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/" rel="alternate" type="text/html"/>
    <title>PhD. Thesis at LAMSADE (Paris Dauphine) (apply by April 30, 2021)</title>
    <summary>PhD. Thesis offer in Paris Dauphine University “Algorithmic aspects of intersection graphs” Website: https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf Email: florian.sikora@dauphine.fr</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>PhD. Thesis offer in Paris Dauphine University “Algorithmic aspects of intersection graphs”</p>
<p>Website: <a href="https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf">https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf</a><br/>
Email: florian.sikora@dauphine.fr</p></div>
    </content>
    <updated>2021-03-01T10:35:30Z</updated>
    <published>2021-03-01T10:35:30Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-07T01:20:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/</id>
    <link href="https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/" rel="alternate" type="text/html"/>
    <title>Good-case Latency of Byzantine Broadcast: a Complete Categorization</title>
    <summary>Guest post by Zhuolun Xiang State Machine Replication and Broadcast Many existing permission blockchains are built using Byzantine fault-tolerant state machine replication (BFT SMR), which ensures all honest replicas agree on the same sequence of client inputs. Most of the practical solutions for BFT SMR are based on the Primary-Backup...</summary>
    <updated>2021-02-28T18:07:00Z</updated>
    <published>2021-02-28T18:07:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2021-03-06T22:47:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5663884325046890461</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5663884325046890461/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5663884325046890461" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5663884325046890461" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html" rel="alternate" type="text/html"/>
    <title>Using number-of-PhD's as a measure of smartness is stupid.</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In <i>Thor:Ragnorak</i> Bruce Banner mentions that he has 7 PhDs. Gee, I wonder how he managed to slip that into a conversation casually.  Later in the movie:</p><p><br/></p><p>Bruce: I don't know how to fly one of those (it an Alien Spacecraft)</p><p>Thor: You're a scientist. Use one of your PhD's </p><p>Bruce: None of them are for flying alien spaceships.</p><p><br/></p><p>On the episode <i>Double Date </i>of Archer (Season 11, Episode 6) Gabrielle notes that she has 2 PhD's whereas Lana only has 1 PhD. </p><p><br/></p><p>I am sure there are other examples of a work of fiction using <i>number of PhDs </i>as a way to say that someone is smart. In reality the number of PhD's one has is... not really a thing. </p><p>In reality if a scientist wants to do work in another field they... do work in that field.</p><p>Godel did research in Physics in the 1950's, but it would have been silly to go back and get a PhD in it.</p><p>Fortnow did research in Economics, but it would have been silly to go back and get a PhD in it. </p><p>Amy Farrah Fowler worked in neurobiology and then in Physics. Her Nobel prize in physics (with Sheldon Cooper) is impressive, getting a PhD in Physics would be ... odd. Imagine someone looking at here resume: <i>She has a Nobel Prize in Physics, but does she have a PhD? Did she pass her qualifying</i> <i>exams?</i>  This is the flip side of what I mentioned in a prior post about PhD's: <i>Not only does Dr. Doom want to take over the world, but his PhD is from The University of Latveria, which is not accredited. </i></p><p>There are other examples.</p><p>There ARE some people who get two PhDs for reasons of job market or other such things. That's absolutely fine of course. However, I wonder if in the real world they brag about it. I doubt it. </p><p>Is there anyone who has 3 PhDs? I would assume yes, but again, I wonder if they brag about it. Or should. </p><p>WHY do TV and movies use number-of-PhDs as a sign of genius? I do not know- especially since there are BETTER ways say someone is a genius in a way the audience can understand:  number-of-Nobel-prizes, number-of-times-mentioned-in-complexityblog,  number of Dundie's (see <a href="https://theoffice.fandom.com/wiki/Dundie">here</a>), etc. </p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-02-28T17:42:00Z</updated>
    <published>2021-02-28T17:42:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-03-07T00:30:40Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/02/28/linkage</id>
    <link href="https://11011110.github.io/blog/2021/02/28/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>STOC 2021 accepted papers (\(\mathbb{M}\)).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="http://acm-stoc.org/stoc2021/accepted-papers.html">STOC 2021 accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/105748480557219533">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://randomascii.wordpress.com/2021/02/16/arranging-invisible-icons-in-quadratic-time/">Arranging invisible icons in quadratic time</a> (<a href="https://mathstodon.xyz/@11011110/105756917532905626">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26152335">via</a>). Yet another instance where using a too-slow algorithm causes a UI hang, with the twist that the better solution would not be to replace it with a faster algorithm, but instead to not do the useless thing that the bad algorithm does at all.</p>
  </li>
  <li>
    <p><a href="https://joshdata.me/iceberger.html">Fun with shapes: draw an iceberg and see which way up and how deep it would float</a> (<a href="https://mathstodon.xyz/@11011110/105768276511155377">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26201160">via</a>, <a href="https://www.metafilter.com/190533/Iceberger">via2</a>, <a href="https://boingboing.net/2021/02/20/make-your-own-iceberg-with-iceberger.html">via3</a>). Inspired by <a href="https://mobile.twitter.com/GlacialMeg/status/1362557149147058178">a twitter thread by Megan Thompson-Munson</a> pointing out that many supposed photos or illustrations of icebergs are fake and wrong.</p>
  </li>
  <li>
    <p>Draw an infinite subgraph of the 3d integer lattice in which each vertex has four co-planar neighbors, in a perpendicular plane to each of its neighbors (<a href="https://mathstodon.xyz/@11011110/105771494222747316">\(\mathbb{M}\)</a>). This completely determines the subgraph, which is 4-regular and highly symmetric. It is the graph of adjacencies of the cubes in the <a href="https://en.wikipedia.org/wiki/Tetrastix">tetrastix structure</a>. Does this graph have a name and history?</p>
  </li>
  <li>
    <p><a href="https://cacm.acm.org/magazines/2021/3/250708-gender-trends-in-computer-science-authorship">Gender trends in computer science authorship</a> (<a href="https://mathstodon.xyz/@11011110/105781841287243050">\(\mathbb{M}\)</a>). Takeaways for me (mostly from the barely-readable Fig. 4) are:</p>

    <ul>
      <li>
        <p>Roughly one in four coauthors of CS research publications are currently female, up from a big dip of one in seven in the 1970s to 1990s.</p>
      </li>
      <li>
        <p>Mathematics started lower and is currently more or less the same.</p>
      </li>
      <li>
        <p>We are not on track to gender parity.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>I’m sad that the only way to find a viewable version of the 1991 short film <em><a href="https://en.wikipedia.org/wiki/Not_Knot">Not Knot</a></em> (on the hyperbolic geometry of knot complements) seems to be through pirate copies (<a href="https://mathstodon.xyz/@11011110/105785401264334824">\(\mathbb{M}\)</a>). Or you could pay $45 to Amazon for a copy on DVD. Do most people still have DVD players? At least they’re not still trying to sell it on VHS only.</p>
  </li>
  <li>
    <p><a href="https://cscresearchblog.wordpress.com/2018/11/16/karp-sipser-heuristic-and-reductions/">On the slow spread of knowledge of nice theorems</a> (<a href="https://mathstodon.xyz/@11011110/105793165233864617">\(\mathbb{M}\)</a>), an amusing cartoon at the end of a longer blog post on fast graph matching heuristics.</p>
  </li>
  <li>
    <p>Today’s LaTeX formatting tip (<a href="https://mathstodon.xyz/@11011110/105796107362586793">\(\mathbb{M}\)</a>): You know that bug where amsthm + hyperref, with one numbering for theorems and lemmas and corollaries and whatever, causes <code class="language-plaintext highlighter-rouge">\autoref</code> to call them theorems even when they’re really lemmas and corollaries and whatever? If you don’t, you’re lucky. Anyway, there’s a very simple workaround: after loading amsthm and hyperref, add one more package:</p>

    <p><code class="language-plaintext highlighter-rouge">\usepackage[capitalize,nameinlink]{cleveref}</code></p>

    <p>Then, just use <code class="language-plaintext highlighter-rouge">\cref</code> everywhere you were using <code class="language-plaintext highlighter-rouge">\autoref</code>. Problem solved!</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm">Lloyd’s algorithm</a> animated for 3d points (<a href="https://mathstodon.xyz/@tpfto/105553548210257285">\(\mathbb{M}\)</a>). See also <a href="https://mathstodon.xyz/@tpfto/105803635782297523">the spherical version</a>.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wordpress.com/2021/02/27/new-old-ancient-results/">Applications of the no-3-in-line problem and cap-sets to complexity theory</a> (<a href="https://mathstodon.xyz/@11011110/105807834096788492">\(\mathbb{M}\)</a>). “What is most curious to us is that for matrix multiplication, the cap-set related technique frustrates a better complexity upper bound, whereas [for linear algebraic circuits] it frustrates a better lower bound.”</p>
  </li>
  <li>
    <p><a href="https://www.bldgblog.com/2013/08/tensioned-suspension/">Tensioned suspension</a> (<a href="https://mathstodon.xyz/@11011110/105811049795181041">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=9093187">via</a>): sculptures by Dan Grayber in which the weight of mechanical linkages causes them to push out against the sides of their glass enclosures, seemingly causing them to hang suspended in air. More at <a href="http://www.dangrayber.com/">Grayber’s web site</a>.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-02-28T16:24:00Z</updated>
    <published>2021-02-28T16:24:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-03-01T01:04:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18225</id>
    <link href="https://rjlipton.wordpress.com/2021/02/27/new-old-ancient-results/" rel="alternate" type="text/html"/>
    <title>New, Old, Ancient Results</title>
    <summary>Nonexistence theorems and attempts at lower bounds Cropped from src Joshua Grochow is an assistant professor in Computer Science and Mathematics at the University of Colorado at Boulder. He was a student of Ketan Mulmuley and Lance Fortnow at Chicago; his dissertation and some subsequent papers did much to widen the horizons of the “Geometric […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Nonexistence theorems and attempts at lower bounds</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/02/jgcropped.jpg"><img alt="" class="alignright wp-image-18228" height="168" src="https://rjlipton.files.wordpress.com/2021/02/jgcropped.jpg?w=141&amp;h=168" width="141"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="https://home.cs.colorado.edu/~jgrochow/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Joshua Grochow is an assistant professor in Computer Science and Mathematics at the University of Colorado at Boulder. He was a student of Ketan Mulmuley and Lance Fortnow at Chicago; his <a href="https://home.cs.colorado.edu/~jgrochow/grochow-thesis.pdf">dissertation</a> and <a href="https://arxiv.org/pdf/1304.6333.pdf">some</a> <a href="https://arxiv.org/pdf/1112.2012.pdf">subsequent</a> <a href="https://arxiv.org/pdf/1605.02815.pdf">papers</a> did much to widen the horizons of the “Geometric Complexity Theory” (GCT) program. He is also a gifted expositor.</p>
<p>
Today we will highlight some of his work and some of his exposition of new and old theorems.<br/>
<span id="more-18225"/></p>
<p>
An ancient one is Stephen Mahaney’s famous theorem on the nonexistence of sparse <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-complete sets (unless <img alt="{\mathsf{NP = P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP+%3D+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>). Grochow <a href="https://arxiv.org/pdf/1610.05825.pdf">discusses</a> a simpler proof of the theorem by Manindra Agrawal and gives some further impacts on GCT. </p>
<p>
A recent <a href="https://drops.dagstuhl.de/opus/volltexte/2021/13570/pdf/LIPIcs-ITCS-2021-31.pdf">one</a> with Youming Qiao is on an old topic and is in the 2021 Innovations in Theoretical Computer Science conference. It is titled, “On the Complexity of Isomorphism Problems for Tensors, Groups, and Polynomials I: Tensor Isomorphism-Completeness,” and grows out of a 2019 <a href="https://arxiv.org/abs/1907.00309">paper</a> by the same authors. </p>
<p>
This came to my attention through communications with Grochow’s <a href="https://michaellevet.github.io">student</a>, Michael Levet. Indeed, Levet is the reason for my putting this all together. He raised through email some questions about an ancient result of mine on group isomorphism. I reported <a href="https://rjlipton.wordpress.com/2013/05/11/advances-on-group-isomorphism/">previously</a>:</p>
<blockquote><p><b> </b> <em> Long ago Bob Tarjan and Zeke Zalcstein and I made a simple observation: Group isomorphism could be done in time 	</em></p><em>
<p align="center"><img alt="\displaystyle  n^{\log_{2} n + O(1)}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n%5E%7B%5Clog_%7B2%7D+n+%2B+O%281%29%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
</em><p><em>This relies on the easy-to-prove fact that every group has at most <img alt="{\log_{2} n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog_%7B2%7D+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> generators. We have discussed this idea earlier <a href="https://rjlipton.wordpress.com/2011/10/08/an-annoying-open-problem/">here</a>. </em>
</p></blockquote>
<p>
Levet raised an issue about related observations of mine—ones that were misleading at best. I think he has a good point and we are still trying to unravel exactly what I meant back then. I applaud him for reading ancient stuff, for trying to extend it, and for working on such problems. I wish him well.</p>
<p>
</p><h2> No Three In a Row </h2><p/>
<p>While Levet and I work that out and think about Grochow’s paper on isomorphism problems with Qiao, Ken and I want to highlight a different expository <a href="https://www.ams.org/journals/bull/2019-56-01/S0273-0979-2018-01648-0/S0273-0979-2018-01648-0.pdf">paper</a> by Grochow on news from 2016 that we <a href="https://rjlipton.wordpress.com/2016/06/15/polynomial-prestidigitation/">covered</a> then. Grochow’s paper appeared in the <em>AMS Bulletin</em> and is titled, “New Applications Of The Polynomial Method: The Cap Set Conjecture And Beyond.” </p>
<p>
To lead in to the subject, here is a <a href="https://archive.org/stream/amusementsinmath00dude#page/94/mode/2up">problem</a> from 1917 by the English puzzlemaster Henry Dudeney titled, “A Puzzle With Pawns”:</p>
<blockquote><p><b> </b> <em> Place two pawns in the middle of the chess- board, one at Q 4 and the other at K 5. Now, place the remaining fourteen pawns (sixteen in all) so that no three shall be in a straight line in any possible direction. Note that I purposely do not say queens, because by the words ” any possible direction ” I go beyond attacks on diagonals. The pawns must be regarded as mere points in space — at the centres of the squares. </em>
</p></blockquote>
<p>
Sixteen is obviously the maximum possible for a standard <img alt="{8 \times 8}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> chessboard because a seventeenth pawn would make three in some row and some column. For an <img alt="{r \times r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Ctimes+r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> board, the limit is <img alt="{2r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> by similar reasoning—this is an example of the <em>pigeonhole principle</em> which we just <a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/">mentioned</a>. </p>
<p>
It is possible to achieve the maximum for all <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> up to <img alt="{46}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B46%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and then the only other cases known are <img alt="{48}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B48%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{50}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B50%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="{52}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. That’s it. Here are solutions for <img alt="{r = 10}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{r = 52}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The latter was found by Achim Flammenkamp, whose <a href="http://wwwhomes.uni-bielefeld.de/achim/no3in/readme.html">page</a> has encyclopedic information. On the former, the pieces are positioned on gridpoints like stones in Go, which seems a better context for this problem than chess. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/02/3inarow20and52.jpg"><img alt="" class="aligncenter wp-image-18230" height="257" src="https://rjlipton.files.wordpress.com/2021/02/3inarow20and52.jpg?w=550&amp;h=257" width="550"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://en.wikipedia.org/wiki/No-three-in-line_problem">src1</a>, <a href="https://mathworld.wolfram.com/No-Three-in-a-Line-Problem.html">src2</a></font>
</td>
</tr>
</tbody></table>
<p>
The conjecture is not only that a <img alt="{2n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-size solution exists for only finitely many <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, but also that the maximum size for all sufficiently large <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is bounded by <img alt="{cr}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bcr%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with <img alt="{c &lt; 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, indeed, with <img alt="{c &lt; 1.815}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+1.815%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. It is known that <img alt="{(1.5-\epsilon_r)r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281.5-%5Cepsilon_r%29r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> stones can always be placed with no three collinear, where the <img alt="{\epsilon_r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> depends on the closeness of a prime to <img alt="{\frac{r}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Br%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </p>
<p>
The problem can be taken to dimensions <img alt="{n \ge 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that are beyond the plane. We can also extend what is meant by a “line” via various notions of wrapping-around. Then the question is how close the maximum size can stay to being linear in the size of the space—as <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and/or the size <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of an individual dimension increase.</p>
<p>
</p><h2> Not As Easy As Tic-Tac-Toe </h2><p/>
<p>
The theme of the no-three-in-a-line problem is fundamental to combinatorics. There are tons of problems of the form: </p>
<blockquote><p><b> </b> <em> How many objects can one place, so that no pattern of some certain type exists? </em>
</p></blockquote>
<p>
In <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-dimensional space the smallest <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of interest is <img alt="{r = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This means playing on higher-dimensional versions of the <img alt="{3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> grid and <img alt="{3 \times 3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> cube. Then the only Euclidean lines are the kind we know from tic-tac-toe: straight across or down, or diagonal. </p>
<p>
For dimension <img alt="{n \geq 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> there are other kinds of diagonals, such as within a face or through the center of the cube, but they all win at <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-dimensional tic-tac-toe. So the problem becomes: what is the maximum number of moves you can make by yourself without creating a win at tic-tac-toe? The <em>cap-set problem</em> adds a twist by extending the notion of what is a <em>line</em>. It is like playing tic-tac-toe on a floor of <img alt="{3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> tiles where a play in one tile is replicated in all of them. Then you can make a line by playing in a corner and in the middle of the two opposite edges, as shown at left in the following diagram (original drawing).</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2021/02/lineandcapset3x3.png"><img alt="" class="aligncenter wp-image-18231" height="222" src="https://rjlipton.files.wordpress.com/2021/02/lineandcapset3x3.png?w=550&amp;h=222" width="550"/></a></p>
<p>
The four orange O’s at right have no 3-in-a-line even with this extended notion of line. Note that the four blank cells in the top two rows also avoid putting 3 in a line. Four is the maximum, however: it is not possible to have a drawn game in extended <img alt="{3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> tic-tac-toe. </p>
<p>
The theorem <a href="https://arxiv.org/pdf/1605.09223v1.pdf">proved</a> by Jordan Ellenberg and Dion Gijswijt in 2016 is that the upper bound is not only a vanishing fraction of the size <img alt="{3^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of the space as <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> grows, it is bounded by <img alt="{c^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where <img alt="{c &lt; 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Namely:</p>
<blockquote><p><b>Theorem 1</b> <em> Every cap set in the <img alt="{3^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cube has size at most <img alt="{2.756^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2.756%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </em>
</p></blockquote>
<p>
</p><h2> Using Polynomials </h2><p/>
<p>There is a simple way to express the extended notion of “line” that works for all dimensions <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: Number the coordinates of each dimension <img alt="{0,1,2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2C1%2C2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Make the space <img alt="{\{0,1,2\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%2C2%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with addition modulo <img alt="{q = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, that is, make it <img alt="{\mathbb{Z}_3^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_3%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then the condition for three points <img alt="{A,B,C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be in a line is simply </p>
<p align="center"><img alt="\displaystyle  A + B = 2C. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A+%2B+B+%3D+2C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>It is easy to write polynomial equations over the field <img alt="{\mathbb{F}_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to express the property of a set having such a line. What was unexpected, until Ernie Croot, Vsevolod Lev, and Péter Pál Pach solved a related problem with <img alt="{q = 4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq+%3D+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, was that there would be</p>
<blockquote><p><b> </b> <em> “an ingeniously simple way to split the polynomial[s] into pieces with smaller exponents, which led to a bound on the size of collections with no [lines].” </em>
</p></blockquote>
<p>
The quotation comes from an <a href="https://www.quantamagazine.org/set-proof-stuns-mathematicians-20160531">article</a> by Erica Klarreich for <em>Quanta</em> right then in 2016. A 2016 AMS Feature <a href="http://www.ams.org/publicoutreach/feature-column/fc-2016-08">column</a> by David Austin covers how to make this say a set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of points is a cap set modulo 3: </p>
<p align="center"><img alt="\displaystyle  S \uplus S \cap 2S = \emptyset, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%5Cuplus+S+%5Ccap+2S+%3D+%5Cemptyset%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>where we (not Austin) write <img alt="{S \uplus S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Cuplus+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to mean the set of sums <img alt="{a + b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where <img alt="{a,b \in S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb+%5Cin+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{b \neq a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb+%5Cneq+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. If there is an element <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the intersection then <img alt="{a + b = r = 2c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b+%3D+r+%3D+2c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and since <img alt="{3c = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3c+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we get <img alt="{a + b + c = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b+%2B+c+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with <img alt="{a,b,c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and all distinct, a contradiction. (If <img alt="{c = b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then <img alt="{a + 2b = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+2b+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, so <img alt="{a = b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.) Let <img alt="{m_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> stand for the number of monomials of degree at most <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> variables. The key first insight is:</p>
<blockquote><p><b>Lemma 2</b> <em> If a polynomial <img alt="{p(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> of degree <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> vanishes on <img alt="{S\uplus S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Cuplus+S%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then <img alt="{p(x) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%28x%29+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> for all but at most <img alt="{2m_{d/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2m_%7Bd%2F2%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> points of <img alt="{2S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2S%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </em>
</p></blockquote>
<p>
One could first try to interpret this as saying that <img alt="{S \uplus S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Cuplus+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> “looks like” <img alt="{2S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to polynomials <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of “low” degree <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. However, if <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> stays low relative to <img alt="{|S|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then the “if” part would hold vacuously, opposing the goal of bounding <img alt="{|S|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and making the whole idea self-defeating. In fact, the important tension comes when <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is intermediate: <img alt="{d = (q-1)n/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+%28q-1%29n%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which for <img alt="{q = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> makes <img alt="{d/2 = n/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%2F2+%3D+n%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{d = 2n/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+2n%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> neatly occupy the middle of the range <img alt="{\{1,\dots,n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cdots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
The proof also uses the trick that if a product of two monomials has degree <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then one of them must have degree at most <img alt="{\lfloor d/2\rfloor}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clfloor+d%2F2%5Crfloor%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. As I (Ken writing these sections) <a href="https://rjlipton.wordpress.com/2016/06/15/polynomial-prestidigitation/">wrote</a> about it back in 2016, this reminds of Roman Smolensky’s degree-halving <a href="https://rjlipton.wordpress.com/2012/03/11/a-note-on-distributions-and-approximation/">trick</a> in his celebrated 1987 <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.883&amp;rep=rep1&amp;type=pdf">theorem</a> on lower bounds for mod-<img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> versus mod-<img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This trick, however, runs from <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to <img alt="{n/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for all moduli. </p>
<p>
In any event, the 2016 papers were a new form of the polynomial method that led to striking new results. What Grochow’s survey does for us now is bring out wider implications of this ingenuity.</p>
<p>
</p><h2> Applications in Complexity </h2><p/>
<p>Grochow’s four application areas in section 4 of his survey are:</p>
<ol>
<li>
Progress on various forms of `sunflower’ conjectures. <p/>
</li><li>
Barriers to attempts to show that the exponent of matrix multiplication is <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. <p/>
</li><li>
Removing edges to make graphs triangle-free. <p/>
</li><li>
Matrix rigidity and lower bounds.
</li></ol>
<p>
We say a little more about the last of these. For any <img alt="{N \times N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%5Ctimes+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{r \leq N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Cleq+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> define the <em>rigidity</em> <img alt="{R_A(r)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_A%28r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be the minimum number of entries in which <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> differs from some matrix of rank (at most) <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The highest possible rigidity for rank <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{(N - r)^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28N+-+r%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, since zeroing out an <img alt="{(n-r)\times(n-r)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28n-r%29%5Ctimes%28n-r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> block leaves a matrix of rank at most <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Sufficiently random matrices meet this upper bound with high probability, but the best lower bounds for explicit families of matrices are <img alt="{\Omega(\frac{N^2}{r}\log\frac{N}{r})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28%5Cfrac%7BN%5E2%7D%7Br%7D%5Clog%5Cfrac%7BN%7D%7Br%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is only quasi-linear when <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is close to <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The question is whether we can inch this up to <img alt="{\Omega(n^{1+\epsilon})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5E%7B1%2B%5Cepsilon%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<blockquote><p><b>Definition 3</b> <em> A family of matrices <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <em>significantly rigid</em> if there is an <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that taking <img alt="{r = \frac{N}{\log\log N}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+%5Cfrac%7BN%7D%7B%5Clog%5Clog+N%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> makes <img alt="{R_{A_N}(r) = \Omega(N^{1+\epsilon})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_%7BA_N%7D%28r%29+%3D+%5COmega%28N%5E%7B1%2B%5Cepsilon%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </em>
</p></blockquote>
<p>
The interest in this definition comes from a lack of lower bounds on linear algebraic circuits computing natural families <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of linear transformations that seems even more extreme than our lack of super-linear lower bounds on Boolean circuits, nor better than <img alt="{\Omega(N\log N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28N%5Clog+N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for general algebraic circuits computing polynomials in <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> variables of degree <img alt="{B^{O(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. It is still consistent with our knowledge that every natural family <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can be computed by linear algebraic circuits of <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> size <b>and</b> <img alt="{O(\log N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> depth. Leslie Valiant in 1977 proved the following sufficient condition to improve this state of affairs.</p>
<blockquote><p><b>Theorem 4</b> <em> Every significantly rigid family <img alt="{A_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> cannot be computed by linear algebraic circuits of linear size and logarithmic depth. </em>
</p></blockquote>
<p>
So for coming on half a century the question has been:</p>
<blockquote><p><b> </b> <em> Can we construct a natural explicit family of significantly rigid matrices? </em>
</p></blockquote>
<p>
Beliefs that the Hadamard matrices provided such a family were <a href="https://arxiv.org/pdf/1611.05558.pdf">refuted</a> by Josh Alman and Ryan Williams at STOC 2017, and <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.3100">known</a> <a href="https://core.ac.uk/download/pdf/82556808.pdf">results</a> for Vandermonde matrices do not have <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> close enough to <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </p>
<p>
One hope had been to derive such matrices from explicit functions <img alt="{f_n(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_n%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> over <img alt="{\mathbb{Z}_p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> prime by taking <img alt="{N = p^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+p%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and defining </p>
<p align="center"><img alt="\displaystyle  A_{f_n}[\vec{x},\vec{y}] = f(x_1 + y_1,\dots,x_n + y_n). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A_%7Bf_n%7D%5B%5Cvec%7Bx%7D%2C%5Cvec%7By%7D%5D+%3D+f%28x_1+%2B+y_1%2C%5Cdots%2Cx_n+%2B+y_n%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Unfortunately, the polynomial method for cap sets shows that no such attempt can work. Zeev Dvir and Benjamin Edelman <a href="https://theoryofcomputing.org/articles/v015a008/">proved</a> that no matter how <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are chosen, there is <img alt="{\delta &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that for all large enough <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, </p>
<p align="center"><img alt="\displaystyle  R_{A_{f_n}}(N^{1-\delta}) &lt; n^{1+\epsilon}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R_%7BA_%7Bf_n%7D%7D%28N%5E%7B1-%5Cdelta%7D%29+%3C+n%5E%7B1%2B%5Cepsilon%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>This means we cannot get <img alt="{R_{A_{f_n}}(r) = \Omega(n^{1+\Omega(1)})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_%7BA_%7Bf_n%7D%7D%28r%29+%3D+%5COmega%28n%5E%7B1%2B%5COmega%281%29%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="{r = \frac{N}{\log\log N}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+%5Cfrac%7BN%7D%7B%5Clog%5Clog+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, indeed, far from it. What is most curious to us is that for matrix multiplication, the cap-set related technique frustrates a better complexity upper bound, whereas here it frustrates a better lower bound.</p>
<p>
</p><h2> Open Problems </h2><p/>
<p>What further applications can we find for the polynomial method?</p>
<p/></font></font></div>
    </content>
    <updated>2021-02-27T20:10:57Z</updated>
    <published>2021-02-27T20:10:57Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="Teaching"/>
    <category term="trick"/>
    <category term="algebraic complexity"/>
    <category term="cap sets"/>
    <category term="circuits"/>
    <category term="complexity"/>
    <category term="Dion Gijswijt"/>
    <category term="Ernie Croot"/>
    <category term="Henry Dudeney"/>
    <category term="Jordan Ellenberg"/>
    <category term="Joshua Grochow"/>
    <category term="Ketan Mulmuley"/>
    <category term="Leslie Valiant"/>
    <category term="lower bounds"/>
    <category term="no-three-in-row problem"/>
    <category term="Peter Pach"/>
    <category term="polynomial method"/>
    <category term="Stephen Mahaney"/>
    <category term="Vsevolod Lev"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-03-07T01:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/028</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/028" rel="alternate" type="text/html"/>
    <title>TR21-028 |  Branching Programs with Bounded Repetitions and $\mathrm{Flow}$ Formulas | 

	Anastasia Sofronova, 

	Dmitry Sokolov</title>
    <summary>Restricted branching programs capture various complexity measures like space in Turing machines or length of proofs in proof systems. In this paper, we focus on the application in the proof complexity that was discovered by Lovasz et al. '95 who showed the equivalence between regular Resolution and read-once branching programs for ``unsatisfied clause search problem'' ($\mathrm{Search}_{\varphi}$). This connection is widely used, in particular, in the recent breakthrough result about the Clique problem in regular Resolution by Atserias et al. '18.

We study the branching programs with bounded repetitions, so-called $(1, +k)$-BPs (Sieling '96) in application to the $\mathrm{Search}_{\varphi}$ problem. On the one hand, it is a natural generalization of read-once branching programs. On the other hand, this model gives a powerful proof system that can efficiently certify the unsatisfiability of a wide class of formulas that is hard for Resolution (Knop '17).


We deal with $\mathrm{Search}_{\varphi}$ that is ``relatively easy'' compared to all known hard examples for the $(1, +k)$-BPs. We introduce the first technique for proving exponential lower bounds for the $(1, +k)$-BPs on $\mathrm{Search}_{\varphi}$. To do it we combine a well-known technique for proving lower bounds on the size of branching programs (Sieling '96; Sieling, Wegener '94; Jukna, Razborov '98) with the modification of the ``closure'' technique (Alekhnovich et al. 04; Alekhnovich, Razborov '03). In contrast with the most Resolution lower bounds, our technique uses not only ``local'' properties of the formula, but also a ``global'' structure. Our hard examples are based on the $\mathrm{Flow}$ formulas introduced in (Alekhnovich, Razborov '03).</summary>
    <updated>2021-02-27T19:40:44Z</updated>
    <published>2021-02-27T19:40:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-07T01:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/26/phd-position-at-university-of-amsterdam-apply-by-march-18-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/26/phd-position-at-university-of-amsterdam-apply-by-march-18-2021/" rel="alternate" type="text/html"/>
    <title>PhD position at University of Amsterdam (apply by March 18, 2021)</title>
    <summary>The University of Amsterdam encourages applications for an open PhD position in the theory of quantum computing and quantum networks. Potential research topics include multi-party quantum computation, secure positioning, and multi-party communication complexity. Website: https://www.uva.nl/shared-content/uva/en/vacancies/2021/02/21-069-phd-position-on-the-theory-of-quantum-networks.html Email: f.speelman@uva.nl</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The University of Amsterdam encourages applications for an open PhD position in the theory of quantum computing and quantum networks. Potential research topics include multi-party quantum computation, secure positioning, and multi-party communication complexity.</p>
<p>Website: <a href="https://www.uva.nl/shared-content/uva/en/vacancies/2021/02/21-069-phd-position-on-the-theory-of-quantum-networks.html">https://www.uva.nl/shared-content/uva/en/vacancies/2021/02/21-069-phd-position-on-the-theory-of-quantum-networks.html</a><br/>
Email: f.speelman@uva.nl</p></div>
    </content>
    <updated>2021-02-26T10:38:43Z</updated>
    <published>2021-02-26T10:38:43Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-07T01:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/26/faculty-at-krea-university-india-apply-by-may-1-2021-2/</id>
    <link href="https://cstheory-jobs.org/2021/02/26/faculty-at-krea-university-india-apply-by-may-1-2021-2/" rel="alternate" type="text/html"/>
    <title>Faculty at KREA University, India (apply by May 1, 2021)</title>
    <summary>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science. Open House, 27th Feb 2021 9:00 AM [IST]: https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf Website: https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d Email: sias.chair_sciences@krea.edu.in</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science.</p>
<p>Open House, 27th Feb 2021 9:00 AM [IST]:<br/>
<a href="https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf">https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf</a></p>
<p>Website: <a href="https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d">https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d</a><br/>
Email: sias.chair_sciences@krea.edu.in</p></div>
    </content>
    <updated>2021-02-26T08:49:00Z</updated>
    <published>2021-02-26T08:49:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-07T01:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/25/tenure-track-open-rank-at-university-of-illinois-urbana-champaign-apply-by-june-1-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/25/tenure-track-open-rank-at-university-of-illinois-urbana-champaign-apply-by-june-1-2021/" rel="alternate" type="text/html"/>
    <title>Tenure Track (Open Rank) at University of Illinois, Urbana-Champaign (apply by March 15, 2021; or as soon as possible)</title>
    <summary>The Department of Computer Science at the University of Illinois Urbana-Champaign invites applications for full-time tenure-track faculty positions at all levels (Assistant Professor, Associate Professor, Full Professor). We particularly encourage applications in quantum computing, but also welcome applications from exceptional candidates in other areas. Website: https://cs.illinois.edu/about/positions/faculty-positions Email: chekuri@illinois.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Illinois Urbana-Champaign invites applications for full-time tenure-track faculty positions at all levels (Assistant Professor, Associate Professor, Full Professor). We particularly encourage applications in quantum computing, but also welcome applications from exceptional candidates in other areas.</p>
<p>Website: <a href="https://cs.illinois.edu/about/positions/faculty-positions">https://cs.illinois.edu/about/positions/faculty-positions</a><br/>
Email: chekuri@illinois.edu</p></div>
    </content>
    <updated>2021-02-25T22:59:03Z</updated>
    <published>2021-02-25T22:59:03Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-07T01:20:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2202248828009562800</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2202248828009562800/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/complexity-is-enemy-of-speed.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2202248828009562800" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2202248828009562800" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/complexity-is-enemy-of-speed.html" rel="alternate" type="text/html"/>
    <title>Complexity is the Enemy of Speed</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The title of this post came from an <a href="https://www.wsj.com/articles/connecticuts-covid-vaccine-lesson-11614124012">opinion piece</a> in the Wall Street Journal yesterday on vaccine distribution. Many attempts to get the vaccines to the right groups first have slowed down distribution and sometime even caused <a href="https://www.nbcnews.com/news/us-news/thousands-covid-19-vaccines-wind-garbage-because-fed-state-guidelines-n1254364">vaccines to go to waste</a>. Rules to help spread vaccines across minority groups often backfire. Often when some rules lead to inequity, we try to fix it with more rules when we need less much less. Attempts to distribute vaccines to multiple medical and pharmacy sites have made it difficult to get appointments even if you are eligible.</p><p>Randomness is the simplest way to fairness. The movie Contagion got it right, just choose birthdays by picking balls from a bin to distribute the vaccine. Then people can just show up at a few chosen sites with proof of birthday. No need to sign up.</p><p>You could argue to add back conditions like age, medical conditions, jobs but that just leads you down the same problematic path. The fastest way to get past this pandemic is to get vaccines into arms. Trust the randomness.</p></div>
    </content>
    <updated>2021-02-25T14:19:00Z</updated>
    <published>2021-02-25T14:19:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-03-07T00:30:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/25/faculty-at-krea-university-india-apply-by-may-1-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/25/faculty-at-krea-university-india-apply-by-may-1-2021/" rel="alternate" type="text/html"/>
    <title>Faculty at KREA University, India (apply by May 1, 2021)</title>
    <summary>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science. Open House, 27th Feb 2021 9:00 AM [IST]: https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf Website: https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d Email: sias.chair_sciences@krea.edu.in</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science.</p>
<p>Open House, 27th Feb 2021 9:00 AM [IST]:<br/>
<a href="https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf">https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf</a></p>
<p>Website: <a href="https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d">https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d</a><br/>
Email: sias.chair_sciences@krea.edu.in</p></div>
    </content>
    <updated>2021-02-25T09:47:05Z</updated>
    <published>2021-02-25T09:47:05Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-07T01:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=532</id>
    <link href="https://tcsplus.wordpress.com/2021/02/24/tcs-talk-wednesday-march-3-steve-hanneke-ttic/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, March 3 — Steve Hanneke, TTIC</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, March 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Steve Hanneke from TTIC will speak about “A Theory of Universal Learning” (abstract below). You can reserve a spot as an individual or a group to join us […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, March 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Steve Hanneke</strong> from TTIC will speak about “<em>A Theory of Universal Learning</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> aftwerwards, so people who did not sign up will still be able to watch the talk.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its “learning curve”, that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy.</p>
<p>In this work, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this work is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case.</p>
<p>Joint work with Olivier Bousquet, Shay Moran, Ramon van Handel, and Amir Yehudayoff.</p></blockquote></div>
    </content>
    <updated>2021-02-25T02:03:43Z</updated>
    <published>2021-02-25T02:03:43Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-03-07T01:21:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8008</id>
    <link href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/" rel="alternate" type="text/html"/>
    <title>Unsupervised Learning and generative models</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Scribe notes by Richard Xu Previous post: What do neural networks learn and when do they learn it Next post: TBD. See also all seminar posts and course webpage. lecture slides (pdf) – lecture slides (Powerpoint with animation and annotation) – video In this lecture, we move from the world of supervised learning to unsupervised … <a class="more-link" href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">Continue reading <span class="screen-reader-text">Unsupervised Learning and generative models</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Scribe notes by <a href="https://github.com/rxu18">Richard Xu</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/">What do neural networks learn and when do they learn it</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/?order=asc">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_3.pdf">lecture slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_3.pptx">lecture slides (Powerpoint with animation and annotation)</a> – <a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=70cafab0-bdea-412b-a353-acc90173fd61">video</a></p>



<p>In this lecture, we move from the world of supervised learning to unsupervised learning, with a focus on generative models. We will</p>



<ul><li>Introduce unsupervised learning and the relevant notations.</li><li>Discuss various approaches for generative models, such as PCA, VAE, Flow Models, and GAN.</li><li>Discuss theoretical and practical results we currently have for these approaches.</li></ul>



<h2>Setup for Unsupervised Learning</h2>



<p>In <em>supervised learning</em>, we have data <img alt="x_i\sim p\subset \mathbb R^d" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%5Csim+p%5Csubset+%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i\sim p\subset \mathbb R^d"/> and we want to understand the distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>. For example,</p>



<ol><li><em>Probability estimation:</em> Given <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, can we compute/approximate <img alt="p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)"/> (the probability that <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is output under <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>)?</li><li><em>Generation:</em> Can we sample from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, or from a “nearby” distribution?</li><li><em>Encoding:</em> Can we find a representation <img alt="E:\mathrm{Support}(p) \rightarrow \mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathrm%7BSupport%7D%28p%29+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathrm{Support}(p) \rightarrow \mathbb{R}^r"/> such that for <img alt="x \sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x \sim p"/>, <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> makes it easy to answer semantic questions on <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>? And such that <img alt="\langle E(x) , E(x') \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+E%28x%29+%2C+E%28x%27%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle E(x) , E(x') \rangle"/> corresponds to “semantic similarity” of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> and <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x'"/>?</li><li><em>Prediction:</em> We would like to be able to predict (for example) the second half of <img alt="x \sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x \sim p"/> from the first half. More generally, we want to solve the <em>conditional generation</em> task, where given some function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> (e.g., the projection to the first half) and some value <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/>, we can sample from the conditional probability distribution <img alt="p|f(x)=y" class="latex" src="https://s0.wp.com/latex.php?latex=p%7Cf%28x%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p|f(x)=y"/>.</li></ol>



<p>Our “dream” is to solve all of those by the following setup:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/EPyXsSW.png"/></figure>



<p>There is an “encoder” <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> that maps <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> into a representation <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> in the latent space, and then a “decoder” <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> that can transform such a representation back into <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. We would like it to be the case that:</p>



<ol><li><em>Generation:</em> For <img alt="x\sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim p"/>, the induced distribution <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> is “nice” and efficiently sampleable (e.g., the standard normal <img alt="N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,I)"/> over <img alt="\mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{R}^r"/>) such that we can (approximately) sample from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> by sampling <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> and outputting <img alt="D(z)" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)"/>.</li><li><em>Density estimation:</em> We would like to be able to evaluate the probability that <img alt="D(z)=x" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)=x"/>. For example, if <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> is the inverse of <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/>, and <img alt="z \sim N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Csim+N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z \sim N(0,I)"/> we could do so by computing <img alt="| E(x) |" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+E%28x%29+%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| E(x) |"/>.</li><li><em>Semantic representation:</em> We would like the latent representation <img alt="E(z)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(z)"/> to map <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> into meaningful latent space. Ideally, linear directions in this space will correspond to semantic attributes.</li><li><em>Conditional sampling:</em> We would like to be able to do conditional generation, and in particular for some functions <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> and values <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/>, be able to sample from the set of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/>‘s such that <img alt="f(E(z))=y" class="latex" src="https://s0.wp.com/latex.php?latex=f%28E%28z%29%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(E(z))=y"/></li></ol>



<p>Ideally, if we could map images to the latent variables used to generate them and vice versa (as in the cartoon from the last lecture), then we could achieve these goals:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/3X4aqfl.png"/></figure>



<p>At the moment, we do not have a single system that can solve all these problems for a natural domain such as images or language, but we have several approaches that achieve part of the dream.</p>



<p><strong>Digressions.</strong> Before discussing concrete models, we make three digressions. One will be non-technical, and the other three technical. The three technical digressions are the following:</p>



<ol><li>If we have multiple objectives, we want a way to interpolate between them.</li><li>To measure how good our models are, we have to measure distances between statistical distributions.</li><li>Once we come up with generating models, we would <em>metrics</em> for measuring how good they are.</li></ol>



<h2>Non-technical digression: Is deep learning a cargo cult science? (spoiler: no)</h2>



<p>In an <a href="https://calteches.library.caltech.edu/51/2/CargoCult.htm">influential essay</a>, Richard Feynman coined the term “cargo cult science” for the activities that have superficial similarities to science but do not follow the scientific method. Some of the tools we use in machine learning look suspiciously close to “cargo cult science.” We use the tools of classical learning, but in a setting in which they were not designed to work in and on which we have no guarantees that they will work. For example, we run (stochastic) gradient descent – an algorithm designed to minimize a convex function – to minimize convex loss. We also write use <em>empirical risk minimization</em> – minimizing loss on our training set – in a setting where we have no guarantee that it will not lead to “overfitting.”</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/tBw6UsX.png"/></figure>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/s5G6xfj.png"/></figure>



<p>And yet, unlike the original cargo cults, in deep learning, “the planes do land”, or at least they often do. When we use a tool <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> in a situation <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X"/> that it was not designed to work in, it can play out in one (or mixture) of the following scenarios:</p>



<ul><li><strong>Murphy’s Law:</strong> “Anything that can go wrong will go wrong.” As computer scientists, we are used to this scenario. The natural state of our systems is that they have bugs and errors. There is a reason why software engineering talks about “contracts”, “invariants”, preconditions” and “postconditions”: typically, if we try to use a component <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> in a situation that it wasn’t designed for, it will not turn out well. This is doubly the case in security and cryptography, where people have learned the hard way time and again that Murphy’s law holds sway.</li><li><strong>“Marley’s Law”:</strong> “Every little thing gonna be alright”. In machine learning, we sometimes see the opposite phenomenon- we use algorithms outside the conditions under which they have been analysed or designed to work in, but they still produce good results. Part of it could be because ML algorithms are already robust to certain errors in their inputs, and their output was only guaranteed to be approximately correct in the first place.</li></ul>



<p>Murphy’s law does occasionally pop up, even in machine learning. We will see examples of both phenomena in this lecture.</p>



<h2>Technical digression 1: Optimization with Multiple Objectives</h2>



<p>During machine learning, we often have multiple objectives to optimize. For example, we may want both an efficient encoder and an effective decoder, but there is a tradeoff between them.</p>



<p>Suppose we have 2 loss functions <img alt="L_1(w)" class="latex" src="https://s0.wp.com/latex.php?latex=L_1%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1(w)"/> and <img alt="L_2(w)" class="latex" src="https://s0.wp.com/latex.php?latex=L_2%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_2(w)"/>, but there can be a trade off between them. The <em>pareto curve</em> is the set <img alt="P={(a,b): \forall w\in W, L_1(w)\ge a\vee L_2(w)\ge b.}" class="latex" src="https://s0.wp.com/latex.php?latex=P%3D%7B%28a%2Cb%29%3A+%5Cforall+w%5Cin+W%2C+L_1%28w%29%5Cge+a%5Cvee+L_2%28w%29%5Cge+b.%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P={(a,b): \forall w\in W, L_1(w)\ge a\vee L_2(w)\ge b.}"/></p>



<figure class="wp-block-image"><img alt="Pareto curve for 2 loss functions" src="https://i.imgur.com/QbPRQtR.jpg"/></figure>



<p>If a model is above the curve, it is not optimal. If it is below the curve, the model is infeasible.</p>



<p>When the set <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> is convex, we can reach any point on the curve <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> by minimizing <img alt="L_1(w)+\lambda L_2(w)" class="latex" src="https://s0.wp.com/latex.php?latex=L_1%28w%29%2B%5Clambda+L_2%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1(w)+\lambda L_2(w)"/>. The proof is by the picture above: for any point <img alt="(a_0,b_0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(a_0,b_0)"/> on the curve, there is a tangent line at <img alt="(a_0,b_0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(a_0,b_0)"/> that is strictly below the curve. If <img alt="a+\lambda b" class="latex" src="https://s0.wp.com/latex.php?latex=a%2B%5Clambda+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="a+\lambda b"/> is the normal vector for this line, then the global minimum of <img alt="a+\lambda b" class="latex" src="https://s0.wp.com/latex.php?latex=a%2B%5Clambda+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="a+\lambda b"/> on the feasible set will be <img alt="(a_0,b_0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(a_0,b_0)"/>.<br/>This motivates the common practice of minimizing two introducing a hyperparameter <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda"/> to aggregate two objectives into one.</p>



<p>When <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> is not convex, it may well be that:</p>



<ul><li>Some points on <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> are not minima of <img alt="L_1 + \lambda L_2" class="latex" src="https://s0.wp.com/latex.php?latex=L_1+%2B+%5Clambda+L_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1 + \lambda L_2"/></li><li><img alt="L_1 + \lambda L_2" class="latex" src="https://s0.wp.com/latex.php?latex=L_1+%2B+%5Clambda+L_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1 + \lambda L_2"/> might have multiple minima</li><li>Depending on the path one takes, it is possible to get “stuck” in a point that is <em>not</em> a global minima</li></ul>



<p>The following figure demonstrates all three possibilities</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Rjg4iZU.png"/></figure>



<p>Par for the course, this does not stop people in machine learning from using this approach to minimize different objectives, and often “Marley’s Law” holds, and this works fine. But this is not always the case. A <a href="https://engraved.ghost.io/why-machine-learning-algorithms-are-hard-to-tune/">nice blog post by Degrave and Kurshonova</a> discusses this issue and why sometimes we do in fact, see “Murphy’s law” when we combine objectives. They also detail some other approaches for combining objectives, but there is no single way that will work in all cases.</p>



<p>Figure from Degrave-Kurshonova demonstrating where the algorithm could reach in the non-convex case depending on initialization and <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda"/>:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/4VZZaRR.gif"/></figure>



<h2>Technical digression 2: Distances between probability measures</h2>



<p>Suppose we have two distributions <img alt="p,q" class="latex" src="https://s0.wp.com/latex.php?latex=p%2Cq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p,q"/> over <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/>. There are two common ways of measuring the distances between them.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/5JTzg6D.png"/></figure>



<p>The <em>Total Variance (TV)</em> (also known as statistical distance) between <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is equal to</p>



<p><img alt="\Delta_{TV}(p,q)=\frac12 \sum_{x\in D}|p(x)-q(x)| = \max_{f:D\to {0,1}} | \mathbb{E}_p(f)-\mathbb{E}_q(f)|." class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29%3D%5Cfrac12+%5Csum_%7Bx%5Cin+D%7D%7Cp%28x%29-q%28x%29%7C+%3D+%5Cmax_%7Bf%3AD%5Cto+%7B0%2C1%7D%7D+%7C+%5Cmathbb%7BE%7D_p%28f%29-%5Cmathbb%7BE%7D_q%28f%29%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q)=\frac12 \sum_{x\in D}|p(x)-q(x)| = \max_{f:D\to {0,1}} | \mathbb{E}_p(f)-\mathbb{E}_q(f)|."/></p>



<p>The second equality can be proved by constructing <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> that outputs 1 on <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> where <img alt="p(x)-q(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29-q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)-q(x)"/> and vice versa. The <img alt="\max_f" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\max_f"/> definition has a crypto-flavored interpretation: For any adversary <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>, the TV measures the advantage they can have over half of determining whether <img alt="x\sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim p"/> or <img alt="x\sim q" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim q"/>.</p>



<p>Second, the <em>Kullback–Leibler (KL) Divergence</em> between <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is equal to</p>



<p><img alt="\Delta_{KL}(p||q)=\mathbb{E}_{x\sim p}(\log p(x)/q(x))." class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%3D%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+p%28x%29%2Fq%28x%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)=\mathbb{E}_{x\sim p}(\log p(x)/q(x))."/></p>



<p>(The total variation distance is symmetric, in the sense that <img alt="\Delta_{TV}(p,q)=\Delta_{TV}(q,p)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29%3D%5CDelta_%7BTV%7D%28q%2Cp%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q)=\Delta_{TV}(q,p)"/>, but the KL divergence is not. Both have the property that they are non-negative and equal to zero if and only if <img alt="p=q" class="latex" src="https://s0.wp.com/latex.php?latex=p%3Dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=q"/>.)</p>



<p>Unlike the total variation distance, which is bounded between <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> and <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/>, the KL divergence can be arbitrarily large and even infinite (though it can be shown using the concavity of log that it is always non-negative). To interpret the KL divergence, it is helpful to separate between the case that <img alt="\Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)"/> is close to zero and the case where it is a large number. If <img alt="\Delta_{KL}(p||q) \approx \delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx \delta"/> for some <img alt="\delta \ll 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cll+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta \ll 1"/>, then we would need about <img alt="1/\delta" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\delta"/> samples to distinguish between samples of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and samples of <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>. In particular, suppose that we get <img alt="x_1,\ldots,x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_n"/> and we want to distinguish between the case that we they were independently sampled from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and the case that they were independently sampled from <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>. A natural (and as it turns out, optimal) approach is to use a <em>likelihood ratio test</em> where we decide the samples came from <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T"/> if <img alt="\Pr_p[x_1,\ldots,x_n]/\Pr_q[x_1,\ldots,x_n]&gt;T" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr_p%5Bx_1%2C%5Cldots%2Cx_n%5D%2F%5CPr_q%5Bx_1%2C%5Cldots%2Cx_n%5D%3ET&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Pr_p[x_1,\ldots,x_n]/\Pr_q[x_1,\ldots,x_n]&gt;T"/>. For example, if we set <img alt="T=20" class="latex" src="https://s0.wp.com/latex.php?latex=T%3D20&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T=20"/> then this approach will guarantee that our “false positive rate” (announcing that samples came from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> when they really came from <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>) will be most <img alt="1/20=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F20%3D5%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/20=5\%"/>. Taking logs and using the fact that the probability of these independent samples is the product of probabilities, this amounts to testing whether <img alt="\sum_{i=1}^n \log \left(\tfrac{p(x_i)}{q(x_i)}\right) \geq \log T" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5En+%5Clog+%5Cleft%28%5Ctfrac%7Bp%28x_i%29%7D%7Bq%28x_i%29%7D%5Cright%29+%5Cgeq+%5Clog+T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_{i=1}^n \log \left(\tfrac{p(x_i)}{q(x_i)}\right) \geq \log T"/>. When samples come from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, the expectation of the righthand side is <img alt="n\cdot \Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ccdot+%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n\cdot \Delta_{KL}(p||q)"/>, so we see that to ensure <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T"/> is larger than <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/> we need the number samples to be at least <img alt="1/\Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\Delta_{KL}(p||q)"/> (and as it turns out, this will do).</p>



<p>When the <img alt="KL" class="latex" src="https://s0.wp.com/latex.php?latex=KL&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="KL"/> divergence is a large number <img alt="k&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=k%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k&gt;1"/>, we can think of it as the number of bits of “surprise” in <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> as opposed to <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>. For example, in the common case where <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is obtained by conditioning <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> on some event <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/>, <img alt="\Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)"/> will typically be <img alt="\log 1/\Pr[A]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+1%2F%5CPr%5BA%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log 1/\Pr[A]"/> (some fine print applies). In general, if <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is obtained from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> by revealing <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/> bits of information (i.e., by conditioning on a random variable whose mutual information with <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> is <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/>) then <img alt="\Delta_{KL}(p||q)=k" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%3Dk&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)=k"/>.</p>



<p><strong>Generalizations:</strong> The total variation distance is a special case of metrics of the form <img alt="\Delta(p,q) = \max_{f \in \mathcal{F}} |\mathbb{E}_{x\sim p} f(x) - \mathbb{E}_{x \sim q} f(x)|" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta%28p%2Cq%29+%3D+%5Cmax_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D+%7C%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D+f%28x%29+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+f%28x%29%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta(p,q) = \max_{f \in \mathcal{F}} |\mathbb{E}_{x\sim p} f(x) - \mathbb{E}_{x \sim q} f(x)|"/>. These are known as <a href="https://arxiv.org/abs/0901.2698">integral probability metrics</a> and include examples such as the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. KL divergence is a special case of divergence measures known as <a href="https://en.wikipedia.org/wiki/F-divergence"><img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>-divergence</a>, which are measures of the form <img alt="\Delta_f(p||q)= \mathbb{E}_{x \sim q} f\left(\tfrac{p(x)}{q(x)}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_f%28p%7C%7Cq%29%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+f%5Cleft%28%5Ctfrac%7Bp%28x%29%7D%7Bq%28x%29%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_f(p||q)= \mathbb{E}_{x \sim q} f\left(\tfrac{p(x)}{q(x)}\right)"/>. The KL divergence is obtained by setting <img alt="f(t) = t \log t" class="latex" src="https://s0.wp.com/latex.php?latex=f%28t%29+%3D+t+%5Clog+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(t) = t \log t"/>. (In fact even the TV distance is a special case of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> divergence by setting <img alt="f(t)=|t-1|/2" class="latex" src="https://s0.wp.com/latex.php?latex=f%28t%29%3D%7Ct-1%7C%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(t)=|t-1|/2"/>.)</p>



<p><strong>Normal distributions:</strong> It is a useful exercise to calculate the TV and KL distances for normal random variables. If <img alt="p=N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3DN%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=N(0,1)"/> and <img alt="q=N(-\epsilon,1)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%28-%5Cepsilon%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(-\epsilon,1)"/>, then since most probability mass in the regime where <img alt="p(x) \approx (1\pm \epsilon) q(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Capprox+%281%5Cpm+%5Cepsilon%29+q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x) \approx (1\pm \epsilon) q(x)"/>, <img alt="\Delta_{TV}(p,q) \approx \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29+%5Capprox+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q) \approx \epsilon"/> (i.e., up to some multiplicative constant). For KL divergence, if we selected <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> from a normal between <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> then with probability about half we’ll have <img alt="p(x) \approx q(x)(1+\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Capprox+q%28x%29%281%2B%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x) \approx q(x)(1+\epsilon)"/> and with probability about half we will have <img alt="p(q) \approx q(x)(1-\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28q%29+%5Capprox+q%28x%29%281-%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(q) \approx q(x)(1-\epsilon)"/>. By selecting <img alt="x\sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim p"/>, we increase probability of the former to <img alt="\approx 1/2+\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+1%2F2%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\approx 1/2+\epsilon"/> and the decrease the probability of the latter to <img alt="\approx 1/2 - \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+1%2F2+-+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\approx 1/2 - \epsilon"/>. So we have <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\epsilon"/> bias towards <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>‘s where <img alt="p(x)/q(x) \approx 1+\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29%2Fq%28x%29+%5Capprox+1%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)/q(x) \approx 1+\epsilon"/>, or <img alt="\log p(x)/q(x) \approx \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+p%28x%29%2Fq%28x%29+%5Capprox+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log p(x)/q(x) \approx \epsilon"/>. Hence <img alt="\Delta_{KL}(p||q) \approx \epsilon^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx \epsilon^2"/>. The above generalizes to higher dimensions. If <img alt="p= N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3D+N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p= N(0,I)"/> is a <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/>-variate normal, and <img alt="q=N(\mu,I)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%28%5Cmu%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(\mu,I)"/> for <img alt="\mu \in \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mu \in \mathbb{R}^d"/>, then (for small <img alt="|\mu|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cmu%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|\mu|"/>) <img alt="\Delta_{TV}(p,q) \approx |\mu|" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29+%5Capprox+%7C%5Cmu%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q) \approx |\mu|"/> while <img alt="\Delta_{KL}(p||q)\approx |\mu|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%5Capprox+%7C%5Cmu%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)\approx |\mu|^2"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/w3aXr99.png"/></figure>



<p>If <img alt="p=N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3DN%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=N(0,1)"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is a “narrow normal” of the form <img alt="q=N(0,\epsilon^2)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%280%2C%5Cepsilon%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(0,\epsilon^2)"/> then their TV distance is close to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/> while <img alt="\Delta_{KL}(p||q) \approx 1/\epsilon^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+1%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx 1/\epsilon^2"/>. In the <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> dimensional case, if <img alt="p=N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=N(0,I)"/> and <img alt="q=N(0,V)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%280%2CV%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(0,V)"/> for some covariance matrix <img alt="V" class="latex" src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="V"/>, then <img alt="\Delta_{KL}(p||q) \approx \mathrm{Tr}(V^{-1}) - d + \ln \det V" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cmathrm%7BTr%7D%28V%5E%7B-1%7D%29+-+d+%2B+%5Cln+%5Cdet+V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx \mathrm{Tr}(V^{-1}) - d + \ln \det V"/>. The two last terms are often less significant. For example if <img alt="V = \epsilon^2 I" class="latex" src="https://s0.wp.com/latex.php?latex=V+%3D+%5Cepsilon%5E2+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="V = \epsilon^2 I"/> then <img alt="\delta_{KL}(p||q) \approx d/\epsilon^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+d%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta_{KL}(p||q) \approx d/\epsilon^2"/>.</p>



<h2>Technical digression 3: benchmarking generative models</h2>



<p>Given a distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> of natural data and a purported generative model <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/>, how do we measure the quality of <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/>?</p>



<p>A natural measure is the KL divergence <img alt="\Delta_{KL}(p||g)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||g)"/> but it can be hard to evaluate, since it involves the term <img alt="p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)"/> which we cannot evaluate. However, we can rewrite the KL divergence as <img alt="\mathbb{E}_{x\sim p}(\log p(x)) - \mathbb{E}_{x\sim p}(\log q(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+p%28x%29%29+-+%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+q%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_{x\sim p}(\log p(x)) - \mathbb{E}_{x\sim p}(\log q(x))"/>. The term <img alt="\mathbb{E}{x\sim p} \log p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%7Bx%5Csim+p%7D+%5Clog+p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}{x\sim p} \log p(x)"/> is equal to <img alt="-H(p)" class="latex" src="https://s0.wp.com/latex.php?latex=-H%28p%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-H(p)"/> where <img alt="H(p)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p)"/> is the <em>entropy</em> of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>. The term <img alt="-\mathbb{E}_{x \sim p} \log q(x)" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+%5Clog+q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\mathbb{E}_{x \sim p} \log q(x)"/> is known as the <em>cross entropy </em>of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>. Note that the cross-entropy of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> is simply the expectation of the negative log likelihood of <img alt="g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g(x)"/> for <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> sampled from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>.</p>



<p>When <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> is fixed, minimizing <img alt="\Delta_{KL}(p||g)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||g)"/> corresponds to minimizing the cross entropy <img alt="H(p,g)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p,g)"/> or equivalently, maximizing the log likelihood. This is useful since often is the case that we can sample elements from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> (e.g., natural images) but can only evaluate the probability function for <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/>. Hence a common metric in such cases is minimizing the cross-entropy / negative log likelihood <img alt="H(p,g)= -\mathbb{E}_{x sim p} \log g(x) = \mathbb{E}_{x \sim p} \log (1/g(x))" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2Cg%29%3D+-%5Cmathbb%7BE%7D_%7Bx+sim+p%7D+%5Clog+g%28x%29+%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+%5Clog+%281%2Fg%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p,g)= -\mathbb{E}_{x sim p} \log g(x) = \mathbb{E}_{x \sim p} \log (1/g(x))"/>. For images, a common metric is “bits per pixel” which simply equals <img alt="H(p,q)/d" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2Cq%29%2Fd&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p,q)/d"/> where <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> is the length of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. Another metric (often used in natural language processing) is perplexity, which interchanges the expectation and the logarithm. The logarithm of the perplexity of <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> is <img alt="- \tfrac{1}{d}\log \mathbb{E}_{x \sim p} g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=-+%5Ctfrac%7B1%7D%7Bd%7D%5Clog+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="- \tfrac{1}{d}\log \mathbb{E}_{x \sim p} g(x)"/> where <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> is the length of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> (e.g., in tokens). Another way to write this is that log of the perplexity is the average of <img alt="\log g(x_i|x{&lt;i})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+g%28x_i%7Cx%7B%3Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log g(x_i|x{&lt;i})"/> where <img alt="g(x_i|x_{&lt;i})" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x_i%7Cx_%7B%3Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g(x_i|x_{&lt;i})"/> is the probability of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/> under <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> conditioned on the first <img alt="i-1" class="latex" src="https://s0.wp.com/latex.php?latex=i-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i-1"/> parts of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>.</p>



<p><strong>Memorization for log-likelihood.</strong> The issue of “overfitting” is even more problematic for generative models than for classifiers. Given samples <img alt="x_1,\ldots,x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_n"/> and enough parameters, we can easily come up with a model corresponding to the uniform distribution <img alt="{ x_1,\ldots, x_n }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+x_1%2C%5Cldots%2C+x_n+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ x_1,\ldots, x_n }"/>. This is obviously a useless model that will never generate new examples. However, this model will not only get a large log likelihood value on the training set, in fact, it will get <em>even better log likelihood</em> than the true distribution! For example, any reasonable natural distribution on images would have at least tens of millions, if not billions or trillions of potential images. In contrast, a typical training set might have fewer than 1M samples. Hence, unlike in the classification setting, for generation, the “overfitting” model will not only match but can, in fact, beat the ground truth. (This is reminiscent of the following quote from <a href="https://etc.usf.edu/lit2go/86/peter-pan/1602/chapter-12-the-children-are-carried-off/">Peter and Wendy</a>: <em>“Not a sound is to be heard, save when they give vent to a wonderful imitation of the lonely call of the coyote. The cry is answered by other braves; and some of them do it even better than the coyotes, who are not very good at it.”</em>)</p>



<p>If we cannot compute the density function, then benchmarking becomes more difficult. What often happens in practice is an “I know it when I see it” approach. The paper includes a few pictures generated by the model, and if the pictures look realistic, we think it is a good model. However, this can be deceiving. After all, we are feeding in good pictures into the model, so generating a good photo may not be particularly hard (e.g. the model might memorize some good pictures and use those as outputs).</p>



<p>There is another metric called the <em>inception score</em>, which loosely corresponds to how similar the “inception” neural network finds the GAN model to ImageNet (in the sense that inception thinks it covers many of the ImageNet classes and that produces images on which inception has high confidence)  but it too has its problems. <a href="https://arxiv.org/pdf/1905.10887.pdf">Ravuri-Vinyalis 2019</a> used a GAN model with a good inception score used its outputs to train a different model on ImageNet. Despite the high inception score (which should have indicated that the GANs output are as good as ImageNets) the accuracy when training on the GAN output dropped from the original value of <img alt="74\%" class="latex" src="https://s0.wp.com/latex.php?latex=74%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="74\%"/> to as low as <img alt="5\%" class="latex" src="https://s0.wp.com/latex.php?latex=5%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="5\%"/>!  (Even in the best case, accuracy dropped by at least 30 points.) Compare this with the  11-14% drop when we train on ImageNet and test on <a href="https://arxiv.org/abs/1902.10811">ImageNet v2</a>.</p>



<p>This figure from <a href="https://arxiv.org/abs/1701.00160">Goodfellow’s tutorial</a> describes generative models where we know and don’t know how to compute the density function:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/3hVJBPl.png"/></figure>



<h1>Auto Encoder / Decoder</h1>



<p>We now shift our attention to the encoder/decoder architecture mentioned above.</p>



<p>Recall that we want to understand <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, generate new elements <img alt="x^*" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^*"/>, and find a good representation of the elements. Our dream is to solve all of the issues with auto encoder/decoder, whose setup is as follows:</p>



<figure class="wp-block-image"><img alt="Setup for Auto Encoder/Decoder" src="https://i.imgur.com/udIY089.png"/></figure>



<p>That is, we want <img alt="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathbb{R}^d \rightarrow \mathbb{R}^r"/>, <img alt="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathbb%7BR%7D%5Er+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D:\mathbb{R}^r \rightarrow \mathbb{R}^d"/> such that</p>



<ul><li><img alt="D(E(x)) \approx x" class="latex" src="https://s0.wp.com/latex.php?latex=D%28E%28x%29%29+%5Capprox+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(E(x)) \approx x"/></li><li>The representation <img alt="E,D" class="latex" src="https://s0.wp.com/latex.php?latex=E%2CD&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E,D"/> enables us to solve tasks such as generation, classification, etc..</li></ul>



<p>To each the first point, we can aim to minimize <img alt="\sum_i ||x_i - D(E(x_i))||^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_i+%7C%7Cx_i+-+D%28E%28x_i%29%29%7C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_i ||x_i - D(E(x_i))||^2"/>. However, we can of course, make this loss zero by letting <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> and <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> be the identity function. Much of the framework of generative models can be considered as placing some restrictions on the “communication channel” that rule out this trivial approach, with the hope that would require the encoder and decoder to “intelligently” correspond to the structure of the natural data.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/XW6cDE7.png"/></figure>



<h2>Auto Encoders: noiseless short <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/></h2>



<p>A natural idea is to simply restrict the dimension of the latent space to be small (<img alt="r \ll d" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cll+d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r \ll d"/>). In principle, the optimal compression scheme for a probability distribution will require knowing the distribution. Moreover, the optimal compression will maximize the entropy of the latent data <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/>. Since the maximum entropy distribution is uniform (in the discrete case), we could easily sample from it. (In the continuous setting, the standard normal distribution plays the role of the uniform distribution.)</p>



<p>For starter, consider the case of picking <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> to be small and minimizing <img alt="\sum ||x_i - D(E(x_i))||^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+%7C%7Cx_i+-+D%28E%28x_i%29%29%7C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum ||x_i - D(E(x_i))||^2"/> for <em>linear</em> <img alt="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathbb{R}^d \rightarrow \mathbb{R}^r"/>, <img alt="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathbb%7BR%7D%5Er+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D:\mathbb{R}^r \rightarrow \mathbb{R}^d"/>. Since <img alt="DE" class="latex" src="https://s0.wp.com/latex.php?latex=DE&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="DE"/> is a rank <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> matrix, we can write this as finding a rank <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> matrix <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L"/> that minimizes <img alt="| (I-L)X|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+%28I-L%29X%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| (I-L)X|^2"/> where <img alt="X = (x_1,\ldots,x_n)" class="latex" src="https://s0.wp.com/latex.php?latex=X+%3D+%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X = (x_1,\ldots,x_n)"/> is our input data. It can be shown that <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L"/> that would minimize this will be the projection to the top <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> eigenvectors of <img alt="XX^\top" class="latex" src="https://s0.wp.com/latex.php?latex=XX%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="XX^\top"/> which exactly corresponds to <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis (PCA)</a>.</p>



<p>In the nonlinear case, we can obtain better compression. However, we do not achieve our other goals:</p>



<ul><li>It is not the case that we can generate realistic data by sampling uniform/normal <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> and output <img alt="D(z)" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)"/></li><li>It is not the case that semantic similarity between <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> and <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x'"/> corresponds to large dot product between <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> and <img alt="E(x')" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x')"/>.</li></ul>



<p>It seems that model just rediscovers a compression algorithm like JPEG. We do not expect the JPEG encoding of an image to be semantically informative, and JPEG decoding of a random file will not be a good way to generate realistic images. It turns out that sometimes “Murphy’s law” does hold and if it’s possible to minimize the loss in a not very useful way then that will indeed be the case.</p>



<h2>Variational Auto Encoder (VAE)</h2>



<p>We now discuss <em>variational auto encoders</em> (VAEs). We can think of these as generalization auto-encoders to the case where the channel has some Gaussian noise. We will describe VAEs in two nearly equivalent ways:</p>



<ul><li>We can think of VAEs as trying to optimize two objectives: both the auto-encoder objective of minimizing <img alt="| D(E(x))-x|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+D%28E%28x%29%29-x%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| D(E(x))-x|^2"/> and another objective of minimizing the KL divergence between <img alt="D(x)" class="latex" src="https://s0.wp.com/latex.php?latex=D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(x)"/> and the standard normal distribution <img alt="N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,I)"/>.</li><li>We can think of VAEs as trying to maximize a proxy for the log-likelihood. This proxy is a quantity known as the “Evidence Lower Bound (ELBO)” which we can evaluate using <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> and <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> and is always smaller or equal to the log-likelihood.</li></ul>



<p>We start with the first description. One view of VAEs is that we search for a pair <img alt="E,D" class="latex" src="https://s0.wp.com/latex.php?latex=E%2CD&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E,D"/> of encoder and decoder that are aimed at minimizing the following two objectives:</p>



<ul><li><img alt="| x - D(E(x))|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+x+-+D%28E%28x%29%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| x - D(E(x))|^2"/> (standard AE objective)</li><li><img alt="\Delta_{KL}( E(x) || N(0,I) )" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28+E%28x%29+%7C%7C+N%280%2CI%29+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}( E(x) || N(0,I) )"/> (distance of latent from the standard normal)</li></ul>



<p>To make the second term a function of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, we consider <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> as a probability distribution with respect to a <em>fixed</em> <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. To ensure this makes sense, we need to make <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> <em>randomized</em>. A randomized Neural network has “sampling neurons” that take no input, have parameters <img alt="\mu,\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%2C%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mu,\sigma"/> and produce an element <img alt="v \sim N(\mu,\sigma^2)" class="latex" src="https://s0.wp.com/latex.php?latex=v+%5Csim+N%28%5Cmu%2C%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v \sim N(\mu,\sigma^2)"/>. We can train such a network by fixing a random <img alt="t \sim N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Csim+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="t \sim N(0,1)"/> and defining the neuron to simply output <img alt="\mu + \sigma t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%2B+%5Csigma+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mu + \sigma t"/>.</p>



<p><strong>ELBO derivation:</strong> Another view of VAEs is that they aim at maximizing a term known as the evidence lower bound or ELBO. We start by deriving this bound. Let <img alt="Z=N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z=N(0,I)"/> be the standard normal distribution over the latent space. Define <img alt="p_x" class="latex" src="https://s0.wp.com/latex.php?latex=p_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p_x"/> to be the distribution of <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z"/> conditioned on <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> decoding to <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> (i.e., <img alt="Z= z\sim Z|D(z)=x" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3D+z%5Csim+Z%7CD%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z= z\sim Z|D(z)=x"/>, and define <img alt="q_x" class="latex" src="https://s0.wp.com/latex.php?latex=q_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q_x"/> be the distribution <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/>. Since <img alt="\Delta_{KL}(q_x||p_x) \geq 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28q_x%7C%7Cp_x%29+%5Cgeq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(q_x||p_x) \geq 0"/>, we know that</p>



<p><img alt="0 \leq -H(q_x)- \mathbb{E}_{z \sim q_x} \log p_x(z)" class="latex" src="https://s0.wp.com/latex.php?latex=0+%5Cleq+-H%28q_x%29-+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+p_x%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0 \leq -H(q_x)- \mathbb{E}_{z \sim q_x} \log p_x(z)"/></p>



<p>By the definition of <img alt="p_x" class="latex" src="https://s0.wp.com/latex.php?latex=p_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p_x"/>, <img alt="p_x(z) = \Pr[ Z=z \;\wedge\; D(z)=x ] / \Pr[D(Z)=x]" class="latex" src="https://s0.wp.com/latex.php?latex=p_x%28z%29+%3D+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2F+%5CPr%5BD%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p_x(z) = \Pr[ Z=z \;\wedge\; D(z)=x ] / \Pr[D(Z)=x]"/>. Hence we can derive that</p>



<p><img alt="0 \leq -H(q_x) - \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + \log \Pr[ D(Z)=x]" class="latex" src="https://s0.wp.com/latex.php?latex=0+%5Cleq+-H%28q_x%29+-+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2B+%5Clog+%5CPr%5B+D%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0 \leq -H(q_x) - \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + \log \Pr[ D(Z)=x]"/><br/>(since <img alt="\Pr[ D(Z)=x]" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B+D%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Pr[ D(Z)=x]"/> depends only on <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, given that <img alt="Z=N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z=N(0,I)"/>.)</p>



<p>Rearranging, we see that</p>



<p><img alt="\log Pr[ D(Z)=x] \geq \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + H(q_x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+Pr%5B+D%28Z%29%3Dx%5D+%5Cgeq+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2B+H%28q_x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log Pr[ D(Z)=x] \geq \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + H(q_x)"/></p>



<p>or in other words, we have the following theorem:</p>



<p><strong>Theorem (ELBO):</strong> For every (possibly randomized) maps <img alt="E:\mathcal{X} \rightarrow \mathcal{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathcal%7BZ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathcal{X} \rightarrow \mathcal{Z}"/> and <img alt="D:\mathcal{Z} \rightarrow \mathcal{X}" class="latex" src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathcal%7BZ%7D+%5Crightarrow+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D:\mathcal{Z} \rightarrow \mathcal{X}"/>, distribution <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z"/> over <img alt="\mathcal{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BZ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{Z}"/> and <img alt="x\in \mathcal{X}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\in \mathcal{X}"/>,</p>



<p><img alt="\log \Pr[ D(Z)=x] \geq \Pr_{z \sim E(x), z'  \sim Z}[ D(z) = x \wedge z=z' ] + H(E(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+%5CPr%5B+D%28Z%29%3Dx%5D+%5Cgeq+%5CPr_%7Bz+%5Csim+E%28x%29%2C+z%27++%5Csim+Z%7D%5B+D%28z%29+%3D+x+%5Cwedge+z%3Dz%27+%5D+%2B+H%28E%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log \Pr[ D(Z)=x] \geq \Pr_{z \sim E(x), z'  \sim Z}[ D(z) = x \wedge z=z' ] + H(E(x))"/></p>



<p>The left-hand side of this inequality is simply the log-likelihood of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. The right-hand side (which, as the inequality shows, is always smaller or equal to it) is known as the <em>evidence lower bound</em> or ELBO. We can think of VAEs as trying to maximize the ELBO.</p>



<p>The reason that the two views are roughly equivalent is the follows:</p>



<ul><li>The first term of the ELBO, known as the <em>reconstruction term</em>, is <img alt="\mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ]"/> if we assume some normal noise, then the probabiility taht <img alt="D(z)=x" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)=x"/> will be proportional to <img alt="\exp(-|x-D(z)|^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%7Cx-D%28z%29%7C%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\exp(-|x-D(z)|^2)"/> since for <img alt="q_x" class="latex" src="https://s0.wp.com/latex.php?latex=q_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q_x"/>, <img alt="z=E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=z%3DE%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z=E(x)"/> we get that <img alt="\log Pr[ Z=z \;\wedge\; D(z)=x ] \approx -| x- D(E(x))|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+Pr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%5Capprox+-%7C+x-+D%28E%28x%29%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log Pr[ Z=z \;\wedge\; D(z)=x ] \approx -| x- D(E(x))|^2"/> and hence maximizing this term corresponds to minimizing the square distance.</li><li>The second term of the ELBO, known as the <em>divergence term</em>, is <img alt="H(q_x)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28q_x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(q_x)"/> which is roughly equal to <img alt="r -\Delta_{KL}(q_x||N(0,I))" class="latex" src="https://s0.wp.com/latex.php?latex=r+-%5CDelta_%7BKL%7D%28q_x%7C%7CN%280%2CI%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r -\Delta_{KL}(q_x||N(0,I))"/>, where <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> is the dimension of the latent space. Hence maximizing this term corresponds to minimizing the KL divergence between <img alt="q_x=E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=q_x%3DE%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q_x=E(x)"/> and the standard normal distribution.</li></ul>



<p>How well does VAE work? First of all, we can actually generate images using them. We also find that similar inputs will have similar encodings, which is good. However, sometimes VAEs can still “cheat” (as in auto encoders). There is a risk that the learned model will split <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z"/> to two parts of the form <img alt="(N(0,I), JPEG(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%28N%280%2CI%29%2C+JPEG%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(N(0,I), JPEG(x))"/>. The first part of the data is there to minimize divergence, while the second part is there for reconstruction. Such a model is similarly uninformative.</p>



<p>However, VAEs have found practical success. For example, <a href="https://arxiv.org/pdf/1610.00291.pdf">Hou et. al 2016</a> used VAE to create an encoding where two dimensions seem to correspond to “sunglasses” and “blondness”, as illustrated below. We do note that “sunglasses” and “blondness” are somewhere between “semantic” and “syntactic” attributes. They do correspond to relatively local changes in “pixel space”.</p>



<figure class="wp-block-image"><img alt="VAE Example 1" src="https://i.imgur.com/O48nnWB.jpg"/></figure>



<p>The picture can be blurry because of the noise we injected to make <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> random. However, recent models have used new techniques (e.g. <a href="https://arxiv.org/abs/1906.00446">vector quantized VAE</a> and <a href="https://arxiv.org/abs/2007.03898">hierarchical VAE</a>) to resolve the blurriness and significantly improve on state of art.</p>



<h2>Flow Models</h2>



<p>In a flow model, we flip the order of <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> and <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> and set <img alt="E=D^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=E%3DD%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E=D^{-1}"/> (so <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> must be invertible). The input <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> to <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> will come from the standard normal distribution <img alt="N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,I)"/>. The idea is that we obtain <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> by a composition of simple invertible functions. We use the fact that if we can compute the density function of a distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> over <img alt="\mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{R}^d"/> and <img alt="f:\mathbb{R}^d \rightarrow \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f:\mathbb{R}^d \rightarrow \mathbb{R}^d"/> is invertible and differentiable, then we can compute the density function of <img alt="f\circ p" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f\circ p"/> (i.e., the distribution obtained by sampling <img alt="w \sim p" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w \sim p"/> and outputting <img alt="f(w)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(w)"/>). To see why this is the case, consider the setting when <img alt="d=2" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d=2"/> and a small <img alt="\delta \times \delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Ctimes+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta \times \delta"/> rectangle <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/>. If <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> is small enough, <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> will be roughly linear and hence will map <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> into a parallelogram <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/>. Shifting the <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> coordinate by <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> corresponds to shifting the output of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> by the vector <img alt="\delta (\tfrac{d f_x}{dx}, \tfrac{d f_y}{dx})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%28%5Ctfrac%7Bd+f_x%7D%7Bdx%7D%2C+%5Ctfrac%7Bd+f_y%7D%7Bdx%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta (\tfrac{d f_x}{dx}, \tfrac{d f_y}{dx})"/> and shifting the <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/> coordinate by <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> corresponds to shifting the output of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> by the vector <img alt="\delta (\tfrac{d f_x}{dy}, \tfrac{d f_y}{dy})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%28%5Ctfrac%7Bd+f_x%7D%7Bdy%7D%2C+%5Ctfrac%7Bd+f_y%7D%7Bdy%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta (\tfrac{d f_x}{dy}, \tfrac{d f_y}{dy})"/>. For every <img alt="z \in B" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Cin+B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z \in B"/>, the density of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> under <img alt="f\circ p" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f\circ p"/> will be proportional to the density of <img alt="f^{-1}(z)" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7B-1%7D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f^{-1}(z)"/> with the proportionality fector being <img alt="vol(A)/vol(B)" class="latex" src="https://s0.wp.com/latex.php?latex=vol%28A%29%2Fvol%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="vol(A)/vol(B)"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/pet8tBU.png"/></figure>



<p>Overall we the density of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> under <img alt="f \circ p" class="latex" src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f \circ p"/> will equal <img alt="p(f^{-1}(z))" class="latex" src="https://s0.wp.com/latex.php?latex=p%28f%5E%7B-1%7D%28z%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(f^{-1}(z))"/> times the inverse determinant of the <em>Jacobian</em> of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> at the point <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/></p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/ItOPqSX.png"/></figure>



<p>There are different ways to compose together simple reversible functions to compute a complex one. Indeed, this issue also arises in cryptography and quantum computing (e.g., the <a href="https://en.wikipedia.org/wiki/Feistel_cipher">Fiestel cipher</a>). Using similar ideas, it is not hard to show that any probability distribution can be approximated by a (sufficiently big) combination of simple reversible functions.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/AlYrWJx.png"/></figure>



<p>In practice, we have some recent succcessful flow models. A few examples of these models are in the lecture slides.</p>



<h1>Giving up on the dream</h1>



<p>In section 2, we had a dream of doing both representation and generation at once. So far, we have not been able to find success with these models. What if we do each goal separately?</p>



<p>The tasks of representation becomes self-supervised learning with approaches such SIMCLR. The task of generation can be solved by GANs. Both areas have had recent success.</p>



<figure class="wp-block-image"><img alt="Model after we separate E and D" src="https://i.imgur.com/D0CpobJ.jpg"/></figure>



<p>Open-AI <a href="https://openai.com/blog/clip">CLIP</a> and <a href="https://openai.com/blog/dall-e/">DALL-E</a> is a pair of models that perform each part of these tasks well, and suggest an approach to merge them.<br/>CLIP does representation for both texts and images where the two encoders are aligned, i.e. <img alt="\langle E(\text{'cat'}), E(\text{img of cat)}\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+E%28%5Ctext%7B%27cat%27%7D%29%2C+E%28%5Ctext%7Bimg+of+cat%29%7D%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle E(\text{'cat'}), E(\text{img of cat)}\rangle"/> is large. DALL-E, given some text, generates an image corresponding to the text. Below are images generated by DALL-E when asked for an armchair in the shape of an avocado.</p>



<figure class="wp-block-image"><img alt="DALL-E Example" src="https://i.imgur.com/ZcsHXKE.png"/></figure>



<h2>Contrastive learning</h2>



<p>The general approach used in CLIP is called contrastive learning.</p>



<p>Suppose we have some representation function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> and inputs <img alt="u_i,v_i" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u_i,v_i"/> which represent similar objects. Let <img alt="M_{i,j}=f(u_i\cdot v_j)" class="latex" src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Cj%7D%3Df%28u_i%5Ccdot+v_j%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M_{i,j}=f(u_i\cdot v_j)"/>, then we want <img alt="M_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M_{i,j}"/> to be large when <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/>, but small when <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/>. So, let the loss function be <img alt="L(M)=\sum M_{i,i} / \sum_{i\neq j} M_{i,j}." class="latex" src="https://s0.wp.com/latex.php?latex=L%28M%29%3D%5Csum+M_%7Bi%2Ci%7D+%2F+%5Csum_%7Bi%5Cneq+j%7D+M_%7Bi%2Cj%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(M)=\sum M_{i,i} / \sum_{i\neq j} M_{i,j}."/> How do we create similar <img alt="u_i,v_i" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u_i,v_i"/>? In SIMCLR, <img alt="u_i,v_i" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u_i,v_i"/> are augmentations of the same image <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/>. In CLIP, <img alt="(u_i,v_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%28u_i%2Cv_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(u_i,v_i)"/> is an image and a text that describes it.</p>



<p>CLIPs representation space does seem to have nice properties such as correspondence between semantic attributes and linear directions, which enables doing some “semantic linear algebra” on representations: (see this based on <a href="https://github.com/haltakov/natural-language-image-search">Vladimir Hatlakov’s code</a> – in the snippet below <code>tenc</code> maps text to its encoding/representation and <code>get_img</code> finds nearest image to representation in a the unsplash dataset):</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/02/image-6.png"><img alt="" class="wp-image-8017" src="https://windowsontheory.files.wordpress.com/2021/02/image-6.png?w=854"/></a></figure>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/02/image-7.png"><img alt="" class="wp-image-8019" src="https://windowsontheory.files.wordpress.com/2021/02/image-7.png?w=444"/></a></figure>



<h2>GANs</h2>



<p>The theory of GANs is currently not well-developed. As an objective, we want images that “look real” (which is not well defined), and we have no posterior distribution. If we just define the distribution based on real images, our GAN might memorize the photos to beat us.</p>



<p>However, we know that Neural Networks are good at discriminating real vs. fake images. So, we add in a discriminator <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> and define the loss function <img alt="L(D) = \max_{f:\mathbb R^d\to \mathbb R} |\mathbb{E}_{\hat x\sim D(z)}f(\hat x)-\mathbb{E}_{x\sim p}f(x)|." class="latex" src="https://s0.wp.com/latex.php?latex=L%28D%29+%3D+%5Cmax_%7Bf%3A%5Cmathbb+R%5Ed%5Cto+%5Cmathbb+R%7D+%7C%5Cmathbb%7BE%7D_%7B%5Chat+x%5Csim+D%28z%29%7Df%28%5Chat+x%29-%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7Df%28x%29%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(D) = \max_{f:\mathbb R^d\to \mathbb R} |\mathbb{E}_{\hat x\sim D(z)}f(\hat x)-\mathbb{E}_{x\sim p}f(x)|."/></p>



<p>The generator model and discriminator model form a 2-player game, which are often harder to train and very delicate. We typically train by changing a player’s action to the best response. However, we need to be careful if the two players have very different skill levels. They may be stuck in a setting where no change of strategies will make much difference, since the stronger player always dominates the weaker one. In particular in GANs we need to ensure that the generator is not cheating by using a degenerate distribution that still succeeds with respect to the discriminator.</p>



<p>If a 2-player model makes training more difficult, why do we use it? If we fix the discriminator, then the generator can find a picture that the discriminator thinks is real and only output that one, obtaining low loss. As a result, the discriminator needs to update along with the generator. This example also highlights that the discriminator’s job is often harder. To fix this, we have to somehow require the generator to give us good entropy.</p>



<p>Finally, how good are GANs in practice? Recently, we have had GANs that make great images as well as audios. For example, modern deepfake techniques often use GANs in their architecture. However, it is still unclear how rich the images are.</p></div>
    </content>
    <updated>2021-02-24T23:21:58Z</updated>
    <published>2021-02-24T23:21:58Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-03-07T01:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18187</id>
    <link href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/" rel="alternate" type="text/html"/>
    <title>A Quiz of Quotes</title>
    <summary>Everything that can be invented has been invented—Charles Duell, Commissioner, U.S. Office of Patents, 1899 MathQuotes src George Cantor has been featured here and here and here before on GLL. Of course, he invented modern set theory and changed math forever. His birthday is soon, so we thought we would talk about him now—he was […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Everything that can be invented has been invented—Charles Duell, Commissioner, U.S. Office of Patents, 1899</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/cantormathquote/" rel="attachment wp-att-18201"><img alt="" class="alignright wp-image-18201" height="150" src="https://rjlipton.files.wordpress.com/2021/02/cantormathquote.jpg?w=150&amp;h=150" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">MathQuotes <a href="https://www.facebook.com/mathsqoutes/posts/the-mathematician-does-not-study-pure-mathematics-because-it-is-useful-he-studie/144012037239958/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
George Cantor has been featured <a href="https://rjlipton.wordpress.com/2014/07/31/the-cantor-bernstein-schroder-theorem/">here</a> and <a href="https://rjlipton.wordpress.com/2009/04/18/cantors-non-diagonal-proof/">here</a> and <a href="https://rjlipton.wordpress.com/2012/09/04/thinking-out-of-the-notation-box/">here</a> before on GLL. Of course, he invented modern set theory and changed math forever. His birthday is soon, so we thought we would talk about him now—he was born on March 3rd in 1845.</p>
<p>
Today we thought it might be fun to have a quiz on math quotes.<br/>
<span id="more-18187"/></p>
<p>
Wait. Cantor did not invent quotation marks, nor is he known for many quotes. He does of course have many famous results, and they will live forever. But his results were subject to immediate horrible criticism and therefore memorable quotes. </p>
<p>Leopold Kronecker was a particular source of barbs.  For example: “What good is your beautiful proof on the transcendence of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? Why investigate such problems, given that irrational numbers do not even exist?” </p>
<p>
As a complexity theorist I must say that Kronecker has a point when he also said: </p>
<blockquote><p><b> </b> <em> “Definitions must contain the means of reaching a decision in a finite number of steps, and existence proofs must be conducted so that the quantity in question can be calculated with any degree of accuracy.” </em>
</p></blockquote>
<p>David Hilbert defended Cantor and said: “No one shall expel us from the paradise that Cantor has created.”</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/cantor/" rel="attachment wp-att-18190"><img alt="" class="aligncenter wp-image-18190" height="205" src="https://rjlipton.files.wordpress.com/2021/02/cantor.png?w=600&amp;h=205" width="600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">BBVA Open Mind <a href="https://www.bbvaopenmind.com/en/science/mathematics/georg-cantor-the-man-who-discovered-different-infinities/">src</a>
</font></td>
</tr>
</tbody></table>
<p/><h2> Quotes Quiz </h2><p/>
<p/><p>
On to the quiz. Each quote is followed by two possible authors in alphabetical order. You should pick the one you think is correct. The players are: </p>
<blockquote><p><b> </b> <em> 1. Douglas Adams  2. Bernard Baruch  3. Eric Temple Bell  4. Raoul Bott<br/>
5. Paul Erdős  6. Richard Hamming  7. Godfrey Hardy  8. David Hilbert<br/>
9. Admiral Grace Hooper  10. Alan Kay  11. Donald Knuth  12. John von Neumann<br/>
13. Alan Perlis  14. Henri Poincaré  15. Srinivasa Ramanujan  16. Marcus du Sautoy<br/>
17. Raymond Smullyan  18. Alan Turing  19. Moshe Vardi  20. Andrew Wiles<br/>
</em>
</p></blockquote>
<p>
</p><ol>
<p/><li>
Those who can imagine anything, can create the impossible.<br/>
—Kay <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Turing<p/>
<p/></li><li>
I really didn’t foresee the Internet. But then, neither did the computer industry. Not that that tells us very much of course–the computer industry didn’t even foresee that the century was going to end.<br/>
— Adams <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Knuth<p/>
<p/></li><li>
One man’s constant is another man’s variable.<br/>
—Perlis <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> du Sautoy<p/>
<p/></li><li>
The most damaging phrase in the language is: “It’s always been done that way.”<br/>
—-Hopper <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Perlis<p/>
<p/></li><li>
The best way to predict the future is to invent it.<br/>
—Kay <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Turing<p/>
<p/></li><li>
The purpose of computing is insight, not numbers.<br/>
Adams <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Hamming<p/>
<p/></li><li>
Beware of bugs in the above code; I have only proved it correct, not tried it.<br/>
—Knuth <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Vardi<p/>
<p/></li><li>
No, it is a very interesting number, it is the smallest number expressible as a sum of two cubes in two different ways.<br/>
—Bell <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Ramanujan <p/>
<p/></li><li>
Beauty is the first test: there is no permanent place in the world for ugly mathematics. <br/>
—Erdős <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Hardy<p/>
<p/></li><li>
Mathematics is the art of giving the same name to different things. <br/>
—Hooper <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Poincaré <p/>
<p/></li><li>
There’s no sense in being precise when you don’t even know what you’re talking about.<br/>
—Bott <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> von Neumann<p/>
<p/></li><li>
I hope we’ll be able to solve these problems before we leave. <br/>
—Erdős <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Perlis<p/>
<p/></li><li>
Some people are always critical of vague statements. I tend rather to be critical of precise statements; they are the only ones which can correctly be labeled ‘wrong’. <br/>
—Knuth <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Smullyan<p/>
<p/></li><li>
Everything that humans can do a machine can do. <br/>
—Perlis <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Vardi<p/>
<p/></li><li>
“Obvious” is the most dangerous word in mathematics.<br/>
— Bell <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Hooper<p/>
<p/></li><li>
Just because we can’t find a solution, it doesn’t mean there isn’t one.<br/>
— Adams <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Wiles<p/>
<p/></li><li>
Mathematics is a place where you can do things which you can’t do in the real world.<br/>
— du Sautoy <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Turing<p/>
<p/></li><li>
Millions saw the apple fall, but Newton asked why.<br/>
— Baruch <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Hopper<p/>
<p/></li><li>
The definition of a good mathematical problem is the mathematics it generates rather than the problem itself.<br/>
— Hilbert <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Wiles<p/>
<p/></li><li>
There are two ways to do great mathematics. The first is to be smarter than everybody else. The second way is to be stupider than everybody else – but persistent.<br/>
— Bott <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Knuth<p/>
</li></ol>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
“I always have a quotation for everything—it saves original thinking.”<br/>
—Dorothy Sayers</p>
<p>
Here are the answers:</p>
<p><a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/ans/" rel="attachment wp-att-18194"><img alt="" class="alignright size-full wp-image-18194" height="702" src="https://rjlipton.files.wordpress.com/2021/02/ans.png?w=600&amp;h=702" width="600"/></a></p></font></font></div>
    </content>
    <updated>2021-02-24T17:44:38Z</updated>
    <published>2021-02-24T17:44:38Z</published>
    <category term="History"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Cantor"/>
    <category term="fun"/>
    <category term="quiz"/>
    <category term="quote"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-03-07T01:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/027</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/027" rel="alternate" type="text/html"/>
    <title>TR21-027 |  Almost Optimal Super-Constant-Pass Streaming Lower Bounds for Reachability | 

	Lijie Chen, 

	Gillat Kol, 

	Dmitry Paramonov, 

	Raghuvansh Saxena, 

	Zhao Song, 

	Huacheng Yu</title>
    <summary>We give an almost quadratic $n^{2-o(1)}$ lower bound on the space consumption of any $o(\sqrt{\log n})$-pass streaming algorithm solving the (directed) $s$-$t$ reachability problem. This means that any such algorithm must essentially store the entire graph. As corollaries, we obtain almost quadratic space lower bounds for additional fundamental problems, including maximum matching, shortest path, matrix rank, and linear programming.

Our main technical contribution is the definition and construction of set hiding graphs, that may be of independent interest: we give a general way of encoding a set $S \subseteq [k]$ as a directed graph with $n = k^{ 1 + o( 1 ) }$ vertices, such that deciding whether $i \in S$ boils down to deciding if $t_i$ is reachable from $s_i$, for a specific pair of vertices $(s_i,t_i)$ in the graph. Furthermore, we prove that our graph ``hides'' $S$, in the sense that no low-space streaming algorithm with a small number of passes can learn (almost) anything about $S$.</summary>
    <updated>2021-02-24T02:18:04Z</updated>
    <published>2021-02-24T02:18:04Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-07T01:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5350</id>
    <link href="https://www.scottaaronson.com/blog/?p=5350" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5350#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5350" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Stop emailing my utexas address</title>
    <summary xml:lang="en-US">A month ago, UT Austin changed its email policies—banning auto-forwarding from university accounts to Gmail accounts, apparently as a way to force the faculty and other employees to separate their work email from their personal email, and thereby comply with various government regulations. Ever since that change, the email part of my life has been […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>A month ago, UT Austin changed its email policies—banning auto-forwarding from university accounts to Gmail accounts, apparently as a way to force the faculty and other employees to separate their work email from their personal email, and thereby comply with various government regulations.  Ever since that change, the email part of my life has been a <em>total, unmitigated disaster</em>.  I’ve missed (or been late to see) dozens of important work emails, with the only silver lining being that that’s arguably UT’s problem more than it is mine!</p>



<p>And yes, I’ve already gone to technical support; the only answer I’ve gotten is that (in so many words) there <em>is</em> no answer.  Other UT faculty are somehow able to deal with this because they are them; I am unable to deal with it because I am me.  As a mere PhD in computer science, I’m utterly unqualified to set up a technical fix for this sort of problem.</p>



<p>So the bottom line is: <strong>from now on, if you want me to see an email, send it to scott@scottaaronson.com</strong>.  Really.  If you try sending it to aaronson@cs.utexas.edu, it will land in a separate inbox that I can access only with great inconvenience.  And if, God forbid, you try sending it to aaronson@utexas.edu, the email will bounce and I’ll never see it at all.  Indeed, a central purpose of this post is just to have a place to point the people who contact me every day, shocked that their emails to me bounced.</p>



<p>This whole episode has given me <em>immense</em> sympathy for Hillary Clinton, and for the factors that led her to set up clintonemail.com from her house.  It’s not merely that her private email server was a laughably trivial reason to end the United States’ 240-year run of democratic government.  Rather it’s that, even on the narrow question of emails, I now feel certain that <em>Hillary was 100% right</em>.  Bureaucracy that impedes communication is a cancer on human civilization.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> Thanks so much to commenter Avraham and to my colleague Etienne Vouga, who quickly gave me the crucial information that tech support would not, and thereby let me solve this problem.  I can once again easily read emails sent to aaronson@cs.utexas.edu … well, at least for now!  I’m now checking about aaronson@utexas.edu.  Again, though, <strong>scott@scottaaronson.com to be safe</strong>.</p></div>
    </content>
    <updated>2021-02-23T21:00:37Z</updated>
    <published>2021-02-23T21:00:37Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-03-05T06:08:50Z</updated>
    </source>
  </entry>
</feed>
