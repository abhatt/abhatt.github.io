<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-11-30T14:21:33Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/29/postdoc-at-university-of-waterloo-apply-by-december-31-2019/</id>
    <link href="https://cstheory-jobs.org/2019/11/29/postdoc-at-university-of-waterloo-apply-by-december-31-2019/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Waterloo (apply by December 31, 2019)</title>
    <summary>The Algorithms &amp; Complexity group at the University of Waterloo is offering one postdoctoral position starting in the Fall of 2020. We seek candidates from all areas of TCS. Interested applicants should have their CV, research statement, and three recommendation letters emailed to theory.waterloo@gmail.com. Applications are due December 31st. Questions should be sent to the […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Algorithms &amp; Complexity group at the University of Waterloo is offering one postdoctoral position starting in the Fall of 2020. We seek candidates from all areas of TCS.<br/>
Interested applicants should have their CV, research statement, and three recommendation letters emailed to theory.waterloo@gmail.com. Applications are due December 31st.<br/>
Questions should be sent to the email above.</p>
<p>Website: <a href="https://algcomp.uwaterloo.ca/#MoreInfo">https://algcomp.uwaterloo.ca/#MoreInfo</a><br/>
Email: theory.waterloo@gmail.com</p></div>
    </content>
    <updated>2019-11-29T20:24:00Z</updated>
    <published>2019-11-29T20:24:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-30T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16420</id>
    <link href="https://rjlipton.wordpress.com/2019/11/29/predicating-predictivity/" rel="alternate" type="text/html"/>
    <title>Predicating Predictivity</title>
    <summary>Plus predicaments of error modeling Cropped from Bacon Sandwich source Sir David Spiegelhalter is a British statistician. He is a strong voice for the public understanding of statistics. His work extends to all walks of life, including risk, coincidences, murder, and sex. Today we talk about extending one of his inventions. His invention has to […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="color: #0044cc;"><br/>
<em>Plus predicaments of error modeling</em><br/>
</span></p>
<table class="image alignright">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2019/11/29/predicating-predictivity/spiegelhalterbacon/" rel="attachment wp-att-16422"><img alt="" class="alignright wp-image-16422" height="168" src="https://rjlipton.files.wordpress.com/2019/11/spiegelhalterbacon.jpg?w=200&amp;h=168" width="200"/></a></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Bacon Sandwich <a href="https://www.youtube.com/watch?v=4szyEbU94ig">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Sir David Spiegelhalter is a British statistician. He is a strong voice for the public understanding of statistics. His work extends to all walks of life, including <a href="https://www.regulation.org.uk/library/2017-Spiegelhalter-Risk_and_Uncertainty_Communication.pdf">risk</a>, <a href="https://understandinguncertainty.org/coincidences">coincidences</a>, <a href="https://www.spectator.co.uk/2019/04/i-could-have-stopped-harold-shipmans-killing-spree-and-saved-175-lives/">murder</a>, and <a href="https://www.ft.com/content/f8793aaa-dfa1-11e4-a06a-00144feab7de">sex</a>.</p>
<p>
Today we talk about extending one of his inventions.</p>
<p>
His invention has to do with grading the performance of people and models that make predictions. A <b>scoring rule</b> grades how often predictions are right. But it may not tell how difficult the situations are. It is easy to look good with predictions when they start with a high chance of success. A weather forecaster predicting sunny-versus-rainy will be right more often in Las Vegas than in Boston. Quoting this FiveThirtyEight <a href="https://fivethirtyeight.com/features/which-city-has-the-most-unpredictable-weather/">item</a>:</p>
<blockquote><p><b> </b> <em> If you want to have an easy life as a weather forecaster, you should get a job in Las Vegas, Phoenix or Los Angeles. Predict that it won’t rain in one of those cities, and you’ll be right about 90 percent of the time. </em>
</p></blockquote>
<p/><p>
In a 1986 <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780050506">paper</a>, for a particular scoring rule <a href="https://www.semanticscholar.org/paper/VERIFICATION-OF-FORECASTS-EXPRESSED-IN-TERMS-OF-Brier/feee6551179612b9691f021b583d8a99b81b9b86">defined</a> by Glenn Brier in 1950, Spiegelhalter worked out how to equalize the forecaster grading. He applied his <b>Z-test</b> not to weather as Brier was concerned with but to medical prognoses and clinical trials. </p>
<p>
What I am doing with a small group of graduate students in Buffalo is trying to turn Spiegelhalter’s kind of Z-test around once more. If a forecaster fares poorly, we will try to flag not the model but the behavior of the subjects being modeled. In weather we would want to tell when Mother Nature, not the models, has gone off the rails. Well, we are actually looking for ways to tell when a human being has left the bounds of human predictability for reasons that are inhuman—such as cheating with a computer at chess. And maybe it can shed more light on whether our computers can possibly “cheat” with quantum mechanics.</p>
<p>
</p><p/><h2> Prediction Scores </h2><p/>
<p/><p>
Let’s consider situations <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> in which the number <img alt="{\ell = \ell_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3D+%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell = \ell_t}"/> is usually more than <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>, that is, usually more than “rain” or “no rain.” The forecaster lays down projections <img alt="{\vec{q} = \vec{q}_t = (q_1,\dots,q_\ell)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bq%7D_t+%3D+%28q_1%2C%5Cdots%2Cq_%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{q} = \vec{q}_t = (q_1,\dots,q_\ell)}"/> for the chance of each outcome. If outcome <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> happens, then the <em>Brier score</em> for that forecast is <a name="Brier"/></p><a name="Brier">
<p align="center"><img alt="\displaystyle  B^{\vec{q}}(r) = (1 - q_r)^2 + \sum_{j \neq r} q_j^2. \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++B%5E%7B%5Cvec%7Bq%7D%7D%28r%29+%3D+%281+-+q_r%29%5E2+%2B+%5Csum_%7Bj+%5Cneq+r%7D+q_j%5E2.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  B^{\vec{q}}(r) = (1 - q_r)^2 + \sum_{j \neq r} q_j^2. \ \ \ \ \ (1)"/></p>
</a><p><a name="Brier"/> If the forecaster was certain that <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> would happen and so put <img alt="{q_r = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_r+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_r = 1}"/>, all other <img alt="{q_j = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_j+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_j = 0}"/>, then the score would be zero. Thus lower is better for the Brier score. </p>
<p>
If you put probability <img alt="{q_r &lt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_r+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_r &lt; 1}"/> on the outcome that happened, then you get penalized both for the difference and for the remaining probability which you put on outcomes that did not happen. It is possible to <em>decompose</em> the score in another way that changes the emphasis: </p>
<p align="center"><img alt="\displaystyle  B^{\vec{q}}(r) = 1 + Q - 2q_r \qquad\text{where}\qquad Q = \sum_{j=1}^\ell q_j^2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++B%5E%7B%5Cvec%7Bq%7D%7D%28r%29+%3D+1+%2B+Q+-+2q_r+%5Cqquad%5Ctext%7Bwhere%7D%5Cqquad+Q+%3D+%5Csum_%7Bj%3D1%7D%5E%5Cell+q_j%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  B^{\vec{q}}(r) = 1 + Q - 2q_r \qquad\text{where}\qquad Q = \sum_{j=1}^\ell q_j^2. "/></p>
<p>
Then <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> is a fixed measure of how you spread your forecasts around, while all the variability in your score comes from how much stock you placed in the outcome that happened. The worst case is having put <img alt="{q_r = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_r+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_r = 0}"/>, whereupon your Brier penalty is <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. </p>
<p>
We would like our forecasts always to be perfect, but reality gives us situations that are inherently nondeterministic—with unknown “true probabilities” <img alt="{\vec{p}_t = (p_1,\dots,p_\ell)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bp%7D_t+%3D+%28p_1%2C%5Cdots%2Cp_%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{p}_t = (p_1,\dots,p_\ell)}"/>. The vital point is that the forecaster should not try to hit <img alt="{r = r_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+r_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = r_t}"/> on the nose at every time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> but rather to match the true probabilities. Once we postulate <img alt="{\vec{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{p}}"/>, the <em>expected Brier score</em> is </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  \mathsf{E}_{\vec{p}}[B^{\vec{q}}] &amp;=&amp; \sum_{i=1}^\ell p_i B^{\vec{q}}(i)\\ &amp;=&amp; \sum_{i=1}^\ell p_i (1 - 2q_i + Q)\\ &amp;=&amp; 1 + Q - 2\sum_{i=1}^\ell p_i q_i. \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%26%3D%26+%5Csum_%7Bi%3D1%7D%5E%5Cell+p_i+B%5E%7B%5Cvec%7Bq%7D%7D%28i%29%5C%5C+%26%3D%26+%5Csum_%7Bi%3D1%7D%5E%5Cell+p_i+%281+-+2q_i+%2B+Q%29%5C%5C+%26%3D%26+1+%2B+Q+-+2%5Csum_%7Bi%3D1%7D%5E%5Cell+p_i+q_i.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}  \mathsf{E}_{\vec{p}}[B^{\vec{q}}] &amp;=&amp; \sum_{i=1}^\ell p_i B^{\vec{q}}(i)\\ &amp;=&amp; \sum_{i=1}^\ell p_i (1 - 2q_i + Q)\\ &amp;=&amp; 1 + Q - 2\sum_{i=1}^\ell p_i q_i. \end{array} "/></p>
<p>This is uniquely minimized by setting <img alt="{q_i = p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i = p_i}"/> for each <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>, which defines <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> as a <b>strictly proper</b> scoring rule. Without the second term <img alt="{\sum_{j \neq r} q_j^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bj+%5Cneq+r%7D+q_j%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{j \neq r} q_j^2}"/> in (<a href="https://rjlipton.wordpress.com/feed/#Brier">1</a>) the rule would not be proper for <img alt="{\ell &gt; 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3E+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell &gt; 2}"/>. When <img alt="{\vec{q} = \vec{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{q} = \vec{p}}"/>, <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> becomes equal to <img alt="{P = \sum_{j=1}^\ell p_j^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP+%3D+%5Csum_%7Bj%3D1%7D%5E%5Cell+p_j%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P = \sum_{j=1}^\ell p_j^2}"/>. Thus <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> represents an unavoidable prediction penalty from the intrinsic variance. If all <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> are equal, <img alt="{p_i = \frac{1}{\ell}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+%5Cfrac%7B1%7D%7B%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i = \frac{1}{\ell}}"/>, then the expected score cannot be less than <img alt="{1 - \frac{1}{\ell}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7B1%7D%7B%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 - \frac{1}{\ell}}"/>. </p>
<p>
A second example, the log-likelihood prediction scoring rule, is in the original longer <a href="https://cse.buffalo.edu/~regan/GLL/wspiegelhalterLong.pdf">draft</a> of this post.</p>
<p/><h2> Spiegelhalter’s Z </h2><p/>
<p/><p>
Spiegelhalter’s <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-score neatly drops out the unavoidable penalty term by taking the difference of the score with the expectation. Schematically it is defined as </p>
<p align="center"><img alt="\displaystyle  \mathsf{Z}[B] = \frac{B - \mathsf{E}[B]}{\sqrt{\mathsf{Var}[B]}}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BZ%7D%5BB%5D+%3D+%5Cfrac%7BB+-+%5Cmathsf%7BE%7D%5BB%5D%7D%7B%5Csqrt%7B%5Cmathsf%7BVar%7D%5BB%5D%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{Z}[B] = \frac{B - \mathsf{E}[B]}{\sqrt{\mathsf{Var}[B]}}, "/></p>
<p>where <img alt="{\mathsf{Var}[B]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BVar%7D%5BB%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Var}[B]}"/> means the projected variance <img alt="{\mathsf{E}_{\vec{p}}[B^2] - (\mathsf{E}_{\vec{p}}[B])^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5E2%5D+-+%28%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5D%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{E}_{\vec{p}}[B^2] - (\mathsf{E}_{\vec{p}}[B])^2}"/>. However, here is where it is important to notate the whole series of forecasting situations <img alt="{t = 1,\dots,T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+1%2C%5Cdots%2CT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t = 1,\dots,T}"/> with outcomes <img alt="{r_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_t}"/> for each <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. The actual statistic is <a name="ZB"/></p><a name="ZB">
<p align="center"><img alt="\displaystyle  \mathsf{Z}_{\vec{p}}[B^{\vec{q}}] = \frac{\sum_{t=1}^T B^{\vec{q}_t}(r_t) - \mathsf{E}_{\vec{p}_t}[B^{\vec{q}_t}]}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{p}_t}[B^{\vec{q}_t}]}}. \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%3D+%5Cfrac%7B%5Csum_%7Bt%3D1%7D%5ET+B%5E%7B%5Cvec%7Bq%7D_t%7D%28r_t%29+-+%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D_t%7D%5BB%5E%7B%5Cvec%7Bq%7D_t%7D%5D%7D%7B%5Csqrt%7B%5Csum_%7Bt%3D1%7D%5ET+%5Cmathsf%7BVar%7D_%7B%5Cvec%7Bp%7D_t%7D%5BB%5E%7B%5Cvec%7Bq%7D_t%7D%5D%7D%7D.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{Z}_{\vec{p}}[B^{\vec{q}}] = \frac{\sum_{t=1}^T B^{\vec{q}_t}(r_t) - \mathsf{E}_{\vec{p}_t}[B^{\vec{q}_t}]}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{p}_t}[B^{\vec{q}_t}]}}. \ \ \ \ \ (2)"/></p>
</a><p><a name="ZB"/> The denominator presumes that the forecast situations are independent so that the variances add. The numerator expands to be </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T \left(2\sum_{i=1}^{\ell_t} p_{i,t} q_{i,t}\right) - 2q_{r,t}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%282%5Csum_%7Bi%3D1%7D%5E%7B%5Cell_t%7D+p_%7Bi%2Ct%7D+q_%7Bi%2Ct%7D%5Cright%29+-+2q_%7Br%2Ct%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^T \left(2\sum_{i=1}^{\ell_t} p_{i,t} q_{i,t}\right) - 2q_{r,t}. "/></p>
<p>
The original application is a confidence test of the “null hypothesis” that the projections <img alt="{\vec{q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{q}}"/> are good. Thus we plug in <img alt="{p_{i,t} = q_{i,t}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bi%2Ct%7D+%3D+q_%7Bi%2Ct%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{i,t} = q_{i,t}}"/> for all <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> and <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> so that we test </p>
<p align="center"><img alt="\displaystyle  \mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 2\frac{\sum_{t=1}^T \left(\sum_{i=1}^{\ell_t} q_{i,t}^2 \right) - q_{r,t}}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{q}_t}[B^{\vec{q}_t}]}}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%3D+2%5Cfrac%7B%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%28%5Csum_%7Bi%3D1%7D%5E%7B%5Cell_t%7D+q_%7Bi%2Ct%7D%5E2+%5Cright%29+-+q_%7Br%2Ct%7D%7D%7B%5Csqrt%7B%5Csum_%7Bt%3D1%7D%5ET+%5Cmathsf%7BVar%7D_%7B%5Cvec%7Bq%7D_t%7D%5BB%5E%7B%5Cvec%7Bq%7D_t%7D%5D%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 2\frac{\sum_{t=1}^T \left(\sum_{i=1}^{\ell_t} q_{i,t}^2 \right) - q_{r,t}}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{q}_t}[B^{\vec{q}_t}]}}. "/></p>
<p>
To illustrate, suppose we do ten independent trials of an event with four outcomes whose true probabilities are <img alt="{(0.1,0.2,0.3,0.4)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%280.1%2C0.2%2C0.3%2C0.4%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(0.1,0.2,0.3,0.4)}"/>. The sum in parentheses is <img alt="{10(0.01 + 0.04 + 0.09 + 0.16) = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B10%280.01+%2B+0.04+%2B+0.09+%2B+0.16%29+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{10(0.01 + 0.04 + 0.09 + 0.16) = 3}"/>. If the outcomes conform exactly to these probabilities then <img alt="{q_{r,t}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_%7Br%2Ct%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_{r,t}}"/> equals <img alt="{0.1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.1}"/> once, <img alt="{0.2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.2}"/> twice, <img alt="{0.3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.3}"/> three times, and <img alt="{0.4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.4}"/> four times. This exactly cancels the <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/>, so <img alt="{\vec{q} = \vec{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{q} = \vec{p}}"/> makes <img alt="{\mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 0}"/>, as expected. Most trials will give a nonzero numerator, but in the long run, the numerator divided by <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> tends toward zero and the denominator scales to match it, thus keeping the <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/>-statistic normally distributed.</p>
<p>
A high <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/>, on the other hand—highly positive or highly negative—indicates that the forecasting is way off. That (<a href="https://rjlipton.wordpress.com/feed/#ZB">2</a>) is an aggregate statistic over independent trials justifies treating the <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/>-values as <a href="https://en.wikipedia.org/wiki/Standard_score">standard</a> <a href="https://en.wikipedia.org/wiki/Z-test">scores</a>. This applies also to <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/>-tests made similarly from other scoring rules besides the Brier score. The test thus becomes a verdict on the model. High <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/>-values on certain subsets of the data may reveal biases. </p>
<p>
Our idea is the opposite. Suppose we know that the forecasts are true, or suppose they have biases that are known and correctable over moderately large data sets. We may then be able to fit <img alt="{\mathsf{Z}[B]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%5BB%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}[B]}"/> as an unbiased estimator (of zero) over large training sets. Then it can become a judgment of whether the data has become unnatural. </p>
<p>
</p><p/><h2> Why This Z? </h2><p/>
<p/><p>
As I have detailed in <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">numerous</a> <a href="https://rjlipton.wordpress.com/2013/09/17/littlewoods-law/">posts</a> <a href="https://rjlipton.wordpress.com/2018/10/18/london-calling/">on</a> <a href="https://rjlipton.wordpress.com/2013/07/27/thirteen-sigma/">this</a> <a href="https://rjlipton.wordpress.com/2011/10/12/empirical-humility/">blog</a>, my system for detecting cheating with computers at chess already provides several statistical <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-scores. Why would I want another one?</p>
<p>
The motive involves the presence of multiple strong chess-playing programs, each with its own quirks and distribution of values for moves. They are used in two different ways:</p>
<ol>
<li>
As inputs telling the relative values <img alt="{v_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_i}"/> of moves <img alt="{m_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_i}"/>, which my model converts into its probability projections <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/>. <p/>
</li><li>
As output predicates telling how often the player chose the move recommended by a specific program and/or quantifying the magnitude of error for different played moves.
</li></ol>
<p>
Having multiple engines helps point 1. My intent to blend the <em>values</em> <img alt="{v_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_i}"/> from different engines has been blunted by issues I discussed <a href="https://rjlipton.wordpress.com/2018/09/07/sliding-scale-problems/">here</a>.  Thus I now have to train my model separately (and expensively) for each (new version of each) program. I can then blend the <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/>, but point 2 still remains at issue: My tests measure concordance with a specific program. Originally the program Rybka 3 was primary and Houdini 4B secondary. Now Stockfish 7 is primary and Komodo 10.0 secondary—until I update to their latest versions. The second engine is supposed to confirm a positive result from the first one.  This already means that my model is not trying to detect exactly which program was used.</p>
<p>
Nevertheless, my results often vary between testing engines. The engines <a href="https://rjlipton.wordpress.com/2014/12/28/the-new-chess-world-champion/">compete</a> against each other and may be crafted to disagree on certain kinds of moves. They agree with each other barely 75–80% in my tests. I would like to factor these differences out. </p>
<p>
The Spiegelhalter <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/>-test appeals because its reference is not to a particular chess program, but to the prediction quality of my model itself—which per point 1 can be informed by many programs in concert. It gives a way to <em>predicate predictivity</em>. A high value will attest that the sequence of played moves falls outside the range of predictability for human players of the same rated skill level. </p>
<p>
</p><p/><h2> The Method </h2><p/>
<p/><p>
To harness <img alt="{\mathsf{Z}[F]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%5BF%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}[F]}"/> for some scoring rule <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>, we need to quantify the nature of my model’s <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> projections. In fact, my model has a clear bias toward conservatism in judging the frequency of particular non-optimal moves. This is discussed in my August <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">post</a> on my model upgrade and shown graphically in an appended <a href="https://cse.buffalo.edu/~regan/chess/computer/ModelTradeoffs.png">note</a> on why the conservative setting of a “gradient” parameter is needed to preserve dynamical stability. The fitting offsets this in a way that creates an opposite bias elsewhere. I hope to correct both biases at the same stroke by a specific means of modeling how the <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> err with respect to the postulated true probabilities <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/>.</p>
<p>
We postulate an original source of error terms <img alt="{\epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon_i}"/> all <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">i.i.d.</a> as <img alt="{\mathcal{N}(0,\delta^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{N}(0,\delta^2)}"/>, where <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> governs the magnitude of Gaussian noise. This noise can be <em>transformed</em> and related in various ways, e.g.:</p>
<ol>
<li>
<img alt="{q_i = p_i \pm \epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i = p_i \pm \epsilon_i}"/>, <p/>
</li><li>
<img alt="{q_i = p_i(1 \pm \epsilon_i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i%281+%5Cpm+%5Cepsilon_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i = p_i(1 \pm \epsilon_i)}"/>, <p/>
</li><li>
<img alt="{\frac{1}{q_i} = \frac{1}{p_i} \pm \epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Bq_i%7D+%3D+%5Cfrac%7B1%7D%7Bp_i%7D+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{q_i} = \frac{1}{p_i} \pm \epsilon_i}"/>, <p/>
</li><li>
<img alt="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i}) \pm \epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29+%3D+%5Clog%28%5Cfrac%7B1%7D%7Bp_i%7D%29+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i}) \pm \epsilon_i}"/>, <p/>
</li><li>
<img alt="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i})(1 \pm \epsilon_i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29+%3D+%5Clog%28%5Cfrac%7B1%7D%7Bp_i%7D%29%281+%5Cpm+%5Cepsilon_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i})(1 \pm \epsilon_i)}"/>, <p/>
</li><li>
<img alt="{\ln(\frac{q_i}{1 - q_i}) = \ln(\frac{p_i}{1 - p_i}) \pm \epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cln%28%5Cfrac%7Bq_i%7D%7B1+-+q_i%7D%29+%3D+%5Cln%28%5Cfrac%7Bp_i%7D%7B1+-+p_i%7D%29+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ln(\frac{q_i}{1 - q_i}) = \ln(\frac{p_i}{1 - p_i}) \pm \epsilon_i}"/>.
</li></ol>
<p>
There are further forms to consider and it is not yet clear from data within my model which one most applies. We would be interested in examples where these representations have been employed and in observations about their natures. </p>
<p>
Given the error terms, we can write each <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> as a function of <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> and <img alt="{\epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon_i}"/>. One issue is having at most <img alt="{\ell-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell-1}"/> degrees of freedom among <img alt="{\epsilon_1,\dots,\epsilon_\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_1%2C%5Cdots%2C%5Cepsilon_%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon_1,\dots,\epsilon_\ell}"/>, owing to the constraint that the <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> as well as <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> sum to <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. We handle this by choosing some fixed <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> as the “pivot” and using the constraints to eliminate <img alt="{p_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_k}"/> and <img alt="{q_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_k}"/>, leaving the other error terms free. In all cases, the proposed method of defining what we notate as <img alt="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%2C%5Cvec%7B%5Cepsilon%7D%7D%5BF%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}"/> is:</p>
<ul>
<li>
Substitute the terms with <img alt="{q_i,\epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%2C%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i,\epsilon_i}"/> for each free <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> into <img alt="{\mathsf{Z}_{\vec{p}}[F^{\vec{q}}]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bp%7D%7D%5BF%5E%7B%5Cvec%7Bq%7D%7D%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}_{\vec{p}}[F^{\vec{q}}]}"/>. <p/>
</li><li>
Compute the expectation over <img alt="{\epsilon_i \sim \mathcal{N}(0,\delta^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon_i \sim \mathcal{N}(0,\delta^2)}"/> for the numerator and denominator of (<a href="https://rjlipton.wordpress.com/feed/#ZB">2</a>), separately. <p/>
</li><li>
Holding the other previously-fitted model parameters in place, fit <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> so that <img alt="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%2C%5Cvec%7B%5Cepsilon%7D%7D%5BF%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}"/> is zero over the training set (or sets, for each level of Elo rating <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/>, so <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> becomes a function of <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/>).
</li></ul>
<p>
If the resulting <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/>-scores parameterized by <img alt="{\delta_R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_R%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta_R}"/> make sense, the last step will be adjusting them to conform to normal distribution, via the resampling process mentioned recently <a href="https://rjlipton.wordpress.com/2019/08/20/our-trip-to-monte-carlo/">here</a> and earlier <a href="https://rjlipton.wordpress.com/2011/10/12/empirical-humility/">here</a>. We are not there yet. But observations from Spiegelhalter tests with <img alt="{\vec{q} = \vec{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{q} = \vec{p}}"/> (equivalently, with <img alt="{\delta_R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_R%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta_R}"/> fixed to zero) suggest that the resulting single, authoritative, “pure” predictivity test may rival the sharpness of my current tests involving specific chess programs.</p>
<p>
</p><p/><h2> Error Quirks and Queries </h2><p/>
<p/><p>
To see a key wrinkle, consider the first error form. It is symmetrical: <img alt="{p_i = q_i \pm \epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+q_i+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i = q_i \pm \epsilon_i}"/>. When we substitute <img alt="{q_i + \epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i+%2B+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i + \epsilon_i}"/> for <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> and take <img alt="{\mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[\cdots]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D_%7B%5Cepsilon_i+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5B%5Ccdots%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[\cdots]}"/>, the symmetry of <img alt="{\epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon_i}"/> around <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> makes it drop out of the numerator of (<a href="https://rjlipton.wordpress.com/feed/#ZB">2</a>), and out of everything in the denominator except one place where <img alt="{p_i^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i^2}"/> becomes <img alt="{(q_i^2 + 2\epsilon q_i + \epsilon_i^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28q_i%5E2+%2B+2%5Cepsilon+q_i+%2B+%5Cepsilon_i%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(q_i^2 + 2\epsilon q_i + \epsilon_i^2)}"/>. There is hence nothing for <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> to fit and we are basically left with the original Spiegelhalter <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z}"/>. </p>
<p>
In the second form, however, we get <img alt="{p_i = q_i \cdot \frac{1}{1 + \epsilon_i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+q_i+%5Ccdot+%5Cfrac%7B1%7D%7B1+%2B+%5Cepsilon_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i = q_i \cdot \frac{1}{1 + \epsilon_i}}"/>. If we presume <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> small enough to make the distribution of <img alt="{\mathcal{N}(0,\delta^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{N}(0,\delta^2)}"/> outside <img alt="{(-1,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28-1%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(-1,1)}"/> negligible, then we can use the series expansion to approximate </p>
<p align="center"><img alt="\displaystyle  p_i \approx q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_i+%5Capprox+q_i%281+-+%5Cepsilon_i+%2B+%5Cepsilon_i%5E2+-+%5Cepsilon_i%5E3+%2B+%5Cepsilon_i%5E4%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_i \approx q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4). "/></p>
<p>Under normal expectation, the odd-power terms drop out (so their signs don’t matter) and we get </p>
<p align="center"><img alt="\displaystyle  \mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4)] = q_i(1 + \delta^2 + 3\delta^4). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cepsilon_i+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5Bq_i%281+-+%5Cepsilon_i+%2B+%5Cepsilon_i%5E2+-+%5Cepsilon_i%5E3+%2B+%5Cepsilon_i%5E4%29%5D+%3D+q_i%281+%2B+%5Cdelta%5E2+%2B+3%5Cdelta%5E4%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4)] = q_i(1 + \delta^2 + 3\delta^4). "/></p>
<p>This credits <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> as being greater than <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/>. Provided the projections for the substituted indices <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> were generally slightly conservative, this has hope of correcting them.</p>
<p>
Already, however, we have traipsed over some pitfalls of methodology. One is that the normal expectation </p>
<p align="center"><img alt="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[\frac{1}{1+\epsilon}] = +\infty, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cepsilon+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5B%5Cfrac%7B1%7D%7B1%2B%5Cepsilon%7D%5D+%3D+%2B%5Cinfty%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[\frac{1}{1+\epsilon}] = +\infty, "/></p>
<p>regardless of how small <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> is. For any <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/>, regions around the pole <img alt="{\epsilon = -1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon = -1}"/> get some fixed finite probability. Another is the simple paradox of our second form saying:</p>
<blockquote><p><b> </b> <em> <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{q_i}"/> is an unbiased estimator of <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{p_i}"/>, but <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{p_i}"/> is not an unbiased (or even finite) estimator of <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{q_i}"/>. </em>
</p></blockquote>
<p/><p>
A third curiosity comes from the fourth error form. It gives <img alt="{q_i = p_i e^{\epsilon_i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i+e%5E%7B%5Cepsilon_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i = p_i e^{\epsilon_i}}"/>, so <img alt="{p_i = q_i e^{-\epsilon_i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+q_i+e%5E%7B-%5Cepsilon_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i = q_i e^{-\epsilon_i}}"/>. We have </p>
<p align="center"><img alt="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[e^{b\epsilon}] = e^{0.5b^2 \delta^2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cepsilon+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5Be%5E%7Bb%5Cepsilon%7D%5D+%3D+e%5E%7B0.5b%5E2+%5Cdelta%5E2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[e^{b\epsilon}] = e^{0.5b^2 \delta^2} "/></p>
<p>exactly, without approximation. Again the sign of <img alt="{\epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon_i}"/> does not matter. So we get </p>
<p align="center"><img alt="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[p_i] = q_i e^{0.5\delta^2} &gt; q_i. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cvec%7B%5Cepsilon%7D%7D%5Bp_i%5D+%3D+q_i+e%5E%7B0.5%5Cdelta%5E2%7D+%3E+q_i.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[p_i] = q_i e^{0.5\delta^2} &gt; q_i. "/></p>
<p>But by the original fourth equation we get </p>
<p align="center"><img alt="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[q_i] = p_i e^{0.5\delta^2} &gt; p_i. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cvec%7B%5Cepsilon%7D%7D%5Bq_i%5D+%3D+p_i+e%5E%7B0.5%5Cdelta%5E2%7D+%3E+p_i.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[q_i] = p_i e^{0.5\delta^2} &gt; p_i. "/></p>
<p>So we have <img alt="{\mathsf{E}[q_i] &gt; p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D%5Bq_i%5D+%3E+p_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{E}[q_i] &gt; p_i}"/> and <img alt="{\mathsf{E}[p_i] &gt; q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D%5Bp_i%5D+%3E+q_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{E}[p_i] &gt; q_i}"/>, with both expectations being over the same noise terms. This is like the famous Lake Wobegon <a href="https://trustedadvisor.com/trustmatters/lake-wobegon-syndrome-believing-were-all-above-average">syndrome</a>. What it indicates is the need for care in where and how to apply these error representations.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Have you seen this idea of directly testing (un)predictability in the literature? Might it improve the currently much-debated statistical tests for quantum supremacy?</p>
<p>
Which error model seems most likely to apply? Where have the paradoxes in our last section been noted?</p>
<p/><p><br/>
[some wording tweaks]</p></div>
    </content>
    <updated>2019-11-29T14:35:27Z</updated>
    <published>2019-11-29T14:35:27Z</published>
    <category term="All Posts"/>
    <category term="chess"/>
    <category term="detection"/>
    <category term="Ideas"/>
    <category term="cheating"/>
    <category term="David Spiegelhalter"/>
    <category term="predictability"/>
    <category term="predictive modeling"/>
    <category term="statistics"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-11-30T14:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1093</id>
    <link href="http://corner.mimuw.edu.pl/?p=1093" rel="alternate" type="text/html"/>
    <title>Call for Invited Talk Nominations: HALG 2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">5th Highlights of Algorithms conference (HALG 2020) ETH Zurich, June 3-5, 2020​http://2020.highlightsofalgorithms.org/ The HALG 2020 conference seeks high-quality nominations for invited talks that will highlight recent advances in algorithmic research. Similarly to previous years, there are two categories of invited talks: A. survey … <a href="http://corner.mimuw.edu.pl/?p=1093">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>

5th Highlights of Algorithms conference (HALG 2020)</p>



<p>ETH Zurich, June 3-5, 2020<br/>​<br/><a href="http://2020.highlightsofalgorithms.org/" rel="noreferrer noopener" target="_blank">http://2020.highlightsofalgorithms.org/</a></p>



<p/>



<p>The HALG 2020 conference seeks high-quality nominations for invited talks that will highlight recent advances in algorithmic research. Similarly to previous years, there are two categories of invited talks:</p>



<p>A. survey (60 minutes): a survey of an algorithmic topic that has seen exciting developments in last couple of years.</p>



<p>B. paper (30 minutes): a significant algorithmic result appearing in a paper in 2019 or later.</p>



<p>To nominate, please email <a href="mailto:halg2020.nominations@gmail.com" rel="noreferrer noopener" target="_blank">halg2020.nominations@gmail.com</a> the following information:</p>



<ol><li>Basic details: speaker name + topic (for survey talk) or paper’s title, authors, conference/arxiv + preferable speaker (for paper talk).</li><li>Brief justification: Focus on the benefits to the audience, e.g., quality of results, importance/relevance of topic, clarity of talk, speaker’s presentation skills.</li></ol>



<p>All nominations will be reviewed by the Program Committee (PC) to select speakers that will be invited to the conference.</p>



<p>Nominations deadline: December 20, 2020 (for full consideration).</p></div>
    </content>
    <updated>2019-11-29T13:30:10Z</updated>
    <published>2019-11-29T13:30:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2019-11-29T23:21:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/28/tenure-track-professor-at-university-of-british-columbia-apply-by-december-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/11/28/tenure-track-professor-at-university-of-british-columbia-apply-by-december-15-2019/" rel="alternate" type="text/html"/>
    <title>Tenure-Track Professor at University of British Columbia (apply by December 15, 2019)</title>
    <summary>The Department of Computer Science at the University of British Columbia is inviting applications for at least three positions at the rank of Assistant Professor. We invite applications from candidates of outstanding scientific talent in all areas of computer science. Appointment at a higher rank will be considered for an applicant of exceptional qualifications. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of British Columbia is inviting applications for at least three positions at the rank of Assistant Professor. We invite applications from candidates of outstanding scientific talent in all areas of computer science. Appointment at a higher rank will be considered for an applicant of exceptional qualifications.</p>
<p>Website: <a href="https://www.cs.ubc.ca/our-department/employment/faculty-sessional-positions/tenure-track-faculty-positions-research-stre-0">https://www.cs.ubc.ca/our-department/employment/faculty-sessional-positions/tenure-track-faculty-positions-research-stre-0</a><br/>
Email: research-recruiting-chair@cs.ubc.ca</p></div>
    </content>
    <updated>2019-11-28T23:16:33Z</updated>
    <published>2019-11-28T23:16:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-30T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/172</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/172" rel="alternate" type="text/html"/>
    <title>TR19-172 |  Schur Polynomials do not have small formulas if the Determinant doesn&amp;#39;t!  | 

	Chandra Kanta Mohapatra, 

	Mrinal Kumar, 

	Nutan Limaye, 

	Srikanth Srinivasan, 

	Prasad Chaugule, 

	Adrian She</title>
    <summary>Schur Polynomials are families of symmetric polynomials that have been
classically studied in Combinatorics and Algebra alike. They play a central
role in the study of Symmetric functions, in Representation theory [Sta99], in
Schubert calculus [LM10] as well as in Enumerative combinatorics [Gas96, Sta84,
Sta99]. In recent years, they have also shown up in various incarnations in
Computer Science, e.g, Quantum computation [HRTS00, OW15] and Geometric
complexity theory [IP17].
  However, unlike some other families of symmetric polynomials like the
Elementary Symmetric polynomials, the Power Symmetric polynomials and the
Complete Homogeneous Symmetric polynomials, the computational complexity of
syntactically computing Schur polynomials has not been studied much. In
particular, it is not known whether Schur polynomials can be computed
efficiently by algebraic formulas. In this work, we address this question, and
show that unless \emph{every} polynomial with a small algebraic branching
program (ABP) has a small algebraic formula, there are Schur polynomials that
cannot be computed by algebraic formula of polynomial size. In other words,
unless the algebraic complexity class $\mathrm{VBP}$ is equal to the complexity
class $\mathrm{VF}$, there exist Schur polynomials which do not have polynomial
size algebraic formulas.
  As a consequence of our proof, we also show that computing the determinant of
certain \emph{generalized} Vandermonde matrices is essentially as hard as
computing the general symbolic determinant. To the best of our knowledge, these
are one of the first hardness results of this kind for families of polynomials
which are not \emph{multilinear}. A key ingredient of our proof is the study of
composition of \emph{well behaved} algebraically independent polynomials with a homogeneous polynomial, and might be of independent interest.</summary>
    <updated>2019-11-28T06:40:42Z</updated>
    <published>2019-11-28T06:40:42Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-30T14:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=381</id>
    <link href="https://tcsplus.wordpress.com/2019/11/27/tcs-talk-wednesday-december-4-nihar-shah-cmu/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, December 4 — Nihar Shah, CMU</title>
    <summary>The next TCS+ talk, and last of the Fall season, will take place this coming Wednesday, December 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Nihar Shah from CMU will speak about “Battling Demons in Peer Review” (abstract below). Please make sure you reserve a spot for your group […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk, and last of the Fall season, will take place this coming Wednesday, December 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Nihar Shah</strong> from CMU will speak about “<em>Battling Demons in Peer Review</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Peer review is the backbone of scholarly research. It is however faced with a number of challenges (or “demons”) which cause unfairness to authors, and degrade the overall quality of the process. This talk will present principled and practical approaches to battle these demons in peer review:</p>
<ol>
<li>Subjectivity: How to ensure that all papers are judged by the same yardstick?</li>
<li>Mis-calibration: How to use ratings in presence of arbitrary or adversarial mis-calibration?</li>
<li>Bias: How to rigorously test for existence of (gender/fame/race/…) biases in peer review?</li>
<li>Strategic behavior: How to insulate peer review from strategic behavior of author-reviewers?</li>
<li>Noise: How to assign reviewers to papers to simultaneously ensure fair and accurate evaluations in the presence of review noise?</li>
</ol>
<p>The work uses tools from social choice theory, statistics and learning theory, information theory, game theory and decision theory. No prior knowledge on these topics will be assumed.</p></blockquote>
<p>Bio:<br/>
<em><a href="http://cs.cmu.edu/~nihars">Nihar B. Shah</a> is an Assistant Professor in the Machine Learning and Computer Science departments at Carnegie Mellon University (CMU). His research interests include statistics, machine learning, information theory, and game theory, with a focus on applications to learning from people. He is a recipient of the the 2017 David J. Sakrison memorial prize from EECS Berkeley for a “truly outstanding and innovative PhD thesis”, the Microsoft Research PhD Fellowship 2014-16, the Berkeley Fellowship 2011-13, the IEEE Data Storage Best Paper and Best Student Paper Awards for the years 2011/2012, and the SVC Aiya Medal 2010, and has supervised the Best Student Paper at AAMAS 2019.</em></p></div>
    </content>
    <updated>2019-11-28T01:56:48Z</updated>
    <published>2019-11-28T01:56:48Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-11-30T14:21:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/27/faculty-asst-and-assoc-prof-at-university-of-washington-apply-by-december-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/11/27/faculty-asst-and-assoc-prof-at-university-of-washington-apply-by-december-15-2019/" rel="alternate" type="text/html"/>
    <title>Faculty (Asst. and Assoc. Prof) at University of Washington (apply by December 15, 2019)</title>
    <summary>This year we have a targeted search in all areas of quantum computing, with a particular emphasis on quantum algorithms and quantum complexity theory. Website: https://apply.interfolio.com/64708 Email: jrl@cs.washington.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This year we have a targeted search in all areas of quantum computing, with a particular emphasis on quantum algorithms and quantum complexity theory.</p>
<p>Website: <a href="https://apply.interfolio.com/64708">https://apply.interfolio.com/64708</a><br/>
Email: jrl@cs.washington.edu</p></div>
    </content>
    <updated>2019-11-27T22:00:37Z</updated>
    <published>2019-11-27T22:00:37Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-30T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7585</id>
    <link href="https://windowsontheory.org/2019/11/27/halg-2020-call-for-nominations-guest-post-by-yossi-azar/" rel="alternate" type="text/html"/>
    <title>HALG 2020 call for nominations (guest post by Yossi Azar)</title>
    <summary>[Guest post by Yossi Azar – I attended HALG once and enjoyed it quite a lot; I highly recommend people make such nominations –Boaz] Call for Invited Talk Nominations :5th Highlights of Algorithms conference (HALG 2020) ETH Zurich, June 3-5, 2020​http://2020.highlightsofalgorithms.org/ The HALG 2020 conference seeks high-quality nominations for invited talks that will highlight recent advances in algorithmic research. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Yossi Azar – I attended HALG once and enjoyed it quite a lot; I highly recommend people make such nominations –Boaz]</em></p>



<h3><strong>Call for Invited Talk Nominations</strong> :<strong>5th Highlights of Algorithms conference (HALG 2020)</strong></h3>



<p>ETH Zurich, June 3-5, 2020<br/>​<br/><a href="http://2020.highlightsofalgorithms.org/" rel="noreferrer noopener" target="_blank">http://2020.highlightsofalgorithms.org/</a></p>



<p/>



<p>The HALG 2020 conference seeks high-quality nominations for invited talks that will highlight recent advances in algorithmic research. Similarly to previous years, there are two categories of invited talks:</p>



<p>A. survey (60 minutes): a survey of an algorithmic topic that has seen exciting developments in last couple of years.</p>



<p>B. paper (30 minutes): a significant algorithmic result appearing in a paper in 2019 or later.</p>



<p>To nominate, please email <a href="mailto:halg2020.nominations@gmail.com" rel="noreferrer noopener" target="_blank">halg2020.nominations@gmail.com</a> the following information:</p>



<ol><li>Basic details: speaker name + topic (for survey talk) or paper’s title, authors, conference/arxiv + preferable speaker (for paper talk).</li><li>Brief justification: Focus on the benefits to the audience, e.g., quality of results, importance/relevance of topic, clarity of talk, speaker’s presentation skills.</li></ol>



<p>All nominations will be reviewed by the Program Committee (PC) to select speakers that will be invited to the conference.</p>



<p>Nominations deadline: <strong>December 20, 2020 </strong>(for full consideration).</p></div>
    </content>
    <updated>2019-11-27T18:42:42Z</updated>
    <published>2019-11-27T18:42:42Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-11-30T14:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3451</id>
    <link href="https://agtb.wordpress.com/2019/11/27/new-book-introduction-to-multi-armed-bandits-by-alex-slivkins/" rel="alternate" type="text/html"/>
    <title>New Book: Introduction to Multi-Armed Bandits by Alex Slivkins</title>
    <summary>Here’s Alex’s announcement of his new book, which I am very excited about, and many in our community would no doubt find extremely useful (there’s even an open version on arXiv!): I am pleased to announce Introduction to multi-armed bandits, a broad and accessible introduction to the area which emphasizes connections to operations research, game […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Here’s Alex’s announcement of his new book, which I am very excited about, and many in our community would no doubt find extremely useful (there’s even an open version on arXiv!):</p>
<hr/>
<p>I am pleased to announce <a href="https://www.nowpublishers.com/article/Details/MAL-068">Introduction to multi-armed bandits</a>, a broad and accessible introduction to the area which emphasizes connections to operations research, game theory, and mechanism design. The said connections have generated a considerable amount of interest (and publications) in the Economics and Computation community.</p>
<p>The book is teachable by design: each chapter corresponds to one week of my class. Each chapter handles one big direction in the literature on bandits, covers the first-order concepts and results on a technical level, and provides a detailed literature review for further exploration. There are no prerequisites other than a certain level of mathematical maturity.</p>
<p>The chapters are as follows: stochastic bandits; lower bounds; Bayesian bandits and Thompson Sampling; Lipschitz Bandits; full feedback and adversarial costs; adversarial bandits; linear costs and semi-bandits; contextual bandits; bandits and games; bandits with knapsacks; bandits and incentives.</p>
<p>The book is also <a href="https://arxiv.org/abs/1904.07272">available on arxiv</a> (in a plain-format version).</p>
<p>Aleksandrs Slivkins<br/>
Microsoft Research NYC</p></div>
    </content>
    <updated>2019-11-27T17:44:13Z</updated>
    <published>2019-11-27T17:44:13Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Yannai A. Gonczarowski</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-11-30T14:20:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/11/27/recoloring-infinite-paths</id>
    <link href="https://11011110.github.io/blog/2019/11/27/recoloring-infinite-paths.html" rel="alternate" type="text/html"/>
    <title>Recoloring infinite paths</title>
    <summary>Suppose you have a uniformly random 3-coloring of a doubly-infinite path graph. This can be generated by choosing any one vertex, choosing any one of the three colors for it, and then propagating the coloring outwards from it, choosing one of the two available colors for each successive vertex. It’s convenient to imagine the three colors as being represented by the three numbers 0, 1, and 2 mod 3. Now perform the following process, repeatedly: change the color of every cell whose two neighbors both have the color that is one plus its color (mod 3). In other words, the middle vertex of a triple of vertices that is colored 1–0–1 changes to color 2, the middle vertex of 2–1–2 changes to 0, and the middle vertex of 0–2–0 changes to 1. What does this do to the coloring?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Suppose you have a uniformly random 3-coloring of a doubly-infinite path graph. This can be generated by choosing any one vertex, choosing any one of the three colors for it, and then propagating the coloring outwards from it, choosing one of the two available colors for each successive vertex. It’s convenient to imagine the three colors as being represented by the three numbers 0, 1, and 2 mod 3. Now perform the following process, repeatedly: change the color of every cell whose two neighbors both have the color that is one plus its color (mod 3). In other words, the middle vertex of a triple of vertices that is colored 1–0–1 changes to color 2, the middle vertex of 2–1–2 changes to 0, and the middle vertex of 0–2–0 changes to 1. What does this do to the coloring?</p>

<p>You might think that it stays uniformly random, but it doesn’t. What happens is that you get increasingly large 2-colored regions, whose typical size is proportional to the square root of the number of recoloring steps, separated by triples of vertices that use all three colors. Within each 2-colored region, each recoloring step changes one of the two colors to the third color. The 3-colored triples form the boundaries between these regions and move leftwards or rightwards (depending on the ordering of their three colors). When two boundaries moving in opposite directions collide, they annihilate each other, leaving a larger 2-colored region.</p>

<p>I think the easiest way to see this is to use height functions for colorings. As described in <a href="https://11011110.github.io/blog/2019/11/25/reconfiguring-3-colorings.html">my previous post</a>, a height function is a mapping from the vertices to integers (the heights of the vertices), so that taking the height of each vertex mod 3 gives its color, and such that neighboring vertices have heights that differ by . It’s easy to construct these for colorings of the infinite path, again by starting from an arbitrary choice of height for a single arbitrarily-chosen vertex and propagating outwards. More strongly, for infinite bipartite graphs, the existence of height functions is equivalent to the non-existence of certain special graph homomorphisms to a six-cycle, as described for the finite case in my previous post. However in the infinite case the existence of height functions does not ensure the connectivity of the space of 3-colorings; for instance there is no way to change the infinite periodic 3-coloring …0–1–2–0–1–2… into anything else.</p>

<p>In terms of height functions, what’s happening in each recoloring step is that the height increases by two at each of its local minima. You can think of the height function as giving the height of a pile of particles over each vertex; then each recoloring step adds a particle wherever it can sit in place without rolling downhill.</p>

<p style="text-align: center;"><img alt="Particle deposition view of Rule 184" src="https://11011110.github.io/blog/assets/2019/rule-184-deposition.svg"/></p>

<p>The image above is one I drew several years ago for the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Rule_184">cellular automaton rule 184</a>. This cellular automaton has two states per cell (which we might as well think of as 1 and 0), and it acts by repeatedly swapping the states of pairs of consecutive cells containing the pattern 10. In the figure, the 1’s and 0’s translate to pieces of surface boundary that slope downwards or upwards (respectively), so a 10 pattern is a local minimum of the surface, and filling it in gives a 10 instead.</p>

<p>Rule 184 has an amazing variety of seemingly-unrelated interpretations; for instance, you can think of the cells of the automaton as a gridlocked highway, and the cells with 1’s in them as the cars of a traffic jam (perhaps <a href="https://www.latimes.com/california/story/2019-11-27/how-405-freeway-gridlock-became-the-iconic-image-of-l-a-thanksgiving">stuck in traffic for their Thanksgiving Day travels</a>). When a 1 has an open space ahead of it (a 0 cell) it moves forward, and otherwise it stays in place. This seemingly basic model displays a lot of the same features of real traffic such as freely flowing traffic when the total number of vehicles is small but waves of stop-and-go motion when the number of vehicles is high. It forms the basis for many more-sophisticated models of traffic flow.</p>

<p>But it is a different interpretation of Rule 184 that works best for understanding the question I started with, of what happens to a random coloring under recoloring operations. These operations are exactly what happens when you apply Rule 184 to the height function of a random coloring. But you can also view Rule 184 as describing a system of two kinds of particles, left-moving ones (11 patterns) and right-moving ones (00 patterns), separated by empty space (alternating 0’s and 1’s), that annihilate each other when they collide. In terms of the coloring, these particles are just the triples of vertices colored with three colors, and the parts of the cellular automaton with no particles are the 2-colored regions. The uniformly random coloring that we started with has the convenient property that particles of both types are equally likely. A right-moving particle will survive to step  if, in every prefix of the random sequence of particles to the right of it of length proportional to , there are more right-moving particles than left-moving particles. A standard calculation on random walks shows that this survival probability is , and this is also the density of remaining particles after  steps.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/103212704886997345">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-11-27T16:11:00Z</updated>
    <published>2019-11-27T16:11:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-11-28T00:29:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/171</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/171" rel="alternate" type="text/html"/>
    <title>TR19-171 |  Improved bounds on the AN-complexity of multilinear functions | 

	Oded Goldreich</title>
    <summary>We consider arithmetic circuits with arbitrary large (multi-linear) gates for computing multi-linear functions. An adequate complexity measure for such circuits is the maximum between the arity of the gates and their number. 
This model and the corresponding complexity measure were introduced by Goldreich and Wigderson (ECCC, TR13-043, 2013), and is called the AN-complexity.

The AN-complexity of a multi-linear function yields an upper bound on the size of depth-three Boolean circuits for computing the function, and it is not clear whether or not significantly smaller depth-three Boolean functions exist. Specifically, the depth-three size of Boolean circuits is at most exponential in the AN-complexity of the function. Hence, proving linear lower bounds on the AN-complexity of explicit multi-linear function is a essential step towards proving that depth-three Boolean circuits for these functions requires exponential size.

In this work we present explicit multi-linear functions that require depth-two multi-linear circuits of almost linear AN-complexity. Specifically, for every $\epsilon&gt;0$, we show an explicit $\poly(1/\epsilon)$-linear function $f:\{0,1\}^{\poly(1/\epsilon)\cdot n}\to\{0,1\}$ such that any depth-two circuit (with general multi-linear gates) that computes $f$ must use gates of arity at least $n^{1-\epsilon}$. This improves over a corresponding lower bound of $\tildeOM(n^{2/3})$ that was known for an explicit tri-linear function
(Goldreich and Tal, Computational Complexity, 2018), but leaves open the problem of showing similar AN-complexity lower bounds for multi-linear circuits of larger depth. 

A key aspect in our proof is considering many (extremely skewed) random restrictions, and contrasting the sum of the values of the original function and the circuit (which supposedly computes it) taken over a (carefully chosen) subset of these random restrictions. We show that if the original circuit has too low AN-complexity, then these two sums cannot be equal, which yields a contradiction.</summary>
    <updated>2019-11-27T14:53:19Z</updated>
    <published>2019-11-27T14:53:19Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-30T14:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/170</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/170" rel="alternate" type="text/html"/>
    <title>TR19-170 |  A Quadratic Lower Bound for  Algebraic Branching Programs | 

	Prerona Chatterjee, 

	Mrinal Kumar, 

	Adrian She, 

	Ben Lee Volk</title>
    <summary>We show that any Algebraic Branching Program (ABP) computing the polynomial $\sum_{i = 1}^n x_i^n$ has at least $\Omega(n^2)$ vertices. This improves upon the lower bound of $\Omega(n\log n)$, which follows from the classical result of Baur and Strassen [Str73, BS83], and extends the results by Kumar [Kum19], which showed a quadratic lower bound  for $\text{homogeneous}$ ABPs computing the same polynomial.

Our proof relies on a notion of depth reduction which is reminiscent of similar statements in the context of matrix rigidity, and shows that any small enough ABP computing the polynomial $\sum_{i=1}^n x_i^n$ can be depth reduced to essentially a homogeneous ABP of the same size which computes the polynomial $\sum_{i = 1}^n x_i^n + \varepsilon(\mathbf{x})$, for a structured ``error polynomial'' $\varepsilon(\mathbf{x})$. To complete the proof, we then observe that the lower bound in [Kum19] is robust enough and continues to hold for all polynomials $\sum_{i = 1}^n x_i^n + \varepsilon(\mathbf{x})$, where  $\varepsilon(\mathbf{x})$ has the appropriate structure.</summary>
    <updated>2019-11-27T10:09:19Z</updated>
    <published>2019-11-27T10:09:19Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-30T14:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3448</id>
    <link href="https://agtb.wordpress.com/2019/11/27/youngec-workshop-in-tel-aviv-31-12-19-2-1-20/" rel="alternate" type="text/html"/>
    <title>YoungEC Workshop in Tel-Aviv, 31/12/19-2/1/20</title>
    <summary>The “Young” Workshop on Economics and Computation (YoungEC) will be held in Tel-Aviv University, Israel, during December 31st, 2019 to January 2nd, 2020. The list of speakers includes a small number of established central figures in the field together with a larger number of bright rising stars worldwide. The workshop is now open for registration.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://www.cs.tau.ac.il/~mfeldman/youngec19/index.php">“Young” Workshop on Economics and Computation (YoungEC)</a> will be held in Tel-Aviv University, Israel, during December 31st, 2019 to January 2nd, 2020. The <a href="https://www.cs.tau.ac.il/~mfeldman/youngec19/participants.php">list of speakers</a> includes a small number of established central figures in the field together with a larger number of bright rising stars worldwide.</p>
<p>The workshop is now open for <a href="https://www.cs.tau.ac.il/~mfeldman/youngec19/registration.php">registration</a>.</p></div>
    </content>
    <updated>2019-11-27T08:49:26Z</updated>
    <published>2019-11-27T08:49:26Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>algorithmicgametheory</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-11-30T14:20:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/27/tenure-track-assistant-professor-at-university-of-victoria-apply-by-december-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/11/27/tenure-track-assistant-professor-at-university-of-victoria-apply-by-december-1-2019/" rel="alternate" type="text/html"/>
    <title>Tenure-track assistant professor at University of Victoria (apply by December 1, 2019)</title>
    <summary>The Department of Computer Science at the University of Victoria is seeking applicants for two positions at the rank of Assistant Professor with eligibility for tenure and with an anticipated start date of July 1, 2020. We are particularly seeking candidates in the areas of Graphics, Systems, AI and Theory. Website: https://www.uvic.ca/engineering/computerscience/people/employment-opportunities/ Email: search@csc.uvic.ca</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Victoria is seeking applicants for two positions at the rank of Assistant Professor with eligibility for tenure and with an anticipated start date of July 1, 2020. We are particularly seeking candidates in the areas of Graphics, Systems, AI and Theory.</p>
<p>Website: <a href="https://www.uvic.ca/engineering/computerscience/people/employment-opportunities/">https://www.uvic.ca/engineering/computerscience/people/employment-opportunities/</a><br/>
Email: search@csc.uvic.ca</p></div>
    </content>
    <updated>2019-11-27T05:40:09Z</updated>
    <published>2019-11-27T05:40:09Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-30T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/26/assistant-professor-at-university-of-california-san-diego-apply-by-january-15-2020/</id>
    <link href="https://cstheory-jobs.org/2019/11/26/assistant-professor-at-university-of-california-san-diego-apply-by-january-15-2020/" rel="alternate" type="text/html"/>
    <title>Assistant Professor  at University of California – San Diego (apply by January 15, 2020)</title>
    <summary>The UC San Diego Department of Computer Science and Engineering (CSE) invites applications for multiple tenure-track faculty positions at Assistant Professor rank. The department is looking for exceptional candidates in all areas of Computer Science and Engineering. A Ph.D. or advancement to candidacy in Computer Science &amp; Engineering or related disciplines is required at the […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The UC San Diego Department of Computer Science and Engineering (CSE) invites applications for multiple tenure-track faculty positions at Assistant Professor rank. The department is looking for exceptional candidates in all areas of Computer Science and Engineering. A Ph.D. or advancement to candidacy in Computer Science &amp; Engineering or related disciplines is required at the time of application.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/JPF02337">https://apol-recruit.ucsd.edu/JPF02337</a><br/>
Email: nherrera@eng.ucsd.edu</p></div>
    </content>
    <updated>2019-11-26T23:19:25Z</updated>
    <published>2019-11-26T23:19:25Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-30T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/26/postdoc-at-boston-college-apply-by-january-15-2020/</id>
    <link href="https://cstheory-jobs.org/2019/11/26/postdoc-at-boston-college-apply-by-january-15-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc at Boston College (apply by January 15, 2020)</title>
    <summary>Applications are invited for a postdoc position hosted by Hsin-Hao Su at Boston College. Areas of specific interests include but not limited to distributed graph algorithms, local algorithms, dynamic graph algorithms, gossip algorithms, and MPC algorithms. To apply, please send the materials indicated in the link to Hsin-Hao Su by January 15, 2020. Website: https://sites.google.com/site/distributedhsinhao/postdoc […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are invited for a postdoc position hosted by Hsin-Hao Su at Boston College. Areas of specific interests include but not limited to distributed graph algorithms, local algorithms, dynamic graph algorithms, gossip algorithms, and MPC algorithms.</p>
<p>To apply, please send the materials indicated in the link to Hsin-Hao Su by January 15, 2020.</p>
<p>Website: <a href="https://sites.google.com/site/distributedhsinhao/postdoc">https://sites.google.com/site/distributedhsinhao/postdoc</a><br/>
Email: suhx@bc.edu</p></div>
    </content>
    <updated>2019-11-26T21:08:13Z</updated>
    <published>2019-11-26T21:08:13Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-30T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4432</id>
    <link href="https://www.scottaaronson.com/blog/?p=4432" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4432#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4432" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Guest post by Greg Kuperberg: Archimedes’ other principle and quantum supremacy</title>
    <summary xml:lang="en-US">Scott’s Introduction: Happy Thanksgiving! Please join me in giving thanks for the beautiful post below, by friend-of-the-blog Greg Kuperberg, which tells a mathematical story that stretches from the 200s BC all the way to Google’s quantum supremacy result last month. Archimedes’ other principle and quantum supremacy by Greg Kuperberg Note: UC Davis is hiring in […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Scott’s Introduction:</strong> Happy Thanksgiving!  Please join me in giving thanks for the beautiful post below, by friend-of-the-blog <a href="https://www.math.ucdavis.edu/~greg/">Greg Kuperberg</a>, which tells a mathematical story that stretches from the 200s BC all the way to Google’s quantum supremacy result last month.</p>



<h2>Archimedes’ other principle and quantum supremacy</h2>



<p>by Greg Kuperberg</p>



<p><strong>Note:</strong> UC Davis is <a href="https://recruit.ucdavis.edu/JPF03248">hiring in CS theory</a>! Scott offered me free ad space if I wrote a guest post, so here we are.  The position is in all areas of CS theory, including QC theory although the search is not limited to that.</p>



<p>In this post, I wear the hat of a pure mathematician in a box provided by Archimedes.  I thus set aside what everyone else thinks is important about Google’s 53-qubit quantum supremacy experiment, that it is a dramatic milestone in quantum computing technology.  That’s only news about the physical world, whose significance pales in comparison to the Platonic world of mathematical objects.  In my happy world, I like quantum supremacy as a demonstration of a beautiful coincidence in mathematics that has been known for more than 2000 years in a special case. The single-qubit case was discovered by Archimedes, duh.  As Scott mentions in <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565/ref=sr_1_1?keywords=quantum+computing+since+democritus&amp;qid=1574801358&amp;sr=8-1"><em>Quantum Computing Since Democritus</em></a>, Bill Wootters stated the general result in a <a href="https://link.springer.com/article/10.1007/BF01883491">1990 paper</a>, but Wootters credits a <a href="https://link.springer.com/article/10.1007/BF01019475">1974 paper</a> by the Czech physicist Stanislav Sýkora.  I learned of it in the substantially more general context of symplectic geometric that mathematicians developed independently between Sýkora’s prescient paper and Wootters’ more widely known citation.  Much as I would like to clobber you with highly abstract mathematics, I will wait for some other time.</p>



<p>Suppose that you pick a pure state \(|\psi\rangle\) in the Hilbert space \(\mathbb{C}^d\) of a \(d\)-dimensional qudit, and then make many copies and fully measure each one, so that you sample many times from some distribution \(\mu\) on the \(d\) outcomes.  You can think of such a distribution \(\mu\) as a classical randomized state on a digit of size \(d\).  The set of all randomized states on a \(d\)-digit makes a \((d-1)\)-dimensional simplex \(\Delta^{d-1}\) in the orthant \(\mathbb{R}_{\ge 0}^d\).  The coincidence is that if \(|\psi\rangle\) is uniformly random in the unit sphere in \(\mathbb{C}^d\), then \(\mu\) is uniformly random in \(\Delta^{d-1}\).  I will call it the Born map, since it expresses the Born rule of quantum mechanics that amplitudes yield probabilities.  Here is a diagram of the Born map of a qutrit, except with the laughable simplification of the 5-sphere in \(\mathbb{C}^3\) drawn as a 2-sphere. </p>



<figure class="wp-block-image"><img alt="" src="https://www.scottaaronson.com/f1-qutrit.png"/></figure>



<p>If you pretend to be a bad probability student, then you might not be surprised by this coincidence, because you might suppose that all probability distributions are uniform other than treacherous exceptions to your intuition.  However, the principle is certainly not true for a “rebit” (a qubit with real amplitudes) or for higher-dimensional “redits.”  With real amplitudes, the probability density goes to infinity at the sides of the simplex \(\Delta^{d-1}\) and is even more favored at the corners.  It also doesn’t work for mixed qudit states; the projection then favors the middle of \(\Delta^{d-1}\). </p>



<h3>Archimedes’ theorem</h3>



<p> The theorem of Archimedes is that a natural projection from the unit sphere to a circumscribing vertical cylinder preserves area.  The projection is the second one that you might think of: Project radially from a vertical axis rather than radially in all three directions.  Since Archimedes was a remarkably prescient mathematician, he was looking ahead to the Bloch sphere of pure qubit states \(|\psi\rangle\langle\psi|\) written in density operator form.  If you measure \(|\psi\rangle\langle\psi|\) in the computational basis, you get a randomized bit state \(\mu\) somewhere on the interval from guaranteed 0 to guaranteed 1. </p>



<figure class="wp-block-image"><img alt="" src="https://www.scottaaronson.com/f2-bloch.png"/></figure>



<p>This transformation from a quantum state to a classical randomized state is a linear projection to a vertical axis.  It is the same as Archimedes’ projection, except without the angle information.  It doesn’t preserve dimension, but it does preserve measure (area or length, whatever) up to a factor of \(2\pi\).  In particular, it takes a uniformly random \(|\psi\rangle\langle\psi|\) to a uniformly random \(\mu\).</p>



<p>Archimedes’ projection is also known as the Lambert cylindrical map of the world.  This is the map that squishes Greenland along with the top of North America and Eurasia to give them proportionate area. </p>



<figure class="wp-block-image"><img alt="" src="https://www.scottaaronson.com/f3-lambert.jpg"/></figure>



<p>(I forgive Lambert if he didn’t give prior credit to Archimedes.  There was no Internet back then to easily find out who did what first.)  Here is a calculus-based proof of Archimedes’ theorem: In spherical coordinates, imagine an annular strip on the sphere at a polar angle of \(\theta\).  (The polar angle is the angle from vertical in spherical coordinates, as depicted in red in the Bloch sphere diagram.)   The strip has a radius of \(\sin\theta\), which makes it shorter than its unit radius friend on the cylinder.  But it’s also tilted from vertical by an angle of \(\frac{\pi}2-\theta\), which makes it wider by a factor of \(1/(\sin \theta)\) than the height of its projection onto the \(z\) axis.  The two factors exactly cancel out, making the area of the strip exactly proportional to the length of its projection onto the \(z\) axis.  This is a coincidence which is special to the 2-sphere in 3 dimensions.  As a corollary, we get that the surface area of a unit sphere is \(4\pi\), the same as an open cylinder with radius 1 and height 2.  If you want to step through this in even more detail, Scott mentioned to me an <a href="https://www.youtube.com/watch?v=GNcFjFmqEc8">action video</a> which is vastly spiffier than anything that I could ever make.</p>



<p>The projection of the Bloch sphere onto an interval also shows what goes wrong if you try this with a rebit.  The pure rebit states — again expressed in density operator form \(|\psi\rangle\langle\psi|\) are a great circle in the Bloch sphere.  If you linearly project a circle onto an interval, then the length of the circle is clearly bunched up at the ends of the interval and the projected measure on the interval is not uniform. </p>



<h3>Sýkora’s generalization</h3>



<p> It is a neat coincidence that the Born map of a qubit preserves measure, but a proof that relies on Archimedes’ theorem seems to depend on the special geometry of the Bloch sphere of a qubit.  That the higher-dimensional Born map also preserves measure is downright eerie.  Scott challenged me to write an intuitive explanation.  I  remembered two different (but similar) proofs, neither of which is original to me. Scott and I disagree as to which proof is nicer.</p>



<p>As a first step of the first proof, it is easy to show that the Born map \(p = |z|^2\) for a single amplitude \(z\) preserves measure as a function from the complex plane \(\mathbb{C}\) to the ray \(\mathbb{R}_{\ge 0}\).  The region in the complex numbers \(\mathbb{C}\) where the length of \(z\) is between \(a\) and \(b\), or \(a \le |z| \le b\), is \(\pi(b^2 – a^2)\).  The corresponding interval for the probability is \(a^2 \le p \le b^2\), which thus has length \(b^2-a^2\).  That’s all, we’ve proved it!  More precisely, the area of any circularly symmetric region in \(\mathbb{C}\) is \(\pi\) times the length of its projection onto \(\mathbb{R}_{\ge 0}\). </p>



<figure class="wp-block-image"><img alt="" src="https://www.scottaaronson.com/f4-washer.png"/></figure>



<p>The second step is to show the same thing for the Born map from the \(d\)-qudit Hilbert space \(\mathbb{C}^d\) to the \(d\)-digit orthant \(\mathbb{R}_{\ge 0}^d\), again without unit normalization.  It’s also measure-preserving, up to a factor of \(\pi^d\) this time, because it’s the same thing in each coordinate separately.  To be precise, the volume ratio holds for any region in \(\mathbb{C}^d\) that is invariant under separately rotating each of the \(d\) phases. (Because you can approximate any such region with a union of products of thin annuli.)</p>



<p>The third and final step is the paint principle for comparing surface areas.  If you paint the hoods of two cars with the same thin layer of paint and you used the same volume of paint for each one, then you can conclude that the two car hoods have nearly same area.  In our case, the Born map takes the region \[ 1 \le |z_0|^2 + |z_1|^2 + \cdots + |z_{d-1}|^2 \le 1+\epsilon \] in \(\mathbb{C}^d\) to the region \[ 1 \le p_0 + p_1 + \cdots + p_{d-1} \le 1+\epsilon \] in the orthant \(\mathbb{R}_{\ge 0}^d\).  The former is the unit sphere \(S^{2d-1}\) in \(\mathbb{C}^d\) painted to a thickness of roughly \(\epsilon/2\).  The latter is the probability simplex \(\Delta^{n-1}\) painted to a thickness of exactly \(\epsilon\). Taking the limit \(\epsilon \to 0\), the Born map from \(S^{2d-1}\) to \(\Delta^{n-1}\) preserves measure up to a factor of \(2\pi^n\).</p>



<p>You might wonder “why” this argument works even if you accept that it does work.  The key is that the exponent 2 appears in two different ways: as the dimension of the complex numbers, and as the exponent used to set probabilities and define spheres.  If we try the same argument with real amplitudes, then the volume between “spheres” of radius \(a\) and \(b\) is just \(2(b-a)\), which does not match the length \(b^2-a^2\).  The Born map for a single real amplitude is the parabola \(p = x^2\), which clearly distorts length since it is not linear.  The higher-dimensional real Born map similarly distorts volumes, whether or not you restrict to unit-length states.</p>



<p>If you’re a bitter-ender who still wants Archimedes’ theorem for real amplitudes, then you might consider the variant formula \(p = |x|\) to obtain a probability \(p\) from a “quantum amplitude” \(x\).  Then the “Born” map does preserve measure, but for the trivial reason that \(x = \pm p\) is not really a quantum amplitude, it is a probability with a vestigial sign.  Also the unit “sphere” in \(\mathbb{R}^d\) is not really a sphere in this theory, it is a hyperoctahedron.  The only “unitary” operators that preserve the unit hyperoctahedron are signed permutation matrices.  You can only use them for reversible classical computing or symbolic dynamics; they don’t have the strength of true quantum computing or quantum mechanics.</p>



<p>The fact that the Born map preserves measure also yields a bonus calculation of the volume of the unit ball in \(2d\) real dimensions, if we interpret that as \(d\) complex dimensions.  The ball \[ |z_0|^2 + |z_1|^2 + \cdots + |z_{d-1}|^2 \le 1 \] in \(\mathbb{C}^d\) is sent to a different simplex \[ p_0 + p_1 + \cdots + p_{d-1} \le 1 \] in \(\mathbb{R}_{\ge 0}^d\).  If we recall that the volume of a \(d\)-dimensional pyramid is \(\frac1d\) times base times height and calculate by induction on \(d\), we get that this simplex has volume \(\frac1{d!}\).  Thus, the volume of the \(2d\)-dimensional unit ball is \(\frac{\pi^d}{d!}\).</p>



<p>You might ask whether the volume of a \(d\)-dimensional unit ball is always \(\frac{\pi^{d/2}}{(d/2)!}\) for both \(d\) even and odd.  The answer is yes if we interpret factorials using the gamma function formula \(x! = \Gamma(x+1)\) and look up that \(\frac12! = \Gamma(\frac32) = \frac{\sqrt{\pi}}2\).  The gamma function was discovered by Euler as a solution to the question of defining fractional factorials, but the notation \(\Gamma(x)\) and the cumbersome shift by 1 is due to Legendre.  Although Wikipedia says that no one knows why Legendre defined it this way, I wonder if his goal was to do what the Catholic church later did for itself in 1978: It put a Pole at the origin.</p>



<p>(Scott wanted to censor this joke. In response, I express my loyalty to my nation of birth by quoting the opening of the Polish national anthem: “Poland has not yet died, so long as we still live!”  I thought at first that Stanislav Sýkora is Polish since Stanisław and Sikora are both common Polish names, but his name has Czech spelling and he is Czech. Well, the Czechs are cool too.)</p>



<p>Sýkora’s 1974 proof of the generalized Archimedes’ theorem is different from this one.  He calculates multivariate moments of the space of unit states \(S^{2d-1} \subseteq \mathbb{C}^d\), and confirms that they match the moments in the probability simplex \(\Delta^{d-1}\).  There are inevitably various proofs of this result, and I will give another one. </p>



<h3>Another proof, and quantum supremacy</h3>



<p>There is a well-known and very useful algorithm to generate a random point on the unit sphere in either \(\mathbb{R}^d\) or \(\mathbb{C}^d\), and a similar algorithm to generate a random point in a simplex.  In the former algorithm, we make each real coordinate \(x\) into an independent Gaussian random variable with density proportional to \(e^{-x^2}\;dx\), and then rescale the result to unit length.  Since the exponents combine as \[ e^{-x_0^2}e^{-x_1^2}\cdots e^{-x_{d-1}^2} =       e^{-(x_0^2 + x_1^2 + \cdots + x_{d-1}^2)}, \] we learn that the total exponent is spherically symmetric.  Therefore after rescaling, the result is a uniformly random point on the unit sphere \(S^{d-1} \subseteq \mathbb{R}^d\).  Similarly, the other algorithm generates a point in the orthant \(\mathbb{R}_{\ge 0}^d\) by making each real coordinate \(p \ge 0\) an independent random variable with exponential distribution \(e^{-p}\;dp\).  This time we rescale the vector until its sum is 1.  This algorithm likewise produces a uniformly random point in the simplex \(\Delta^{d-1} \subseteq \mathbb{R}_{\ge 0}^d\) because the total exponent of the product \[ e^{-p_0}e^{-p_1}\cdots e^{-p_{d-1}} =       e^{-(p_0 + p_1 + \cdots + p_{d-1})} \] only depends on the sum of the coordinates.  Wootters describes both of these algorithms in his 1990 paper, but instead of relating them to give his own proof of the generalized Archimedes’ theorem, he cites Sýkora.</p>



<p>The gist of the proof is that the Born map takes the Gaussian algorithm to the exponential algorithm.  Explicitly, the Gaussian probability density for a single complex amplitude \[ z = x+iy = re^{i\theta} \] can be converted from Cartesian to polar coordinate as follows: \[ \frac{e^{-|z|^2}\;dx\;dy}{\pi} = \frac{e^{-r^2}r\;dr\;d\theta}{\pi}. \] I have included the factor of \(r\) that is naturally present in an area integral in polar coordinates.  We will need it, and it is another way to see that the theorem relies on the fact that the complex numbers are two-dimensional.  To complete the proof, we substitute \(p = r^2\) and remember that \(dp = 2r\;dr\), and then integrate over \(\theta\) (trivially, since the integrand does not depend on \(\theta\)).  The density simplifies to \(e^{-p}\;dp\), which is exactly the exponential distribution for a real variable \(p \ge 0\).  Since the Born map takes the Gaussian algorithm to the exponential algorithm, and since each algorithm produces a uniformly random point, the Born map must preserve uniform measure.  (Scott likes this proof better because it is algorithmic, and because it is probabilistic.)</p>



<p>Now about quantum supremacy.  You might think that a random chosen quantum circuit on \(n\) qubits produces a nearly uniformly random quantum state \(|\psi\rangle\) in their joint Hilbert space, but it’s not quite not that simple.  When \(n=53\), or otherwise as \(n \to \infty\), a manageable random circuit is not nearly creative enough to either reach or approximate most of the unit states in the colossal Hilbert space of dimension \(d = 2^n\).  The state \(|\psi\rangle\) that you get from (say) a polynomial-sized circuit resembles a fully random state in various statistical and computational respects, both proven and conjectured.  As a result, if you measure the qubits in the computational basis, you get a randomized state on \(n\) bits that resembles a uniformly random point in \(\Delta^{2^n-1}\).</p>



<p>If you choose \(d\) probabilities, and if each one is an independent exponential random variable, then the law of large numbers says that the sum (which you use for rescaling) is close to \(d\) when \(d\) is large. When \(d\) is really big like \(2^{53}\), a histogram of the probabilities of the bit strings of a supremacy experiment looks like an exponential curve \(f(p) \propto e^{-pd}\).  In a sense, the statistical distribution of the bit strings is almost the same almost every time, independent of which random quantum circuit you choose to generate them.  The catch is that the position of any given bit string does depend on the circuit and is highly scrambled.  I picture it in my mind like this: </p>



<figure class="wp-block-image"><img alt="" src="https://www.scottaaronson.com/f5-samples.png"/></figure>



<p>

It is thought to be computationally intractable to calculate where each bit string lands on this exponential curve, or even where just one of them does.  (The exponential curve is attenuated by noise in the actual experiment, but it’s the same principle.)  That is one reason that random quantum circuits are supreme.</p>



<p/></div>
    </content>
    <updated>2019-11-26T20:50:50Z</updated>
    <published>2019-11-26T20:50:50Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-11-27T22:08:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/26/postdoctoral-fellowships-at-umass-amherst-apply-by-december-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/11/26/postdoctoral-fellowships-at-umass-amherst-apply-by-december-1-2019/" rel="alternate" type="text/html"/>
    <title>Postdoctoral Fellowships at UMass Amherst (apply by December 1, 2019)</title>
    <summary>The UMass Amherst TRIPODS Institute for Theoretical Foundations of Data Science invites applications for postdoctoral fellowships. Research areas of interest include: algorithms and computational models for processing massive data sets; statistical performance and data acquisition in interactive data collection; model robustness, approximate inference and uncertainty quantification. Website: https://www.cics.umass.edu/job/postdoctoral-research-associate-tripods-institute-theoretical-foundations-data-science Email: mcgregor@cs.umass.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The UMass Amherst TRIPODS Institute for Theoretical Foundations of Data Science invites applications for postdoctoral fellowships. Research areas of interest include: algorithms and computational models for processing massive data sets; statistical performance and data acquisition in interactive data collection; model robustness, approximate inference and uncertainty quantification.</p>
<p>Website: <a href="https://www.cics.umass.edu/job/postdoctoral-research-associate-tripods-institute-theoretical-foundations-data-science">https://www.cics.umass.edu/job/postdoctoral-research-associate-tripods-institute-theoretical-foundations-data-science</a><br/>
Email: mcgregor@cs.umass.edu</p></div>
    </content>
    <updated>2019-11-26T16:48:00Z</updated>
    <published>2019-11-26T16:48:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-30T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/11/26/quantum-computer-science-school-2020/</id>
    <link href="https://cstheory-events.org/2019/11/26/quantum-computer-science-school-2020/" rel="alternate" type="text/html"/>
    <title>Quantum Computer Science School 2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">January 14-16, 2020 University of Technology Sydney, Australia http://conference.iiis.tsinghua.edu.cn/QCSS2020/ The Quantum Computer Science School 2020 will consist of three days of lectures and academic activities, targeting at senior undergraduates and graduate students in Australian and Asian universities. The lecturers are Mingsheng Ying, Luming Duan, Michael Bremner, Troy Lee, and Ran Duan, and the topics include … <a class="more-link" href="https://cstheory-events.org/2019/11/26/quantum-computer-science-school-2020/">Continue reading <span class="screen-reader-text">Quantum Computer Science School 2020</span></a></div>
    </summary>
    <updated>2019-11-26T01:06:34Z</updated>
    <published>2019-11-26T01:06:34Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-11-30T14:21:15Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/11/25/reconfiguring-3-colorings</id>
    <link href="https://11011110.github.io/blog/2019/11/25/reconfiguring-3-colorings.html" rel="alternate" type="text/html"/>
    <title>Reconfiguring 3-colorings</title>
    <summary>I talked briefly about how to get from one 3-coloring of a grid graph to another by changing one vertex color at a time, in a recent blog post about analogous problems for origami folding patterns. It turns out that this problem of reconfiguring 3-colorings has been treated in much greater generality in the 2007 doctoral dissertation of Luis Cereceda, “Mixing graph colourings”, as I discovered when I read the dissertation in the process of writing a new Wikipedia article on Cereceda’s conjecture, the conjecture that the space of -colorings of -degenerate graphs (under moves that change the color of one vertex at a time) has at most quadratic diameter. Here’s an illustration from the new article showing the space of 3-colorings of a path graph:</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I talked briefly about how to get from one 3-coloring of a grid graph to another by changing one vertex color at a time, in <a href="https://11011110.github.io/blog/2019/10/16/from-one-fold.html">a recent blog post about analogous problems for origami folding patterns</a>. It turns out that this problem of reconfiguring 3-colorings has been treated in much greater generality in the 2007 doctoral dissertation of Luis Cereceda, “<a href="http://etheses.lse.ac.uk/131/">Mixing graph colourings</a>”, as I discovered when I read the dissertation in the process of writing a new Wikipedia article on <a href="https://en.wikipedia.org/wiki/Cereceda%27s_conjecture">Cereceda’s conjecture</a>, the conjecture that the space of -colorings of -degenerate graphs (under moves that change the color of one vertex at a time) has at most quadratic diameter. Here’s an illustration from the new article showing the space of 3-colorings of a path graph:</p>

<p style="text-align: center;"><img alt="The space of 3-colorings of a path graph" src="https://11011110.github.io/blog/assets/2019/path-3-colorings.svg"/></p>

<p>Cereceda proved that the following properties of a graph are equivalent:</p>

<ul>
  <li>It has a connected space of 3-colorings.</li>
  <li>Every 3-coloring has a height function.</li>
  <li>It is bipartite and is not “pinchable” to a 6-cycle</li>
</ul>

<p>If we consider the three colors to be used for 3-colorings to be the numbers 0, 1, and 2 (mod 3) then a height function can be defined as an assignment of integers (not mod 3) to the vertices, such that taking them mod 3 produces the given coloring, and such that adjacent vertices have heights that differ by . If it exists for a given coloring, it can be constructed easily by choosing the height of one vertex arbitrarily and then using the  requirement for adjacent heights to propagate this choice to neighbors in the graph until every vertex has a height. What can go wrong is that this propagation somehow leaves two adjacent vertices with heights that are too far apart. For instance, if the colors around a 6-cycle have the cyclic order 0–1–2–0–1–2 then you will come back to the start six units higher than you started.</p>

<p>Continuing to explain the terms in Cereceda’s equivalence, I think the 6-cycle pinchability condition is most easily explained in terms of <a href="https://en.wikipedia.org/wiki/Graph_homomorphism">graph homomorphisms</a>, maps from one graph to another that preserve adjacency. Consider a 6-cycle, 3-colored 0–1–2–0–1–2. It is also a bipartite graph, 2-colored black–white–black–white–black–white, with one vertex for each combination of colors in the 3-coloring and the 2-coloring. If you have any 3-coloring of a bipartite graph, you can map it onto the 6-cycle so that both the 3-coloring and the bipartition are preserved. If your 3-coloring does not have a height function (that is, there is a cycle that, when you propagate heights around it, comes back to its start at a different height) then the winding number of this cycle, as it maps around the 6-cycle, will tell you the difference in heights from start to end (divided by six). So a coloring without a height function gives you a homomorphism to the 6-cycle in which some cycle has nonzero winding number. On the other hand, if you have such a homomorphism, you can lift the colors from the 6-cycle back to the starting graph to give you a coloring without a height function. Cereceda’s “pinching” operations are a special type of homomorphism in which vertices at distance two from each other are repeatedly merged. Not every homomorphism comes from pinching in this way (with pinches you can only map a 12-cycle once around a 6-cycle, not twice around, for instance) so the proof that pinching homomorphisms exist for graphs without height functions is a little more complicated.</p>

<p>It’s possible to construct in polynomial time the shortest sequence of color changes to go from one 3-coloring to another, whenever the space of colorings is connected. Cereceda almost finds this algorithm but misses a small trick and ends up stating its existence only as a conjecture. As I explained in my previous post, if you choose the correct offset for the height functions of the two colorings, you can find this shortest sequence by greedily changing the color at a vertex of one color where the current height is farthest from the goal height. The trick that Cereceda misses is that if you don’t know the correct offset between the two height functions, you can just try them all (or more quickly use a median algorithm to find the optimal offset and the distance between colorings in linear time).</p>

<p>Unfortunately, as Cereceda proved, testing pinchability to a 6-cycle is -complete and therefore testing the connectivity of the space of 3-colorings (and the applicability of the height-based shortest reconfiguration algorithm) is -complete. For instance, I’m pretty sure the 10-vertex Möbius ladder shown below has height functions for all its 3-colorings, but I don’t know of an elegant way to prove this, and -completeness suggests that not all graphs have elegant proofs for this. This graph has lots of 6-cycles, for instance formed by any diagonal and the path around the outer cycle connecting its two endpoints. But trying to map the whole graph to a 6-cycle by folding it flat across a diagonal doesn’t work because it would map some edges to non-edges, and nothing else seems to work either.</p>

<p style="text-align: center;"><img alt="A 10-vertex M&#xF6;bius ladder" src="https://11011110.github.io/blog/assets/2019/M%C3%B6bius-ladder-10.svg"/></p>

<p>To save this post from being content-free, I want to describe a more easily recognized family of graphs that meets Cereceda’s criterion, and does have height functions for all its 3-colorings (so we can quickly find the shortest reconfiguration sequences in these graphs). It is the class of graphs in which the <a href="https://en.wikipedia.org/wiki/Cycle_space">cycle space</a> is generated by the 4-cycles, or in other words the graphs in which there exists a <a href="https://en.wikipedia.org/wiki/Cycle_basis">cycle basis</a> consisting only of 4-cycles. These graphs can be recognized simply by doing some mod-2 linear algebra to test whether the space generated by the 4-cycles has the same dimension as the cycle space. For instance, the Möbius ladder above is not in this class because the dimension of its cycle space (the number of edges beyond the ones in a spanning forest) is six but its number of 4-cycles (the cycles between two consecutive diagonals) is only five. Despite this example, the graphs generated by 4-cycles include a lot of natural classes of graphs that we might want to reconfigure 3-colorings of:</p>

<ul>
  <li>Trees</li>
  <li>Rectangular grid graphs and higher-dimensional hyperrectangular grid graphs</li>
  <li><a href="https://en.wikipedia.org/wiki/Squaregraph">Squaregraphs</a></li>
  <li>The dual graphs of simple line arrangements</li>
  <li>Complete bipartite graphs</li>
  <li><a href="https://en.wikipedia.org/wiki/Chordal_bipartite_graph">Chordal bipartite graphs</a></li>
  <li>The <a href="https://en.wikipedia.org/wiki/Cartesian_product_of_graphs">Cartesian products</a> of other graphs generated by 4-cycles</li>
</ul>

<p>To prove that Cartesian products preserve the property of being generated by 4-cycles, consider any cycle  in the product graph; we want to represent it as a mod-2 sum of 4-cycles. If two consecutive edges of  come from the two different factors, then the product of these edges is a 4-cycle, and adding this 4-cycle to  swaps the order of the two edges within . By repeated swaps we can segregate all the edges from the two factors from each other, changing  into two cycles, one from each factor, that meet at a common vertex. Then we can represent the two cycles separately within the two factors.</p>

<p>It’s straightforward to see that when 4-cycles generate the -homology of a graph, then height functions always exist, because the height difference around any cycle is the sum of the height differences of the 4-cycles generating it, which are all zero. But here by using the binary cycle space we’re looking at the -homology, which might be a different thing. There could be graphs whose -homology is generated by 4-cycles but whose -homology isn’t. For instance I think that adding one bipartite edge to the 10-vertex Möbius ladder produces an example of this phenomenon. So we need a proof that -homology is good enough. Or in simpler terms, when the cycle space is generated by 4-cycles, all 3-colorings have height functions.</p>

<p>We’ll prove this by contradiction, so let’s suppose that we have the smallest possible counterexample. That is, we have a graph , a 3-coloring of , and a cycle  in , such that the cycle space of  is generated by 4-cycles but going around  using the  rule to propagate heights produces a different height than you started with. By “smallest” I mean first, that  is as short as possible, second, that for that length of  the rest of  has as few vertices as possible, and third, that the number of 4-cycles in the representation of  is as small as possible.</p>

<p>Then  cannot have any chords, because if it did then a chord plus one of the two segments of  connecting the chord endpoints would form a shorter cycle with the same inconsistent heights. Because we are assuming that  is generated by 4-cycles, consider any set  of 4-cycles whose sum is . That is, each edge of  appears an odd number of times in cycles of , and each other edge of  appears an even number of times (possibly zero). Each 4-cycle in  must have a coloring of the form ––– or ––– for some colors , , and . If at least one of the two -colored vertices does not belong to , form a smaller graph  by merging these two -colored vertices. This merger does not change  or the coloring, so  stays a bad cycle. And it doesn’t change the property of  that it is generated by 4-cycles, because any cycle in the merged graph can be lifted to a cycle in the unmerged graph (possibly passing through –– in the merged 4-cycle and possibly not simple), represented by 4-cycles in  itself, and then merged back down to get a representation in the smaller graph. But because we’re assuming  was the smallest possible counterexample, this shrinkage can’t happen, so all of the 4-cycles in  have both -vertices in .</p>

<p>Now if we have a 4-cycle with two -vertices in , at least one of the other two vertices does not belong to , because  must be longer than four edges (else it would not have a bad coloring) and we’ve already ruled out the possibility that  has the chords that would be needed to make a 4-cycle using vertices only from . Let  be this vertex outside of , so we have a path –– connecting the two -vertices in . One of two things can happen: First, the two -vertices might only be two units apart in . But then, replacing the two-edge path between them by the path through  produces a new bad cycle of the same length, in the same graph , representable by a smaller set of 4-cycles. This contradicts our choice of  and  as being the smallest possible counterexample. Second, when the two -vertices are farther apart in , there are two cycles formed by path –– and by one of the two paths in  connecting the same two -vertices. Both of these cycles are shorter than , and at least one of them continues to have a bad coloring. So again we have found a smaller counterexample and a contradiction.</p>

<p>This case analysis leading in all cases to a contradiction shows that a smallest counterexample cannot exist, and therefore that all graphs generated by 4-cycles have height functions for all of their 3-colorings.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/103202898221719422">Discuss on Mastodon</a>; see also <a href="https://11011110.github.io/blog/2010/09/12/rapid-mixing-for.html">an earlier post on reconfiguring 3-colorings of cycles using stronger moves</a>; edited 2019-11-27 to correct rectraction vs pinchability in Cereceda’s characterization)</p></div>
    </content>
    <updated>2019-11-25T22:24:00Z</updated>
    <published>2019-11-25T22:24:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-11-28T00:29:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/25/postdoctoral-at-yale-university-apply-by-december-20-2019/</id>
    <link href="https://cstheory-jobs.org/2019/11/25/postdoctoral-at-yale-university-apply-by-december-20-2019/" rel="alternate" type="text/html"/>
    <title>POSTDOCTORAL at YALE UNIVERSITY (apply by December 20, 2019)</title>
    <summary>Applications are solicited for multiple postdoctoral positions at Yale in Algorithms, Optimization, Sampling, and Fairness. The positions are expected to start in Fall 2020 but can start earlier. Applicants should have their CV, research statement, and three letters emailed directly to nisheeth.vishnoi@gmail.com. Applications completed by December 20, 2019, will receive full consideration. Website: http://cs.yale.edu/homes/vishnoi/Positions.html Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are solicited for multiple postdoctoral positions at Yale in Algorithms, Optimization, Sampling, and Fairness. The positions are expected to start in Fall 2020 but can start earlier. Applicants should have their CV, research statement, and three letters emailed directly to nisheeth.vishnoi@gmail.com. Applications completed by December 20, 2019, will receive full consideration.</p>
<p>Website: <a href="http://cs.yale.edu/homes/vishnoi/Positions.html">http://cs.yale.edu/homes/vishnoi/Positions.html</a><br/>
Email: nisheeth.vishnoi@gmail.com</p></div>
    </content>
    <updated>2019-11-25T15:05:09Z</updated>
    <published>2019-11-25T15:05:09Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-30T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1539</id>
    <link href="https://theorydish.blog/2019/11/24/motwani-postdoctoral-fellowship-2/" rel="alternate" type="text/html"/>
    <title>Motwani Postdoctoral Fellowship</title>
    <summary>The theory group at Stanford invites applications for the Motwani postdoctoral fellowship in theoretical computer science. Information and application instructions below. Applications will be accepted until the positions are filled, but review of applicants will begin after Dec 15. Website: https://academicjobsonline.org/ajo/jobs/15578 Email: theory.stanford@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The theory group at Stanford invites applications for the Motwani postdoctoral fellowship in theoretical computer science. Information and application instructions below.</p>
<p>Applications will be accepted until the positions are filled, but review of applicants will begin after Dec 15.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/15578">https://academicjobsonline.org/ajo/jobs/15578</a><br/>
Email: theory.stanford@gmail.com</p></div>
    </content>
    <updated>2019-11-25T00:02:30Z</updated>
    <published>2019-11-25T00:02:30Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-11-30T14:21:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/25/postdoctoral-position-in-computational-social-choice-at-university-of-toronto-apply-by-december-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/11/25/postdoctoral-position-in-computational-social-choice-at-university-of-toronto-apply-by-december-15-2019/" rel="alternate" type="text/html"/>
    <title>Postdoctoral position in Computational Social Choice at University of Toronto (apply by December 15, 2019)</title>
    <summary>Postdoctoral position beginning in Fall 2020; fellow will work with Prof. Nisarg Shah on topics such as (but not limited to): computational social choice, fairness and incentives in machine learning, algorithmic game theory, and mechanism design. Applicants should have (prior to starting) a PhD in computer science, economics, operations research, or a related field. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Postdoctoral position beginning in Fall 2020; fellow will work with Prof. Nisarg Shah on topics such as (but not limited to): computational social choice, fairness and incentives in machine learning, algorithmic game theory, and mechanism design. Applicants should have (prior to starting) a PhD in computer science, economics, operations research, or a related field.</p>
<p>Website: <a href="https://www.cs.toronto.edu/theory/positions.html">https://www.cs.toronto.edu/theory/positions.html</a><br/>
Email: nisarg@cs.toronto.edu</p></div>
    </content>
    <updated>2019-11-25T00:01:22Z</updated>
    <published>2019-11-25T00:01:22Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-30T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/24/postdocs-at-university-of-toronto-apply-by-december-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/11/24/postdocs-at-university-of-toronto-apply-by-december-15-2019/" rel="alternate" type="text/html"/>
    <title>Postdocs at University of Toronto (apply by December 15, 2019)</title>
    <summary>The theory group at the University of Toronto anticipates up to three postdoctoral positions beginning September 2020. We seek candidates from all areas of theoretical computer science including algorithms, complexity theory, cryptography, differential privacy, distributed computing, graph theory, quantum computing, and theoretical aspects of machine learning. Website: https://www.cs.toronto.edu/theory/positions.html Email: hyuen@cs.toronto.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The theory group at the University of Toronto anticipates up to three postdoctoral positions beginning September 2020. We seek candidates from all areas of theoretical computer science including algorithms, complexity theory, cryptography, differential privacy, distributed computing, graph theory, quantum computing, and theoretical aspects of machine learning.</p>
<p>Website: <a href="https://www.cs.toronto.edu/theory/positions.html">https://www.cs.toronto.edu/theory/positions.html</a><br/>
Email: hyuen@cs.toronto.edu</p></div>
    </content>
    <updated>2019-11-24T23:59:18Z</updated>
    <published>2019-11-24T23:59:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-30T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/169</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/169" rel="alternate" type="text/html"/>
    <title>TR19-169 |  On Exponential-Time Hypotheses, Derandomization, and Circuit Lower Bounds | 

	Roei Tell, 

	Lijie Chen, 

	Ron Rothblum, 

	Eylon Yogev</title>
    <summary>The Exponential-Time Hypothesis ($ETH$) is a strengthening of the $\mathcal{P} \neq \mathcal{NP}$ conjecture, stating that $3\text{-}SAT$ on $n$ variables cannot be solved in time $2^{\epsilon\cdot n}$, for some $\epsilon&gt;0$. In recent years, analogous hypotheses that are ``exponentially-strong'' forms of other classical complexity conjectures (such as $\mathcal{NP}\not\subseteq\mathcal{BPP}$ or $co\text{-}\mathcal{NP}\not\subseteq \mathcal{NP}$) have also been considered. These Exponential-Time Hypotheses have been widely influential across different areas of complexity theory. However, their connections to *derandomization and circuit lower bounds* have yet to be systematically studied. Such study is indeed the focus of the current work, and we prove a sequence of results demonstrating that *the connections between exponential-time hypotheses, derandomization, and circuit lower bounds are remarkably strong*.

First, we show that if $3\text{-}SAT$ (or even $TQBF$) cannot be solved by probabilistic algorithms that run in time $2^{n/\mathrm{polylog}(n)}$, then $\mathcal{BPP}$ can be deterministically simulated ``on average case'' in (nearly-)polynomial-time (i.e., in time $n^{\mathrm{polyloglog}(n)}$). This result addresses a long-standing lacuna in uniform ``hardness-to-randomness'' results, which did not previously extend to such parameter settings. Moreover, we extend this result to support an ``almost-always'' derandomization conclusion from an ``almost-always'' lower bound hypothesis.

Secondly, we show that *disproving* certain exponential-time hypotheses requires proving breakthrough circuit lower bounds. In particular, if $CircuitSAT$ for circuits over $n$ bits of size $\mathrm{poly}(n)$ can be solved by *probabilistic algorithms* in time $2^{n/\mathrm{polylog}(n)}$, then $\mathcal{BPE}$ does not have circuits of quasilinear size. The main novel feature of this result is that we only assume the existence of a *randomized* circuit-analysis algorithm, whereas previous similar results crucially relied on the hypothesis that the circuit-analysis algorithm does not use randomness.

Thirdly, we show that a very weak exponential-time hypothesis is closely-related to the classical question of whether derandomization and circuit lower bounds are *equivalent*. Specifically, we show two-way implications between the hypothesis that the foregoing equivalence holds and the hypothesis that $\mathcal{E}$ cannot be decided by ``small'' circuits that are *uniformly generated* by relatively-efficient non-deterministic machines. This highlights a sufficient-and-necessary path for progress towards proving that derandomization and circuit lower bounds are indeed equivalent.</summary>
    <updated>2019-11-24T11:50:42Z</updated>
    <published>2019-11-24T11:50:42Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-30T14:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/168</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/168" rel="alternate" type="text/html"/>
    <title>TR19-168 |  Beyond Natural Proofs: Hardness Magnification and Locality | 

	Ján Pich, 

	Lijie Chen, 

	Shuichi Hirahara, 

	Igor Carboni Oliveira, 

	Ninad Rajgopal, 

	Rahul Santhanam</title>
    <summary>Hardness magnification reduces major complexity separations (such as $EXP \not\subseteq NC^1$) to proving lower bounds for some natural problem $Q$ against weak circuit models. Several recent works [OS18, MMW19, CT19, OPS19, CMMW19, Oli19, CJW19a] have established results of this form. In the most intriguing cases, the required lower bound is known for problems that appear to be significantly easier than $Q$, while $Q$ itself is susceptible to lower bounds but these are not yet sufficient for magnification. 

In this work, we provide more examples of this phenomenon, and investigate the prospects of proving new lower bounds using this approach. In particular, we consider the following essential questions associated with the hardness magnification program:

– Does hardness magnification avoid the natural proofs barrier of Razborov and Rudich [RR97]? 
– Can we adapt known lower bound techniques to establish the desired lower bound for $Q$?

We establish that some instantiations of hardness magnification overcome the natural proofs barrier in the following sense: slightly superlinear-size circuit lower bounds for certain versions of the minimum circuit size problem MCSP imply the non-existence of natural proofs. As a corollary of our result, we show that certain magnification theorems not only imply strong worst-case circuit lower bounds but also rule out the existence of efficient learning algorithms. 

Hardness magnification might sidestep natural proofs, but we identify a source of difficulty when trying to adapt existing lower bound techniques to prove strong lower bounds via magnification. This is captured by a locality barrier: existing magnification theorems unconditionally show that the problems $Q$ considered above admit highly efficient circuits extended with small fan-in oracle gates, while lower bound techniques against weak circuit models quite often easily extend to circuits containing such oracles. This explains why direct adaptations of certain lower bounds are unlikely to yield strong complexity separations via hardness magnification.</summary>
    <updated>2019-11-24T05:59:30Z</updated>
    <published>2019-11-24T05:59:30Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-30T14:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7582</id>
    <link href="https://windowsontheory.org/2019/11/23/harvard-opportunity-lecturing-advising-position/" rel="alternate" type="text/html"/>
    <title>Harvard opportunity: lecturing / advising position</title>
    <summary>Harvard Computer Science is seeking a Lecturer/Assistant Director of Undergraduate Studies. A great candidate would be someone passionate about teaching and mentoring and excited to build a diverse and inclusive Undergraduate Computer Science community at Harvard. The position requires a Ph.D and is open to all areas of computer science and related fields, but of course […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Harvard Computer Science is seeking a Lecturer/Assistant Director of Undergraduate Studies. A great candidate would be someone passionate about teaching and mentoring and excited to build a diverse and inclusive Undergraduate Computer Science community at Harvard. The position requires a Ph.D and is open to all areas of computer science and related fields, but of course personally I would love to have a theorist fill this role.</p>



<p>Key responsibilities are:</p>



<p>* Teach (or co-teach) one undergraduate Computer Science course per semester.</p>



<p>* Join and help lead the Computer Science Undergraduate Advising team (which includes mentoring and advising undergraduate students and developing materials, initiatives, and events to foster a welcoming and inclusive Harvard Computer Science community.)</p>



<p>The job posting with all details is at <a href="https://tiny.cc/harvardadus">https://tiny.cc/harvardadus</a> <br/></p>



<p>Any questions about this position, feel free to contact me or Steve Chong  (the co directors of undergraduate studies for CS at Harvard) at cs-dus at seas.harvard.edu <br/></p></div>
    </content>
    <updated>2019-11-23T17:56:24Z</updated>
    <published>2019-11-23T17:56:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-11-30T14:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=680</id>
    <link href="https://emanueleviola.wordpress.com/2019/11/21/manucomic-1/" rel="alternate" type="text/html"/>
    <title>manucomic #1</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="wp-block-image size-large"><img alt="" class="wp-image-681" src="https://emanueleviola.files.wordpress.com/2019/11/manucomic1.jpg?w=746"/></figure></div>
    </content>
    <updated>2019-11-21T16:54:34Z</updated>
    <published>2019-11-21T16:54:34Z</published>
    <category term="Uncategorized"/>
    <category term="manucomic"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2019-11-30T14:21:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/167</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/167" rel="alternate" type="text/html"/>
    <title>TR19-167 |  UTIME Easy-witness Lemma &amp;amp; Some Consequences | 

	Anant Dhayal, 

	Russell Impagliazzo</title>
    <summary>We prove an easy-witness lemma ($\ewl$) for unambiguous non-deterministic verfiers. We show that if $\utime(t)\subset\mathcal{C}$, then for every $L\in\utime(t)$, for every $\utime(t)$ verifier $V$ for $L$, and for every $x\in L$, there is a certificate $y$ satisfing $V(x,y)=1$, that can be encoded as a truth-table of a $\mathcal{C}$ circuit. Our technique is simple compared to the $\ntime$ $\ewl$s \cite{IKW02,Wil-ES13,MW18}, and yields fine-grained results in terms of the time and size parameters. It also works for all {\it typical} non-uniform circuit classes without any additional machinery. Using this $\ewl$ we prove a Karp-Lipton \cite{KL80} style theorem ($\klt$) for $\uexp$. We show that $\uexp\subset\size(poly)\implies\uexp=\ma$. We also prove similar $\ewl$ and $\klt$ for $\uexp\cap\couexp$ and $\fewexp$.

Circuit lower bound techniques that entail natural properties of Razborov and Rudich \cite{RR97} are called natural, and are known to contradict widely believed cryptographic assumptions in the course of proving strong lower bounds. Thus attempts have been made to understand un-natural techniques. Natural properties satisfy three conditions: usefulness, constructiveness, and largeness. Usefulness is unavoidable in any lower-bound technique. In \cite{Wil-NP16,Ig13} it was shown that obtaining $\nexp$ lower bounds is equivalent to obtaining $\pt$-constructive (with $\log n$ advice) properties.

In this paper we consider properties that avoid largeness. We introduce a new notion called unique properties, which is opposite to natural properties in the sense of largeness. A unique property contains exactly one element of each input length (that is a power of 2). We show that $\pt$-constructivity and uniqueness (opposite of largeness) both are unavoidable for certain lower bounds. We prove, $\uexp\cap\couexp\not\subset\mathcal{C}$ if and only if there is a $\pt$-constructive unique property against $\mathcal{C}$. We also establish equivalences between lower bounds against $\uexp$ (with and without advice), and the existence of different restrictions of $\pt$-constructive unique properties that use advice. 

The ``derandomization (of $\bpp$) from uniform/non-uniform lower bounds for $\Gamma$'' type of results are known for $\Gamma=\expo,\nexp,\nexp\cap\conexp,\rexp$ \cite{NW94,BFNW93,IW01,IKW02,Wil-NP16}. Using the above equivalences we obtain a super-set of these results that also includes the classes $\uexp,\uexp\cap\couexp,\zpexp$.  

One important application of the $\nexp$ $\ewl$ and $\klt$ is the connection between fast ($\sat$ and learning) algorithms and $\nexp$ lower bounds \cite{Wil-ES13,FK09,Ig13}. Using our $\utime$ $\ewl$ and $\klt$ we derive connections between fast unambiguous algorithms and $\utime$ lower bounds. Finally we show results that generalize the lower bound frameworks -- that work only for unrestricted Boolean circuits -- such that they work for any restricted typical circuit class. This will help us to get lower bounds against any typical circuit class from fast algorithms that work for that particular class (and not for the super-class of unrestricted Boolean circuits).</summary>
    <updated>2019-11-21T01:41:00Z</updated>
    <published>2019-11-21T01:41:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-30T14:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/166</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/166" rel="alternate" type="text/html"/>
    <title>TR19-166 |  Top-down induction of decision trees: rigorous guarantees and inherent limitations | 

	Guy Blanc, 

	Jane Lange, 

	Li-Yang Tan</title>
    <summary>Consider the following heuristic for building a decision tree for a function $f : \{0,1\}^n \to \{\pm 1\}$.  Place the most influential variable $x_i$ of $f$ at the root, and recurse on the subfunctions $f_{x_i=0}$ and $f_{x_i=1}$ on the left and right subtrees respectively; terminate once the tree is an $\varepsilon$-approximation of $f$.   We analyze the quality of this heuristic, obtaining near-matching upper and lower bounds:  

$\circ$ Upper bound: For every $f$ with decision tree size $s$ and every $\varepsilon \in (0,\frac1{2})$, this heuristic builds a decision tree of size at most $s^{O(\log(s/\varepsilon)\log(1/\varepsilon))}$. 

$\circ$ Lower bound: For every $\varepsilon \in (0,\frac1{2})$ and $s \le 2^{\tilde{O}(\sqrt{n})}$, there is an $f$ with decision tree size $s$ such that this heuristic builds a decision tree of size $s^{\tilde{\Omega}(\log s)}$. 

We also obtain upper and lower bounds for monotone functions: $s^{O(\sqrt{\log s}/\varepsilon)}$ and $s^{\tilde{\Omega}(\sqrt[4]{\log s }
)}$ respectively.  The lower bound disproves conjectures of Fiat and Pechyony (2004) and Lee (2009).

Our upper bounds yield new algorithms for properly learning decision trees under the uniform distribution.  We show that these algorithms---which are motivated by widely employed and empirically successful top-down decision tree learning heuristics such as ID3, C4.5, and CART---achieve provable guarantees that compare favorably with those of the current fastest algorithm (Ehrenfeucht and Haussler, 1989), and even have certain qualitative advantages. Our lower bounds shed new light on the limitations of these heuristics. 

Finally, we revisit the classic work of Ehrenfeucht and Haussler.  We extend it to give the first uniform-distribution proper learning algorithm that achieves polynomial sample and memory complexity, while matching its state-of-the-art quasipolynomial runtime.</summary>
    <updated>2019-11-20T19:06:43Z</updated>
    <published>2019-11-20T19:06:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-30T14:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3445</id>
    <link href="https://agtb.wordpress.com/2019/11/20/test-of-time-award-call-for-nominations/" rel="alternate" type="text/html"/>
    <title>Test of time award: call for nominations</title>
    <summary>Please nominate for the SIGecom Test of Time Award. The SIGecom Test of Time Award recognizes the author or authors of an influential paper or series of papers published between ten and twenty-five years ago that has significantly impacted research or applications exemplifying the interplay of economics and computation. To be eligible, a paper or […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="font-weight: 400;">Please nominate for the SIGecom Test of Time Award.</p>
<p style="font-weight: 400;">The SIGecom Test of Time Award recognizes the author or authors of an influential paper or series of papers published between ten and twenty-five years ago that has significantly impacted research or applications exemplifying the interplay of economics and computation.</p>
<p style="font-weight: 400;">To be eligible, a paper or series of papers must be on a topic in the intersection of economics and computation, and must have been first published, in preliminary or final form, in an archival journal or conference proceedings no less than ten years and no more than twenty-five years before the year the award is conferred. Papers for which all authors are deceased at the time the Award Committee makes its decision are not eligible for the award.</p>
<p style="font-weight: 400;">The 2020 SIGecom Test of Time Award will be given for papers published no earlier than 1995 and no later than 2010. <strong>Nominations are due by February 29th, 2020</strong>, and must be made by email to the Award Committee with “2020 ACM SIGecom Test of Time Award” in the subject.</p>
<p style="font-weight: 400;">See details at <a href="https://www.sigecom.org/awardt.html">https://www.sigecom.org/awardt.html</a></p>
<p> </p>
<p style="font-weight: 400;">The 2020 Test of Time Award Committee</p>
<p style="font-weight: 400;">Paul Milgrom, Stanford University</p>
<p style="font-weight: 400;">Noam Nisan, The Hebrew University of Jerusalem</p>
<p style="font-weight: 400;">Éva Tardos (chair), Cornell University</p></div>
    </content>
    <updated>2019-11-20T07:55:55Z</updated>
    <published>2019-11-20T07:55:55Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>algorithmicgametheory</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-11-30T14:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16399</id>
    <link href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/" rel="alternate" type="text/html"/>
    <title>A Clever Way To Find Compiler Bugs</title>
    <summary>Your comments are valuable, we thank you. source Xuejun Yang is a Senior Staff Engineer at FutureWei Technologies. He is the DFA on the 2011 paper, “Finding and Understanding Bugs in C Compilers.” Today Ken and I discuss a clever idea from that paper. The paper was brought to our attention just now in a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Your comments are valuable, we thank you.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/profile_image174/" rel="attachment wp-att-16401"><img alt="" class="alignright  wp-image-16401" src="https://rjlipton.files.wordpress.com/2019/11/profile_image174.jpg?w=220" width="220"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.flux.utah.edu/profile/jxyang">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Xuejun Yang is a Senior Staff Engineer at FutureWei Technologies. He is the DFA on the 2011 <a href="http://www.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf">paper</a>, “Finding and Understanding Bugs in C Compilers.”</p>
<p>
Today Ken and I discuss a clever idea from that paper.<br/>
<span id="more-16399"/></p>
<p>
The paper was brought to our attention just now in a meaty <a href="https://rjlipton.wordpress.com/2019/10/21/a-polemical-overreach/#comment-106276">comment</a> by Paul D. We thank him for it—the topic interests both of us. We don’t think Paul D. means to be anonymous, but in keeping with that we’ll give just a cryptic hint to his identity: The saying “a man on the make” is widely known, but for more than the millennium he has been the unique person in the world to whom it applies literally.  <b>Update 11/20</b>: Turns out we (I, Ken) were wrong about the identity, see <a href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/#comment-106348">this</a>.</p>
<p>
Yang was made unique by being listed out of alphabetical order on the paper. This is notable because the most common practice in our field is to list alphabetically irrespective of prominence. Hence we’ve invented the term ‘DFA’ for “Designated” or “Distinguished” First Author. The other authors are Yang Chen, Eric Eide, and John Regehr, all from the University of Utah. </p>
<p>
</p><p/><h2> The Topic </h2><p/>
<p/><p>
Paul D.’s comment notes that there was evidence that verification methods could improve compiler correctness. By <i>compiler</i> we mean the program that transforms high level code into machine code. These programs are used countless times every day and their correctness is clearly very important. </p>
<p>
Their correctness is tricky for several reasons. The main one is that almost all compilers try to optimize code. That is when they transform code into instructions they try to rewrite or rearrange the instructions to yield better performance. Compilers have been doing this forever. The trouble is that changing instructions to increase performance is dangerous. The changes must not affect the values that are computed. If they are not done carefully they can actually make the answers faster, but incorrect. This is the reason correctness is tricky.</p>
<p>
Formal verification requires a lot of effort. The highest effort should go into mission-critical software. But compilers are mission-critical <em>already</em>, unless we know mission-critical software won’t be compiled on a particular one. Hence it is notable when formal verification makes a compiler more reliable. </p>
<p>
</p><p/><h2> The Paper </h2><p/>
<p/><p>
The idea in the paper Paul referenced is quite elegant. They built a program called Csmith. It operates as follows: </p>
<blockquote><p><b> </b> <em> Suppose that <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X}"/> is a compiler they wish to test. Then generate various legal C programs <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{P}"/>. For each of these let <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> be the answer that <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X(P)}"/> yields. Here <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X(P)}"/> is the compiled program. Then check whether <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is correct. </em>
</p></blockquote>
<p/><p>
For example:  </p>
<pre>int foo (void) { 
    signed char x = 1; 
    unsigned char y = 255; 
    return x &gt; y; 
} 
</pre>
<p>
Some compilers returned <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, but the correct answer is <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. There are further examples in a 2012 companion <a href="https://www.cs.utah.edu/~regehr/papers/pldi12-preprint.pdf">paper</a> and these <a href="https://www.flux.utah.edu/download?uid=115&amp;slides=1&amp;type=pptx">slides</a> from an earlier version. The Csmith <a href="https://embed.cs.utah.edu/csmith/">homepage</a> has long lists of compiler bugs they found. </p>
<p>
Of course if <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X(P)}"/> crashes or refuse to compile <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> then the compiler is wrong. But what happens if <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is computed. How does Csmith know if the answer is correct? This seems to be really hard. This correctness testing must be automated: the whole approach is based on allowing tons of random programs to be tested. They cannot assume that humans will be used to check the outputs.</p>
<p>
This is the clever idea of this paper. They assume that there are at least two compilers say <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> and <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/>. Then let <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> be the output of <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X(P)}"/> and let <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> be the output of <img alt="{Y(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y(P)}"/>. The key insight is: </p>
<blockquote><p>
<b>If <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is not equal to <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B}"/>, then one of the compilers is wrong</b>.
</p></blockquote>
<p>
A very neat and elegant idea. For software in general it is called <a href="https://en.wikipedia.org/wiki/Differential_testing">differential</a> <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.83.445">testing</a>. </p>
<p>
This at least alerts when there are problems with some compilers and some programs. One can use this trick to discover programs that cause at least some compilers to have problems. This is extremely valuable. It allowed Csmith to discover hundreds of errors in production compilers—errors that previously were missed.</p>
<p>
</p><p/><h2> Smart Fuzzing </h2><p/>
<p/><p>
<a href="https://en.wikipedia.org/wiki/Fuzzing">Fuzzing</a> is defined by Wikipedia as testing by “providing invalid, unexpected, or random data as inputs to a computer program.” An early historical example, Apple’s “<a href="https://en.wikipedia.org/wiki/Monkey_testing">Monkey</a>” program, worked completely randomly. To ensure that the found bugs are <em>meaningful</em> and <em>analyzable</em>, Csmith needed a deeper, structured, “intelligent” design, not just the generation of <a href="https://en.wikipedia.org/wiki/Mayhem_(advertising_character)">Mayhem</a>.</p>
<p>
For one, Csmith needed to avoid programs <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> than do not have deterministic behavior. The formal C standards itemize cases in which compilers are allowed to have arbitrary, even self-inconsistent, behavior. There are lots of them in C. A bug with dubious code could be dismissed out of hand.</p>
<p>
For another, the probability that a program <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> built haphazardly by the original Csmith version would reveal bugs was observed to peak at about 80KB source-code size, about 1,000 lines across multiple pages. Those don’t make great examples. So Csmith has its own routines to compress bug instances it has found. Simple tricks are shortening numerical expressions to use only the bug-sensitive parts. Others are lifting local variables out of blocks and bypassing pointer jumps.</p>
<p>
A third goal is that the generator should branch out to all aspects of the language—in this case, C—not just the “grungy” parts that are ripe for finding compiler bugs. The paper talks about this at length. Regehr, who was Yang’s advisor, is also a blogger. His current <a href="https://blog.regehr.org/archives/1700">post</a>, dated November 4, is titled, “Helping Generative Fuzzers Avoid Looking Only Where the Light is Good, Part 1.” We guess that “Part 2” will go even more into details.</p>
<p>
</p><p/><h2> Formal Methods as Bugscreen </h2><p/>
<p/><p>
Regarding the formally-verified CompCert compiler, Paul D. quoted from the <a href="http://www.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf">paper</a>:</p>
<blockquote><p><b> </b> <em> The striking thing about our CompCert results is that the middle-end bugs we found in all other compilers are absent. As of early 2011, the under-development version of CompCert is the only compiler we have tested for which Csmith cannot find wrong-code errors. This is not for lack of trying: we have devoted about six CPU-years to the task. The apparent unbreakability of CompCert supports a strong argument that developing compiler optimizations within a proof framework, where safety checks are explicit and machine-checked, has tangible benefits for compiler users. </em>
</p></blockquote>
<p/><p>
This August 2019 <a href="https://arxiv.org/pdf/1902.09334.pdf">paper</a> by Michaël Marcozzi, Qiyi Tang, Alastair Donaldson, and Cristian Cadar gives recent results involving Csmith and other tools. They have an interesting discussion on page 2, from which we excerpt:</p>
<blockquote><p><b> </b> <em> In our experience working in the area […], we have found compiler fuzzing to be a contentious topic. Research talks on compiler fuzzing are often followed by questions about the importance of the discovered bugs, and whether compiler fuzzers might be improved by taking inspiration from bugs encountered by users of compilers “in the wild.” Some … argue that any miscompilation bug, whether fuzzer-found or not, is a ticking bomb that should be regarded as severe, or avoided completely via formal verification (in the spirit of CompCert). </em>
</p></blockquote>
<p/><p>
They go on to say, however, that when a fully-developed compiler is used for non-critical software, the kinds of bugs typically found by fuzzing tend to have questionable importance. Their paper is titled, “A Systematic Impact Study for Fuzzer-Found Compiler Bugs.” </p>
<p>
So far they have found definite results that seem to have mixed implications. In their future-work section they note that they have evaluated the impact of bugs in compilers on the intended function of programs they compile, but not on possible security holes—which as we noted in our Cloudflare <a href="https://rjlipton.wordpress.com/2017/03/08/is-computer-security-possible/">post</a> can come from (misuse of) simple code that is completely correct. This leads us further to wonder, coming full-circle, whether formal methods might help quantify the relative importance of aspects of a language and areas of a compiler to guide more-intelligent generation of test cases.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>
The above comment is interesting, but perhaps finding obscure bugs is important. Perhaps such bugs could be used to attack systems. That is perhaps some one could use them to break into a system. Security may be compromised by any error, even an unlikely one to occur in the wild. </p>
<p>
What do you think?</p>
<p/></font></font></div>
    </content>
    <updated>2019-11-20T00:57:14Z</updated>
    <published>2019-11-20T00:57:14Z</published>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Results"/>
    <category term="bugs"/>
    <category term="compiler"/>
    <category term="errors"/>
    <category term="security"/>
    <category term="testing"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-11-30T14:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3443</id>
    <link href="https://agtb.wordpress.com/2019/11/19/a-market-for-tcs-papers/" rel="alternate" type="text/html"/>
    <title>A Market for TCS Papers??</title>
    <summary>By David Eppstein &amp; Vijay Vazirani No, not to make theoreticians rich! Besides, who will buy your papers anyway? (Quite the opposite, you will be lucky if you can convince someone to take them for free, just for sake of publicity!) What we are proposing is a market in which no money changes hands – […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>By David Eppstein &amp; Vijay Vazirani</em></p>
<p>No, not to make theoreticians rich! Besides, who will buy your papers anyway? (Quite the opposite, you will be lucky if you can convince someone to take them for free, just for sake of publicity!) What we are proposing is a market in which no money changes hands – a matching market – for matching papers to conferences.</p>
<p>First, a short preamble on how the idea emerged.</p>
<p><strong>Preamble</strong> (by Vijay):  Soon after my recent <a href="https://www.youtube.com/watch?v=MLr6Ud5qmt4&amp;t=74s"><u>Simons talk</u></a> on Matching Markets, I sent its url to Al Roth. Obviously, I wasn’t expecting a return email. However, the perfect gentleman and ultimate scholar that Al is, he did reply, and mentioned that he did not like my “definition” of matching markets and said, “I guess I would say matching markets are markets because they aggregate information that is held by the participants, which is what markets do (even if they don’t use prices to do it..).” This hit me like lightening from the sky – suddenly it crystallized the innate intuition about markets which I had formed through work on algorithmic aspects of markets! I thanked Al profusely and added, “This definitely helps in me get the right perspective on the notion!”</p>
<p>About a week ago, while updating my talk for a seminar at Columbia University, I included this beautiful insight in it and then a thought occurred: Each PC meeting involves aggregation of information from a large number of agents: PC members as well as external experts. Hence, isn’t a conference a matching market? Excitedly, I sent this question to Al. He replied, “… the conference process, matching papers to conferences, is a market and a particular conference might be a marketplace … ”</p>
<p>When I returned home, my esteemed colleague, David Eppstein, stunned me by declaring that he had thought of a market relevant to our field in which no money changes hands. I immediately knew he was thinking of the conference process. But he got to it out of the blue … and not the long process it took me!</p>
<p><strong>Back to the idea:  </strong>In the past, matching markets have brought immense efficiency and order in allocation problems in which use of money is considered repugnant, the prime examples being matching medical residents to hospitals, kidney exchange, and assignment of students of a large city to its schools.</p>
<p>At present we are faced with massive inefficiencies in the conference process – numerous researchers are trapped in unending cycles of submit … get reject … incorporate comments … resubmit — often to the next deadline which has been conveniently arranged a couple of days down the road so the unwitting participants are conditioned into mindlessly keep coming back for more, much like Pavlov’s dog.</p>
<p>We are proposing a matching market approach to finally obliterate this madness. We believe such a market is feasible using the following ideas. No doubt our scheme will have some drawbacks; however, as should be obvious, the advantages far outweigh them.</p>
<p>First, for co-located symposia within a larger umbrella conference, such as the<br/>
conferences within ALGO or FCRC, the following process should be a no-brainer:</p>
<p>1). Ensure a common deadline for all symposia; denote the latter by <em>S.</em></p>
<p>2). Let <em>R</em> denote the set of researchers who wish to submit one paper to a symposium in this umbrella conference – assume that researchers submitting more than one paper will have multiple names, one for each submission. Each researcher will provide a strict preference order over the subset of symposia to which they wish to submit their paper. Let <em>G</em> denote the bipartite graph with vertex sets (<em>R, S</em>) and an edge (<em>r, s</em>) only if researcher <em>r</em> chose symposium <em>s.</em></p>
<p>3). The umbrella conference will have a large common PC with experts representing all of its symposia. The process of assigning papers to PC members will of course use <em>G</em> in a critical way.</p>
<p>Once papers are reviewed by PC members and external reviewers, each symposium will rank its submissions using its own criteria of acceptance. We believe the overhead of ranking each paper multiple times is minimal since that is just an issue of deciding how “on-topic” a paper is – an easy task once the reviews of the paper are available.</p>
<p>4). Finally, using all these preference lists, a researcher-proposing stable matching is computed using the Gale-Shapley algorithm. As is well-known, this mechanism will be dominant strategy incentive compatible for researchers.</p>
<p>With a little extra effort, a similar scheme can also be used for a group of conferences at diverse locations but similar times, such as some of the annual summer theory conferences, STOC, ICALP, ESA, STAC, WADS/SWAT, etc.</p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2019-11-19T01:31:12Z</updated>
    <published>2019-11-19T01:31:12Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Kevin Leyton-Brown</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-11-30T14:20:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3436178446517561376</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3436178446517561376/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/11/fields-used-to-be-closer-together-than.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3436178446517561376" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3436178446517561376" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/11/fields-used-to-be-closer-together-than.html" rel="alternate" type="text/html"/>
    <title>Fields used to be closer together than they are now. Good? Bad?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">There was a retired software Eng professor that I had heard two very non-controversial rumors about:<br/>
<br/>
1) He got his PhD in Numerical Analysis<br/>
<br/>
2) He got his PhD in Compiler Optimization.<br/>
<br/>
So I asked him which was true.<br/>
<br/>
The answer: Both! In those days you had to optimize your code to get your NA code to run fast enough.<br/>
<br/>
We cannot imagine that anymore. Or at least I cannot.<br/>
<br/>
Over time the fields of computer science advance more so its hard to be  master of more than one field.  But its not that simple: there has been work recently applying Machine Learning to... well<br/>
everything really. Even so, I think the trend is more towards separation. Or perhaps it oscillates.<br/>
<br/>
I am NOT going to be the grumpy old man (Google once thought I was 70, see <a href="https://blog.computationalcomplexity.org/2018/10/google-added-years-to-my-life.html">here</a>) who says things were better in my day when the fields were closer together. But I will ask the question:<br/>
<br/>
1) Are people more specialized new? While I think yes since each field has gotten more complicated and harder to master. There are exceptions: Complexity theory uses much more sophisticated mathematics then when I was a grad student (1980-1985), and of course Quantum Computing has lead to more comp sci majors knowing physics.<br/>
<br/>
2) Is it good for the field that people are specialized? I am supposed to say that it is terrible and that great advances are made when people are interdiscplinary. But there are many more small advances that are made by someone who has a mastery of one (or two) fields.<br/>
<br/>
3) The PhD Process and the Tenure Process encourage specialization. This I think IS bad since there are different modes of research that should all be respected.'<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-11-18T04:53:00Z</updated>
    <published>2019-11-18T04:53:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-11-30T09:43:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/165</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/165" rel="alternate" type="text/html"/>
    <title>TR19-165 |  Random Restrictions of High-Dimensional Distributions and Uniformity Testing with Subcube Conditioning | 

	Clement Canonne, 

	Xi Chen, 

	Gautam Kamath, 

	Amit Levi, 

	Erik Waingarten</title>
    <summary>We give a nearly-optimal algorithm for testing uniformity of distributions supported on $\{-1,1\}^n$, which makes $\tilde O (\sqrt{n}/\varepsilon^2)$ queries to a subcube conditional sampling oracle (Bhattacharyya and Chakraborty (2018)). The key technical component is a natural notion of random restriction for distributions on $\{-1,1\}^n$, and a quantitative analysis of how such a restriction affects the mean vector of the distribution. Along the way, we consider the problem of mean testing with independent samples and provide a nearly-optimal algorithm.</summary>
    <updated>2019-11-18T01:16:46Z</updated>
    <published>2019-11-18T01:16:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-30T14:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4414</id>
    <link href="https://www.scottaaronson.com/blog/?p=4414" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4414#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4414" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Aaronson-Ambainis Conjecture (2008-2019)</title>
    <summary xml:lang="en-US">(see new update at end of post) Around 1999, one of the first things I ever did in quantum computing theory was to work on a problem that Fortnow and Rogers suggested in a paper: is it possible to separate P from BQP relative to a random oracle? (That is, without first needing to separate […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>(see new update at end of post)</em></p>



<p>Around 1999, one of the first things I ever did in quantum computing theory was to work on a problem that <a href="https://arxiv.org/abs/cs/9811023">Fortnow and Rogers</a> suggested in a paper: is it possible to separate <a href="https://en.wikipedia.org/wiki/P_(complexity)">P</a> from <a href="https://en.wikipedia.org/wiki/BQP">BQP</a> relative to a <a href="https://en.wikipedia.org/wiki/Random_oracle">random oracle</a>?  (That is, without first needing to separate P from PSPACE or whatever in the real world?)  Or to the contrary: suppose that a quantum algorithm Q makes T queries to a Boolean input string X.  Is there then a classical simulation algorithm that makes poly(T) queries to X, and that approximates Q’s acceptance probability for <em>most</em> values of X?  Such a classical simulation, were it possible, would still be consistent with the existence of quantum algorithms like <a href="https://en.wikipedia.org/wiki/Simon%27s_problem">Simon’s</a> and <a href="https://en.wikipedia.org/wiki/Shor%27s_algorithm">Shor’s</a>, which are able to achieve exponential (and even greater) speedups in the black-box setting.  It would simply demonstrate the importance, for Simon’s and Shor’s algorithms, of global structure that makes the string X extremely <em>non</em>-random: for example, encoding a periodic function (in the case of Shor’s algorithm), or encoding a function that hides a secret string s (in the case of Simon’s).  It would underscore that superpolynomial quantum speedups depend on structure.</p>



<p>I never managed to solve this problem.  Around 2008, though, I noticed that a solution would follow from a perhaps-not-obviously-related conjecture, about <em>influences</em> in low-degree polynomials.  Namely, let p:R<sup>n</sup>→R be a degree-d real polynomial in n variables, and suppose p(x)∈[0,1] for all x∈{0,1}<sup>n</sup>.  Define the <i>variance</i> of p to be<br/>   Var(p):=E<sub>x,y</sub>[|p(x)-p(y)|],<br/>and define the <i>influence</i> of the i<sup>th</sup> variable to be<br/>   Inf<sub>i</sub>(p):=E<sub>x</sub>[|p(x)-p(x<sup>i</sup>)|].<br/>Here the expectations are over strings in {0,1}<sup>n</sup>, and x<sup>i</sup> means x with its i<sup>th</sup> bit flipped between 0 and 1.  Then the conjecture is this: there must be some variable i such that Inf<sub>i</sub>(p) ≥ poly(Var(p)/d) (in other words, that “explains” a non-negligible fraction of the variance of the entire polynomial).</p>



<p>Why would this conjecture imply the statement about quantum algorithms?  Basically, because of the seminal result of <a href="https://arxiv.org/abs/quant-ph/9802049">Beals et al.</a> from 1998: that if a quantum algorithm makes T queries to a Boolean input X, then its acceptance probability can be written as a real polynomial over the bits of X, of degree at most 2T.  Given that result, if you wanted to classically simulate a quantum algorithm Q on most inputs—and if you only cared about query complexity, not computation time—you’d simply need to do the following:<br/>(1) Find the polynomial p that represents Q’s acceptance probability.<br/>(2) Find a variable i that explains at least a 1/poly(T) fraction of the total remaining variance in p, and query that i.<br/>(3) Keep repeating step (2), until p has been restricted to a polynomial with not much variance left—i.e., to nearly a constant function p(X)=c.  Whenever that happens, halt and output the constant c.<br/>The key is that by hypothesis, this algorithm will halt, with high probability over X, after only poly(T) steps.</p>



<p>Anyway, around the same time, Andris Ambainis had a major break on a different problem that I’d told him about: namely, whether randomized and quantum query complexities are polynomially related for all partial functions with permutation symmetry (like the collision and the element distinctness functions).  Andris and I decided to write up the two directions jointly.  The result was our 2011 paper entitled <a href="https://arxiv.org/abs/0911.0996">The Need for Structure in Quantum Speedups</a>.</p>



<p>Of the two contributions in the “Need for Structure” paper, the one about random oracles and influences in low-degree polynomials was clearly the weaker and less satisfying one.  As the reviewers pointed out, that part of the paper didn’t solve anything: it just reduced one unsolved problem to a new, slightly different problem that was <em>also</em> unsolved.  Nevertheless, that part of the paper acquired a life of its own over the ensuing decade, as the world’s experts in analysis of Boolean functions and polynomials began referring to the “Aaronson-Ambainis Conjecture.”  Ryan O’Donnell, Guy Kindler, and many others had a stab.  I even got Terry Tao to spend an hour or two on the problem when I visited UCLA.</p>



<p>Now, at long last, Nathan Keller and Ohad Klein say they’ve proven the Aaronson-Ambainis Conjecture, in a preprint whose title is a riff on ours: <a href="https://arxiv.org/abs/1911.03748">“Quantum Speedups Need Structure.”</a></p>



<p>Their paper hasn’t yet been peer-reviewed, and I haven’t yet carefully studied it, but I <em>could</em> and <em>should</em>: at 19 pages, it looks very approachable and clear, if not as radically short as (say) <a href="https://www.scottaaronson.com/blog/?p=4229">Huang’s proof of the Sensitivity Conjecture</a>.  Keller and Klein’s argument subsumes all the earlier results that I knew would need to be subsumed, and involves all the concepts (like a real analogue of block sensitivity) that I knew would need to be involved somehow.</p>



<p>My plan had been as follows:<br/>(1) Read their paper in detail.  Understand every step of their proof.<br/>(2) Write a blog post that reflects my detailed understanding.</p>



<p>Unfortunately, this plan did not sufficiently grapple with the fact that I now have two kids.  It got snagged for a week at step (1).  So I’m now executing an alternative plan, which is to jump immediately to the blog post.</p>



<p>Assuming Keller and Klein’s result holds up—as I expect it will—by combining it with the observations in my and Andris’s paper, one immediately gets an explanation for why no one has managed to separate P from BQP relative to a <em>random</em> oracle, but only relative to non-random oracles.  This complements the work of <a href="https://www.uncg.edu/mat/faculty/cdsmyth/thesis.pdf">Kahn, Saks, and Smyth</a>, who around 2000 gave a precisely analogous explanation for the difficulty of separating P from NP∩coNP relative to a random oracle.</p>



<p>Unfortunately, the polynomial blowup is quite enormous: from a quantum algorithm making T queries, Keller and Klein apparently get a classical algorithm making O(T<sup>18</sup>) queries.  But such things can almost always be massively improved.</p>



<p>Feel free to use the comments to ask any questions about this result or its broader context.  I’ll either do my best to answer from the limited amount I know, or else I’ll pass the questions along to Nathan and Ohad themselves.  Maybe, at some point, I’ll even be forced to understand the new proof.</p>



<p>Congratulations to Nathan and Ohad!</p>



<p><strong><font color="red">Update (Nov. 20):</font></strong> Tonight I finally did what I should’ve done two weeks ago, and worked through the paper from start to finish.  Modulo some facts about noise operators, hypercontractivity, etc. that I took on faith, I now have a reasonable (albeit imperfect) understanding of the proof.  It’s great!</p>



<p>In case it’s helpful to anybody, here’s my one-paragraph summary of how it works.  First, you hit your bounded degree-d function f with a random restriction to attenuate its higher-degree Fourier coefficients (reminiscent of <a href="http://www.ma.huji.ac.il/~ehudf/courses/anal09/LMN.pdf">Linial-Mansour-Nisan</a>).  Next, in that attenuated function, you find a small “coalition” of influential variables—by which we mean, a set of variables for which there’s <em>some</em> assignment that substantially biases f.  You keep iterating—finding influential coalitions in subfunctions on n/4, n/8, etc. variables.  All the while, you keep track of <em>the norm of the vector of all the block-sensitivities of all the inputs</em> (the authors don’t clearly explain this in the intro, but they reveal it near the end).  Every time you find another influential coalition, that norm goes down by a little, but by approximation theory, it can only go down O(d<sup>2</sup>) times until it hits rock bottom and your function is nearly constant.  By the end, you’ll have approximated f itself by a decision tree of depth poly(d, 1/ε, log(n)).  Finally, you get rid of the log(n) term by using the fact that f essentially depended on at most exp(O(d)) variables anyway. </p>



<p>Anyway, I’m not sure how helpful it is to write more: the <a href="https://arxiv.org/pdf/1911.03748.pdf">paper itself</a> is about 95% as clear as it could possibly be, and even where it isn’t, you’d probably need to read it first (and, uh, know something about influences, block sensitivity, random restrictions, etc.) before any further clarifying remarks would be of use.  But happy to discuss more in the comments, if anyone else is reading it.</p></div>
    </content>
    <updated>2019-11-17T23:33:08Z</updated>
    <published>2019-11-17T23:33:08Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-11-27T22:08:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/164</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/164" rel="alternate" type="text/html"/>
    <title>TR19-164 |  Improved bounds for perfect sampling of $k$-colorings in graphs | 

	Siddharth Bhandari, 

	Sayantan Chakraborty</title>
    <summary>We present a randomized algorithm that takes as input an undirected $n$-vertex graph $G$ with maximum degree $\Delta$ and an integer $k &gt; 3\Delta$, and returns a random proper $k$-coloring of $G$. The 
 distribution of the coloring is perfectly uniform over the set of all proper $k$-colorings; the expected running time of the algorithm is $\mathrm{poly}(k,n)=\widetilde{O}(n\Delta^2\cdot \log(k))$. 
 This improves upon a result of Huber~(STOC 1998) who obtained polynomial time perfect sampling algorithm for $k&gt;\Delta^2+2\Delta$.
 Prior to our work, no algorithm with expected running time $\mathrm{poly}(k,n)$ was known to guarantee perfectly sampling for $\Delta = \omega(1)$ and for any $k \leq \Delta^2+2\Delta$. 
 
 Our algorithm (like several other perfect sampling algorithms including Huber's) is based on  the Coupling from the Past method. Inspired by the bounding chain approach pioneered independently by H\"aggstr\"om \&amp; Nelander~(Scand.{} J.{} Statist., 1999) and Huber~(STOC 1998), our algorithm is based on a novel bounding chain for the coloring problem.</summary>
    <updated>2019-11-17T10:56:43Z</updated>
    <published>2019-11-17T10:56:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-30T14:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/163</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/163" rel="alternate" type="text/html"/>
    <title>TR19-163 |  Approximating the Distance to Monotonicity of Boolean Functions | 

	Ramesh Krishnan S. Pallavoor, 

	Sofya Raskhodnikova, 

	Erik Waingarten</title>
    <summary>We design a nonadaptive algorithm that, given a Boolean function $f\colon \{0,1\}^n \to \{0,1\}$ which is $\alpha$-far from monotone, makes poly$(n, 1/\alpha)$ queries and returns an estimate that, with high probability, is an $\widetilde{O}(\sqrt{n})$-approximation to the distance of $f$ to monotonicity. Furthermore, we show that for any constant $\kappa &gt; 0,$ approximating the distance to monotonicity up to $n^{1/2 - \kappa}$-factor requires $2^{n^\kappa}$ nonadaptive queries, thereby ruling out a poly$(n, 1/\alpha)$-query nonadaptive algorithm for such approximations. This answers a question of Seshadhri (Property Testing Review, 2014) for the case of nonadaptive algorithms. Approximating the distance to a property is closely related to tolerantly testing that property. Our lower bound stands in contrast to standard (non-tolerant) testing of monotonicity that can be done nonadaptively with $\widetilde{O}(\sqrt{n} / \varepsilon^2)$ queries.

We obtain our lower bound by proving an analogous bound for erasure-resilient testers. An $\alpha$-erasure-resilient tester for a desired property gets oracle access to a function that has at most an $\alpha$ fraction of values erased. The tester has to accept (with probability at least 2/3) if the erasures can be filled in to ensure that the resulting function has the property and to reject (with probability at least 2/3) if every completion of erasures results in a function that is $\varepsilon$-far from having the property. Our method yields the same lower bounds for unateness and being a $k$-junta. These lower bounds improve exponentially on the existing lower bounds for these properties.</summary>
    <updated>2019-11-16T16:11:38Z</updated>
    <published>2019-11-16T16:11:38Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-30T14:20:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/11/15/linkage</id>
    <link href="https://11011110.github.io/blog/2019/11/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>OMICS Group now charging for article withdrawals (): a new way for predatory journals to be predatory. It’s probably even legal: they have begun providing you with a service (reviewing of your paper) and told you up front what the charges are. Whether it’s ethical for scientific publishing is an entirely different question… So let this be a lesson to be careful where you submit, because unsubmitting could be difficult.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://scholarlyoa.com/omics-group-now-charging-for-article-withdrawals/">OMICS Group now charging for article withdrawals</a> (<a href="https://mathstodon.xyz/@11011110/103069657170279386"/>): a new way for predatory journals to be predatory. It’s probably even legal: they have begun providing you with a service (reviewing of your paper) and told you up front what the charges are. Whether it’s ethical for scientific publishing is an entirely different question… So let this be a lesson to be careful where you submit, because unsubmitting could be difficult.</p>
  </li>
  <li>
    <p><a href="https://www.shapeoperator.com/2016/12/12/sunset-geometry/">Sunset geometry</a> (<a href="https://mathstodon.xyz/@11011110/103075417779881667"/>, <a href="https://news.ycombinator.com/item?id=21413358">via</a>). How to tell the radius of the earth from a photo of a sunset over a still body of water (knowing the height of the camera over the water). Not explained: how still the water needs to be, how badly the results are affected by atmospheric refraction, how accurately the measurements need to be performed to get a meaningful estimate, or how likely you are to see the sun meet the horizon before low clouds get in the way.</p>
  </li>
  <li>
    <p><a href="https://www.ams.org/profession/ams-fellows/new-fellows">The new AMS fellows</a> (<a href="https://mathstodon.xyz/@11011110/103077029444407287"/>) include graph theorists Daniel Kráľ and Bojan Mohar, and fellow Wikipedia editor Marie Vitulli. Their announcement also led me to create new Wikipedia articles on new fellows <a href="https://en.wikipedia.org/wiki/Chikako_Mese">Chikako Mese</a>, <a href="https://en.wikipedia.org/wiki/Julianna_Tymoczko">Julianna Tymoczko</a>, and <a href="https://en.wikipedia.org/wiki/Jang-Mei_Wu">Jang-Mei Wu</a>, and Vitulli to create one for <a href="https://en.wikipedia.org/wiki/Tara_E._Brendle">Tara Brendle</a>. Congratulations, all!</p>
  </li>
  <li>
    <p><a href="https://www.maa.org/programs/maa-awards/writing-awards/the-graph-menagerie-abstract-algebra-and-the-mad-veterinarian">The graph menagerie: abstract algebra and the mad veterinarian </a> (<a href="https://mathstodon.xyz/@11011110/103083916322783935"/>). Or, how to solve puzzles like: “Suppose a mad veterinarian creates a transmogrifier that can convert one cat into two dogs and five mice, or one dog into three cats and three mice, or a mouse into a cat and a dog. It can also do each of these operations in reverse. Can it, through any sequence of operations, convert two cats into a pack of dogs? How about one cat?”</p>
  </li>
  <li>
    <p><a href="http://processalgebra.blogspot.com/2019/11/call-for-opinions-length-of-papes-in.html">LIPIcs series editor Luca Aceto polls the community on page limits</a> (<a href="https://mathstodon.xyz/@11011110/103086511516971922"/>). It used to be that conferences in theoretical computer science had page limits because you couldn’t bind volumes with too many paper pages, now long irrelevant. So now that we <em>can</em> publish much longer conference papers, should we? Limits encourage authors to publish full details in a properly refereed journal version, but unlimited length recognizes the reality that many authors are too lazy to make journal versions.</p>
  </li>
  <li>
    <p><a href="https://understandinguncertainty.org/squaring-square-glass">Squaring the square, in stained glass</a> (<a href="https://mathstodon.xyz/@11011110/103095223291132423"/>, <a href="https://scilogs.spektrum.de/hlf/perfect-squares/">via</a>). By David Spiegelhalter, 2013.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Edge_tessellation">Tiling the plane by edge reflection</a> (<a href="https://mathstodon.xyz/@11011110/103099409883184951"/>). Here’s a proof sketch that there are eight ways to do this:
Each prototile vertex must have angle  for integer , and if  is odd, the subsequence of the remaining angles must be symmetric. For an -gon, considering the sum of interior angles shows that</p>

    

    <p>Searching for sequences of integers with these properties (choosing the smallest integers first to make the search bounded) finds that the only cyclic sequences of integers meeting these constraints are (3,12,12), (4,8,8), (4,6,12), (6,6,6), (3,4,6,4), (3,6,3,6), (4,4,4,4), and (3,3,3,3,3,3), the sequences of the 8 known tessellations.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2019/11/08/turkish-academics-sound-alarm-over-gender-segregation-plans">Turkish academics sound alarm over gender segregation plans</a> (<a href="https://mathstodon.xyz/@11011110/103103753533385831"/>). When women’s universities are set up to provide alternatives in the face of persistent discrimination against women in the existing universities (as they were in the US and Korea), that’s one thing. When the women are already successful in the existing universities but women’s universities are set up as a pathway to push them out, that’s entirely different.</p>
  </li>
  <li>
    <p><a href="http://news.mit.edu/2019/leonardo-da-vinci-bridge-test-1010">Engineers put Leonardo da Vinci’s bridge design to the test:
proposed bridge would have been the world’s longest at the time; new analysis shows it would have worked</a> (<a href="https://mathstodon.xyz/@11011110/103111919197001339"/>, <a href="https://arstechnica.com/science/2019/10/testing-leonardo-da-vincis-bridge-his-design-was-stable-study-finds/">via</a>). I don’t think the link does justice to the scale of the thing. Da Vinci proposed a single stone arch across the Golden Horn in Istanbul, roughly 280m. That’s much longer than anything on the current <a href="https://en.wikipedia.org/wiki/List_of_longest_masonry_arch_bridge_spans">list of the world’s biggest stone arches</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Order_polytope">Order polytope</a> (<a href="https://mathstodon.xyz/@11011110/103115656245496084"/>). New Wikipedia article on a convex polytope derived from any finite partial order as the points in a unit hypercube whose coordinate order is consistent with the partial order. Its vertices come from upper sets, its faces come from quotients, its facets come from covering pairs, and its volume comes from the number of linear extensions of the partial order. Coordinatewise min and max gives its points the structure of a continuous distributive lattice.</p>
  </li>
  <li>
    <p><a href="https://www.wired.com/story/socked-into-the-puppet-hole-on-wikipedia/">Socked into the puppet-hole on Wikipedia</a> (<a href="https://mathstodon.xyz/@11011110/103120959491226706"/>). Journalist Noam Cohen’s Wikipedia biography is collateral damage in the war on slowking4, a prolific creator of Wikipedia articles whose problematic behavior (copying content from other sites, creating sockpuppet accounts to hide their identity, and reinstating articles from another user that were so riddled with errors that they were deleted en masse) has led to delete-on-sight actions.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Arithmetic_billiards">Arithmetic billiards</a> (<a href="https://mathstodon.xyz/@11011110/103129171790722530"/>): using billiard ball trajectories to compute number-theoretic functions.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@olligobber/103066273568117018">olligober made a regex to match all multiples of 7</a>, but it was more than 10,000 characters long so grep couldn’t handle it. Applying <a href="https://en.wikipedia.org/wiki/Kleene%27s_algorithm">Kleene’s algorithm</a> to convert the natural DFA for this sort of problem into a regular expression blows its size up from polynomial in the modulus to exponential, but is this necessary? And if it is, what is the best possible base for the exponential?</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/questions/338888/dividing-a-chocolate-bar-into-any-proportions">Dividing a chocolate bar into any proportions</a> (<a href="https://mathstodon.xyz/@11011110/103140599116798730"/>). The bar has  squares, and you want to give each of  people an integer number of squares, but the integers are not known in advance. How to break the bar into few pieces so this will always be possible? Reid Hardison asked this months ago but Ilya Bogdanov answered with an efficient construction of the optimal partition much more recently.</p>
  </li>
  <li>
    <p><a href="http://www.personal.psu.edu/sot2/books/billiardsgeometry.pdf">Geometry and Billiards</a> (<a href="https://mathstodon.xyz/@11011110"/>). An undergraduate-level textbook on the mathematics of reflection by Serge Tabachnikov.</p>
  </li>
</ul></div>
    </content>
    <updated>2019-11-15T22:20:00Z</updated>
    <published>2019-11-15T22:20:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-11-28T00:29:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7574</id>
    <link href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/" rel="alternate" type="text/html"/>
    <title>Puzzles of modern machine learning</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-jetpack-markdown"><p>It is often said that "we don’t understand deep learning" but it is not as often clarified what is it exactly that we don’t understand. In this post I try to list some of the "puzzles" of modern machine learning, from a theoretical perspective. This list is neither comprehensive nor authoritative.  Indeed, I  only started looking at these issues last year, and am very much in the position of not yet fully understanding the questions, let alone potential answers.
On the other hand, at the rate ML research is going, a calendar year corresponds to about 10 "ML years"…</p>
<p>Machine learning  offers many opportunities for theorists; there are many more questions than answers, and it is clear that a better theoretical understanding of what makes certain training procedures work or fail is desperately needed. Moreover, recent advances in software frameworks made it much easier to test out intuitions and conjectures. While in the past running training procedures might have required a Ph.D in machine learning, recently the "barrier to entry" was reduced to first to undergraduates, then to high school students, and these days it’s so easy that even theoretical computer scientists can do it <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/></p>
<p>To set the context for this discussion, I focus on the task of supervised learning. In this setting we are given a <em>training set</em> <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> examples of the form <img alt="(x_i,y_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_i%2Cy_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x_i,y_i)"/> where <img alt="x_i \in \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i \in \mathbb{R}^d"/> is some vector (think of it as the pixels of an image) and <img alt="y_i \in { \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=y_i+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_i \in { \pm 1 }"/> is some label (think of <img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_i"/> as equaling <img alt="+1" class="latex" src="https://s0.wp.com/latex.php?latex=%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="+1"/> if <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> is the image of a dog and <img alt="-1" class="latex" src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-1"/> if <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/> is the image of a cat). The goal in supervised learning is to find a <em>classifier</em> <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/> such that <img alt="f(x)=y" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(x)=y"/> will hold for many future samples <img alt="(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x,y)"/>.</p>
<p>The standard approach is to consider some parameterized family of classifiers, where for every vector <img alt="\theta \in \mathbb{R}^m" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta \in \mathbb{R}^m"/>  of parameters, we associate a classifier <img alt="f_\theta :\mathbb{R}^d \rightarrow { \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta+%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta :\mathbb{R}^d \rightarrow { \pm 1 }"/>. For example, we can fix a certain neural network architecture (depth, connections, activation functions, etc.) and let <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/>  be the vector of weights that characterizes every network in this architecture. People then run some optimizing algorithm such as stochastic gradient descent with the objective function set as finding the vector <img alt="\theta \in \mathbb{R}^m" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta \in \mathbb{R}^m"/> that minimizes a <em>loss function</em> <img alt="L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\theta)"/>. This loss function can be the fraction of labels that <img alt="f_\theta" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta"/> gets wrong on the set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> or a more continuous loss that takes into account the confidence level or other parameters of <img alt="f_\theta" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta"/> as well. By now this general approach has been successfully applied to a many classification tasks, in many cases achieving near-human to super-human performance.  In the rest of this post I want to discuss some of the questions that arise when trying to obtain a theoretical understanding of both the powers and the limitations of the above approach. I focus on deep learning, though there are still some open questions even for over-parameterized linear regression.</p>
<h2>The generalization puzzle</h2>
<p>The approach outlined above has been well known and  analyzed for many decades in the statistical learning literature. There are many cases where we can <em>prove</em> that a classifier obtained in this case has a small <em>generalization gap</em>, in the sense that if the training set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> was obtained by sampling <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> independent and identical samples from a distribution <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/>, then the performance of a classifier <img alt="f_\theta" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta"/> on new samples from <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/> will be close to its performance on the training set.</p>
<p>Ultimately, these results all boil down to the Chernoff bound. Think of the random variables <img alt="X_1,\ldots,X_n" class="latex" src="https://s0.wp.com/latex.php?latex=X_1%2C%5Cldots%2CX_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_1,\ldots,X_n"/> where <img alt="X_i=1" class="latex" src="https://s0.wp.com/latex.php?latex=X_i%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_i=1"/> if the classifier makes an error on the <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>-th training example. The Chernoff bound tells us that  probability that  that <img alt="\sum X_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+X_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum X_i"/> deviates by more than <img alt="\epsilon n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon n"/> from its expectation is something like <img alt="\exp(-\epsilon^2 n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cepsilon%5E2+n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(-\epsilon^2 n)"/> and so as long as the total number of classifiers is less than <img alt="2^k" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^k"/> for <img alt="k &lt; \epsilon^2 n" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3C+%5Cepsilon%5E2+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k &lt; \epsilon^2 n"/>, we can use a union bound over all possible classifiers to argue that if we make a <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> fraction of errors on the training set, the probability we make an error on a new example is at  most <img alt="p+\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=p%2B%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p+\epsilon"/>. We can of course "bunch together" classifiers that behave similarly on our distribution, and so it is enough if there are at most <img alt="2^{\epsilon^2 n}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Cepsilon%5E2+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{\epsilon^2 n}"/> of these equivalence classes. Another approach is to add a "regularizing term" <img alt="R(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=R%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R(\theta)"/> to the objective function, which amounts to restricting attention to the set of all classifiers <img alt="f_\theta" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta"/> such that <img alt="R(\theta) \leq \mu" class="latex" src="https://s0.wp.com/latex.php?latex=R%28%5Ctheta%29+%5Cleq+%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R(\theta) \leq \mu"/> for some parameter <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>. Again, as long as the number of equivalence classes in this set is less than <img alt="2^{\epsilon^2 n}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Cepsilon%5E2+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{\epsilon^2 n}"/>, we can use this bound.</p>
<p>To a first approximation, the number of classifiers (even after "bunching together") is roughly exponential in the number <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> of parameters, and so these results tell us that as long as the number of <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> of parameters is smaller than the number of examples, we can expect to have a small <em>generalization gap</em> and can infer future performance (known as "test performance") from the performance on the set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> (known as "train performance").
Once the number of parameters <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> becomes close to or even bigger than the number of samples <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>, we are in danger of "overfitting" where we could have excellent train performance but terrible test performance. Thus according to the classical statistical learning theory, the ideal number of parameters would be some number between <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> and the number of samples <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/>, with the precise value governed by the so called "bias variance tradeoff".</p>
<p>This is a beautiful theory, but unfortunately the classical theorems yield vacous  results in the realm of modern machine learning, where we  often train networks with millions of parameters on a mere tens of thousands of examples. Moreover, <a href="https://arxiv.org/abs/1611.03530">Zhang et al</a> showed that this is not just a question of counting  parameters better. They showed that modern deep networks can in fact "overfit" and achieve 100% success on the training set even if you gave them random or arbitrary labels.</p>
<p>The results above in particular show that we can find classifiers that perform great on the training set but perform terribly on the future tests, as well as classifiers that perform terrible on the training set but pretty good on future test. Specifically, consider an architecture that has the capacity to fit <img alt="20n" class="latex" src="https://s0.wp.com/latex.php?latex=20n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="20n"/> arbitrary labels, and suppose that we train it on a set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> examples.  Then we can find a setting of parameters <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> that both fits the training set exactly (i.e.,  satisfies <img alt="f_\theta(x)=y" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta%28x%29%3Dy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta(x)=y"/> for all <img alt="(x,y)\in S" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29%5Cin+S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x,y)\in S"/>) but also satisfies that the additional constraint that <img alt="f_\theta(x)= -y" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta%28x%29%3D+-y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta(x)= -y"/> (i.e., the negation of the label <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/>) for every <img alt="(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x,y)"/> in some additional set <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> of <img alt="19m" class="latex" src="https://s0.wp.com/latex.php?latex=19m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="19m"/> pairs.
(The set <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> is not part of the actual training set, but rather an "auxiliary set" that we simply use for the sake of constructing this counterexample; note that we can use <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> as means to generate the initial network which can then be fed into standard stochastic gradient descent on the set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/>.) The network <img alt="f_\theta" class="latex" src="https://s0.wp.com/latex.php?latex=f_%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_\theta"/> fits its training set perfectly, but since it effectively corresponds to training with 95% label noise, it will  perform  worse than even a coin toss.</p>
<p>In an analogous way, we can find parameters <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> that completely fail on the training set, but fit correctly the additional "auxiliary set" <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/>. This will correspond to the case of standard training with 5% label noise, which typically yields about 95% of the performance on the noiseless distribution.</p>
<p>The above insights  break the <strong>separation of concerns</strong> or separation of <strong>computational problems</strong> from <strong>algorithms</strong> which we theorists  like so much. Ideally, we would like to phrase the "machine learning problem" as a well defined optimization objective, such as finding, given a set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/>, the vector <img alt="\theta \in \mathbb{R}^m" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cin+%5Cmathbb%7BR%7D%5Em&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta \in \mathbb{R}^m"/> that mimimizes <img alt="L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\theta)"/>. Once phrased in this way, we can try to find with an algorithm that achieves this goal as efficiently as possible.</p>
<p>Unfortunately, modern machine learning does not currently lend itself to such a clean partition. In particular, since not all optima are equally good, we  <em>don’t  actually want</em> to solve the task of minimizing the loss function in a "black box" way. In fact, many of the ideas that make optimization faster such as accelaration, lower learning rate, second order methods and others, yield <em>worse</em>  generalization performance. Thus, while the objective function is somewhat correlated with generalization performance, it is neither necessary nor sufficient for it. This is a clear sign that we don’t really understand what makes machine learning work, and there is still much left to discover. I don’t know what machine learning textbooks in the 2030’s will contain, but my guess is that they would <em>not</em> prescribe running stochastic gradient descent on one of these loss functions. (Moritz Hardt counters that what we teach in ML today is not that far from the <a href="https://www.amazon.com/Pattern-Classification-Scene-Analysis-Richard/dp/0471223611">1973 book of Duda and Hart</a>, and that by some measures ML moved <em>slower</em> than other areas of CS.)</p>
<p>The <em>generalization puzzle</em> of machine learning can be phrased as the question of understanding what properties of procedures that map a training set <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> into a classifier <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> lead to good generalization performance with respect to certain distributions. In particular we would like to understand what are the properties of natural  natural distributions and stochastic gradient descent that make the latter into such a map.</p>
<h2>The computational puzzle</h2>
<p>Yet another puzzle in modern machine learning arises from the fact that we are able to find the minimum of <img alt="L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\theta)"/> in the first place. A priori this is surprising since, apart from very special cases (e.g., linear regression with a square loss), the function <img alt="\theta \mapsto L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta+%5Cmapsto+L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta \mapsto L_S(\theta)"/> is in general <em>non convex</em>. Indeed, for almost any natural loss function, the problem of finding <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> that minimizes <img alt="L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\theta)"/> is NP hard.
However, if we look at the computational question in the context of the generalization puzzle above, it might not be as mysterious. As we have seen, the fact that the <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> we output is a global minimizer (or close to minimizer)  of <img alt="L_S(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ccdot%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\cdot)"/> is in some sense accidental and by far not the  the most important property of <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/>. There are many minima of the loss function that  generalize badly, and many non minima that  generalize well.</p>
<p>So perhaps the right way to phrase the computational puzzle is as</p>
<blockquote>
<p><em>"How come that we are able to use stochastic gradient descent to find the vector <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\theta"/> that is output by stochastic gradient descent."</em></p>
</blockquote>
<p>which when phrased like that, doesn’t seem like much of a puzzle after all.</p>
<h2>The off-distribution performance puzzle</h2>
<p>In the supervised learning problem, the training samples <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S"/> are drawn from the same distribution as the final test sample. But in any applications of machine learning, classifiers are expected to perform on samples that arise from very different settings. The image that the camera of a self-driving car observes is not drawn from ImageNet, and yet it still needs to (and often can) detect whether not it is seeing a dog or a cat (at which point it will break or accelerate, depending on whether the programmer was a dog or cat lover). Another insight to this question comes from a recent work of <a href="https://arxiv.org/abs/1902.10811">Recht et al</a>. They generated a new set of images that is very similar to the original ImageNet test set, but not identical to it. One can think of it as generated from a distribution <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D'"/> that is close but not the same as the original distribution <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/> of ImageNet. They then checked how well do neural networks that were trained on the original ImageNet distribution <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/> perform on <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D'"/>. They saw that while these networks performed significantly worse on <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D'"/> than they did on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/>, their performance on <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D'"/> was <em>highly correlated</em> with their performance on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/>. Hence doing better on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D"/> did correspond to being better in a way that carried over to the (very closely related) <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D'"/>. (However, the networks did perform worse on $D’$ so off-distribution performance is by no means a full success story.)</p>
<p>Coming up with a theory that can supply some predictions for learning in a way that is not as tied to the particular distribution is still very much open. I see it as somewhat akin to finding a theory for the performance of algorithms that is somewhere between average-case complexity (which is highly dependant on the distribution) and worst-case complexity (which does not depend on the distribution at all, but is not always achievable).</p>
<h2>The robustness puzzle</h2>
<p>If the previous puzzles were about understanding why deep networks are surprisingly good, the next one is about understanding why they are surprisingly bad. Images of physical objects have the property that if we modify them in some ways,  such as perturbing them in  a small number of pixels or by few shades or rotating by an angle, they still correspond to the same object. Deep neural networks do not seem to "pick up" on this property.
Indeed, there are many examples of how tiny perturbations can cause a neural net to think that one image is another, and people have even <a href="https://youtu.be/piYnd_wYlT8">printed a 3D turtle</a> that most modern systems recognize as a rifle. (See this <a href="https://adversarial-ml-tutorial.org/">excellent tutorial</a>, though note an "ML decade" has already passed since it was published). This "brittleness" of neural networks can be a significant concern when we deploy them in the wild. (Though perhaps mixing up turtles and rifles is not so bad: I can imagine  some people that would normally resist regulations to protect the environment but would support them if they confused turtles with guns..)
Perhaps one reason for this brittleness is that neural networks can be thought of as a way of embedding a set of examples in dimension <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> into dimension <img alt="\ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ell"/> (where <img alt="\ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ell"/> is the number of neurons in the penultimate layer) in a way that will make the positive examples be linearly separable from the negative examples. Amplifying small differences can help in achieving such a separation, even if it hurts robustness.</p>
<p>Recent works have attempted to rectify this, by using a variants of the loss function where <img alt="L_S(\theta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_S%28%5Ctheta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_S(\theta)"/> corresponds to the maximum error under all possible such perturbations of the data. A priori you would think that while robust training might come at a computational cost, statistically it would be a "win win" with the resulting classifiers not only being more robust but also overall better at classifying. After all, we are providing the training procedure with the additional information (i.e., "updating its prior") that the label should be unchanged by certain transformations, which should be equivalent to supplying it with more data. Surprisingly, the robust classifiers currently perform <em>worse</em> than standard trained classifiers on unperturbed data. <a href="https://arxiv.org/abs/1905.02175">Ilyas et al</a> argued that this may be because even if humans ignore information encoded in, for example, whether the intensity level of a pixel is odd or even, it does not mean that this information is not predictive of the label. Suppose that (with no basis whatsoever – just as an example) cat owners are wealthier than dog owners and hence cat pictures tend to be taken with higher quality lenses. One could imagine that a neural network would pick up on that, and use some of the fine grained information in the pixels to help in classification. When we force such a network to be robust it would perform worse. Distill journal published <a href="https://distill.pub/2019/advex-bugs-discussion/">six discussion pieces</a> on the Ilyas et al paper. I like the idea of such "paper discussions" very much and hope it catches on in machine learning and beyond.</p>
<h2>The interpretability puzzle</h2>
<p>Deep neural networks are inspired by our brain, and it is tempting to try to understand their internal structure just like we try to understand the brain and see if it has a <a href="https://en.wikipedia.org/wiki/Grandmother_cell">"grandmother neuron"</a>. For example, we could try to see if there is a certain neuron (i.e., gate) in a neural network that "fires" only when it is fed images with certain high level features (or more generally find vectors that have large correlation with the state at a certain layer only when the image has some features). This also of practical importance, as we increasingly use classifiers to make decisions such as whether to approve or deny bail, whether to prescribe to a patient treatment A or B, or whether a car should steer left or right, and would like to understand what is the basis for such decisions. There are beautiful visualizations of neural networks’ decisions and internal structures , but given the robustness puzzle above, it is unclear if these really capture the decision process. After all, if we could change the classification from a cat to a dog by perturbing a tiny number of pixels, in what sense can we explain <em>why</em> the network made this decision or the other.</p>
<h2>The natural distributions puzzle</h2>
<p>Yet another puzzle (pointed out to me by Ilya Sutskever) is to understand what is it about "natural" distributions such as images, texts, etc.. that makes them so amenable to learning via neural networks, even though such networks can have a very hard time with learning even simple concepts such as parities.
Perhaps this is related to the "noise robustness" of natural concepts which is related to being correlated with low degree polynomials.
Another suggestion could be that at least for text etc.., human languages are implicitly designed to fit neural network. Perhaps on some other planets there are languages where the meaning of a sentence completely changes depending on whether it has an odd or an even number of letters…</p>
<h2>Summary</h2>
<p>The above are just a few puzzles that modern machine learning offers us. Not all of those might have answers in the form of mathematical theorems, or even well stated conjectures, but it is clear that there is still much to be discovered, and plenty of research opportunities for theoretical computer scientists. In this blog I focused on supervised learning, where at least the problem is well defined, but there are other areas of machine learning, such as transfer learning and generative modeling, where we don’t even yet know how to phrase the computational task, let alone prove that any particular procedure solves it. In several ways, the state of machine learning today seems to me as similar to the state of cryptography in the late 1970’s. After the discovery of public key cryptography, researchers has highly promising techniques and great intuitions, but still did not really understand even what security means, let alone how to achieve it. In the decades since, cryptography has turned from an art to a science, and I hope and believe the same will happen to machine learning.</p>
<p><strong>Acknowledgements:</strong> Thanks to Preetum Nakkiran, Aleksander Mądry, Ilya Sutskever and Moritz Hardt for helpful comments. (In particular, I dropped an interpretability experiment suggested in an earlier version of this post since Moritz informed me that several similar experiments have been done.) Needless to say, none of them is responsible for any of the speculations and/or errors above.</p>
</div>



<p/></div>
    </content>
    <updated>2019-11-15T15:51:43Z</updated>
    <published>2019-11-15T15:51:43Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-11-30T14:20:54Z</updated>
    </source>
  </entry>
</feed>
