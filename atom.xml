<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-11-20T03:21:38Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16399</id>
    <link href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/" rel="alternate" type="text/html"/>
    <title>A Clever Way To Find Compiler Bugs</title>
    <summary>Your comments are valuable, we thank you. source Xuejun Yang is a Senior Staff Engineer at FutureWei Technologies. He is the DFA on the 2011 paper, “Finding and Understanding Bugs in C Compilers.” Today Ken and I discuss a clever idea from that paper. The paper was brought to our attention just now in a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Your comments are valuable, we thank you.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/11/19/a-clever-way-to-find-compiler-bugs/profile_image174/" rel="attachment wp-att-16401"><img alt="" class="alignright  wp-image-16401" src="https://rjlipton.files.wordpress.com/2019/11/profile_image174.jpg?w=220" width="220"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.flux.utah.edu/profile/jxyang">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Xuejun Yang is a Senior Staff Engineer at FutureWei Technologies. He is the DFA on the 2011 <a href="http://www.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf">paper</a>, “Finding and Understanding Bugs in C Compilers.”</p>
<p>
Today Ken and I discuss a clever idea from that paper.</p>
<p>
The paper was brought to our attention just now in a meaty <a href="https://rjlipton.wordpress.com/2019/10/21/a-polemical-overreach/#comment-106276">comment</a> by Paul D. We thank him for it—the topic interests both of us. We don’t think Paul D. means to be anonymous, but in keeping with that we’ll give just a cryptic hint to his identity: The saying “a man on the make” is widely known, but for more than the millennium he has been the unique person in the world to whom it applies literally.</p>
<p>
Yang was made unique by being listed out of alphabetical order on the paper. This is notable because the most common practice in our field is to list alphabetically irrespective of prominence. Hence we’ve invented the term ‘DFA’ for “Designated” or “Distinguished” First Author. The other authors are Yang Chen, Eric Eide, and John Regehr, all from the University of Utah. </p>
<p>
</p><p/><h2> The Topic </h2><p/>
<p/><p>
Paul D.’s comment notes that there was evidence that verification methods could improve compiler correctness. By <i>compiler</i> we mean the program that transforms high level code into machine code. These programs are used countless times every day and their correctness is clearly very important. </p>
<p>
Their correctness is tricky for several reasons. The main one is that almost all compilers try to optimize code. That is when they transform code into instructions they try to rewrite or rearrange the instructions to yield better performance. Compilers have been doing this forever. The trouble is that changing instructions to increase performance is dangerous. The changes must not affect the values that are computed. If they are not done carefully they can actually make the answers faster, but incorrect. This is the reason correctness is tricky.</p>
<p>
Formal verification requires a lot of effort. The highest effort should go into mission-critical software. But compilers are mission-critical <em>already</em>, unless we know mission-critical software won’t be compiled on a particular one. Hence it is notable when formal verification makes a compiler more reliable. </p>
<p>
</p><p/><h2> The Paper </h2><p/>
<p/><p>
The idea in the paper Paul referenced is quite elegant. They built a program called Csmith. It operates as follows: </p>
<blockquote><p><b> </b> <em> Suppose that <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X}"/> is a compiler they wish to test. Then generate various legal C programs <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{P}"/>. For each of these let <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> be the answer that <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X(P)}"/> yields. Here <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X(P)}"/> is the compiled program. Then check whether <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is correct. </em>
</p></blockquote>
<p/><p>
For example:  </p>
<pre>int foo (void) { 
    signed char x = 1; 
    unsigned char y = 255; 
    return x &gt; y; 
} 
</pre>
<p>
Some compilers returned <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, but the correct answer is <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. There are further examples in a 2012 companion <a href="https://www.cs.utah.edu/~regehr/papers/pldi12-preprint.pdf">paper</a> and these <a href="https://www.flux.utah.edu/download?uid=115&amp;slides=1&amp;type=pptx">slides</a> from an earlier version. The Csmith <a href="https://embed.cs.utah.edu/csmith/">homepage</a> has long lists of compiler bugs they found. </p>
<p>
Of course if <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X(P)}"/> crashes or refuse to compile <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> then the compiler is wrong. But what happens if <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is computed. How does Csmith know if the answer is correct? This seems to be really hard. This correctness testing must be automated: the whole approach is based on allowing tons of random programs to be tested. They cannot assume that humans will be used to check the outputs.</p>
<p>
This is the clever idea of this paper. They assume that there are at least two compilers say <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> and <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/>. Then let <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> be the output of <img alt="{X(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X(P)}"/> and let <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> be the output of <img alt="{Y(P)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%28P%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y(P)}"/>. The key insight is: </p>
<blockquote><p>
<b>If <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is not equal to <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B}"/>, then one of the compilers is wrong</b>.
</p></blockquote>
<p>
A very neat and elegant idea. For software in general it is called <a href="https://en.wikipedia.org/wiki/Differential_testing">differential</a> <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.83.445">testing</a>. </p>
<p>
This at least alerts when there are problems with some compilers and some programs. One can use this trick to discover programs that cause at least some compilers to have problems. This is extremely valuable. It allowed Csmith to discover hundreds of errors in production compilers—errors that previously were missed.</p>
<p>
</p><p/><h2> Smart Fuzzing </h2><p/>
<p/><p>
<a href="https://en.wikipedia.org/wiki/Fuzzing">Fuzzing</a> is defined by Wikipedia as testing by “providing invalid, unexpected, or random data as inputs to a computer program.” An early historical example, Apple’s “<a href="https://en.wikipedia.org/wiki/Monkey_testing">Monkey</a>” program, worked completely randomly. To ensure that the found bugs are <em>meaningful</em> and <em>analyzable</em>, Csmith needed a deeper, structured, “intelligent” design, not just the generation of <a href="https://en.wikipedia.org/wiki/Mayhem_(advertising_character)">Mayhem</a>.</p>
<p>
For one, Csmith needed to avoid programs <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> than do not have deterministic behavior. The formal C standards itemize cases in which compilers are allowed to have arbitrary, even self-inconsistent, behavior. There are lots of them in C. A bug with dubious code could be dismissed out of hand.</p>
<p>
For another, the probability that a program <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> built haphazardly by the original Csmith version would reveal bugs was observed to peak at about 80KB source-code size, about 1,000 lines across multiple pages. Those don’t make great examples. So Csmith has its own routines to compress bug instances it has found. Simple tricks are shortening numerical expressions to use only the bug-sensitive parts. Others are lifting local variables out of blocks and bypassing pointer jumps.</p>
<p>
A third goal is that the generator should branch out to all aspects of the language—in this case, C—not just the “grungy” parts that are ripe for finding compiler bugs. The paper talks about this at length. Regehr, who was Yang’s advisor, is also a blogger. His current <a href="https://blog.regehr.org/archives/1700">post</a>, dated November 4, is titled, “Helping Generative Fuzzers Avoid Looking Only Where the Light is Good, Part 1.” We guess that “Part 2” will go even more into details.</p>
<p>
</p><p/><h2> Formal Methods as Bugscreen </h2><p/>
<p/><p>
Regarding the formally-verified CompCert compiler, Paul D. quoted from the <a href="http://www.cs.utah.edu/~regehr/papers/pldi11-preprint.pdf">paper</a>:</p>
<blockquote><p><b> </b> <em> The striking thing about our CompCert results is that the middle-end bugs we found in all other compilers are absent. As of early 2011, the under-development version of CompCert is the only compiler we have tested for which Csmith cannot find wrong-code errors. This is not for lack of trying: we have devoted about six CPU-years to the task. The apparent unbreakability of CompCert supports a strong argument that developing compiler optimizations within a proof framework, where safety checks are explicit and machine-checked, has tangible benefits for compiler users. </em>
</p></blockquote>
<p/><p>
This August 2019 <a href="https://arxiv.org/pdf/1902.09334.pdf">paper</a> by Michaël Marcozzi, Qiyi Tang, Alastair Donaldson, and Cristian Cadar gives recent results involving Csmith and other tools. They have an interesting discussion on page 2, from which we excerpt:</p>
<blockquote><p><b> </b> <em> In our experience working in the area […], we have found compiler fuzzing to be a contentious topic. Research talks on compiler fuzzing are often followed by questions about the importance of the discovered bugs, and whether compiler fuzzers might be improved by taking inspiration from bugs encountered by users of compilers “in the wild.” Some … argue that any miscompilation bug, whether fuzzer-found or not, is a ticking bomb that should be regarded as severe, or avoided completely via formal verification (in the spirit of CompCert). </em>
</p></blockquote>
<p/><p>
They go on to say, however, that when a fully-developed compiler is used for non-critical software, the kinds of bugs typically found by fuzzing tend to have questionable importance. Their paper is titled, “A Systematic Impact Study for Fuzzer-Found Compiler Bugs.” </p>
<p>
So far they have found definite results that seem to have mixed implications. In their future-work section they note that they have evaluated the impact of bugs in compilers on the intended function of programs they compile, but not on possible security holes—which as we noted in our Cloudflare <a href="https://rjlipton.wordpress.com/2017/03/08/is-computer-security-possible/">post</a> can come from (misuse of) simple code that is completely correct. This leads us further to wonder, coming full-circle, whether formal methods might help quantify the relative importance of aspects of a language and areas of a compiler to guide more-intelligent generation of test cases.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>
The above comment is interesting, but perhaps finding obscure bugs is important. Perhaps such bugs could be used to attack systems. That is perhaps some one could use them to break into a system. Security may be compromised by any error, even an unlikely one to occur in the wild. </p>
<p>
What do you think?</p>
<p/></font></font></div>
    </content>
    <updated>2019-11-20T00:57:14Z</updated>
    <published>2019-11-20T00:57:14Z</published>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Results"/>
    <category term="bugs"/>
    <category term="compiler"/>
    <category term="errors"/>
    <category term="security"/>
    <category term="testing"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-11-20T03:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08376</id>
    <link href="http://arxiv.org/abs/1911.08376" rel="alternate" type="text/html"/>
    <title>Extending General Compact Querieable Representations to GIS Applications</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brisaboa:Nieves_R=.html">Nieves R. Brisaboa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cerdeira=Pena:Ana.html">Ana Cerdeira-Pena</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bernardo:Guillermo_de.html">Guillermo de Bernardo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pedreira:Oscar.html">Oscar Pedreira</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08376">PDF</a><br/><b>Abstract: </b>The raster model is commonly used for the representation of images in many
domains, and is especially useful in Geographic Information Systems (GIS) to
store information about continuous variables of the space (elevation,
temperature, etc.). Current representations of raster data are usually designed
for external memory or, when stored in main memory, lack efficient query
capabilities. In this paper we propose compact representations to efficiently
store and query raster datasets in main memory. We present different
representations for binary raster data, general raster data and time-evolving
raster data. We experimentally compare our proposals with traditional storage
mechanisms such as linear quadtrees or compressed GeoTIFF files. Results show
that our structures are up to 10 times smaller than classical linear quadtrees,
and even comparable in space to non-querieable representations of raster data,
while efficiently answering a number of typical queries.
</p></div>
    </summary>
    <updated>2019-11-20T03:05:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08374</id>
    <link href="http://arxiv.org/abs/1911.08374" rel="alternate" type="text/html"/>
    <title>Concurrent Expandable AMQs on the Basis of Quotient Filters</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maier:Tobias.html">Tobias Maier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sanders:Peter.html">Peter Sanders</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williger:Robert.html">Robert Williger</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08374">PDF</a><br/><b>Abstract: </b>A quotient filter is a cache efficient AMQ data structure. Depending on the
fill degree of the filter most insertions and queries only need to access one
or two consecutive cache lines. This makes quotient filters fast compared to
the more commonly used Bloom filters that incur multiple cache misses. However,
concurrent Bloom filters are easy to implement and can be implemented lock-free
while concurrent quotient filters are not as simple. Usually concurrent
quotient filters work by using an external array of locks -- each protecting a
region of the table. Accessing this array incurs one additional cache miss per
operation. We propose a new locking scheme that has no memory overhead. Using
this new locking scheme we achieve 1.8 times higher speedups than with the
common external locking scheme.
</p>
<p>Another advantage of quotient filters over Bloom filters is that a quotient
filter can change its size when it is becoming full. We implement this growing
technique for our concurrent quotient filters and adapt it in a way that allows
unbounded growing while keeping a bounded false positive rate. We call the
resulting data structure a fully expandable quotient filter. Its design is
similar to scalable Bloom filters, but we exploit some concepts inherent to
quotient filters to improve the space efficiency and the query speed.
</p>
<p>We also propose quotient filter variants that are aimed to reduce the number
of status bits (2-status-bit variant) or to simplify concurrent implementations
(linear probing quotient filter). The linear probing quotient filter even leads
to a lock-free concurrent filter implementation. This is especially
interesting, since we show that any lock-free implementation of another common
quotient filter variant would incur significant overheads in the form of
additional data fields or multiple passes over the accessed data.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08372</id>
    <link href="http://arxiv.org/abs/1911.08372" rel="alternate" type="text/html"/>
    <title>Improved Compressed String Dictionaries</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brisaboa:Nieves_R=.html">Nieves R. Brisaboa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cerdeira=Pena:Ana.html">Ana Cerdeira-Pena</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bernardo:Guillermo_de.html">Guillermo de Bernardo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08372">PDF</a><br/><b>Abstract: </b>We introduce a new family of compressed data structures to efficiently store
and query large string dictionaries in main memory. Our main technique is a
combination of hierarchical Front-coding with ideas from longest-common-prefix
computation in suffix arrays. Our data structures yield relevant space-time
tradeoffs in real-world dictionaries. We focus on two domains where string
dictionaries are extensively used and efficient compression is required: URL
collections, a key element in Web graphs and applications such as Web mining;
and collections of URIs and literals, the basic components of RDF datasets. Our
experiments show that our data structures achieve better compression than the
state-of-the-art alternatives while providing very competitive query times.
</p></div>
    </summary>
    <updated>2019-11-20T02:53:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08339</id>
    <link href="http://arxiv.org/abs/1911.08339" rel="alternate" type="text/html"/>
    <title>The Power of Factorization Mechanisms in Local and Central Differential Privacy</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alexander Edmonds, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nikolov:Aleksandar.html">Aleksandar Nikolov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Ullman:Jonathan.html">Jonathan Ullman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08339">PDF</a><br/><b>Abstract: </b>We give new characterizations of the sample complexity of answering linear
queries (statistical queries) in the local and central models of differential
privacy:
</p>
<p>*In the non-interactive local model, we give the first approximate
characterization of the sample complexity. Informally our bounds are tight to
within polylogarithmic factors in the number of queries and desired accuracy.
Our characterization extends to agnostic learning in the local model.
</p>
<p>*In the central model, we give a characterization of the sample complexity in
the high-accuracy regime that is analogous to that of Nikolov, Talwar, and
Zhang (STOC 2013), but is both quantitatively tighter and has a dramatically
simpler proof.
</p>
<p>Our lower bounds apply equally to the empirical and population estimation
problems. In both cases, our characterizations show that a particular
factorization mechanism is approximately optimal, and the optimal sample
complexity is bounded from above and below by well studied factorization norms
of a matrix associated with the queries.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08320</id>
    <link href="http://arxiv.org/abs/1911.08320" rel="alternate" type="text/html"/>
    <title>Property Testing of LP-Type Problems</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Rogers Epstein, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silwal:Sandeep.html">Sandeep Silwal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08320">PDF</a><br/><b>Abstract: </b>Given query access to a set of constraints $S$, we wish to quickly check if
some objective function $\varphi$ subject to these constraints is at most a
given value $k$. We approach this problem using the framework of property
testing where our goal is to distinguish the case $\varphi(S) \le k$ from the
case that at least an $\epsilon$ fraction of the constraints in $S$ need to be
removed for $\varphi(S) \le k$ to hold. We restrict our attention to the case
where $(S, \varphi)$ are LP-Type problems which is a rich family of
combinatorial optimization problems with an inherent geometric structure. By
utilizing a simple sampling procedure which has been used previously to study
these problems, we are able to create property testers for any LP-Type problem
whose query complexities are independent of the number of constraints. To the
best of our knowledge, this is the first work that connects the area of LP-Type
problems and property testing in a systematic way. Among our results is a tight
upper bound on the query complexity of testing clusterability with one cluster
considered by Alon, Dar, Parnas, and Ron (FOCS 2000). We also supply a
corresponding tight lower bound for this problem and other LP-Type problems
using geometric constructions.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08297</id>
    <link href="http://arxiv.org/abs/1911.08297" rel="alternate" type="text/html"/>
    <title>Beyond Natural Proofs: Hardness Magnification and Locality</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Lijie.html">Lijie Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hirahara:Shuichi.html">Shuichi Hirahara</a>, Igor C. Oliveira, Jan Pich, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rajgopal:Ninad.html">Ninad Rajgopal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Santhanam:Rahul.html">Rahul Santhanam</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08297">PDF</a><br/><b>Abstract: </b>Hardness magnification reduces major complexity separations (such as
$\mathsf{\mathsf{EXP}} \nsubseteq \mathsf{NC}^1$) to proving lower bounds for
some natural problem $Q$ against weak circuit models. Several recent works
[OS18, MMW19, CT19, OPS19, CMMW19, Oli19, CJW19a] have established results of
this form. In the most intriguing cases, the required lower bound is known for
problems that appear to be significantly easier than $Q$, while $Q$ itself is
susceptible to lower bounds but these are not yet sufficient for magnification.
</p>
<p>In this work, we provide more examples of this phenomenon, and investigate
the prospects of proving new lower bounds using this approach. In particular,
we consider the following essential questions associated with the hardness
magnification program:
</p>
<p>Does hardness magnification avoid the natural proofs barrier of Razborov and
Rudich [RR97]?
</p>
<p>Can we adapt known lower bound techniques to establish the desired lower
bound for $Q$?
</p></div>
    </summary>
    <updated>2019-11-20T02:22:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08275</id>
    <link href="http://arxiv.org/abs/1911.08275" rel="alternate" type="text/html"/>
    <title>Corrfunc: Blazing fast correlation functions with AVX512F SIMD Intrinsics</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Manodeep Sinha, Lehman H. Garrison <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08275">PDF</a><br/><b>Abstract: </b>Correlation functions are widely used in extra-galactic astrophysics to
extract insights into how galaxies occupy dark matter halos and in cosmology to
place stringent constraints on cosmological parameters. A correlation function
fundamentally requires computing pair-wise separations between two sets of
points and then computing a histogram of the separations. Corrfunc is an
existing open-source, high-performance software package for efficiently
computing a multitude of correlation functions. In this paper, we will discuss
the SIMD AVX512F kernels within Corrfunc, capable of processing 16 floats or 8
doubles at a time. The latest manually implemented Corrfunc AVX512F kernels
show a speedup of up to $\sim 4\times$ relative to compiler-generated code for
double-precision calculations. The AVX512F kernels show $\sim 1.6\times$
speedup relative to the AVX kernels and compare favorably to a theoretical
maximum of $2\times$. In addition, by pruning pairs with too large of a minimum
possible separation, we achieve a $\sim 5-10\%$ speedup across all the SIMD
kernels. Such speedups highlight the importance of programming explicitly with
SIMD vector intrinsics for complex calculations that can not be efficiently
vectorized by compilers. Corrfunc is publicly available at
https://github.com/manodeep/Corrfunc/.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08130</id>
    <link href="http://arxiv.org/abs/1911.08130" rel="alternate" type="text/html"/>
    <title>Topological computing of arrangements with (co)chains</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paoluzzi:Alberto.html">Alberto Paoluzzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shapiro:Vadim.html">Vadim Shapiro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/DiCarlo:Antonio.html">Antonio DiCarlo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Furiani:Francesco.html">Francesco Furiani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martella:Giulio.html">Giulio Martella</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scorzelli:Giorgio.html">Giorgio Scorzelli</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08130">PDF</a><br/><b>Abstract: </b>In many areas of applied geometric/numeric computational mathematics,
including geo-mapping, computer vision, computer graphics, finite element
analysis, medical imaging, geometric design, and solid modeling, one has to
compute incidences, adjacencies and ordering of cells, generally using
disparate and often incompatible data structures and algorithms. This paper
introduces computational topology algorithms to discover the 2D/3D space
partition induced by a collection of geometric objects of dimension 1D/2D,
respectively. Methods and language are those of basic geometric and algebraic
topology. Only sparse vectors and matrices are used to compute both spaces and
maps, i.e., the chain complex, from dimension zero to three.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08101</id>
    <link href="http://arxiv.org/abs/1911.08101" rel="alternate" type="text/html"/>
    <title>Two-message verification of quantum computation</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alagic:Gorjan.html">Gorjan Alagic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Childs:Andrew_M=.html">Andrew M. Childs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hung:Shih=Han.html">Shih-Han Hung</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08101">PDF</a><br/><b>Abstract: </b>We describe a two-message protocol that enables a purely classical verifier
to delegate any quantum computation to an untrusted quantum prover. The
protocol begins with the verifier publishing a problem instance together with a
public cryptographic key. The prover then transmits the computation result,
appropriately encoded. Finally, the verifier uses their private key to detect
any cheating and extract the result.
</p>
<p>We achieve this by upgrading the verification protocol of Mahadev in two
steps. First, the protocol is repeated many times in parallel, yielding a
four-message protocol with negligible soundness error. This enables the second
step: the "challenge round" is eliminated via the Fiat-Shamir transform, in
which the prover computes their own challenges using a public hash function.
</p>
<p>We show that this protocol is secure under the same assumptions underlying
many candidate schemes for post-quantum public-key cryptography. Specifically,
it is secure in the Quantum Random Oracle Model, and assuming the quantum
hardness of the Learning with Errors problem. The main technical advance in our
security proof is a parallel repetition theorem for the Mahadev protocol.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08085</id>
    <link href="http://arxiv.org/abs/1911.08085" rel="alternate" type="text/html"/>
    <title>Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diakonikolas:Ilias.html">Ilias Diakonikolas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karmalkar:Sushrut.html">Sushrut Karmalkar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel.html">Daniel Kane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Price:Eric.html">Eric Price</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stewart:Alistair.html">Alistair Stewart</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08085">PDF</a><br/><b>Abstract: </b>We study high-dimensional sparse estimation tasks in a robust setting where a
constant fraction of the dataset is adversarially corrupted. Specifically, we
focus on the fundamental problems of robust sparse mean estimation and robust
sparse PCA. We give the first practically viable robust estimators for these
problems. In more detail, our algorithms are sample and computationally
efficient and achieve near-optimal robustness guarantees. In contrast to prior
provable algorithms which relied on the ellipsoid method, our algorithms use
spectral techniques to iteratively remove outliers from the dataset. Our
experimental evaluation on synthetic data shows that our algorithms are
scalable and significantly outperform a range of previous approaches, nearly
matching the best error rate without corruptions.
</p></div>
    </summary>
    <updated>2019-11-20T02:24:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08043</id>
    <link href="http://arxiv.org/abs/1911.08043" rel="alternate" type="text/html"/>
    <title>Mapping NP-hard and NP-complete optimisation problems to Quadratic Unconstrained Binary Optimisation problems</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Bas Lodewijks <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08043">PDF</a><br/><b>Abstract: </b>We discuss several mappings from well-known NP-hard problems to Quadratic
Unconstrained Binary Optimisation problems which are treated incorrectly by
Lucas in \cite{Luc14}. We provide counterexamples and correct the mappings. We
also extend the body of QUBO formulations of NP-complete and NP-hard
optimisation problems by discussing additional problems.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08015</id>
    <link href="http://arxiv.org/abs/1911.08015" rel="alternate" type="text/html"/>
    <title>Low-Rank Toeplitz Matrix Estimation via Random Ultra-Sparse Rulers</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lawrence:Hannah.html">Hannah Lawrence</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jerry.html">Jerry Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musco:Cameron.html">Cameron Musco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musco:Christopher.html">Christopher Musco</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08015">PDF</a><br/><b>Abstract: </b>We study how to estimate a nearly low-rank Toeplitz covariance matrix $T$
from compressed measurements. Recent work of Qiao and Pal addresses this
problem by combining sparse rulers (sparse linear arrays) with frequency
finding (sparse Fourier transform) algorithms applied to the Vandermonde
decomposition of $T$. Analytical bounds on the sample complexity are shown,
under the assumption of sufficiently large gaps between the frequencies in this
decomposition. In this work, we introduce random ultra-sparse rulers and
propose an improved approach based on these objects. Our random rulers
effectively apply a random permutation to the frequencies in $T$'s Vandermonde
decomposition, letting us avoid frequency gap assumptions and leading to
improved sample complexity bounds. In the special case when $T$ is circulant,
we theoretically analyze the performance of our method when combined with
sparse Fourier transform algorithms based on random hashing. We also show
experimentally that our ultra-sparse rulers give significantly more robust and
sample efficient estimation then baseline methods.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.08004</id>
    <link href="http://arxiv.org/abs/1911.08004" rel="alternate" type="text/html"/>
    <title>Consistent recovery threshold of hidden nearest neighbor graphs</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Jian.html">Jian Ding</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Yihong.html">Yihong Wu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Jiaming.html">Jiaming Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Dana.html">Dana Yang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.08004">PDF</a><br/><b>Abstract: </b>Motivated by applications such as discovering strong ties in social networks
and assembling genome subsequences in biology, we study the problem of
recovering a hidden $2k$-nearest neighbor (NN) graph in an $n$-vertex complete
graph, whose edge weights are independent and distributed according to $P_n$
for edges in the hidden $2k$-NN graph and $Q_n$ otherwise. The special case of
Bernoulli distributions corresponds to a variant of the Watts-Strogatz
small-world graph. We focus on two types of asymptotic recovery guarantees as
$n\to \infty$: (1) exact recovery: all edges are classified correctly with
probability tending to one; (2) almost exact recovery: the expected number of
misclassified edges is $o(nk)$. We show that the maximum likelihood estimator
achieves (1) exact recovery for $2 \le k \le n^{o(1)}$ if $ \liminf
\frac{2\alpha_n}{\log n}&gt;1$; (2) almost exact recovery for $ 1 \le k \le
o\left( \frac{\log n}{\log \log n} \right)$ if $\liminf
\frac{kD(P_n||Q_n)}{\log n}&gt;1$, where $\alpha_n \triangleq -2 \log \int \sqrt{d
P_n d Q_n}$ is the R\'enyi divergence of order $\frac{1}{2}$ and $D(P_n||Q_n)$
is the Kullback-Leibler divergence. Under mild distributional assumptions,
these conditions are shown to be information-theoretically necessary for any
algorithm to succeed. A key challenge in the analysis is the enumeration of
$2k$-NN graphs that differ from the hidden one by a given number of edges.
</p></div>
    </summary>
    <updated>2019-11-20T02:28:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07981</id>
    <link href="http://arxiv.org/abs/1911.07981" rel="alternate" type="text/html"/>
    <title>New lower bounds for matrix multiplication and the 3x3 determinant</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Conner:Austin.html">Austin Conner</a>, Alicia Harper, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Landsberg:J=_M=.html">J. M. Landsberg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07981">PDF</a><br/><b>Abstract: </b>Let $M_{\langle u,v,w\rangle}\in C^{uv}\otimes C^{vw}\otimes C^{wu}$ denote
the matrix multiplication tensor (and write $M_n=M_{\langle n,n,n\rangle}$) and
let $det_3\in ( C^9)^{\otimes 3}$ denote the determinant polynomial considered
as a tensor. For a tensor $T$, let $\underline R(T)$ denote its border rank. We
(i) give the first hand-checkable algebraic proof that $\underline
R(M_2)=7$,(ii) prove $\underline R(M_{\langle 223\rangle})=10$, and $\underline
R(M_{\langle 233\rangle})=14$, where previously the only nontrivial matrix
multiplication tensor whose border rank had been determined was $M_2$,(iii)
prove $\underline R( M_3)\geq 17$, (iv) prove $\underline R( det_3)=17$,
improving the previous lower bound of $12$, (v) prove $\underline R(M_{\langle
2nn\rangle})\geq n^2+1.32n$ for all $n\geq 25$ (previously only $\underline
R(M_{\langle 2nn\rangle})\geq n^2+1$ was known) as well as lower bounds for
$4\leq n\leq 25$, and (vi) prove $\underline R(M_{\langle 3nn\rangle})\geq
n^2+2 n+1$ for all $ n\geq 21$, where previously only $\underline R(M_{\langle
3nn\rangle})\geq n^2+2$ was known, as well as lower boundsfor $4\leq n\leq 21$.
</p>
<p>Our results utilize a new technique initiated by Buczy\'{n}ska and
Buczy\'{n}ski, called border apolarity. The two key ingredients are: (i) the
use of a multi-graded ideal associated to a border rank $r$ decomposition of
any tensor, and (ii) the exploitation of the large symmetry group of $T$ to
restrict to $B_T$-invariant ideals, where $B_T$ is a maximal solvable subgroup
of the symmetry group of $T$.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07976</id>
    <link href="http://arxiv.org/abs/1911.07976" rel="alternate" type="text/html"/>
    <title>Estimating Entropy of Distributions in Constant Space</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Acharya:Jayadev.html">Jayadev Acharya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhadane:Sourbh.html">Sourbh Bhadane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Ziteng.html">Ziteng Sun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07976">PDF</a><br/><b>Abstract: </b>We consider the task of estimating the entropy of $k$-ary distributions from
samples in the streaming model, where space is limited. Our main contribution
is an algorithm that requires $O\left(\frac{k \log
(1/\varepsilon)^2}{\varepsilon^3}\right)$ samples and a constant $O(1)$ memory
words of space and outputs a $\pm\varepsilon$ estimate of $H(p)$. Without space
limitations, the sample complexity has been established as
$S(k,\varepsilon)=\Theta\left(\frac k{\varepsilon\log k}+\frac{\log^2
k}{\varepsilon^2}\right)$, which is sub-linear in the domain size $k$, and the
current algorithms that achieve optimal sample complexity also require
nearly-linear space in $k$.
</p>
<p>Our algorithm partitions $[0,1]$ into intervals and estimates the entropy
contribution of probability values in each interval. The intervals are designed
to trade off the bias and variance of these estimates.
</p></div>
    </summary>
    <updated>2019-11-20T03:03:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07972</id>
    <link href="http://arxiv.org/abs/1911.07972" rel="alternate" type="text/html"/>
    <title>Learning-Assisted Competitive Algorithms for Peak-Aware Energy Scheduling</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Russell Lee, Mohammad H. Hajiesmaili, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jian.html">Jian Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07972">PDF</a><br/><b>Abstract: </b>In this paper, we study the peak-aware energy scheduling problem using the
competitive framework with machine learning prediction. With the uncertainty of
energy demand as the fundamental challenge, the goal is to schedule the energy
output of local generation units such that the electricity bill is minimized.
While this problem has been tackled using classic competitive design with
worst-case guarantee, the goal of this paper is to develop learning-assisted
competitive algorithms to improve the performance in a provable manner. We
develop two deterministic and randomized algorithms that are provably robust
against the poor performance of learning prediction, however, achieve the
optimal performance as the error of prediction goes to zero. Extensive
experiments using real data traces verify our theoretical observations and show
15.13% improved performance against pure online algorithms.
</p></div>
    </summary>
    <updated>2019-11-20T02:30:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07945</id>
    <link href="http://arxiv.org/abs/1911.07945" rel="alternate" type="text/html"/>
    <title>Optimal Single-Choice Prophet Inequalities from Samples</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinstein:Aviad.html">Aviad Rubinstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Jack_Z=.html">Jack Z. Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinberg:S=_Matthew.html">S. Matthew Weinberg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07945">PDF</a><br/><b>Abstract: </b>We study the single-choice Prophet Inequality problem when the gambler is
given access to samples. We show that the optimal competitive ratio of $1/2$
can be achieved with a single sample from each distribution. When the
distributions are identical, we show that for any constant $\varepsilon &gt; 0$,
$O(n)$ samples from the distribution suffice to achieve the optimal competitive
ratio ($\approx 0.745$) within $(1+\varepsilon)$, resolving an open problem of
Correa, D\"utting, Fischer, and Schewior.
</p></div>
    </summary>
    <updated>2019-11-20T02:26:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07504</id>
    <link href="http://arxiv.org/abs/1911.07504" rel="alternate" type="text/html"/>
    <title>Minimum-Width Double-Strip and Parallelogram Annulus</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bae:Sang_Won.html">Sang Won Bae</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07504">PDF</a><br/><b>Abstract: </b>In this paper, we study the problem of computing a minimum-width double-strip
or parallelogram annulus that encloses a given set of $n$ points in the plane.
A double-strip is a closed region in the plane whose boundary consists of four
parallel lines and a parallelogram annulus is a closed region between two
edge-parallel parallelograms. We present several first algorithms for these
problems. Among them are $O(n^2)$ and $O(n^3 \log n)$-time algorithms that
compute a minimum-width double-strip and parallelogram annulus, respectively,
when their orientations can be freely chosen.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07352</id>
    <link href="http://arxiv.org/abs/1911.07352" rel="alternate" type="text/html"/>
    <title>Robust Algorithms for the Secretary Problem</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bradac:Domagoj.html">Domagoj Bradac</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zuzic:Goran.html">Goran Zuzic</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07352">PDF</a><br/><b>Abstract: </b>In classical secretary problems, a sequence of $n$ elements arrive in a
uniformly random order, and we want to choose a single item, or a set of size
$K$. The random order model allows us to escape from the strong lower bounds
for the adversarial order setting, and excellent algorithms are known in this
setting. However, one worrying aspect of these results is that the algorithms
overfit to the model: they are not very robust. Indeed, if a few "outlier"
arrivals are adversarially placed in the arrival sequence, the algorithms
perform poorly. E.g., Dynkin's popular $1/e$-secretary algorithm fails with
even a single adversarial arrival.
</p>
<p>We investigate a robust version of the secretary problem. In the Byzantine
Secretary model, we have two kinds of elements: green (good) and red (rogue).
The values of all elements are chosen by the adversary. The green elements
arrive at times uniformly randomly drawn from $[0,1]$. The red elements,
however, arrive at adversarially chosen times. Naturally, the algorithm does
not see these colors: how well can it solve secretary problems?
</p>
<p>We give algorithms which get value comparable to the value of the optimal
green set minus the largest green item. Specifically, we give an algorithm to
pick $K$ elements that gets within $(1-\varepsilon)$ factor of the above
benchmark, as long as $K \geq \mathrm{poly}(\varepsilon^{-1} \log n)$. We
extend this to the knapsack secretary problem, for large knapsack size $K$.
</p>
<p>For the single-item case, an analogous benchmark is the value of the
second-largest green item. For value-maximization, we give a $\mathrm{poly}
\log^* n$-competitive algorithm, using a multi-layered bucketing scheme that
adaptively refines our estimates of second-max over time. For
probability-maximization, we show the existence of a good randomized algorithm,
using the minimax principle.
</p></div>
    </summary>
    <updated>2019-11-20T00:04:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07306</id>
    <link href="http://arxiv.org/abs/1911.07306" rel="alternate" type="text/html"/>
    <title>Quantum Speedup for Graph Sparsification, Cut Approximation and Laplacian Solving</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Apers:Simon.html">Simon Apers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wolf:Ronald_de.html">Ronald de Wolf</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07306">PDF</a><br/><b>Abstract: </b>Graph sparsification underlies a large number of algorithms, ranging from
approximation algorithms for cut problems to solvers for linear systems in the
graph Laplacian. In its strongest form, "spectral sparsification" reduces the
number of edges to near-linear in the number of nodes, while approximately
preserving the cut and spectral structure of the graph. The breakthrough work
by Bencz\'ur and Karger (STOC'96) and Spielman and Teng (STOC'04) showed that
sparsification can be done optimally in time near-linear in the number of edges
of the original graph.
</p>
<p>In this work we show that quantum algorithms allow to speed up spectral
sparsification, and thereby many of the derived algorithms. Given
adjacency-list access to a weighted graph with $n$ nodes and $m$ edges, our
algorithm outputs an $\epsilon$-spectral sparsifier in time
$\widetilde{O}(\sqrt{mn}/\epsilon)$. We prove that this is tight up to
polylog-factors. The algorithm builds on a string of existing results, most
notably sparsification algorithms by Spielman and Srivastava (STOC'08) and
Koutis and Xu (TOPC'16), a spanner construction by Thorup and Zwick (STOC'01),
a single-source shortest-paths quantum algorithm by D\"urr et al. (ICALP'04)
and an efficient $k$-wise independent hash construction by Christiani, Pagh and
Thorup (STOC'15). Combining our sparsification algorithm with existing
classical algorithms yields the first quantum speedup, roughly from
$\widetilde{O}(m)$ to $\widetilde{O}(\sqrt{mn})$, for approximating the max
cut, min cut, min $st$-cut, sparsest cut and balanced separator of a graph.
Combining our algorithm with a classical Laplacian solver, we demonstrate a
similar speedup for Laplacian solving, for approximating effective resistances,
cover times and eigenvalues of the Laplacian, and for spectral clustering.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07287</id>
    <link href="http://arxiv.org/abs/1911.07287" rel="alternate" type="text/html"/>
    <title>A Crossing Lemma for Families of Jordan Curves with a Bounded Intersection Number</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Maya Bechler-Speicher <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07287">PDF</a><br/><b>Abstract: </b>A family of closed simple (i.e., Jordan) curves is {\it $m$-intersecting} if
any pair of its curves have at most $m$ points of common intersection. We say
that a pair of such curves {\it touch} if they intersect at a single point of
common tangency. In this work we show that any $m$-intersecting family of $n$
Jordan curves in general position in the plane contains
$O\left(n^{2-\frac{1}{3m+15}}\right)$ touching pairs.\footnote{A family of
Jordan curves is in general position if no three of its curves pass through the
same point, and no two of them overlap. The constant of proportionality with
the $O(\cdot)$-notation may depend on $m$.}
</p>
<p>Furthermore, we use the string separator theorem of Fox and Pach \cite{FP10}
in order to establish the following Crossing Lemma for contact graphs of Jordan
curves: Let $\Gamma$ be an $m$-intersecting family of closed Jordan curves in
general position in the plane with exactly $T=\Omega(n)$ touching pairs of
curves, then the curves of $\Gamma$ determine
$\Omega\left(T\cdot\left(\frac{T}{n}\right)^{\frac{1}{9m+45}}\right)$
intersection points.
</p>
<p>This extends the similar bounds that were previously established by Salazar
for the special case of pairwise intersecting (and $m$-intersecting) curves.
Specializing to the case at hand, this substantially improves the bounds that
were recently derived by Pach, Rubin and Tardos for arbitrary families of
Jordan curves.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07255</id>
    <link href="http://arxiv.org/abs/1911.07255" rel="alternate" type="text/html"/>
    <title>Deep geometric matrix completion: Are we doing it right?</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boyarski:Amit.html">Amit Boyarski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vedula:Sanketh.html">Sanketh Vedula</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bronstein:Alex.html">Alex Bronstein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07255">PDF</a><br/><b>Abstract: </b>We address the problem of reconstructing a matrix from a subset of its
entries. Current methods, branded as geometric matrix completion, augment
classical rank regularization techniques by incorporating geometric information
into the solution. This information is usually provided as graphs encoding
relations between rows/columns. In this work we propose a simple spectral
approach for solving the matrix completion problem, via the framework of
functional maps. We introduce the zoomout loss, a multiresolution spectral
geometric loss inspired by recent advances in shape correspondence, whose
minimization leads to state-of-the-art results on various recommender systems
datasets. Surprisingly, for some datasets we were able to achieve comparable
results even without incorporating geometric information. This puts into
question both the quality of such information and current methods' ability to
use it in a meaningful and efficient way.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07166</id>
    <link href="http://arxiv.org/abs/1911.07166" rel="alternate" type="text/html"/>
    <title>On the existence of four or more curved foldings with common creases and crease patterns</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Honda:Atsufumi.html">Atsufumi Honda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Naokawa:Kosuke.html">Kosuke Naokawa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saji:Kentaro.html">Kentaro Saji</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Umehara:Masaaki.html">Masaaki Umehara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yamada:Kotaro.html">Kotaro Yamada</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07166">PDF</a><br/><b>Abstract: </b>Consider a curve $\Gamma$ in a domain $D$ in the plane $\boldsymbol{R}^2$.
Thinking of $D$ as a piece of paper, one can make a curved folding in the
Euclidean space $\boldsymbol{R}^3$. This can be expressed as the image of an
`origami map' $\varphi:D\to \boldsymbol{R}^3$ such that $\Gamma$ is the
singular set of $\varphi$, the word `origami' coming from the Japanese term for
paper folding. We call the singular set image $C:=\varphi(\Gamma)$ the crease
of $\varphi$ and the singular set $\Gamma$ the crease pattern of $\varphi$. We
are interested in the number of origami maps whose creases and crease patterns
are $C$ and $\Gamma$ respectively. Two such possibilities have been known. In
the previous authors' work, two other new possibilities and an explicit example
with four such non-congruent distinct curved foldings were established. In this
paper, we show that the case of four mutually non-congruent curved foldings
with the same crease and crease pattern occurs if and only if $\Gamma$ and $C$
do not admit any symmetries. Moreover, when $C$ is a closed curve, we show that
there are infinitely many distinct possibilities for curved foldings with the
same crease and crease pattern, in general.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.06924</id>
    <link href="http://arxiv.org/abs/1911.06924" rel="alternate" type="text/html"/>
    <title>Approximating the Distance to Monotonicity of Boolean Functions</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pallavoor:Ramesh_Krishnan_S=.html">Ramesh Krishnan S. Pallavoor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raskhodnikova:Sofya.html">Sofya Raskhodnikova</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Waingarten:Erik.html">Erik Waingarten</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06924">PDF</a><br/><b>Abstract: </b>We design a nonadaptive algorithm that, given a Boolean function $f\colon
\{0,1\}^n \to \{0,1\}$ which is $\alpha$-far from monotone, makes poly$(n,
1/\alpha)$ queries and returns an estimate that, with high probability, is an
$\widetilde{O}(\sqrt{n})$-approximation to the distance of $f$ to monotonicity.
Furthermore, we show that for any constant $\kappa &gt; 0,$ approximating the
distance to monotonicity up to $n^{1/2 - \kappa}$-factor requires
$2^{n^\kappa}$ nonadaptive queries, thereby ruling out a poly$(n,
1/\alpha)$-query nonadaptive algorithm for such approximations. This answers a
question of Seshadhri (Property Testing Review, 2014) for the case of
nonadaptive algorithms. Approximating the distance to a property is closely
related to tolerantly testing that property. Our lower bound stands in contrast
to standard (non-tolerant) testing of monotonicity that can be done
nonadaptively with $\widetilde{O}(\sqrt{n} / \varepsilon^2)$ queries.
</p>
<p>We obtain our lower bound by proving an analogous bound for erasure-resilient
testers. An $\alpha$-erasure-resilient tester for a desired property gets
oracle access to a function that has at most an $\alpha$ fraction of values
erased. The tester has to accept (with probability at least 2/3) if the
erasures can be filled in to ensure that the resulting function has the
property and to reject (with probability at least 2/3) if every completion of
erasures results in a function that is $\varepsilon$-far from having the
property. Our method yields the same lower bounds for unateness and being a
$k$-junta. These lower bounds improve exponentially on the existing lower
bounds for these properties.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.06907</id>
    <link href="http://arxiv.org/abs/1911.06907" rel="alternate" type="text/html"/>
    <title>Strategy-Stealing is Non-Constructive</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodwin:Greg.html">Greg Bodwin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grossman:Ofer.html">Ofer Grossman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.06907">PDF</a><br/><b>Abstract: </b>In many combinatorial games, one can prove that the first player wins under
best play using a simple but non-constructive argument called
strategy-stealing. This work is about the complexity behind these proofs: how
hard is it to actually find a winning move in a game, when you know by
strategy-stealing that one exists? We prove that this problem is PSPACE-hard
already for Minimum Poset Games and Symmetric Maker-Maker Games, which are
simple classes of games that capture two of the main types of strategy-stealing
arguments in the current literature.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.09415</id>
    <link href="http://arxiv.org/abs/1907.09415" rel="alternate" type="text/html"/>
    <title>Quantum Computing: Lecture Notes</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wolf:Ronald_de.html">Ronald de Wolf</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.09415">PDF</a><br/><b>Abstract: </b>This is a set of lecture notes suitable for a Master's course on quantum
computation and information from the perspective of theoretical computer
science. The first version was written in 2011, with many extensions and
improvements in subsequent years. The first 10 chapters cover the circuit model
and the main quantum algorithms (Deutsch-Jozsa, Simon, Shor, Hidden Subgroup
Problem, Grover, quantum walks, Hamiltonian simulation and HHL). They are
followed by 2 chapters about complexity, 4 chapters about distributed ("Alice
and Bob") settings, and a final chapter about quantum error correction.
Appendices A and B give a brief introduction to the required linear algebra and
some other mathematical and computer science background. All chapters come with
exercises, with some hints provided in Appendix C.
</p></div>
    </summary>
    <updated>2019-11-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1805.00506</id>
    <link href="http://arxiv.org/abs/1805.00506" rel="alternate" type="text/html"/>
    <title>Adaptive View Planning for Aerial 3D Reconstruction</title>
    <feedworld_mtime>1574208000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Cheng.html">Cheng Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Isler:Volkan.html">Volkan Isler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1805.00506">PDF</a><br/><b>Abstract: </b>With the proliferation of small aerial vehicles, acquiring close up aerial
imagery for high quality reconstruction of complex scenes is gaining
importance. We present an adaptive view planning method to collect such images
in an automated fashion. We start by sampling a small set of views to build a
coarse proxy to the scene. We then present (i)~a method that builds a view
manifold for view selection, and (ii) an algorithm to select a sparse set of
views. The vehicle then visits these viewpoints to cover the scene, and the
procedure is repeated until reconstruction quality converges or a desired level
of quality is achieved. The view manifold provides an effective
efficiency/quality compromise between using the entire 6 degree of freedom pose
space and using a single view hemisphere to select the views.
</p>
<p>Our results show that, in contrast to existing "explore and exploit" methods
which collect only two sets of views, reconstruction quality can be drastically
improved by adding a third set. They also indicate that three rounds of data
collection is sufficient even for very complex scenes. We compare our algorithm
to existing methods in three challenging scenes. We require each algorithm to
select the same number of views. Our algorithm generates views which produce
the least reconstruction error.
</p></div>
    </summary>
    <updated>2019-11-20T03:16:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-11-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3443</id>
    <link href="https://agtb.wordpress.com/2019/11/19/a-market-for-tcs-papers/" rel="alternate" type="text/html"/>
    <title>A Market for TCS Papers??</title>
    <summary>By David Eppstein &amp; Vijay Vazirani No, not to make theoreticians rich! Besides, who will buy your papers anyway? (Quite the opposite, you will be lucky if you can convince someone to take them for free, just for sake of publicity!) What we are proposing is a market in which no money changes hands – […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>By David Eppstein &amp; Vijay Vazirani</em></p>
<p>No, not to make theoreticians rich! Besides, who will buy your papers anyway? (Quite the opposite, you will be lucky if you can convince someone to take them for free, just for sake of publicity!) What we are proposing is a market in which no money changes hands – a matching market – for matching papers to conferences.</p>
<p>First, a short preamble on how the idea emerged.</p>
<p><strong>Preamble</strong> (by Vijay):  Soon after my recent <a href="https://www.youtube.com/watch?v=MLr6Ud5qmt4&amp;t=74s"><u>Simons talk</u></a> on Matching Markets, I sent its url to Al Roth. Obviously, I wasn’t expecting a return email. However, the perfect gentleman and ultimate scholar that Al is, he did reply, and mentioned that he did not like my “definition” of matching markets and said, “I guess I would say matching markets are markets because they aggregate information that is held by the participants, which is what markets do (even if they don’t use prices to do it..).” This hit me like lightening from the sky – suddenly it crystallized the innate intuition about markets which I had formed through work on algorithmic aspects of markets! I thanked Al profusely and added, “This definitely helps in me get the right perspective on the notion!”</p>
<p>About a week ago, while updating my talk for a seminar at Columbia University, I included this beautiful insight in it and then a thought occurred: Each PC meeting involves aggregation of information from a large number of agents: PC members as well as external experts. Hence, isn’t a conference a matching market? Excitedly, I sent this question to Al. He replied, “… the conference process, matching papers to conferences, is a market and a particular conference might be a marketplace … ”</p>
<p>When I returned home, my esteemed colleague, David Eppstein, stunned me by declaring that he had thought of a market relevant to our field in which no money changes hands. I immediately knew he was thinking of the conference process. But he got to it out of the blue … and not the long process it took me!</p>
<p><strong>Back to the idea:  </strong>In the past, matching markets have brought immense efficiency and order in allocation problems in which use of money is considered repugnant, the prime examples being matching medical residents to hospitals, kidney exchange, and assignment of students of a large city to its schools.</p>
<p>At present we are faced with massive inefficiencies in the conference process – numerous researchers are trapped in unending cycles of submit … get reject … incorporate comments … resubmit — often to the next deadline which has been conveniently arranged a couple of days down the road so the unwitting participants are conditioned into mindlessly keep coming back for more, much like Pavlov’s dog.</p>
<p>We are proposing a matching market approach to finally obliterate this madness. We believe such a market is feasible using the following ideas. No doubt our scheme will have some drawbacks; however, as should be obvious, the advantages far outweigh them.</p>
<p>First, for co-located symposia within a larger umbrella conference, such as the<br/>
conferences within ALGO or FCRC, the following process should be a no-brainer:</p>
<p>1). Ensure a common deadline for all symposia; denote the latter by <em>S.</em></p>
<p>2). Let <em>R</em> denote the set of researchers who wish to submit one paper to a symposium in this umbrella conference – assume that researchers submitting more than one paper will have multiple names, one for each submission. Each researcher will provide a strict preference order over the subset of symposia to which they wish to submit their paper. Let <em>G</em> denote the bipartite graph with vertex sets (<em>R, S</em>) and an edge (<em>r, s</em>) only if researcher <em>r</em> chose symposium <em>s.</em></p>
<p>3). The umbrella conference will have a large common PC with experts representing all of its symposia. The process of assigning papers to PC members will of course use <em>G</em> in a critical way.</p>
<p>Once papers are reviewed by PC members and external reviewers, each symposium will rank its submissions using its own criteria of acceptance. We believe the overhead of ranking each paper multiple times is minimal since that is just an issue of deciding how “on-topic” a paper is – an easy task once the reviews of the paper are available.</p>
<p>4). Finally, using all these preference lists, a researcher-proposing stable matching is computed using the Gale-Shapley algorithm. As is well-known, this mechanism will be dominant strategy incentive compatible for researchers.</p>
<p>With a little extra effort, a similar scheme can also be used for a group of conferences at diverse locations but similar times, such as some of the annual summer theory conferences, STOC, ICALP, ESA, STAC, WADS/SWAT, etc.</p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2019-11-19T01:31:12Z</updated>
    <published>2019-11-19T01:31:12Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Kevin Leyton-Brown</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-11-20T03:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07536</id>
    <link href="http://arxiv.org/abs/1911.07536" rel="alternate" type="text/html"/>
    <title>Time-inconsistent Planning: Simple Motivation Is Hard to Find</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Str=oslash=mme:Torstein_J=_F=.html">Torstein J. F. Strømme</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07536">PDF</a><br/><b>Abstract: </b>With the introduction of the graph-theoretic time-inconsistent planning model
due to Kleinberg and Oren, it has been possible to investigate the
computational complexity of how a task designer best can support a
present-biased agent in completing the task. In this paper, we study the
complexity of finding a choice reduction for the agent; that is, how to remove
edges and vertices from the task graph such that a present-biased agent will
remain motivated to reach his target even for a limited reward. While this
problem is NP-complete in general, this is not necessarily true for instances
which occur in practice, or for solutions which are of interest to task
designers. For instance, a task designer may desire to find the best task graph
which is not too complicated.
</p>
<p>We therefore investigate the problem of finding simple motivating subgraphs.
These are structures where the agent will modify his plan at most $k$ times
along the way. We quantify this simplicity in the time-inconsistency model as a
structural parameter: The number of branching vertices (vertices with
out-degree at least $2$) in a minimal motivating subgraph.
</p>
<p>Our results are as follows: We give a linear algorithm for finding an optimal
motivating path, i.e. when $k=0$. On the negative side, we show that finding a
simple motivating subgraph is NP-complete even if we allow only a single
branching vertex --- revealing that simple motivating subgraphs are indeed hard
to find. However, we give a pseudo-polynomial algorithm for the case when $k$
is fixed and edge weights are rationals, which might be a reasonable assumption
in practice.
</p></div>
    </summary>
    <updated>2019-11-19T23:23:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07465</id>
    <link href="http://arxiv.org/abs/1911.07465" rel="alternate" type="text/html"/>
    <title>Implicit Enumeration of Topological-Minor-Embeddings and Its Application to Planar Subgraph Enumeration</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakahata:Yu.html">Yu Nakahata</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kawahara:Jun.html">Jun Kawahara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Horiyama:Takashi.html">Takashi Horiyama</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Minato:Shin=ichi.html">Shin-ichi Minato</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07465">PDF</a><br/><b>Abstract: </b>Given graphs $G$ and $H$, we propose a method to implicitly enumerate
topological-minor-embeddings of $H$ in $G$ using decision diagrams. We show a
useful application of our method to enumerating subgraphs characterized by
forbidden topological minors, that is, planar, outerplanar, series-parallel,
and cactus subgraphs. Computational experiments show that our method can find
all planar subgraphs in a given graph at most five orders of magnitude faster
than a naive backtracking-based method.
</p></div>
    </summary>
    <updated>2019-11-19T23:37:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07417</id>
    <link href="http://arxiv.org/abs/1911.07417" rel="alternate" type="text/html"/>
    <title>Algorithmic Discrepancy Minimization</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Whitmeyer:Michael.html">Michael Whitmeyer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Jonathan.html">Jonathan Liu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07417">PDF</a><br/><b>Abstract: </b>This report will be a literature review on a result in algorithmic
discrepancy theory. We will begin by providing a quick overview on discrepancy
theory and some major results in the field, and then focus on an important
result by Shachar Lovett and Raghu Meka. We restate the main algorithm and
ideas of the paper, and rewrite proofs for some of the major results in the
paper.
</p></div>
    </summary>
    <updated>2019-11-19T23:37:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07378</id>
    <link href="http://arxiv.org/abs/1911.07378" rel="alternate" type="text/html"/>
    <title>Finding Skewed Subcubes Under a Distribution</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gopalan:Parikshit.html">Parikshit Gopalan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levin:Roie.html">Roie Levin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wieder:Udi.html">Udi Wieder</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07378">PDF</a><br/><b>Abstract: </b>Say that we are given samples from a distribution $\psi$ over an
$n$-dimensional space. We expect or desire $\psi$ to behave like a product
distribution (or a $k$-wise independent distribution over its marginals for
small $k$). We propose the problem of enumerating/list-decoding all large
subcubes where the distribution $\psi$ deviates markedly from what we expect;
we refer to such subcubes as skewed subcubes. Skewed subcubes are certificates
of dependencies between small subsets of variables in $\psi$. We motivate this
problem by showing that it arises naturally in the context of algorithmic
fairness and anomaly detection.
</p>
<p>In this work we focus on the special but important case where the space is
the Boolean hypercube, and the expected marginals are uniform. We show that the
obvious definition of skewed subcubes can lead to intractable list sizes, and
propose a better definition of a minimal skewed subcube, which are subcubes
whose skew cannot be attributed to a larger subcube that contains it. Our main
technical contribution is a list-size bound for this definition and an
algorithm to efficiently find all such subcubes. Both the bound and the
algorithm rely on Fourier-analytic techniques, especially the powerful
hypercontractive inequality.
</p>
<p>On the lower bounds side, we show that finding skewed subcubes is as hard as
the sparse noisy parity problem, and hence our algorithms cannot be improved on
substantially without a breakthrough on this problem which is believed to be
intractable. Motivated by this, we study alternate models allowing query access
to $\psi$ where finding skewed subcubes might be easier.
</p></div>
    </summary>
    <updated>2019-11-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07375</id>
    <link href="http://arxiv.org/abs/1911.07375" rel="alternate" type="text/html"/>
    <title>Top-down induction of decision trees: rigorous guarantees and inherent limitations</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, Jane Lange, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07375">PDF</a><br/><b>Abstract: </b>Consider the following heuristic for building a decision tree for a function
$f : \{0,1\}^n \to \{\pm 1\}$. Place the most influential variable $x_i$ of $f$
at the root, and recurse on the subfunctions $f_{x_i=0}$ and $f_{x_i=1}$ on the
left and right subtrees respectively; terminate once the tree is an
$\varepsilon$-approximation of $f$. We analyze the quality of this heuristic,
obtaining near-matching upper and lower bounds:
</p>
<p>$\circ$ Upper bound: For every $f$ with decision tree size $s$ and every
$\varepsilon \in (0,\frac1{2})$, this heuristic builds a decision tree of size
at most $s^{O(\log(s/\varepsilon)\log(1/\varepsilon))}$.
</p>
<p>$\circ$ Lower bound: For every $\varepsilon \in (0,\frac1{2})$ and $s \le
2^{\tilde{O}(\sqrt{n})}$, there is an $f$ with decision tree size $s$ such that
this heuristic builds a decision tree of size $s^{\tilde{\Omega}(\log s)}$.
</p>
<p>We also obtain upper and lower bounds for monotone functions:
$s^{O(\sqrt{\log s}/\varepsilon)}$ and $s^{\tilde{\Omega}(\sqrt[4]{\log s } )}$
respectively. The lower bound disproves conjectures of Fiat and Pechyony (2004)
and Lee (2009).
</p>
<p>Our upper bounds yield new algorithms for properly learning decision trees
under the uniform distribution. We show that these algorithms---which are
motivated by widely employed and empirically successful top-down decision tree
learning heuristics such as ID3, C4.5, and CART---achieve provable guarantees
that compare favorably with those of the current fastest algorithm (Ehrenfeucht
and Haussler, 1989). Our lower bounds shed new light on the limitations of
these heuristics.
</p>
<p>Finally, we revisit the classic work of Ehrenfeucht and Haussler. We extend
it to give the first uniform-distribution proper learning algorithm that
achieves polynomial sample and memory complexity, while matching its
state-of-the-art quasipolynomial runtime.
</p></div>
    </summary>
    <updated>2019-11-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07357</id>
    <link href="http://arxiv.org/abs/1911.07357" rel="alternate" type="text/html"/>
    <title>Random Restrictions of High-Dimensional Distributions and Uniformity Testing with Subcube Conditioning</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Canonne:Cl=eacute=ment_L=.html">Clément L. Canonne</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xi.html">Xi Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamath:Gautam.html">Gautam Kamath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levi:Amit.html">Amit Levi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Waingarten:Erik.html">Erik Waingarten</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07357">PDF</a><br/><b>Abstract: </b>We give a nearly-optimal algorithm for testing uniformity of distributions
supported on $\{-1,1\}^n$, which makes $\tilde O (\sqrt{n}/\varepsilon^2)$
queries to a subcube conditional sampling oracle (Bhattacharyya and Chakraborty
(2018)). The key technical component is a natural notion of random restriction
for distributions on $\{-1,1\}^n$, and a quantitative analysis of how such a
restriction affects the mean vector of the distribution. Along the way, we
consider the problem of mean testing with independent samples and provide a
nearly-optimal algorithm.
</p></div>
    </summary>
    <updated>2019-11-19T23:43:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07324</id>
    <link href="http://arxiv.org/abs/1911.07324" rel="alternate" type="text/html"/>
    <title>Testing Properties of Multiple Distributions with Few Samples</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aliakbarpour:Maryam.html">Maryam Aliakbarpour</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silwal:Sandeep.html">Sandeep Silwal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07324">PDF</a><br/><b>Abstract: </b>We propose a new setting for testing properties of distributions while
receiving samples from several distributions, but few samples per distribution.
Given samples from $s$ distributions, $p_1, p_2, \ldots, p_s$, we design
testers for the following problems: (1) Uniformity Testing: Testing whether all
the $p_i$'s are uniform or $\epsilon$-far from being uniform in
$\ell_1$-distance (2) Identity Testing: Testing whether all the $p_i$'s are
equal to an explicitly given distribution $q$ or $\epsilon$-far from $q$ in
$\ell_1$-distance, and (3) Closeness Testing: Testing whether all the $p_i$'s
are equal to a distribution $q$ which we have sample access to, or
$\epsilon$-far from $q$ in $\ell_1$-distance. By assuming an additional natural
condition about the source distributions, we provide sample optimal testers for
all of these problems.
</p></div>
    </summary>
    <updated>2019-11-19T23:36:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07234</id>
    <link href="http://arxiv.org/abs/1911.07234" rel="alternate" type="text/html"/>
    <title>Approximation of Steiner Forest via the Bidirected Cut Relaxation</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ali Çivril <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07234">PDF</a><br/><b>Abstract: </b>The classical algorithm of Agrawal, Klein and Ravi [SIAM J. Comput., 24
(1995), pp. 440-456], stated in the setting of the primal-dual schema by
Goemans and Williamson [SIAM J. Comput., 24 (1995), pp. 296-317] uses the
undirected cut relaxation for the Steiner forest problem. Its approximation
ratio is $2-\frac{1}{k}$, where $k$ is the number of terminal pairs. A variant
of this algorithm more recently proposed by K\"onemann et al. [SIAM J. Comput.,
37 (2008), pp. 1319-1341] is based on the lifted cut relaxation. In this paper,
we continue this line of work and consider the bidirected cut relaxation for
the Steiner forest problem, which lends itself to a novel algorithmic idea
yielding the same approximation ratio as the classical algorithm. In doing so,
we introduce an extension of the primal-dual schema in which we run two
different phases to satisfy connectivity requirements in both directions. This
reveals more about the combinatorial structure of the problem. In particular,
there are examples on which the classical algorithm fails to give a good
approximation, but the new algorithm finds a near-optimal solution.
</p></div>
    </summary>
    <updated>2019-11-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07232</id>
    <link href="http://arxiv.org/abs/1911.07232" rel="alternate" type="text/html"/>
    <title>5/4-Approximation of Minimum 2-Edge-Connected Spanning Subgraph</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ali Çivril <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07232">PDF</a><br/><b>Abstract: </b>We provide a $5/4$-approximation algorithm for the 2-edge connected spanning
subgraph problem. This improves upon the previous best ratio of $4/3$. The
algorithm is based on applying local improvement steps on a starting solution
provided by a standard ear decomposition. Unlike previous approaches, we
consider modifications involving $5$-ears together with $3$-ears.
</p></div>
    </summary>
    <updated>2019-11-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07154</id>
    <link href="http://arxiv.org/abs/1911.07154" rel="alternate" type="text/html"/>
    <title>Sparse Hopsets in Congested Clique</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nazari:Yasamin.html">Yasamin Nazari</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07154">PDF</a><br/><b>Abstract: </b>We give the first Congested Clique algorithm that computes a sparse hopset
with polylogarithmic hopbound in polylogarithmic time. Given a graph $G=(V,E)$,
a $(\beta,\epsilon)$-hopset $H$ with "hopbound" $\beta$, is a set of edges
added to $G$ such that for any pair of nodes $u$ and $v$ in $G$ there is a path
with at most $\beta$ hops in $G \cup H$ with length within $(1+\epsilon)$ of
the shortest path between $u$ and $v$ in $G$.
</p>
<p>Our hopsets are significantly sparser than the recent construction of
Censor-Hillel et al. [6], that constructs a hopset of size
$\tilde{O}(n^{3/2})$, but with a smaller polylogarithmic hopbound. On the other
hand, the previously known constructions of sparse hopsets with polylogarithmic
hopbound in the Congested Clique model, proposed by Elkin and Neiman
[10],[11],[12], all require polynomial rounds.
</p>
<p>One tool that we use is an efficient algorithm that constructs an
$\ell$-limited neighborhood cover, that may be of independent interest.
</p>
<p>Finally, as a side result, we also give a hopset construction in a variant of
the low-memory Massively Parallel Computation model, with improved running time
over existing algorithms.
</p></div>
    </summary>
    <updated>2019-11-19T23:37:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.07151</id>
    <link href="http://arxiv.org/abs/1911.07151" rel="alternate" type="text/html"/>
    <title>A one-phase tree-based algorithm for mining high-utility itemsets from a transaction database</title>
    <feedworld_mtime>1574121600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dawar:Siddharth.html">Siddharth Dawar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Vikram.html">Vikram Goyal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bera:Debajyoti.html">Debajyoti Bera</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.07151">PDF</a><br/><b>Abstract: </b>High-utility itemset mining finds itemsets from a transaction database with
utility no less than a fixed user-defined threshold. The utility of an itemset
is defined as the sum of the utilities of its item. Several algorithms were
proposed to mine high-utility itemsets. However, no state-of-the-art algorithm
performs consistently good across dense and sparse datasets. In this paper, we
propose a novel data structure called Utility-Tree, and a tree-based algorithm
called UT-Miner that mines high-utility itemsets in one-phase only without
generating any candidates and uses a lightweight construction method to reduce
the cost of creating projected databases during the search space exploration.
The transaction information is stored compactly with every node of the
Utility-Tree, and the information is computed efficiently during the recursive
invocation of the algorithm. Experimental results on several real-life dense
and sparse datasets reveal that UT-Miner is among the top-performing efficient
algorithms across different datasets.
</p></div>
    </summary>
    <updated>2019-11-19T23:36:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-19T01:30:00Z</updated>
    </source>
  </entry>
</feed>
