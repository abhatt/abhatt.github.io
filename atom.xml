<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-05-09T22:21:45Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/068</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/068" rel="alternate" type="text/html"/>
    <title>TR19-068 |  LARGE CLIQUE IS HARD ON AVERAGE FOR RESOLUTION | 

	Shuo Pang</title>
    <summary>We prove resolution lower bounds for $k$-Clique on the Erdos-Renyi random graph $G(n,n^{-{2\xi}\over{k-1}})$ (where $\xi&gt;1$ is constant). First we show for $k=n^{c_0}$, $c_0\in(0,1/3)$, an $\exp({\Omega(n^{(1-\epsilon)c_0})})$ average lower bound on resolution where $\epsilon$ is arbitrary constant. 

We then propose the model of $a$-irregular resolution. Extended from regular resolution, this model is interesting in that the power of general-over-regular resolution from all {\it known} exponential separations is below it. We prove an $n^{\Omega(k)}$ average lower bound of $k$-Clique for this model, for {\it any} $k&lt;n^{1/3-\Omega(1)}$.</summary>
    <updated>2019-05-09T16:39:36Z</updated>
    <published>2019-05-09T16:39:36Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-09T22:20:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-722419176401069744</id>
    <link href="https://blog.computationalcomplexity.org/feeds/722419176401069744/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/multiple-provers.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/722419176401069744" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/722419176401069744" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/multiple-provers.html" rel="alternate" type="text/html"/>
    <title>Multiple Provers</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Just over thirty years ago on May 5, 1989, I defended my PhD Thesis <a href="https://lance.fortnow.com/papers/files/thesis.pdf">Complexity-Theoretic Aspects of Interactive Proof Systems</a>. It's starts off with a parable for interactive proof systems.<br/>
<blockquote class="tr_bq">
Victor, a venture capitalist, had everything a man could desire: money, women and power.
But he felt something missing. He decided he lacked knowledge. So Victor packed up his
bags and headed to the Himalayas in search of ultimate truths.
The natives pointed Victor to a tall mountain and mentioned rumors of a great man full of
wisdom. Victor, who smartly brought some climbing equipment, tackled the mountain until
he reached a small cave near the summit. Victor found the great Pulu, grand guru of all that
is known. Victor inquired to some ultimate truths and Pulu responded,
<i>I will teach you but you must not trust my words</i>.
Victor agreed and found he learned much even though he had to verify all the sayings
of the great Pulu. Victor though lacked complete happiness and he asked if he could learn
knowledge beyond what he could learn in this manner. The grand guru replied,
<i>You may ask and I will answer</i>.
Victor pondered this idea for a minute and said, "Since you know all that is known, why can you not predict my questions?" A silence reigned over the mountain for a short while until the guru  finally spoke,
<i>You must use other implements, symbols of your past life</i>.
Victor thought for a while and reached into his backpack and brought out some spare
change he had unwittingly carried with him. Even the great Pulu can not predict the flip of
a coin. He started flipping the coins to ask the guru and wondered what can I learn now?</blockquote>
Without the coins, one gets the complexity class NP. My thesis didn't answer the last question, but by the end of the year, <a href="https://doi.org/10.1109/FSCS.1990.89519">Shamir</a> building on work of <a href="https://doi.org/10.1145/146585.146605">Lund, Fortnow, Karloff and Nisan</a> showed this class IP was equal to PSPACE, the problems we could solve in a polynomial amount of memory.<br/>
<br/>
Part of my thesis explored the class MIP where we had multiple Pulus (provers) on different mountain tops unable to communicate. The news was disappointing, we failed to get a PSPACE upper bound for MIP, only NEXP (nondeterministic exponential time) and our proof that two provers sufficed relied on a bad assumption on how errors get reduced when you run multiple protocols in parallel. Later Babai, Lund and myself showed <a href="http://doi.org/10.1007/BF01200056">MIP = NEXP</a> and Ran Raz <a href="https://doi.org/10.1137/S0097539795280895">showed</a> parallel repetition does reduce the error sufficiently.<br/>
<br/>
Back in the 80's we didn't even imagine the possibility that the Pulus had shared entangled quantum bits. Does the entanglement allow the provers to cheat or can the entanglement allow them to prove more things? Turns out to be much more, as a <a href="https://arxiv.org/abs/1904.05870">new result</a> by Anand Natarajan and John Wright shows that MIP*, MIP with classical communication, classical verifier and two provers with previously entangled quantum bits, can compute everything in NEEXP, nondeterministic double exponential time. This is only a lower bound for MIP*, possibly one can do even more.<br/>
<br/>
Neat to see my three-decade old thesis explored ideas that people are still thinking about today.</div>
    </content>
    <updated>2019-05-09T12:31:00Z</updated>
    <published>2019-05-09T12:31:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-09T12:31:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.03204</id>
    <link href="http://arxiv.org/abs/1905.03204" rel="alternate" type="text/html"/>
    <title>Efficient On-line Computation of Visibility Graphs</title>
    <feedworld_mtime>1557360000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yela:Delia_Fano.html">Delia Fano Yela</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thalmann:Florian.html">Florian Thalmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nicosia:Vincenzo.html">Vincenzo Nicosia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stowell:Dan.html">Dan Stowell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sandler:Mark.html">Mark Sandler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.03204">PDF</a><br/><b>Abstract: </b>A visibility algorithm maps time series into complex networks following a
simple criterion. The resulting visibility graph has recently proven to be a
powerful tool for time series analysis. However its straightforward computation
is time-consuming and rigid, motivating the development of more efficient
algorithms. Here we present a highly efficient method to compute visibility
graphs with the further benefit of flexibility: on-line computation. We propose
an encoder/decoder approach, with an on-line adjustable binary search tree
codec for time series as well as its corresponding decoder for visibility
graphs. The empirical evidence suggests the proposed method for computation of
visibility graphs offers an on-line computation solution at no additional
computation time cost. The source code is available online.
</p></div>
    </summary>
    <updated>2019-05-09T01:23:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.03194</id>
    <link href="http://arxiv.org/abs/1905.03194" rel="alternate" type="text/html"/>
    <title>Zeros and approximations of Holant polynomials on the complex plane</title>
    <feedworld_mtime>1557360000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Casel:Katrin.html">Katrin Casel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischbeck:Philipp.html">Philipp Fischbeck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Friedrich:Tobias.html">Tobias Friedrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=ouml=bel:Andreas.html">Andreas Göbel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lagodzinski:J=_A=_Gregor.html">J. A. Gregor Lagodzinski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.03194">PDF</a><br/><b>Abstract: </b>We present fully polynomial approximation schemes for general classes of
Holant problems with complex edge weights, which we call Holant polynomials.
Our results are based on a recent technique for approximating graph polynomials
of degree $n$ by computing the first $\log n$ coefficients of the Taylor
expansion of the logarithm of the polynomial. Central to this technique is the
absence of roots in regions on the complex plane. We establish these zero-free
regions by translating our problem into partition functions of abstract
geometric structures known as polymers in statistical physics. Our results are
the first to consider Holant polynomials on the complex plane with such diverse
classes of constraints. A noteworthy consequence of our research is an
alternative angle on the problem of approximating the number of perfect
matchings of general graphs - a central problem of unresolved complexity in the
area of computational counting.
</p></div>
    </summary>
    <updated>2019-05-09T01:22:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.03148</id>
    <link href="http://arxiv.org/abs/1905.03148" rel="alternate" type="text/html"/>
    <title>The asymptotic induced matching number of hypergraphs: balanced binary strings</title>
    <feedworld_mtime>1557360000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Arunachalam:Srinivasan.html">Srinivasan Arunachalam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vrana:P=eacute=ter.html">Péter Vrana</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zuiddam:Jeroen.html">Jeroen Zuiddam</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.03148">PDF</a><br/><b>Abstract: </b>We compute the asymptotic induced matching number of the $k$-partite
$k$-uniform hypergraphs whose edges are the $k$-bit strings of Hamming weight
$k/2$, for any large enough even number $k$. Our lower bound relies on the
higher-order extension of the well-known Coppersmith-Winograd method from
algebraic complexity theory, which was proven by Christandl, Vrana and Zuiddam.
Our result is motivated by the study of the power of this method as well as of
the power of the Strassen support functionals (which provide upper bounds on
the asymptotic induced matching number), and the connections to questions in
tensor theory, quantum information theory and theoretical computer science.
</p>
<p>Phrased in the language of tensors, as a direct consequence of our result, we
determine the asymptotic subrank of any tensor with support given by the
aforementioned hypergraphs. In the context of quantum information theory, our
result amounts to an asymptotically optimal $k$-party stochastic local
operations and classical communication (slocc) protocol for the problem of
distilling GHZ-type entanglement from a subfamily of Dicke-type entanglement.
</p></div>
    </summary>
    <updated>2019-05-09T01:20:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.03134</id>
    <link href="http://arxiv.org/abs/1905.03134" rel="alternate" type="text/html"/>
    <title>Finding cuts of bounded degree: complexity, FPT and exact algorithms, and kernelization</title>
    <feedworld_mtime>1557360000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Guilherme C. M. Gomes, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.03134">PDF</a><br/><b>Abstract: </b>A matching cut is a partition of the vertex set of a graph into two sets $A$
and $B$ such that each vertex has at most one neighbor in the other side of the
cut. The MATCHING CUT problem asks whether a graph has a matching cut, and has
been intensively studied in the literature. Motivated by a question posed by
Komusiewicz et al. [IPEC 2018], we introduce a natural generalization of this
problem, which we call $d$-CUT: for a positive integer $d$, a $d$-cut is a
bipartition of the vertex set of a graph into two sets $A$ and $B$ such that
each vertex has at most $d$ neighbors across the cut. We generalize (and in
some cases, improve) a number of results for the MATCHING CUT problem. Namely,
we begin with an NP-hardness reduction for $d$-CUT on $(2d+2)$-regular graphs
and a polynomial algorithm for graphs of maximum degree at most $d+2$. The
degree bound in the hardness result is unlikely to be improved, as it would
disprove a long-standing conjecture in the context of internal partitions. We
then give FPT algorithms for several parameters: the maximum number of edges
crossing the cut, treewidth, distance to cluster, and distance to co-cluster.
In particular, the treewidth algorithm improves upon the running time of the
best known algorithm for MATCHING CUT. Our main technical contribution,
building on the techniques of Komusiewicz et al. [IPEC 2018], is a polynomial
kernel for $d$-CUT for every positive integer $d$, parameterized by the
distance to a cluster graph. We also rule out the existence of polynomial
kernels when parameterizing simultaneously by the number of edges crossing the
cut, the treewidth, and the maximum degree. Finally, we provide an exact
exponential algorithm slightly faster than the naive brute force approach
running in time $O^*(2^n)$.
</p></div>
    </summary>
    <updated>2019-05-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.03124</id>
    <link href="http://arxiv.org/abs/1905.03124" rel="alternate" type="text/html"/>
    <title>Key-agreement based on automaton groups</title>
    <feedworld_mtime>1557360000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Rostislav Grigorchuk, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grigoriev:Dima.html">Dima Grigoriev</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.03124">PDF</a><br/><b>Abstract: </b>We suggest several automaton groups as key-agreement platforms for
Anshl-Anshel-Goldfeld metascheme, they include Grigorchuk and universal
Grigorchuk groups, Hanoi 3-Towers group, Basilica group and a subgroup of the
affine group with the unsolvable conjugacy problem
</p></div>
    </summary>
    <updated>2019-05-09T01:20:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.03070</id>
    <link href="http://arxiv.org/abs/1905.03070" rel="alternate" type="text/html"/>
    <title>Testing Bipartitness in an Augmented VDF Bounded-Degree Graph Model</title>
    <feedworld_mtime>1557360000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldreich:Oded.html">Oded Goldreich</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.03070">PDF</a><br/><b>Abstract: </b>In a recent work (ECCC, TR18-171, 2018), we introduced models of testing
graph properties in which, in addition to answers to the usual graph-queries,
the tester obtains {\em random vertices drawn according to an arbitrary
distribution $D$}. Such a tester is required to distinguish between graphs that
have the property and graphs that are far from having the property, {\em where
the distance between graphs is defined based on the unknown vertex distribution
$D$}. These ("vertex-distribution free" (VDF)) models generalize the standard
models in which $D$ is postulated to be uniform on the vertex-set, and they
were studies both in the dense graph model and in the bounded-degree graph
model.
</p>
<p>The focus of the aforementioned work was on testers, called {\sf strong},
whose query complexity depends only on the proximity parameter $\epsilon$.
Unfortunately, in the standard bounded-degree graph model, some natural
properties such as Bipartiteness do not have strong testers, and others (like
cycle-freeness) do not have strong testers of one-sided error (whereas
one-sided error was shown inherent to the VDF model). Hence, it was suggested
to study general (i.e., non-strong) testers of "sub-linear" complexity.
</p>
<p>In this work, we pursue the foregoing suggestion, but do so in a model that
augments the model presented in the aforementioned work. Specifically, we
provide the tester with an evaluation oracle to the unknown distribution $D$,
in addition to samples of $D$ and oracle access to the tested graph. Our main
results are testers for Bipartitness and cycle-freeness, in this augmented
model, having complexity that is almost-linear in the square root of the
"effective support size" of $D$.
</p></div>
    </summary>
    <updated>2019-05-09T01:24:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.03037</id>
    <link href="http://arxiv.org/abs/1905.03037" rel="alternate" type="text/html"/>
    <title>The Guided Team-Partitioning Problem: Definition, Complexity, and Algorithm</title>
    <feedworld_mtime>1557360000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bahargam:Sanaz.html">Sanaz Bahargam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lappas:Theodoros.html">Theodoros Lappas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Terzi:Evimaria.html">Evimaria Terzi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.03037">PDF</a><br/><b>Abstract: </b>A long line of literature has focused on the problem of selecting a team of
individuals from a large pool of candidates, such that certain constraints are
respected, and a given objective function is maximized. Even though extant
research has successfully considered diverse families of objective functions
and constraints, one of the most common limitations is the focus on the
single-team paradigm. Despite its well-documented applications in multiple
domains, this paradigm is not appropriate when the team-builder needs to
partition the entire population into multiple teams. Team-partitioning tasks
are very common in an educational setting, in which the teacher has to
partition the students in her class into teams for collaborative projects. The
task also emerges in the context of organizations, when managers need to
partition the workforce into teams with specific properties to tackle relevant
projects. In this work, we extend the team formation literature by introducing
the Guided Team-Partitioning (GTP) problem, which asks for the partitioning of
a population into teams such that the centroid of each team is as close as
possible to a given target vector. As we describe in detail in our work, this
formulation allows the team-builder to control the composition of the produced
teams and has natural applications in practical settings. Algorithms for the
GTP need to simultaneously consider the composition of multiple non-overlapping
teams that compete for the same population of candidates. This makes the
problem considerably more challenging than formulations that focus on the
optimization of a single team. In fact, we prove that GTP is NP-hard to solve
and even to approximate. The complexity of the problem motivates us to consider
efficient algorithmic heuristics, which we evaluate via experiments on both
real and synthetic datasets.
</p></div>
    </summary>
    <updated>2019-05-09T01:22:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02805</id>
    <link href="http://arxiv.org/abs/1905.02805" rel="alternate" type="text/html"/>
    <title>Network Coding Gaps for Completion Times of Multiple Unicasts</title>
    <feedworld_mtime>1557360000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haeupler:Bernhard.html">Bernhard Haeupler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wajc:David.html">David Wajc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zuzic:Goran.html">Goran Zuzic</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02805">PDF</a><br/><b>Abstract: </b>Arguably the most common network communication problem is multiple-unicasts:
Distinct packets at different nodes in a network need to be delivered to a
destination specific to each packet as fast as possible.
</p>
<p>The famous multiple-unicast conjecture posits that, for this natural problem,
there is no performance gap between routing and network coding, at least in
terms of throughput.
</p>
<p>We study the same network coding gap but in terms of completion time. While
throughput corresponds to the completion time for asymptotically-large
transmissions, we look at completion times of multiple unicasts for arbitrary
amounts of data. We develop nearly-matching upper and lower bounds. In
particular, we prove that the network coding gap for the completion time of $k$
unicasts is at most polylogarithmic in $k$, and there exist instances of $k$
unicasts for which this coding gap is polylogarithmic in $k$.
</p></div>
    </summary>
    <updated>2019-05-09T01:24:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02800</id>
    <link href="http://arxiv.org/abs/1905.02800" rel="alternate" type="text/html"/>
    <title>Online and Offline Greedy Algorithms for Routing with Switching Costs</title>
    <feedworld_mtime>1557360000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schwartz:Roy.html">Roy Schwartz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Mohit.html">Mohit Singh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yazdanbod:Sina.html">Sina Yazdanbod</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02800">PDF</a><br/><b>Abstract: </b>Motivated by the use of high speed circuit switches in large scale data
centers, we consider the problem of circuit switch scheduling. In this problem
we are given demands between pairs of servers and the goal is to schedule at
every time step a matching between the servers while maximizing the total
satisfied demand over time. The crux of this scheduling problem is that once
one shifts from one matching to a different one a fixed delay $\delta$ is
incurred during which no data can be transmitted.
</p>
<p>For the offline version of the problem we present a
$(1-\frac{1}{e}-\epsilon)$ approximation ratio (for any constant $\epsilon
&gt;0$). Since the natural linear programming relaxation for the problem has an
unbounded integrality gap, we adopt a hybrid approach that combines the
combinatorial greedy with randomized rounding of a different suitable linear
program. For the online version of the problem we present a (bi-criteria) $
((e-1)/(2e-1)-\epsilon)$-competitive ratio (for any constant $\epsilon &gt;0$ )
that exceeds time by an additive factor of $O(\frac{\delta}{\epsilon})$. We
note that no uni-criteria online algorithm is possible. Surprisingly, we obtain
the result by reducing the online version to the offline one.
</p></div>
    </summary>
    <updated>2019-05-09T01:23:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02752</id>
    <link href="http://arxiv.org/abs/1905.02752" rel="alternate" type="text/html"/>
    <title>Kendall Tau Sequence Distance: Extending Kendall Tau from Ranks to Sequences</title>
    <feedworld_mtime>1557360000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cicirello:Vincent_A=.html">Vincent A. Cicirello</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02752">PDF</a><br/><b>Abstract: </b>An edit distance is a measure of the minimum cost sequence of edit operations
to transform one structure into another. Edit distance is most commonly
encountered within the context of strings, where Wagner and Fischer's string
edit distance is perhaps the most well-known. However, edit distance is not
limited to strings. For example, there are several edit distance measures for
permutations, including Wagner and Fischer's string edit distance since a
permutation is a special case of a string. However, another edit distance for
permutations is Kendall tau distance, which is the number of pairwise element
inversions. On permutations, Kendall tau distance is equivalent to an edit
distance with adjacent swap as the edit operation. A permutation is often used
to represent a total ranking over a set of elements. There exist multiple
extensions of Kendall tau distance from total rankings (permutations) to
partial rankings (i.e., where multiple elements may have the same rank), but
none of these are suitable for computing distance between sequences. We set out
to explore extending Kendall tau distance in a different direction, namely from
the special case of permutations to the more general case of strings or
sequences of elements from some finite alphabet. We name our distance metric
Kendall tau sequence distance, and define it as the minimum number of adjacent
swaps necessary to transform one sequence into the other. We provide two $O(n
\lg n)$ algorithms for computing it, and experimentally compare their relative
performance. We also provide reference implementations of both algorithms in an
open source Java library.
</p></div>
    </summary>
    <updated>2019-05-09T01:22:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=353</id>
    <link href="https://tcsplus.wordpress.com/2019/05/08/tcs-talk-wednesday-may-15th-ewin-tang-university-of-washington/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 15th — Ewin Tang, University of Washington</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 15th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Ewin Tang from University of Washington will tell us about “Quantum-inspired classical linear algebra algorithms: why and how?” (abstract below). Please make sure you reserve a spot for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 15th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong><a href="https://ewintang.com/" rel="noopener" target="_blank">Ewin Tang</a></strong> from University of Washington will tell us about “<em>Quantum-inspired classical linear algebra algorithms: why and how?</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Over the past ten years, the field of quantum machine learning (QML) has produced many polylogarithmic-time procedures for linear algebra routines, assuming certain “state preparation” assumptions. Though such algorithms are formally incomparable with classical computing, a recent line of work uses an analogous classical model of computation as an effective point of comparison to reveal speedups (or lack thereof) gained by QML. The resulting “dequantized” algorithms assume sampling access to input to speed up runtimes to polylogarithmic in input size.</p>
<p>In this talk, we will discuss the motivation behind this model and its relation to existing randomized linear algebra literature. Then, we will delve into an example quantum-inspired algorithm: Gilyen, Lloyd, and Tang’s algorithm for low-rank matrix inversion. This dequantizes a variant of Harrow, Hassidim, and Lloyd’s matrix inversion algorithm, a seminal work in QML. Finally, we will consider the implications of this work on exponential speedups in QML. No background of quantum computing is assumed for this talk.</p></blockquote></div>
    </content>
    <updated>2019-05-08T22:26:55Z</updated>
    <published>2019-05-08T22:26:55Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-05-09T22:21:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02709</id>
    <link href="http://arxiv.org/abs/1905.02709" rel="alternate" type="text/html"/>
    <title>Hiring Under Uncertainty</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raghavan:Manish.html">Manish Raghavan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Purohit:Manish.html">Manish Purohit</a>, Sreenivas Gollupadi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02709">PDF</a><br/><b>Abstract: </b>In this paper we introduce the hiring under uncertainty problem to model the
questions faced by hiring committees in large enterprises and universities
alike. Given a set of $n$ eligible candidates, the decision maker needs to
choose the sequence of candidates to make offers so as to hire the $k$ best
candidates. However, candidates may choose to reject an offer (for instance,
due to a competing offer) and the decision maker has a time limit by which all
positions must be filled. Given an estimate of the probabilities of acceptance
for each candidate, the hiring under uncertainty problem is to design a
strategy of making offers so that the total expected value of all candidates
hired by the time limit is maximized. We provide a 2-approximation algorithm
for the setting where offers must be made in sequence, an 8-approximation when
offers may be made in parallel, and a 10-approximation for the more general
stochastic knapsack setting with finite probes.
</p></div>
    </summary>
    <updated>2019-05-08T23:26:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02687</id>
    <link href="http://arxiv.org/abs/1905.02687" rel="alternate" type="text/html"/>
    <title>The algorithm for the recovery of integer vector via linear measurements</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ryutin:K=_S=.html">K. S. Ryutin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02687">PDF</a><br/><b>Abstract: </b>In this paper we continue the studies on the integer sparse recovery problem
that was introduced in \cite{FKS} and studied in \cite{K},\cite{KS}. We provide
an algorithm for the recovery of an unknown sparse integer vector for the
measurement matrix described in \cite{KS} and estimate the number of
arithmetical operations.
</p></div>
    </summary>
    <updated>2019-05-08T23:20:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02620</id>
    <link href="http://arxiv.org/abs/1905.02620" rel="alternate" type="text/html"/>
    <title>External Memory Planar Point Location with Fast Updates</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iacono:John.html">John Iacono</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karsin:Ben.html">Ben Karsin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koumoutsos:Grigorios.html">Grigorios Koumoutsos</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02620">PDF</a><br/><b>Abstract: </b>We study dynamic planar point location in the External Memory Model or Disk
Access Model (DAM). Previous work in this model achieves polylog query and
polylog amortized update time. We present a data structure with $O( \log_B^2
N)$ query time and $O(\frac{1}{ B^{1-\epsilon}} \log_B N)$ amortized update
time, where $N$ is the number of segments, $B$ the block size and $\epsilon$ is
a small positive constant. This is a $B^{1-\epsilon}$ factor faster for updates
than the fastest previous structure, and brings the cost of insertion and
deletion down to subconstant amortized time for reasonable choices of $N$ and
$B$. Our structure solves the problem of vertical ray-shooting queries among a
dynamic set of interior-disjoint line segments; this is well-known to solve
dynamic planar point location for a connected subdivision of the plane.
</p></div>
    </summary>
    <updated>2019-05-08T23:27:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02592</id>
    <link href="http://arxiv.org/abs/1905.02592" rel="alternate" type="text/html"/>
    <title>Distributed Construction of Light Networks</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Elkin:Michael.html">Michael Elkin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Arnold.html">Arnold Filtser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neiman:Ofer.html">Ofer Neiman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02592">PDF</a><br/><b>Abstract: </b>A $t$-{\em spanner} $H$ of a weighted graph $G=(V,E,w)$ is a subgraph that
approximates all pairwise distances up to a factor of $t$. The {\em lightness}
of $H$ is defined as the ratio between the weight of $H$ to that of the minimum
spanning tree. An $(\alpha,\beta)$-{\em Shallow Light Tree} (SLT) is a tree of
lightness $\beta$, that approximates all distances from a designated root
vertex up to a factor of $\alpha$. A long line of works resulted in efficient
algorithms that produce (nearly) optimal light spanners and SLTs.
</p>
<p>Some of the most notable algorithmic applications of light spanners and SLTs
are in distributed settings. Surprisingly, so far there are no known efficient
distributed algorithms for constructing these objects in general graphs. In
this paper we devise efficient distributed algorithms in the CONGEST model for
constructing light spanners and SLTs, with near optimal parameters.
Specifically, for any $k\ge 1$ and $0&lt;\epsilon&lt;1$, we show a
$(2k-1)\cdot(1+\epsilon)$-spanner with lightness $O(k\cdot n^{1/k})$ can be
built in $\tilde{O}\left(n^{\frac12+\frac{1}{4k+2}}+D\right)$ rounds (where
$n=|V|$ and $D$ is the hop-diameter of $G$). In addition, for any $\alpha&gt;1$ we
provide an $(\alpha,1+\frac{O(1)}{\alpha-1})$-SLT in $(\sqrt{n}+D)\cdot
n^{o(1)}$ rounds. The running time of our algorithms cannot be substantially
improved.
</p>
<p>We also consider spanners for the family of doubling graphs, and devise a
$(\sqrt{n}+D)\cdot n^{o(1)}$ rounds algorithm in the CONGEST model that
computes a $(1+\epsilon)$-spanner with lightness $(\log n)/\epsilon^{O(1)}$. As
a stepping stone, which is interesting in its own right, we first develop a
distributed algorithm for constructing nets (for arbitrary weighted graphs),
generalizing previous algorithms that worked only for unweighted graphs.
</p></div>
    </summary>
    <updated>2019-05-08T23:27:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02589</id>
    <link href="http://arxiv.org/abs/1905.02589" rel="alternate" type="text/html"/>
    <title>Order-Preserving Pattern Matching Indeterminate Strings</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Costa:Diogo.html">Diogo Costa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Russo:Lu=iacute=s_M=_S=.html">Luís M. S. Russo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henriques:Rui.html">Rui Henriques</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Francisco:Alexandre_P=.html">Alexandre P. Francisco</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02589">PDF</a><br/><b>Abstract: </b>Given an indeterminate string pattern $p$ and an indeterminate string text
$t$, the problem of order-preserving pattern matching with character
uncertainties ($\mu$OPPM) is to find all substrings of $t$ that satisfy one of
the possible orderings defined by $p$. When the text and pattern are
determinate strings, we are in the presence of the well-studied exact
order-preserving pattern matching (OPPM) problem with diverse applications on
time series analysis. Despite its relevance, the exact OPPM problem suffers
from two major drawbacks: 1) the inability to deal with indetermination in the
text, thus preventing the analysis of noisy time series; and 2) the inability
to deal with indetermination in the pattern, thus imposing the strict
satisfaction of the orders among all pattern positions. This paper provides the
first polynomial algorithm to answer the $\mu$OPPM problem when indetermination
is observed on the pattern or text. Given two strings with length $m$ and
$O(r)$ uncertain characters per string position, we show that the $\mu$OPPM
problem can be solved in $O(mr\lg r)$ time when one string is indeterminate and
$r\in\mathbb{N}^+$. Mappings into satisfiability problems are provided when
indetermination is observed on both the pattern and the text, and results
concerning the general problem complexity are presented as well, with $\mu$OPPM
problem proved to be NP-hard in general.
</p></div>
    </summary>
    <updated>2019-05-08T23:24:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02518</id>
    <link href="http://arxiv.org/abs/1905.02518" rel="alternate" type="text/html"/>
    <title>Incorporating Weisfeiler-Leman into algorithms for group isomorphism</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brooksbank:Peter_A=.html">Peter A. Brooksbank</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grochow:Joshua_A=.html">Joshua A. Grochow</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yinan.html">Yinan Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qiao:Youming.html">Youming Qiao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wilson:James_B=.html">James B. Wilson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02518">PDF</a><br/><b>Abstract: </b>In this paper we combine many of the standard and more recent algebraic
techniques for testing isomorphism of finite groups (GpI) with combinatorial
techniques that have typically been applied to Graph Isomorphism. In
particular, we show how to combine several state-of-the-art GpI algorithms for
specific group classes into an algorithm for general GpI, namely: composition
series isomorphism (Rosenbaum-Wagner, Theoret. Comp. Sci., 2015; Luks, 2015),
recursively-refineable filters (Wilson, J. Group Theory, 2013), and low-genus
GpI (Brooksbank-Maglione-Wilson, J. Algebra, 2017). Recursively-refineable
filters -- a generalization of subgroup series -- form the skeleton of this
framework, and we refine our filter by building a hypergraph encoding low-genus
quotients, to which we then apply a hypergraph variant of the k-dimensional
Weisfeiler-Leman technique. Our technique is flexible enough to readily
incorporate additional hypergraph invariants or additional characteristic
subgroups.
</p></div>
    </summary>
    <updated>2019-05-08T23:20:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02472</id>
    <link href="http://arxiv.org/abs/1905.02472" rel="alternate" type="text/html"/>
    <title>Self-Adjusting Linear Networks</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avin:Chen.html">Chen Avin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Duijn:Ingo_van.html">Ingo van Duijn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmid:Stefan.html">Stefan Schmid</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02472">PDF</a><br/><b>Abstract: </b>Emerging networked systems become increasingly flexible and reconfigurable.
This introduces an opportunity to adjust networked systems in a demand-aware
manner, leveraging spatial and temporal locality in the workload for online
optimizations. However, it also introduces a trade-off: while more frequent
adjustments can improve performance, they also entail higher reconfiguration
costs.
</p>
<p>This paper initiates the formal study of linear networks which self-adjust to
the demand in an online manner, striking a balance between the benefits and
costs of reconfigurations. We show that the underlying algorithmic problem can
be seen as a distributed generalization of the classic dynamic list update
problem known from self-adjusting datastructures: in a network, requests can
occur between node pairs. This distributed version turns out to be
significantly harder than the classical problem in generalizes. Our main
results are a $\Omega(\log{n})$ lower bound on the competitive ratio, and a
(distributed) online algorithm that is $O(\log{n})$-competitive if the
communication requests are issued according to a linear order.
</p></div>
    </summary>
    <updated>2019-05-08T23:26:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02469</id>
    <link href="http://arxiv.org/abs/1905.02469" rel="alternate" type="text/html"/>
    <title>Robust two-stage combinatorial optimization problems under convex uncertainty</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goerigk:Marc.html">Marc Goerigk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kasperski:Adam.html">Adam Kasperski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zielinski:Pawel.html">Pawel Zielinski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02469">PDF</a><br/><b>Abstract: </b>In this paper a class of robust two-stage combinatorial optimization problems
is discussed. It is assumed that the uncertain second stage costs are specified
in the form of a convex uncertainty set, in particular polyhedral or
ellipsoidal ones. It is shown that the robust two-stage versions of basic
network and selection problems are NP-hard, even in a very restrictive cases.
Some exact and approximation algorithms for the general problem are
constructed. Polynomial and approximation algorithms for the robust two-stage
versions of basic problems, such as the selection and shortest path problems,
are also provided.
</p></div>
    </summary>
    <updated>2019-05-08T23:21:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02424</id>
    <link href="http://arxiv.org/abs/1905.02424" rel="alternate" type="text/html"/>
    <title>Equal-Subset-Sum Faster Than the Meet-in-the-Middle</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mucha:Marcin.html">Marcin Mucha</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nederlof:Jesper.html">Jesper Nederlof</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pawlewicz:Jakub.html">Jakub Pawlewicz</a>, Karol Węgrzycki <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02424">PDF</a><br/><b>Abstract: </b>In the Equal-Subset-Sum problem, we are given a set $S$ of $n$ integers and
the problem is to decide if there exist two disjoint nonempty subsets $A,B
\subseteq S$, whose elements sum up to the same value. The problem is
NP-complete. The state-of-the-art algorithm runs in $O^{*}(3^{n/2}) \le
O^{*}(1.7321^n)$ time and is based on the meet-in-the-middle technique. In this
paper, we improve upon this algorithm and give $O^{*}(1.7088^n)$ worst case
Monte Carlo algorithm. This answers the open problem from Woeginger's
inspirational survey.
</p>
<p>Additionally, we analyse the polynomial space algorithm for Equal-Subset-Sum.
A naive polynomial space algorithm for Equal-Subset-Sum runs in $O^{*}(3^n)$
time. With read-only access to the exponentially many random bits, we show a
randomized algorithm running in $O^{*}(2.6817^n)$ time and polynomial space.
</p></div>
    </summary>
    <updated>2019-05-08T23:28:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02383</id>
    <link href="http://arxiv.org/abs/1905.02383" rel="alternate" type="text/html"/>
    <title>Gaussian Differential Privacy</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Jinshuo.html">Jinshuo Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roth:Aaron.html">Aaron Roth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Su:Weijie_J=.html">Weijie J. Su</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02383">PDF</a><br/><b>Abstract: </b>Differential privacy has seen remarkable success as a rigorous and practical
formalization of data privacy in the past decade. But it also has some well
known weaknesses: notably, it does not tightly handle composition. This
weakness has inspired several recent relaxations of differential privacy based
on Renyi divergences. We propose an alternative relaxation of differential
privacy, which we term "$f$-differential privacy", which has a number of
appealing properties and avoids some of the difficulties associated with
divergence based relaxations. First, it preserves the hypothesis testing
interpretation of differential privacy, which makes its guarantees easily
interpretable. It allows for lossless reasoning about composition and
post-processing, and notably, a direct way to import existing tools from
differential privacy, including privacy amplification by subsampling. We define
a canonical single parameter family of definitions within our class which we
call "Gaussian Differential Privacy", defined based on the hypothesis testing
of two shifted Gaussian distributions. We show that this family is focal by
proving a central limit theorem, which shows that the privacy guarantees of
\emph{any} hypothesis-testing based definition of privacy (including
differential privacy) converges to Gaussian differential privacy in the limit
under composition. We also prove a finite (Berry-Esseen style) version of the
central limit theorem, which gives a useful tool for tractably analyzing the
exact composition of potentially complicated expressions. We demonstrate the
use of the tools we develop by giving an improved analysis of the privacy
guarantees of noisy stochastic gradient descent.
</p></div>
    </summary>
    <updated>2019-05-08T23:52:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02367</id>
    <link href="http://arxiv.org/abs/1905.02367" rel="alternate" type="text/html"/>
    <title>Adversarially Robust Submodular Maximization under Knapsack Constraints</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avdiukhin:Dmitrii.html">Dmitrii Avdiukhin</a>, Slobodan Mitrović, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yaroslavtsev:Grigory.html">Grigory Yaroslavtsev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02367">PDF</a><br/><b>Abstract: </b>We propose the first adversarially robust algorithm for monotone submodular
maximization under single and multiple knapsack constraints with scalable
implementations in distributed and streaming settings. For a single knapsack
constraint, our algorithm outputs a robust summary of almost optimal (up to
polylogarithmic factors) size, from which a constant-factor approximation to
the optimal solution can be constructed. For multiple knapsack constraints, our
approximation is within a constant-factor of the best known non-robust
solution.
</p>
<p>We evaluate the performance of our algorithms by comparison to natural
robustifications of existing non-robust algorithms under two objectives: 1)
dominating set for large social network graphs from Facebook and Twitter
collected by the Stanford Network Analysis Project (SNAP), 2) movie
recommendations on a dataset from MovieLens. Experimental results show that our
algorithms give the best objective for a majority of the inputs and show strong
performance even compared to offline algorithms that are given the set of
removals in advance.
</p></div>
    </summary>
    <updated>2019-05-08T23:54:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02358</id>
    <link href="http://arxiv.org/abs/1905.02358" rel="alternate" type="text/html"/>
    <title>Exponential Separations Between Turnstile Streaming and Linear Sketching</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kallaugher:John.html">John Kallaugher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Price:Eric.html">Eric Price</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02358">PDF</a><br/><b>Abstract: </b>Almost every known turnstile streaming algorithm is implementable as a linear
sketch. Is this necessarily true, or can there exist turnstile streaming
algorithms that use much less space than any linear sketch?
</p>
<p>It was shown in~\cite{LNW14} that, if a turnstile algorithm works for
arbitrarily long streams with arbitrarily large coordinates at intermediate
stages of the stream, then the turnstile algorithm can be implemented as a
linear sketch. Our results have the opposite form: if either the stream length
or the maximum value of the stream are substantially restricted, there exist
problems where linear sketching is exponentially harder than turnstile
streaming.
</p></div>
    </summary>
    <updated>2019-05-08T23:53:05Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02354</id>
    <link href="http://arxiv.org/abs/1905.02354" rel="alternate" type="text/html"/>
    <title>PRSim: Sublinear Time SimRank Computation on Large Power-Law Graphs</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei:Zhewei.html">Zhewei Wei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Xiaodong.html">Xiaodong He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xiao:Xiaokui.html">Xiaokui Xiao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Sibo.html">Sibo Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yu.html">Yu Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Du:Xiaoyong.html">Xiaoyong Du</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wen:Ji=Rong.html">Ji-Rong Wen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02354">PDF</a><br/><b>Abstract: </b>{\it SimRank} is a classic measure of the similarities of nodes in a graph.
Given a node $u$ in graph $G =(V, E)$, a {\em single-source SimRank query}
returns the SimRank similarities $s(u, v)$ between node $u$ and each node $v
\in V$. This type of queries has numerous applications in web search and social
networks analysis, such as link prediction, web mining, and spam detection.
Existing methods for single-source SimRank queries, however, incur query cost
at least linear to the number of nodes $n$, which renders them inapplicable for
real-time and interactive analysis.
</p>
<p>{ This paper proposes \prsim, an algorithm that exploits the structure of
graphs to efficiently answer single-source SimRank queries. \prsim uses an
index of size $O(m)$, where $m$ is the number of edges in the graph, and
guarantees a query time that depends on the {\em reverse PageRank} distribution
of the input graph. In particular, we prove that \prsim runs in sub-linear time
if the degree distribution of the input graph follows the power-law
distribution, a property possessed by many real-world graphs. Based on the
theoretical analysis, we show that the empirical query time of all existing
SimRank algorithms also depends on the reverse PageRank distribution of the
graph.} Finally, we present the first experimental study that evaluates the
absolute errors of various SimRank algorithms on large graphs, and we show that
\prsim outperforms the state of the art in terms of query time, accuracy, index
size, and scalability.
</p></div>
    </summary>
    <updated>2019-05-08T23:29:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02322</id>
    <link href="http://arxiv.org/abs/1905.02322" rel="alternate" type="text/html"/>
    <title>Orthogonal Range Reporting and Rectangle Stabbing for Fat Rectangles</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chan:Timothy_M=.html">Timothy M. Chan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nekrich:Yakov.html">Yakov Nekrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smid:Michiel.html">Michiel Smid</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02322">PDF</a><br/><b>Abstract: </b>In this paper we study two geometric data structure problems in the special
case when input objects or queries are fat rectangles. We show that in this
case a significant improvement compared to the general case can be achieved.
</p>
<p>We describe data structures that answer two- and three-dimensional orthogonal
range reporting queries in the case when the query range is a \emph{fat}
rectangle. Our two-dimensional data structure uses $O(n)$ words and supports
queries in $O(\log\log U +k)$ time, where $n$ is the number of points in the
data structure, $U$ is the size of the universe and $k$ is the number of points
in the query range. Our three-dimensional data structure needs
$O(n\log^{\varepsilon}U)$ words of space and answers queries in $O(\log \log U
+ k)$ time. We also consider the rectangle stabbing problem on a set of
three-dimensional fat rectangles. Our data structure uses $O(n)$ space and
answers stabbing queries in $O(\log U\log\log U +k)$ time.
</p></div>
    </summary>
    <updated>2019-05-08T23:22:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02316</id>
    <link href="http://arxiv.org/abs/1905.02316" rel="alternate" type="text/html"/>
    <title>Parallel Cut Pursuit For Minimization of the Graph Total Variation</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raguet:Hugo.html">Hugo Raguet</a>, Loic Landrieu <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02316">PDF</a><br/><b>Abstract: </b>We present a parallel version of the cut-pursuit algorithm for minimizing
functionals involving the graph total variation. We show that the decomposition
of the iterate into constant connected components, which is at the center of
this method, allows for the seamless parallelization of the otherwise costly
graph-cut based refinement stage. We demonstrate experimentally the efficiency
of our method in a wide variety of settings, from simple denoising on huge
graphs to more complex inverse problems with nondifferentiable penalties. We
argue that our approach combines the efficiency of graph-cuts based optimizers
with the versatility and ease of parallelization of traditional proximal
</p></div>
    </summary>
    <updated>2019-05-08T23:52:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02313</id>
    <link href="http://arxiv.org/abs/1905.02313" rel="alternate" type="text/html"/>
    <title>Optimal Convergence Rate of Hamiltonian Monte Carlo for Strongly Logconcave Distributions</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Zongchen.html">Zongchen Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vempala:Santosh_S=.html">Santosh S. Vempala</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02313">PDF</a><br/><b>Abstract: </b>We study Hamiltonian Monte Carlo (HMC) for sampling from a strongly
logconcave density proportional to $e^{-f}$ where $f:\mathbb{R}^d \to
\mathbb{R}$ is $\mu$-strongly convex and $L$-smooth (the condition number is
$\kappa = L/\mu$). We show that the relaxation time (inverse of the spectral
gap) of ideal HMC is $O(\kappa)$, improving on the previous best bound of
$O(\kappa^{1.5})$; we complement this with an example where the relaxation time
is $\Omega(\kappa)$. When implemented using a nearly optimal ODE solver, HMC
returns an $\varepsilon$-approximate point in $2$-Wasserstein distance using
$\widetilde{O}((\kappa d)^{0.5} \varepsilon^{-1})$ gradient evaluations per
step and $\widetilde{O}((\kappa d)^{1.5}\varepsilon^{-1})$ total time.
</p></div>
    </summary>
    <updated>2019-05-08T23:54:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02303</id>
    <link href="http://arxiv.org/abs/1905.02303" rel="alternate" type="text/html"/>
    <title>Design Space Exploration as Quantified Satisfaction</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldman:Alexander.html">Alexander Feldman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kleer:Johan_de.html">Johan de Kleer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matei:Ion.html">Ion Matei</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02303">PDF</a><br/><b>Abstract: </b>We propose novel algorithms for design and design space exploration. The
designs computed by these algorithms are compositions of function types
specified in component libraries. Our algorithms reduce the design problem to
quantified satisfiability and use advanced solvers to find solutions that
represent useful systems. The algorithms we present in this paper are sound and
complete and are guaranteed to discover correct designs of optimal size, if
they exist. We apply our method to the design of Boolean systems and discover
new and more optimal classical and quantum circuits for common arithmetic
functions such as addition and multiplication. The performance of our
algorithms is evaluated through extensive experimentation. We have first
created a benchmark consisting of specifications of scalable synthetic digital
circuits and real-world mirochips. We have then generated multiple circuits
functionally equivalent to the ones in the benchmark. The quantified
satisfiability method shows more than four orders of magnitude speed-up,
compared to a generate and test method that enumerates all non-isomorphic
circuit topologies. Our approach generalizes circuit optimization. It uses
arbitrary component libraries and has applications to areas such as digital
circuit design, diagnostics, abductive reasoning, test vector generation, and
combinatorial optimization.
</p></div>
    </summary>
    <updated>2019-05-08T23:22:12Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02298</id>
    <link href="http://arxiv.org/abs/1905.02298" rel="alternate" type="text/html"/>
    <title>Even Faster Elastic-Degenerate String Matching via Fast Matrix Multiplication</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bernardini:Giulia.html">Giulia Bernardini</a>, Paweł Gawrychowski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pisanti:Nadia.html">Nadia Pisanti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pissis:Solon_P=.html">Solon P. Pissis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosone:Giovanna.html">Giovanna Rosone</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02298">PDF</a><br/><b>Abstract: </b>An elastic-degenerate (ED) string is a sequence of $n$ sets of strings of
total length $N$, which was recently proposed to model a set of similar
sequences. The ED string matching (EDSM) problem is to find all occurrences of
a pattern of length $m$ in an ED text. The EDSM problem has recently received
some attention in the combinatorial pattern matching community, and an
$\mathcal{O}(nm^{1.5}\sqrt{\log m} + N)$-time algorithm is known [Aoyama et
al., CPM 2018]. The standard assumption in the prior work on this question is
that $N$ is substantially larger than both $n$ and $m$, and thus we would like
to have a linear dependency on the former. Under this assumption, the natural
open problem is whether we can decrease the 1.5 exponent in the time
complexity, similarly as in the related (but, to the best of our knowledge, not
equivalent) word break problem [Backurs and Indyk, FOCS 2016].
</p>
<p>Our starting point is a conditional lower bound for the EDSM problem. We use
the popular combinatorial Boolean matrix multiplication (BMM) conjecture
stating that there is no truly subcubic combinatorial algorithm for BMM [Abboud
and Williams, FOCS 2014]. By designing an appropriate reduction we show that a
combinatorial algorithm solving the EDSM problem in
$\mathcal{O}(nm^{1.5-\epsilon} + N)$ time, for any $\epsilon&gt;0$, refutes this
conjecture. Of course, the notion of combinatorial algorithms is not clearly
defined, so our reduction should be understood as an indication that decreasing
the exponent requires fast matrix multiplication.
</p>
<p>Two standard tools used in algorithms on strings are string periodicity and
fast Fourier transform. Our main technical contribution is that we successfully
combine these tools with fast matrix multiplication to design a
non-combinatorial $\mathcal{O}(nm^{1.381} + N)$-time algorithm for EDSM. To the
best of our knowledge, we are the first to do so.
</p></div>
    </summary>
    <updated>2019-05-08T23:53:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.02270</id>
    <link href="http://arxiv.org/abs/1905.02270" rel="alternate" type="text/html"/>
    <title>Lifted Multiplicity Codes</title>
    <feedworld_mtime>1557273600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Ray.html">Ray Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wootters:Mary.html">Mary Wootters</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.02270">PDF</a><br/><b>Abstract: </b>Lifted Reed Solomon Codes (Guo, Kopparty, Sudan 2013) were introduced in the
context of locally correctable and testable codes. They are multivariate
polynomials whose restriction to any line is a codeword of a Reed-Solomon code.
We generalize their construction by introducing lifted multiplicity codes,
multivariate polynomial codes whose restriction to any line is a codeword of a
multiplicity code (Kopparty, Saraf, Yekhanin 2014). We show that lifted
multiplicity codes have a better trade-off between redundancy and a notion of
locality called the $t$-disjoint-repair-group property than previously known
constructions. More precisely, we show that lifted multiplicity codes with
length $N$ and redundancy $O(t^{0.585} \sqrt{N})$ have the property that any
symbol of a codeword can be reconstructed in $t$ different ways, each using a
disjoint subset of the other coordinates. This gives the best known trade-off
for this problem for any super-constant $t &lt; \sqrt{N}$. We also give an
alternative analysis of lifted Reed Solomon codes using dual codes, which may
be of independent interest.
</p></div>
    </summary>
    <updated>2019-05-08T23:20:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/067</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/067" rel="alternate" type="text/html"/>
    <title>TR19-067 |  Sign rank vs Discrepancy | 

	Hamed Hatami, 

	Kaave Hosseini, 

	Shachar Lovett</title>
    <summary>Sign-rank and discrepancy are two central notions in communication complexity. The seminal work of  Babai, Frankl, and Simon from 1986  initiated an active line of research that investigates  the gap between these two notions.
In this article, we establish the strongest possible separation  by constructing a Boolean matrix whose sign-rank is only $3$, and yet its discrepancy is  $2^{-\Omega(n)}$. We note that every matrix of sign-rank $2$ has discrepancy $n^{-O(1)}$.
Our result in particular implies that there are Boolean functions with $O(1)$ unbounded error randomized communication complexity while having $\Omega(n)$ weakly unbounded error randomized communication complexity.</summary>
    <updated>2019-05-07T09:52:08Z</updated>
    <published>2019-05-07T09:52:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-09T22:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15840</id>
    <link href="https://rjlipton.wordpress.com/2019/05/06/the-network-coding-conjecture-is-powerful/" rel="alternate" type="text/html"/>
    <title>The Network Coding Conjecture Is Powerful</title>
    <summary>More hard Boolean functions Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen (AFKL) have a recent paper which we just discussed. Today Ken and I will update our discussion. Their paper assumes the network coding conjecture (NCC) and proves a lower bound on the Boolean complexity of integer multiplication. The main result of AFKL […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>More hard Boolean functions</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2019/05/akklshifted.jpg"><img alt="" class="alignright wp-image-15841" height="162" src="https://rjlipton.files.wordpress.com/2019/05/akklshifted.jpg?w=197&amp;h=162" width="197"/></a></p>
<p>
Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen (AFKL) have a recent <a href="https://arxiv.org/abs/1902.10935">paper</a> which we just <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">discussed</a>. </p>
<p>
Today Ken and I will update our discussion. </p>
<p>
Their paper assumes the network coding conjecture (NCC) and proves a lower bound on the Boolean complexity of integer multiplication. The main result of AFKL is:</p>
<blockquote><p><b>Theorem 1</b> <em> If the NCC is true, then every Boolean circuit that computes the <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/> function has size <img alt="{\Omega(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Omega(n \log n)}"/>. </em>
</p></blockquote>
<p/><p>
The <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/> function is: Given an <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-bit number <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and a number <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> so that <img alt="{1 \le k \le n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+k+%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \le k \le n}"/>, compute the <img alt="{2n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2n}"/>-bit product of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> by <img alt="{2^{k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{k}}"/>: 	</p>
<p align="center"><img alt="\displaystyle  x \times 2^{k}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%5Ctimes+2%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x \times 2^{k}. "/></p>
<p>This is a special case of the integer multiplication problem. In symbols it maps <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> to <img alt="{0^k x 0^{n-k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5Ek+x+0%5E%7Bn-k%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0^k x 0^{n-k}}"/>, as in our photo above. </p>
<p>
Our point, however, is not about integer multiplication. Nor even about NCC—no knowledge of it will be needed today, so read on even if you are not aware of NCC. No. Our point is that a whole lot of other Boolean functions would inherit the same <img alt="{\Omega(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(n \log n)}"/> circuit lower bound as <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/>. And several aspects of that seem troubling.</p>
<p>
</p><p/><h2> Some Worry </h2><p/>
<p/><p>
We are impressed by the AFKL paper but also worried. Proving a super-linear lower bound in the unrestricted Boolean complexity model has long been considered a difficult problem. Maybe a hopeless problem. Yes they are proving it not for a single-output function; they are proving it for a multiple-output function. Still I thought that it seems too good to be correct. Even worse, assuming NCC they also resolve other open problems in complexity theory. I am worried.</p>
<p>
What we suggest is to catalog and study the consequences of their results. If we find that their results lead to a contradiction, then there was something to be worried about. Or perhaps it would mean that NCC is false. If we find no contradiction, then everything we discover is also a consequence of NCC. Either way we learn more.</p>
<p>
</p><p/><h2> AFKL Functions </h2><p/>
<p/><p>
Let’s call a Boolean function an <i>AFKL function</i> provided it has Boolean circuit complexity <img alt="{\Omega(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(n \log n)}"/> if the NCC is true. Thanks to AFKL, we now know that integer multiplication is an AFKL function. I started to think about: What functions are in this class? Here are some examples: </p>
<ul>
<li>
Integer multiplication <p/>
</li><li>
Integer multiplication by a power of <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> <p/>
</li><li>
<img alt="{\mathsf{FLIP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}}"/> <p/>
</li><li>
Discrete convolution <p/>
</li><li>
Sparse convolution
</li></ul>
<p>
We describe the last three next. We show they have linear size-preserving reductions from the <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/> function.</p>
<p>
</p><p/><h2> The Flip Function </h2><p/>
<p/><p>
Define <img alt="{\mathsf{FLIP}_{n}: \{0,1\}^{n} \rightarrow \{0,1\}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%3A+%5C%7B0%2C1%5C%7D%5E%7Bn%7D+%5Crightarrow+%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}_{n}: \{0,1\}^{n} \rightarrow \{0,1\}^{n}}"/> by </p>
<p align="center"><img alt="\displaystyle  \mathsf{FLIP}_{n}(0^{k}1\alpha)= 1\alpha 0^{k}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BFLIP%7D_%7Bn%7D%280%5E%7Bk%7D1%5Calpha%29%3D+1%5Calpha+0%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{FLIP}_{n}(0^{k}1\alpha)= 1\alpha 0^{k}. "/></p>
<p>for <img alt="{k \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k \ge 1}"/>. For any input not of this form, let <img alt="{\mathsf{FLIP}_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}_{n}}"/> be <img alt="{0^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0^{n}}"/>.</p>
<blockquote><p><b>Theorem 2</b> <em> The Boolean function <img alt="{\mathsf{FLIP}_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}_{n}}"/> is an AFKL function. </em>
</p></blockquote>
<p/><p>
<em>Proof:</em>  Let <img alt="{x,k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,k}"/> be the input to <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/> where <img alt="{|x| = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cx%7C+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|x| = n}"/> and <img alt="{k \leq n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k \leq n}"/> in binary. In linear size we can test <img alt="{k = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = 0}"/>, when there is nothing to do, so presume <img alt="{k \geq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k \geq 1}"/>. The first step is to create </p>
<p align="center"><img alt="\displaystyle  0^{n-k}10^{k-1}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0%5E%7Bn-k%7D10%5E%7Bk-1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  0^{n-k}10^{k-1}. "/></p>
<p>This is just binary-to-unary conversion and has linear-size circuits—as in multiplex decoding and as remarked by AFKL. This becomes the first <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits of an application of <img alt="{\mathsf{FLIP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}}"/> to the <img alt="{2n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2n}"/>-bit string </p>
<p align="center"><img alt="\displaystyle  0^{n-k}10^{k-1} x. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0%5E%7Bn-k%7D10%5E%7Bk-1%7D+x.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  0^{n-k}10^{k-1} x. "/></p>
<p>It yields </p>
<p align="center"><img alt="\displaystyle  10^{k-1} x 0^{n-k}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++10%5E%7Bk-1%7D+x+0%5E%7Bn-k%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  10^{k-1} x 0^{n-k}. "/></p>
<p>Changing the first bit to <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> then leaves the desired output of <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/>. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
The point is that <img alt="{\mathsf{FLIP}_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}_{n}}"/> is a super-simple function. It just moves the initial block of <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>‘s in a string to the end. It is amazing that this function should have only non-linear, indeed <img alt="{\Omega(n\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(n\log n)}"/>-sized, circuits. </p>
<p>
This also means that Ken’s <a href="https://blog.computationalcomplexity.org/2007/07/concrete-open-problem.html">function</a>, which takes <img alt="{x \in \{0,1,2\}^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%5C%7B0%2C1%2C2%5C%7D%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x \in \{0,1,2\}^*}"/> and moves all the <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>s to the end of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, is hard even in the special cases where all the <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>‘s are at the front. What’s strange is that Ken proves his function equivalent to another special case where <img alt="{|x|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cx%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|x|}"/> is even and exactly half the characters are <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. This latter case is one in which <img alt="{\mathsf{FLIP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}}"/> is easy, but the two cases are separate. All this is touch-and-go enough to compound our “worry.”</p>
<p>
</p><p/><h2> The Sparse Convolution Function </h2><p/>
<p/><p>
The following is also an AFKL function. 	</p>
<p align="center"><img alt="\displaystyle  y_{l} = \bigvee_{i=1}^{n-l} w_{i} \wedge x_{i+l}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bl%7D+%3D+%5Cbigvee_%7Bi%3D1%7D%5E%7Bn-l%7D+w_%7Bi%7D+%5Cwedge+x_%7Bi%2Bl%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{l} = \bigvee_{i=1}^{n-l} w_{i} \wedge x_{i+l}, "/></p>
<p>for <img alt="{l=1,\dots,n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bl%3D1%2C%5Cdots%2Cn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{l=1,\dots,n}"/> where an empty OR is defined to be <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. This can even further be restricted to the case where exactly one of the <img alt="{w_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_{i}}"/> are <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> and the rest are <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. Call this the <i>sparse convolution function</i>.</p>
<blockquote><p><b>Theorem 3</b> <em> The sparse convolution is a monotone AFKL function. </em>
</p></blockquote>
<p/><p>
<em>Proof:</em>  We will give a sketch of why this is true. Define 	</p>
<p align="center"><img alt="\displaystyle  y_{l} = \bigvee_{i = 1}^{n-l} \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1} \wedge x_{i+l}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bl%7D+%3D+%5Cbigvee_%7Bi+%3D+1%7D%5E%7Bn-l%7D+%5Cbar%7Bx%7D_%7B1%7D+%5Cwedge+%5Ccdots+%5Cwedge+%5Cbar%7Bx%7D_%7Bi%7D+%5Cwedge+x_%7Bi%2B1%7D+%5Cwedge+x_%7Bi%2Bl%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{l} = \bigvee_{i = 1}^{n-l} \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1} \wedge x_{i+l}. "/></p>
<p>It is not hard to show that this yields the FLIP function. We can reduce computing it to a convolution of the <img alt="{x_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{i}}"/>‘s and <img alt="{\Gamma(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Gamma(i)}"/> where 	</p>
<p align="center"><img alt="\displaystyle  \Gamma(i) = \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma%28i%29+%3D+%5Cbar%7Bx%7D_%7B1%7D+%5Cwedge+%5Ccdots+%5Cwedge+%5Cbar%7Bx%7D_%7Bi%7D+%5Cwedge+x_%7Bi%2B1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Gamma(i) = \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1}. "/></p>
<p>The key is to note that exactly one <img alt="{\Gamma(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Gamma(i)}"/> will be non-zero, and so the convolution is sparse. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
The sparse convolution function raises an interesting question: Are the methods for sparse FFT useful here? The lower bound for AFKL functions suggests that they are not applicable. </p>
<p>
</p><p/><h2> Is the NCC-Boolean Connection New? </h2><p/>
<p/><p>
The subtitle of our <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">post</a> marveled that a core-theory advance on circuits for multiplication had come via the practical side of throughput in computer networks. AFKL deserve plaudits for linking two communities. We should mention that one theoretician we both know well, Mark Braverman, with his students Sumegha Garg and Ariel Schvartzman at Princeton, <a href="https://arxiv.org/pdf/1608.06545.pdf">proved</a> a fact about NCC that is relevant to this discussion:</p>
<blockquote><p><b>Theorem 4</b> <em><a name="BGS"/> Either NCC is false, or bit-operations save a whole <img alt="{(\log n)^{\Omega(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7B%5COmega%281%29%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{(\log n)^{\Omega(1)}}"/> factor in the network size. </em>
</p></blockquote>
<p/><p>
Even this paper, however, does not address lower bounds on Boolean circuits. The only prior link between NCC and Boolean complexity is a 2007 <a href="https://www.combinatorics.org/ojs/index.php/eljc/article/view/v14i1r44">paper</a> by Søren Riis, which is cited by AFKL, and has a 2011 <a href="http://emis.impa.br/EMIS/journals/EJC/Volume_18/PDF/v18i1p192.pdf">followup</a> by Demetres Christofides and Klas Markström. The paper by Riis has a new “guessing game” on graphs and a demonstration that a lower-bound conjecture of Leslie Valiant needs to be rescued by dividing by a <img alt="{\log\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log\log n}"/> factor. Theorem <a href="https://rjlipton.wordpress.com/feed/#BGS">4</a>, however, seems to say that no such <img alt="{\log\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log\log n}"/> shading can apply to NCC.</p>
<p>
When we ask Google “network coding conjecture Boolean circuit lower bounds” (without quotes), the first page shows AFKL, our posts, and this 2014 <a href="https://people.csail.mit.edu/rrw/ccc14-survey.pdf">survey</a> by Ryan Williams—which mentions neural networks but not NCC. On the next page of hits we see Riis and the followup paper but nothing else that seems directly relevant. Nor does appending `-multiplication’ help screen out AFKL and our posts.</p>
<p>
There is said to be empirical evidence for NCC. We wonder, however, whether that has reached the intensity of thought about circuit lower bounds. We say this because the implications from NCC make three giant steps:</p>
<ol>
<li>
Not only does it assert a super-linear circuit lower bound (okay, for a function with <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> output bits), but… <p/>
</li><li>
…it asserts <img alt="{\Omega(n\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(n\log n)}"/>… <p/>
</li><li>
…for functions that are easily in Turing machine linear time.
</li></ol>
<p>
So one side of our worry is whether NCC can actually shed light on so many fundamental issues from complexity theory, more than absorbing light. At the very least, AFKL have re-stimulated interest in all of these issues. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Is <img alt="{\mathsf{FLIP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}}"/> hard? Is NCC true? What other Boolean functions are AFKL functions? What about other consequences of the NCC to complexity theory?</p>
<p/></font></font></div>
    </content>
    <updated>2019-05-07T03:05:05Z</updated>
    <published>2019-05-07T03:05:05Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="Results"/>
    <category term="AFKL functions"/>
    <category term="Casper Freksen"/>
    <category term="circuit complexity"/>
    <category term="conjecture"/>
    <category term="integer multiplication"/>
    <category term="Kasper Larsen"/>
    <category term="Lior Kamma"/>
    <category term="lower bounds"/>
    <category term="Mark Braverman"/>
    <category term="network coding"/>
    <category term="Peyman Afshani"/>
    <category term="reductions"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-09T22:20:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://kintali.wordpress.com/?p=1235</id>
    <link href="https://kintali.wordpress.com/2019/05/06/preventing-future-college-admissions-scandals-using-blockchain/" rel="alternate" type="text/html"/>
    <title>Preventing future college admissions scandals using Blockchain</title>
    <summary>The recent college admissions scandal resulted in the largest case of its kind to be prosecuted by the US Justice Department. A massive federal investigation code-named ‘Operation Varsity Blues’ uncovered this scandal and charged several high-profile people with bribery, racketeering, money laundering, conspiracy to commit mail and wire fraud. The internet commentary about this topic includes phrases […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="graf graf--p graf-after--h3" id="5bf6">The recent <a class="markup--anchor markup--p-anchor" href="https://en.wikipedia.org/wiki/2019_college_admissions_bribery_scandal" rel="nofollow noopener noopener noopener nofollow noopener" target="_blank">college admissions scandal</a> resulted in the largest case of its kind to be prosecuted by the US Justice Department. A massive federal investigation code-named ‘Operation Varsity Blues’ uncovered this scandal and charged several high-profile people with bribery, racketeering, money laundering, conspiracy to commit mail and wire fraud. The internet commentary about this topic includes phrases like “broken admissions system”, “rich can buy their way into college”, etc etc.</p>
<p class="graf graf--p graf-after--p" id="4b6a">There must be several other cases of fraud that go unnoticed on a daily basis. It’s in human nature to shortcut the rules, collude and cheat to achieve one’s own short-term goals, hoping to get away with one’s fraudulent actions. Let’s discuss how to use Blockchain technology and create a rigorous system to prevent some aspects of such scandals in future.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="b66e"><strong class="markup--strong markup--p-strong">Problems</strong>: Fake athletic certificates and phony athletic profile. Taking photos of students on a stationary rowing machine. Photoshopping students’ face on another athlete’s photo.</p>
<p class="graf graf--p graf-after--p" id="ad90"><strong class="markup--strong markup--p-strong">Solution</strong>: A genuine high-school athlete achieves his/her athletic credentials during a four year period. Getting a genuine athletic certificate involves achieving several intermediate goals. For example, if you achieved a black belt in karate in your 11th grade, you must have progressed through beginner level (with white, yellow and orange belts), intermediate level (with green, blue, purple, brown and red belts) and then reached the advanced level (with a black belt). <a class="markup--anchor markup--p-anchor" href="https://truecerts.co/" rel="noopener nofollow" target="_blank">TrueCerts</a> technology allows sports coaching centers to create certificates for each of these intermediate athletic achievements, sign them using their private keys and post only the SHA-256 hash of the certificate on a public blockchain, thus immutably time-stamping each achievement at the specific day/time the athlete achieved it. The athletic photos can be time-stamped similarly.</p>
<p class="graf graf--p graf-after--p" id="2c0c">This process achieves the seemingly impossible combination of <strong class="markup--strong markup--p-strong">security, privacy and transparency </strong>!! Security is achieved by the asymmetric key encryption. Privacy is achieved by the one-way nature of the cryptographic hash function <a class="markup--anchor markup--p-anchor" href="https://en.wikipedia.org/wiki/SHA-2" rel="noopener nofollow" target="_blank">SHA 256</a>. Transparency is achieved by the fact that anybody with the access to the sports certificate (upon student’s consent) can compute the SHA 256 hash of the certificate and verify that it is stored on a public blockchain validly signed by the private key of the issuer (sports coaching center). This eliminates the fraudulent behavior of creating a bunch of fake credentials and photos, all at once, in a brief period of time. Using a fairly decentralized public blockchain is very important here.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="c253"><strong class="markup--strong markup--p-strong">Problem</strong>: Bribing coaches to accept certain students in their sports teams or issue fake credentials.</p>
<p class="graf graf--p graf-after--p" id="6290"><strong class="markup--strong markup--p-strong">Solution</strong>: This problem can be solved by having several people (perhaps five coaches, some administrative assistants, one principal, one vice-principal, etc) in the organization collectively responsible (using multi-signature wallets and m-of-n signatures) to issue credentials. Each of the involved person is accountable for every issued certificate.</p>
<p class="graf graf--p graf-after--p" id="eb99">Bribing one coach is easy. Bribing ten people is hard. People often hesitate bribing multiple people. Collusion becomes increasingly hard when you increase the size of the group involved. When there are more people involved, there is a higher chance that at least one of them is honest (and brave) to overcome the pressure of the others and blow the whistle.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="0d42"><strong class="markup--strong markup--p-strong">Problem</strong>: Fake college entrance exam (SAT, ACT) test scores</p>
<p class="graf graf--p graf-after--p" id="fe6e"><strong class="markup--strong markup--p-strong">Solution</strong>: Current paper-based test score issuance and verification system is too time-consuming and error-prone. These credentials can be easily faked or tampered with. An ideal solution involves creating a digital certificate, validly signed (or multi-signed) by the issuer and time-stamped with a <a class="markup--anchor markup--p-anchor" href="https://en.wikipedia.org/wiki/SHA-2" rel="nofollow noopener" target="_blank">SHA 256</a> hash on a public blockchain. See <a class="markup--anchor markup--p-anchor" href="https://medium.com/@truecerts/dr-kintalis-motivation-behind-developing-truecerts-platform-aba93f4d290a" rel="noopener" target="_blank">my previous post</a> about preventing fraud in academic transcripts and making the entire system efficient.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="b957"><strong class="markup--strong markup--p-strong">Problem</strong>: Other students taking entrance tests on your behalf.</p>
<p class="graf graf--p graf-after--p" id="0e07"><strong class="markup--strong markup--p-strong">Solution</strong>: This is a problem of verifying the identity of the student taking the test. The current system of using paper-based credentials is broken. It’s easy to create fake driver’s license, passports etc. At <a class="markup--anchor markup--p-anchor" href="https://truecerts.co/" rel="nofollow noopener" target="_blank">TrueCerts</a>, we have created an automated multi-step identity verification that combines your standard KYC identity procedures, utility bills and more importantly biometrics. We define identity as a several data points achieved over a period of time, not just one piece of paper. It’s very hard to cheat all of these steps. If you have a look-alike twin then consider yourself lucky <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/></p>
<p> </p>
<p class="graf graf--p graf-after--p" id="a16a"><strong class="markup--strong markup--p-strong">Partially solvable problems</strong>: Bribing officials to change student’s answers in paper-based exam can be solved to certain extent by using a completely computer-based exam. Bribing proctors to tell the answers to a student (during the test) can be solved with computer-vision-based cheating detection software. Some photoshopped images can be detected using image analysis techniques.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="4d0f"><strong class="markup--strong markup--p-strong">Hard to solve problems</strong>: (1) Getting a fake medical certificate that your kid requires an isolated room to take the test and then bribing the proctor to tell him/her all the answers during the test. (2) Laundering bribes using a non-profit entity. Phew…. These things actually happened during this college admissions scandal. As the famous saying goes “a person is capable of as much atrocity as he/she has imagination”.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="5ad0">In summary, combining the existing technologies we can solve several of the above mentioned problems and simultaneously achieve <strong class="markup--strong markup--p-strong">security, privacy and transparency</strong>. The main goal here is to make the bad people’s job as difficult as possible and simultaneously making the good people’s job very efficient.</p>
<p class="graf graf--p graf-after--p" id="a4f5">If you want to know more about how we (at <a class="markup--anchor markup--p-anchor" href="https://truecerts.co/" rel="nofollow noopener" target="_blank">TrueCerts</a>) are using Blockchain and other technologies to prevent fraud and corruption in broad range of areas, <a class="markup--anchor markup--p-anchor" href="https://truecerts.co/" rel="nofollow noopener" target="_blank">contact us</a>.</p>
<p class="graf graf--p graf-after--p" id="e889">Stay tuned for my next post about a huge list of real-world fraud and corruption stories that can be prevented rigorously by using cutting-edge technologies.</p>
<p><img alt="TrueCertsTrustSimplified" class="alignnone size-full wp-image-1236" src="https://kintali.files.wordpress.com/2019/05/truecertstrustsimplified.png?w=660"/></p></div>
    </content>
    <updated>2019-05-07T01:29:32Z</updated>
    <published>2019-05-07T01:29:32Z</published>
    <category term="Education"/>
    <category term="Uncategorized"/>
    <category term="bitcoin"/>
    <category term="blockchain"/>
    <category term="college admissions scandal"/>
    <author>
      <name>kintali</name>
    </author>
    <source>
      <id>https://kintali.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/e1376dd220aa259d0efd0638d7619231?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://kintali.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://kintali.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://kintali.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://kintali.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computational Complexity, Polyhedral Combinatorics, Algorithms and Graph Theory</subtitle>
      <title>My Brain is Open</title>
      <updated>2019-05-09T22:20:59Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2431469456247088523</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2431469456247088523/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/thoughts-on-recent-jeopardy-streak.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2431469456247088523" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2431469456247088523" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/thoughts-on-recent-jeopardy-streak.html" rel="alternate" type="text/html"/>
    <title>Thoughts on the recent Jeopardy streak (SPOILERS)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">James Holzhauer  has won 22 consecutive games of Jeopardy and has made around 1.6 million dollars. Nice work if you can get it. Here are some thoughts no this<br/>
<br/>
1) Before James H the records for number of consecutive games was, and still is, Ken Jennings winning 74 in a row, and second place was 20. I was surprised that Ken was that much better than the competition.<br/>
<br/>
2) Before James H the record for amount of money in normal play (not extra from, say, tournament of champions or losing to a computer) was around 400,000. I was surprised that Ken was that much better than the competition.<br/>
<br/>
3) James is obliterating the records for most wins in a single game. He holds the top 12 records for this.  This is due to his betting A LOT on the daily doubles and the final jeop, as well as of course answering so many questions right.<br/>
<br/>
4) One reason players in Jeopardy don't have long streaks is fatigue. The actually play<br/>
5 games a day, two days of the week.  James H is getting a break since he has two weeks off now since they will soon have the Teachers Tournament. This could work either way--- he gets a break or he loses being in-the-zone.<br/>
<br/>
5) James strategy is:<br/>
<br/>
a) Begin with the harder (and more lucrative) questions.<br/>
<br/>
b) Bet A LOT on the daily doubles (which are more likely to be in the more lucrative questions) and almost always go into final jeop with more than twice your opponent (He failed to do this only once.)<br/>
<br/>
c) Bet A LOT on Final Jeop- though not enough so that if you lose you lose the game. I think he's gotten every Final Jeop question right.<br/>
<br/>
For more on his strategy see this article by Oliver Roeder in Nate Silvers Blog: <a href="https://fivethirtyeight.com/features/the-man-who-solved-jeopardy/">here</a><br/>
<br/>
6) I tend to think of this as being a high-risk, high-reward strategy and thus it is unlikely he will beat Ken Jennings, but every time he wins that thought seems sillier and sillier. While we are here, how likely is it that someone will beat Ken Jennings? In an article before all of this Ben Morrison in Nate Silvers Blog wrote that it was quite likely SOMEONE would break Ken J's record,  see <a href="https://fivethirtyeight.com/features/ken-jennings-has-nothing-on-joe-dimaggio/">here</a>.<br/>
<br/>
7) OKAY, how does James H compare to Ken J? According to Oliver Roeder in Nate Silvers Blog,<br/>
<a href="https://fivethirtyeight.com/features/the-battle-for-jeopardy-supremacy/">here</a>, they are similar in terms of percent of questions answered right, but James H bets so much more (bets better?) which is why he is getting so much money. I'll be curious to see a head-to-head contest at some point. But to the issue at hand, they don't give James H that good a chance to break Ken J's record.<br/>
<br/>
8) Jeop used to have a  5-game limit. Maybe that was a good idea- its not that interesting seeing the same person with the same strategy win 22 in a row. Also, the short-talk-with-Alex T-- James is running out of interesting things to say. I wonder what Alex did with Ken J after 50 games.<br/>
``So Ken, I hear you're good at Jeopardy''<br/>
<br/>
9) Misc: Ken J was the inspiration for IBM to do Watson.<br/>
<br/>
10) Will future players use James Strategy? Note that you have to be REALLY GOOD in the first place for it to help you. Maybe a modified version where you go for the lucrative questions and bet a lot on Daily Doubles (more than people have done in the past) when its an area you know really well (I'll take Ramsey Theory for $2000.)<br/>
<br/>
11) I used to DVR and watch Jeop but didn't mind if I was a few behind. Now I have to stay on top of it so articles like those pointed to above don't give me a spoiler.<br/>
<br/>
12) My prediction: He will beat Ken Jenning for money but not for number-of-games. I have no real confidence in these predictions.</div>
    </content>
    <updated>2019-05-07T00:41:00Z</updated>
    <published>2019-05-07T00:41:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-09T12:31:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17422</id>
    <link href="https://gilkalai.wordpress.com/2019/05/07/answer-to-tyi-37-arithmetic-progressions-in-3d-brownian-motion/" rel="alternate" type="text/html"/>
    <title>Answer to TYI 37: Arithmetic Progressions in 3D Brownian Motion</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Consider a Brownian motion in three dimensional space. We asked (TYI 37) What is the largest number of points on the path described by the motion which form an arithmetic progression? (Namely, , so that all are equal.) Here is … <a href="https://gilkalai.wordpress.com/2019/05/07/answer-to-tyi-37-arithmetic-progressions-in-3d-brownian-motion/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Consider a Brownian motion in three dimensional space. <a href="https://gilkalai.wordpress.com/2019/03/07/test-your-intuition-or-simply-guess-37-arithmetic-progressions-for-brownian-motion-in-space/">We asked (TYI 37)</a> What is the largest number of points on the path described by the motion which form an arithmetic progression? (Namely, <img alt="x_1,x_2, x_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2Cx_2%2C+x_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1,x_2, x_t"/>, so that all <img alt="x_{i+1}-x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bi%2B1%7D-x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_{i+1}-x_i"/> are equal.)</p>
<p>Here is what you voted for</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/poll-apb.png"><img alt="" class="alignnone size-full wp-image-17423" src="https://gilkalai.files.wordpress.com/2019/05/poll-apb.png?w=640"/></a></p>
<p><span style="color: #ff0000;"><strong>TYI37 poll: Final-results</strong></span></p>
<p>Analysis of the poll results:  Almost surely 2 is the winner with 30.14% of the 209 votes, and almost surely infinity (28.71%) comes close at second place. In the  third place is  almost surely 3 (14.83%),  and then comes positive probability for each integer (13.4%), almost surely 5 (5.26%),  almost surely 6 (2.87%), and  almost surely 4 (2.39%).</p>
<h2>Test your political intuition: which coalition is going to be formed?</h2>
<p>Almost surely 2 (briefly AS2) and almost surely infinity (ASI) can form a government  with no need for a larger coalition. But they represent two political extremes. Is AS3 politically closer to AS2 or to ASI? “k with probability p_k for every k&gt;2” (briefly, COM) represent a complicated political massage. Is it closer to AS2 or to ASI? (See the old posts on <a href="https://gilkalai.wordpress.com/2009/02/16/which-coalition/">which coalition</a> <a href="https://gilkalai.wordpress.com/2009/02/17/which-coalition-to-form-2/">will be formed</a>.)</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/poll-br.png"><img alt="" class="alignnone size-medium wp-image-17424" height="300" src="https://gilkalai.files.wordpress.com/2019/05/poll-br.png?w=131&amp;h=300" width="131"/></a> <a href="https://gilkalai.files.wordpress.com/2019/05/poll-brown.png"><img alt="" class="alignnone size-medium wp-image-17425" height="300" src="https://gilkalai.files.wordpress.com/2019/05/poll-brown.png?w=133&amp;h=300" width="133"/> </a><a href="https://gilkalai.files.wordpress.com/2019/05/poll189.png"><img alt="" class="alignnone size-medium wp-image-17427" height="300" src="https://gilkalai.files.wordpress.com/2019/05/poll189.png?w=127&amp;h=300" width="127"/></a></p>
<p><span style="color: #ff0000;"><strong>TYI37 poll: Partial results. It was exciting to see how the standing of the answers changed in the process of counting the votes.</strong></span></p>
<p>And the correct answer is: <span id="more-17422"/></p>
<h2><strong>5 (FIVE)</strong></h2>
<p>See the paper:</p>
<p class="title mathjax">Itai Benjamini and Gady Kozma: <a href="https://arxiv.org/abs/1810.10077">Arithmetic progressions in the trace of Brownian motion in space</a></p>
<p> </p></div>
    </content>
    <updated>2019-05-06T21:38:48Z</updated>
    <published>2019-05-06T21:38:48Z</published>
    <category term="Combinatorics"/>
    <category term="Open discussion"/>
    <category term="Probability"/>
    <category term="Brownian motion"/>
    <category term="Gady Kozma"/>
    <category term="Itai Benjamini"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-05-09T22:20:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4240</id>
    <link href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 3: Follow the Regularized Leader</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The multiplicative weights algorithm is simple to define and analyze, and it has several applications, but both its definition and its analysis seem to come out of nowhere. We mentioned that all the quantities arising in the algorithm and its … <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 The multiplicative weights algorithm is simple to define and analyze, and it has several applications, but both its definition and its analysis seem to come out of nowhere. We mentioned that all the quantities arising in the algorithm and its analysis have statistical physics interpretations, but even this observation brings up more questions than it answers. The Gibbs distribution, for example, does put more weight on lower-energy states, and so it makes sense in an optimization setting, but to get good approximations one wants to use lower temperatures, while the distributions used by the multiplicative weights algorithms have temperature <img alt="{1/\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/\epsilon}"/>, where <img alt="{2\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\epsilon}"/> is the final “amortized” regret bound, so that one uses, quite counterintuitively, higher temperatures for better approximations. </p>
<p>
Furthermore, it is not clear how we would generalize the ideas of multiplicative weights to the case in which the set of feasible solutions <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is anything other than the set of distributions.</p>
<p>
Today we discuss the <em>“Follow the Regularized Leader”</em> method, which provides a framework to design and analyze online algorithms in a versatile and well-motivated way. We will then see how we can “discover” the definition and analysis of multiplicative weights, and how to “discover” another online algorithm which can be seen as a generalization of projected gradient descent (that is, one can derive the projected gradient descent algorithm and its analysis from this other online algorithm).</p>
<p>
<span id="more-4240"/></p>
<p>
</p><p><b>1. Follow The Regularized Leader </b></p>
<p/><p>
We will first state some results in full generality, making no assumptions on the set <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> of feasible solutions or on the set of loss functions <img alt="{f_t : K \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t : K \rightarrow {\mathbb R}}"/> encountered by the algorithm at each step.</p>
<p>
Let us try to define an online optimization algorithm from scratch. The solution <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> proposed by the algorithm at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> can only depend on the previous cost functions <img alt="{f_1,\ldots,f_{t-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,\ldots,f_{t-1}}"/>; how should it depend on it? If the offline optimal solution <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is consistently better than all others at each time step, then we would like <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> to be that solution, so we want <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> to be a solution that would have worked well in the previous steps. The most extreme way of implementing this idea is the <em>Follow the Leader</em> algorithm (abbreviated FTL), in which we set the solution at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/></p>
<p/><p align="center"><img alt="\displaystyle  x_t := \arg\min_{x\in K} \sum_{k=1}^{t-1} f_k(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t := \arg\min_{x\in K} \sum_{k=1}^{t-1} f_k(x) "/></p>
<p> to be the best solution for the previous steps. (Note that the algorithm does not prescribe what solution to use at step <img alt="{t=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1}"/>.)</p>
<p>
It is possible for FTL to perform very badly. Consider for example the “experts” setting in which we analyzed multiplicative weights: the set of feasible solutions <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is the set <img alt="{\Delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Delta}"/> of probability distributions over <img alt="{\{1,\ldots,n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{1,\ldots,n\}}"/>, and the cost functions are linear <img alt="{f_t(x) = \sum_i \ell_t(i) x(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Csum_i+%5Cell_t%28i%29+x%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x) = \sum_i \ell_t(i) x(i)}"/> with coefficients <img alt="{0\leq \ell_t(i) \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5Cleq+%5Cell_t%28i%29+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0\leq \ell_t(i) \leq 1}"/>. Suppose that <img alt="{n=2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=2}"/> and that <img alt="{x_1 = (0.5,.0.5)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%280.5%2C.0.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1 = (0.5,.0.5)}"/>. Then a possible run of the algorithm could be: </p>
<ol>
<li> <img alt="{x_1 = (.5,.5)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%28.5%2C.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1 = (.5,.5)}"/>, <img alt="{\ell_1 = (0,.5)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1+%3D+%280%2C.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_1 = (0,.5)}"/>
</li><li> <img alt="{x_2 = (1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_2+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_2 = (1,0)}"/>, <img alt="{\ell_2 = (1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_2 = (1,0)}"/>
</li><li> <img alt="{x_3 = (0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_3+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_3 = (0,1)}"/>, <img alt="{\ell_3 = (0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_3+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_3 = (0,1)}"/>
</li><li> <img alt="{x_4 = (1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_4+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_4 = (1,0)}"/>, <img alt="{\ell_4 = (1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_4+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_4 = (1,0)}"/>
</li><li> <img alt="{x_5 = (0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_5+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_5 = (0,1)}"/>, <img alt="{\ell_5 = (0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_5+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_5 = (0,1)}"/>
</li></ol>
<p align="center"><img alt="\displaystyle \vdots" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cvdots&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \vdots"/></p>
<p>
In which, after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps, the algorithm suffers a loss of <img alt="{T- O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT-+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T- O(1)}"/> while the offline optimum is <img alt="{T/2 + O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2F2+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T/2 + O(1)}"/>. Thus, the regret is about <img alt="{T/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T/2}"/>, which compares very unfavorably to the <img alt="{O(\sqrt T)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+T%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\sqrt T)}"/> regret of the multiplicative weight algorithm. For general <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, a similar example shows that the regret of FTL can be as high as about <img alt="{T\cdot \left( 1- \frac 1n \right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%5Ccdot+%5Cleft%28+1-+%5Cfrac+1n+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T\cdot \left( 1- \frac 1n \right)}"/>.</p>
<p>
In the above bad example, the algorithm keeps “overfitting” to the past history: if an expert is a bit better than the others, the algorithm puts all its probability mass on that expert, and the algorithm keeps changing its mind at every step. Interestingly, this is the only failure mode of the algorithm.</p>
<blockquote><p><b>Theorem 1 (Analysis of FTL)</b> <em> For any sequence of cost functions <img alt="{f_1,\ldots,f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,\ldots,f_t}"/> and any number of time steps <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>, the FTL algorithm satisfies the regret bound </em></p><em>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \sum_{t=1}^T f_t(x_t ) - f_t(x_{t+1} ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t+%29+-+f_t%28x_%7Bt%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \sum_{t=1}^T f_t(x_t ) - f_t(x_{t+1} ) "/></p>
</em><p><em> </em></p></blockquote>
<p> So that if the functions <img alt="{f_t(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(\cdot)}"/> are Lipschitz with respect to a distance function on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>, then the only way for the regret to be large is for <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> to typically be far, in that distance, from <img alt="{x_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{t+1}}"/>.</p>
<p>
<em>Proof:</em>  Recalling the definition of regret, </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T := \sum_{t=1}^T f_t(x_t) - \min_{x\in K} \sum_{t=1}^T f_t(x) \ , " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3A%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T := \sum_{t=1}^T f_t(x_t) - \min_{x\in K} \sum_{t=1}^T f_t(x) \ , "/></p>
<p> the theorem is equivalent to <a name="ftl.analysis"/></p><a name="ftl.analysis">
<p align="center"><img alt="\displaystyle   \sum_{t=1}^T f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t(x) \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_%7Bt%2B1%7D%29+%5Cleq+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \sum_{t=1}^T f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t(x) \ \ \ \ \ (1)"/></p>
</a><p><a name="ftl.analysis"/> We will prove <a href="https://lucatrevisan.wordpress.com/feed/#ftl.analysis">(1)</a> by induction. The base case <img alt="{T=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T=1}"/> is just the definition of <img alt="{x_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_2}"/>. Assuming that $latex {<a href="https://lucatrevisan.wordpress.com/feed/#ftl.analysis">(1)</a>}&amp;fg=000000$ is true up to <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> we have </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^{T+1} f_t (x_{t+1}) = \left( \sum_{t=1}^{T} f_t (x_{t+1}) \right) + f_{T+1} (x_{T+2}) \leq \sum_{t=1}^{T+1} f_t (x_{T+2}) = \min_{x\in K} \sum_{t=1}^{T+1} f_t(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t+%28x_%7Bt%2B1%7D%29+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5E%7BT%7D+f_t+%28x_%7Bt%2B1%7D%29+%5Cright%29+%2B+f_%7BT%2B1%7D+%28x_%7BT%2B2%7D%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t+%28x_%7BT%2B2%7D%29+%3D+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^{T+1} f_t (x_{t+1}) = \left( \sum_{t=1}^{T} f_t (x_{t+1}) \right) + f_{T+1} (x_{T+2}) \leq \sum_{t=1}^{T+1} f_t (x_{T+2}) = \min_{x\in K} \sum_{t=1}^{T+1} f_t(x) "/></p>
<p> where the middle step follows from the use of the inductive assumption, which gives </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^{T} f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t (x) \leq \sum_{t=1}^T f_t (x_{T+2}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5E%7BT%7D+f_t+%28x_%7Bt%2B1%7D%29+%5Cleq+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_%7BT%2B2%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^{T} f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t (x) \leq \sum_{t=1}^T f_t (x_{T+2}) "/></p>
<p> <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
The above example and analysis suggest that we should modify FTL in such a way that the choices of the algorithm don’t change too much from step to step, and that the solution <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> should be a compromise between optimizing with respect to previous cost functions and not changing too much from step to step.</p>
<p>
In order to do this, we introduce a new function <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/>, called a <em>regularizer</em> (more on it later), and, at each step, we compute the solution</p>
<p/><p align="center"><img alt="\displaystyle  x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} f_k(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+R%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} f_k(x) "/></p>
<p> This algorithm is called <em>Follow the Regularized Leader</em> or FTRL. Typically, the function <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> is chosen to be strictly convex and to take values that are rather big in magnitude. Then <img alt="{x_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1}"/> will be the unique minimum of <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> and, at each subsequent step, <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> will be selected in a way to balance the pull toward the minimum of <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> and the pull toward the FTL solution <img alt="{\arg\min_{x\in K} \sum_k f_k(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5Csum_k+f_k%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\arg\min_{x\in K} \sum_k f_k(x)}"/>. In particular, if <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> is large in magnitude compared to each <img alt="{f_t(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(\cdot)}"/>, the solution will not change too much from step to step.</p>
<p>
We have the following analysis that makes no assumptions on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>, on the cost functions <img alt="{f_t(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(\cdot)}"/> and on the regularizer (not even that the regularizer is convex).</p>
<blockquote><p><b>Theorem 2 (Analysis of FTRL)</b> <em> For every sequence of cost functions and every regularizer function, the regret after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps of the FTRL algorithm is bounded as follows: for every <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, </em></p><em>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T(x) \leq \left( \sum_{t=1}^T f_t(x_{t}) - f_t (x_{t+1}) \right) + R(x) - R(x_1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_%7Bt%7D%29+-+f_t+%28x_%7Bt%2B1%7D%29+%5Cright%29+%2B+R%28x%29+-+R%28x_1%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T(x) \leq \left( \sum_{t=1}^T f_t(x_{t}) - f_t (x_{t+1}) \right) + R(x) - R(x_1)"/></p>
<p> where </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T (x) := \sum_{t=1}^T f_t (x_t) - f_t (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%28x%29+%3A%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_t%29+-+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T (x) := \sum_{t=1}^T f_t (x_t) - f_t (x) "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
<em>Proof:</em>  Let us run for <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps the FTRL algorithm with regularizer <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> and cost functions <img alt="{f_1,\ldots,f_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,\ldots,f_T}"/>, and call <img alt="{x_1,\ldots,x_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cldots%2Cx_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1,\ldots,x_T}"/> the solutions computed by the FTL algorithm. </p>
<p>
Now consider the following mental experiment: we run the FTL algorithm for <img alt="{T+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T+1}"/> steps, with the sequence of cost functions <img alt="{R,f_1,\ldots,f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%2Cf_1%2C%5Cldots%2Cf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R,f_1,\ldots,f_t}"/>, and we use <img alt="{x_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1}"/> as a first solution. Then we see that the solutions computed by the FTL algorithm will be precisely <img alt="{x_1,x_1,x_2,\ldots,x_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%2Cx_1%2Cx_2%2C%5Cldots%2Cx_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1,x_1,x_2,\ldots,x_T}"/>. The regret bound for FTL implies that, for every <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>,</p>
<p/><p align="center"><img alt="\displaystyle  R(x_1) - R(x) + \sum_{t=1}^T f_t(x_t) - f_t(x) \leq R(x_1) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x_1%29+-+R%28x%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_t%28x%29+%5Cleq+R%28x_1%29+-+R%28x_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  R(x_1) - R(x) + \sum_{t=1}^T f_t(x_t) - f_t(x) \leq R(x_1) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1}) "/></p>
<p> <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
Having established these results, the general recipe to solve an online optimization problem will be to find a regularizer function <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> such that the minimum of <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> “pulls away from” solutions that would make the FTL algorithm overfit, and such that there is a good balance between how big <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> gets over <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> (because we pay <img alt="{R(x^*) - R(x_1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%5E%2A%29+-+R%28x_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(x^*) - R(x_1)}"/> in the regret, where <img alt="{x^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^*}"/> is the offline optimum) and how stable is the minimum of <img alt="{R(x) + \sum_{k=1}^{t-1} f_k(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(x) + \sum_{k=1}^{t-1} f_k(x)}"/> as <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> varies.</p>
<p>
</p><p><b>2. Negative-Entropy Regularization </b></p>
<p/><p>
Let us consider again the “experts” setting, that is, the online optimization setup in which <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is the set of probability distributions over <img alt="{\{ 1,\ldots, n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2C+n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 1,\ldots, n\}}"/> and the cost functions are linear <img alt="{f_t (x) = \sum_i \ell_t (i) x(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+%5Cell_t+%28i%29+x%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t (x) = \sum_i \ell_t (i) x(i)}"/> with bounded coefficients.</p>
<p>
The example we showed above showed that FTL will tend to put all the probability mass on one expert. We would like to choose a regularizer that fights this tendency by penalizing “concentrated” distributions and favoring “spread-out” distributions. This observation might trigger the thought that the <em>entropy</em> of a distribution is a good measure of how concentrated or spread out it is, although the entropy is actually higher for spread-out distribution and smaller for concentrated ones. So we will use as a regularizer <em>minus the entropy</em>, multiplied by an appropriate scaling factor: </p>
<p align="center"><img alt="\displaystyle  R(x) := c \cdot \sum_{i=1}^n x_i \ln x_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3A%3D+c+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  R(x) := c \cdot \sum_{i=1}^n x_i \ln x_i "/></p>
<p> (Entropy is usually defined using logarithms in base 2, but using natural logarithms will make it cleaner to take derivatives, and it only affects the constant factor <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/>.) With this choice of regularizer, we have</p>
<p/><p align="center"><img alt="\displaystyle  x_t = \arg\min_{x\in \Delta} \ \ \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \sum_{i=1}^n x_i \ln x_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+%5CDelta%7D+%5C+%5C+%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+x_k+%5Crangle+%5Cright+%29+%2B+c+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t = \arg\min_{x\in \Delta} \ \ \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \sum_{i=1}^n x_i \ln x_i "/></p>
<p> To compute the minimum of the above function we will use the method of Lagrange multipliers. Specialized to our setting, the method of Lagrange multiplier states that if we want to solve the constrained minimization problem </p>
<p align="center"><img alt="\displaystyle  \min_{x : \ a^Tx = b } \ \ f(x ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx+%3A+%5C+a%5ETx+%3D+b+%7D+%5C+%5C+f%28x+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min_{x : \ a^Tx = b } \ \ f(x ) "/></p>
<p> we introduce a new parameter <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> and define the function </p>
<p align="center"><img alt="\displaystyle  f_\lambda (x) := f(x) + \lambda \cdot (a^T x - b ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_%5Clambda+%28x%29+%3A%3D+f%28x%29+%2B+%5Clambda+%5Ccdot+%28a%5ET+x+-+b+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_\lambda (x) := f(x) + \lambda \cdot (a^T x - b ) "/></p>
<p> Then it is possible to prove that if <img alt="{x^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^*}"/> is a feasible minimizer of <img alt="{f(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(\cdot)}"/>, then there is at least a value of <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> such that <img alt="{\nabla f_\lambda (x^*) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f_%5Clambda+%28x%5E%2A%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\nabla f_\lambda (x^*) = 0}"/>, that is, such that <img alt="{x^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^*}"/> is a stable point of <img alt="{f_\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_\lambda}"/>. So one can proceed by finding all <img alt="{x,\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2C%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,\lambda}"/> such that <img alt="{\nabla f_\lambda (x) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f_%5Clambda+%28x%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\nabla f_\lambda (x) = 0}"/> and then filtering out the values of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> such that <img alt="{a^T x \neq b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%5ET+x+%5Cneq+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a^T x \neq b}"/>, and finally looking at which of the remaining <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> minimizes <img alt="{f(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(\cdot)}"/>.</p>
<p>
Ignoring for a moment the non-negativity constraints, the constraint <img alt="{x\in \Delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in \Delta}"/> reduces to <img alt="{\sum_i x_i = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+x_i+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_i x_i = 1}"/>, so we have to consider the function </p>
<p align="center"><img alt="\displaystyle  \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \left( \sum_{i=1}^n x_i \ln x_i \right) + \lambda \cdot \left( \langle x, {\bf 1} \rangle - 1\right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+x_k+%5Crangle+%5Cright+%29+%2B+c+%5Ccdot+%5Cleft%28+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+%5Cright%29+%2B+%5Clambda+%5Ccdot+%5Cleft%28+%5Clangle+x%2C+%7B%5Cbf+1%7D+%5Crangle+-+1%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \left( \sum_{i=1}^n x_i \ln x_i \right) + \lambda \cdot \left( \langle x, {\bf 1} \rangle - 1\right) "/></p>
<p> The partial derivative of the above expression with respect to <img alt="{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_i}"/> is </p>
<p align="center"><img alt="\displaystyle  \left( \sum_{k=1}^{t-1} \ell_k (i) \right ) + c \cdot \left( 1 + \ln x_i \right) + \lambda " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29+%5Cright+%29+%2B+c+%5Ccdot+%5Cleft%28+1+%2B+%5Cln+x_i+%5Cright%29+%2B+%5Clambda+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left( \sum_{k=1}^{t-1} \ell_k (i) \right ) + c \cdot \left( 1 + \ln x_i \right) + \lambda "/></p>
<p> If we want the gradient to be zero then we want all the above expressions to be zero, which translates to </p>
<p align="center"><img alt="\displaystyle  x_i = {\rm exp} \left( -1 - \frac \lambda c - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_i+%3D+%7B%5Crm+exp%7D+%5Cleft%28+-1+-+%5Cfrac+%5Clambda+c+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_i = {\rm exp} \left( -1 - \frac \lambda c - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) "/></p>
<p> There is only one value of <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> that makes the above solution a probability distribution, and the corresponding solution is </p>
<p align="center"><img alt="\displaystyle  x_i = \frac {{\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) } {\sum_j {\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(j) \right) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_i+%3D+%5Cfrac+%7B%7B%5Crm+exp%7D+%5Cleft%28+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+%5Cright%29+%7D+%7B%5Csum_j+%7B%5Crm+exp%7D+%5Cleft%28+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28j%29+%5Cright%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_i = \frac {{\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) } {\sum_j {\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(j) \right) } "/></p>
<p> Notice that this is exactly the solution computed by the multiplicative weights algorithm, if we choose <img alt="{c = 1/\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c = 1/\epsilon}"/>. So we have “rediscovered” the multiplicative weights algorithm and we have also “explained” what it does: at every step it balances the goals of finding a solution that is good for the past and that has large entropy.</p>
<p>
Now it remains to bound, at each time step, </p>
<p/><p align="center"><img alt="\displaystyle  f_t (x_t) - f_t (x_{t+1}) = \langle \ell_t , x_t - x_{t+1} \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x_t%29+-+f_t+%28x_%7Bt%2B1%7D%29+%3D+%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t (x_t) - f_t (x_{t+1}) = \langle \ell_t , x_t - x_{t+1} \rangle "/></p>
<p> For this, it is convenient to return to the notation that we used in describing the multiplicative weights algorithm, that is, it is convenient to work with the weights defined as </p>
<p align="center"><img alt="\displaystyle  w_1(i) = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w_1%28i%29+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  w_1(i) = 1"/></p>
<p align="center"><img alt="\displaystyle  w_{t+1} (i) = w_t (i) \cdot e^{ \ell_t (i) / c}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w_%7Bt%2B1%7D+%28i%29+%3D+w_t+%28i%29+%5Ccdot+e%5E%7B+%5Cell_t+%28i%29+%2F+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  w_{t+1} (i) = w_t (i) \cdot e^{ \ell_t (i) / c}"/></p>
<p> so that, at each time step </p>
<p align="center"><img alt="\displaystyle  x_t(i) = \frac {w_t(i)}{\sum_j w_t (j) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7Bw_t%28i%29%7D%7B%5Csum_j+w_t+%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t(i) = \frac {w_t(i)}{\sum_j w_t (j) } "/></p>
<p> We are assuming <img alt="{0 \leq \ell_t (i) \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%5Cleq+%5Cell_t+%28i%29+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 \leq \ell_t (i) \leq 1}"/>, so the weights are non-increasing with time. Then </p>
<p align="center"><img alt="\displaystyle  x_{t+1} (i) = \frac {w_{t+1} (i) }{\sum_j w_{t+1} (j) } = \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t+1} (j) } \geq \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t} (j) } \geq x_t(i) \cdot e^{-1/c} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%28i%29+%3D+%5Cfrac+%7Bw_%7Bt%2B1%7D+%28i%29+%7D%7B%5Csum_j+w_%7Bt%2B1%7D+%28j%29+%7D+%3D+%5Cfrac+%7Bw_%7Bt%7D+%28i%29+e%5E%7B-%5Cell_t+%28i%29+%2Fc+%7D%7D%7B%5Csum_j+w_%7Bt%2B1%7D+%28j%29+%7D+%5Cgeq+%5Cfrac+%7Bw_%7Bt%7D+%28i%29+e%5E%7B-%5Cell_t+%28i%29+%2Fc+%7D%7D%7B%5Csum_j+w_%7Bt%7D+%28j%29+%7D+%5Cgeq+x_t%28i%29+%5Ccdot+e%5E%7B-1%2Fc%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} (i) = \frac {w_{t+1} (i) }{\sum_j w_{t+1} (j) } = \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t+1} (j) } \geq \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t} (j) } \geq x_t(i) \cdot e^{-1/c} "/></p>
<p> For every <img alt="{c \geq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c \geq 1}"/> we have <img alt="{e^{-1/c} \geq 1 - 1/c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-1%2Fc%7D+%5Cgeq+1+-+1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{-1/c} \geq 1 - 1/c}"/>, so </p>
<p align="center"><img alt="\displaystyle  x_{t}(i) - x_{t+1}(i) \leq \frac 1c x_t (i) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%7D%28i%29+-+x_%7Bt%2B1%7D%28i%29+%5Cleq+%5Cfrac+1c+x_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t}(i) - x_{t+1}(i) \leq \frac 1c x_t (i) "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \langle \ell_t , x_t - x_{t+1} \rangle \leq \sum_i \ell_t(i) \cdot \frac 1c x_t(i) \leq \frac 1c " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%5Cleq+%5Csum_i+%5Cell_t%28i%29+%5Ccdot+%5Cfrac+1c+x_t%28i%29+%5Cleq+%5Cfrac+1c+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \langle \ell_t , x_t - x_{t+1} \rangle \leq \sum_i \ell_t(i) \cdot \frac 1c x_t(i) \leq \frac 1c "/></p>
<p> Putting it all together, we have </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \frac Tc + c \ln n " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cfrac+Tc+%2B+c+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \frac Tc + c \ln n "/></p>
<p> Choosing <img alt="{c = \sqrt{\frac T {\ln n}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Csqrt%7B%5Cfrac+T+%7B%5Cln+n%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c = \sqrt{\frac T {\ln n}}}"/>, we have </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} "/></p>
<p> Thus, we have reconstructed the analysis of the multiplicative weights algorithm.</p>
<p>
Interestingly, the analysis that we derived today is not exactly identical to the one from the post on multiplicative weights. There, we derived the bound</p>
<p/><p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t^2 (i) x_t (i) \ + \frac {\ln n}{\epsilon } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+%5Cell_t%5E2+%28i%29+x_t+%28i%29+%5C+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t^2 (i) x_t (i) \ + \frac {\ln n}{\epsilon } "/></p>
<p> while here, setting <img alt="{\epsilon = 1/c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon = 1/c}"/>, we derived </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t (i) x_t(i) + \frac {\ln n}{\epsilon } - \frac 1 \epsilon H(x^*) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+%5Cell_t+%28i%29+x_t%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon+%7D+-+%5Cfrac+1+%5Cepsilon+H%28x%5E%2A%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t (i) x_t(i) + \frac {\ln n}{\epsilon } - \frac 1 \epsilon H(x^*) "/></p>
<p> where <img alt="{x^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^*}"/> is the offline optimum and <img alt="{H(x) = \sum_i x_i \ln \frac 1 {x_i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%28x%29+%3D+%5Csum_i+x_i+%5Cln+%5Cfrac+1+%7Bx_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H(x) = \sum_i x_i \ln \frac 1 {x_i}}"/> is the entropy function (computed using natural logarithms). </p>
<p>
</p><p><b>3. L2 Regularization </b></p>
<p/><p>
Now that we have a general method, let us apply it to a new context: suppose that, as before, our cost functions are linear, but let <img alt="{K = {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK+%3D+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K = {\mathbb R}^n}"/>. With linear cost functions and no bound on the size of solutions, it will not be possible to talk about regret with respect to the offline optimum, because the offline optimum will always be <img alt="{-\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-\infty}"/>, but it will be possible to talk about regret with respect to a particular offline solution <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, which will already lead to interesting consequences.</p>
<p>
What regularizer should we use? In reasoning about regularizers, it can be helpful to think about what would go wrong if we use FTL, and then considering what regularizer would successfully “pull away” from the bad solutions found by FTL. In this context of linear loss functions and unbounded solutions, FTL will pick an infinitely big solution at each step, or, to be more precise, the “max” in the definition of FTL is undefined. To fight this tendency of FTL to go off to infinity, it makes sense for the regularizer to be a measure of how big a solution is. Since we are going to have to compute derivatives, it is good to use a measure of “bigness” with a nice gradient, and <img alt="{||x ||^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7Cx+%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{||x ||^2}"/> is a natural choice. So, for a scale parameter <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> to be optimized later, our regularizer will be </p>
<p align="center"><img alt="\displaystyle  R(x) := c || x||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3A%3D+c+%7C%7C+x%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  R(x) := c || x||^2 "/></p>
<p> This tells us that </p>
<p align="center"><img alt="\displaystyle  x_ 1 = {\bf 0} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_+1+%3D+%7B%5Cbf+0%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_ 1 = {\bf 0} "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  x_{t+1} = \arg\min_{x \in {\mathbb R}^n} \ \ c ||x||^2 + \sum_{k=1}^t \langle \ell_k , x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx+%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+c+%7C%7Cx%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5Et+%5Clangle+%5Cell_k+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} = \arg\min_{x \in {\mathbb R}^n} \ \ c ||x||^2 + \sum_{k=1}^t \langle \ell_k , x \rangle "/></p>
<p> The function that we are minimizing in the above expression is convex, so we just have to compute the gradient and set it to zero </p>
<p align="center"><img alt="\displaystyle  2c x + \sum_{k=1}^t \ell_k = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2c+x+%2B+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_k+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  2c x + \sum_{k=1}^t \ell_k = 0 "/></p>
<p align="center"><img alt="\displaystyle  x = - \frac 1 {2c} \sum_{k=1}^t \ell_k " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+-+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x = - \frac 1 {2c} \sum_{k=1}^t \ell_k "/></p>
<p> Which can be also expressed as </p>
<p align="center"><img alt="\displaystyle  x_1 = {\bf 0}; \ \ \ x_{t+1} = x_t - \frac 1 {2c} \ell_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%7B%5Cbf+0%7D%3B+%5C+%5C+%5C+x_%7Bt%2B1%7D+%3D+x_t+-+%5Cfrac+1+%7B2c%7D+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_1 = {\bf 0}; \ \ \ x_{t+1} = x_t - \frac 1 {2c} \ell_t "/></p>
<p> This makes perfect sense because, in the “experts” interpretation, we want to penalize the experts that performed badly in the past. Here we have no constraints on our allocations, so we simply decrease (additively this time, not multiplicatively) the allocation to the experts that caused a higher loss.</p>
<p>
To compute the regret bound, we have </p>
<p align="center"><img alt="\displaystyle  f_t(x_t) - f_{t} (x_{t+1}) = \langle \ell_t, x_t - x_{t+1} \rangle = \left\langle \ell_t , \frac 1 {2c} \ell_t \right\rangle = \frac 1 {2c} || \ell_t||^2 || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D%29+%3D+%5Clangle+%5Cell_t%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%3D+%5Cleft%5Clangle+%5Cell_t+%2C+%5Cfrac+1+%7B2c%7D+%5Cell_t+%5Cright%5Crangle+%3D+%5Cfrac+1+%7B2c%7D+%7C%7C+%5Cell_t%7C%7C%5E2+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t(x_t) - f_{t} (x_{t+1}) = \langle \ell_t, x_t - x_{t+1} \rangle = \left\langle \ell_t , \frac 1 {2c} \ell_t \right\rangle = \frac 1 {2c} || \ell_t||^2 || "/></p>
<p> and so the regret with respect to a solution <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T(x) \leq R(x) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1} ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+R%28x%29+-+R%28x_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T(x) \leq R(x) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1} ) "/></p>
<p align="center"><img alt="\displaystyle  = c || x||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+c+%7C%7C+x%7C%7C%5E2+%2B+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bt%3D1%7D%5ET+%7C%7C+%5Cell_t%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = c || x||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t||^2 "/></p>
<p> If we know a bound </p>
<p align="center"><img alt="\displaystyle  \forall t: \ \ || \ell_t || \leq L " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+t%3A+%5C+%5C+%7C%7C+%5Cell_t+%7C%7C+%5Cleq+L+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall t: \ \ || \ell_t || \leq L "/></p>
<p align="center"><img alt="\displaystyle  || x|| \leq D " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x%7C%7C+%5Cleq+D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || x|| \leq D "/></p>
<p> then we can optimize <img alt="{c = \sqrt{\frac T {2D^2 L^2}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Csqrt%7B%5Cfrac+T+%7B2D%5E2+L%5E2%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c = \sqrt{\frac T {2D^2 L^2}}}"/> and we have </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T(x) \leq D L \sqrt{ 2 T} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D+L+%5Csqrt%7B+2+T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T(x) \leq D L \sqrt{ 2 T} "/></p>
<p>
</p><p><b>  3.1. Dealing with Constraints </b></p>
<p/><p>
Consider now the case in which the loss functions are linear and <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is an arbitrary convex set. Using the same regularizer <img alt="{R(x) = c || x||^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%3D+c+%7C%7C+x%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(x) = c || x||^2}"/> we have the algorithm </p>
<p align="center"><img alt="\displaystyle  x_1 = \arg\min_{x\in K} c ||x ||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+c+%7C%7Cx+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_1 = \arg\min_{x\in K} c ||x ||^2 "/></p>
<p align="center"><img alt="\displaystyle  x_{t+1} = \arg\min_{x\in K} \ \ c ||x ||^2 + \sum_{k=1}^{t} \langle \ell_t , x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+c+%7C%7Cx+%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt%7D+%5Clangle+%5Cell_t+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} = \arg\min_{x\in K} \ \ c ||x ||^2 + \sum_{k=1}^{t} \langle \ell_t , x \rangle "/></p>
<p> How can we solve the above constrained optimization problem? A very helpful observation is that we can first solve the unconstrained optimization and then project on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>, that is we can proceed as follows: </p>
<p align="center"><img alt="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} \ \ c ||y ||^2 + \sum_{k=1}^{t} \langle \ell_t , y \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+c+%7C%7Cy+%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt%7D+%5Clangle+%5Cell_t+%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} \ \ c ||y ||^2 + \sum_{k=1}^{t} \langle \ell_t , y \rangle "/></p>
<p align="center"><img alt="\displaystyle  x'_{t+1} = \Pi_K (y_{t+1} ) = \arg\min_{x\in K} || x - y_{t+1} || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%27_%7Bt%2B1%7D+%3D+%5CPi_K+%28y_%7Bt%2B1%7D+%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x'_{t+1} = \Pi_K (y_{t+1} ) = \arg\min_{x\in K} || x - y_{t+1} || "/></p>
<p> and we claim that we always have <img alt="{x'_{t+1} = x_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27_%7Bt%2B1%7D+%3D+x_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x'_{t+1} = x_{t+1}}"/>. The fact that we can reduce a regularized constrained optimization problem to an unconstrained problem and a projection is part of a broader theory that we will describe in a later post. For now, we will limit to prove the equivalence in this specific setting. First of all, we already have an expression for <img alt="{y_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_{t+1}}"/>, namely </p>
<p align="center"><img alt="\displaystyle  y_{t+1} = - \frac 1{2c} \sum_{k=1}^t \ell_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+-+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{t+1} = - \frac 1{2c} \sum_{k=1}^t \ell_t "/></p>
<p> Now the definition of <img alt="{x'_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x'_{t+1}}"/> is </p>
<p align="center"><img alt="\displaystyle  x'_{t+1} = \arg\min_{x\in K} || x - y_{t+1} || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%27_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x'_{t+1} = \arg\min_{x\in K} || x - y_{t+1} || "/></p>
<p align="center"><img alt="\displaystyle  = \arg\min_{x\in K} || x - y_{t+1} ||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \arg\min_{x\in K} || x - y_{t+1} ||^2 "/></p>
<p align="center"><img alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle + || y_{t+1} ||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft+%5Clangle+x+%2C+y_%7Bt%2B1%7D+%5Cright%5Crangle+%2B+%7C%7C+y_%7Bt%2B1%7D+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle + || y_{t+1} ||^2 "/></p>
<p align="center"><img alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft+%5Clangle+x+%2C+y_%7Bt%2B1%7D+%5Cright%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle "/></p>
<p align="center"><img alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left\langle x , \frac 1{2c} \sum_{k=1}^t \ell_t \right\rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft%5Clangle+x+%2C+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_t+%5Cright%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left\langle x , \frac 1{2c} \sum_{k=1}^t \ell_t \right\rangle "/></p>
<p align="center"><img alt="\displaystyle  = \arg\min_{x\in K} c ||x||^2 - \sum_{k=1}^t \left\langle x , \ell_t \right\rangle = x_{t+1} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+c+%7C%7Cx%7C%7C%5E2+-+%5Csum_%7Bk%3D1%7D%5Et+%5Cleft%5Clangle+x+%2C+%5Cell_t+%5Cright%5Crangle+%3D+x_%7Bt%2B1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \arg\min_{x\in K} c ||x||^2 - \sum_{k=1}^t \left\langle x , \ell_t \right\rangle = x_{t+1} "/></p>
<p>
In order to bound the regret, we have to compute </p>
<p align="center"><img alt="\displaystyle  f_t(x_t) - f_t(x_{t+1} ) = \langle \ell_t , x_t - x_{t+1} \rangle \leq || \ell_t || \cdot ||x_t - x_{t+1} || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x_t%29+-+f_t%28x_%7Bt%2B1%7D+%29+%3D+%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%5Cleq+%7C%7C+%5Cell_t+%7C%7C+%5Ccdot+%7C%7Cx_t+-+x_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t(x_t) - f_t(x_{t+1} ) = \langle \ell_t , x_t - x_{t+1} \rangle \leq || \ell_t || \cdot ||x_t - x_{t+1} || "/></p>
<p> and since L2 projections cannot increase L2 distances, we have </p>
<p align="center"><img alt="\displaystyle  || x_t - x_{t+1} || \leq || y_t - y_{t+1} || = \frac 1 {2c} || \ell_t || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x_t+-+x_%7Bt%2B1%7D+%7C%7C+%5Cleq+%7C%7C+y_t+-+y_%7Bt%2B1%7D+%7C%7C+%3D+%5Cfrac+1+%7B2c%7D+%7C%7C+%5Cell_t+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || x_t - x_{t+1} || \leq || y_t - y_{t+1} || = \frac 1 {2c} || \ell_t || "/></p>
<p>
So the regret bound is </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq c ||x^*||^2 - c|| x_1||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t ||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+c+%7C%7Cx%5E%2A%7C%7C%5E2+-+c%7C%7C+x_1%7C%7C%5E2+%2B+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bt%3D1%7D%5ET+%7C%7C+%5Cell_t+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq c ||x^*||^2 - c|| x_1||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t ||^2 "/></p>
<p> If <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is an upper bound to <img alt="{\max_{x\in K} || x||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bx%5Cin+K%7D+%7C%7C+x%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\max_{x\in K} || x||}"/>, and <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> is an upper bound to the norm <img alt="{|| \ell_t ||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+%5Cell_t+%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|| \ell_t ||}"/> of all the loss vectors, then</p>
<p/><p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq c D^2 + \frac 1 {2c} T L^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+c+D%5E2+%2B+%5Cfrac+1+%7B2c%7D+T+L%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq c D^2 + \frac 1 {2c} T L^2 "/></p>
<p> which can be optimized to </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq DL \sqrt {2T} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+DL+%5Csqrt+%7B2T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq DL \sqrt {2T} "/></p>
<p>
</p><p><b>  3.2. Deriving the Analysis of Gradient Descent </b></p>
<p/><p>
Suppose that <img alt="{g: K \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g: K \rightarrow {\mathbb R}}"/> is a convex function whose gradient <img alt="{\nabla g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\nabla g}"/> is well defined at all points in <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>, and that we are interested in minimizing <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>. Then a way to reduce this problem to online optimization would be to use the function <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> as loss function at each step. Then the offline optimum would be the minimizer of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>, and achieving small regret means that <img alt="{\frac 1T \sum_t g(x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1T+%5Csum_t+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac 1T \sum_t g(x_t)}"/> is close to the minimum of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>, and so the best <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> is an approximate minimizer.</p>
<p>
Unfortunately, this is not a very helpful idea, because if we ran an FTRL algorithm against an adversary that keeps proposing <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> as a cost function at each step then we would have </p>
<p align="center"><img alt="\displaystyle  x_{t+1} = \arg\min_{x\in K} R(x) + t \cdot g(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+R%28x%29+%2B+t+%5Ccdot+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} = \arg\min_{x\in K} R(x) + t \cdot g(x) "/></p>
<p> which, for large <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, is essentially the same problem as minimizing <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>, so we have basically reduced the problem of minimizing <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> to itself.</p>
<p>
Indeed, the power of the FTRL algorithm is that the algorithm does well even though it does not know the cost function, and if we keep using the same cost function at each step we are not making a good use of its power. Now, suppose that we use cost functions <img alt="{f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t}"/> such that </p>
<ul>
<li> <img alt="{f_t(x_t) = g(x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%3D+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x_t) = g(x_t)}"/>
</li><li> <img alt="{\forall x\in K \ \ f_t(x) \leq g(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cforall+x%5Cin+K+%5C+%5C+f_t%28x%29+%5Cleq+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\forall x\in K \ \ f_t(x) \leq g(x)}"/>
</li></ul>
<p> Then, after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps, we have </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T g(x_t) = \sum_{t=1}^T f_t(x_t) = {\rm Regret}_T + \min{x\in K} \sum_{t=1}^T f_t(x) \leq {\rm Regret}_T + \min_{x\in K} \sum_{t=1}^T g (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%3D+%7B%5Crm+Regret%7D_T+%2B+%5Cmin%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cleq+%7B%5Crm+Regret%7D_T+%2B+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+g+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^T g(x_t) = \sum_{t=1}^T f_t(x_t) = {\rm Regret}_T + \min{x\in K} \sum_{t=1}^T f_t(x) \leq {\rm Regret}_T + \min_{x\in K} \sum_{t=1}^T g (x) "/></p>
<p> meaning </p>
<p align="center"><img alt="\displaystyle  \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%5Cleq+%5Cfrac+%7B%7B%5Crm+Regret%7D_T%7DT+%2B+%5Cmin_%7Bx%5Cin+K%7D+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) "/></p>
<p> and so one of the <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> is an approximate minimizer. Indeed, using convexity, we also have </p>
<p align="center"><img alt="\displaystyle  g \left( \frac 1T \sum_{t=1}^T x_t \right) \leq \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cleft%28+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+x_t+%5Cright%29+%5Cleq+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%5Cleq+%5Cfrac+%7B%7B%5Crm+Regret%7D_T%7DT+%2B+%5Cmin_%7Bx%5Cin+K%7D+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g \left( \frac 1T \sum_{t=1}^T x_t \right) \leq \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) "/></p>
<p> and so the average of the <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> is also an approximate minimizer. From the point of view of exploiting FTRL do to minimize <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>, cost functions <img alt="{f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t}"/> as above work just as well as presenting <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> as a cost functions at each step.</p>
<p>
How do we find cost functions that satisfy the above two properties and for which the FTRL algorithm is easy to implement? The idea is to let <img alt="{f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t}"/> be the linear approximation of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> at <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/>: </p>
<p align="center"><img alt="\displaystyle  f_t (x) := g(x_t) + \langle \nabla g (x_t), x - x_t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+g%28x_t%29+%2B+%5Clangle+%5Cnabla+g+%28x_t%29%2C+x+-+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t (x) := g(x_t) + \langle \nabla g (x_t), x - x_t \rangle "/></p>
<p> The <img alt="{f_t(x_t) = g(x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%3D+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x_t) = g(x_t)}"/> condition is immediate, and </p>
<p align="center"><img alt="\displaystyle  g(x) \geq f_t (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%5Cgeq+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(x) \geq f_t (x) "/></p>
<p> is a consequence of the convexity of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>.</p>
<p>
The cost functions that we have defined are affine functions, that is, each of them equals a constant plus a linear function </p>
<p align="center"><img alt="\displaystyle  f_t(x) = \left( g(x_t) - \langle \nabla g(x_t) , x_t\rangle \right) + \langle \nabla g(x_t) , x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x%29+%3D+%5Cleft%28+g%28x_t%29+-+%5Clangle+%5Cnabla+g%28x_t%29+%2C+x_t%5Crangle+%5Cright%29+%2B+%5Clangle+%5Cnabla+g%28x_t%29+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t(x) = \left( g(x_t) - \langle \nabla g(x_t) , x_t\rangle \right) + \langle \nabla g(x_t) , x \rangle "/></p>
<p>
Adding a constant term to a cost function does not change the iteration of FTRL, and does not change the regret (because the same term is added both to the solution found by the algorithm and to the offline optimum), so the algorithm is just initialized with</p>
<p/><p align="center"><img alt="\displaystyle  y_1 = {\bf 0} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_1+%3D+%7B%5Cbf+0%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_1 = {\bf 0} "/></p>
<p align="center"><img alt="\displaystyle  x_1 = \Pi_K({\bf 0}) = \arg\min_{x\in K} || x|| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5CPi_K%28%7B%5Cbf+0%7D%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_1 = \Pi_K({\bf 0}) = \arg\min_{x\in K} || x|| "/></p>
<p> and then continues with the update rules </p>
<p align="center"><img alt="\displaystyle  y_{t+1} =y_t -\frac 1 {2c} \nabla g (x_t) \mbox{ for } t \geq 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3Dy_t+-%5Cfrac+1+%7B2c%7D+%5Cnabla+g+%28x_t%29+%5Cmbox%7B+for+%7D+t+%5Cgeq+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{t+1} =y_t -\frac 1 {2c} \nabla g (x_t) \mbox{ for } t \geq 1"/></p>
<p align="center"><img alt="\displaystyle  x_{t+1} = \Pi_K(y_{t+1}) = \arg\min_{x\in K} || x - y_{t+1} || \mbox{ for } t \geq 1 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5CPi_K%28y_%7Bt%2B1%7D%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+%5Cmbox%7B+for+%7D+t+%5Cgeq+1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} = \Pi_K(y_{t+1}) = \arg\min_{x\in K} || x - y_{t+1} || \mbox{ for } t \geq 1 "/></p>
<p> which is just projected gradient descent.</p>
<p>
If we have known upper bounds </p>
<p align="center"><img alt="\displaystyle  \forall x \in K \ \ || \nabla g(x) || \leq L " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cin+K+%5C+%5C+%7C%7C+%5Cnabla+g%28x%29+%7C%7C+%5Cleq+L+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall x \in K \ \ || \nabla g(x) || \leq L "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \forall x \in K \ \ || x || \leq D " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cin+K+%5C+%5C+%7C%7C+x+%7C%7C+%5Cleq+D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall x \in K \ \ || x || \leq D "/></p>
<p> then we have </p>
<p align="center"><img alt="\displaystyle  g \left( \frac 1 T \sum_{t=1}^T x_t \right ) \leq DL \cdot \sqrt{\frac 2 T} + \min_{x\in K} \sum_{t=1}^T g (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cleft%28+%5Cfrac+1+T+%5Csum_%7Bt%3D1%7D%5ET+x_t+%5Cright+%29+%5Cleq+DL+%5Ccdot+%5Csqrt%7B%5Cfrac+2+T%7D+%2B+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+g+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g \left( \frac 1 T \sum_{t=1}^T x_t \right ) \leq DL \cdot \sqrt{\frac 2 T} + \min_{x\in K} \sum_{t=1}^T g (x) "/></p>
<p> which means that to achieve additive error <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> it is enough to proceed for <img alt="{2D^2L^2 / \epsilon^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2D%5E2L%5E2+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2D^2L^2 / \epsilon^2}"/> steps. </p></div>
    </content>
    <updated>2019-05-06T14:05:54Z</updated>
    <published>2019-05-06T14:05:54Z</published>
    <category term="theory"/>
    <category term="follow the leader"/>
    <category term="follow the regularized leader"/>
    <category term="gradient descent"/>
    <category term="online optimization"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-09T22:20:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=628</id>
    <link href="https://emanueleviola.wordpress.com/2019/05/05/e-ink-on-the-move/" rel="alternate" type="text/html"/>
    <title>E-ink on the move</title>
    <summary>Today I was overjoyed to notice that the MBTA is installing e-ink signs. I didn’t know about this when I wrote in the previous post that the market for e-ink monitors will be huge. I was actually about to report more on my experience, and by another standard coincidence today a reader asks: Some time […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: justify;">Today I was overjoyed to notice that the <a href="https://www.mbta.com/projects/solar-powered-e-ink-signs">MBTA is installing e-ink signs</a>. I didn’t know about this when <a href="https://emanueleviola.wordpress.com/2019/03/07/a-dream-come-true-sort-of-e-ink-monitors/">I wrote in the previous post</a> that the market for e-ink monitors will be huge.</p>
<p style="text-align: justify;">I was actually about to report more on my experience, and by another standard coincidence today a reader asks:</p>
<blockquote><p>Some time have passed, is your evaluation the same? Did you come across any unexpected difficulties?</p></blockquote>
<p style="text-align: justify;">Well, I wrote a <a href="http://www.ccs.neu.edu/home/viola/papers/tm.pdf">paper</a> entirely in e-ink. But I regret to admit that towards the end of the semester I got really busy with the usual end-of-Spring matters at the university, and I switched back to my back-lit 30-inch Dell monitor.  I had to interact with a number of computer systems where I could not easily change font size (the story of my life), and where color tended to matter, and I felt that the new monitor was slowing me down.  I haven’t switched back to the e-ink monitor yet, partly because I am still recovering from the burst.</p>
<p style="text-align: justify;">However I look forward to using the e-ink monitor more during this summer, especially outdoors.  Here the fact that it’s usb powered will be essential.  In the MBTA project they use solar power which I think is really cool and makes me think of bringing my monitor to the secluded off-the-grid cabin in Maine I don’t have.</p></div>
    </content>
    <updated>2019-05-06T00:40:40Z</updated>
    <published>2019-05-06T00:40:40Z</published>
    <category term="Uncategorized"/>
    <category term="health"/>
    <category term="tech"/>
    <author>
      <name>Emanuele</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>By Emanuele Viola</subtitle>
      <title>Thoughts</title>
      <updated>2019-05-09T22:21:15Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://gradientscience.org/adv/</id>
    <link href="http://gradientscience.org/adv/" rel="alternate" type="text/html"/>
    <title>Adversarial Examples Are Not Bugs, They Are Features</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/1905.02175" style="float: left;">
<i class="fas fa-file-pdf"/>
    Read the paper
</a>
<a class="bbutton" href="http://git.io/adv-datasets" style="float: right;">
<i class="fab fa-github"/>
   Download the datasets
</a></p>

<p>Over the past few years, adversarial examples – or inputs that have been slightly perturbed by an adversary to cause unintended behavior in machine learning systems – have received significant attention in the machine learning community (for more background, read our introduction to adversarial examples <a href="https://gradientscience.org/intro_adversarial">here</a>). There has been much work on training models that are not vulnerable to adversarial examples (in previous posts, we discussed methods for training robust models: <a href="https://gradientscience.org/robust_opt_pt1/">part 1</a>, <a href="https://gradientscience.org/robust_opt_pt2/">part 2</a>, but all this research does not really confront the fundamental question: <i>why</i> do these adversarial examples arise in the first place?</p>

<p>So far, the prevailing view has been that adversarial examples stem from “quirks” of the models that will eventually disappear once we make enough progress towards better training algorithms and larger scale data collection. Common views include adversarial examples being either a consequence of the input space being high-dimensional (e.g. <a href="https://arxiv.org/abs/1801.02774">here</a>) or attributed to finite-sample phenomena (e.g. <a href="https://arxiv.org/abs/1608.07690">here</a> or <a href="https://arxiv.org/abs/1804.11285">here</a>).</p>

<p>Today we will discuss our <a href="https://gradientscience.org/adv.pdf">recent work</a> that provides a new perspective on the reasons for adversarial examples arise. However, before we dive into the details, let us first tell you a short story:</p>

<h2 id="a-planet-called-erm">A Planet called <i>Erm</i></h2>

<p>Our tale begins on <i><span class="sc">Erm</span></i>, a distant planet inhabited by members of an ancient alien race known as <i>Nets</i>. The Nets are a strange species; each individual’s place in the social hierarchy is determined by their ability to classify bizarre 32-by-32 pixel images (which are meaningless to the Nets) into ten completely arbitrary categories. These images are drawn from a top-secret dataset, <span class="sc">See-Far</span>—outside of looking at those curious pixelated images, the Nets live their lives totally blind.</p>

<p>As Nets grow older and wiser they begin to discover more and more tell-tale <i>patterns</i> in <span class="sc">See-Far</span>. Each new pattern that an alien discovers helps them classify the dataset even more accurately. Due to the immense societal value of increased classification accuracy, the aliens have names for the most predictive image patterns:</p>

<p><img alt="A Toogit, highly indicative of a '1' image." src="https://gradientscience.org/images/featsnotbugs/toogit.png" style="width: 30%;"/></p>
<div class="footnote">
        A TOOGIT, highly indicative of a "1" image.
        Nets are extremely sensitive to TOOGITs.
</div>

<p>The most powerful aliens were remarkably adept at spotting patterns, and thus were extremely sensitive to their presence in <span class="sc">See-Far</span> images.</p>

<p>Somehow (perhaps looking for <span class="sc">See-Far</span> classification tips), some of the aliens obtain access to a <i>human-written</i> machine learning paper. One figure in
particular caught the aliens’ eye:</p>

<p>
<img alt="An quote-unquote adversarial example?" src="https://gradientscience.org/images/featsnotbugs/bagaboop.png"/>
</p><div class="footnote">
An "adversarial example"?
</div>
<p/>

<p>The figure was relatively simple, they thought: on the left was a “2”, in the middle there was a GAB pattern, which was known to indicate “4”—unsurprisingly, adding a GAB to the image on the left resulted in a new image, <i>which looked (to the Nets) exactly like an image corresponding to the “4” category</i>.</p>

<p>The Nets could not understand why, according to the paper, the original and final images, being completely different, should be identically classified. Confused, the Nets flipped on through the manuscript, wondering what other useful patterns humans were oblivious to$\ldots$</p>

<h2 id="what-we-can-learn-from-erm">What we can learn from <i>Erm</i></h2>

<p>As the names may suggest, this story is not merely one of aliens and their curious social constructs: the way Nets develop is reminiscent of how we train machine learning models. In particular, we maximize accuracy without incorporating much prior context about classified classes, the physical world, or other human-related concepts. The result in the story is that the aliens are able to realize that what humans think of as meaningless adversarial perturbation are actually patterns crucial to <span class="sc">See-Far</span> classification. The tale of the Nets should thus make us wonder:</p>

<p><i>Are adversarial perturbations really unnatural and meaningless?</i></p>

<h3 id="a-simple-experiment">A simple experiment</h3>
<p>To investigate this issue, let us first perform a simple experiment:</p>

<ul>
  <li>We start with an image from the training set of a standard dataset (e.g. CIFAR10):</li>
</ul>

<p><img alt="An image from the training set" src="https://gradientscience.org/images/featsnotbugs/train.png" style="width: 50%;"/></p>

<ul>
  <li>We synthesize a targeted adversarial example (on a standard pre-trained model) from each (x, y) towards the “next” class y+1 (or 0, if y is the last class):</li>
</ul>

<p><img alt="An adversarial perturabtion to the next class" src="https://gradientscience.org/images/featsnotbugs/adv.png"/></p>

<ul>
  <li>We then construct a new training set, by labeling these adversarial examples with their corresponding target class:</li>
</ul>

<p><img alt="A new training set based on the perturbation" src="https://gradientscience.org/images/featsnotbugs/newtrain.png" style="width: 35%;"/></p>

<p>Now, the resulting training set is imperceptibly perturbed from the original, but the labels have been changed—as such, it looks <i>completely</i> mislabeled to a human. In fact, the mislabelings are even consistent with a “permuted” hypothesis (i.e. every dog is labeled as a cat, every cat as a bird, etc.).</p>

<p>We train a new classifier (not necessarily with the same architecture as the first) on the “mislabeled dataset.” How will this classifier perform on the  <em>original (unaltered) test set</em> (i.e. the standard CIFAR-10 test set)?</p>

<p>Remarkably, we find that the resulting classifier actually has moderate accuracy (e.g. 44% for CIFAR)! This is despite the fact that training inputs are associated with their “true” labels <em>solely through imperceptible perturbations</em>, and are associated with a different (now incorrect) label matching through <i>all</i> visible features.</p>

<p>What’s going on here?</p>

<h2 id="our-conceptual-model-for-adversarial-examples">Our Conceptual Model for Adversarial Examples</h2>

<p>The experiment we just described establishes adversarial perturbations of standard models as patterns predictive of the target class in a <i>well-generalizing</i> sense. That is, adversarial perturbations in the training set alone allowed moderately accurate predictions on the test set. In this light, one might wonder: perhaps these patterns are <i>not</i> fundamentally different from what humans use to classify images (e.g. ears, whiskers, snouts)! This is precisely our hypothesis—there exist a variety of features of the input that are predictive of the label, and only some of these are perceptible to humans.</p>

<p>More precisely, we posit that predictive features of the data can be split into “robust” and “non-robust” features. Robust features correspond to patterns that are predictive of the true label <em>even when adversarially perturbed</em> (e.g. the presence of “fur” in an image) under some pre-defined (and crucially human-defined) perturbation set, e.g. the $\ell_2$ ball. Conversely, non-robust features correspond to patterns that while predictive, can be “flipped” by an adversary within a pre-defined perturbation set to be indicate a wrong class (see <a href="https://gradientscience.org/adv.pdf">our paper</a> for a formal definition).</p>

<p>Since we always only consider perturbation sets that do not affect human classification performance,  we expect humans to rely solely on robust features. However, when the goal is maximizing (standard) test-set accuracy, non-robust features can be just as useful as robust ones–in fact, the two types of features are completely interchangeable. This is illustrated in the following figure:</p>

<p><img alt="" src="https://gradientscience.org/images/featsnotbugs/model.png"/></p>

<p>From this perspective, our experiment describes something quite simple. In the original training set, both the robust and non-robust features of the input are predictive of the label. When we make a small adversarial perturbation, we cannot significantly affect the robust features (essentially by definition), but  we can still flip <i>non-robust features</i>. For instance, every dog image now retains the robust features of a dog (and thus appears to us to be a dog), but has non-robust features of a cat. After the training set is relabelled, we make it so that the robust features actually point in the <i>wrong direction</i> (i.e. the pictures with robust “dog” features are labeled as cats) and hence only the non-robust features actually provide correct guidance for generalization.</p>

<p>In summary, both robust and non-robust features are predictive on the training set, but <i>only non-robust features will yield generalization to the original test set</i>:</p>

<p><img alt="" src="https://gradientscience.org/images/featsnotbugs/diagram.png"/></p>

<p>Thus, the fact that models trained on this dataset actually generalize to the standard test set indicates that (a) non-robust features exist and are sufficient for good generalization, and (b) deep neural networks indeed rely on these non-robust features, even in the presence of predictive robust features.</p>

<h2 id="do-robust-models-learn-robust-features">Do Robust Models Learn Robust Features?</h2>

<p>Our experiments establish that adversarial perturbations are not meaningless artifacts but actually correspond directly to perturbing features that are crucial to generalization. At the same time, our blog posts about adversarial examples (<a href="https://gradientscience.org/robust1">here</a> and <a href="https://gradientscience.org/robust2">here</a>) showed that by using robust optimization, we can get models that are more robust to adversarial examples.</p>

<p>So a natural question to ask is: can we verify that robust models actually rely on <i>robust</i> features? To test this, we establish a methodology for restricting (in a best-effort sense) inputs to only the features a model is sensitive to (for deep neural networks, features correspond to the penultimate layer activations). Using this method, we create a new training set that is restricted to only contain the features that an already-trained robust model utilizes:</p>

<p><img alt="" src="https://gradientscience.org/images/featsnotbugs/frog.png"/></p>

<p>We then train a model on the resulting dataset  <em>without</em> adversarial training and find that the resulting model has non-trivial accuracy <em>and</em> robustness! This is in stark contrast to training on the standard training set which leads to models that are accurate yet completely brittle.</p>

<p><img alt="" src="https://gradientscience.org/images/featsnotbugs/CIFAR_res.png"/></p>
<div class="footnote">
Standard and robust accuracy, tested on CIFAR-10 test set ($\mathcal{D}$). Training: <br/>
<strong>Left</strong>: training normally on CIFAR-10 ($\mathcal{D}$) <br/>
<strong>Middle</strong>: training adversarially on CIFAR-10 ($\mathcal{D}$) <br/>
<strong>Right</strong>: training normally on constructed dataset ($\widehat{\mathcal{D}}_R$)
</div>

<p>Our results thus indicate that robustness (and in turn non-robustness) can in fact arise as a property of the dataset itself. In particular, when we remove non-robust features from the original training set, we can get robust models just via standard (non-adversarial) training. This is further evidence that adversarial examples arise as a result of non-robust features and are not necessarily tied to the standard training framework.</p>

<h2 id="transferability">Transferability</h2>

<p>An immediate consequence of this change in perspective is that the <i>transferability</i> of adversarial examples (the thus-far mysterious phenomenon that perturbations for one model are often adversarial for others) no longer requires a separate explanation. Specifically, now that we view adversarial vulnerability as a direct product of the <i>features derived from the dataset</i> (as opposed to quirks in the training of individual models), we would expect similarly expressive models to be able to find and use these features to improve their classification accuracy too.</p>

<p>To further explore this idea, we study how the tendency of different architectures to learning similar non-robust features relates to the transferability of adversarial examples between them:</p>

<p><img alt="" src="https://gradientscience.org/images/featsnotbugs/transfer.png"/>
In the above, we generate the dataset that we described in our very first experiment (a training set of adversarial examples labeled with the target class), using a ResNet-50 to construct the adversarial examples. We can think of the resulting dataset as having all of the ResNet-50’s non-robust features “flipped” to the target class. We then train the five architectures shown above on this dataset, and record their generalization performance on the true test set: this corresponds to how well the architecture is able to generalize using only the non-robust features from a ResNet-50.</p>

<p>When we analyze the results we see that, as our new view of adversarial examples suggests, models’ ability to pick up the non-robust features introduced by the ResNet-50 correlates extremely well with the adversarial transferability from ResNet-50 to standard models of each architecture.</p>

<h2 id="implications">Implications</h2>

<p>Our discussion and experiments establish adversarial examples as a purely human-centric phenomenon. From the perspective of classification performance there is no reason for a model to prefer robust over non-robust features. After all, the notion of robustness is human-specified. Hence, if we want our models to rely mostly on robust features we need to explicitly account for that by incorporating priors into architecture or training process. From that perspective, adversarial training (and more broadly robust optimization) can be thought of as a tool to incorporate desired invariances into the learned model. For example, robust training can be viewed as attempting to undermine the predictiveness of non-robust features by constantly “flipping” them, and thus steering the trained model away from relying on them.</p>

<p>At the same time, the reliance of standard models on non-robust (and hence incomprehensible to humans) features needs to be accounted for when designing interpretability methods. In particular, any “explanation” of a standardly trained model’s prediction should either highlight such non-robust features (and hence not be fully human-meaningful) or hide them (and hence not be fully faithful to the model’s decision process). Therefore, if we want interpretability methods that are both human-meaningful and faithful, resorting only to post-training processing is fundamentally insufficient: one needs to intervene <i>at</i> training time.</p>

<h2 id="more-in-the-paper">More in the Paper</h2>

<p>In <a href="https://gradientscience.org/adv.pdf">our paper</a>, we also describe a precise framework for discussing robust and non-robust features, further experiments corroborating our hypothesis, and a theoretical model under which we can study the dynamics of robust training in the presence of non-robust features.</p></div>
    </summary>
    <updated>2019-05-06T00:00:00Z</updated>
    <published>2019-05-06T00:00:00Z</published>
    <source>
      <id>http://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="http://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="http://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2019-05-08T23:57:18Z</updated>
    </source>
  </entry>
</feed>
