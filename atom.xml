<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-07-15T23:47:23Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/106</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/106" rel="alternate" type="text/html"/>
    <title>TR20-106 |  Explicit Extremal Designs and Applications to Extractors | 

	Eshan Chattopadhyay, 

	Jesse Goodman</title>
    <summary>An $(n,r,s)$-design, or $(n,r,s)$-partial Steiner system, is an $r$-uniform hypergraph over $n$ vertices with pairwise hyperedge intersections of size $0$, we extract from $(N,K,n,k)$-adversarial sources of locality $0$, where $K\geq N^\delta$ and $k\geq\text{polylog }n$. The previous best result (Chattopadhyay et al., STOC 2020) required $K\geq N^{1/2+o(1)}$. As a result, we get extractors for small-space sources over $n$ bits with entropy requirement $k\geq n^{1/2+\delta}$, whereas the previous best result (Chattopadhyay et al., STOC 2020) required $k\geq n^{2/3+\delta}$.</summary>
    <updated>2020-07-15T16:25:04Z</updated>
    <published>2020-07-15T16:25:04Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-15T23:44:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-07-15-asynchronous-fault-tolerant-computation-with-optimal-resilience/</id>
    <link href="https://decentralizedthoughts.github.io/2020-07-15-asynchronous-fault-tolerant-computation-with-optimal-resilience/" rel="alternate" type="text/html"/>
    <title>Asynchronous Fault Tolerant Computation with Optimal Resilience</title>
    <summary>A basic question of distributed computing: Is there a fundamental limit to fault tolerant computation in the Asynchronous model? The celebrated FLP theorem says that any protocol that solves Agreement in the asynchronous model that is resilient to at least one crash failure must have a non-terminating execution. This means...</summary>
    <updated>2020-07-15T08:39:00Z</updated>
    <published>2020-07-15T08:39:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-07-15T23:47:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.07161</id>
    <link href="http://arxiv.org/abs/2007.07161" rel="alternate" type="text/html"/>
    <title>Graph Sparsification by Universal Greedy Algorithms</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lai:Ming=Jun.html">Ming-Jun Lai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xie:Jiaxin.html">Jiaxin Xie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Zhiqiang.html">Zhiqiang Xu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07161">PDF</a><br/><b>Abstract: </b>Graph sparsification is to approximate an arbitrary graph by a sparse graph
and is useful in many applications, such as simplification of social networks,
least squares problems, numerical solution of symmetric positive definite
linear systems and etc. In this paper, inspired by the well-known sparse signal
recovery algorithm called orthogonal matching pursuit (OMP), we introduce a
deterministic, greedy edge selection algorithm called universal greedy
algorithm(UGA) for graph sparsification. The UGA algorithm can output a
$\frac{(1+\epsilon)^2}{(1-\epsilon)^2}$-spectral sparsifier with
$\lceil\frac{n}{\epsilon^2}\rceil$ edges in $O(m+n^2/\epsilon^2)$ time with $m$
edges and $n$ vertices for a general random graph satisfying a mild sufficient
condition. This is a linear time algorithm in terms of the number of edges that
the community of graph sparsification is looking for. The best result in the
literature to the knowledge of the authors is the existence of a deterministic
algorithm which is almost linear, i.e. $O(m^{1+o(1)})$ for some
$o(1)=O(\frac{(\log\log(m))^{2/3}}{\log^{1/3}(m)})$. We shall point out that
several random graphs satisfy the sufficient condition and hence, can be
sparsified in linear time. For a general spectral sparsification problem, e.g.,
positive subset selection problem, a nonnegative UGA algorithm is proposed
which needs $O(mn^2+ n^3/\epsilon^2)$ time and the convergence is established.
</p></div>
    </summary>
    <updated>2020-07-15T23:43:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.07049</id>
    <link href="http://arxiv.org/abs/2007.07049" rel="alternate" type="text/html"/>
    <title>Quantum exploration algorithms for multi-armed bandits</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Daochen.html">Daochen Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/You:Xuchen.html">Xuchen You</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Tongyang.html">Tongyang Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Childs:Andrew_M=.html">Andrew M. Childs</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07049">PDF</a><br/><b>Abstract: </b>Identifying the best arm of a multi-armed bandit is a central problem in
bandit optimization. We study a quantum computational version of this problem
with coherent oracle access to states encoding the reward probabilities of each
arm as quantum amplitudes. Specifically, we show that we can find the best arm
with fixed confidence using
$\tilde{O}\bigl(\sqrt{\sum_{i=2}^n\Delta^{\smash{-2}}_i}\bigr)$ quantum
queries, where $\Delta_{i}$ represents the difference between the mean reward
of the best arm and the $i^\text{th}$-best arm. This algorithm, based on
variable-time amplitude amplification and estimation, gives a quadratic speedup
compared to the best possible classical result. We also prove a matching
quantum lower bound (up to poly-logarithmic factors).
</p></div>
    </summary>
    <updated>2020-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.07040</id>
    <link href="http://arxiv.org/abs/2007.07040" rel="alternate" type="text/html"/>
    <title>Hybrid divide-and-conquer approach for tree search algorithms</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rennela:Mathys.html">Mathys Rennela</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Laarman:Alfons.html">Alfons Laarman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dunjko:Vedran.html">Vedran Dunjko</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07040">PDF</a><br/><b>Abstract: </b>As we are entering the era of real-world small quantum computers, finding
applications for these limited devices is a key challenge. In this vein, it was
recently shown that a hybrid classical-quantum method can help provide
polynomial speed-ups to classical divide-and-conquer algorithms, even when only
given access to a quantum computer much smaller than the problem itself. In
this work we study the hybrid divide-and-conquer method in the context of tree
search algorithms, and extend it by including quantum backtracking, which
allows better results than previous Grover-based methods. Further, we provide
general criteria for polynomial speed-ups in the tree search context, and
provide a number of examples where polynomial speed ups, using arbitrarily
smaller quantum computers, can still be obtained. This study possible speed-ups
for the well known algorithm of DPLL and prove threshold-free speed-ups for the
tree search subroutines of the so-called PPSZ algorithm - which is the core of
the fastest exact Boolean satisfiability solver - for certain classes of
formulas. We also provide a simple example where speed-ups can be obtained in
an algorithm-independent fashion, under certain well-studied
complexity-theoretical assumptions. Finally, we briefly discuss the fundamental
limitations of hybrid methods in providing speed-ups for larger problems.
</p></div>
    </summary>
    <updated>2020-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.07025</id>
    <link href="http://arxiv.org/abs/2007.07025" rel="alternate" type="text/html"/>
    <title>A Nearly Optimal Deterministic Online Algorithm for Non-Metric Facility Location</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bienkowski:Marcin.html">Marcin Bienkowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldkord:Bj=ouml=rn.html">Björn Feldkord</a>, Paweł Schmidt <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07025">PDF</a><br/><b>Abstract: </b>In the online non-metric variant of the facility location problem, there is a
given graph consisting of set $F$ of facilities (each with a certain opening
cost), set $C$ of potential clients, and weighted connections between them. The
online part of the input is a sequence of clients from $C$, and in response to
any requested client, an online algorithm may open an additional subset of
facilities and must connect the given client to an open facility.
</p>
<p>We give the first online, polynomial-time deterministic algorithm for this
problem, with competitive ratio of $O(\log |F| \cdot (\log |C| + \log \log
|F|))$. The result is optimal up to loglog factors. Previously, the only known
solution for this problem with a sub-linear competitive ratio was randomized
[Alon et al., TALG 2006]. Our approach is based on solving a different
fractional relaxation than that of Alon et al., where we combine dual fitting
and multiplicative weight updates approaches. By maintaining certain
monotonicity properties of the created fractional solution, we are able to
handle the dependencies between facilities and connections in a rounding
routine.
</p>
<p>Our result, combined with the algorithm by Naor et al. [FOCS 2011] implies
the first deterministic algorithm for the online node-weighted Steiner tree
problem. The resulting competitive ratio is $O(\log k \cdot \log^2 \ell)$ on
graphs of $\ell$ nodes and $k$ terminals.
</p></div>
    </summary>
    <updated>2020-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06979</id>
    <link href="http://arxiv.org/abs/2007.06979" rel="alternate" type="text/html"/>
    <title>The Collatz process embeds a base conversion algorithm</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/St=eacute=rin:Tristan.html">Tristan Stérin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woods:Damien.html">Damien Woods</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06979">PDF</a><br/><b>Abstract: </b>The Collatz process is defined on natural numbers by iterating the map $T(x)
= T_0(x) = x/2$ when $x\in\mathbb{N}$ is even and $T(x)=T_1(x) =(3x+1)/2$ when
$x$ is odd. In an effort to understand its dynamics, and since Generalised
Collatz Maps are known to simulate Turing Machines [Conway, 1972], it seems
natural to ask what kinds of algorithmic behaviours it embeds. We define a
quasi-cellular automaton that exactly simulates the Collatz process on the
square grid: on input $x\in\mathbb{N}$, written horizontally in base 2,
successive rows give the Collatz sequence of $x$ in base 2. We show that
vertical columns simultaneously iterate the map in base 3. This leads to our
main result: the Collatz process embeds an algorithm that converts any natural
number from base 3 to base 2. We also find that the evolution of our automaton
computes the parity of the number of 1s in any ternary input. It follows that
predicting about half of the bits of the iterates $T^i(x)$, for $i = O(\log
x)$, is in the complexity class NC$^1$ but outside AC$^0$. Finally, we show
that in the extension of the Collatz process to numbers with infinite binary
expansions ($2$-adic integers) [Lagarias, 1985], our automaton constructs
Collatz cycles and encodes the cyclic Collatz conjecture as a natural
reachability problem. These results show that the Collatz process is capable of
some simple, but non-trivial, computation in bases 2 and 3, suggesting an
algorithmic approach to thinking about existence, prediction and structure of
cycles in the Collatz process.
</p></div>
    </summary>
    <updated>2020-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06920</id>
    <link href="http://arxiv.org/abs/2007.06920" rel="alternate" type="text/html"/>
    <title>A Practical Algorithm with Performance Guarantees for the Art~Gallery Problem</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Simon Hengeveld, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miltzow:Tillmann.html">Tillmann Miltzow</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06920">PDF</a><br/><b>Abstract: </b>Given a closed simple polygon $P$, we say two points $p,q$ see each other if
the segment $pq$ is fully contained in $P$. The art gallery problem seeks a
minimum size set $G\subset P$ of guards that sees $P$ completely. The only
currently correct algorithm to solve the art gallery problem exactly uses
algebraic methods and is attributed to Sharir. As the art gallery problem is
ER-complete, it seems unlikely to avoid algebraic methods, without additional
assumptions. In this paper, we introduce the notion of vision stability. In
order to describe vision stability consider an enhanced guard that can see
``around the corner'' by an angle of $\delta$ or a diminished guard whose
vision is by an angle of $\delta$ ``blocked'' by reflex vertices. A polygon $P$
has vision stability $\delta$ if the optimal number of enhanced guards to guard
$P$ is the same as the optimal number of diminished guards to guard $P$. We
will argue that most relevant polygons are vision stable. We describe a
one-shot vision stable algorithm that computes an optimal guard set for
visionstable polygons using polynomial time and solving one integer program. It
guarantees to find the optimal solution for every vision stable polygon. We
implemented an iterative visionstable algorithm and show its practical
performance is slower, but comparable with other state of the art algorithms.
Our iterative algorithm is inspired and follows closely the one-shot algorithm.
It delays several steps and only computes them when deemed necessary. Given a
chord $c$ of a polygon, we denote by $n(c)$ the number of vertices visible from
$c$. The chord-width of a polygon is the maximum $n(c)$ over all possible
chords $c$. The set of vision stable polygons admits an FPT algorithm when
parametrized by the chord-width. Furthermore, the one-shot algorithm runs in
FPT time, when parameterized by the number of reflex vertices.
</p></div>
    </summary>
    <updated>2020-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06896</id>
    <link href="http://arxiv.org/abs/2007.06896" rel="alternate" type="text/html"/>
    <title>Component Order Connectivity in Directed Graphs</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>J. Bang-Jensen, E. Eiben, G. Gutin, M. Wahlstrom, A. Yeo <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06896">PDF</a><br/><b>Abstract: </b>A directed graph $D$ is semicomplete if for every pair $x,y$ of vertices of
$D,$ there is at least one arc between $x$ and $y.$ \viol{Thus, a tournament is
a semicomplete digraph.} In the Directed Component Order Connectivity (DCOC)
problem, given a digraph $D=(V,A)$ and a pair of natural numbers $k$ and
$\ell$, we are to decide whether there is a subset $X$ of $V$ of size $k$ such
that the largest strong connectivity component in $D-X$ has at most $\ell$
vertices. Note that DCOC reduces to the Directed Feedback Vertex Set problem
for $\ell=1.$ We study parametered complexity of DCOC for general and
semicomplete digraphs with the following parameters: $k, \ell,\ell+k$ and
$n-\ell$. In particular, we prove that DCOC with parameter $k$ on semicomplete
digraphs can be solved in time $O^*(2^{16k})$ but not in time $O^*(2^{o(k)})$
unless the Exponential Time Hypothesis (ETH) fails. \gutin{The upper bound
$O^*(2^{16k})$ implies the upper bound $O^*(2^{16(n-\ell)})$ for the parameter
$n-\ell.$ We complement the latter by showing that there is no algorithm of
time complexity $O^*(2^{o({n-\ell})})$ unless ETH fails.} Finally, we improve
\viol{(in dependency on $\ell$)} the upper bound of G{\"{o}}ke, Marx and Mnich
(2019) for the time complexity of DCOC with parameter $\ell+k$ on general
digraphs from $O^*(2^{O(k\ell\log (k\ell))})$ to $O^*(2^{O(k\log (k\ell))}).$
Note that Drange, Dregi and van 't Hof (2016) proved that even for the
undirected version of DCOC on split graphs there is no algorithm of running
time $O^*(2^{o(k\log \ell)})$ unless ETH fails and it is a long-standing
problem to decide whether Directed Feedback Vertex Set admits an algorithm of
time complexity $O^*(2^{o(k\log k)}).$
</p></div>
    </summary>
    <updated>2020-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06869</id>
    <link href="http://arxiv.org/abs/2007.06869" rel="alternate" type="text/html"/>
    <title>Robust Identifiability in Linear Structural Equation Models of Causal Inference</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sankararaman:Karthik_Abinav.html">Karthik Abinav Sankararaman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Louis:Anand.html">Anand Louis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Navin.html">Navin Goyal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06869">PDF</a><br/><b>Abstract: </b>In this work, we consider the problem of robust parameter estimation from
observational data in the context of linear structural equation models (LSEMs).
LSEMs are a popular and well-studied class of models for inferring causality in
the natural and social sciences. One of the main problems related to LSEMs is
to recover the model parameters from the observational data. Under various
conditions on LSEMs and the model parameters the prior work provides efficient
algorithms to recover the parameters. However, these results are often about
generic identifiability. In practice, generic identifiability is not sufficient
and we need robust identifiability: small changes in the observational data
should not affect the parameters by a huge amount. Robust identifiability has
received far less attention and remains poorly understood. Sankararaman et al.
(2019) recently provided a set of sufficient conditions on parameters under
which robust identifiability is feasible. However, a limitation of their work
is that their results only apply to a small sub-class of LSEMs, called
``bow-free paths.'' In this work, we significantly extend their work along
multiple dimensions. First, for a large and well-studied class of LSEMs, namely
``bow free'' models, we provide a sufficient condition on model parameters
under which robust identifiability holds, thereby removing the restriction of
paths required by prior work. We then show that this sufficient condition holds
with high probability which implies that for a large set of parameters robust
identifiability holds and that for such parameters, existing algorithms already
achieve robust identifiability. Finally, we validate our results on both
simulated and real-world datasets.
</p></div>
    </summary>
    <updated>2020-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06828</id>
    <link href="http://arxiv.org/abs/2007.06828" rel="alternate" type="text/html"/>
    <title>Network Flow Methods for the Minimum Covariates Imbalance Problem</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hochbaum:Dorit_S=.html">Dorit S. Hochbaum</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rao:Xu.html">Xu Rao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06828">PDF</a><br/><b>Abstract: </b>The problem of balancing covariates arises in observational studies where one
is given a group of control samples and another group, disjoint from the
control group, of treatment samples. Each sample, in either group, has several
observed nominal covariates. The values, or categories, of each covariate
partition the treatment and control samples to a number of subsets referred to
as \textit{levels} where the samples at every level share the same covariate
value. We address here a problem of selecting a subset of the control group so
as to balance, to the best extent possible, the sizes of the levels between the
treatment group and the selected subset of control group, the min-imbalance
problem.
</p>
<p>It is proved here that the min-imbalance problem, on two covariates, is
solved efficiently with network flow techniques. We present an integer
programming formulation of the problem where the constraint matrix is totally
unimodular, implying that the linear programming relaxation to the problem has
all basic solutions, and in particular the optimal solution, integral. This
integer programming formulation is linked to a minimum cost network flow
problem which is solvable in $O(n\cdot (n' + n\log n))$ steps, for $n$ the size
of the treatment group and $n'$ the size of the control group. A more efficient
algorithm is further devised based on an alternative, maximum flow, formulation
of the two-covariate min-imbalance problem, that runs in $O(n'^{3/2}\log^2n)$
steps.
</p></div>
    </summary>
    <updated>2020-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06819</id>
    <link href="http://arxiv.org/abs/2007.06819" rel="alternate" type="text/html"/>
    <title>Lower Bounds of Algebraic Branching Programs and Layerization</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Engels:Christian.html">Christian Engels</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06819">PDF</a><br/><b>Abstract: </b>In this paper we improve the lower bound of Chatterjee et al.\ (ECCC 2019) to
an $\Omega(n^2)$ lower bound for unlayered Algebraic Branching Programs. We
also
</p>
<p>study the impact layerization has on Algebraic Branching Programs. We exhibit
a polynomial that has an unlayered ABP of size $O(n)$ but any layered ABP has
size at least $\Omega(n\sqrt{n})$.
</p>
<p>We exhibit a similar dichotomy in the non-commutative setting where the
unlayered ABP has size $O(n)$ and any layered ABP has size at least
$\Omega(n\log n -\log^2 n)$.
</p></div>
    </summary>
    <updated>2020-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06754</id>
    <link href="http://arxiv.org/abs/2007.06754" rel="alternate" type="text/html"/>
    <title>Consensus Halving for Sets of Items</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goldberg:Paul_W=.html">Paul W. Goldberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hollender:Alexandros.html">Alexandros Hollender</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Igarashi:Ayumi.html">Ayumi Igarashi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manurangsi:Pasin.html">Pasin Manurangsi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Suksompong:Warut.html">Warut Suksompong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06754">PDF</a><br/><b>Abstract: </b>Consensus halving refers to the problem of dividing a resource into two parts
so that every agent values both parts equally. Prior work has shown that when
the resource is represented by an interval, a consensus halving with at most
$n$ cuts always exists, but is hard to compute even for agents with simple
valuation functions. In this paper, we study consensus halving in a natural
setting where the resource consists of a set of items without a linear
ordering. When agents have additive utilities, we present a polynomial-time
algorithm that computes a consensus halving with at most $n$ cuts, and show
that $n$ cuts are almost surely necessary when the agents' utilities are drawn
from probabilistic distributions. On the other hand, we show that for a simple
class of monotonic utilities, the problem already becomes PPAD-hard.
Furthermore, we compare and contrast consensus halving with the more general
problem of consensus $k$-splitting, where we wish to divide the resource into
$k$ parts in possibly unequal ratios, and provide some consequences of our
results on the problem of computing small agreeable sets.
</p></div>
    </summary>
    <updated>2020-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06744</id>
    <link href="http://arxiv.org/abs/2007.06744" rel="alternate" type="text/html"/>
    <title>WOR and $p$'s: Sketches for $\ell_p$-Sampling Without Replacement</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Edith.html">Edith Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pagh:Rasmus.html">Rasmus Pagh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06744">PDF</a><br/><b>Abstract: </b>Weighted sampling is a fundamental tool in data analysis and machine learning
pipelines. Samples are used for efficient estimation of statistics or as sparse
representations of the data. When weight distributions are skewed, as is often
the case in practice, without-replacement (WOR) sampling is much more effective
than with-replacement (WR) sampling: it provides a broader representation and
higher accuracy for the same number of samples. We design novel composable
sketches for WOR $\ell_p$ sampling, weighted sampling of keys according to a
power $p\in[0,2]$ of their frequency (or for signed data, sum of updates). Our
sketches have size that grows only linearly with the sample size. Our design is
simple and practical, despite intricate analysis, and based on off-the-shelf
use of widely implemented heavy hitters sketches such as CountSketch. Our
method is the first to provide WOR sampling in the important regime of $p&gt;1$
and the first to handle signed updates for $p&gt;0$.
</p></div>
    </summary>
    <updated>2020-07-15T23:26:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06693</id>
    <link href="http://arxiv.org/abs/2007.06693" rel="alternate" type="text/html"/>
    <title>The Invisible Hand Heuristic for Origin-Destination Integer Multicommodity Network Flows</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barr:Richard_S=.html">Richard S. Barr</a>, Thomas McLoud <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06693">PDF</a><br/><b>Abstract: </b>Origin-destination integer multicommodity flow problems differ from classic
multicommodity models in that each commodity has one source and one sink, and
each commodity must be routed along a single path. A new invisible-hand
heuristic that mimics economic markets' behavior is presented and tested on
large-scale telecommunications networks, with solution times two orders of
magnitude faster than Cplex's LP relaxation, more dramatic MIP ratios, and
small solution value differences.
</p></div>
    </summary>
    <updated>2020-07-15T23:43:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06658</id>
    <link href="http://arxiv.org/abs/2007.06658" rel="alternate" type="text/html"/>
    <title>Predicates of the 3D Apollonius Diagram</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Manos Kamarianakis <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06658">PDF</a><br/><b>Abstract: </b>In this thesis we study one of the fundamental predicates required for the
construction of the 3D Apollonius diagram (also known as the 3D Additively
Weighted Voronoi diagram), namely the EDGECONFLICT predicate: given five sites
$S_i, S_j,S_k,S_l,S_m$ that define an edge $e_{ijklm}$ in the 3D Apollonius
diagram, and a sixth query site $S_q$, the predicate determines the portion of
$e_{ijklm}$ that will disappear in the Apollonius diagram of the six sites due
to the insertion of $S_q$. Our focus is on the algorithmic analysis of the
predicate with the aim to minimize its algebraic degree. We decompose the main
predicate into sub-predicates, which are then evaluated with the aid of
additional primitive operations. We show that the maximum algebraic degree
required to answer any of the sub-predicates and primitives, and, thus, our
main predicate is 10 in non-degenerate configurations when the trisector is of
Hausdorff dimension 1. We also prove that all subpredicates developed can be
evaluated using 10 or 8-degree demanding operations for degenerate input for
these trisector types, depending on whether they require the evaluation of an
intermediate INSPHERE predicate or not. Among the tools we use is the 3D
inversion transformation and the so-called qualitative symbolic perturbation
scheme. Most of our analysis is carried out in the inverted space, which is
where our geometric observations and analysis is captured in algebraic terms.
</p></div>
    </summary>
    <updated>2020-07-15T23:44:05Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06604</id>
    <link href="http://arxiv.org/abs/2007.06604" rel="alternate" type="text/html"/>
    <title>Update Query Time Trade-off for dynamic Suffix Arrays</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Amir:Amihood.html">Amihood Amir</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boneh:Itai.html">Itai Boneh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06604">PDF</a><br/><b>Abstract: </b>The Suffix Array SA(S) of a string S[1 ... n] is an array containing all the
suffixes of S sorted by lexicographic order. The suffix array is one of the
most well known indexing data structures, and it functions as a key tool in
many string algorithms. In this paper, we present a data structure for
maintaining the Suffix Array of a dynamic string. For every $0 \leq \varepsilon
\leq 1$, our data structure reports SA[i] in $\tilde{O}(n^{\varepsilon})$ time
and handles text modification in $\tilde{O}(n^{1-\varepsilon})$ time.
Additionally, our data structure enables the same query time for reporting
iSA[i], with iSA being the Inverse Suffix Array of S[1 ... n]. Our data
structure can be used to construct sub-linear dynamic variants of static
strings algorithms or data structures that are based on the Suffix Array and
the Inverse Suffix Array.
</p></div>
    </summary>
    <updated>2020-07-15T23:37:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06569</id>
    <link href="http://arxiv.org/abs/2007.06569" rel="alternate" type="text/html"/>
    <title>Conformal mapping in linear time</title>
    <feedworld_mtime>1594771200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bishop:Christopher_J=.html">Christopher J. Bishop</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06569">PDF</a><br/><b>Abstract: </b>Given any $\epsilon &gt;0$ and any planar region $\Omega$ bounded by a simple
n-gon $P$ we construct a ($1 + \epsilon)$-quasiconformal map between $\Omega$
and the unit disk in time $C(\epsilon)n$. One can take $ C(\epsilon) = C + C
\log (1/\epsilon) \log \log (1/\epsilon)$.
</p></div>
    </summary>
    <updated>2020-07-15T23:44:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/105</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/105" rel="alternate" type="text/html"/>
    <title>TR20-105 |  Automating Regular or Ordered Resolution is NP-Hard | 

	Zoë Bell</title>
    <summary>We show that is hard to find regular or even ordered (also known as Davis-Putnam) Resolution proofs, extending the breakthrough result for general Resolution from Atserias and Müller to these restricted forms. Namely, regular and ordered Resolution are automatable if and only if P = NP. Specifically, for a CNF formula $F$ the problem of distinguishing between the existence of a polynomial-size ordered Resolution refutation of $F$ and an at least exponential-size general Resolution proof being required to refute $F$ is NP-complete.</summary>
    <updated>2020-07-14T13:43:02Z</updated>
    <published>2020-07-14T13:43:02Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-15T23:44:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2020/07/14/there-are-none/</id>
    <link href="http://benjamin-recht.github.io/2020/07/14/there-are-none/" rel="alternate" type="text/html"/>
    <title>There are none</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the <a href="http://www.argmin.net/2020/07/08/gain-margin/">last post</a>, we showed that continuous-time LQR has “natural robustness” insofar as the optimal solution is robust to a variety of model-mismatch conditions. LQR makes the assumption that the state of the system is fully, perfectly observed. In many situations, we don’t have access to such perfect state information. What changes?</p>

<p>The generalization of LQR to the case with imperfect state observation is called “Linear Quadratic Gaussian” control (LQG). This is the simplest, special case of a Partially Observed Markov Decision Process (POMDP). We again assume linear dynamics:</p>



<p>where the state is now corrupted by zero-mean Gaussian noise, $w_t$. Instead of measuring the state $x_t$ directly, we instead  measure a signal $y_t$ of the form</p>



<p>Here, $v_t$ is also zero-mean Gaussian noise. Suppose we’d still like to minimize a quadratic cost function</p>



<p>This problem is very similar to our LQR problem except for the fact that we get an indirect measurement of the state and need to apply some sort of <em>filtering</em> of the $y_t$ signal to estimate $x_t$.</p>

<p>The optimal solution for LQG is strikingly elegant. Since the observation of $x_t$ is through a Gaussian process, the maximum likelihood estimation algorithm has a clean, closed form solution, even in continuous time. Our best estimate for $x_t$, denoted $\hat{x}_t$, given all of the data observed up to time $t$ obeys a differential equation</p>



<p>The matrix $L$ that can be found by solving an algebraic Riccati equation that depends on the variance of $v_t$ and $w_t$ and on the matrices $A$ and $C$. In particular, it’s the CARE with data $(A^\top,C^\top,\Sigma_w,\Sigma_v)$. This solution is called a <em>Kalman Filter</em> and is a continuous limit of the discrete time Kalman Filter one might see in a course on graphical models.</p>

<p>The optimal LQG solution takes the estimate of the Kalman Filter, $\hat{x}_t$, and sets the control signal to be</p>



<p>Here, $K$ is gain matrix that would be used to solve the LQR problem with data $(A,B,Q,R)$. That is, LQG performs optimal filtering to compute the best state estimate, and then computes a feedback policy as if this estimate was a noiseless measurement of the state. That this turns out to be optimal is one of the more amazing results in control theory. It decouples the process of designing an optimal filter from designing an optimal controller, enabling simplicity and modularity in control design. This decoupling where we treat the output of our state estimator as the true state is an example of <em>certainty equivalence</em>, the umbrella term for using point estimates of stochastic quantities as if they were the correct value. Though certainty equivalent control may be suboptimal in general, it remains ubiquitous for all of the benefits it brings as a design paradigm. Unfortunately, not only is this decoupled design of filters and controllers often suboptimal, it has many hidden fragilities. LQG highlights a particular scenario where certainty equivalent control leads to misplaced optimism about robustness.</p>

<p>We saw in the previous post that LQR had this amazing robustness property: even if you optimize with the wrong model, you’ll still probably be OK. Is the same true about LQG? What are the guaranteed stability margins for LQG regulators? The answer was succinctly summed up in the <a href="https://ieeexplore.ieee.org/document/1101812">abstract of a 1978 paper by John Doyle</a>: “There are none.”</p>

<p class="center"><img alt="There Are None" src="http://www.argmin.net/assets/there_are_none.png" width="400px"/></p>

<p>What goes wrong? Doyle came up with a simple counterexample, that I’m going to simplify even further for the purpose of contextualizing in our modern discussion. Before presenting the example, let’s first dive into <em>why</em> LQG is likely less robust than LQR. Let’s assume that the true dynamics obeys the ODE:</p>



<p>though we computed the optimal controller with the matrix $B$. Define an error signal, $e_t = x_t - \hat{x}_t$, that measures the current deviation between the actual state and the estimate. Then, using the fact that $u_t = -K \hat{x}_t$, we get the closed loop dynamics</p>



<p>When $B=B_\star$, the bottom left block is equal to zero. The system is then stable provided $A-BK$ and $A-LC$ are both stable matrices (i.e., have eigenvalues in the left half plane). However, small perturbations in the off-diagonal block can make the matrix unstable. For intuition, consider the matrix</p>



<p>The eigenvalues of this matrix are $-1$ and $-2$, so the matrix is clearly stable. But the matrix</p>



<p>has an eigenvalue greater than zero if $t&gt;0.01$. So a tiny perturbation significantly shifts the eigenvalues and makes the matrix unstable.</p>

<p>Similar things happen in LQG. In Doyle’s example he uses the problem instance:</p>







<p>The open loop system here is unstable, having two eigenvalues at $1$. We can stabilize the system only by modifying the second state. The state disturbance is aligned along the $[1;1]$ direction, and the state cost only penalizes states aligned with this disturbance. So the goal is simply to remove as much signal as possible in the $[1;1]$ direction without using too much control authority. We only are able to measure the first component of the state, and this measurement is corrupted by Gaussian noise.</p>

<p>What does the optimal policy look like? Perhaps unsurprisingly, it focuses all of its energy on ensuring that there is little state signal along the disturbance direction. The optimal $K$ and $L$ matrices are</p>



<p>Now what happens when we have model mismatch? If we set $B_\star=tB$ and use the formula for the closed loop above, we see that closed loop state transition matrix is</p>



<p>It’s straight forward to check that when $t=1$ (i.e., no model mismatch), the eigenvalues of  $A-BK$ and $A-LC$ all have negative real parts. For the full closed loop matrix, analytically computing the eigenvalues themselves is a pain, but we can prove instability by looking at the characteristic polynomial. For a matrix to have all of its eigenvalues in the left half plane, its characteristic polynomial necessarily must have all positive coefficients. If we look at the linear term in the polynomial, we see that we must have</p>



<p>if we’d like any hope of having a stable system. Hence, we can guarantee that this closed loop system is unstable if $t\geq 1+\sigma$. This is a very conservative condition, and we could get a tighter bound if we’d like, but it’s good enough to reveal some paradoxical properties of LQG. The most striking is that if we build a sensor that gives us a better and better measurement, our system becomes more and more fragile to perturbation and model mismatch. For machine learning scientists, this seems to go against all of our training. How can a system become <em>less</em> robust if we improve our sensing and estimation?</p>

<p>Let’s look at the example in more detail to get some intuition for what’s happening. When the sensor noise gets small, the optimal Kalman Filter is more aggressive. If the model is true, then the disturbance has equal value in both states, so, when $\sigma$ is small, the filter can effectively just set the value of the second state to be equal to whatever is in the first state. The filter is effectively deciding that the first state should equal the observation $y_t$, and the second state should be equal to the first state. In other words, it rapidly damps any errors in the disturbance direction $[1;1]$ and, as $d$ increases, it damps the $[0;1]$ direction less. When $t \neq 1$, we are effectively introducing a disturbance that makes the two states unequal. That is, $B-B_\star$ is aligned in the $[0;1]$ and can be treated as a disturbance signal. This undamped component of the error is fed errors from the state estimate $\hat{x}$, and these errors compound each other. Since we spend so much time focusing on our control along the direction of the injected state noise, we become highly susceptible to errors in a different direction and these are the exact errors that occur when there is a gain mismatch between the model and reality.</p>

<p>The fragility of LQG has many takeaways. It highlights that noiseless state measurement can be a dangerous modeling assumption, because it is then optimal to trust our model too much. Though we apparently got a freebie with LQR, for LQG, model mismatch must be explicitly accounted for when designing the controller.</p>

<p>This should be a cautionary tale for modern AI systems. Most of the papers I read in reinforcement learning consider MDPs where we get perfect state measurement. Building an entire field around optimal actions with perfect state observation builds too much optimism. Any realistic scenario is going to have partial state observation, and such problems are much thornier.</p>

<p>A second lesson is that it is not enough to just improve the prediction components in feedback systems that are powered by machine learning. I have spoken with many applied machine learning engineers who have told me that they have seen performance degrade in production systems when they improve their prediction model. They might spend months building some state of the art LSTM mumbo jumbo that is orders of magnitude more accurate in prediction, but in production yields worse performance than the legacy system with a boring ARMA model. It is quite possible that these performance drops are due to the Doyle effect: the improved prediction system is increasing sensitivity to a modeling flaw in some other part of the engineering pipeline.</p>

<p>The story turns out to be even worse than what I have described thus far. The supposed robustness guarantees we derived for LQR assume not just full noiseless state measurement, but that the sensors and actuators have infinite bandwidth. That is, they assume you can build controllers $K$ with arbitrarily large entries and that react instantaneously, without delay, to changes in the state. In the next post, I’ll show how realistic sampled data controllers for LQR, even with noiseless state measurement, also have no guarantees.</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <published>2020-07-14T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2020-07-15T23:46:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=3843</id>
    <link href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/" rel="alternate" type="text/html"/>
    <title>Gradient descent for wide two-layer neural networks – II: Generalization and implicit bias</title>
    <summary>In this blog post, we continue our investigation of gradient flows for wide two-layer “relu” neural networks. In the previous post, Francis explained that under suitable assumptions these dynamics converge to global minimizers of the training objective. Today, we build on this to understand qualitative aspects of the predictor learnt by such neural networks. The...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">In this blog post, we continue our investigation of gradient flows for wide two-layer “relu” neural networks. In the <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">previous post</a>, Francis explained that under suitable assumptions these dynamics converge to global minimizers of the training objective. Today, we build on this to understand qualitative aspects of the predictor learnt by such neural networks. The content is mostly based on our recent joint work [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>].</p>



<h2>1. Generalization with weight decay regularization</h2>



<p class="justify-text">Let us start our journey with the comfortable case where the training objective includes an explicit <em>weight decay</em> regularization (i.e. \(\ell_2\)-regularization on the parameters). Using the notations of the previous post, this consists in the following objective function on the space of probability measures on \(\mathbb{R}^{d+1}\):  $$ \underbrace{R\Big(\int_{\mathbb{R}^{d+1}} \Phi(w)d\mu(w)\Big)}_{\text{Data fitting term}} + \underbrace{\frac{\lambda}{2} \int_{\mathbb{R}^{d+1}} \Vert w \Vert^2_2d\mu(w)}_{\text{Regularization}} \tag{1}$$ where \(R\) is the loss and \(\lambda&gt;0\) is the regularization strength. Remember that a  neural network of finite width with \(m\) neurons is recovered with an empirical measure \(\mu = \frac1m \sum_{j=1}^m\delta_{w_j}\), in which case this regularization is proportional to the sum of the squares of all the parameters \(\frac{\lambda}{2m}\sum_{j=1}^m \Vert w_j\Vert^2_2\).</p>



<p class="justify-text"><strong>Variation norm.</strong> In the previous post, we have seen that the Wasserstein gradient flow of this objective function — an idealization of the gradient descent training dynamics in the large width limit — converges to a global minimizer \(\mu^*\) when initialized properly. An example of an admissible initialization is the hidden weights \(b_j\) distributed according to the uniform distribution \(\tau\) on the unit sphere \(\mathbb{S}^{d-1}\subset \mathbb{R}^d\) and the output weights \(a_j\) uniform in \(\{-1,1\}\). What does this minimizer look like in predictor space when the objective function is as in Eq. (1) ? </p>



<p class="justify-text">To answer this question, we define for a predictor \(h:\mathbb{R}^d\to \mathbb{R}\), the quantity $$ \Vert h \Vert_{\mathcal{F}_1} := \min_{\mu \in \mathcal{P}(\mathbb{R}^{d+1})} \frac{1}{2} \int_{\mathbb{R}^{d+1}} \Vert w\Vert^2_2 d\mu(w) \quad \text{s.t.}\quad h = \int_{\mathbb{R}^{d+1}} \Phi(w)d\mu(w).\tag{2} $$ As the notation suggests, \(\Vert \cdot \Vert_{\mathcal{F}_1}\) is a norm in the space of predictors. It is known as the <em>variation norm</em> [<a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">2</a>, <a href="https://www.cs.cas.cz/~vera/publications/journals/I3Edin.pdf">3</a>]. We call \(\mathcal{F}_1\) the space of functions with finite norm, which is a Banach space. By construction, the learnt predictor \(h^* = \int \Phi(w)d\mu^*(w)\) is a minimizer of the \(\mathcal{F}_1\)-regularized regression: $$ \min_{h:\mathbb{R}^d\to \mathbb{R}} R(h) + \lambda \Vert h \Vert_{\mathcal{F}_1} \tag{3}.$$ This \(\mathcal{F}_1\)-norm regularization shares similarity with \(\ell_1\) regularization [<a href="https://arxiv.org/pdf/1412.6614.pdf">4</a>]. To see this, observe that the “magnitude” \(\vert a\vert \Vert b\Vert_2\) of a relu function \(x\mapsto a(b^\top x)_+\) with parameter \(w=(a,b)\) equals \(\Vert w\Vert^2_2/2\) if \(\vert a\vert = \Vert b\Vert_2\) and is smaller otherwise. Thus parameterizing the relus by their direction \(\theta = b/\Vert b\Vert_2\) and optimizing over their signed magnitude \(r(\theta) = a\Vert b\Vert_2\)  we have $$ \Vert h \Vert_{\mathcal{F}_1} = \inf_{r:\mathbb{S}^{d-1}\to \mathbb{R}} \int_{\mathbb{S}^{d-1}} \vert r(\theta)\vert d\tau(\theta) \quad \text{s.t.}\quad h(x) = \int _{\mathbb{S}^{d-1}} r(\theta) (\theta^\top x)_+ d\tau(\theta).\tag{4}$$</p>



<p class="justify-text"><strong>Conjugate RKHS norm.</strong> The regression in the space \(\mathcal{F}_1\) is best understood when compared with the regression obtained by only training the output weights. We consider the same training dynamics with weight decay except that we fix the hidden weights to their initial value, where they are distributed according to the uniform distribution \(\tau\) on the sphere. In that case, the Wasserstein gradient flow also converges to the solution of a regularized regression as in Eq. (3) — this is in fact a convex problem —  but the regularizing norm is different and now defined as $$ \Vert h \Vert_{\mathcal{F}_2}^2 := \min_{r:\mathbb{S}^{d-1}\to \mathbb{R}} \int_{\mathbb{S}^{d-1}} \vert r(\theta)\vert^2 d\tau(\theta) \quad \text{s.t.}\quad h(x) = \int _{\mathbb{S}^{d-1}} r(\theta) (\theta^\top x)_+ d\tau(\theta).$$ We call \(\mathcal{F}_2\) the set of functions with finite norm. It can be shown to be a <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">Reproducing Kernel Hilbert Space</a> (RKHS), with kernel  $$ K(x,x’) = \int_{\mathbb{S}^{d-1}} (\theta^\top x)_+ (\theta^\top x’)_+ d\tau(\theta),$$ which has a closed form expression [<a href="https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">5</a>]. In this context, taking a finite width neural network corresponds to a random feature approximation of the kernel [<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf">6</a>, <a href="https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines">7</a>].</p>



<p class="justify-text">Let us informally compare the properties of these spaces \(\mathcal{F}_1\) and \(\mathcal{F}_2\) (see [<a href="https://arxiv.org/abs/1412.8690">2</a>] for details):</p>



<ul class="justify-text"><li><strong>Approximation power.</strong> In high dimension, only very smooth functions have small \(\mathcal{F}_2\)-norm (in rough terms, the \(\lceil (d+3)/2\rceil\) first derivatives should be small). In contrast, there exists non-smooth functions with small \(\mathcal{F}_1\)-norm, an example being the relu function \(x\mapsto (\theta^\top x)_+\). Remarkably, if we define \(f(x)=g(Ux)\) where \(U\) is an orthogonal projection then \(\Vert f\Vert_{\mathcal{F}_1} \leq  \Vert g\Vert_{\mathcal{F}_2}\). This shows in particular that \(\mathcal{F}_1\) contains \(\mathcal{F}_2\) and that \(\mathcal{F}_1\) is <em>adaptive</em> to lower dimensional structures.</li><li><strong>Statistical complexity.</strong> It could be feared that the good approximation properties of \(\mathcal{F}_1\) come at the price of being “too large” as a hypothesis space, making it difficult to estimate a predictor in \(\mathcal{F}_1\) from few samples. But, as measured by their Rademacher complexities, the unit ball of \(\mathcal{F}_1\) is only \(O(\sqrt{d})\) larger than that of \(\mathcal{F}_2\). By going from \(\mathcal{F}_2\) to \(\mathcal{F}_1\), we thus add some nicely structured predictors to our hypothesis space, but not too much garbage that could fit unstructured noise.</li><li><strong>Generalization guarantees.</strong> By combining the two previous points, it is possible to prove that supervised learning in \(\mathcal{F}_1\) breaks the curse of dimensionality when the output depends on a lower dimensional projection of the input: the required number of training samples only depends mildly on the dimension \(d\).</li><li><strong>Optimization guarantees.</strong> However \(\mathcal{F}_1\) has a strong drawback : there is no known algorithm that solves the problem of Eq. (3) in polynomial time. On practical problems, gradient descent seems to behave well, but in general only qualitative results such as presented in the previous post are known. In contrast, various provably efficient algorithms can solve regression in \(\mathcal{F}_2\), which is a classical kernel ridge regression problem [Chap. 14.4.3, <a href="https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20A%20Probabilistic%20Perspective%20%5BMurphy%202012-08-24%5D.pdf">8</a>].</li></ul>



<p class="justify-text">In the plot below, we compare the predictor learnt by gradient descent for a 2-D regression with the square loss and weight decay, after training (a) both layers — which is regression in \(\mathcal{F}_1\) — or (b) just the output layer — which is regression in \(\mathcal{F}_2\). This already illustrates some distinctive features of both spaces, although the differences become more stringent in higher dimensions. In particular, observe that in (a) the predictor is the combination of few relu functions, which illustrates  the sparsifying effect of the \(L^1\)-norm in Eq. (4). To simplify notations, we do not include a bias/intercept in the formulas but our numerical experiments include it, so in this plot the input is of the form \(x=(x_1,x_2,1)\) and \(d=3\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4231" height="293" src="https://francisbach.com/wp-content/uploads/2020/07/regularized-2.png" width="564"/>Predictor learnt by the gradient flow on the square loss with weight decay, when training (a) both layers (b) only the output layer. The markers indicate the location of the training samples  \((x_i)_{i=1}^n\). <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_weightdecay.jl">[code]</a></figure></div>



<p class="justify-text">The qualitative picture is quite clear so far, but something is a bit unsettling: weight decay is often not needed to obtain a good performance in practice. Our line of reasoning however completely falls apart without such a regularization: if the objective function depends on the predictor only via its values on the training set, being a minimizer does not guarantee anything about generalization outside of the training set (remember that wide relu neural networks are <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximators</a>). Why does it still work in the unregularized case? There must be something in the algorithm…</p>



<h2>2. Implicit bias: linear classification</h2>



<p class="justify-text">This something is called the <em>implicit bias</em> : when there are several minimizers, the optimization algorithm makes a specific choice. In the unregularized case, the “quality” of this choice is a crucial property of an algorithm; much more crucial than, say, its convergence speed on the training objective. To gradually build our intuition of the implicit bias of gradient flows, let us put neural networks aside for a moment and consider, following Soudry, Hoffer, Nacson, Gunasekar and Srebro [<a href="http://www.jmlr.org/papers/volume19/18-188/18-188.pdf">9</a>], a linear classification task.</p>



<p class="justify-text"><strong>Gradient flow of the smooth-margin.</strong> Let \((x_i,y_i)_{i=1}^n\) be a training set of \(n\) pairs of inputs \(x_i\in \mathbb{R}^d\) and outputs \(y_i\in \{-1,1\}\) and let us choose the exponential loss. The analysis that follows also apply to the logistic loss (which is the same as the cross-entropy loss after a sigmoid non-linearity) because only the “tail” of the loss matters, but it is more straightforward with the exponential loss. In order to give a natural “scale” to the problem, we  renormalize the empirical risk by taking minus its logarithm and consider the concave objective $$ F_\beta(a) = -\frac{1}{\beta}\log\Big( \frac1n \sum_{i=1}^n \exp(-\beta y_i \ x_i^\top a) \Big).\tag{5}$$ </p>



<p class="justify-text">Here \(\beta&gt;0\) is a parameter that will be useful in a moment. For now, we take \(\beta=1\) and we note \(F(a)=F_1(a)\).  In this context, the <em>margin</em> of a vector \(a\in \mathbb{R}^d\) is the quantity \(\min_{i} y_i\ x_i^\top a\) which quantifies how far this linear predictor is from making a wrong prediction on the training set.  </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4274" height="386" src="https://francisbach.com/wp-content/uploads/2020/07/max_margin-4.png" width="453"/>The margin of the linear predictor \(x \mapsto a^\top x\) with parameters \(a \in \mathbb{S}^{d-1}\) is the smallest distance of a training point to the decision boundary. We show here the max-margin predictor.</figure></div>



<p class="justify-text">Obtained via simple manipulations, the inequalities  $$ \min_i y_i\ x_i^\top a \leq F_\beta(a) \leq \min_i y_i\ x_i^\top a +\frac{\log(n)}{\beta}, \tag{6}$$ suggest to call \(F_\beta\) the <em>smooth-margin</em> because, well, it is smooth and converges to the margin \(F_\infty(a) := \min_i y_i x_i^\top a\) as \(\beta\to \infty\). Let us look at the gradient flow in the ascent direction that maximizes the smooth-margin: $$ a'(t) = \nabla F(a(t))$$ initialized with \(a(0)=0\) (here the initialization does not matter so much). The path followed by this gradient flow is exactly the same as the gradient flow on the empirical risk: taking the logarithm only changes the time parameterization or, in practice, the step-size.</p>



<p class="justify-text"><strong>Convergence to the max-margin.</strong> Assume that the data set is linearly separable, which means that the \(\ell_2\)-max-margin $$ \gamma := \max_{\Vert a\Vert_2 \leq 1} \min_i y_i x_i^\top a$$ is positive. In this case \(F\) is unbounded (indeed \(\lim_{\alpha \to \infty} F(\alpha a) =\infty\) whenever \(a\)  has a positive margin) and thus \(a(t)\) diverges. This is not an issue as such, since for classification, only the sign of the prediction matters.  This just means that the relevant question is not “where does \(a(t)\) converge?” but rather “towards which direction does it diverge?”. In other words, we are interested in the limit of \(\bar a(t):= a(t)/\Vert a(t)\Vert_2\) (in convex analysis, this is called the <em>cosmic limit</em> of \(a(t)\) [Chap. 3, <a href="https://www.springer.com/gp/book/9783540627722">10</a>], isn’t it beautiful ?).</p>



<p class="justify-text">The argument that follows is adapted from [<a href="https://arxiv.org/pdf/1802.08246.pdf">11</a>, <a href="https://arxiv.org/pdf/1803.07300.pdf">12</a>] and can be traced back to [<a href="http://proceedings.mlr.press/v28/telgarsky13-supp.pdf">13</a>] for coordinate ascent. It can be shown by looking at the structure of the gradient (see the end of the blog post) that \(\Vert \nabla F(a)\Vert_2\geq \gamma\) for all \(a\in \mathbb{R}^d\). By the inequality of Eq. (6) and the gradient flow property \(\frac{d}{dt}F(a(t))=\Vert \nabla F(a(t))\Vert_2^2\), it follows $$\begin{aligned}\min_i y_i x_i^\top a(t) \geq F(a(t)) \  – \log(n) \geq \gamma \int_0^t \Vert \nabla F(a(s))\Vert_2ds -\log (n).\end{aligned}$$  For \(t&gt; \log(n)/\gamma^2\), this lower bound is positive. We can then divide the left-hand side by \(\Vert a(t)\Vert_2\) and the right-hand side by the larger quantity \(\int_0^t \Vert\nabla F(a(s))\Vert_2ds\), and we get $$\min_i y_i x_i^\top \bar a(t) \geq \gamma -\frac{\log(n)}{\int_0^t \Vert\nabla F(a(s))\Vert_2ds} \geq \gamma -\frac{\log(n)}{\gamma t}.$$ This shows that the margin of \(\bar a(t) := a(t)/\Vert a(t)\Vert_2\) converges to the \(\ell_2\)-max-margin at a rate \(\log(n)/\gamma t\). That’s it, the implicit bias of this gradient flow is exposed!</p>



<p class="justify-text"><strong>Stability to step-size choice.</strong> To translate this argument to discrete time, we need decreasing step-sizes of order \(1/\sqrt{t}\) which deteriorates the convergence rate to \(\tilde O(1/\sqrt{t})\), see [<a href="https://arxiv.org/pdf/1802.08246.pdf">11</a>, <a href="https://arxiv.org/pdf/1803.07300.pdf">12</a>]. In [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>], we proposed a different proof strategy (based on an online optimization interpretation of \(\bar a(t)\), as below) which recovers the same convergence rate \(O(1/\sqrt{t})\) with <em>exponentially larger</em> step-sizes. This suggests that these diverging trajectories are extremely robust to the choice of step-size.</p>



<p class="justify-text"><strong>Illustration. </strong>In the figure below, we plot on the left the evolution of the parameter \(a(t)\) and on the right the predictor \(x\mapsto (x,1)^\top a(t)\) with \(x\in \mathbb{R}^2\). In parameter space, we apply the hyperbolic tangent to the radial component which allows to easily visualize diverging trajectories. This way, the unit sphere represents the <em>horizon</em> of \(\mathbb{R}^d\), i.e., the set of directions at infinity [Chap. 3 in <a href="https://www.springer.com/gp/book/9783540627722">9</a>]. We will use the same convention in the other plots below.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4106" height="288" src="https://francisbach.com/wp-content/uploads/2020/07/linear.gif" width="586"/>Implicit bias of gradient descent for a linear classification task with the exponential loss: (left) parameter space, (right) predictor space.</figure></div>



<h2>3. Implicit bias:  training only the output layer</h2>



<p class="justify-text">Despite its apparently restrictive setting, the previous result already tells us something about wide neural networks. Consider the situation touched upon earlier where we only train the output weights \(a_j\) and the hidden weights \(b_j\) are picked uniformly at random on the sphere. This corresponds to learning a linear classifier on top of the random feature \([(b_j^\top x)_+]_{j=1}^m\). </p>



<p class="justify-text">As we have just shown, if the training set is separable, the normalized gradient flow of the unregularized exponential loss (or logistic loss) converges to a solution to  $$ \max_{\Vert a\Vert_2 \leq 1}\min_i y_i \sum_{j=1}^m  a_j (b_j^\top x_i)_+.$$ </p>



<p class="justify-text">This is a random feature approximation for the unregularized kernel support vector machine problem in the RKHS \(\mathcal{F}_2\), which is recovered in the large width limit \(m\to \infty\):  $$\max_{\Vert h\Vert_{\mathcal{F}_2}\leq 1} \min_i y_i h(x_i).$$ Notice that if \(m\) is large enough, the linear separability assumption is not even needed anymore, because any training set is separable in \(\mathcal{F}_2\) (at least if all \(x_i\)s are distinct and if we do not forget to include the bias/intercept).</p>



<p class="justify-text"><strong>Illustration.</strong> In the animation below, we plot on the left the evolution of the parameters and on the right the predictor for a 2-D classification task. In parameter space, each particle represents a neuron: their direction is fixed, their distance to \(0\) is their absolute weight and the color is red (+) or blue (-) depending on the sign of the weight. As above, the unit sphere is at infinity and the particles diverge. In predictor space, the markers represent the training samples of both classes, the color shows the predictor and the black line is the decision boundary. The fact that the predictor has a smooth decision boundary is in accordance with the properties of \(\mathcal{F}_2\) given above. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" class="wp-image-4275" src="https://francisbach.com/wp-content/uploads/2020/07/film_output_comp-1.gif"/>Gradient descent on the output layer of a two-layer relu neural network with the exponential loss: (left) parameter space, (right) predictor space. <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_output.jl">[code]</a></figure></div>



<h2>4. Implicit bias: 2-homogeneous linear classifiers</h2>



<p class="justify-text">Although the analyses where neural networks behave like kernel methods are pleasant for us theoreticians because we are in conquered territory, they miss essential aspects of neural networks such as their adaptivity and their ability to learn a representation. Let us see if we can characterize the implicit bias of the gradient flow of the unregularized exponential loss when training <em>both</em> layers of the neural network.</p>



<p class="justify-text"><strong>A 2-homogeneous linear model.</strong> From an optimization point of view, an important property of two layer relu neural networks is that \(\Phi(\alpha w)= \alpha^2 \Phi(w)\) for all \(\alpha&gt;0\), i.e., they are positively 2-homogeneous in the training parameters. In contrast, a linear model is 1-homogeneous in the parameters. This seemingly little difference leads to drastic changes in the gradient flow dynamics. </p>



<p class="justify-text">Let us again build our intuition with a simplified model that captures key aspects of the dynamics, namely the linear classification setting of above. This time, we take any initialization \(r(0)\in \mathbb{R}^d\) with positive entries and the gradient flow in the ascent direction of the function \( F(r\odot r)\) where \(\odot\) is the pointwise product between two vectors and \(F\) is defined in Eq. (5). This is just a trick to obtain a 2-homogeneous parameterization of a linear model. This gradient flow satisfies $$ r'(t) = 2 r(t)\odot \nabla F(r(t)\odot r(t)).$$ </p>



<p class="justify-text"><strong>Normalized dynamics.</strong> Let us define \(\bar a(t):=(r(t)\odot r(t))/\Vert r(t)\Vert_2^2\) the normalized predictor associated to our dynamics which, by definition, belongs to the simplex \(\Delta_d\), i.e., the set of nonnegative vectors in \(\mathbb{R}^d\) that sum to one. Using the fact that \(\nabla F(\beta a) = \nabla F_\beta (a)\) for all \(\beta&gt;0\), we obtain $$\begin{aligned} \bar a'(t) &amp;= 2\frac{r(t)\odot r'(t)}{\Vert r(t)\Vert_2^2} -2 (r(t)^\top r'(t))\frac{r(t)\odot r(t)}{\Vert r(t)\Vert_2^4}\\ &amp;=4\bar a(t) \odot \nabla F_{\Vert r(t)\Vert_2^2}(\bar a(t))\ – \alpha(t) \bar a(t)\end{aligned}$$ where \(\alpha(t)\) is the scalar such that \(\sum_{i=1}^d a’_i(t) =0\). Online optimization experts might have recognized that this is (continuous time) <em>online mirror ascent in the simplex</em> for the sequence of smooth-margin functions \(F_{\Vert r(t)\Vert_2^2}\). Notice in particular the multiplicative updates: they correspond to the entropy mirror function, and they are particularly well suited for optimization in the high dimensional simplex [Chap.4, <a href="https://arxiv.org/pdf/1405.4980.pdf">14</a>].</p>



<p>What do we learn from this reformulation? </p>



<ul class="justify-text"><li>We can prove (by similar means) that if the data set is linearly separable then \(\Vert r(t)\Vert_2^2\) diverges. So the sequence of functions \(F_{\Vert r\Vert_2^2}\) converges to the margin \(F_\infty\) which means that \(\bar a(t)\) just ends up optimizing the function \(F_\infty\). As a consequence, we have $$\lim_{t\to \infty} y_i x_i^\top \bar a(t) = \max_{a\in \Delta_d} \min_{i} y_i x_i^\top a.$$ This exposes another implicit bias of gradient flow. Notice the key difference with the implicit bias obtained with a linear parameterization: we obtain here the \(\ell_1\)-max-margin (over classifiers with non-negative entries) instead of the \(\ell_2\)-max-margin.  </li><li>Beyond exposing the implicit bias, this reformulation shows that \(\bar a(t)\) implicitly optimizes a sequence of smooth objectives which converge to the margin \(F_\infty\). Unknowingly, we have recovered the well-principled optimization method that consists in approximating a non-smooth objective with smooth functions [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">15</a>].</li><li>While the conclusion above was only formal, this point of view leads to rigorous proofs of convergence and convergence rates in discrete time in \(\tilde O(1/\sqrt{t})\) with a step-size in \(O(1/\sqrt{t})\), by  exploiting tools from online optimization, see [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>].</li></ul>



<h2>5. Implicit bias: fully trained 2-layer neural networks</h2>



<p class="justify-text">Once again this argument about linear predictors applies to neural networks: if we train both layers but only the magnitude of the hidden weights and not their direction, then this is equivalent to learning a 2-homogeneous linear model on top of the random feature \([  a_j(0) (x_i^\top b_j(0))_+]_{j=1}^m\). If each feature appears twice with opposite signs — which is essentially the case in the large width limit — then the simplex constraint can be equivalently replaced by an \(\ell_1\)-norm constraint on the weights. Recalling the definition of the \(\mathcal{F}_1\)-norm from Eq. (4), we thus obtain that, in the infinite-width limit, the normalized predictor converges to a solution to $$ \max_{\Vert h\Vert_{\mathcal{F}_1} \leq 1} \min_i y_i h(x_i).$$</p>



<p class="justify-text">This result is correct, but it is not relevant. In contrast to functions in \(\mathcal{F}_2\), functions in \(\mathcal{F}_1\) <em>can not</em> in general be approximated with few <em>random</em> features in high dimension. In fact, lower bounds that are exponential in the dimension exist in certain settings [Sec. X, <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">16</a>]. They can be approximated with a small number of features but those need to be data-dependent: in that sense, it is necessary to learn a representation – here,  a distribution over the hidden weights — in order to learn in \(\mathcal{F}_1\). </p>



<p class="justify-text">This raises the following question: do we obtain the same implicit bias when training both layers of the neural network, without fixing the direction of the input weights? In the following result, which is the main theorem of our paper [<a href="https://arxiv.org/abs/2002.04486">1</a>], we answer by the affirmative.</p>



<p class="justify-text"><strong>Theorem</strong> (C. and Bach [<a href="https://arxiv.org/abs/2002.04486">1</a>], informal). Assume that for some \(\sigma&gt;0\), the hidden weights \(b_j\) are initialized uniformly on the sphere of radius \(\sigma\) and the output weights \(a_j\) are uniform in \(\{-\sigma,\sigma\}\). Let \(\mu_t\) be the Wasserstein gradient flow for the unregularized exponential loss and \(h_t = \int \Phi(w)d\mu_t(w)\) be the corresponding dynamics in predictor space. Under some technical assumptions, the normalized predictor \(h_t/\Vert h_t\Vert_{\mathcal{F}_1}\) converges to a solution to the \(\mathcal{F}_1\)-max-margin problem: $$\max_{\Vert h\Vert_{\mathcal{F}_1} \leq 1} \min_i y_i h(x_i).$$</p>



<p class="justify-text">Giving an idea of proof would be a bit too technical for this blog post, but let us make some remarks:</p>



<ul class="justify-text"><li>The strength of this result is that although this dynamics could get trapped towards limit directions which are not optimal, this choice of initialization allows to avoid them all and to only converge to <em>global</em> minimizers of this max-margin problem. The principle behind this is similar to the global convergence result in the previous blog post. </li><li>The fact that optimizing on the direction of the hidden weights is compatible with the global optimality conditions of the \(\mathcal{F}_1\)-max-margin problem is very specific to the structure of positively 2-homogeneous problems, and should not be taken for granted for other architectures of neural networks.</li><li>Although at a formal level this result works for any initialization that is diverse enough (such as the standard Gaussian initialization), the initialization proposed here yields dynamics with a better behavior for relu networks: by initializing the hidden and output weights with equal norms – a property preserved by the dynamics – we avoid some instabilities in the gradient. Also notice that this result applies to any scale \(\sigma&gt;0\) of the initialization (we’ll see an intriguing consequence of this in the next section).</li></ul>



<p class="justify-text"><strong>Illustration.</strong> In the figure below, we plot the training dynamics when both layers are trained. In parameter space (left), each particle represents a neuron: its position is \(\vert a_j\vert b_j\) and its color depends on the sign of \(a_j\).  Here again the unit sphere is at infinity. The inactive neurons at the bottom correspond to those with a bias that is “too negative” at initialization. We observe that all the other neurons gather into few clusters: this is the sparsifying effect of the \(L^1\)-norm in Eq. (4). In predictor space, we obtain a polygonal classifier, as expected for a \(\mathcal{F}_1\)-max-margin classifier. See the paper [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>] for experiments that illustrate the strengths of this classifier in terms of generalization.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" class="wp-image-4194" src="https://francisbach.com/wp-content/uploads/2020/07/film_both_comp.gif"/>Training both layers of a wide relu neural network with the exponential loss: (left) space of parameters, (right) space of predictors. <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_bothlayers.jl">[code]</a></figure></div>



<h2>6. Lazy regime and the neural tangent kernel</h2>



<p class="justify-text">This blog post would not be complete without mentioning the <em>lazy regime</em>. This is yet another kind of implicit bias which, in our context, takes place when at initialization the weights have a large magnitude and the step-size is small. It was first exhibited in [<a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">17</a>] for deep neural networks.</p>



<p class="justify-text"><strong>Lazy training via scaling.</strong> This phenomenon is in fact very general so let us present it with a generic parametric predictor \(h(W)\) with differential \(Dh(W)\). We introduce a scaling factor \(\alpha&gt;0\) and look at the gradient flow of \(F(W) := R(\alpha h(W))\) with a step-size \(1/\alpha^2\), that is $$ W'(t) = \ – \frac{1}{\alpha}Dh(W(t))^\top \nabla R(\alpha h(W(t))),$$ with initialization \(W(0)\). In terms of the predictor \(\alpha h(W)\), this yields the dynamics $$\frac{d}{dt} \alpha h(W(t)) = \ – Dh(W(t))Dh(W(t))^\top \nabla R(\alpha h(W(t)).$$ </p>



<p class="justify-text">Lazy training [<a href="https://arxiv.org/pdf/1812.07956.pdf">18</a>] happens when we take \(\alpha\) large while making sure that \(\alpha h(W(0))\) stays bounded. In this case, we see that the parameters change at a rate \(O(1/\alpha)\), while the predictor changes at a rate independent of \(\alpha\). On any bounded time interval, in the limit of  a large \(\alpha\), the parameters only move infinitesimally, while the predictor still makes significant progress, hence the name <em>lazy training</em>.</p>



<p class="justify-text"><strong>Equivalent linear model.</strong> Since the parameters hardly move, if we assume that \(Dh(W(0))\neq 0\) then we can replace the map \(h\) by its linearization \(W \mapsto h(W(0))+Dh(W(0))(W-W(0))\). This means that the training dynamics essentially follows the gradient flow of the  objective $$ R\big ( \alpha h(W(0)) + \alpha Dh(W(0))(W-W(0)) \big)$$ which is a convex function of \(W\) as soon as \(R\) is convex.</p>



<p class="justify-text">If this objective admits a minimizer that is not too far away from \(W(0)\), then \(W(t)\) converges to this minimizer. If in contrast all  the minimizers are too far away (think of the exponential loss where they are at infinity), then the parameters will eventually move significantly and the lazy regime is just a transient regime in the early phase of training.  Of course, all these behaviors can be quantified and made more precise, because this phenomenon brings us back to the realm of linear models. </p>



<p class="justify-text">What all of this has to do with two-layer neural networks? As it happens, this scale factor appears implicit in various situations for these models; let us detail two of them. </p>



<p class="justify-text"><strong>Neural networks with \(1/\sqrt{m}\) scaling.</strong> For two-layer neural networks, lazy training occurs if we define \(h = \frac{1}{\sqrt{m}} \sum_{j=1}^m \Phi(w_j)\) instead of \(h=\frac{1}{m} \sum_{j=1}^m \Phi(w_j)\) before taking the infinite width limit. Indeed:</p>



<ul class="justify-text"><li>This induces a scaling factor \(\alpha = \sqrt{m} \to \infty\) compared to \(1/m\) which, as we have already seen, is the “correct” scaling that leads to a non-degenerate dynamics in parameter space as \(m\) increases. </li><li>Moreover, by the central limit theorem,  \(\frac{1}{\sqrt{m}} \sum_{j=1}^m \Phi(w_j(0)) = O(1)\) for typical random initializations of the parameters. So the initial predictor stays bounded.</li></ul>



<p class="justify-text">To take the Wasserstein gradient flow limit, the step-size has to be of order \(m\) (see previous blog post). So here we should take a step-size of order \(m/\alpha^2 = 1\). With such a step-size, all the conditions for lazy training are gathered when \(m\) is large. Intuitively, each neuron only moves infinitesimally, but they collectively produce a significant movement in predictor space.</p>



<p class="justify-text"><strong>Neural networks with large initialization.</strong> Coming back to our scaling in \(1/m\) and our Wasserstein gradient flow that is obtained in the large width limit, there is another way to enter the lazy regime: by increasing the variance of the initialization. </p>



<p class="justify-text">To see this, assume that \(h\) is a positively \(p\)-homogeneous parametric predictor, which means that \(h(\sigma W)=\sigma^p h(W)\) for all \(\sigma&gt;0\) and some \(p&gt;1\) (remember that this is true with \(p=2\) for our two-layer relu neural network). Take an initialization of the form \(W(0) = \sigma \bar W_0\) where \(\sigma&gt;0\) and \(h(\bar W_0)=0\) (which is also satisfied for our infinite width neural networks with the initialization considered previously). Consider the gradient flow of \(R(h(W))\) with step-size \(\sigma^{2-2p}\).   By defining \(\bar W(t) = W(t)/\sigma\) and using the fact that the differential of a p-homogeneous function <a href="https://en.wikipedia.org/wiki/Homogeneous_function#Positive_homogeneity">is (p-1)-homogeneous</a>, we have, on the one hand $$ \bar W'(t) = -\sigma^{-p} Dh(\bar W(t))^\top \nabla R(\sigma^p h(\bar W(t))), $$ and on the other hand $$\frac{d}{dt} \sigma^p h(\bar W(t)) =\  – Dh(\bar W(t))Dh(\bar W(t))^\top \nabla R(\sigma^p h(\bar W(t))).$$ So in terms of the dynamics \(\bar W(t)\), the situation is exactly equivalent to having a scaling factor \(\alpha=\sigma^p\). This implies that as the magnitude \(\sigma\) of the initialization increases, we enter the lazy regime, provided the step-size is of order \(\sigma^{2-2p}\).</p>



<p class="justify-text"><strong>Neural tangent kernel. </strong>What does the lazy regime tell us about the learnt predictor for two-layer neural networks? Assuming for simplicity that the predictor at initialization is \(0\), this regime amounts to learning a linear model on top of the feature \([(b_j^\top x)_+]_{j=1}^m\) — the derivative with respect to the output weights — concatenated with the feature \([x a_j 1_{b_j^\top x &gt; 0} ]_{j=1}^m\)  — the derivative with respect to the input weights. Compared to training only the output layer, this thus simply adds some features. </p>



<p class="justify-text">Assume for concreteness, that at initialization the hidden weights \(b_j\) are uniform on a sphere of large radius \(\sigma&gt;0\) and the output weights are uniform on \(\{-\kappa\sigma, \kappa\sigma\}\) where \(\kappa\geq 0\). For a large width and a large \(\sigma\), we enter the lazy regime which amounts to learning in a RKHS — let us call it \(\mathcal{F}_{2,\kappa}\) — that is slightly different from \(\mathcal{F}_2 = \mathcal{F}_{2,0}\), since its kernel \(K_\kappa\) contains another term: $$ K_\kappa(x,x’) = \int_{\mathbb{S}^{d-1}} (\theta^\top x)_+ (\theta^\top x’)_+d\tau(\theta) + \kappa^2 \int_{\mathbb{S}^{d-1}} (x^\top x’) 1_{\theta^\top x &gt; 0}1_{\theta^\top x’ &gt; 0}d\tau(\theta). $$</p>



<p class="justify-text">This kernel is called the Neural Tangent Kernel [<a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">17</a>] and the properties of the associated RKHS have been studied in [<a href="https://arxiv.org/pdf/1904.12191.pdf">19</a>, <a href="http://papers.nips.cc/paper/9449-on-the-inductive-bias-of-neural-tangent-kernels.pdf">20</a>], where it is shown to include functions that are slightly less smooth than those of \(\mathcal{F}_2\) when \(\kappa\) increases. This is illustrated in the plot below, obtained by training a wide neural network with \(\sigma\) large (to reach the lazy regime) on the square loss, and various values of \(\kappa\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4213" height="287" src="https://francisbach.com/wp-content/uploads/2020/07/interp-4.png" width="574"/>1-D regression with a wide two-layer relu neural network (gradient descent on square loss, in the lazy regime) with 4 training samples (black dots). At initialization, output weights have \(\kappa\) times the (large) magnitude of the hidden weights. This implicitly solves kernel ridgeless regression for a kernel that depends on \(\kappa\). <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_NTK.jl">[code]</a></figure></div>



<p class="justify-text"><strong>Two implicit biases in one shot.</strong> The attentive reader might have noticed that for large initialization scale \(\sigma\gg 1\), when training both layers on the unregularized exponential loss, two of our analyses apply:  lazy training — that leads to a max-margin predictor in \(\mathcal{F}_{2,\kappa}\) — and the asymptotic implicit bias — that leads to a max-margin predictor in \(\mathcal{F}_{1}\).  So, where is the catch? </p>



<p class="justify-text">There is none! Since the minimizers of this loss are at infinity, the lazy regime is just a transient phase and we will observe both implicit biases along the training dynamics! Take a look at the video below: we observe that in early phases of training, the neurons do not move while learning a smooth classifier — this is the lazy regime and the classifier approaches the \(\mathcal{F}_{2,\kappa}\)-max-margin classifier. In later stages of training, the neurons start moving and the predictor converges to a \(\mathcal{F}_1\)-max-margin classifier as stated by the main theorem. The predictor jitters a little bit during training because I have chosen rather aggressive step-sizes.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large"><img alt="" class="wp-image-4219" src="https://francisbach.com/wp-content/uploads/2020/07/film_lazy2sparse_ns_comp-1.gif"/>Training both layers with gradient descent for the unregularized exponential loss. The only difference with the previous video is that at initialization the variance \(\sigma^2\) is larger and the step-size smaller \(\approx \sigma^{-2}\). First the network learns a classifier in the lazy regime (a kernel max-margin classifier) and eventually converges to the \(\mathcal{F}_1\)-max-margin classifier.</figure></div>



<h2>Discussion</h2>



<p class="justify-text">In this blog post, I described how analyses of the training dynamics can help us understand the properties of the predictor learnt by neural networks even in the absence of an explicit regularization. Already for the simplest algorithm one can think of — gradient descent — we have found a variety of behaviors depending on the loss, the initialization or the step-size. </p>



<p class="justify-text">To achieve this description, the infinite width limit is of great help. It allows to obtain synthetic and precise characterizations of the learnt predictor, that can be used to derive generalization bounds. Yet, there are many interesting non-asymptotic effects caused by having a finite width.  In that sense, we were only concerned with the end of the curve of double descent [<a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">21</a>].</p>



<h2>References</h2>



<p class="justify-text">[1] Lénaïc Chizat, Francis Bach. <a href="https://arxiv.org/pdf/2002.04486.pdf">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss.</a> <em>To appear in Conference On Learning Theory</em>, 2020.<br/>[2] Francis Bach. <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the curse of dimensionality with convex neural networks.</a> <em>The Journal of Machine Learning Research</em>, <em>18</em>(1), 629-681, 2017.<br/>[3]  Vera Kurková, Marcello Sanguineti. <a href="https://www.cs.cas.cz/~vera/publications/journals/I3Edin.pdf">Bounds on rates of variable-basis and neural-network approximation.</a> <em>IEEE Transactions on Information Theory</em>, 47(6):2659-2665, 2001.  <br/>[4] Behnam Neyshabur, Ryota Tomioka, Nathan Srebro. <a href="https://arxiv.org/pdf/1412.6614.pdf">In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning.</a> <em>ICLR (Workshop)</em>. 2015.<br/>[5] Youngmin Cho, Lawrence K. SAUL.  <a href="https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Kernel methods for deep learning.</a> <em>Advances in neural information processing systems</em>. 342-350, 2009.<br/>[6] Radford M. Neal. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf"><em>Bayesian learning for neural networks</em>.</a> Springer Science &amp; Business Media, 2012.<br/>[7] Ali Rahimi, Benjamin Recht. <a href="https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">Random features for large-scale kernel machines.</a> <em>Advances in neural information processing systems</em>. 1177-1184, 2008.<br/>[8] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. <em>The MIT Press</em>, 2012<br/>[9] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro. <a href="http://www.jmlr.org/papers/volume19/18-188/18-188.pdf">The Implicit Bias of Gradient Descent on Separable Data.</a><em> The Journal of Machine Learning Research</em>, <em>19</em>(1), 2822-2878, 2018.<br/>[10] R. Tyrrell Rockafellar, Roger J-B. Wets. <a href="https://www.springer.com/gp/book/9783540627722"><em>Variational analysis</em>.</a> Springer Science &amp; Business Media, 2009.<br/>[11] Suriya Gunasekar,  Jason D. Lee, Daniel Soudry, Nathan Srebro.  <a href="https://par.nsf.gov/servlets/purl/10107856">Characterizing implicit bias in terms of optimization geometry.</a> <em>International Conference on Machine Learning</em>, 2018.<br/>[12] Ziwei Ji, Matus Telgarsky. <a href="https://arxiv.org/pdf/1803.07300.pdf">Risk and parameter convergence of logistic regression.</a> 2018.<br/>[13] Matus Telgarsky. <a href="https://arxiv.org/abs/1303.4172">Margins, Shrinkage, and Boosting.</a> <em>International Conference on Machine Learning</em>, 307-315, 2013.<br/>[14] Sébastien Bubeck. <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity.</a> Foundations and Trends in Machine Learning, 8(3-4):231-357, 2015.<br/>[15] Yuri Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions.</a> <em>Mathematical programming</em>, 103(1):127-152, 2005.<br/>[16] Anrew R. Barron. <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">Universal approximation bounds for superpositions of a sigmoidal function.</a> <em>IEEE Transactions on Information theory. </em>39(3), 930-945, 1993.<br/>[17] Jacot, Arthur, Franck Gabriel, Clément Hongler. <a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">Neural tangent kernel: Convergence and generalization in neural networks.</a> <em>Advances in neural information processing systems.</em> 8571-8580, 2018.<br/>[18] Lénaïc Chizat, Édouard Oyallon, Francis Bach. <a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">On lazy training in differentiable programming.</a> <em>Advances in Neural Information Processing Systems.</em> 2937-2947, 2019.<br/>[19] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, Andrea Montanari. <a href="https://arxiv.org/pdf/1904.12191.pdf">Linearized two-layers neural networks in high dimension.</a> To appear in <em>Annals of Statistics</em>. 2019.<br/>[20] Alberto Bietti, Julien Mairal. <a href="http://papers.nips.cc/paper/9449-on-the-inductive-bias-of-neural-tangent-kernels">On the Inductive Bias of Neural Tangent Kernels</a>. <em>Advances in Neural Information Processing Systems.</em> p. 12893-12904, 2019.<br/>[21] Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal. <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">Reconciling modern machine-learning practice and the classical bias–variance trade-off.</a> <em>Proceedings of the National Academy of Sciences.</em> <em>116</em>(32), 15849-15854, 2019.</p>



<h3>Lower bound on the gradient norm for linear classification with the exponential loss</h3>



<p class="justify-text">In the context of Section 2, we want to prove that \(\Vert \nabla F(a)\Vert_2\geq \gamma\). For this, let \(Z\in \mathbb{R}^{n\times d}\) be the matrix with rows \(y_i x_i\) and let \(\Delta_n\) be the simplex in \(\mathbb{R}^n\). We have by duality $$ \gamma = \max_{\Vert a\Vert_2\leq 1}\min_{p\in \Delta_n} p^\top Z a =   \min_{p\in \Delta_n} \max_{\Vert a\Vert_2\leq 1} a^\top Z^\top p = \min_{p\in \Delta_n} \Vert Z^\top p\Vert_2 .$$  Also, notice that \(\nabla F(a) = Z^\top p\) with \(p_i = \frac{e^{-y_ix_i^\top a}}{\sum_{j=1}^n e^{-y_{j}x_{j}^\top a}}\). Since \(p \in \Delta_n\), we conclude that \(\Vert \nabla F(a)\Vert_2\geq \min_{p\in \Delta_n} \Vert Z^\top p\Vert_2 = \gamma\).</p></div>
    </content>
    <updated>2020-07-13T19:39:11Z</updated>
    <published>2020-07-13T19:39:11Z</published>
    <category term="Machine learning"/>
    <author>
      <name>Lénaïc Chizat</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-07-15T23:47:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7770</id>
    <link href="https://windowsontheory.org/2020/07/13/simons-institute-lectures-on-analysis-of-boolean-functions/" rel="alternate" type="text/html"/>
    <title>Simons institute lectures on analysis of Boolean functions</title>
    <summary>(Does it still make sense to blog such announcements or is these days Twitter the only way to go about this? Asking for a friend 🙂 ) Prasad Raghavendra and Avishay Tal have organized a sequence of 6 lectures on some of the exciting recent advances in analysis of Boolean functions. Lecture Series: Advances in […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>(Does it still make sense to blog such announcements or is these days <a href="https://twitter.com/boazbaraktcs/status/1282443765224017920">Twitter</a> the only way to go about this? Asking for a friend <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/> )</em></p>



<p>Prasad Raghavendra and Avishay Tal have organized a sequence of 6 lectures on some of the exciting recent advances in analysis of Boolean functions. </p>



<p><a href="https://simons.berkeley.edu/events/boolean" rel="noreferrer noopener" target="_blank">Lecture Series: Advances in Boolean Function Analysis</a><br/></p>



<p>The Simons Institute is organizing a series of lectures on Advances in Boolean Function Analysis, that will highlight a few major developments in the area. The series will feature weekly two-hour lectures from July 15th to Aug 18th.  The lectures aim to address both the broad context of the results and their technical details. Though closely related in theme, each lecture will be self-contained.  The schedule is attached below (more info at <a href="https://simons.berkeley.edu/events/boolean" rel="noreferrer noopener" target="_blank">link</a>). </p>



<p>Talks take place on Wednesdays at 10am Pacific time (1pm Eastern). If you can’t catch them live, they will be redcorded.</p>



<p><strong>Zoom Link: </strong><a href="https://berkeley.zoom.us/j/93086371156" rel="noreferrer noopener" target="_blank">https://berkeley.zoom.us/j/93086371156</a><br/></p>



<p><strong><u>Talk Schedule:</u></strong></p>



<p>July 15, Wednesday  10:00am PDT (1pm EDT) <em>Dor Minzer (Institute of Advanced Study)<a href="https://simons.berkeley.edu/events/boolean-1" rel="noreferrer noopener" target="_blank">On the Fourier-Entropy Influence Conjecture</a></em></p>



<p><br/>July 22, Wednesday, 10:00am PDT (1pm EDT) <em>Hao Huang (Emory University) &amp; Avishay Tal (UC Berkeley)<a href="https://simons.berkeley.edu/events/boolean-3" rel="noreferrer noopener" target="_blank">Sensitivity Conjecture and Its Applications</a></em></p>



<p><br/>August 3rd, Monday, 10:00am PDT (1pm EDT)<em>  Shachar Lovett (UC San Diego)<a href="https://simons.berkeley.edu/events/boolean-2" rel="noreferrer noopener" target="_blank">Improved Bounds for the Sunflower Lemma</a></em></p>



<p><br/>August 5, Wednesday, 10:00am PDT (1pm EDT) <em>Ronen Eldan (Weizmann Institute)</em><a href="https://simons.berkeley.edu/events/boolean-4" rel="noreferrer noopener" target="_blank"><em>Concentration on the Boolean Hypercube via Pathwise Stochastic Analysis</em></a></p>



<p><br/>August 12, Wednesday, 10:00am <em>Esty Kelman (Tel Aviv University) <a href="https://simons.berkeley.edu/events/boolean-5" rel="noreferrer noopener" target="_blank">KKL via Random Restrictions</a></em></p>



<p><br/>August 18, Tuesday, 10:00am <em>Pooya Hatami (Ohio State University)<a href="https://simons.berkeley.edu/events/boolean-6" rel="noreferrer noopener" target="_blank">Pseudorandom Generators from Polarizing Random Walks</a></em></p></div>
    </content>
    <updated>2020-07-13T16:18:40Z</updated>
    <published>2020-07-13T16:18:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-07-15T23:45:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3507</id>
    <link href="https://agtb.wordpress.com/2020/07/13/adfocs-2020-market-design-and-computational-fair-division/" rel="alternate" type="text/html"/>
    <title>ADFOCS 2020 (Market Design and Computational Fair Division)</title>
    <summary>Via Pieter Kleer: 21st Max Planck Summer School: Advanced Course on the Foundations of Computer Science (ADFOCS 2020) August 24 – 28, 2020 Saarbruecken, Germany THIS IS A VIRTUAL EVENT http://www.mpi-inf.mpg.de/conference/adfocs ————————————————————————————————— About ADFOCS ADFOCS is an international summer school that has been held annually for the last twenty years at the Max Planck Institute […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Via Pieter Kleer:</p>
<hr/>
<p>21st Max Planck Summer School:<br/>
Advanced Course on the Foundations of Computer Science (ADFOCS 2020)</p>
<p>August 24 – 28, 2020</p>
<p>Saarbruecken, Germany</p>
<p>THIS IS A VIRTUAL EVENT<a href="http://www.mpi-inf.mpg.de/conference/adfocs" rel="noopener noreferrer" target="_blank"/></p><a href="http://www.mpi-inf.mpg.de/conference/adfocs" rel="noopener noreferrer" target="_blank">
</a><p><a href="http://www.mpi-inf.mpg.de/conference/adfocs" rel="noopener noreferrer" target="_blank">http://www.mpi-inf.mpg.de/conference/adfocs</a><br/>
—————————————————————————————————</p>
<p><b>About ADFOCS</b><br/>
ADFOCS is an international summer school that has been held annually for the last twenty years at the Max Planck Institute for Informatics (MPII) in Saarbruecken, Germany. It is organized as part of the activities of the MPII, in particular the International Max Planck Research School (IMPRS), MPII’s graduate program. The purpose of this summer school is to introduce young researchers to topics which are the focus of current research in theoretical computer science. We bring together leading researchers in the field and international participants at the graduate level and above. This year’s focus is on:</p>
<p><b>*** Market Design and Computational Fair Division ***</b><br/>
<b>Program</b><br/>
Our invited speakers give five 60-min lectures with subsequent exercise and discussion sessions. These sessions will take place daily from 14:30 to 18:30 UTC+2 (CEST) in the week of August 24-28. On some days there will be a social event after the regular schedule. This year’s speakers are:</p>
<p>* Nicole Immorlica, Microsoft Research Lab, New York City, USA<br/>
* Jugal Garg and Ruta Mehta, University of Illinois at Urbana-Champaign, USA<br/>
<b>Registration</b><br/>
This year registration is free as the event takes place virtually. Nevertheless, registration is MANDATORY and can be done through the website (at the latest August 10)</p>
<p><b>Contact</b><br/>
The homepage of ADFOCS, including forms for registration, can be found at <a href="http://www.mpi-inf.mpg.de/conference/adfocs" rel="noopener noreferrer" target="_blank">http://www.mpi-inf.mpg.de/conference/adfocs</a></p>
<p>If you have further questions, please do not hesitate to contact the ADFOCS team by sending an email to <a href="mailto:adfocs@mpi-inf.mpg.de" rel="noopener" target="_blank">adfocs@mpi-inf.mpg.de</a></p>
<p>Organizers: Cosmina Croitoru, Sandor Kisfaludi-Bak and Pieter Kleer</p></div>
    </content>
    <updated>2020-07-13T01:25:45Z</updated>
    <published>2020-07-13T01:25:45Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>timroughgarden</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-07-15T23:45:01Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-127536327837647663</id>
    <link href="https://blog.computationalcomplexity.org/feeds/127536327837647663/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/ronald-graham-summary-of-blog-posts-we.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/127536327837647663" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/127536327837647663" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/ronald-graham-summary-of-blog-posts-we.html" rel="alternate" type="text/html"/>
    <title>Ronald Graham: A summary of blog Posts We had about his work</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">To Honor Ronald Graham I summarize the blog posts we had about his work.<br/>
<br/>
1) Blog post <a href="https://blog.computationalcomplexity.org/2016/05/new-ramsey-result-that-will-be-hard-to.html">New Ramsey Result that will be hard to verify but Ronald Graham thinks its right which is good enough for me</a>.<br/>
<br/>
Wikipedia (see <a href="https://en.wikipedia.org/wiki/Boolean_Pythagorean_triples_problem">here</a>) says that in the early 1980's (can't Wikipedia be more precise than that?) Ronald Graham conjectured the following:<br/>
<br/>
For all 2-colorings of N, there exists x,y,z all the same color such that (x,y,z) form a Pythagorean triple.<br/>
<br/>
I cannot imagine he did not also conjecture this to be true for all finite colorings.<br/>
<br/>
I suspect that when he conjectured it, the outcomes thought to be likely were:<br/>
<br/>
a) A purely combinatorial (my spell check says that combinatorial  is not a word. Really? It gets 14,000,000 hits) proof. Perhaps a difficult one. (I think Szemeredi's proof of his density theorem is a rather difficult but purely combinatorial proof).<br/>
<br/>
b) A proof that uses advanced mathematics, like Roth's proof of the k=3 case of Sz-density, or Furstenberg's proof of Sz theorem.<br/>
<br/>
c) The question stays open though with some progress over the years, like R(5).<br/>
<br/>
What actually happened was<br/>
<br/>
d) A SAT Solver solves it AND gets exact bounds:<br/>
<br/>
For all 2-colorings of {1,...,7285} there is a mono Pythag triple.<br/>
<br/>
There exists a 2-coloring of {1,...,7284} with no mono Pythag triple.<br/>
<br/>
I wonder if this would have been guessed as the outcome back in the early 1980's.<br/>
<br/>
-------------------------------------------------------------------------------<br/>
2) Blog Post <a href="https://blog.computationalcomplexity.org/2019/05/ronald-grahams-other-large-number-well.html">Ronald Graham's Other Large Number- well it was large in 1964 anyway</a><br/>
<br/>
Let<br/>
<br/>
a(n) = a(n-1) + a(n-2)<br/>
<br/>
I have not given a(0) and a(1). Does there exists rel prime values of a(0) and a(1) such that for all n, a(n) is composite.<br/>
<br/>
In 1964 Ronald Graham showed yes, though the numbers he found (with the help of 1964-style computing) were<br/>
<br/>
a(0) = 1786772701928802632268715130455793<br/>
<br/>
a(1) = 2059683225053915111058164141686995<br/>
<br/>
I suspect it is open to get smaller numbers, though I do not know.<br/>
<br/>
<br/>
------------------------------------------------------------------------------<br/>
3) Blog Post <a href="https://blog.computationalcomplexity.org/2011/12/solution-to-reciprocals-problem.html">Solution to the reciprocals problem</a><br/>
<br/>
Prove or disprove that there exists 10 natural numbers a,...,j such that<br/>
<br/>
2011= a+ ... + j<br/>
1 = 1/a + ... + 1/j<br/>
<br/>
I had pondered putting this on a HS math competition in 2011; however, the committee thought it was too hard. I blogged on the problem asking for solutions, seeing if there was one that a HS student could have gotten. The following post (this one) gave those solutions. My conclusion is that it could have been put on the competition, but its a close call.<br/>
<br/>
All of the answers submitted had some number repeated.<br/>
<br/>
So I wondered if there was a way to do this with distinct a,...,j.<br/>
<br/>
 I was told about Ronald Grahams result:<br/>
<br/>
For all n at least 78, n can be written as the sum of DISTINCT naturals, where the sum of<br/>
the reciprocals is 1.<br/>
<br/>
This is tight: 77 cannot be so written.<br/>
<br/>
Comment on that blog DID include solutions  to my original problem with all distinct numbers<br/>
<br/>
----------------------------------------------------------------------<br/>
4) Blog Post <a href="https://blog.computationalcomplexity.org/2013/04/a-nice-case-of-interdisciplinary.html">A nice case of interdisplanary research</a> tells the story of how the study of history lead to R(5) being determined (see <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/ramseykings.pdf">here</a> for the actual paper on the subject). One of the main players in the story is the mathematician<br/>
<br/>
Alma Grand-Rho.<br/>
<br/>
Note that this is an anagram of<br/>
<br/>
Ronald Graham.<br/>
<br/>
What is the probability that two mathematicians have names that are anagrams. I suspect very small. However, see <a href="https://blog.computationalcomplexity.org/2013/04/post-mortem-on-april-fools-day-joke.html">this</a> blog post to see why the probability is not as small as it might be.<br/>
<br/>
-----------------------------------------------------------------------<br/>
5) Blog Post <a href="https://blog.computationalcomplexity.org/search?q=Meme">Winner of Ramsey Meme Contest</a> This post didn't mention Ronald Graham; however I think he would have liked it.<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2020-07-12T20:18:00Z</updated>
    <published>2020-07-12T20:18:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-07-15T12:57:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/07/12/graham-pollak-partitions</id>
    <link href="https://11011110.github.io/blog/2020/07/12/graham-pollak-partitions.html" rel="alternate" type="text/html"/>
    <title>Graham–Pollak partitions</title>
    <summary>As you’ve probably already seen, Ron Graham recently died. I first met him many years ago at Xerox PARC; what I remember from that meeting is this old guy easily beating me at ping-pong, and I was startled to learn (while working to beef up his Wikipedia article after his death) that that was exactly Graham’s first impression of Paul Erdős. We’ve chatted about research, most recently in 2018 in Barbados, but somehow never published anything together; on the other hand, Graham’s work in computational geometry, Ramsey theory, and approximation algorithms has certainly had a strong influence on me. Anyway, as part of the project of improving his Wikipedia article, I put together a separate new article on the Graham–Pollak theorem, the theorem that partitioning the edges of an -vertex complete graph into complete bipartite subgraphs requires at least subgraphs. And while doing that, I started to wonder about what the optimal partitions look like, and how many there are.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As you’ve probably already seen, Ron Graham recently died. I first met him many years ago at Xerox PARC; what I remember from that meeting is this old guy easily beating me at ping-pong, and I was startled to learn (while working to beef up <a href="https://en.wikipedia.org/wiki/Ronald_Graham">his Wikipedia article</a> after his death) that that was exactly Graham’s first impression of Paul Erdős. We’ve chatted about research, most recently in <a href="https://www.ics.uci.edu/~eppstein/pix/bellairs18/index.html">2018 in Barbados</a>, but somehow never published anything together; on the other hand, Graham’s work in computational geometry, Ramsey theory, and approximation algorithms has certainly had a strong influence on me. Anyway, as part of the project of improving his Wikipedia article, I put together a separate new article on <a href="https://en.wikipedia.org/wiki/Graham%E2%80%93Pollak_theorem">the Graham–Pollak theorem</a>, the theorem that partitioning the edges of an -vertex complete graph into complete bipartite subgraphs requires at least  subgraphs. And while doing that, I started to wonder about what the optimal partitions look like, and how many there are.</p>

<p>In <em>Proofs from THE BOOK</em>, Aigner and Ziegler describe a simple construction for an -subgraph partition: just order the vertices of the complete graph, and make a star connecting each vertex (except the last) to its later neighbors.
But there are a lot more partitions than that. For instance, you can take any rooted binary tree whose leaves are the vertices of the complete graph, and form a partition in which each complete bipartite subgraph connects the left and right descendants of one of the interior nodes of the tree. The ordered star partition is the special case of this where each internal node has one leaf child.</p>

<p style="text-align: center;"><img alt="Graham&#x2013;Pollak partitions from binary trees" src="https://11011110.github.io/blog/assets/2020/graham-pollak-hierarchy.svg"/></p>

<p>Even these are not the only possibilities. For instance, a four-vertex complete graph can be partitioned into  subgraphs in this triskelion pattern:</p>

<p style="text-align: center;"><img alt="Graham&#x2013;Pollak partitions from binary trees" src="https://11011110.github.io/blog/assets/2020/graham-pollak-triskelion.svg"/></p>

<p>More generally, whenever one has a partition of , one can form a partition of a larger complete graph by partitioning its vertices into  subsets, applying the partition of  to the edges that go from one subset to another, and then recursively partitioning the edges within each subset. This is already enough to show that there is a rapidly growing number of these partitions, but not enough to count them more precisely.</p>

<p>This still leaves many questions. How many Graham–Pollak partitions does  have, as a function of ? How complicated can they be? If we define a state space whose states are Graham–Pollak partitions, and whose state transitions correspond to re-partitioning the subgraph formed by two of the complete bipartite graphs, is it connected? Can a graph traversal of this state space list all the Graham–Pollak partitions faster than a brute force search? What does a random partition look like?</p>

<p>It’s too bad Ron’s no longer around to help answer some of them.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104503441875881282">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-07-12T15:46:00Z</updated>
    <published>2020-07-12T15:46:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-07-12T23:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/104</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/104" rel="alternate" type="text/html"/>
    <title>TR20-104 |  On Counting $t$-Cliques Mod 2 | 

	Oded Goldreich</title>
    <summary>For a constant integer $t$, we consider the problem of counting the number of $t$-cliques $\bmod 2$ in a given graph. 
We show that this problem is not easier than determining whether a given graph contains a $t$-clique, and present a simple worst-case to average-case reduction for it. The reduction runs in linear time when graphs are presented by their adjacency matrices, and average-case is with respect to the uniform distribution over graphs with a given number of vertices.</summary>
    <updated>2020-07-12T15:14:08Z</updated>
    <published>2020-07-12T15:14:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-15T23:44:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/103</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/103" rel="alternate" type="text/html"/>
    <title>TR20-103 |  One-Tape Turing Machine and Branching Program Lower Bounds for MCSP | 

	Mahdi Cheraghchi, 

	Shuichi Hirahara, 

	Dimitrios Myrisiotis, 

	Yuichi Yoshida</title>
    <summary>For a size parameter $s\colon\mathbb{N}\to\mathbb{N}$, the Minimum Circuit Size Problem (denoted by ${\rm MCSP}[s(n)]$) is the problem of deciding whether the minimum circuit size of a given function $f \colon \{0,1\}^n \to \{0,1\}$ (represented by a string of length $N := 2^n$) is at most a threshold $s(n)$. A recent line of work exhibited ``hardness magnification'' phenomena for MCSP: A very weak lower bound for MCSP implies a breakthrough result in complexity theory. For example, McKay, Murray, and Williams (STOC 2019) implicitly showed that, for some constant $\mu_1 &gt; 0$, if ${\rm MCSP}[2^{\mu_1\cdot n}]$ cannot be computed by a one-tape Turing machine (with an additional one-way read-only input tape) running in time $N^{1.01}$, then ${\rm P}\neq{\rm NP}$.
    
    In this paper, we present the following new lower bounds against one-tape Turing machines and branching programs:
    \begin{enumerate}
        \item  A randomized two-sided error one-tape Turing machine (with an additional one-way read-only input tape) cannot compute ${\rm MCSP}[2^{\mu_2\cdot n}]$ in time $N^{1.99}$, for some constant $\mu_2 &gt; \mu_1$.  
        \item A non-deterministic (or parity) branching program of size $o(N^{1.5}/\log N)$ cannot compute MKTP, which is a time-bounded Kolmogorov complexity analogue of MCSP. This is shown by directly applying the Nechiporuk method to MKTP, which previously appeared to be difficult.
    \end{enumerate}
    These results are the first non-trivial lower bounds for MCSP and MKTP against one-tape Turing machines and non-deterministic branching programs, and essentially match the best-known lower bounds for any explicit functions against these computational models.
    
    The first result is based on recent constructions of pseudorandom generators for read-once oblivious branching programs (ROBPs) and combinatorial rectangles (Forbes and Kelley, FOCS 2018; Viola 2019). En route, we obtain several related results:
    \begin{enumerate}
        \item There exists a (local) hitting set generator with seed length $\widetilde{O}(\sqrt{N})$ secure against read-once polynomial-size non-deterministic branching programs on $N$-bit inputs.
        \item Any read-once co-non-deterministic branching program computing MCSP must have size at least $2^{\widetilde{\Omega}(N)}$.
    \end{enumerate}</summary>
    <updated>2020-07-11T05:31:21Z</updated>
    <published>2020-07-11T05:31:21Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-15T23:44:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/</id>
    <link href="https://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/" rel="alternate" type="text/html"/>
    <title>2020 Virtual Telluride Neuromorphic Cognition Engineering Workshop</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 27-31, 2020 Telluride CO (virtual) https://sites.google.com/view/telluride2020/home We are happy to announce a Virtual Telluride Neuromorphic Cognition Engineering Workshop 2020 (https://tellurideneuromorphic.org/) this year in replacement of our usual Workshop in Telluride. The workshop will take place from July 27 to July 31 (8am to 10am PDT, or 17:00 to 19:00 CET). The format will be … <a class="more-link" href="https://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/">Continue reading <span class="screen-reader-text">2020 Virtual Telluride Neuromorphic Cognition Engineering Workshop</span></a></div>
    </summary>
    <updated>2020-07-11T05:05:01Z</updated>
    <published>2020-07-11T05:05:01Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-07-15T23:46:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/07/11/international-conference-on-neuromorphic-systems/</id>
    <link href="https://cstheory-events.org/2020/07/11/international-conference-on-neuromorphic-systems/" rel="alternate" type="text/html"/>
    <title>International Conference on Neuromorphic Systems</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 28-30, 2020 Oak Ridge National Laboratory (virtual) https://icons.ornl.gov ICONS 2020 will be held as a virtual conference. The goal of this conference is to bring together leading researchers in neuromorphic computing to present new research, develop new collaborations, and provide a forum to publish work in this area. Our focus will be on architectures, … <a class="more-link" href="https://cstheory-events.org/2020/07/11/international-conference-on-neuromorphic-systems/">Continue reading <span class="screen-reader-text">International Conference on Neuromorphic Systems</span></a></div>
    </summary>
    <updated>2020-07-11T05:04:39Z</updated>
    <published>2020-07-11T05:04:39Z</published>
    <category term="conference"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-07-15T23:46:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/07/11/neuromorphic-computing-opportunities-challenges-and-perspectives/</id>
    <link href="https://cstheory-events.org/2020/07/11/neuromorphic-computing-opportunities-challenges-and-perspectives/" rel="alternate" type="text/html"/>
    <title>Neuromorphic Computing: Opportunities, Challenges, and Perspectives</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 19, 2020 Virtual https://teuscher-lab.com/dac2020_neuromorphic_workshop/program/ The objective of this workshop is to bring together researchers from multiple disciplines, ranging from physical to biological sciences, to discuss the most promising approaches and overarching goals of neuromorphic computing technologies and paradigms that have the potential to drastically improve conventional approaches. The neuromorphic computing workshop aims to establish … <a class="more-link" href="https://cstheory-events.org/2020/07/11/neuromorphic-computing-opportunities-challenges-and-perspectives/">Continue reading <span class="screen-reader-text">Neuromorphic Computing: Opportunities, Challenges, and Perspectives</span></a></div>
    </summary>
    <updated>2020-07-11T05:04:18Z</updated>
    <published>2020-07-11T05:04:18Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-07-15T23:46:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/Welcome/</id>
    <link href="https://differentialprivacy.org/Welcome/" rel="alternate" type="text/html"/>
    <title>Welcome to differentialprivacy.org</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Hello, welcome to this new website. As you can see, we haven’t got much content yet. But it will come. :-)</p>

<p>We anticipate posting a variety of content, from announcements to mini-surveys of topics in the differential privacy research literature.</p>

<p>Please suggest posts (in the comments below or by contacting the organizers directly). We do hope that members of the community will contribute to this website.</p>

<p>To get things started, here is a definition:</p>

<blockquote>
  <p>A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) is \((\varepsilon,\delta)\)-differentially private if, for all \(x,x’ \in \mathcal{X}^n\) differing on a single entry and all measurable \(E \subseteq \mathcal{Y}\), we have \[\mathbb{P}[M(x) \in E] \le e^\varepsilon \cdot \mathbb{P}[M(x’) \in E]  + \delta.\]</p>
</blockquote></div>
    </summary>
    <updated>2020-07-11T00:00:00Z</updated>
    <published>2020-07-11T00:00:00Z</published>
    <author>
      <name>Thomas Steinke</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-07-15T23:47:23Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-ii/</id>
    <link href="https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-ii/" rel="alternate" type="text/html"/>
    <title>Encrypted Blockchain Databases (Part II)</title>
    <summary>In this second part of the series on Encrypted Blockchain Databases, we are going to describe three schemes to store dynamic encrypted multi-maps (EMMs) on blockchains, each of which achieves different tradeoffs between query, add and delete efficiency. A List-Based Scheme (LSX) Recall that a multi-map is a collection of...</summary>
    <updated>2020-07-10T20:25:00Z</updated>
    <published>2020-07-10T20:25:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-07-15T23:47:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-i/</id>
    <link href="https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-i/" rel="alternate" type="text/html"/>
    <title>Encrypted Blockchain Databases (Part I)</title>
    <summary>Blockchain databases are storage systems that combine properties of both blockchains and databases like decentralization, tamper-resistance, low query latency, and support for complex queries. As they gain wider adoption, concerns over the confidentiality of the data they manage will increase. Already, several projects use blockchains to store sensitive data like...</summary>
    <updated>2020-07-10T20:10:00Z</updated>
    <published>2020-07-10T20:10:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-07-15T23:47:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7764</id>
    <link href="https://windowsontheory.org/2020/07/10/tcs-book-call-for-github-issues/" rel="alternate" type="text/html"/>
    <title>TCS book: Call for GitHub issues</title>
    <summary>I originally planned this summer to finish the work on my Introduction to Theoretical Computer Science book, and in particular write the two missing chapters on space complexity and interactive proof systems. Needless to say, this summer did not go as planned and I won’t be able to write these chapters. However, I still intend […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I originally planned this summer to finish the work on my <a href="https://introtcs.org/">Introduction to Theoretical Computer Science</a> book, and in particular write the two missing chapters on space complexity and interactive proof systems. Needless to say, this summer did not go as planned and I won’t be able to write these chapters. However, I still intend to go over the existing chapters, fixing typos, adding examples, exercises, and generally making it friendlier to beginning undergraduate students. </p>



<p>Toward this end, I would be grateful for people posting bugs, typos, and suggestions as <a href="https://github.com/boazbk/tcs/issues">GitHub issues</a> (I currently have 267 closed and 14 open issues which I hope to get to soon). Of course, if you are technically inclined and there’s a simple local fix, you can also make  a <a href="https://github.com/boazbk/tcs/pulls">pull request</a>.</p>



<p>Aside from these fixes, I am making two more “global” changes to the book. First, I am adding a “non mathy overview” for each chapter. While some students got a lot from reading the book prior to lectures, others were intimidated by the mathematical notation, and so I hope this more gentle introduction will be helpful. I am also adding more examples &amp; solved exercises toward this end. </p>



<p>Another change is that I now follow the more traditional way of presenting deterministic finite automata <em>before </em>Turing machines – DFAs are still optional and can be skipped without missing anything, but some instructors find them as a good introduction to Turing Machines. Thus the order of presentation of materials in the book is roughly as follows:<br/></p>



<ol><li><strong>Introduction, representing objects as strings</strong> –  Representing numbers, lists, etc. Specifying computational tasks as functions mapping binary strings to binary strings,  Cantor’s theorem.</li><li><strong>Finite functions and Boolean circuits</strong> – Every function can be computed by some circuit, circuits as straightline programs, representing circuits as strings, universal circuit evaluator, counting lower bound.</li><li><strong>Computing on unbounded inputs</strong> – DFAs (optional), Turing Machines, equivalence between Turing machines, RAM machines and programming languages, λ calculus (optional), cellular automata (optional)</li><li><strong>Uncomputability</strong> – Universal Turing machine, Halting problem, reductions, Rice’s Theorem. Optional: Gödel’s incompleteness theorem, uncomputability of quantified arithmetic statements, context free grammars.</li><li><strong>Efficient computation</strong> – Modeling running time, time hierarchy theorem,  <strong>P</strong> and <strong>EXP</strong></li><li><strong>NP and NP completeness</strong> – Polynomial-time reductions, Cook-Levin Theorem (using circuits), definition of <strong>NP</strong> using “proof system”/”verifying algorithms” (no non-deterministic TMs), <strong>NP</strong> completeness, consequences of <strong>P</strong>=<strong>NP</strong>: search to decision, optimal machine learning, etc..</li><li><strong>Randomized computation:</strong> Worst-case randomized computation, defining <strong>BPP</strong>,  Sipser-Gács, does <strong>BPP</strong>=<strong>P</strong>? (a little on derandomization)</li><li><strong>Cryptography:</strong> One time pad, necessity of long keys for information theoretic crypto,  pseudorandom generators and stream ciphers, taste of public key and “magic” (ZKP, FHE, MPC)</li><li><strong>Quantum computing:</strong> Some quantum mechanics background – double slit experiment,  Bell’s inequality. Modeling quantum computation. Bird’s eye view of Shor’s algorithm and quantum Fourier transform.</li></ol>



<p/></div>
    </content>
    <updated>2020-07-10T17:29:02Z</updated>
    <published>2020-07-10T17:29:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-07-15T23:45:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17276</id>
    <link href="https://rjlipton.wordpress.com/2020/07/10/ron-graham-1935-2020/" rel="alternate" type="text/html"/>
    <title>Ron Graham, 1935–2020</title>
    <summary>Ron Graham passed away, but he lives on… Cropped from tribute by Tom Leighton Ron Graham just passed away Monday at the age of in La Jolla near UCSD. Today Ken and I wish to say a few words about Ron. Tributes are being written as we write, including this from the Simons Foundation. Here […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Ron Graham passed away, but he lives on…</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/07/grahamjuggling.jpg"><img alt="" class="alignright wp-image-17278" height="128" src="https://rjlipton.files.wordpress.com/2020/07/grahamjuggling.jpg?w=175&amp;h=128" width="175"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="https://securityboulevard.com/2020/07/ronald-graham-and-the-magic-of-math/">tribute</a> by Tom Leighton</font></td>
</tr>
</tbody>
</table>
<p>
Ron Graham just passed away Monday at the age of <img alt="{84}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{84}"/> in La Jolla near UCSD. </p>
<p>
Today Ken and I wish to say a few words about Ron.</p>
<p>
Tributes are being written as we write, including <a href="https://www.simonsfoundation.org/2016/01/11/ronald-graham/">this</a> from the Simons Foundation. Here is the American Mathematical Society <a href="https://www.ams.org/news?news_id=6244">announcement</a>, which we saw first: </p>
<blockquote><p><b> </b> <em> Ron Graham, a leader in discrete mathematics and a former president of both the AMS (1993-1994) and the MAA (2003-2004), died on July 6. He was 84. Graham published more than 350 papers and books with many collaborators, including more than 90 with his wife, Fan Chung, and more than 30 with Paul Erdős. He was known for his infectious enthusiasm, his originality, and his accessibility to anyone who had a mathematics question. </em>
</p></blockquote>
<p/><p>
A <a href="https://www.bradyharanblog.com/blog/the-day-i-met-ron-graham">tribute</a> by Brady Haran embeds several short videos of Ron and his work. Fan’s own <a href="http://www.math.ucsd.edu/~fan/ron/">page</a> for Ron has much more. We have made a collage of images from his life:</p>
<p/><p><br/>
<a href="https://rjlipton.files.wordpress.com/2020/07/rongrahamcollage.jpg"><img alt="" class="aligncenter size-full wp-image-17279" src="https://rjlipton.files.wordpress.com/2020/07/rongrahamcollage.jpg?w=600"/></a></p>
<p/><p><br/>
Ron was special and will be greatly missed by all. We at GLL send our thoughts to his dear wife, Fan. Ken and I knew Ron for many years. Ken knew Ron since a visit to Bell Labs in the 1980s and meeting Fan too at STOC 1990. I knew Ron since I was at Yale in the 1970’s—a long time ago. I recall fondly meeting him for the first time when he was at Bell Labs.</p>
<p>
</p><p/><h2> Some Stories </h2><p/>
<p/><p>
Ken and I thought we would give some personal stories about Graham. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Ken’s story is told <a href="https://rjlipton.wordpress.com/2013/03/28/happy-100th-birthday-paul-erdos/">here</a>. In breaking a confidence by telling Erdős the secret about Bobby Fischer recounted there, Ken hoped that it would spread behind the scenes to enough people that Fischer would be less blamed for failing to play Anatoly Karpov in 1975. Since Erdős was staying with the Grahams, presumably it would have emerged there. The social excursion during STOC 1990 was a dinner cruise in Baltimore’s harbor. Ron and Fan and Ken found each other right away, and some questions to Ken about chess quickly went to the Fischer topic. At least Ken knows the secret was retold at least once. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Ron told me once that he was the accountant for Erdős. One of Ron’s jobs was to keep track of the prize money that Erdős owed. Ron would send out the checks to whoever solved the next problem. One of the brilliant insights of Erdős was to make the problems hard, but at least some where solvable. Ron told me that for years no one would actually cash the checks. They would frame them and proudly display them.</p>
<p/><p>
<a href="https://rjlipton.files.wordpress.com/2020/07/check.png"><img alt="" class="aligncenter wp-image-17280" height="102" src="https://rjlipton.files.wordpress.com/2020/07/check.png?w=220&amp;h=102" width="220"/></a></p>
<p/><p><br/>
Ron said that he liked this for the obvious reason—less cash for Erdős to have to pay. But the advent of color xerox machines in the 1970’s changed this. He told me that people began cashing the checks and displaying the color copy. Bummer.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> My first talk at Bell Labs was on my work on the planar separator theorem—joint work with Bob Tarjan. At the beginning of the talk I saw that Ron had a pile of papers on his desk. He was a manager and I guessed he had some paper work to do. I gave my talk. At the end I when up to Ron in the back and he said:</p>
<blockquote><p><b> </b> <em> I did not get any work done. </em>
</p></blockquote>
<p/><p>
I still fondly remember that as high praise. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Graham loved to do hand stands. I recall walking around Bell Labs one day when out of the blue Ron did a full handstand. He said that he liked to do these on the hand rail of the stairs. The trick he said was: “To not fall down.” </p>
<p/><p><br/>
<a href="https://rjlipton.files.wordpress.com/2020/07/well.jpg"><img alt="" class="aligncenter size-full wp-image-17281" src="https://rjlipton.files.wordpress.com/2020/07/well.jpg?w=600"/></a></p>
<p/><p><br/>
I searched for him doing handstands and found out he and Fan lived in a modern beautiful <a href="http://www.math.ucsd.edu/~fan/home/">house</a>. </p>
<blockquote><p><b> </b> <em> When two mathematicians found a circular home designed by architect Kendrick Bangs Kellogg in La Jolla, they treasured their unique discovery. </em>
</p></blockquote>
<p/><p><br/>
<a href="https://rjlipton.files.wordpress.com/2020/07/home1.jpg"><img alt="" class="aligncenter size-full wp-image-17283" src="https://rjlipton.files.wordpress.com/2020/07/home1.jpg?w=600"/></a></p>
<p>
</p><p/><h2> Fun and Games </h2><p/>
<p/><p>
Ron kept a simply organized <a href="http://www.math.ucsd.edu/~ronspubs/">page</a> of all his papers. They are not sorted by subject or kind, but the titles are so descriptive that you can tell at a glance where the fun is. A number of them are expositions in the popular magazines of the AMS and MAA. </p>
<p>
Among them, we’ll mention this <a href="http://www.math.ucsd.edu/~ronspubs/16_02_insert_and_add.pdf">note</a> from 2016, titled “Inserting Plus Signs and Adding.” It is joint with Steve Butler, who penned his own <a href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html">reminiscence</a> for Lance and Bill’s blog, and Richard Strong. </p>
<p>
Say that a number <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is “reducible” to a number <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> in one step (in base <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/>) if there is a way to insert one or more <img alt="{+}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+}"/> signs into the base-<img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> representation of <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> so that the resulting numbers add up to <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/>. For example, 1935 is reducible to 99 via <img alt="{1 + 93 + 5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+93+%2B+5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + 93 + 5}"/>. The number 99 reduces only to 18 via <img alt="{9+9}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B9%2B9%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{9+9}"/>, and 18 reduces only to 9, which cannot be reduced further. Thus Ron’s birth year took <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/> reduction steps to become a single digit. However, doing <img alt="{1+9+3+5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2B9%2B3%2B5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1+9+3+5}"/> gives 18 straightaway and thus saves a step. The paper gives cases where inserting <img alt="{+}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+}"/> everywhere is <em>not</em> a quickest way to reduce to a single digit.</p>
<blockquote><p><b>Definition 1</b> <em> For any base <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{b}"/> and number <img alt="{n \geq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n \geq 1}"/> denoting an input <b>length</b>, not magnitude, define <img alt="{f_b(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_b%28n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f_b(n)}"/> to be the least integer <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{m}"/> such that all base-<img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{b}"/> numbers of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> can be reduced to a single digit within <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{m}"/> steps. </em>
</p></blockquote>
<p/><p>
The question—of a complexity theoretic nature—is:</p>
<blockquote><p><b> </b> <em> Given <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{b}"/>, what is the growth rate of <img alt="{f_b(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_b%28n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f_b(n)}"/> as <img alt="{n \rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n \rightarrow \infty}"/>? </em>
</p></blockquote>
<p/><p>
Here are some possible answers—which would you expect to be correct in the case where <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> is base 10?</p>
<ul>
<li>
<img alt="{f_{10}(n) = \Theta(\sqrt{n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+%5CTheta%28%5Csqrt%7Bn%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{10}(n) = \Theta(\sqrt{n})}"/>. <p/>
</li><li>
<img alt="{f_{10}(n) = \Theta(n^{1/10})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+%5CTheta%28n%5E%7B1%2F10%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{10}(n) = \Theta(n^{1/10})}"/>. <p/>
</li><li>
<img alt="{f_{10}(n) = O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+O%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{10}(n) = O(\log n)}"/>. <p/>
</li><li>
<img alt="{f_{10}(n) = O(\log\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+O%28%5Clog%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{10}(n) = O(\log\log n)}"/>. <p/>
</li><li>
<img alt="{f_{10}(n) = O(\alpha(n))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+O%28%5Calpha%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{10}(n) = O(\alpha(n))}"/>, where <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> is the inverse Ackermann <a href="https://en.wikipedia.org/wiki/Ackermann_function#Inverse">function</a>.
</li></ul>
<p>
Your expectation might be wrong—see the paper for the answer and its nifty proof. For a warmup, if you want to answer without looking at the paper, prove that the final reduced digit is the same regardless of the sequence of reductions.</p>
<p>
Ron is also known for very big integers, including <a href="https://en.wikipedia.org/wiki/Graham's_number">one</a> that held the record for largest to appear in a published mathematical proof. You can find it among the above tributes and also on a <a href="https://www.zazzle.com/store/grahamsnumber">T-shirt</a>.  We could also mention his role in the largest <a href="https://news.slashdot.org/story/16/05/30/2241225/computer-generates-largest-math-proof-ever-at-200tb-of-data">proof</a> known to date—at 200 terabytes it almost doubles the size of the <a href="http://tb7.chessok.com/">tables</a> for proving results of seven-piece chess endgames.</p>
<p>
If you desire serious fun, look also to Ron’s books. He wrote several, including co-authoring the nonpareil <a href="https://en.wikipedia.org/wiki/Concrete_Mathematics">textbook</a> <em>Concrete Mathematics</em> with Don Knuth and Oren Patashnik.</p>
<p>
</p><p/><h2> Some Prizes </h2><p/>
<p/><p>
Ron, in the tradition famously followed by Erdős, liked to put <a href="https://www.quantamagazine.org/cash-for-math-the-erdos-prizes-live-on-20170605/">money</a> on problems. A $10 dollar problem was much easier than a $100 one. A $1,000 one is extremely hard, and so on. In Ron’s paper on his favorite <a href="http://www.math.ucsd.edu/~ronspubs/20_02_favorite.pdf">problems</a> he stated this one: </p>
<blockquote><p><b> </b> <em> Let <img alt="{H_{n} = \sum_{j=1}^{n} \frac{1}{j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH_%7Bn%7D+%3D+%5Csum_%7Bj%3D1%7D%5E%7Bn%7D+%5Cfrac%7B1%7D%7Bj%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{H_{n} = \sum_{j=1}^{n} \frac{1}{j}}"/>. Challenge: prove the inequality for all <img alt="{n \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n \ge 1}"/>, </em></p><em>
<p align="center"><img alt="\displaystyle  \sum_{d | n} d \le H_{n} + \exp(H_{n})\log(H_{n}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bd+%7C+n%7D+d+%5Cle+H_%7Bn%7D+%2B+%5Cexp%28H_%7Bn%7D%29%5Clog%28H_%7Bn%7D%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{d | n} d \le H_{n} + \exp(H_{n})\log(H_{n}). "/></p>
</em><p><em>	 </em>
</p></blockquote>
<p/><p>
And he put the prize at $1,000,000. He added:</p>
<blockquote><p><b> </b> <em/></p><em>
</em><p><em>
Why is this reward so outrageous? Because this <a href="https://arxiv.org/pdf/math/0008177.pdf">conjecture</a> is equivalent to the Riemann Hypothesis! A single <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> violating would imply there are infinitely many zeroes of the Riemann zeta function off the critical line <img alt="{R(z) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28z%29+%3D+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{R(z) = 1}"/>. Of course, the $1,000,000 prize is not from me but rather is offered by the Clay Mathematics Institute since the Riemann Hypothesis is one of their six remaining Millennium Prize Problems. We hope to live to see progress in the Challenges and Conjectures mentioned in this note, especially the last one! </em>
</p></blockquote>
<p/><p>
Alas Ron did not get to see this resolved. Nor of course did Erdős, nor may any of us. But Ron is prominently mentioned on another Simons <a href="https://www.simonsfoundation.org/2015/12/10/new-erdos-paper-solves-egyptian-fraction-problem/">page</a> where Erdős lives on, and so may Ron.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Ron died at age <img alt="{84}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{84}"/>. Perhaps he liked that it is the sum of a twin prime <img alt="{41 + 43}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B41+%2B+43%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{41 + 43}"/>, and also three times a perfect number. We will always remember <img alt="{84}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{84}"/> because of Ron.  <b>Added 7/10:</b> <img alt="{84}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{84}"/> is also his current h-index <a href="https://scholar.google.com/citations?user=qrPaF3QAAAAJ&amp;hl=en&amp;oi=sra&amp;fbclid=IwAR2Tx1GkRQ6-6K-hlumvpBqWUku2Msea6_dybwrYK8tVeNUuYOD6czZ24ZY">according to</a> Google Scholar.  HT in <a href="https://rjlipton.wordpress.com/2020/07/10/ron-graham-1935-2020/#comment-111482">comment</a>.</p>
<p>
[some word changes, update about h-index]</p></font></font></div>
    </content>
    <updated>2020-07-10T16:08:04Z</updated>
    <published>2020-07-10T16:08:04Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Teaching"/>
    <category term="trick"/>
    <category term="Fan Chung Graham"/>
    <category term="games"/>
    <category term="in memoriam"/>
    <category term="number theory"/>
    <category term="Paul Erdos"/>
    <category term="Ron Graham"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-07-15T23:45:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/102</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/102" rel="alternate" type="text/html"/>
    <title>TR20-102 |  Notes on Hazard-Free Circuits | 

	Stasys Jukna</title>
    <summary>The problem of constructing hazard-free Boolean circuits (those avoiding electronic glitches) dates back to the 1940s and is an important problem in circuit design. Recently, Ikenmeyer et al. [J. ACM, 66:4 (2019), Article 25] have shown that the hazard-free circuit complexity of any Boolean function $f(x)$ is lower-bounded by the monotone circuit complexity of the monotone Boolean function which accepts an input $x$ iff $f(z)=1$ for some vector $z\leq x$. We give a short and amazingly simple proof of this interesting result. We also show that a circuit is hazard-free if and only if the circuit and its dual produce (purely syntactically) all prime implicants of the functions they compute. This extends a classical result of Eichelberger [IBM J. Res. Develop., 9 (1965)] showing this property for depth-two circuits producing no terms containing a variable together with its negation. Finally, we give a very simple non-monotone Boolean function whose hazard-free circuit complexity is super-polynomially larger than its unrestricted circuit complexity.</summary>
    <updated>2020-07-09T19:01:07Z</updated>
    <published>2020-07-09T19:01:07Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-15T23:44:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-713901807945793095</id>
    <link href="https://blog.computationalcomplexity.org/feeds/713901807945793095/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/713901807945793095" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/713901807945793095" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html" rel="alternate" type="text/html"/>
    <title>Reflections on Ronald Graham by Steve Butler</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>
<i>Ronald Graham passed away on July 6 at the age of 84. We present reflections on Ronald Graham by </i><i>Steve Butler.</i></div>
<div>
<i><br/></i></div>
<hr/>
<div>
<br/></div>
<div>
Getting to work with Ron Graham</div>
<div>
<br/></div>
<div>
Ron Graham has helped transform the mathematics community and in particular been a leader in discrete mathematics for more than 50 years. It is impossible to fully appreciate the breadth of his work in one sitting, and I will not try to do so here. Ron has put his papers online and made them <a href="http://www.math.ucsd.edu/~ronspubs/">freely available</a>, a valuable treasure; and there are still many a hidden gem inside of these papers that are waiting to be picked up, polished, and pushed further.</div>
<div>
<br/></div>
<div>
I want to share about how I got to know and work with Ron. To be fair I knew about Ron long before I ever knew Ron. He was that rare pop-star mathematician who had managed to reach out and become visible outside of the mathematical community. And so as a teenager I read about Ron in a book about Erdos. I thought to myself that this guy sounds really cool and someday I might even get to see him give a talk (if I was lucky).</div>
<div>
<br/></div>
<div>
I went to UC San Diego for graduate school and after a series of near-misses ended up studying under Fan Chung. I passed Ron in the stairwell once, and then also helped them move some furniture between their two adjoining homes (graduate students are great for manual labor). But I became determined to try and find a way to start a conversation with Ron and maybe work up to working on a problem. So I took the usual route: I erased the chalkboards for him.</div>
<div>
<br/></div>
<div>
Before his class on discrete mathematics would start, I would come in and clean the chalkboards making them pristine. It also gave me time to occasionally engage in some idle chat, and he mentioned that his papers list was far from complete. I jumped on it and got to work right away and put his papers online and have been maintaining that list for the last fifteen years. This turned out to be no small feat and required about six months of work.  Many papers had no previous online version, and there were even a few papers that Ron had written that he had forgotten about! But this gave me a reason to come to Ron and talk with him about his various papers and then he would mention some problems he was working on with others and where they were stuck and thought I might give them a try.</div>
<div>
<br/></div>
<div>
So I started to work on these problems and started to make progress. And Ron saw what I was able to do and would send me more problems that fit my abilities and interests, and I would come back and show him partial solutions, or computations, and then he would often times fill in the gaps. He was fun to work with, because we almost always made progress; even when we didn't make progress we still understood things more fully. Little by little our publications (and friendship) grew and we now have 25+ joint publications, and one more book that will be coming out in the next few years about the enumerating juggling patterns.</div>
<div>
<br/></div>
<div>
After all of that though, I discovered something. I could have just gone to Ron's door and knocked and he would have talked to me, and given me problems (though our friendship would not become so deep if I had chosen the forthright method). But almost no graduate students in math were brave enough to do it; they were scared off by his reputation. As a consequence, Ron had far fewer math graduate students than you would expect. (To any math graduate student out there, don't let fear stop you from talking with professors; many of them are much nicer than you think, and the ones that are not nice are probably not that great to work with.)</div>
<div>
<br/></div>
<div>
So one of the most important lessons I learned from Ron was the importance of kindness. Ron was generous and kind to everyone (and I really stress the word everyone) that he met. It didn't matter what walk of life you were in, what age you were, or what level of math (if any) that you knew, he was kind and willing to share his time and talents. He always had something in reach in his bag or pocket that he could pull out and show someone and give them an unexpected sense of wonder.</div>
<div>
<br/></div>
<div>
Richard Hamming <a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">once said</a> "you can be a nice guy or you can be a great scientist", the implication being that you cannot do both. Ron showed that you can be a nice guy and a great scientist. And I believe that a significant portion of his success is owed to his being kind; all of us should learn from his examples and show more kindness towards others.</div>
<div>
<br/></div>
<div>
This is only one of many lessons I learned from Ron. Another thing I learned from Ron is the importance of data. I have seen multiple times when we would work on a problem and generate data resulting in what I thought were hopeless numbers to understand. But Ron looked at that same data and with a short bit of trial and error was able to make a guess of what the general form was. And almost inevitably he would be right! One way that Ron could do this was to start by factoring the values, and if all the prime factors were small he could guess that the expression was some combination of factorials and powers and then start to play with expressions until things worked out. Even when I knew what he did, I still am amazed that he was able to do it.</div>
<div>
<br/></div>
<div>
I will miss Ron, I will never have a collaboration as deep, as meaningful, and as personal. I am better for having worked with him, and learning from him about how to be a better mathematician and a better person.</div>
<div>
<br/></div>
<div>
Thank you, Ron.</div></div>
    </content>
    <updated>2020-07-09T15:57:00Z</updated>
    <published>2020-07-09T15:57:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-07-15T12:57:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1361</id>
    <link href="https://ptreview.sublinear.info/?p=1361" rel="alternate" type="text/html"/>
    <title>Policy on reporting papers</title>
    <summary>While we at PTReview always look through the posted papers, we do not check for correctness. We make a serious attempt to make sure the paper is reasonable. In a few instances, we have decided not to post a (topically relevant) paper, because it looks absolutely wrong. Our position is: the benefit of doubt goes […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>While we at PTReview always look through the posted papers, we do not check for correctness. We make a serious attempt to make sure the paper is reasonable. In a few instances, we have decided not to post a (topically relevant) paper, because it looks absolutely wrong. Our position is: the benefit of doubt goes to the author, and a borderline paper should be posted. We are only curating relevant tech reports, not passing judgment on results. </p>



<p>In some borderline cases, readers familiar with the subject complained to us that the paper should be not be considered a scientific contribution (because of, say, unspecified algorithms, blatantly incorrect or unverifiable central claims). These are cases where we were also unsure of the paper. We have usually removed/not posted such papers.</p>



<p><strong>If the paper author(s) feels that his/her paper should nonetheless be posted, then they should email us at little.oh.of.n@gmail.com.</strong> As long as the paper is not complete nonsense and appears to cite relevant history, we will defer to the authors’ wishes.</p></div>
    </content>
    <updated>2020-07-09T00:38:05Z</updated>
    <published>2020-07-09T00:38:05Z</published>
    <category term="Announcement"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-07-15T23:46:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4399</id>
    <link href="https://lucatrevisan.wordpress.com/2020/07/08/silver-linings/" rel="alternate" type="text/html"/>
    <title>Silver linings</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">To put it mildly, 2020 is not shaping up to be a great year, so it is worthwhile to emphasize the good news, wherever we may find them. Karlin, Klein, and Oveis Gharan have just posted a paper in which, … <a href="https://lucatrevisan.wordpress.com/2020/07/08/silver-linings/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>To put it mildly, 2020 is not shaping up to be a great year, so it is worthwhile to emphasize the good news, wherever we may find them.</p>
<p>Karlin, Klein, and Oveis Gharan have just <a href="https://arxiv.org/abs/2007.01409">posted a paper</a> in which, at long last, they improve over the 1.5 approximation ratio for metric TSP which was achieved, in 1974, by Christofides. For a long time, it was suspected that the Held-Karp relaxation of metric TSP had an approximation ratio better than 1.5, but there was no viable approach to prove such a result. In 2011, two different approaches were developed to improve 1.5 in the case of shortest-path metrics on unweighted graphs: one by Oveis Gharan, Saberi and Singh and one by Momke and Svensson. The algorithm of Karlin, Klein and Oveis Gharan (which does not establish that the Held-Karp relaxation has an integrality gap better than 1.5) takes as a starting point ideas from the work of Oveis Gharan, Saberi and Singh. </p>
<p><span id="more-4399"/></p>
<p>Yesterday, Bloom and Sisask <a href="https://arxiv.org/abs/2007.03528">posted a paper</a> in which they show that there is a constant <img alt="c&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=c%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c&gt;0"/> such that, for every sufficiently large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/>, if <img alt="A \subseteq \{1,\ldots, N \}" class="latex" src="https://s0.wp.com/latex.php?latex=A+%5Csubseteq+%5C%7B1%2C%5Cldots%2C+N+%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A \subseteq \{1,\ldots, N \}"/> has cardinality at least <img alt="N / (\log N)^{1+c}" class="latex" src="https://s0.wp.com/latex.php?latex=N+%2F+%28%5Clog+N%29%5E%7B1%2Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N / (\log N)^{1+c}"/>, then <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> contains a non-trivial length-3 arithmetic progression. Without context, this may seem like a strange result to get excited about, but it sits at the nexus of a number of fundamental results and open questions in combinatorics. Gil Kalai <a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">has written an excellent post</a> telling the story of this problem, so instead of writing a worse version of it I will refer the reader to Gil’s blog.</p>
<p>Back to bad news, the day after Harvard announced that it would deliver courses online only in 2020-21, the Trump administration announced that it would void student visas of students who are not attending in-person classes in 2020-21. Back to good news, Harvard and MIT announced that they will sue the federal government over this, and other universities, including the University of California system, are planning similar responses. Apart from the action, I was really heartened to read MIT’s President <a href="http://news.mit.edu/2020/mit-and-harvard-file-suit-against-new-ice-regulations-0708">statement on the matter</a> (thanks to Vinod Vaikuntanathan for bringing it my attention) which is worth reproducing:</p>
<blockquote><p>
To the members of the MIT community,</p>
<p>On Monday, in a surprising development, a division of Immigration and Customs Enforcement announced that it will not permit international students on F-1 visas to take a full online course load this fall while studying in the United States. As I wrote yesterday, this ruling has potentially serious implications for MIT’s international students and those enrolled at institutions across the country.</p>
<p>This morning, in response, MIT and Harvard jointly filed suit against ICE and the US Department of Homeland Security in federal court in Massachusetts. In the lawsuit, we ask the court to prevent ICE and DHS from enforcing the new guidance and to declare it unlawful.</p>
<p>The announcement disrupts our international students’ lives and jeopardizes their academic and research pursuits. ICE is unable to offer the most basic answers about how its policy will be interpreted or implemented. And the guidance comes after many US colleges and universities either released or are readying their final decisions for the fall – decisions designed to advance their educational mission and protect the health and safety of their communities.</p>
<p>Our international students now have many questions – about their visas, their health, their families and their ability to continue working toward an MIT degree. Unspoken, but unmistakable, is one more question: Am I welcome?</p>
<p>At MIT, the answer, unequivocally, is yes.</p>
<p>MIT’s strength is its people – no matter where they come from. I know firsthand the anxiety of arriving in this country as a student, excited to advance my education, but separated from my family by thousands of miles. I also know that welcoming the world’s brightest, most talented and motivated students is an essential American strength.</p>
<p>While we pursue legal protections for our international students, we will continue to stay in close touch with them through email and updates on the International Students Office’s website. If you have questions, you may write to the ISO at iso-help@mit.edu.</p>
<p>Sincerely,</p>
<p>L. Rafael Reif
</p></blockquote>
<p>This way of talking like a human being, and like you actually care about the matter at hand, is a big contrast with the robotic statements that usually come out of campus leadership. The corresponding message from UC Berkeley’s Chancellor is the way such statements usually are like:</p>
<blockquote><p>
Dear campus community,</p>
<p>Yesterday, the Department of Homeland Security issued new guidance to universities related to international students and fall instruction requirements. The guidance is deeply concerning: it could potentially force the return of many international students to their home countries if they are unable to find the appropriate balance of in-person and remote classes. These requirements run counter to our values of being an inclusive community and one that has a long tradition of welcoming international students from around the globe. International students enrich campus life immeasurably, through their participation in classes, research collaborations and extracurricular activities.</p>
<p>We will explore all of our options, legal and otherwise, to counter the deleterious effects of these policies that imp act the ability for international students to achieve their academic goals. It is not only important for UC Berkeley but for all of higher education across the U.S. to take every step possible to mitigate these policies that send a message of exclusion to our international community of scholars. We will partner with our professional associations to advocate for sound legislation that continues to support international educational exchange.</p>
<p>More immediately, we are working with colleagues across our campus to identify a path that will allow us to comply with these requirements while ensuring a healthy learning environment, and paying attention to the needs of our international students. We recognize the concern and anxiety these new rules have created, and we are moving quickly to ensure that we offer the proper balance of online and in-person classes so that our students can remain in the U.S. and satisfy their visa requirements, and that those students residing outside the U.S. can maintain their enrollment status.</p>
<p>We expect to announce more details soon. Should you have any questions, please contact the Berkeley International Office at internationaloffice@berkeley.edu.</p>
<p>Sincerely,</p>
<p>Carol Christ<br/>
Chancellor</p>
<p>Lisa Alvarez-Cohen<br/>
Vice Provost for Academic Planning and Senior International Officer
</p></blockquote>
<p>It is interesting to think about where this difference in tone is coming from. Carol Christ is a renown humanities scholar who, I suppose, writes well. She comes across as charismatic and caring, and she is definitely straight-talking in person. Probably, as for everything else, Berkeley has a byzantine process to create announcements and press releases, and if Stephen Colbert was the Chancellor of UC Berkeley, after a couple of weeks on the job he would sound just as <i>deeply concerned</i> and just as into <i>exploring all options</i>, while meanwhile <i>working to identify a path</i> and <i>paying attention</i> about something that is totally fucked up and needs action <i>today</i>.</p>
<p>Which brings me to all the statements in support of Black Lives Matter that have been coming out of every scholarly institution in the last few days. While their messages are generally unobjectionable, there is a certain sameness to their form (“we say their names…”, “we will do the work…”, “we see you…”) and they don’t sound at all like the way the people putting them out speak. This has complicated causes, including the fact that many such statements came out of letter-writing campaigns that demanded statements in a very specific way, without leaving a lot of room for individual expression. The association of American Poets, for example, put out a statement of solidarity with the Black community; in response, a <a href="https://www.nytimes.com/2020/06/09/books/poetry-foundation-black-lives-matter.html">letter with 1800 signatories</a> claimed that it was too weak a statement and that it was, in fact, itself an act of violence against Black people; several resignations followed. The Board of the National Book Critics Circle was working on such a statement, and the work devolved into acrimony and several rounds of “I am outraged and I resign,” “no <i>I</i> am outraged at your outrage and <i>I</i> resign, “well then <i>I</i> am outraged that you are outraged at her outrage” until almost the whole board was gone in a “<a href="https://www.vulture.com/2020/06/national-book-critics-circle-resignations.html">sequence of events [that] was bizarre and bloody in an end-of-a-Tarantino-movie way</a>.”</p>
<p>Also, people in America talk about race the way UC Berkeley administrators talk about anything, that is extremely carefully and vacuously. But, back to the statements about foreign students, the difference between the administrative cultures at MIT and Berkeley is not the only difference between the statements of Reif and Christ: clearly a big difference is that Reif is an immigrant himself. When Trayvon Martin was killed, Obama talked about the killing in a way that was very different, and much more meaningful, than other politicians: if I had a son, Obama said, he would look a lot like Trayvon. If there were more people of color in positions of academic leadership, I think that we would have seen an academic response to Black Lives Matter that would have been less fearful, dogmatic and robotic and more meaningful and productive. Or perhaps we would have all ended up like the National Book Critic Circle, it’s hard to say.</p></div>
    </content>
    <updated>2020-07-08T21:11:06Z</updated>
    <published>2020-07-08T21:11:06Z</published>
    <category term="Berkeley"/>
    <category term="philosophy"/>
    <category term="things that are excellent"/>
    <category term="things that are terrible"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-07-15T23:20:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=14203</id>
    <link href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 7: Bloom and Sisask just broke the logarithm barrier for Roth’s theorem!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Thomas Bloom and Olof Sisask: Breaking the logarithmic barrier in Roth’s theorem on arithmetic progressions,    arXiv:200703528   Once again Extraordinary news regarding Roth Theorem! (I thank Ryan Alweiss for telling me about it and Rahul Santhanam for telling me … <a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><img alt="rothnew" class="alignnone size-full wp-image-19954" src="https://gilkalai.files.wordpress.com/2020/07/rothnew.png?w=640"/></p>
<h3 class="title mathjax">Thomas Bloom and Olof Sisask: <a href="https://arxiv.org/abs/2007.03528">Breaking the logarithmic barrier in Roth’s theorem on arithmetic progressions,    arXiv:200703528</a></h3>
<p> </p>
<p>Once again Extraordinary news regarding Roth Theorem! (I thank Ryan <span class="qu"><span class="gD">Alweiss for telling me about it and </span></span>Rahul Santhanam for telling me about Thomas and Olof’s earlier attempts.)</p>
<p>Suppose that <img alt="R_n" class="latex" src="https://s0.wp.com/latex.php?latex=R_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R_n"/>  is a subset of <img alt="\{1,2,\dots, n \}" src="http://l.wordpress.com/latex.php?latex=%5C%7B1%2C2%2C%5Cdots%2C+n+%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\{1,2,\dots, n \}"/> of maximum cardinality not containing an arithmetic progression of length 3. Let <img alt="r_3(n)=|R_n|" class="latex" src="https://s0.wp.com/latex.php?latex=r_3%28n%29%3D%7CR_n%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r_3(n)=|R_n|"/>. Roth proved that <img alt="r_3(n)=o(n)" class="latex" src="https://s0.wp.com/latex.php?latex=r_3%28n%29%3Do%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r_3(n)=o(n)"/>.</p>
<p>A few days ago Thomas Bloom and Olof Sisask proved that for some <img alt="c&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=c%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c&gt;0"/></p>
<p style="text-align: center;"><img alt="r_3(n) \le \frac {n}{\log^{1+c} n}" class="latex" src="https://s0.wp.com/latex.php?latex=r_3%28n%29+%5Cle+%5Cfrac+%7Bn%7D%7B%5Clog%5E%7B1%2Bc%7D+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r_3(n) \le \frac {n}{\log^{1+c} n}"/></p>
<p>This is an extraordinary result!!! I will tell you a little more about it below.</p>
<h2>Ron Graham</h2>
<p>I just heard yesterday the sad news that <a href="https://en.wikipedia.org/wiki/Ronald_Graham">Ron Graham</a> passed away. Ron was an extraordinary mathematician and an extraordinary person. I first met Ron in Montreal in 1978 and we met many times since then. Ron will be dearly missed.</p>
<h3>Back to the new bounds on Roth’s theorem</h3>
<p>From an abstract of a lecture by Thomas and Olof: “This is the integer analogue of a result of Bateman and Katz for the model setting of vector spaces over a finite field, and the proof follows a similar structure.”</p>
<p>A catchy (weaker) formulation which goes back to Erdos and Turan is:</p>
<p>Let <img alt="a_n" class="latex" src="https://s0.wp.com/latex.php?latex=a_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_n"/> be a sequence of integers so that <img alt="\sum \frac{1}{a_n} = \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+%5Cfrac%7B1%7D%7Ba_n%7D+%3D+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum \frac{1}{a_n} = \infty"/>, then the sequence contains an arithmetic progression of length three!!</p>
<p>Bloom and Sisask’s result implies, of course, Van der Korput’s result that the primes contain infinitely many 3-terms arithmetic progression as well as Green’s 2005 result asserting it for every  dense subset of primes.</p>
<p>Szemeredi’s celabrated result extended Roth’s theorem to arithmetic progression of any fixed size, and Green-Tao celebrated 2008 result asserts that the primes (or a dense subsets of primes) contain arithmetic progression of any length. (The case of 3-term AP is so far much simpler for all the results mentioned below.)</p>
<p> </p>
<p>A little more about the history of the problem below the fold</p>
<p><span id="more-14203"/></p>
<h2>Roth, Szemeredi, Heath-Brown, and Bourgain; Salem-Spencer and Behrend.</h2>
<p>Let’s wrire $r_3(n)=n/g(n)$. Roth proved that <img alt="g(n) \ge \log\log n" class="latex" src="https://s0.wp.com/latex.php?latex=g%28n%29+%5Cge+%5Clog%5Clog+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(n) \ge \log\log n"/>. Szemeredi and Heath-Brown improved it to <img alt="g(n) \ge \log^c n" class="latex" src="https://s0.wp.com/latex.php?latex=g%28n%29+%5Cge+%5Clog%5Ec+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(n) \ge \log^c n"/> for some <img/>$latex c&gt;0$ (Szemeredi’s argument gave <img alt="c=1/4" src="http://l.wordpress.com/latex.php?latex=c%3D1%2F4&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c=1/4"/>.) Jean Bourgain improved the bound in 1999 to <img alt="c=1/2" class="latex" src="https://s0.wp.com/latex.php?latex=c%3D1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c=1/2"/> and in 2008 to <img alt="c=3/4" class="latex" src="https://s0.wp.com/latex.php?latex=c%3D3%2F4&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c=3/4"/> (up to lower order terms).</p>
<p>Erdös and Turan who posed the problem in 1936 described a set not containing an arithmetic progression of size <img alt="n^c" src="http://l.wordpress.com/latex.php?latex=n%5Ec&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="n^c"/>.  Salem and Spencer improved this bound to <img alt="g(n) \le e^{logn/ loglogn}" src="http://l.wordpress.com/latex.php?latex=g%28n%29+%5Cle+e%5E%7Blogn%2F+loglogn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="g(n) \le e^{logn/ loglogn}"/>. Behrend’s upper bound from 1946 is of the form <img alt="g(n) \le e^{C\sqrt {\log n}}" src="http://l.wordpress.com/latex.php?latex=g%28n%29+%5Cle+e%5E%7BC%5Csqrt+%7B%5Clog+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="g(n) \le e^{C\sqrt {\log n}}"/>. A small improvement was achieved  by Elkin and is discussed <a href="https://gilkalai.wordpress.com/2008/07/10/pushing-behrend-around/" rel="noopener" target="_blank" title="Elkin's result">here</a>.  (Look also at the remarks following that post.)</p>
<h2>Sanders</h2>
<p>In 2010 Tom Sanders was able to refine Bourgain’s argument and proved that <img alt="g(n) \ge (\log n)^{3/4}" class="latex" src="https://s0.wp.com/latex.php?latex=g%28n%29+%5Cge+%28%5Clog+n%29%5E%7B3%2F4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(n) \ge (\log n)^{3/4}"/>. A few month later  <a href="http://arxiv.org/abs/1011.0104">Tom have managed to reach the logarithmic barrier and to prove </a>that</p>
<p><img alt="g(n) \ge (\log n)/(\log \log n)^{6}." class="latex" src="https://s0.wp.com/latex.php?latex=g%28n%29+%5Cge+%28%5Clog+n%29%2F%28%5Clog+%5Clog+n%29%5E%7B6%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(n) \ge (\log n)/(\log \log n)^{6}."/></p>
<p>We reported about this outstanding achievement in <a href="https://gilkalai.wordpress.com/2010/11/24/roths-theorem-sanders-reaches-the-logarithmic-barrier/">this blog post</a> and quoted from his paper: “There are two main new ingredients in the present work: the first is a way of transforming sumsets introduced by <a href="http://front.math.ucdavis.edu/0802.4371">Nets Katz and Paul Koester</a> in 2008, and the second is a result on the <img alt="L_p" class="latex" src="https://s0.wp.com/latex.php?latex=L_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_p"/>-invariance of convolutions due to <a href="http://front.math.ucdavis.edu/1003.2978">Ernie Croot and Olof Sisask</a> (2010).”</p>
<p>The exponent 6 for the loglog term was improved to 4 by Thomas Bloom and recently to 3 by Thomas Schoen in his paper: <a href="https://arxiv.org/abs/2005.01145">Improved bound in Roth’s theorem on arithmetic progressions.</a> Schoen uses ingredients from Bateman and Katz’s work (see below).</p>
<h2>Cap sets  – Meshulam</h2>
<p>A closely related problem  in <img alt="\Gamma=" src="http://l.wordpress.com/latex.php?latex=%5CGamma%3D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Gamma="/><img alt="\{0,1,2\}^n" src="http://l.wordpress.com/latex.php?latex=%5C%7B0%2C1%2C2%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\{0,1,2\}^n"/>. It is called the <a href="http://terrytao.wordpress.com/2007/02/23/open-question-best-bounds-for-cap-sets/" rel="noopener" target="_blank" title="Cap sets at Tao">cap set problem</a>. A subset of <img alt="\Gamma" src="http://l.wordpress.com/latex.php?latex=%5CGamma&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Gamma"/> is called a cap set if it contains no arithmetic progression of size three or, alternatively, no three vectors that sum up to 0(modulo 3). If $latex <img alt="A" src="http://l.wordpress.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="A"/>_n$  is a cap set of maximum size in <img alt="\Gamma" class="latex" src="https://s0.wp.com/latex.php?latex=%5CGamma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Gamma"/> we can ask how the function <img alt="f(n)=|A_n|" class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29%3D%7CA_n%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(n)=|A_n|"/> behaves. In 1995 Roy Meshulam proved, using Roth’s argument, that <img alt="f(n) \le 3^n/n" class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29+%5Cle+3%5En%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(n) \le 3^n/n"/> . Edell found an example of a cap set of size <img alt="2.2^n" src="http://l.wordpress.com/latex.php?latex=2.2%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="2.2^n"/>.  Again the gap is exponential.  What is the truth? Improving Meshulam’s result may be closely related to crossing the <img alt="\log n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\log n"/> barrier for Roth’s theorem. In 2007 Tom Sanders  <a href="http://arxiv.org/abs/0807.5101">managed to achieve it </a>, not for the cup problem, but for a related problem over Z/4Z.</p>
<h2>Bateman and Katz</h2>
<p>In 2011, <a href="http://front.math.ucdavis.edu/1101.5851" rel="noopener" target="_blank">Michael Bateman and Nets Katz</a> improved, after many years of attempts by many, the Roth-Meshulam bound.  They proved using Fourier methods that <img alt="f(n) \le 3^n/n^{1+c}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29+%5Cle+3%5En%2Fn%5E%7B1%2Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(n) \le 3^n/n^{1+c}"/> for some <img alt="c&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=c%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c&gt;0"/>! This was very exciting.   See these two posts on Gowers’s blog (<a href="http://gowers.wordpress.com/2011/01/11/what-is-difficult-about-the-cap-set-problem/" rel="noopener" target="_blank">I</a>,<a href="http://gowers.wordpress.com/2011/01/18/more-on-the-cap-set-problem/" rel="noopener" target="_blank">II</a>). This raised the question if the new method allows breaking the  logarithmic barrier for Roth’s theorem.</p>
<h3>Polymath 6</h3>
<p>Tim Gowers <a href="https://gowers.wordpress.com/2011/02/05/polymath6-a-is-to-b-as-c-is-to/">proposed in 2011 polymath6 </a> to try to break the logarithmic barrier for Roth based on the Bateman-Katz breakthrough. (Here is<a href="http://michaelnielsen.org/polymath1/index.php?title=Improving_the_bounds_for_Roth%27s_theorem"> the wiki</a>; and a <a href="https://polymathprojects.org/2011/02/05/polymath6-improving-the-bounds-for-roths-theorem/">related post by Sanders</a>, and a <a href="https://polymathprojects.files.wordpress.com/2011/02/polymath-3.pdf">document by Katz</a>) This project did not get off the ground. We can regard the news as giving support that the polymath6 project was timely and of an appropriate level, and also as giving some support to an advantage of the conventional way of doing mathematics compared to the polymath way.</p>
<h2> Croot-Lev-Pach-Ellenberg-Gijswijt solution to the Cap set problem via the polynomial method</h2>
<p>Next and quite recently came a startling development  – the Croot-Lev-Pach-Ellenberg-Gijswijt capset bound <img alt="f(n) \le 2.756^n" class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29+%5Cle+2.756%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(n) \le 2.756^n"/>. (Croot, Lev, and Pach gave an exponential improvement for the Z/4Z case (see <a href="https://gilkalai.wordpress.com/2016/05/10/math-from-facebook/">this post</a>) and a few weeks later Ellenberg and Gijswijt used the method for the Z/3Z case (see <a href="https://gilkalai.wordpress.com/2016/05/15/mind-boggling-following-the-work-of-croot-lev-and-pach-jordan-ellenberg-settled-the-cap-set-problem/">this post</a>).)</p>
<p>A natural question that many people asked was how this development relates to improving the bounds for Roth perhaps even towards the Behrend bound. We discussed it a little over here and in other places. This is still an interesting possibility.</p>
<h2>The new result: Bloom and Sisask</h2>
<p>However, just a <del>few months</del> few years after the Bateman-Katz result have become obsolete for the cap-set problem, the Bateman-Katz method prevailed in this wonderful breakthrough of Bloom and Sisask giving <img alt="g(n) \ge \log^c n" class="latex" src="https://s0.wp.com/latex.php?latex=g%28n%29+%5Cge+%5Clog%5Ec+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(n) \ge \log^c n"/> for <img alt="c&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=c%3E1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c&gt;1"/>.</p>
<h3 style="text-align: center;"><span style="color: #ff0000;">Congratulations!!!</span></h3>
<h2>An old post and poll</h2>
<p>In <a href="https://gilkalai.wordpress.com/2009/03/25/an-open-discussion-and-polls-around-roths-theorem/">an old post we asked</a>: “How does <img alt="r_3(n)" class="latex" src="https://s0.wp.com/latex.php?latex=r_3%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r_3(n)"/> behave? Since we do not really know, will it help talking about it? Can we somehow look beyond the horizon and try to guess what the truth is? (I still don’t know if softly discussing this or other mathematical problems is a fruitful idea, but it can be enjoyable.)” We even had a poll collecting people’s predictions about <img alt="r_3(n)" class="latex" src="https://s0.wp.com/latex.php?latex=r_3%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r_3(n)"/>.  Somewhat surprisingly 18.18% of answerers predicted that <img alt="r_3(n)" class="latex" src="https://s0.wp.com/latex.php?latex=r_3%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r_3(n)"/> behaves like <img alt="\frac{1}{(\log n)^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%28%5Clog+n%29%5Ec%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{(\log n)^c}"/> for some <img alt="c&lt;1" class="latex" src="https://s0.wp.com/latex.php?latex=c%3C1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c&lt;1"/>.</p>
<p> </p></div>
    </content>
    <updated>2020-07-08T16:00:41Z</updated>
    <published>2020-07-08T16:00:41Z</published>
    <category term="Algebra"/>
    <category term="Combinatorics"/>
    <category term="Updates"/>
    <category term="Olef Sisask"/>
    <category term="Thomas Bloom"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-07-15T23:44:58Z</updated>
    </source>
  </entry>
</feed>
