<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-10-21T20:21:50Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/21/tenure-track-faculty-at-portland-state-university-apply-by-december-31-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/21/tenure-track-faculty-at-portland-state-university-apply-by-december-31-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track faculty at Portland State University (apply by December 31, 2020)</title>
    <summary>The Department of Computer Science at Portland State University invites applications for several Assistant Professor positions. Exceptional candidates will also be considered for appointment at the rank of Associate Professor. Candidates in all areas of Computer Science will be considered including theoretical computer science. Website: https://www.pdx.edu/computer-science/open-faculty-position Email: cssearch@pdx.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at Portland State University invites applications for several Assistant Professor positions. Exceptional candidates will also be considered for appointment at the rank of<br/>
Associate Professor. Candidates in all areas of Computer Science will be considered including theoretical computer science.</p>
<p>Website: <a href="https://www.pdx.edu/computer-science/open-faculty-position">https://www.pdx.edu/computer-science/open-faculty-position</a><br/>
Email: cssearch@pdx.edu</p></div>
    </content>
    <updated>2020-10-21T16:42:54Z</updated>
    <published>2020-10-21T16:42:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-21T20:20:56Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/reconstruction-theory/</id>
    <link href="https://differentialprivacy.org/reconstruction-theory/" rel="alternate" type="text/html"/>
    <title>The Theory of Reconstruction Attacks</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We often see people asking whether or not differential privacy might be overkill.  Why do we need strong privacy protections like differential privacy when we’re only releasing approximate, aggregate statistical information about a dataset?  Is it really possible to extract information about specific users from releasing these statistics?  The answer turns out to be a resounding yes!  The textbook by Dwork and Roth <a href="https://www.cis.upenn.edu/~aaroth/privacybook.html">[DR14]</a> calls this phenomenon the Fundamental Law of Information Recovery:</p>

<blockquote>
  <p>Giving overly accurate answers to too many questions will inevitably destroy privacy.</p>
</blockquote>

<p>So what exactly does this fundamental law mean precisely, and how can we prove it?  We can formalize and prove the law via <em>reconstruction attacks</em>, where an attacker can recover secret information from nearly every user in the dataset, simply by observing noisy answers to a modestly large number of (surprisingly simple) queries on the dataset. Reconstruction attacks were introduced in a seminal paper by Dinur and Nissim in 2003 <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>.  Although this paper predates differential privacy by a few years, the discovery of reconstruction attacks directly led to the definition of differential privacy, and shaped a lot of the early research on the topic. We now know that differentially private algorithms can, in some cases, match the limitations on accuracy implied by reconstruction attacks. When this is the case, we have a remarkably sharp transition from a blatant privacy violation when the accuracy is high enough to enable a reconstruction attack, to the strong protection given by differential privacy at the cost of only slightly lower accuracy.</p>

<p>Aside from the theoretical importance of reconstruction attacks, one may wonder if they can be carried out in practice, or if the attack model is unrealistic and can be avoided with some simple workarounds?  In this series of posts, we argue that reconstruction attacks can be quite practical.  In particular, we describe successful attacks by some of this post’s authors on a family of systems called <em>Diffix</em>, that attempt to prevent reconstruction without introducing as much noise as the reconstruction attacks suggest is necessary. To the best of our knowledge, these attacks represent the first successful attempt to reconstruct data from a commercial statistical-database system that is specifically designed to protect the privacy of the underlying data.  A larger and much more significant demonstration of the practical power of reconstruction attacks was carried out by the US Census Bureau in 2018, motivating the Bureau’s adoption of differential privacy for data products derived from the 2020 decennial census <a href="https://queue.acm.org/detail.cfm?ref=rss&amp;id=3295691">[GAM18]</a>.</p>

<p>This series will come in two parts: In this post, we will review the theory of reconstruction attacks, and present a model for reconstruction attacks that corresponds more directly to real attacks than the one that is typically presented.  In the second post, we will describe attacks that were launched against various iterations of the <em>Diffix</em> system. \(
\newcommand{\uni}{\mathcal{X}} % The universe
\newcommand{\usize}{T} % Universe size
\newcommand{\elem}{x} % Generic universe element. 
\newcommand{\pbs}{z} %Non-secret bits
\newcommand{\pbsuni}{\mathcal{Z}}
\renewcommand{\sb}{b} % Secret bit
\newcommand{\pds}{Z} %non-secret part of the data set
\newcommand{\ddim}{d} % Data dimension
\newcommand{\queries}{Q} % A set/workload of queries
\newcommand{\qmat}{\mat{Q}} % Query matrix
\newcommand{\qent}{w} % Entry of the query matrix
\newcommand{\hist}{h} % Histogram vector
\newcommand{\mech}{\mathcal{M}} % Generic Mechanism
\newcommand{\query}{q}
\newcommand{\queryfunc}{\varphi}
\newcommand{\ans}{a} % query answer
\newcommand{\qsize}{k}
\newcommand{\ds}{X}
\newcommand{\dsrow}{\elem} % same as elem above
\newcommand{\dsize}{n}
\newcommand{\priv}{\eps}
\newcommand{\privd}{\delta}
\newcommand{\acc}{\alpha}
\newcommand{\from}{:}
\newcommand{\set}[1]{\left{#1\right}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\eps}{\varepsilon}
\newcommand{\pmass}{\mathbbm{1}}
\newcommand{\zo}{\{0,1\}}
\newcommand{\mat}[1]{#1} % matrix notation: for now nothing
\)</p>
<h3 id="a-model-of-reconstruction-attacks">A Model of Reconstruction Attacks</h3>

<p>This part presents the basic theory of reconstruction attacks.  We’ll introduce a model of reconstruction attacks that is a little different from what you would see if you read the papers, and then describe the main results of Dinur and Nissim.  At the end we will briefly mention some variations that have been considered in the nearly two decades since.</p>

<p>Let us fix a dataset model, so that we can describe the attack precisely. (These attacks are very flexible and the ideas can usually be adapted to new models, as we’ll see at the end of this part.) We take the dataset to be a collection of \(\dsize\) records \(\ds = \{\elem_1,\dots,\elem_n\}\), each corresponding to the data of a single person.  The attacker’s goal is to learn some piece of secret information about as many individuals as possible, so we think of each record as having the form \(\elem_i = (\pbs_i,\sb_i)\) where \(\pbs_i\) is some identifying information, and \(\sb_i \in \zo\) is some secret. We assume that the secret is binary, although this aspect of the model can be generalized. We can visualize such a dataset as a matrix \([\pds \mid \sb]\) with two blocks as follows:
\[ \left[ \begin{array}{c|c} \pbs_1 &amp; \sb_1 \\ \vdots &amp; \vdots \\ \pbs_n &amp; \sb_n \end{array} \right] \]
For a concrete example, suppose each element in the dataset contains \(d\) binary attributes, and the attacker’s goal is to learn the last attribute of each user.  In this case we would write each element as a pair \((\pbs_i, \sb_i)\) where \(\pbs_i \in \zo^{d-1}\) and \(\sb_i \in \zo\).</p>

<p>Note that this distinction between \(\pbs_i\) and \(\sb_i\) is only in the mind of the attacker, who has some prior information about the users, but is trying to learn some specific secret information.  In order to make the attack simpler to describe, we will also assume that the attacker knows \(\pbs_1,\dots,\pbs_\dsize\), which is everything about the dataset except the secret bits, although this assumption can also be relaxed to a large extent. As a shorthand, we will refer to \(\pbs_1, \ldots, \pbs_\dsize\) as the prior information, and to \(\sb_1, \ldots,\sb_\dsize\) as the secret bits.</p>

<p>Our goal is to understand whether asking aggregate queries defined by the prior information can allow an attacker to learn non-trivial information about the secret bits.  Perhaps the most basic type of aggregate query we can ask is a <em>counting query</em>, which is a query that asks what number of the data points satisfy a given property. The Dinur-Nissim attacks assume that the attacker can get approximate answers to a type of counting queries that ask how many data points satisfy some property defined in terms of the prior information, and also have the sensitive bit set to \(1\).  Let us use the notation \(\pbsuni\) for the set of all possible values that the prior information can take. For the purposes of the attack, each query \(\query\) will be specified by a function \(\queryfunc \from \pbsuni \to \zo\) and have the specific form
\[
\query(\ds) = \sum_{j=1}^{\dsize} \queryfunc(\pbs_j) \cdot \sb_j.
\]
This is a good time to make one absolutely crucial point about this model, which is that</p>
<blockquote>
  <p>all the users are treated completely symmetrically by the queries, and the attacker cannot issue a query that targets a specific user \(x_i\) by name or a specific subset of users.  The different users are distinguished only by their data.  Nonetheless, we will see how to learn information about specific users from the answers to these queries.</p>
</blockquote>

<p>Returning to our example with binary attributes, consider the very natural set of queries that asks for the inner product of the secret bits with each attribute in the prior information, which is a measure of the correlation between these two attributes.  Then each query takes the form \(\query_i(\ds) = \sum_{j=1}^{n} \pbs_{j,i} \cdot \sb_{j}\).</p>

<p>The nice thing about this type of query is that we can express the answers to a set of queries \({\query_1,\dots,\query_\qsize}\) defined by \(\queryfunc_1, \ldots, \queryfunc_\qsize\) as the following matrix-vector product \(\qmat_{\pds}\cdot \mat{b}\):
\[ \left[ \begin{array}{c}\query_1(\ds) \\ \vdots  \\ \query_\qsize(\ds) \end{array} \right] = \left[ \begin{array}{ccc} \queryfunc_1(\pbs_1) &amp; \dots &amp; \queryfunc_1(\pbs_\dsize) \\ \vdots &amp;  \ddots  &amp; \vdots \\ \queryfunc_\qsize(\pbs_1) &amp;   \dots &amp; \queryfunc_k(\pbs_\dsize)  \end{array} \right] \left[ \begin{array}{c} \sb_1 \\ \vdots  \\ \sb_n  \end{array} \right]
\]
so we can study this model using tools from linear algebra.</p>

<h3 id="an-inefficient-attack">An Inefficient Attack</h3>

<p>Exact answers to such queries are clearly revealing, because, the attacker can use the predicates \[ \queryfunc_i(z) = \begin{cases} 1 &amp; \textrm{if } \pbs = \pbs_i  \\ 0 &amp;  \textrm{otherwise} \end{cases} \] to single out a specific user and receive their bit \(\sb_i\).  It is less obvious, however, that an attacker can learn a lot about the private bits even given noisy answers to the queries.</p>

<p>The first Dinur-Nissim attack shows that this is indeed possible—if the attacker can ask an unbounded number of counting queries, and each query is answered with, for example, 5% error, then the attacker can reconstruct 80% of the secret bits.  This attack requires exponentially many queries to run, making it somewhat impractical, but it is a proof of concept that an attack can reconstruct a large amount of private information even from very noisy statistics. Later we will see how to scale down the attack to use fewer queries at the cost of requiring more accurate answers.</p>

<p>The attack itself is quite simple:</p>

<ul>
  <li>
    <p>For simplicity, assume all the \(\pbs_1, \ldots, \pbs_\dsize\) are distinct so that each user is uniquely identified by the prior information.</p>
  </li>
  <li>
    <p>The attacker chooses the queries \(\query_1, \ldots, \query_\qsize\) so that the matrix \(\qmat_\pds\) has as its rows all of \(\zo^\dsize\). Namely, \(\qsize=2^\dsize\) and the functions \(\queryfunc_1, \ldots, \queryfunc_\qsize\) defining the queries take all possible values on \(\pbs_1, \ldots, \pbs_\dsize\).</p>
  </li>
  <li>
    <p>The attacker receives a vector \(\ans\) of noisy answers to the queries, where \( |\query_{i}(\ds) - \ans_{i}| &lt; \acc \dsize \) for each query \( \query_i \).  In matrix notation, this means \[ \max_{i = 1}^\qsize  |(\qmat_\pds\cdot {\sb})_i -\ans_i|= \| \qmat_\pds \cdot \sb -\ans\|_\infty  \leq \alpha \dsize. \]
  Note that, for \(\{0,1\}\)-valued queries, the answers range from \(0\) to \(\dsize\), so answers with additive error \(\pm 5\%\) corresponds to \(\acc = 0.05\).</p>
  </li>
  <li>
    <p>Finally, the attacker outputs any guess \(\hat{\sb} = (\hat{\sb}_{1}, \ldots, \hat{\sb}_{n})\) of the private bits vector that is consistent with the answers and the additive error bound \(\acc\). In other words, \(\hat{\sb}\) just needs to satisfy \[\max_{i = 1}^\qsize |\ans_i - (\qmat_\pds\cdot \hat{\sb})_i|= \| \qmat_\pds \cdot \hat\sb - a \|_{\infty} \leq \alpha \dsize \]
  Note that a solution always exists, since the true private bits \(\sb\) will do.</p>
  </li>
</ul>

<p>Our claim is that any such guess \(\hat{b}\) in fact agrees with the true private bits \(b\) for all but \(4\acc \dsize\) of the users. The reason is that if \(\hat{\sb}\) disagreed with more than \(4\acc \dsize\) of the secret bits, then the answer to some query would have eliminated \(\hat{\sb}\) from contention.  To see this, fix some \(\hat{\sb}\in \zo^\dsize\), and let \[ S_{01} = \{j: \hat{\sb}_j = 0,  \sb_j = 1\} \textrm{ and } S_{10} = \{j: \hat{\sb}_j = 1,  \sb_j = 0\}\] 
If \(\hat{\sb}\) and \(\sb\) disagree on more than \(4\acc \dsize\) bits, then at least one of these two sets has size larger than \(2\acc \dsize\). Let us assume that this set is \(S_{01}\), and we’ll deal with the other case by symmetry.  Suppose that the \(i\)-th row of \(\qmat_\pds\) is the indicator vector of \(S_{01}\), i.e., \[(\qmat_\pds)_{i,j} = 1 \iff j \in S_{01}.\] We then have
\[
|(\qmat_{\pds}\cdot {\sb})_i - (\qmat_{\pds}\cdot \hat{\sb})_i|= |S_{01}| &gt; 2 \acc \dsize,
\]
but, at the same time, if \(\hat{\sb}\) were output by the attacker, we would have
\[
|(\qmat_{\pds}\cdot {\sb})_i - (\qmat_{\pds}\cdot \hat{\sb})_i| \le |\ans_i - (\qmat_\pds\cdot \hat{\sb})_i| + |(\qmat_\pds \cdot \sb)_i - \ans_{i}| \le 2\acc \dsize, \]
which is a contradiction. An important point to note is that the attacker does not need to know the set \(S_{10}\), or the corresponding \(i\)-th row of \(\qmat_\pds\) and query \(\query_i\). Since the attacker asks all possible queries determined by the prior information, we can be sure \(\query_i\) is one of these queries, and an accurate answer to it rules out this particular bad choice of \(\hat{\sb}\).  To give you something concrete to cherish, we can summarize this discussion in the following theorem.</p>

<blockquote>
  <p><strong>Theorem <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>:</strong> There is a reconstruction attack that issues \(2^n\) queries to a dataset of \(n\) users, obtains answers with error \(\alpha n\), and reconstructs the secret bits of all but \(4 \alpha n\) users.</p>
</blockquote>

<h3 id="an-efficient-attack">An Efficient Attack</h3>

<p>The exponential Dinur-Nissim attack is quite powerful, as it recovers 80% of the secret bits even from answers with 5% error, but it has the drawback that it requires asking \(2^\dsize\) queries to a dataset with \(\dsize\) users.  Note that this is inherent to some extent.  Suppose we randomly subsample 50% of the dataset and answer the queries using only this subset by rescaling appropriately.  Although this random subsampling does not guarantee any meaningful privacy, clearly no attacker can reconstruct 75% of the secret bits, since some of them are effectively deleted.  However, the guarantees of random sampling tell us that any set of \(\qsize\) queries will be answered with maximum error \( \acc n =  O(\sqrt{n \log \qsize})\), so we can answer \( 2^{\Omega(n)} \) queries with \(5\%\) error while provably preventing this sort of reconstruction.</p>

<p>However, Dinur and Nissim showed that if we obtain <em>highly accurate</em> answers—still noisy, but with error smaller than the sampling error—then we can reconstruct the dataset to high accuracy.  We can also make the reconstruction process computationally efficient by using linear programming to replace the exhaustive search over all \(2^\dsize\) possible vectors of secrets.  Specifically, we change the attack as follows:</p>

<ul>
  <li>
    <p>The attacker now chooses \(\qsize\) <em>randomly chosen</em> functions \( \varphi_i \from \pbsuni \to \{0,1\} \) for a much smaller \(\qsize = O(\dsize) \).</p>
  </li>
  <li>
    <p>Upon receiving an answer vector \(\ans\), the attacker now searches for a <em>real-valued</em> \( \tilde{b} \in [0,1]^{\dsize} \) such that \( \| \ans - \qmat_\pds \cdot \tilde{b} \|_{\infty} \leq \acc n \).  Note that this vector can be found efficiently via linear programming.  The attacker then rounds each \( \tilde{b}_{i} \) to the nearest \( \hat{b}_{i} \in \{0,1\}\).</p>
  </li>
</ul>

<p>It’s now much trickier to analyze this attack and show that it achieves low reconstruction error, and we won’t go into details in this post.  However, the key idea is that, because the queries are chosen randomly, \( \qmat_\pds \) is a random matrix with entries in \( \{0,1\} \), and we can use the statistical properties of this random matrix to argue that, with high probability,
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty^2 \gtrsim |{i: \sb_i \neq \hat{\sb}_i}|.
\]
By the way we chose \(\tilde{\sb}\), we have 
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty \le \|\qmat_\pds \cdot \sb - \ans\|_\infty + \| \ans - \qmat_\pds \cdot \tilde{b} \|_{\infty} \leq 2\acc n,
\]
so, by combining the inequalities we get that the reconstruction error is about \( O(\alpha^2 n^2) \). Note that, in order to reconstruct 80% of the secret bits using this attack, we now need the error to be \( \alpha n  \ll \sqrt{n} \), but as long as this condition on the error is satisfied, we will have a highly accurate reconstruction.  Let’s add this theorem to your goodie bags:</p>

<blockquote>
  <p><strong>Theorem <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>:</strong> There is an efficient reconstruction attack that issues \(O(n)\) random queries to a dataset of \(n\) users, obtains answer with error \(\alpha n\), and, with high probability, reconstructs the secret bits of all but \( O(\alpha^2 n^2)\) users.</p>
</blockquote>

<p>Although we modeled the queries, and thus the matrix \(\qmat_\pds\) as uniformly random, it’s important to note that we really only relied on the fact that 
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty^2 \gtrsim
|\{i: \sb_i \neq \hat{\sb}_i\}|,
\]
and we can reconstruct while tolerating the same \(\Omega(\sqrt{n})\) error for any family of queries that gives rise to a matrix with this property.  Intuitively, any <em>random-enough</em> family of queries will have this property.  More specifically, the property is satisfied by any matrix with no small singular values <a href="https://dl.acm.org/doi/10.1007/978-3-540-85174-5_26">[DY08]</a> or with large discrepancy <a href="https://arxiv.org/abs/1203.5453">[MN12]</a>.  There is a large body of work showing that many specific families of queries lead to reconstruction. For example, we can perform reconstruction using <em>conjunction queries</em> that ask for the marginal distribution of small subsets of the attributes <a href="https://dl.acm.org/doi/abs/10.1145/1806689.1806795">[KLSU10]</a>.  That is, queries of the form “count the number of people with blue eyes and brown hair and a birthday in August.”  In fairness, there are also families of queries that do not satisfy the property, or only satisfy quantitatively weaker versions of it, such as histograms and threshold queries, and for these queries it is indeed possible to achieve differential privacy with \( \ll \sqrt{n} \) error.</p>

<h3 id="conclusion">Conclusion</h3>

<p>This is going to be the end of our technical discussion, but before signing off, let’s mention some of the important extensions of this theorem that have been developed over the years:</p>

<ul>
  <li>
    <p>We can allow the secret information \(\sb\) to be integers or real numbers, rather than bits. The queries still return \(\qmat_\pds\cdot \sb\). The exponential attack then guarantees that, given answers with error \(\acc n\), the reconstruction \(\hat{\sb}\) satisfies \(\|\hat{\sb}-\sb\|_1 \le 4\acc n\). This means, for example, that the reconstructed secrets of all but \(4\alpha n\) users are within \(\pm 1\) of the true secrets. The efficient attack guarantees that \(\|\hat{\sb}-\sb\|_2^2 \le O(\acc^2 n^2)\), which means that the reconstructed secrets are within \(\pm 1\) for all but \(O(\acc^2 n^2)\) users.</p>
  </li>
  <li>
    <p>It’s not crucial that <em>every</em> query be answered with error \( \ll \sqrt{n} \).  If we are willing to settle 
  for an inefficient attack, then we can reconstruct even if only 51% of the queries have small error.  If at least 75% have small error, then we can reconstruct efficiently <a href="https://dl.acm.org/doi/10.1145/1250790.1250804">[DMT07]</a>.</p>
  </li>
  <li>
    <p>The reconstruction attacks still apply to the seemingly more general data model in which the private 
  dataset \(\ds\) is a subset of some arbitrary (but public) data universe \(\uni\).  To see this, note that we can take \(\uni = \{\pbs_1, \ldots, \pbs_\dsize\}\), and we can interpret the secret bits \(\sb_i\) to indicate whether \(\pbs_i\) is an element of \(\ds\). Then the reconstruction attacks allow us to determine, up to some error, which elements of \(\uni\) are contained in \(\ds\). In the setting, the attack is sometimes called <em>membership inference</em>.</p>
  </li>
  <li>
    <p>The fact that the efficient Dinur-Nissim reconstruction attack fails when the error is \( \gg \sqrt{n} \) 
  does not mean it’s easy to achieve privacy with error of that magnitude.  As we mentioned earlier, we can achieve non-trivial error guarantees for a large number of queries simply by using a random subsample of half of the dataset, which is not a private algorithm in any reasonable sense of the word, as it can reveal everything about the chosen subset.  As this example shows,</p>

    <blockquote>
      <p>preventing reconstruction attacks does not mean preserving privacy.</p>
    </blockquote>

    <p>In particular, there are membership-inference attacks that succeed in violating privacy even when the queries are answered with \( \gg \sqrt{n}\) error. We refer the reader to the survey <a href="https://privacytools.seas.harvard.edu/publications/exposed-survey-attacks-private-data">[DSSU17]</a> for a somewhat more in-depth survey of reconstruction and membership-inference attacks.</p>
  </li>
</ul>

<p>Many types of queries give rise to the conditions under which reconstruction is possible.  Stay tuned for our next post, where we show how to generate those types of queries in practice against a family of systems known as <em>Diffix</em> that are specifically designed to thwart reconstruction.</p></div>
    </summary>
    <updated>2020-10-21T16:30:00Z</updated>
    <published>2020-10-21T16:30:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-10-21T18:22:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/21/assistant-and-associate-professors-to-contribute-to-the-future-of-the-department-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-january-11-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/21/assistant-and-associate-professors-to-contribute-to-the-future-of-the-department-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-january-11-2020/" rel="alternate" type="text/html"/>
    <title>Assistant and Associate Professors to contribute to the future of the Department of Computer Science at Department of Computer Science, Aarhus University (apply by January 11, 2020)</title>
    <summary>Aarhus University – an international top-100 University – has made an ambitious strategic investment in a 5-year recruitment plan to radically expand the Department of Computer Science. We expect to hire four candidates this year. Therefore, we invite applications from candidates that are driven by excellence in research and teaching as well as external collaboration […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Aarhus University – an international top-100 University – has made an ambitious strategic investment in a 5-year recruitment plan to radically expand the Department of Computer Science. We expect to hire four candidates this year. Therefore, we invite applications from candidates that are driven by excellence in research and teaching as well as external collaboration on societal challenges.</p>
<p>Website: <a href="https://au.career.emply.com/ad/aarhus-university-is-hiring-assistant-and-associate-professors-to-contribute-to-t/lqwlj1/en">https://au.career.emply.com/ad/aarhus-university-is-hiring-assistant-and-associate-professors-to-contribute-to-t/lqwlj1/en</a><br/>
Email: kgronbak@cs.au.dk</p></div>
    </content>
    <updated>2020-10-21T07:02:53Z</updated>
    <published>2020-10-21T07:02:53Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-21T20:20:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10454</id>
    <link href="http://arxiv.org/abs/2010.10454" rel="alternate" type="text/html"/>
    <title>A practical algorithm to calculate Cap Discrepancy</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bakhshizadeh:Milad.html">Milad Bakhshizadeh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamalinejad:Ali.html">Ali Kamalinejad</a>, Mina Latifi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10454">PDF</a><br/><b>Abstract: </b>Uniform distribution of the points has been of interest to researchers for a
long time and has applications in different areas of Mathematics and Computer
Science. One of the well-known measures to evaluate the uniformity of a given
distribution is Discrepancy, which assesses the difference between the Uniform
distribution and the empirical distribution given by putting mass points at the
points of the given set. While Discrepancy is very useful to measure
uniformity, it is computationally challenging to be calculated accurately. We
introduce the concept of directed Discrepancy based on which we have developed
an algorithm, called Directional Discrepancy, that can offer accurate
approximation for the cap Discrepancy of a finite set distributed on the unit
Sphere, $\mathbb{S}^2.$ We also analyze the time complexity of the Directional
Discrepancy algorithm precisely; and practically evaluate its capacity by
calculating the Cap Discrepancy of a specific distribution, Polar Coordinates,
which aims to distribute points uniformly on the Sphere.
</p></div>
    </summary>
    <updated>2020-10-21T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10408</id>
    <link href="http://arxiv.org/abs/2010.10408" rel="alternate" type="text/html"/>
    <title>A Faster Parameterized Algorithm for Temporal Matching</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zschoche:Philipp.html">Philipp Zschoche</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10408">PDF</a><br/><b>Abstract: </b>A temporal graph is a sequence of graphs (called layers) over the same vertex
set---describing a graph topology which is subject to discrete changes over
time. A $\Delta$-temporal matching $M$ is a set of time edges $(e,t)$ (an edge
$e$ paired up with a point in time $t$) such that for all distinct time edges
$(e,t),(e',t') \in M$ we have that $e$ and $e'$ do not share an endpoint, or
the time-labels $t$ and $t'$ are at least $\Delta$ time units apart. Mertzios
et al. [STACS~'20] provided a $2^{O(\Delta\nu)}\cdot |{\mathcal
G}|^{O(1)}$-time algorithm to compute the maximum size of $\Delta$-temporal
matching in a temporal graph $\mathcal G$, where $|\mathcal G|$ denotes the
size of $\mathcal G$, and $\nu$ is the $\Delta$-vertex cover number of
$\mathcal G$. The $\Delta$-vertex cover number is the minimum number of
vertices which are needed to hit (or cover) all edges in any $\Delta$
consecutive layers of the temporal graph. We show an improved algorithm to
compute a $\Delta$-temporal matching of maximum size with a running time of
$\Delta^{O(\nu)}\cdot |\mathcal G|$ and hence provide an exponential speedup in
terms of $\Delta$.
</p></div>
    </summary>
    <updated>2020-10-21T01:25:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10314</id>
    <link href="http://arxiv.org/abs/2010.10314" rel="alternate" type="text/html"/>
    <title>On the complexity of optimally modifying graphs representing spatial correlation in areal unit count data</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Duncan.html">Duncan Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meeks:Kitty.html">Kitty Meeks</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10314">PDF</a><br/><b>Abstract: </b>Lee and Meeks recently demonstrated that improved inference for areal unit
count data can be achieved by carrying out modifications to a graph
representing spatial correlations; specifically, they delete edges of the
planar graph derived from border-sharing between geographic regions in order to
maximise a specific objective function. In this paper we address the
computational complexity of the associated graph optimisation problem,
demonstrating that it cannot be solved in polynomial time unless P = NP.
</p></div>
    </summary>
    <updated>2020-10-21T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10189</id>
    <link href="http://arxiv.org/abs/2010.10189" rel="alternate" type="text/html"/>
    <title>Primitive Recursive Ordered Fields and Some Applications</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Victor Selivanov, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Selivanova:Svetlana.html">Svetlana Selivanova</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10189">PDF</a><br/><b>Abstract: </b>We establish primitive recursive versions of some known facts about
computable ordered fields of reals and computable reals, and then apply them to
proving primitive recursiveness of some natural problems in linear algebra and
analysis. In particular, we find a partial primitive recursive analogue of
Ershov-Madison's theorem about real closures of computable ordered fields,
relate the corresponding fields to the primitive recursive reals, give
sufficient conditions for primitive recursive root-finding, computing normal
forms of matrices, and computing solution operators of some linear systems of
PDE.
</p></div>
    </summary>
    <updated>2020-10-21T01:22:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10134</id>
    <link href="http://arxiv.org/abs/2010.10134" rel="alternate" type="text/html"/>
    <title>New Techniques and Fine-Grained Hardness for Dynamic Near-Additive Spanners</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Thiago Bergamaschi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henzinger:Monika.html">Monika Henzinger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gutenberg:Maximilian_Probst.html">Maximilian Probst Gutenberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Virginia_Vassilevska.html">Virginia Vassilevska Williams</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wein:Nicole.html">Nicole Wein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10134">PDF</a><br/><b>Abstract: </b>Maintaining and updating shortest paths information in a graph is a
fundamental problem with many applications. As computations on dense graphs can
be prohibitively expensive, and it is preferable to perform the computations on
a sparse skeleton of the given graph that roughly preserves the shortest paths
information. Spanners and emulators serve this purpose. This paper develops
fast dynamic algorithms for sparse spanner and emulator maintenance and
provides evidence from fine-grained complexity that these algorithms are tight.
</p>
<p>Under the popular OMv conjecture, we show that there can be no decremental or
incremental algorithm that maintains an $n^{1+o(1)}$ edge (purely additive)
$+n^{\delta}$-emulator for any $\delta&lt;1/2$ with arbitrary polynomial
preprocessing time and total update time $m^{1+o(1)}$. Also, under the
Combinatorial $k$-Clique hypothesis, any fully dynamic combinatorial algorithm
that maintains an $n^{1+o(1)}$ edge $(1+\epsilon,n^{o(1)})$-spanner or emulator
must either have preprocessing time $mn^{1-o(1)}$ or amortized update time
$m^{1-o(1)}$. Both of our conditional lower bounds are tight.
</p>
<p>As the above fully dynamic lower bound only applies to combinatorial
algorithms, we also develop an algebraic spanner algorithm that improves over
the $m^{1-o(1)}$ update time for dense graphs. For any constant $\epsilon\in
(0,1]$, there is a fully dynamic algorithm with worst-case update time
$O(n^{1.529})$ that whp maintains an $n^{1+o(1)}$ edge
$(1+\epsilon,n^{o(1)})$-spanner.
</p>
<p>Our new algebraic techniques and spanner algorithms allow us to also obtain
(1) a new fully dynamic algorithm for All-Pairs Shortest Paths (APSP) with
update and path query time $O(n^{1.9})$; (2) a fully dynamic
$(1+\epsilon)$-approximate APSP algorithm with update time $O(n^{1.529})$; (3)
a fully dynamic algorithm for near-$2$-approximate Steiner tree maintenance.
</p></div>
    </summary>
    <updated>2020-10-21T01:57:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10028</id>
    <link href="http://arxiv.org/abs/2010.10028" rel="alternate" type="text/html"/>
    <title>Towards and Ethical Framework in the Complex Digital Era</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pastor=Escuredo:David.html">David Pastor-Escuredo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vinuesa:Ricardo.html">Ricardo Vinuesa</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10028">PDF</a><br/><b>Abstract: </b>Since modernity, ethic has been progressively fragmented into specific
communities of practice. The digital revolution enabled by AI and Data is
bringing ethical wicked problems in the crossroads of technology and behavior.
However, the need of a comprehensive and constructive ethical framework is
emerging as digital platforms connect us globally. The unequal structure of the
global system makes that dynamic changes and systemic problems impact more on
those that are most vulnerable. Ethical frameworks based only on the
individual-level are not longer sufficient. A new ethical vision must comprise
the understanding of the scales and complex interconnections of social systems.
Many of these systems are internally fragile and very sensitive to external
factors and threats, which turns into unethical situations that require
systemic solutions. The high scale nature of digital technology that expands
globally has also an impact at the individual level having the risk to make
humans beings more homogeneous, predictable and ultimately controllable. To
preserve the core of humanity ethic must take a stand to preserve and keep
promoting individual rights and uniqueness and cultural heterogeneity tackling
the negative trends and impact of digitalization. Only combining human-centered
and collectiveness-oriented digital development it will be possible to
construct new social models and human-machine interactions that are ethical.
This vision requires science to enhance ethical frameworks and principles with
the actionable insights of relationships and properties of the social systems
that may not be evident and need to be quantified and understood to be solved.
Artificial Intelligence is both a risk and and opportunity for an ethical
development, thus we need a conceptual construct that drives towards a better
digitalizated world.
</p></div>
    </summary>
    <updated>2020-10-21T01:20:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.09929</id>
    <link href="http://arxiv.org/abs/2010.09929" rel="alternate" type="text/html"/>
    <title>On the Sample Complexity of Privately Learning Unbounded High-Dimensional Gaussians</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aden=Ali:Ishaq.html">Ishaq Aden-Ali</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ashtiani:Hassan.html">Hassan Ashtiani</a>, Gautam Kamath <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.09929">PDF</a><br/><b>Abstract: </b>We provide sample complexity upper bounds for agnostically learning
multivariate Gaussians under the constraint of approximate differential
privacy. These are the first finite sample upper bounds for general Gaussians
which do not impose restrictions on the parameters of the distribution. Our
bounds are near-optimal in the case when the covariance is known to be the
identity, and conjectured to be near-optimal in the general case. From a
technical standpoint, we provide analytic tools for arguing the existence of
global "locally small" covers from local covers of the space. These are
exploited using modifications of recent techniques for differentially private
hypothesis selection. Our techniques may prove useful for privately learning
other distribution classes which do not possess a finite cover.
</p></div>
    </summary>
    <updated>2020-10-21T01:28:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.09913</id>
    <link href="http://arxiv.org/abs/2010.09913" rel="alternate" type="text/html"/>
    <title>SlimSell: A Vectorizable Graph Representation for Breadth-First Search</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Besta:Maciej.html">Maciej Besta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marending:Florian.html">Florian Marending</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomonik:Edgar.html">Edgar Solomonik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoefler:Torsten.html">Torsten Hoefler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.09913">PDF</a><br/><b>Abstract: </b>Vectorization and GPUs will profoundly change graph processing. Traditional
graph algorithms tuned for 32- or 64-bit based memory accesses will be
inefficient on architectures with 512-bit wide (or larger) instruction units
that are already present in the Intel Knights Landing (KNL) manycore CPU.
Anticipating this shift, we propose SlimSell: a vectorizable graph
representation to accelerate Breadth-First Search (BFS) based on sparse-matrix
dense-vector (SpMV) products. SlimSell extends and combines the
state-of-the-art SIMD-friendly Sell-C-sigma matrix storage format with
tropical, real, boolean, and sel-max semiring operations. The resulting design
reduces the necessary storage (by up to 50%) and thus pressure on the memory
subsystem. We augment SlimSell with the SlimWork and SlimChunk schemes that
reduce the amount of work and improve load balance, further accelerating BFS.
We evaluate all the schemes on Intel Haswell multicore CPUs, the
state-of-the-art Intel Xeon Phi KNL manycore CPUs, and NVIDIA Tesla GPUs. Our
experiments indicate which semiring offers highest speedups for BFS and
illustrate that SlimSell accelerates a tuned Graph500 BFS code by up to 33%.
This work shows that vectorization can secure high-performance in BFS based on
SpMV products; the proposed principles and designs can be extended to other
graph algorithms.
</p></div>
    </summary>
    <updated>2020-10-21T01:25:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.09884</id>
    <link href="http://arxiv.org/abs/2010.09884" rel="alternate" type="text/html"/>
    <title>Sorting Short Keys in Circuits of Size o(n log n)</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Asharov:Gilad.html">Gilad Asharov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Wei=Kai.html">Wei-Kai Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shi:Elaine.html">Elaine Shi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.09884">PDF</a><br/><b>Abstract: </b>We consider the classical problem of sorting an input array containing $n$
elements, where each element is described with a $k$-bit comparison-key and a
$w$-bit payload. A long-standing open problem is whether there exist $(k + w)
\cdot o(n \log n)$-sized boolean circuits for sorting. We show that one can
overcome the $n\log n$ barrier when the keys to be sorted are short.
Specifically, we prove that there is a circuit with $(k + w) \cdot O(n k) \cdot
\poly(\log^*n - \log^* (w + k))$ boolean gates capable of sorting any input
array containing $n$ elements, each described with a $k$-bit key and a $w$-bit
payload. Therefore, if the keys to be sorted are short, say, $k &lt; o(\log n)$,
our result is asymptotically better than the classical AKS sorting network
(ignoring $\poly\log^*$ terms); and we also overcome the $n \log n$ barrier in
such cases. Such a result might be surprising initially because it is long
known that comparator-based techniques must incur $\Omega(n \log n)$ comparator
gates even when the keys to be sorted are only $1$-bit long (e.g., see Knuth's
"Art of Programming" textbook). To the best of our knowledge, we are the first
to achieve non-trivial results for sorting circuits using non-comparison-based
techniques. We also show that if the Li-Li network coding conjecture is true,
our upper bound is optimal, barring $\poly\log^*$ terms, for every $k$ as long
as $k = O(\log n)$.
</p></div>
    </summary>
    <updated>2020-10-21T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.09852</id>
    <link href="http://arxiv.org/abs/2010.09852" rel="alternate" type="text/html"/>
    <title>Evaluating the Cost of Atomic Operations on Modern Architectures</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schweizer:Hermann.html">Hermann Schweizer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Besta:Maciej.html">Maciej Besta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoefler:Torsten.html">Torsten Hoefler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.09852">PDF</a><br/><b>Abstract: </b>Atomic operations (atomics) such as Compare-and-Swap (CAS) or Fetch-and-Add
(FAA) are ubiquitous in parallel programming. Yet, performance tradeoffs
between these operations and various characteristics of such systems, such as
the structure of caches, are unclear and have not been thoroughly analyzed. In
this paper we establish an evaluation methodology, develop a performance model,
and present a set of detailed benchmarks for latency and bandwidth of different
atomics. We consider various state-of-the-art x86 architectures: Intel Haswell,
Xeon Phi, Ivy Bridge, and AMD Bulldozer. The results unveil surprising
performance relationships between the considered atomics and architectural
properties such as the coherence state of the accessed cache lines. One key
finding is that all the tested atomics have comparable latency and bandwidth
even if they are characterized by different consensus numbers. Another insight
is that the hardware implementation of atomics prevents any instruction-level
parallelism even if there are no dependencies between the issued operations.
Finally, we discuss solutions to the discovered performance issues in the
analyzed architectures. Our analysis enables simpler and more effective
parallel programming and accelerates data processing on various architectures
deployed in both off-the-shelf machines and large compute systems.
</p></div>
    </summary>
    <updated>2020-10-21T01:22:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.09774</id>
    <link href="http://arxiv.org/abs/2010.09774" rel="alternate" type="text/html"/>
    <title>GAMesh: Guided and Augmented Meshing for Deep Point Networks</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Agarwal:Nitin.html">Nitin Agarwal</a>, M Gopi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.09774">PDF</a><br/><b>Abstract: </b>We present a new meshing algorithm called guided and augmented meshing,
GAMesh, which uses a mesh prior to generate a surface for the output points of
a point network. By projecting the output points onto this prior and
simplifying the resulting mesh, GAMesh ensures a surface with the same topology
as the mesh prior but whose geometric fidelity is controlled by the point
network. This makes GAMesh independent of both the density and distribution of
the output points, a common artifact in traditional surface reconstruction
algorithms. We show that such a separation of geometry from topology can have
several advantages especially in single-view shape prediction, fair evaluation
of point networks and reconstructing surfaces for networks which output sparse
point clouds. We further show that by training point networks with GAMesh, we
can directly optimize the vertex positions to generate adaptive meshes with
arbitrary topologies.
</p></div>
    </summary>
    <updated>2020-10-21T02:01:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.04953</id>
    <link href="http://arxiv.org/abs/2006.04953" rel="alternate" type="text/html"/>
    <title>Hedging in games: Faster convergence of external and swap regrets</title>
    <feedworld_mtime>1603238400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xi.html">Xi Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Binghui.html">Binghui Peng</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.04953">PDF</a><br/><b>Abstract: </b>We consider the setting where players run the Hedge algorithm or its
optimistic variant to play an $n$-action game repeatedly for $T$ rounds.
</p>
<p>1) For two-player games, we show that the regret of optimistic Hedge decays
at $\tilde{O}( 1/T ^{5/6} )$, improving the previous bound $O(1/T^{3/4})$ by
Syrgkanis, Agarwal, Luo and Schapire (NIPS'15)
</p>
<p>2) In contrast, we show that the convergence rate of vanilla Hedge is no
better than $\tilde{\Omega}(1/ \sqrt{T})$, addressing an open question posted
in Syrgkanis, Agarwal, Luo and Schapire (NIPS'15).
</p>
<p>For general m-player games, we show that the swap regret of each player
decays at rate $\tilde{O}(m^{1/2} (n/T)^{3/4})$ when they combine optimistic
Hedge with the classical external-to-internal reduction of Blum and Mansour
(JMLR'07). The algorithm can also be modified to achieve the same rate against
itself and a rate of $\tilde{O}(\sqrt{n/T})$ against adversaries. Via standard
connections, our upper bounds also imply faster convergence to coarse
correlated equilibria in two-player games and to correlated equilibria in
multiplayer games.
</p></div>
    </summary>
    <updated>2020-10-21T01:30:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/20/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-17-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/20/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-17-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track Faculty Positions at Simon Fraser University (apply by December 17, 2020)</title>
    <summary>The School of Computing Science at Simon Fraser University (SFU) invites applications for tenure-track faculty positions. The School has multiple openings and will consider applications at all ranks, including assistant, associate and full professor. Excellent applicants in all areas of computer science will be considered. Website: https://www.sfu.ca/computing/job-opportunities.html#tenure Email: cs_faculty_affairs@sfu.ca</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The School of Computing Science at Simon Fraser University (SFU) invites applications for tenure-track faculty positions. The School has multiple openings and will consider applications at all ranks, including assistant, associate and full professor. Excellent applicants in all areas of computer science will be considered.</p>
<p>Website: <a href="https://www.sfu.ca/computing/job-opportunities.html#tenure">https://www.sfu.ca/computing/job-opportunities.html#tenure</a><br/>
Email: cs_faculty_affairs@sfu.ca</p></div>
    </content>
    <updated>2020-10-20T20:36:40Z</updated>
    <published>2020-10-20T20:36:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-21T20:20:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/20/three-year-and-tenure-track-positions-at-toyota-technological-institute-at-chicago-apply-by-december-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/20/three-year-and-tenure-track-positions-at-toyota-technological-institute-at-chicago-apply-by-december-1-2020/" rel="alternate" type="text/html"/>
    <title>Three-year and tenure-track positions at Toyota Technological Institute at Chicago (apply by December 1, 2020)</title>
    <summary>TTIC invites applications for the following faculty positions: research assistant professor (3-year term), tenure-track assistant professor, full or associate professor, and visiting professor. Applicants for research assistant professor positions (RAPs) are encouraged to simultaneously apply for the TTIC RAP program and the Simons-Berkeley Research Fellowship. Website: https://www.ttic.edu/faculty-hiring/ Email: recruiting@ttic.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>TTIC invites applications for the following faculty positions: research assistant professor (3-year term), tenure-track assistant professor, full or associate professor, and visiting professor. Applicants for research assistant professor positions (RAPs) are encouraged to simultaneously apply for the TTIC RAP program and the Simons-Berkeley Research Fellowship.</p>
<p>Website: <a href="https://www.ttic.edu/faculty-hiring/">https://www.ttic.edu/faculty-hiring/</a><br/>
Email: recruiting@ttic.edu</p></div>
    </content>
    <updated>2020-10-20T17:06:50Z</updated>
    <published>2020-10-20T17:06:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-21T20:20:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.09096</id>
    <link href="http://arxiv.org/abs/2010.09096" rel="alternate" type="text/html"/>
    <title>On Near-Linear-Time Algorithms for Dense Subset Sum</title>
    <feedworld_mtime>1603152000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bringmann:Karl.html">Karl Bringmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wellnitz:Philip.html">Philip Wellnitz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.09096">PDF</a><br/><b>Abstract: </b>In the Subset Sum problem we are given a set of $n$ positive integers $X$ and
a target $t$ and are asked whether some subset of $X$ sums to $t$. Natural
parameters for this problem that have been studied in the literature are $n$
and $t$ as well as the maximum input number $\rm{mx}_X$ and the sum of all
input numbers $\Sigma_X$. In this paper we study the dense case of Subset Sum,
where all these parameters are polynomial in $n$. In this regime, standard
pseudo-polynomial algorithms solve Subset Sum in polynomial time $n^{O(1)}$.
</p>
<p>Our main question is: When can dense Subset Sum be solved in near-linear time
$\tilde{O}(n)$? We provide an essentially complete dichotomy by designing
improved algorithms and proving conditional lower bounds, thereby determining
essentially all settings of the parameters $n,t,\rm{mx}_X,\Sigma_X$ for which
dense Subset Sum is in time $\tilde{O}(n)$. For notational convenience we
assume without loss of generality that $t \ge \rm{mx}_X$ (as larger numbers can
be ignored) and $t \le \Sigma_X/2$ (using symmetry). Then our dichotomy reads
as follows:
</p>
<p>- By reviving and improving an additive-combinatorics-based approach by Galil
and Margalit [SICOMP'91], we show that Subset Sum is in near-linear time
$\tilde{O}(n)$ if $t \gg \rm{mx}_X \Sigma_X/n^2$.
</p>
<p>- We prove a matching conditional lower bound: If Subset Sum is in
near-linear time for any setting with $t \ll \rm{mx}_X \Sigma_X/n^2$, then the
Strong Exponential Time Hypothesis and the Strong k-Sum Hypothesis fail.
</p>
<p>We also generalize our algorithm from sets to multi-sets, albeit with
non-matching upper and lower bounds.
</p></div>
    </summary>
    <updated>2020-10-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.09014</id>
    <link href="http://arxiv.org/abs/2010.09014" rel="alternate" type="text/html"/>
    <title>Solving Shisen-Sho boards</title>
    <feedworld_mtime>1603152000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bondt:Michiel_de.html">Michiel de Bondt</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.09014">PDF</a><br/><b>Abstract: </b>We give a simple proof of that determining solvability of Shisen-Sho boards
is NP-complete. Furthermore, we show that under realistic assumptions, one can
compute in logarithmic time if two tiles form a playable pair.
</p>
<p>We combine an implementation of the algoritm to test playability of pairs
with my earlier algorithm to solve Mahjong Solitaire boards with peeking, to
obtain an algorithm to solve Shisen-Sho boards. We sample several Shisen-Sho
and Mahjong Solitaire layouts for solvability for Shisen-Sho and Mahjong
Solitaire.
</p></div>
    </summary>
    <updated>2020-10-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08994</id>
    <link href="http://arxiv.org/abs/2010.08994" rel="alternate" type="text/html"/>
    <title>Log-rank and lifting for AND-functions</title>
    <feedworld_mtime>1603152000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lovett:Shachar.html">Shachar Lovett</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Knop:Alexander.html">Alexander Knop</a>, Sam McGuire, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuan:Weiqiang.html">Weiqiang Yuan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08994">PDF</a><br/><b>Abstract: </b>Let $f: \{0,1\}^n \to \{0, 1\}$ be a boolean function, and let $f_\land (x,
y) = f(x \land y)$ denote the AND-function of $f$, where $x \land y$ denotes
bit-wise AND. We study the deterministic communication complexity of $f_\land$
and show that, up to a $\log n$ factor, it is bounded by a polynomial in the
logarithm of the real rank of the communication matrix of $f_\land$. This comes
within a $\log n$ factor of establishing the log-rank conjecturefor
AND-functions with no assumptions on $f$. Our result stands in contrast with
previous results on special cases of the log-rank conjecture, which needed
significant restrictions on $f$ such as monotonicity or low
$\mathbb{F}_2$-degree. Our techniques can also be used to prove (within a $\log
n$ factor) a lifting theorem for AND-functions, stating that the deterministic
communication complexity of $f_\land$ is polynomially-related to the
AND-decision tree complexity of $f$.
</p>
<p>The results rely on a new structural result regarding boolean functions
$f:\{0, 1\}^n \to \{0, 1\}$ with a sparse polynomial representation, which may
be of independent interest. We show that if the polynomial computing $f$ has
few monomials then the set system of the monomials has a small hitting set, of
size poly-logarithmic in its sparsity. We also establish extensions of this
result to multi-linear polynomials $f:\{0,1\}^n \to \mathbb{R}$ with a larger
range.
</p></div>
    </summary>
    <updated>2020-10-20T23:23:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-10-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08862</id>
    <link href="http://arxiv.org/abs/2010.08862" rel="alternate" type="text/html"/>
    <title>Mad Science is Provably Hard: Puzzles in Hearthstone's Boomsday Lab are NP-hard</title>
    <feedworld_mtime>1603152000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoffmann:Michael.html">Michael Hoffmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lynch:Jayson.html">Jayson Lynch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Winslow:Andrew.html">Andrew Winslow</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08862">PDF</a><br/><b>Abstract: </b>We consider the computational complexity of winning this turn (mate-in-1 or
"finding lethal") in Hearthstone as well as several other single turn puzzle
types introduced in the Boomsday Lab expansion. We consider three natural
generalizations of Hearthstone (in which hand size, board size, and deck size
scale) and prove the various puzzle types in each generalization NP-hard.
</p></div>
    </summary>
    <updated>2020-10-20T23:20:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-10-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08840</id>
    <link href="http://arxiv.org/abs/2010.08840" rel="alternate" type="text/html"/>
    <title>Lazy Search Trees</title>
    <feedworld_mtime>1603152000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sandlund:Bryce.html">Bryce Sandlund</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wild:Sebastian.html">Sebastian Wild</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08840">PDF</a><br/><b>Abstract: </b>We introduce the lazy search tree data structure. The lazy search tree is a
comparison-based data structure on the pointer machine that supports
order-based operations such as rank, select, membership, predecessor,
successor, minimum, and maximum while providing dynamic operations insert,
delete, change-key, split, and merge. We analyze the performance of our data
structure based on a partition of current elements into a set of gaps
$\{\Delta_i\}$ based on rank. A query falls into a particular gap and splits
the gap into two new gaps at a rank $r$ associated with the query operation. If
we define $B = \sum_i |\Delta_i| \log_2(n/|\Delta_i|)$, our performance over a
sequence of $n$ insertions and $q$ distinct queries is $O(B + \min(n \log \log
n, n \log q))$. We show $B$ is a lower bound.
</p>
<p>Effectively, we reduce the insertion time of binary search trees from
$\Theta(\log n)$ to $O(\min(\log(n/|\Delta_i|) + \log \log |\Delta_i|, \; \log
q))$, where $\Delta_i$ is the gap in which the inserted element falls. Over a
sequence of $n$ insertions and $q$ queries, a time bound of $O(n \log q + q
\log n)$ holds; better bounds are possible when queries are non-uniformly
distributed. As an extreme case of non-uniformity, if all queries are for the
minimum element, the lazy search tree performs as a priority queue with $O(\log
\log n)$ time insert and decrease-key operations. The same data structure
supports queries for any rank, interpolating between binary search trees and
efficient priority queues.
</p>
<p>Lazy search trees can be implemented to operate mostly on arrays, requiring
only $O(\min(q, n))$ pointers. Via direct reduction, our data structure also
supports the efficient access theorems of the splay tree, providing a powerful
data structure for non-uniform element access, both when the number of accesses
is small and large.
</p></div>
    </summary>
    <updated>2020-10-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08821</id>
    <link href="http://arxiv.org/abs/2010.08821" rel="alternate" type="text/html"/>
    <title>On the Hardness of Average-case k-SUM</title>
    <feedworld_mtime>1603152000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brakerski:Zvika.html">Zvika Brakerski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stephens=Davidowitz:Noah.html">Noah Stephens-Davidowitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vaikuntanathan:Vinod.html">Vinod Vaikuntanathan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08821">PDF</a><br/><b>Abstract: </b>In this work, we show the first worst-case to average-case reduction for the
classical $k$-SUM problem. A $k$-SUM instance is a collection of $m$ integers,
and the goal of the $k$-SUM problem is to find a subset of $k$ elements that
sums to $0$. In the average-case version, the $m$ elements are chosen uniformly
at random from some interval $[-u,u]$.
</p>
<p>We consider the total setting where $m$ is sufficiently large (with respect
to $u$ and $k$), so that we are guaranteed (with high probability) that
solutions must exist. Much of the appeal of $k$-SUM, in particular connections
to problems in computational geometry, extends to the total setting.
</p>
<p>The best known algorithm in the average-case total setting is due to Wagner
(following the approach of Blum-Kalai-Wasserman), and achieves a run-time of
$u^{O(1/\log k)}$, which beats the known (conditional) lower bounds for
worst-case $k$-SUM. One could wonder whether this can be improved even further.
But, we show a matching average-case lower-bound, by showing a reduction from
worst-case lattice problems, thus introducing a new family of techniques into
the field of fine-grained complexity. In particular, we show that any algorithm
solving average-case $k$-SUM on $m$ elements in time $u^{o(1/\log k)}$ will
give a super-polynomial improvement in the complexity of algorithms for lattice
problems.
</p></div>
    </summary>
    <updated>2020-10-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08691</id>
    <link href="http://arxiv.org/abs/2010.08691" rel="alternate" type="text/html"/>
    <title>Automatic Tree Ring Detection using Jacobi Sets</title>
    <feedworld_mtime>1603152000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Kayla Makela, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ophelders:Tim.html">Tim Ophelders</a>, Michelle Quigley, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munch:Elizabeth.html">Elizabeth Munch</a>, Daniel Chitwood, Asia Dowtin <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08691">PDF</a><br/><b>Abstract: </b>Tree ring widths are an important source of climatic and historical data, but
measuring these widths typically requires extensive manual work. Computer
vision techniques provide promising directions towards the automation of tree
ring detection, but most automated methods still require a substantial amount
of user interaction to obtain high accuracy. We perform analysis on 3D X-ray CT
images of a cross-section of a tree trunk, known as a tree disk. We present
novel automated methods for locating the pith (center) of a tree disk, and ring
boundaries. Our methods use a combination of standard image processing
techniques and tools from topological data analysis. We evaluate the efficacy
of our method for two different CT scans by comparing its results to manually
located rings and centers and show that it is better than current automatic
methods in terms of correctly counting each ring and its location. Our methods
have several parameters, which we optimize experimentally by minimizing edit
distances to the manually obtained locations.
</p></div>
    </summary>
    <updated>2020-10-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-10-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08676</id>
    <link href="http://arxiv.org/abs/2010.08676" rel="alternate" type="text/html"/>
    <title>Fast Spatial Autocorrelation</title>
    <feedworld_mtime>1603152000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Anar Amgalan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mujica=Parodi:Lilianne_R=.html">Lilianne R. Mujica-Parodi</a>, Steven S. Skiena <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08676">PDF</a><br/><b>Abstract: </b>Physical or geographic location proves to be an important feature in many
data science models, because many diverse natural and social phenomenon have a
spatial component. Spatial autocorrelation measures the extent to which locally
adjacent observations of the same phenomenon are correlated. Although
statistics like Moran's $I$ and Geary's $C$ are widely used to measure spatial
autocorrelation, they are slow: all popular methods run in $\Omega(n^2)$ time,
rendering them unusable for large data sets, or long time-courses with moderate
numbers of points. We propose a new $S_A$ statistic based on the notion that
the variance observed when merging pairs of nearby clusters should increase
slowly for spatially autocorrelated variables. We give a linear-time algorithm
to calculate $S_A$ for a variable with an input agglomeration order (available
at https://github.com/aamgalan/spatial_autocorrelation). For a typical dataset
of $n \approx 63,000$ points, our $S_A$ autocorrelation measure can be computed
in 1 second, versus 2 hours or more for Moran's $I$ and Geary's $C$. Through
simulation studies, we demonstrate that $S_A$ identifies spatial correlations
in variables generated with spatially-dependent model half an order of
magnitude earlier than either Moran's $I$ or Geary's $C$. Finally, we prove
several theoretical properties of $S_A$: namely that it behaves as a true
correlation statistic, and is invariant under addition or multiplication by a
constant.
</p></div>
    </summary>
    <updated>2020-10-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08633</id>
    <link href="http://arxiv.org/abs/2010.08633" rel="alternate" type="text/html"/>
    <title>Universal guarantees for decision tree induction via a higher-order splitting criterion</title>
    <feedworld_mtime>1603152000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Neha.html">Neha Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lange:Jane.html">Jane Lange</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08633">PDF</a><br/><b>Abstract: </b>We propose a simple extension of top-down decision tree learning heuristics
such as ID3, C4.5, and CART. Our algorithm achieves provable guarantees for all
target functions $f: \{-1,1\}^n \to \{-1,1\}$ with respect to the uniform
distribution, circumventing impossibility results showing that existing
heuristics fare poorly even for simple target functions. The crux of our
extension is a new splitting criterion that takes into account the correlations
between $f$ and small subsets of its attributes. The splitting criteria of
existing heuristics (e.g. Gini impurity and information gain), in contrast, are
based solely on the correlations between $f$ and its individual attributes.
</p>
<p>Our algorithm satisfies the following guarantee: for all target functions $f
: \{-1,1\}^n \to \{-1,1\}$, sizes $s\in \mathbb{N}$, and error parameters
$\epsilon$, it constructs a decision tree of size $s^{\tilde{O}((\log
s)^2/\epsilon^2)}$ that achieves error $\le O(\mathsf{opt}_s) + \epsilon$,
where $\mathsf{opt}_s$ denotes the error of the optimal size $s$ decision tree.
A key technical notion that drives our analysis is the noise stability of $f$,
a well-studied smoothness measure.
</p></div>
    </summary>
    <updated>2020-10-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08576</id>
    <link href="http://arxiv.org/abs/2010.08576" rel="alternate" type="text/html"/>
    <title>Improving Schroeppel and Shamir's Algorithm for Subset Sum via Orthogonal Vectors</title>
    <feedworld_mtime>1603152000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nederlof:Jesper.html">Jesper Nederlof</a>, Karol Węgrzycki <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08576">PDF</a><br/><b>Abstract: </b>We present an $\mathcal{O}^\star(2^{0.5n})$ time and
$\mathcal{O}^\star(2^{0.249999n})$ space randomized algorithm for solving
worst-case Subset Sum instances with $n$ integers. This is the first
improvement over the long-standing $\mathcal{O}^\star(2^{n/2})$ time and
$\mathcal{O}^\star(2^{n/4})$ space algorithm due to Schroeppel and Shamir (FOCS
1979).
</p>
<p>We breach this gap in two steps: (1) We present a space efficient reduction
to the Orthogonal Vectors Problem (OV), one of the most central problem in
Fine-Grained Complexity. The reduction is established via an intricate
combination of the method of Schroeppel and Shamir, and the representation
technique introduced by Howgrave-Graham and Joux (EUROCRYPT 2010) for designing
Subset Sum algorithms for the average case regime. (2) We provide an algorithm
for OV that detects an orthogonal pair among $N$ given vectors in $\{0,1\}^d$
with support size $d/4$ in time $\tilde{O}(N\cdot2^d/\binom{d}{d/4})$. Our
algorithm for OV is based on and refines the representative families framework
developed by Fomin, Lokshtanov, Panolan and Saurabh (J. ACM 2016).
</p>
<p>Our reduction uncovers a curious tight relation between Subset Sum and OV,
because any improvement of our algorithm for OV would imply an improvement over
the runtime of Schroeppel and Shamir, which is also a long standing open
problem.
</p></div>
    </summary>
    <updated>2020-10-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable</id>
    <link href="https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable.html" rel="alternate" type="text/html"/>
    <title>The graphs of stably matchable pairs</title>
    <summary>The stable matching problem takes as input the preferences from two groups of agents (most famously medical students and supervisors of internships), and pairs up agents from each group in a way that encourages everyone to play along: no pair of agents would rather go their own way together than take the pairings they were both given. A solution can always be found by the Gale–Shapley algorithm, but there are generally many solutions, described by the lattice of stable matchings. Some pairs of agents are included in at least one stable matching, while some other pairs are never matched. In this way, each instance of stable matchings gives rise to a graph, the graph of stably matchable pairs. This graph is the subject and title of my latest preprint, arXiv:2010.09230, which asks: Which graphs can arise this way? How hard is it to recognize these graphs, and infer a stable matching instance that might have generated them? How does the graph structure relate to the lattice structure?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://en.wikipedia.org/wiki/Stable_marriage_problem">stable matching problem</a> takes as input the preferences from two groups of agents (most famously medical students and supervisors of internships), and pairs up agents from each group in a way that encourages everyone to play along: no pair of agents would rather go their own way together than take the pairings they were both given. A solution can always be found by the <a href="https://en.wikipedia.org/wiki/Gale%E2%80%93Shapley_algorithm">Gale–Shapley algorithm</a>, but there are generally many solutions, described by the <a href="https://en.wikipedia.org/wiki/Lattice_of_stable_matchings">lattice of stable matchings</a>. Some pairs of agents are included in at least one stable matching, while some other pairs are never matched. In this way, each instance of stable matchings gives rise to a graph, the <em>graph of stably matchable pairs</em>. This graph is the subject and title of my latest preprint, <a href="https://arxiv.org/abs/2010.09230">arXiv:2010.09230</a>, which asks: Which graphs can arise this way? How hard is it to recognize these graphs, and infer a stable matching instance that might have generated them? How does the graph structure relate to the lattice structure?</p>

<p>For some answers, see the preprint. One detail is connected to <a href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html">my previous post, on polyhedra with no two disjoint faces</a> (even though there are no polyhedra in the new preprint): the (prism,\(K_{3,3}\))-minor-free graphs discussed there come up in proving an equivalence between outerplanar graphs of stably matchable pairs and lattices of <a href="https://en.wikipedia.org/wiki/Closure_problem">closures</a> of <a href="https://en.wikipedia.org/wiki/Polytree">oriented trees</a>. Instead of providing any technical details of any the other results in the paper, though, I thought it would be more fun to show a few visual highlights.</p>

<p>The following figure shows a cute mirror-inversion trick (probably already known, although I don’t know where or by whom) for embedding an arbitrary bipartite graph as an induced subgraph of a regular bipartite graph. I use it to show that graphs of stably matchable pairs have no forbidden induced subgraphs:</p>

<p style="text-align: center;"><img alt="Embedding a bipartite graph as an induced subgraph of a regular bipartite graph" src="https://11011110.github.io/blog/assets/2020/regularize.svg" width="60%"/></p>

<p>This next one depicts a combinatorial description of a stable matching instance having a \(6\times 5\) grid as its graph, in terms of the top and bottom matchings in the lattice of matchings, the “rotations” that can be used to move between matchings in this lattice, and a partial order on the rotations. For what I was doing in this paper, these rotation systems were much more convenient to work with than preferences.</p>

<p style="text-align: center;"><img alt="Rotation system describing a system of stable matchings having a 6x5 grid as its graph" src="https://11011110.github.io/blog/assets/2020/5x6.svg" width="80%"/></p>

<p>All the main ideas for a proof of NP-completeness of recognizing these graphs, by reduction from <a href="https://en.wikipedia.org/wiki/Not-all-equal_3-satisfiability">not-all-equal 3-satisfiability</a>, are visible in the next picture. The proof now in the paper is significantly more complicated, though, because the construction in this image produces nonplanar graphs but I wanted a proof that would also apply in the planar case.</p>

<p style="text-align: center;"><img alt="NP-completeness reduction from NAE3SAT to recognizing graphs of stably matchable pairs" src="https://11011110.github.io/blog/assets/2020/nae3sat-to-matching.svg"/></p>

<p>The last one shows a sparse graph that can be represented as a graph of stably-matching pairs (because it’s outerplanar, bipartite, and biconnected) but has a high-degree vertex. If we tried to test whether it could be realized by doing a brute-force search over preference systems, the time would be factorial in the degree, but my preprint provides faster algorithms that are only singly exponential in the number of edges.</p>

<p style="text-align: center;"><img alt="Outerplanar graph of stably matchable pairs with a factorial number of potential preference systems" src="https://11011110.github.io/blog/assets/2020/factorial.svg"/></p>

<p>(<a href="https://mathstodon.xyz/@11011110/105065476283424319">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-10-19T20:29:00Z</updated>
    <published>2020-10-19T20:29:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-10-20T05:33:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6623846086903606036</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6623846086903606036/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6623846086903606036" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6623846086903606036" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html" rel="alternate" type="text/html"/>
    <title>Nature vs Nurture close to my birthday</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Since I was born on Oct 1, 1960 (that's not true---if I posted  my real birthday I might get my  identity stolen), I will do a nature vs nurture post based on my life, which seems less likely to offend then doing it on someone else's life. I'll just rattle off some random points on Nature vs Nurture.</p><p>1) Is it plausible that I was born with some math talent? Is plausible that I was born with some talent to understand the polynomial van der Warden theorem? What is the granularity of nature-given or nurture-given abilities?</p><p>2) My dad was a HS English teacher and later Vice-Principal. My mom taught English at a Community college. Readers of the blog might think, given my spelling and grammar, that I was switched at birth. My mom says (jokingly?) that I was switched at birth since she thinks I am good at math.</p><p>a) I am not THAT good at math. Also see next point.</p><p>b) While there are some math families, there are not many. See my post <a href="https://blog.computationalcomplexity.org/2009/02/baseball-families-and-math-families.html">here</a>.</p><p>c) I think being raised in an intellectual atmosphere by two EDUCATORS who had the money to send me to college and allowed me the freedom to study what I wanted to  is far more important than the rather incidental matter of what field I studied.</p><p>d) Since my parents never went into math or the sciences it is very hard to tell  if they were `good at math' or even what that means.</p><p>3) There were early signs I was INTERESTED in math, though not that I was good at it.</p><p>a) In fourth grade I wanted to know how many SECONDS were in a century so I spend some time figuring it out on paper. Did I get the right answer?  I forgot about leap years.</p><p>b) I was either a beneficiary of, or a victim of, THE NEW MATH. So I learned about comm. and assoc. operations in 8th grade. We were asked to come up with our own operations. I wanted to come up with an operation that was comm. but not assoc. I did! Today I would write it as f(x,y) = |x-y|. This is the earliest I can think of where I made up a nice math problem. Might have been the last time I made up a nice math problem AND solved it without help. </p><p>c) In 10th grade I took some Martin Gardner books out of the library. The first theorem I learned not-in-school was that a graph is Eulerian iff every vertex has even degree. I read the chapter on Soma cubes and bought a set. (Soma cubes are explained <a href="https://en.wikipedia.org/wiki/Soma_cube">here</a>.) </p><p>d) I had a talent (nature?) at Soma Cubes.  I did every puzzle in the book in a week, diagrammed them, and even understood (on some level) the proofs that some could not be done. Oddly I am NOT good at 3-dim geom. Or even 2-dim geom.  For 1-dim I hold my own!</p><p>e) Throughout my childhood I noticed odd logic and odd math things that were said: </p><p>``Here at WCOZ (a radio station) we have an AXIOM, that's like a saying man, that weekends should be SEVEN DAYS LONG'' (Unless that axiom resolves CH, I don't think it should be assumed.) </p><p>``To PROVE we have the lowest prices in town we will give you a free camera!'' (how does that prove anything?) </p><p>``This margarine tastes JUST LIKE BUTTER'' (Okay-- so why not just buy butter?)</p><p>e) In 9th grade when I learned the quadratic formula I re-derived it once-a-month since I though it was important that one can prove such things.  I heard (not sure from where) that there was no 5th degree equation. At that very moment I told my parents:</p><p><i>I am going to major in math so I can find out why there is no 5th degree equation.</i></p><p>There are worse things for parents to hear from their children. See <a href="https://blog.computationalcomplexity.org/2019/06/a-proof-that-227-pi-0-and-more.html">here</a> for dad's reaction. </p><p>f) When I learned that the earth's orbit around the sun is an ellipse and that the earth was one of the foci, I wondered where the other foci is and if its important. I still wonder about this one. Google has not helped me here, though perhaps I have not phrased the question properly. If you know the answer please leave a comment. </p><p>g) I also thought about The Ketchup problem and other problems, that I won't go into since I already blogged about them  <a href="https://blog.computationalcomplexity.org/2012/06/ketchup-problem.html">here</a></p><p>4) I was on the math team in high school, but wasn't very good at it. I WAS good at making up math team problems. I am now on the committee that makes up the Univ of MD HS math competition. I am still not good at solving the problems but good at making them up. </p><p>5) From 9th grade on before I would study for an exam by making up what I thought would be a good exam and doing that. Often my exam was a better test of knowledge than the exam given. In college I helped people in Math and Physics by making up exams for them to work on as practice. </p><p>6) I was good at reading, understanding, and explaining papers. </p><p>7) I was never shy about asking for help. My curiosity exeeded by ego... by a lot!</p><p>8) Note that items 5,6, and 7 above do not mention SOLVING problems. The papers I have written are of three (overlapping) types:</p><p>a) I come up with the problem, make some inroads on it based on knowledge, and then have people cleverer (this is often) or with more knowledge (this is rarer) help me solve the problems.</p><p>b) I come up with the problem, and combine two things I know from other papers to solve it. </p><p>c) Someone else asks for my help on something and I have the knowledge needed. I can only recall one time where this lead to a paper. </p><p>NOTE- I do not think I have ever had a clever or new technique. CAVEAT: the diff between combining  known knowledge in new ways and having a clever or new technique is murky. </p><p>8) Over time these strengths and weaknesses have gotten more extreme. It has become a self-fulfilling prophecy where I spend A LOT of time making up problems without asking for help, but when I am trying to solve a problem I early on ask for help. Earlier than I should? Hard to know. </p><p>9) One aspect is `how good am I at math' But a diff angle is that I like to work on things that I KNOW are going to work out, so reading an article is better than trying to create new work. This could be a psychological thing. But is that nature or nurture?  </p><p>10) Could I be a better problem solver? Probably. Could I be a MUCH better problem solver? NO. Could I have been a better problem solver  I did more work on that angle when I was younger? Who knows? </p><p>11) Back to the Quintic: I had the following thought in ninth grade, though I could not possibly have expressed it: The question of, given a problem, how hard is it, upper and lower bounds, is a fundamental one that is worth a lifetime of study. As such my interest in complexity theory and recursion theory goes back to ninth grade or even further. My interest in Ramsey Theory for its own sake (and not in the service of complexity theory) is much more recent and does not quite fit into my narrative. But HEY- real life does not have as well defined narratives as fiction does. </p><p>12) Timing and Luck: IF I had been in grad student at a slight diff time I can imagine doing work on  algorithmic  Galois theory. <a href="https://singer.math.ncsu.edu/Algorithmic_slides.pdf">Here</a>  is a talk on Algorithmic  Galois theory. Note that one of the earliest results is by Landau and Miller from 1985---I had a course from Miller on Alg. Group Theory in 1982. This is NOT a wistful `What might have been' thought. Maybe I would have sucked at it, so its just as well I ended up doing recursion theory, then Ramsey theory, then recursion-theoretic Ramsey Theory, then muffins. </p><p><br/></p><p><br/></p><div><br/></div></div>
    </content>
    <updated>2020-10-19T15:55:00Z</updated>
    <published>2020-10-19T15:55:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-21T14:52:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7803</id>
    <link href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/" rel="alternate" type="text/html"/>
    <title>Understanding generalization requires rethinking deep learning?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Yamini Bansal, Gal Kaplun, and Boaz Barak (See also paper on arxiv, code on gitlab, upcoming talk by Yamini&amp;Boaz, video of past talk) A central puzzle of deep learning is the question of generalization. In other words, what can we deduce from the training performance of a neural network about its test performance on fresh … <a class="more-link" href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/">Continue reading <span class="screen-reader-text">Understanding generalization requires rethinking deep learning?</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em><a href="https://yaminibansal.com/about/">Yamini Bansal</a>, <a href="https://www.galkaplun.com/">Gal Kaplun</a>, and Boaz Barak</em></p>



<p>(See also <em><a href="https://arxiv.org/abs/2010.08508">paper on arxiv</a></em>,  <a href="https://gitlab.com/harvard-machine-learning/rationality-generalization">code on gitlab</a>,  <a href="https://cmsa.fas.harvard.edu/10-28-2020-new-technologies-in-mathematics-seminar/">upcoming talk by Yamini&amp;Boaz</a>,  <a href="https://youtu.be/89ixhju1hJ0">video of past talk</a>)</p>



<p>A central <a href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/">puzzle</a> of deep learning is the question of <em>generalization</em>. In other words, what can we deduce from the <em>training performance</em> of a neural network about its <em>test performance</em> on <em>fresh unseen examples</em>. An <a href="https://arxiv.org/abs/1611.03530">influential paper</a> of Zhang, Bengio, Hardt, Recht, and Vinyals showed that the answer could be “nothing at all.” </p>



<p>Zhang et al. gave examples where modern deep neural networks achieve 100% accuracy on classifying their training data, but their performance on unseen data may be no better than chance. Therefore we cannot give meaningful guarantees for deep learning using traditional “generalization bounds” that bound the difference between test and train performance by some quantity that tends to zero as the number of datapoints <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> increases. This is why (to quote their title), Zhang et al. claimed that <strong>“understanding deep learning requires rethinking generalization”</strong>.</p>



<p>But what if the issue isn’t that we’ve been doing generalization bounds wrong, but rather that we’ve been doing deep learning (or more accurately, supervised deep learning) wrong?</p>



<h3>Self Supervised + Simple fit (SSS) learning</h3>



<p>To explain what we mean, let’s take a small detour to contrast “traditional” or “end-to-end” supervised learning with a different approach to supervised learning, which we’ll call here “Self-Supervised + Simple fit” or “SSS algorithms.” (While the name “SSS algorithms” is new, the approach itself has a <a href="http://people.idsia.ch/~juergen/FKI-126-90_%28revised%29bw_ocr.pdf">long history</a> and has recently been used with great success in practice; our work gives no new methods—only new analysis.)</p>



<p>The classical or “end-to-end” approach for supervised learning can be phrased as <em>“ask and you shall receive”</em>. Given labeled data, you ask (i.e., run an optimizer) for a complex classifier (e.g., a deep neural net) that fits the data (i.e., outputs the given labels on the given data points) and hope that it will be successful on future, unseen, data points as well. End-to-end supervised learning achieves state-of-art results for many classification problems, particularly for computer vision datasets ImageNet and CIFAR-10.</p>



<p>However, end-to-end learning does not directly correspond to the way humans learn to recognize objects (see also <a href="https://youtu.be/7I0Qt7GALVk?t=2475">this talk of LeCun</a>). A baby may see millions of images in the first year of her life, but most of them do not come with explicit labels. After seeing those images, a baby can make future classifications using very few labeled examples. For example, it might be enough to show her once what is a dog and what is a cat for her to correctly classify future dogs and cats, even if they look quite different from these examples.</p>



<figure class="wp-block-image"><img alt="End-to-end learning vs SSS algorithms." src="https://i.imgur.com/LXRaTQq.png"/><strong>Figure 1:</strong> Cartoon of end-to-end vs SSS learning </figure>



<p>In recent years, practitioners have proposed algorithms that are more similar to human learning than supervised learning. Such methods separate the process into two stages. In the <em>first stage</em>, we do <strong>representation learning</strong> whereby we use <em>unlabeled</em> data to learn a <em>representation</em>: a complex map (e.g., a deep neural net) mapping the inputs into some “representation space.” In the <em>second stage</em>, we fit a simple classifier (e.g., a linear threshold function) to the representation of the datapoints and the given labels. We call such algorithms <strong>“Self-Supervision + Simple fit”</strong> or <strong>SSS algorithms</strong>. (Note that, unlike other representation-learning based classifiers, the complex representation is “frozen” and not “fine-tuned” in the second stage, where only a simple classifier is used on top of it.)</p>



<p>While we don’t have a formal definition, a “good representation” should make downstream tasks easier, in the sense of allowing for fewer examples or simpler classifiers. We typically learn a representation via <strong>self supervision</strong> , whereby one finds a representation minimizing an objective function that intuitively requires some “insight” into the data. Approaches for self-supervision include reconstruction, where the objective involves recovering data points from partial information (e.g., recover missing <a href="https://arxiv.org/abs/1810.04805">words</a> or <a href="https://arxiv.org/abs/1604.07379">pixels</a>), and <em><a href="https://arxiv.org/abs/2002.05709">contrastive learning</a></em>, where the objective is to find a representation that make similar points close and dissimilar points far (e.g., in Euclidean space).</p>



<p>SSS algorithms have been traditionally used in natural language processing, where unlabeled data is plentiful, but labeled data for a particular task is often scarce. But recently SSS algorithms were also <a href="https://arxiv.org/abs/1902.06162">used with great success</a> even for vision tasks such as ImageNet and CIFAR10 where <em>all data is labeled!</em> While SSS algorithms do not yet beat the state-of-art supervised learning algorithms, they do get <a href="https://arxiv.org/abs/2006.10029">pretty close</a>. SSS algorithms also have other practical advantages over “end-to-end supervised learning”: they can make use of unlabeled data, the representation could be useful for non-classification tasks, and may have improved out of distribution performance. There has also been recent theoretical analysis of contrastive and reconstruction learning under certain statistical assumptions (see <a href="https://arxiv.org/abs/1902.09229">Arora et al</a> and  <a href="https://arxiv.org/abs/2008.01064">Lee et al</a>).</p>



<h3>The generalization gap of SSS algorithms</h3>



<p>In <a href="https://arxiv.org/abs/2010.08508">a recent paper</a>, we show that SSS algorithms not only work in practice, but work in theory too.</p>



<p>Specifically, we show that such algorithms have <strong>(1)</strong> small generalization gap and <strong>(2)</strong> we can <strong>prove</strong> (under reasonable assumptions) that their generalization gap tends to zero with the number of samples, with bounds that are meaningful for many modern classifiers on the CIFAR-10 and ImageNet datasets. We consider the setting where all data is labeled, and the <em>same dataset</em> is used for both learning the representation and fitting a simple classifier. The resulting classifier includes the overparameterized representation, and so we cannot simply apply “off the shelf” generalization bounds. Indeed, a priori it’s not at all clear that the generalization gap for SSS algorithms should be small.</p>



<p>To get some intuition for the generalization gap of SSS algorithms, consider the experiment where we inject some <em>label noise</em> into our distribution. That is, we corrupt an <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/> fraction of the labels in both the train and test set, replacing them with random labels. Already in the noiseless case (<img alt="\eta=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=0"/>), the generalization gap of SSS algorithms is noticeably smaller than that of end-to-end supervised learning. As we increase the noise, the difference becomes starker. End-to-end supervised learning algorithms can always achieve 100% training accuracy, even as the test accuracy deteriorates, since they can “memorize” all the training labels they are given. In contrast, for SSS algorithms, both training and testing accuracy decrease together as we increase the noise, with training accuracy correlating with test performance. </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif"><img alt="" class="wp-image-7826" src="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif?w=812"/></a><strong>Figure 2:</strong> Generalization gap of end-to-end and SSS algorithms on CIFAR 10 as a function of noise (since there are 10 classes, 90% noisy samples corresponds to the Zhang et al experiment). See also <a href="https://plotly.com/~yaminibansal/1.embed" rel="noreferrer noopener" target="_blank">interactive version</a>.</figure>



<p/>



<p>Our main theoretical result is a formal proof of the above statement. To do so, we consider training with a small amount of label noise (say <img alt="\eta=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D5%5C%25&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=5\%"/>) and define the following quantities:</p>



<ul><li>The <strong>robustness gap</strong> is the amount by which training accuracy degrades between the “clean” (<img alt="\eta=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=0"/>) experiment and the noisy one. (In this and all other quantities, the training accuracy is measured with respect to the original uncorrupted labels.)</li><li>The <strong>memorization gap</strong> considers the noisy experiment (<img alt="\eta=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D5%5C%25&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=5\%"/>) and measures the amount by which performance on the corrupted data samples (where we received the wrong label) is worse than performance on the overall training set. If the algorithm can memorize all given labels, it will be perfectly wrong on the corrupted data samples, leading to a large memorization gap.</li><li>The <strong>rationality gap</strong> is the difference between the performance on the corrupted data samples and performance on unseen test examples. For example, if <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is an image of a dog, then it measures the difference between the probability that <img alt="f(x)=\text{&quot;dog&quot;}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Ctext%7B%22dog%22%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=\text{&quot;dog&quot;}"/> when <img alt="(x,\text{&quot;cat&quot;})" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C%5Ctext%7B%22cat%22%7D%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,\text{&quot;cat&quot;})"/> is in the training set and the probability that <img alt="f(x)=\text{&quot;dog&quot;}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Ctext%7B%22dog%22%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=\text{&quot;dog&quot;}"/> when <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is not in the training set at all. Since intuitively, getting the wrong label should be worse than getting no label at all, we typically expect the rationality gap to be around zero or negative. Formally we define the rationality gap to the maximum between <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> and the difference above, so it is always non-negative. We think of an algorithm with a significant positive rationality gap as “irrational.”</li></ul>



<p>By summing up the quantities above, we get the following inequality, which we call the <strong>RRM bound</strong></p>



<p><em><span class="has-inline-color" style="color: #0693e3;">generalization gap</span></em> <img alt="\leq" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\leq"/> <em><span class="has-inline-color" style="color: #00d084;">robustness gap</span></em> <img alt="+" class="latex" src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="+"/> <em><span class="has-inline-color" style="color: #fcb900;">rationality gap</span></em> <img alt="+" class="latex" src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="+"/> <em><span class="has-inline-color" style="color: #cf2e2e;">memorization gap</span></em></p>



<p>In practice, the <strong>robustness</strong> and <strong>rationality</strong> gaps are always small, both for end-to-end supervised algorithms (which have a large generalization gap), and for SSS algorithms (which have a small generalization gap). Thus the main contribution to the generalization gap comes from the <strong>memorization gap</strong>. Roughly speaking, our main result is the following:</p>



<p><em>If the complexity of the second-stage classifier of an SSS algorithm is smaller than the number of samples then the generalization gap is small.</em></p>



<p>See the <a href="https://arxiv.org/abs/2010.08508">paper</a> for the precise definition of “complexity,” but it is bounded by the number of bits that it takes to describe the simple classifier (no matter how complex is the representation used in the first stage). Our bound yields non-vacuous results in various practical settings; see the figures below or their <a href="https://share.streamlit.io/yaminibansal/streamlit-apps/main/figure1.py" rel="noreferrer noopener" target="_blank">interactive version</a>. </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/intro-cifar.png"><img alt="" class="wp-image-7815" src="https://windowsontheory.files.wordpress.com/2020/10/intro-cifar.png?w=1024"/></a><strong>Figure 3:</strong> Empirical study of the generalization gap of a variety of of SSS algorithms on CIFAR-10. Each vertical line corresponds to one model, sorted by generalization gap. The RRM bound is typically near-tight, and our complexity upper bound is often non vacuous. Use <a href="https://share.streamlit.io/yaminibansal/streamlit-apps/main/figure1.py" rel="noreferrer noopener" target="_blank">this webpage</a> to interact with figures 3 and 4.</figure>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/imagenet_gaps.png"><img alt="" class="wp-image-7817" src="https://windowsontheory.files.wordpress.com/2020/10/imagenet_gaps.png?w=1024"/></a><strong>Figure 4:</strong> Empirical study of gaps for the ImageNet dataset. Because of limited computational resources, we only evaluated the theoretical bound for two models in this dataset.</figure>



<h3>What’s next</h3>



<p>There are still many open questions. Can we prove rigorous bounds on robustness and rationality? We have some preliminary results in the paper, but there is much room for improvement. Similarly, our complexity-based upper bound is far from tight at the moment, though the RRM bound itself is often surprisingly tight. Our work only applies to SSS algorithms, but people have the intuition that even end-to-end supervised learning algorithms implicitly learn a representation. So perhaps these tools can apply to such algorithms as well. As mentioned, we don’t yet have formal definitions for “good representations,” and the choice of the self-supervision task is still somewhat of a “black art” – can we find a more principled approach?</p></div>
    </content>
    <updated>2020-10-19T00:30:40Z</updated>
    <published>2020-10-19T00:30:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-10-21T20:21:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint</id>
    <link href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html" rel="alternate" type="text/html"/>
    <title>Polyhedra without disjoint faces</title>
    <summary>Some research I’ve been doing led me to consider the (prism,\(K_{3,3}\))-minor-free graphs. It’s not always easy to go from forbidden minors to the graphs that forbid them, or vice versa, but in this case I think there’s a nice characterization, which I’m posting here because it doesn’t fit into the research writeup: these are the graphs whose nontrivial triconnected components are \(K_5\), wheel graphs, or the graph \(K_5-e\) of the triangular bipyramid. The illustration below shows an example of a graph with this structure, with its nontrivial triconnected components colored red and yellow. There’s a simpler and more geometric way to say almost the same thing: the only convex polyhedra that do not have two vertex-disjoint faces are the pyramids and the triangular bipyramid.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Some research I’ve been doing led me to consider the (prism,\(K_{3,3}\))-minor-free graphs. It’s not always easy to go from <a href="https://en.wikipedia.org/wiki/Forbidden_graph_characterization">forbidden minors</a> to the graphs that forbid them, or vice versa, but in this case I think there’s a nice characterization, which I’m posting here because it doesn’t fit into the research writeup: these are the graphs whose nontrivial triconnected components are \(K_5\), <a href="https://en.wikipedia.org/wiki/Wheel_graph">wheel graphs</a>, or the graph \(K_5-e\) of the <a href="https://en.wikipedia.org/wiki/Triangular_bipyramid">triangular bipyramid</a>. The illustration below shows an example of a graph with this structure, with its nontrivial triconnected components colored red and yellow. There’s a simpler and more geometric way to say almost the same thing: the only convex polyhedra that do not have two vertex-disjoint faces are the pyramids and the triangular bipyramid.</p>

<p style="text-align: center;"><img alt="A (prism, K_{3,3})-minor-free graph, with its nontrivial triconnected components colored red and yellow" src="https://11011110.github.io/blog/assets/2020/prism-k33-free.svg"/></p>

<p>Some definitions:</p>

<ul>
  <li>
    <p>Here by the prism graph I mean the graph of the triangular prism. Any other prism has this one as a minor, and so is irrelevant as a forbidden minor. However, the pyramids in this structure can have any polygon as their base, corresponding to wheel graphs with arbitrarily many vertices.</p>
  </li>
  <li>
    <p>\(K_{3,3}\) is a complete bipartite graph with three vertices on each side of its bipartition, famous as the <a href="https://en.wikipedia.org/wiki/Three_utilities_problem">utility graph</a>, one of the two forbidden minors for planar graphs. The triangular prism graph and \(K_{3,3}\) are the only two <a href="https://en.wikipedia.org/wiki/Cubic_graph">3-regular graphs</a> with six vertices.</p>
  </li>
</ul>

<p style="text-align: center;"><img alt="The prism graph and K_{3,3}" src="https://11011110.github.io/blog/assets/2020/prism-k33.svg"/></p>

<ul>
  <li>
    <p>The triconnected components of a graph are the graphs associated with the nodes of its <a href="https://en.wikipedia.org/wiki/SPQR_tree">SPQR tree</a>, or of the SPQR trees of its biconnected components. These are cycle graphs, dipole multigraphs, or 3-connected graphs, and by “nontrivial” I mean the ones that are not cycles or dipoles. A triconnected component might not be a subgraph of the given graph, because it can have additional edges that correspond to paths in the given graph. For instance, subdividing the edges of any graph into paths, or more generally replacing edges by arbitrary series-parallel graphs, does not change its set of nontrivial triconnected components.</p>
  </li>
  <li>
    <p>I’m using “face” in the usual three-dimensional meaning, a two-dimensional subset of the boundary of the polyhedron. For higher-dimensional polytopes, “face” has a different meaning that also includes vertices and edges, and “facet” would be used to refer to the \((d-1)\)-dimensional faces, but using that terminology seems overly pedantic here.</p>
  </li>
</ul>

<p>Sketch of proof of the characterization of polyhedra without two disjoint faces: Consider any polyhedron without disjoint faces. If one face shares an edge with all the others, it’s a <a href="https://en.wikipedia.org/wiki/Halin_graph">Halin graph</a>, a graph formed by linking the leaves of a tree into a cycle; if the tree is a star, it’s a pyramid, and otherwise contracting all but one of the interior edges of the tree, and then all but four of the cycle edges, will produce a prism minor. In the remaining case, some two faces share only a vertex \(v\), which must have degree four or more. Each face that is disjoint from \(v\) must touch all that faces incident to \(v\), which can only happen when there is one face disjoint from \(v\) (a pyramid) or two faces disjoint from \(v\), neither of which has an edge disjoint from the other one (a bipyramid).</p>

<p>Sketch of a lemma that every convex polyhedron with two disjoint faces has a prism minor: glue a pyramidal cap into each of the two faces, producing a larger convex polyhedron which by either <a href="https://en.wikipedia.org/wiki/Steinitz%27s_theorem">Steinitz’s theorem</a> or <a href="https://en.wikipedia.org/wiki/Balinski%27s_theorem">Balinski’s theorem</a> is necessarily 3-connected, and find three vertex-disjoint paths between the apexes of the attached pyramids. The parts of these paths outside the two glued pyramids, together with the boundaries of the two faces, form a subdivision of a prism.</p>

<p>Sketch of proof of the characterization of (prism,\(K_{3,3}\))-minor-free graphs: The nontrivial triconnected components are exactly the maximal triconnected minors of the given graph, so if either of the two triconnected forbidden minors is to be found in the given graph, it will be found in one of the triconnected components. \(K_5\) and the triangular bipyramid are too small to have one of the forbidden minors. The only 3-connected minors of the pyramid graphs are smaller pyramids, obtained by contracting one of the cycle edges of the pyramid, so these also do not have a forbidden minor. Therefore the graphs of the stated form are all (prism,\(K_{3,3}\))-minor-free.</p>

<p>In the other direction, suppose that a graph is (prism,\(K_{3,3}\))-minor-free.
Each triconnected component is a minor, so it must also be (prism,\(K_{3,3}\))-minor-free. What can these components look like? Forbidding \(K_{3,3}\) as a minor rules out nonplanar components other than \(K_5\), by a theorem of Wagner<sup id="fnref:wagner"><a class="footnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fn:wagner">1</a></sup> and Hall.<sup id="fnref:hall"><a class="footnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fn:hall">2</a></sup> So the remaining components that we need to consider are triconnected planar graphs with no prism minor. These cannot have two disjoint faces by the lemma, and so they can only be pyramids or the triangular bipyramid.</p>

<div class="footnotes">
  <ol>
    <li id="fn:wagner">
      <p>K. Wagner. Über eine Erweiterung des Satzes von Kuratowski. <em>Deutsche Mathematik</em>, 2:280–285, 1937. <a class="reversefootnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fnref:wagner">↩</a></p>
    </li>
    <li id="fn:hall">
      <p>D. W. Hall. A note on primitive skew curves. <em>Bulletin of the American Mathematical Society</em>, 49(12):935–936, 1943. <a href="https://doi.org/10.1090/ S0002-9904-1943-08065-2">doi:10.1090/ S0002-9904-1943-08065-2</a>. <a class="reversefootnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fnref:hall">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/105058649830809584">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-10-18T17:06:00Z</updated>
    <published>2020-10-18T17:06:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-10-20T05:33:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/155</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/155" rel="alternate" type="text/html"/>
    <title>TR20-155 |  Log-rank and lifting for AND-functions | 

	Sam McGuire, 

	Shachar Lovett, 

	Alexander Knop, 

	Weiqiang Yuan</title>
    <summary>Let $f: \{0,1\}^n \to \{0, 1\}$ be a boolean function, and let $f_\land (x, y) = f(x \land y)$ denote the AND-function of $f$, where $x \land y$ denotes bit-wise AND. We study the deterministic communication complexity of $f_\land$ and show that, up to a $\log n$ factor, it is bounded by a polynomial in the logarithm of the real rank of the communication matrix of $f_\land$. This comes within a $\log n$ factor of establishing the log-rank conjecture for AND-functions with no assumptions on $f$. Our result stands in contrast with previous results on special cases of the log-rank 
conjecture, which needed significant restrictions on $f$ such as monotonicity or low $\mathbb{F}_2$-degree. Our techniques can also be used to prove (within a $\log n$ factor) a lifting theorem for AND-functions, stating that the deterministic communication complexity of $f_\land$ is polynomially-related to the AND-decision tree complexity of $f$.

The results rely on a new structural result regarding boolean functions $f:\{0, 1\}^n \to \{0, 1\}$ with a sparse polynomial representation, which may be of independent interest. We show that if the polynomial computing $f$ has few monomials then the set system of the monomials has a small hitting set, of size poly-logarithmic in its sparsity. We also establish extensions of this result to multi-linear polynomials $f:\{0,1\}^n \to \mathbb{R}$ with a larger range.</summary>
    <updated>2020-10-18T14:44:03Z</updated>
    <published>2020-10-18T14:44:03Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-21T20:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=493</id>
    <link href="https://tcsplus.wordpress.com/2020/10/16/tcs-talk-wednesday-october-21-aayush-jain-ucla/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 21 — Aayush Jain, UCLA</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Aayush Jain from UCLA will speak about “Indistinguishability Obfuscation from Well-Founded Assumptions” (abstract below). You can reserve a spot as an individual or a group to join us […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Aayush Jain</strong> from UCLA will speak about “<em>Indistinguishability Obfuscation from Well-Founded Assumptions</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We present a construction of an indistinguishability obfuscation scheme, whose security rests on the subexponential hardness of four well-founded assumptions. We show the existence of an indistinguishability Obfuscation scheme for all circuits assuming sub-exponential security of the following assumptions:</p>



<ul class="wp-block-quote"><li>The Learning with Errors (LWE) assumption with arbitrarily small subexponential modulus-to-noise ratio,</li><li>The SXDH assumption with respect to bilinear groups of prime order <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="p"/>,</li><li>Existence of a Boolean Pseudorandom Generator (PRG) in <img alt="\mathsf{NC}^0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathsf{NC}^0"/> with arbitrary polynomial stretch, that is, mapping <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n"/> bits to <img alt="n^{1+\tau}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2B%5Ctau%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n^{1+\tau}"/> bits, for any constant \tau&gt;0.</li><li>The Learning Parity with Noise (LPN) assumption over <img alt="\mathbb{Z}_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathbb{Z}_p"/> with error-rate <img alt="\ell^{-\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell%5E%7B-%5Cdelta%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\ell^{-\delta}"/>, where <img alt="\ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\ell"/> is the dimension of the secret and <img alt="\delta&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%3E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\delta&gt;0"/> is an arbitrarily small constant.<br/>Further, assuming only polynomial security of these assumptions, there exists a compact public-key functional encryption scheme for all circuits.</li></ul>



<p class="wp-block-quote">The main technical novelty is the introduction and construction of a cryptographic pseudorandom generator that we call a Structured-Seed PRG (sPRG), assuming LPN over <img alt="\mathbb{Z}_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathbb{Z}_p"/> and PRGs in <img alt="\mathsf{NC}^0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathsf{NC}^0"/>. During the talk, I will discuss how structured seed PRGs have evolved from different notions of novel pseudorandom generators proposed in the past few years, and how an interplay between different areas of theoretical computer science played a major role in providing valuable insights leading to this work. Time permitting, I will go into the details of how to construct sPRGs. <br/><br/>Joint work with Huijia (Rachel) Lin and Amit Sahai</p></div>
    </content>
    <updated>2020-10-16T06:33:00Z</updated>
    <published>2020-10-16T06:33:00Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-10-21T20:21:22Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/10/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Mirzakhani and meanders (\(\mathbb{M}\)). On some more-than-coincidental similarities in formulas found by Mirzakhani for numbers of geodesics on hyperbolic surfaces and by Vincent Delecroix, Elise Goujard, Peter Zograf, and Anton Zorich in a new preprint for numbers of meanders, closed curves with a given number of intersections with a line.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://scilogs.spektrum.de/hlf/mirzakhani-and-meanders/">Mirzakhani and meanders</a> (<a href="https://mathstodon.xyz/@11011110/104963847400612388">\(\mathbb{M}\)</a>). On some more-than-coincidental similarities in formulas found by Mirzakhani for numbers of geodesics on hyperbolic surfaces and by Vincent Delecroix, Elise Goujard, Peter Zograf, and Anton Zorich <a href="https://arxiv.org/abs/1705.05190">in a new preprint</a> for numbers of <a href="https://en.wikipedia.org/wiki/Meander_(mathematics)">meanders</a>, closed curves with a given number of intersections with a line.</p>
  </li>
  <li>
    <p><a href="https://mycqstate.wordpress.com/2020/09/29/it-happens-to-everyonebut-its-not-fun/">Retraction of “a proof of soundness of the Raz-Safra low-degree test against entangled-player strategies, a key ingredient in the proof of the quantum low-degree test, itself a key ingredient in the \(\mathsf{MIP}^*=\mathsf{RE}\) paper”</a> (<a href="https://mathstodon.xyz/@11011110/104969573344196233">\(\mathbb{M}\)</a>). \(\mathsf{MIP}^*=\mathsf{RE}\) is patched and remains believed true but not fully refereed. This post provides a lot more than the standard we-found-a-bug notice: a good description of what happened, what it implies technically, and how it affects the authors and community.</p>
  </li>
  <li>
    <p><a href="https://blogs.lse.ac.uk/impactofsocialsciences/2020/09/30/for-academic-publishing-to-be-trans-inclusive-authors-must-be-allowed-to-retroactively-change-their-names/">For academic publishing to be trans-inclusive, authors must be allowed to retroactively change their names</a> (<a href="https://mathstodon.xyz/@11011110/104972193066839079">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2020/10/03/weekend-reads-unicorn-poo-and-other-fraudulent-covid-19-treatments-disgraced-researchers-and-drug-company-payouts-a-fictional-account-of-real-fraud/">via</a>). I agree — more than once in researching Wikipedia bios I found past publications under deadnames. If the authors prefer this to be better hidden, while continuing to be credited for their past work, we should try to honor that preference.</p>
  </li>
  <li>
    <p>It’s easy to point and laugh at the <a href="https://tex.stackexchange.com/questions/565387/mathbb-r-is-not-showing-in-reference-bibtex">researcher who thought bibtex from Google scholar was usable</a> (<a href="https://mathstodon.xyz/@11011110/104980666583964923">\(\mathbb{M}\)</a>), but their question brings up a more serious question: why is Google’s bibtex so bad? Even the junk I get from <code class="language-plaintext highlighter-rouge">curl -LH "Accept: application/x-bibtex" http://doi.org/...</code> is mostly usable in comparison. I’m tempted to suggest that they go to MathSciNet for the good stuff but I’m worried they won’t have access.</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2020/09/30/ten-kinetic-sculptures-by-anne-lilly.html">Ten kinetic sculptures by Anne Lilly</a> (<a href="https://mathstodon.xyz/@11011110/104988795486796768">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://jix.one/the-assembly-language-of-satisfiability/">The assembly language of satisfiability</a> (<a href="https://mathstodon.xyz/@jix/104971574457861322">\(\mathbb{M}\)</a>). Why Boolean satisfiability is too low-level to work well as a way to express the kind of problems satisfiability-solvers can solve, and how <a href="https://en.wikipedia.org/wiki/Satisfiability_modulo_theories">satisfiability modulo theories</a> can help.</p>
  </li>
  <li>
    <p><a href="https://cp4space.hatsya.com/2020/10/01/subsumptions-of-regular-polytopes/">Which regular polytopes have their vertices a subset of other regular polytopes in the same dimension</a> (<a href="https://mathstodon.xyz/@11011110/104998010300898992">\(\mathbb{M}\)</a>)? We don’t know! The answer is closely connected to the existence of <a href="https://en.wikipedia.org/wiki/Hadamard_matrix">Hadamard matrices</a>, which are famously conjectured to exist in dimensions divisible by four. A solution to the Hadamard matrix existence problem would also solve the polytope problem.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/computer-scientists-break-traveling-salesperson-record-20201008/">Computer scientists break traveling salesperson record</a> (<a href="https://mathstodon.xyz/@11011110/105006269895209659">\(\mathbb{M}\)</a>). I <a href="https://11011110.github.io/blog/2020/07/15/linkage.html">linked to this back in July</a> when <a href="https://arxiv.org/abs/2007.01409">Karlin, Klein, and Gharan’s preprint</a> giving a \((1/2-\varepsilon)\)-approximation to TSP first came out, but now it’s getting wider publicity in <em>Quanta</em>. See also <a href="https://www.sciencenews.org/article/shayan-oveis-gharan-theoretical-computer-scientist-sn-10-scientists-watch">an earlier (paywalled) piece on the same story in <em>ScienceNews</em></a>.</p>
  </li>
  <li>
    <p>Symmetry, quasisymmetry, and kite-rhomb tessellations in the mathematical modeling of virus surface structures: <a href="https://ima.org.uk/721/fighting-infections-with-symmetry/">IMA</a>,
<a href="https://inference-review.com/article/mathematical-virology"><em>Inference</em></a>,
<a href="https://archive.bridgesmathart.org/2018/bridges2018-237.pdf">Bridges</a> (<a href="https://mathstodon.xyz/@11011110/105009372623320055">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://shop.deutschepost.de/freies-quadrat-briefmarke-zu-1-70-eur-10er-bogen">New German postage stamp features the missing square puzzle</a> (<a href="https://muensterland.social/@rgx/105007333917605810">\(\mathbb{M}\)</a>, <a href="https://en.wikipedia.org/wiki/Missing_square_puzzle">see also</a>).</p>
  </li>
  <li>
    <p><a href="https://www.newstatesman.com/international/science-tech/2020/07/ra-fisher-and-science-hatred">R. A. Fisher and the science of hatred</a> (<a href="https://mathstodon.xyz/@11011110/105020588148970072">\(\mathbb{M}\)</a>). If you’ve been wondering why noted academics of yesteryear like <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">R. A. Fisher</a> (a major figure in statistics) and <a href="https://en.wikipedia.org/wiki/David_Starr_Jordan">David Starr Jordan</a> (founding president of Stanford University) have been having their names taken off things lately, the link looks like a good explainer of their views on eugenics, and why those views are now regarded as deeply racist, even for their times.</p>
  </li>
  <li>
    <p><a href="http://hardmath123.github.io/minimal-surface.html">Sol LeWitt and the soapy pit</a> (<a href="https://mathstodon.xyz/@11011110/105023612862469185">\(\mathbb{M}\)</a>, <a href="https://abhikjain360.github.io/2020/08/01/The-186th-Carnival-of-Mathematics.html">via</a>, <a href="https://aperiodical.com/2020/10/carnival-of-mathematics-186/">via2</a>). LeWitt was an artist who in 1974 made a piece exhibiting all of the possible subsets of edges of the cube. The comfortably numbered blog examines what you get if you use these as frames for making soap films.</p>
  </li>
  <li>
    <p><a href="http://landezine.com/index.php/2013/02/funenpark-by-landlab/">Funenpark</a> (<a href="https://mathstodon.xyz/@11011110/105029637909838642">\(\mathbb{M}\)</a>). To be clear, Funenpark is not a fun-park. It is a high-density residential development on former industrial land near Amsterdam. What interests me is their <a href="https://www.flickr.com/photos/shiratski/2242870712/">pentagonal tiles</a>. It’s not one of the <a href="https://en.wikipedia.org/wiki/Pentagonal_tiling">15 monohedral pentagon tilings</a>: the tiles have two shapes, one forming half of a regular hexagon (all angles \(&gt; 60^\circ\)) and another surrounding the hexagons (sharp angle \(= 60^\circ\)). Still, a nice pattern.</p>
  </li>
  <li>
    <p>Sometimes when I’ve been doing big literature searches on jstor (manually clicking on dozens of links because jstor’s search results don’t tell me which book is being reviewed, delayed by maybe a second or so per click so that I don’t get stopped by jstor’s anti-bot filters) I then get locked out of Google Scholar for a day or so on the same IP address because Google thinks I’m a bot. It doesn’t happen when I search Scholar directly. Has anyone else noticed this? Any idea how to avoid it? (<a href="https://mathstodon.xyz/@11011110/105037500352288970">\(\mathbb{M}\)</a>)</p>
  </li>
  <li>
    <p>While I’m linking Dutch pentagonal tiling architecture, here’s <a href="https://www.19hetatelier.nl/nieuws/wiskundige-vijfhoek-op-gevel-basisschool-de-garve-lochem/">an elementary school in Lochem decorated with the Mann–McLoud–Von Derau tile</a> (<a href="https://mathstodon.xyz/@11011110/105042753206122004">\(\mathbb{M}\)</a>, <a href="https://twitter.com/alexvdbrandhof/status/1004661466149085184">via</a>), which in 2015 became the 15th and final Euclidean monohedral pentagonal tile to be found. The link is in Dutch but Google translate works well except at one point: the school’s name, “De Garve”, means “the sheaf”, and the article remarks that this is appropriate for a pattern that looks like ears of corn.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-10-15T22:15:00Z</updated>
    <published>2020-10-15T22:15:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-10-20T05:33:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4895611208507130576</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4895611208507130576/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/50-years-of-pbs.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4895611208507130576" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4895611208507130576" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/50-years-of-pbs.html" rel="alternate" type="text/html"/>
    <title>50 Years of PBS</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Public Broadcasting Service (PBS) launched fifty years ago this month in the United States. The New York Times talks about its <a href="https://www.nytimes.com/2020/10/13/arts/television/pbs-50-anniversary.html">fifty reasons</a> how the network mattered. I'll throw in my thoughts.</p><p>I was just slightly too old for shows like Sesame Street, Electric Company, Mr. Rogers and Zoom, not that that stopped me from watching them. My kids grew up on Barney and Friends. My daughter even had a toy Barney that interacted with the show, which went <a href="https://blog.computationalcomplexity.org/2012/02/barney-evil-dinosaur.html">as well as you'd expect</a>. </p><p>PBS introduced me to those great British TV shows for young nerds like me including Monty Python and Doctor Who. I wasn't into Nova but did watch Carl Sagan's Cosmos religiously in high school.</p><p>My favorite PBS show was the American Experience, short documentaries about US culture. I remember learning about this history of Coney Island and the quiz show scandals before Robert Redford made a movie about it.</p><p>Siskel and Ebert got their start on PBS and became my go to source for movie reviews.</p><p>In 1987 PBS broadcasted Ivy League football games. One Saturday I sat down expecting to watch my alma mater and instead got supreme court hearings. Only on PBS could Cornell football get Borked.</p></div>
    </content>
    <updated>2020-10-15T13:00:00Z</updated>
    <published>2020-10-15T13:00:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-21T14:52:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7800</id>
    <link href="https://windowsontheory.org/2020/10/14/itc-2021-guest-post-by-benny-applebaum/" rel="alternate" type="text/html"/>
    <title>ITC 2021 (guest post by Benny Applebaum)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Following last year’s successful launch, we are happy to announce the second edition of the conference on Information-Theoretic Cryptography (ITC). The call for papers for ITC 2021 is out, and, to cheer you up during lockdowns, we prepared a short theme song https://youtu.be/kZT1icVoTp8   Feel free to add your own verse 😉 The submission deadline is … <a class="more-link" href="https://windowsontheory.org/2020/10/14/itc-2021-guest-post-by-benny-applebaum/">Continue reading <span class="screen-reader-text">ITC 2021 (guest post by Benny Applebaum)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Following last year’s successful launch, we are happy to announce the second edition of the conference on <a href="https://itcrypto.github.io/" rel="noreferrer noopener" target="_blank"><em>Information-Theoretic Cryptography</em></a><em> (ITC)</em>.</p>



<p>The <a href="https://itcrypto.github.io/2021/" rel="noreferrer noopener" target="_blank">call for papers</a> for ITC 2021 is out, and, to cheer you up during lockdowns, we prepared a short theme song <a href="https://youtu.be/kZT1icVoTp8" rel="noreferrer noopener" target="_blank">https://youtu.be/kZT1icVoTp8</a>  </p>



<p>Feel free to add your own verse <img alt="&#x1F609;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" style="height: 1em;"/></p>



<p>The submission deadline is <strong>February 1st</strong>. Please submit your best work to ITC 2021! We hope to see many of you there!</p></div>
    </content>
    <updated>2020-10-14T22:05:24Z</updated>
    <published>2020-10-14T22:05:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-10-21T20:21:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blogs.princeton.edu/imabandit/?p=1437</id>
    <link href="https://blogs.princeton.edu/imabandit/2020/10/13/2020/" rel="alternate" type="text/html"/>
    <title>2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest post on this blog was on December 30th 2019. It seems like a lifetime away. The rate at which paradigm shifting events have been happening in 2020 is staggering. And it might very well be that the worst … <a href="https://blogs.princeton.edu/imabandit/2020/10/13/2020/">Continue reading <span class="meta-nav">→</span></a></p></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest post on this blog was on December 30th 2019. It seems like a lifetime away. The rate at which paradigm shifting events have been happening in 2020 is staggering. And it might very well be that the worst of 2020 is ahead of us, especially for those of us currently in the USA. </p>
<p>When I started communicating online broadly (blog, twitter) I promised myself to keep it strictly about science (or very closely neighboring topics), so the few lines above is all I will say about the current worldwide situation.</p>
<p>In other news, as is evident from the 10 months hiatus in blogging, I have taken elsewhere (at least temporarily) my need for rapid communication about theorems that currently excite me. Namely to <a class="liinternal" href="https://www.youtube.com/sebastienbubeck">youtube</a>. Since the beginning of the pandemic I have been recording home videos of what would have been typically blog posts, with currently 5 such videos:</p>
<ol>
<li><a class="liinternal" href="https://youtu.be/uRarIjJGmhs">A law of robustness for neural networks</a> : I explain the conjecture we recently made that, for random data, any interpolating two-layers neural network must have its Lipschitz constant larger than the squareroot of the ratio between the size of the data set and the number of neurons in the network. This would prove that overparametrization is *necessary* for robustness.</li>
<li><a class="liinternal" href="https://youtu.be/U-XsUB69mvc">Provable limitations of kernel methods</a> : I give the proof by Zeyuan Allen-Zhu and Yuanzhi Li that there are simple noisy learning tasks where *no kernel* can perform well while simple two-steps procedures can learn.</li>
<li><a class="liinternal" href="https://youtu.be/6-GBDpe2kuI">Memorization with small neural networks</a> : I explain old (classical combinatorial) and new (NTK style) construction of optimally-sized interpolating two-layers neural networks.</li>
<li><a class="liinternal" href="https://youtu.be/HIwZH2C--nA">Coordination without communication</a> : This video is the only one in the current series where I don’t talk at all about neural networks. Specifically it is about the cooperative multiplayer multiarmed bandit problem. I explain the strategy we devised with Thomas Budzinski to solve this problem (for the stochastic version) without *any* collision at all between the players.</li>
<li><a class="liinternal" href="https://youtu.be/O84mcq7P_es">Randomized smoothing for certified robustness</a> : Finally, in the first video chronologically, I explain the only known technique for provable robustness guarantees in neural networks that can scale up to large models.</li>
</ol>
<p>The next video will be about basic properties of tensors, and how it can be used for smooth interpolation (in particular in the context of our law of robustness conjecture). After that, we will see, maybe more neural networks, maybe more bandits, maybe some non-convex optimization ….</p>
<p>Stay safe out there!</p><p/></div>
    </content>
    <updated>2020-10-14T03:14:07Z</updated>
    <published>2020-10-14T03:14:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Sebastien Bubeck</name>
    </author>
    <source>
      <id>https://blogs.princeton.edu/imabandit</id>
      <link href="https://blogs.princeton.edu/imabandit/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blogs.princeton.edu/imabandit" rel="alternate" type="text/html"/>
      <subtitle>Random topics in optimization, probability, and statistics. By Sébastien Bubeck</subtitle>
      <title>I’m a bandit</title>
      <updated>2020-10-20T23:33:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5013</id>
    <link href="https://www.scottaaronson.com/blog/?p=5013" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5013#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5013" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Vote in person if you can</title>
    <summary xml:lang="en-US">[If you’re not American, or you’re American but a masochist who enjoys the current nightmare, this post won’t be relevant to you—sorry!] Until recently, this blog had a tagline that included “HOLD THE NOVEMBER US ELECTION BY MAIL.” So I thought I should warn readers that circumstances have changed in ways that have important practical […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="wp-block-image"><img alt="Image may contain: 1 person, eyeglasses and closeup" src="https://scontent.faus1-1.fna.fbcdn.net/v/t1.0-9/121693905_4150116585002989_4795318421355902630_n.jpg?_nc_cat=103&amp;_nc_sid=730e14&amp;_nc_ohc=BM8zCXAwmT4AX_q5CWD&amp;_nc_ht=scontent.faus1-1.fna&amp;oh=9fa3f3ccf4147209ce95a470d3dfef58&amp;oe=5FABBE94"/></figure>



<p><em>[If you’re not American, or you’re American but a masochist who enjoys the current nightmare, this post won’t be relevant to you—sorry!]</em></p>



<p>Until recently, this blog had a tagline that included “HOLD THE NOVEMBER US ELECTION BY MAIL.”  So I thought I should warn readers that circumstances have changed in ways that have important practical implications over the next few weeks.  It’s no longer that we <em>don’t know</em> whether Trump and Pence will acknowledge a <a href="https://projects.fivethirtyeight.com/2020-election-forecast/">likely loss</a>—rather, it’s that <em>we know they won’t</em>.  They were repeatedly asked; we all heard their answers.</p>



<p>That means that the <em>best</em> case, the ideal scenario, is already without precedent in the country’s 240-year history.  It’s a president who never congratulates the winner, who refuses to meet him or coordinate a transfer of power, who skips the inauguration, and who’s basically dragged from the White House on January 20, screaming to his supporters (and continuing to scream until his dying breath) that the election was faked.</p>



<p>As I said, that banana-republic outcome is now the <em>best</em> case.  But it’s also plausible that Trump simply declares himself the winner on election night, because the mail-in votes, urban votes, yet-to-be-counted votes, or any other votes that trend the wrong way are fake; social media and the Murdoch press amplify this fantasy; Trump calls on Republican-controlled state legislatures to set aside the “rigged” results and appoint their own slates of electors; the legislatures dutifully comply; and the Supreme Court A-OKs it all.  If you think none of that could happen, <a href="https://www.theatlantic.com/magazine/archive/2020/11/what-if-trump-refuses-concede/616424/?utm_source=facebook&amp;utm_medium=social&amp;utm_campaign=share&amp;fbclid=IwAR28w8ZJIZSlvqoOT-To3UWgMj4wVc9PkDA7Vbs60S_J7T0kOo1_RyVGe98">read this <em>Atlantic</em> article</a> from a few weeks ago, carefully to the end, and be more terrified than you’ve ever been in your life.  And don’t pretend that you know what would happen next.</p>



<p>I know, I know, I’m mentally ill, it’s Trump Derangement Syndrome, I see Nazis behind every corner just because they killed most of my relatives, a little global pandemic here and economic collapse there and riots and apocalyptic fires and resurgent fascism and I act as though it’s the whole world coming to an end.  A few months from now, after everything has gone swimmingly, this post will still be here and you can come back and tell me how crazy I was.  I accept that risk.</p>



<p>For now, though, the best chance to avert a catastrophe is for Trump not merely to lose, but <strong>lose in a landslide that’s already clear by election night</strong>.  Which means: as Michelle Obama <a href="https://www.vox.com/2020/8/18/21373177/michelle-obama-dnc-2020-speech-voting-post-office">advised</a> already in August, put on your mask, brave the virus, and vote in person if you can—<em>especially</em> if you live in a state that’s in play, and that won’t start tallying mail-in ballots till after election day.  If your state allows it, and if early votes will be counted by election night (check this!), vote early, when the lines are shorter.  That’s what Dana and I did this morning; Texas <a href="https://www.washingtonpost.com/outlook/2020/10/04/joe-biden-win-texas/">going blue</a> on election night would be one dramatic way to foreclose shenanigans.  If you can’t vote in person, or if your state counts mail-in ballots earlier, then vote by mail or drop-box, but <em>do it now</em>, so you have a chance to fix any problems well before Election Day.  (Note that, even in normal circumstances—which these aren’t—a substantial fraction of all mail-in ballots get rejected because of trivial errors.)  I welcome other tips in the comments, from the many readers more immersed in this stuff than I am.</p>



<p>And if this post helped spur you in any way, please say so in the comments.  It will improve my mood, thereby helping me finish my next post, which will be on the Continuum Hypothesis.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> It’s always fascinating to check my comments and see the missives from parallel universes, where Trump is a normal candidate who one might decide to vote for based on normal criteria, rather than what he himself has announced he is: a knife to the entire system that underlies such decisions.  For a view from <em>this</em> universe, see (e.g.) <a href="https://www.nature.com/articles/d41586-020-02852-x?utm_source=twitter&amp;utm_medium=social&amp;utm_content=organic&amp;utm_campaign=NGMT_USG_JC01_GL_Nature&amp;fbclid=IwAR1tLfLbZwXPd6U3a8BnqxvvMxo3SUDPWxhQccXYz9BU9_dL8x8deY3_dRc">today’s <em>Nature</em> editorial</a>.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Another Update:</span></strong> If it allays anyone’s fears, I was pleasantly surprised by the level of pandemic preparedness when Dana and I went to vote.  It was in a huge, cavernous gym on the UT campus, the lines were very short, masks and 6ft distancing were strictly enforced, and finger-coverings and hand sanitizer were offered to everyone.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Unrelated Update (10/16):</span></strong> For those who are interested, here’s a <a href="https://mattasher.com/2020/10/16/scott-aaronson-on-the-hunt-for-real-randomness/">new podcast with me and Matt Asher</a>, where we talk about the use of quantum mechanics (especially Bell inequality violations) to generate certified random numbers.</p></div>
    </content>
    <updated>2020-10-14T01:35:53Z</updated>
    <published>2020-10-14T01:35:53Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-10-16T15:33:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6616178737091923837</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6616178737091923837/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/hugh-woodin-kurt-godel-dwayne-rock.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6616178737091923837" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6616178737091923837" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/hugh-woodin-kurt-godel-dwayne-rock.html" rel="alternate" type="text/html"/>
    <title>Hugh Woodin, Kurt Godel, Dwayne `The Rock' Johnson, Robert De Niro, David Frum, Tom Selleck: Do I care what they think? Should I?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> MATH:</p><p>My last <a href="https://blog.computationalcomplexity.org/2020/10/revisiting-continuum-hypothesis.html">post</a> on CH mentioned that Hugh Woodin used to think NOT(CH) but now thinks CH. In both cases his reasons have some math content to them. Also, note that Hugh Woodin seems to believe that CH somehow HAS an answer. Kurt Godel also thought CH HAS an answer. It has been said that he could have announced  his result that CH is consistent by saying  L is THE model, and the problem is now solved. </p><p>Should we care what Hugh Woodin and Kurt Godel think about CH?</p><p>YES- they have both studied the issue A LOT. If you think CH should have an answer, then surely you would care what they think. </p><p>NO-  CH has no answer so there opinions are no better than mine. If you think CH does not have an answer then you might think this; however, I think you should still be at least INTERESTED in what people who have thought about the problem A LOT have to say, even if you will disagree with them.</p><p>But with MATH there are people who clearly know more than you on topics you care about, so it is worth hearing what they have to say. </p><p>POLITICS:</p><p>Recently Dwayne THE ROCK Johnson (by Wikipedia: actor, producer, businessman, and former professional wrestler) ENDORSED Joe Biden. Should we care about his opinion? Maybe, if wrestling fans and former pro wrestler tend to be Republicans, so this may indicate a shift. I do not know if this is the case. </p><p>Robert De Niro was in favor of impeaching Donald Trump. He also said that Trump was like a Gangster. He would know because he was in the movie GOODFELLOWS and later THE IRISHMAN (about Jimmy Hoffa). To be fair I do not think he said that is how he would know. Even so, I don't think I care what he thinks, unless he has some specialized knowledge I do not know about. </p><p>David Frum is a republican who had a break with the party NOT over Donald Trump, but over Obamacare- which you may recall was originally a CONSERVATIVE response to Hillarycare by the Heritage Foundation.  He has a good article on this <a href="https://www.theatlantic.com/politics/archive/2017/03/the-republican-waterloo/520833/">here</a>. Because he is an intelligent  republican in favor of Obamacare (or some version of it) he is worth listening to.</p><p>In POLITICS its trickier- who is worth listening to and why. For all I know, THE ROCK has made a detailed study of the Republican and Democratic platforms (actually this cannot be true since the Republicans did not have a platform this time). </p><p>COMMERCIALS:</p><p>Tom Selleck (Actor-Magnum PI a while back, Blue Bloods now)  does commercials for reverse mortgages. A while back I asked a group of people WHY he is doing them. Here were some answers and reactions</p><p>a) He needs the money. Not likely, he seems to have done well and does not seem to have the kind of bad habits (e.g., drugs) that need money. Maybe he has expensive tastes (my only expensive tastes is in fine European Kit Kat bars--- which actually are not that expensive). </p><p>b) He likes doing commercials. Maybe.</p><p>c) He believes in the product. At this, everyone cracked up in laughter.</p><p>This raises a more general point: Why does ANYONE believe ANY commercial since we KNOW the actor is being PAID to say it. I ask non rhetorically as always. </p></div>
    </content>
    <updated>2020-10-12T17:49:00Z</updated>
    <published>2020-10-12T17:49:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-21T14:52:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5005</id>
    <link href="https://www.scottaaronson.com/blog/?p=5005" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5005#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5005" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">My second podcast with Lex Fridman</title>
    <summary xml:lang="en-US">Here it is—enjoy! (I strongly recommend listening at 2x speed.) We recorded it a month ago—outdoors (for obvious covid reasons), on a covered balcony in Austin, as it drizzled all around us. Topics included: Whether the universe is a simulation Eugene Goostman, GPT-3, the Turing Test, and consciousness Why I disagree with Integrated Information Theory […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://www.youtube.com/watch?v=nAMjv0NAESM&amp;list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4">Here it is—enjoy!</a>  (I strongly recommend listening at 2x speed.)</p>



<p>We recorded it a month ago—outdoors (for obvious covid reasons), on a covered balcony in Austin, as it drizzled all around us.  Topics included:</p>



<ul><li>Whether the universe is a simulation</li><li>Eugene Goostman, GPT-3, the Turing Test, and consciousness</li><li>Why I disagree with Integrated Information Theory</li><li>Why I disagree with Penrose’s ideas about physics and the mind</li><li>Intro to complexity theory, including P, NP, PSPACE, BQP, and SZK</li><li>The US’s catastrophic failure on covid</li><li>The importance of the election</li><li>My objections to cancel culture</li><li>The role of love in my life (!)</li></ul>



<p>Thanks so much to Lex for his characteristically probing questions, apologies as always for my verbal tics, and here’s our <a href="https://lexfridman.com/scott-aaronson-2/">first podcast</a> for those who missed that one.</p></div>
    </content>
    <updated>2020-10-12T14:38:07Z</updated>
    <published>2020-10-12T14:38:07Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Metaphysical Spouting"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-10-16T15:33:42Z</updated>
    </source>
  </entry>
</feed>
