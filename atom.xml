<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-08-15T06:22:35Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.05766</id>
    <link href="http://arxiv.org/abs/2108.05766" rel="alternate" type="text/html"/>
    <title>Fast Approximation of Persistence Diagrams with Guarantees</title>
    <feedworld_mtime>1628899200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vidal:Jules.html">Jules Vidal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tierny:Julien.html">Julien Tierny</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.05766">PDF</a><br/><b>Abstract: </b>This paper presents an algorithm for the efficient approximation of the
saddle-extremum persistence diagram of a scalar field. Vidal et al. introduced
recently a fast algorithm for such an approximation (by interrupting a
progressive computation framework). However, no theoretical guarantee was
provided regarding its approximation quality. In this work, we revisit the
progressive framework of Vidal et al. and we introduce in contrast a novel
approximation algorithm, with a user controlled approximation error,
specifically, on the Bottleneck distance to the exact solution. Our approach is
based on a hierarchical representation of the input data, and relies on local
simplifications of the scalar field to accelerate the computation, while
maintaining a controlled bound on the output error. The locality of our
approach enables further speedups thanks to shared memory parallelism.
Experiments conducted on real life datasets show that for a mild error
tolerance (5% relative Bottleneck distance), our approach improves runtime
performance by 18% on average (and up to 48% on large, noisy datasets) in
comparison to standard, exact, publicly available implementations. In addition
to the strong guarantees on its approximation error, we show that our algorithm
also provides in practice outputs which are on average 5 times more accurate
(in terms of the L2-Wasserstein distance) than a naive approximation baseline.
We illustrate the utility of our approach for interactive data exploration and
we document visualization strategies for conveying the uncertainty related to
our approximations.
</p></div>
    </summary>
    <updated>2021-08-14T22:51:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-08-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.05697</id>
    <link href="http://arxiv.org/abs/2108.05697" rel="alternate" type="text/html"/>
    <title>Local Correlation Clustering with Asymmetric Classification Errors</title>
    <feedworld_mtime>1628899200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jafarov:Jafar.html">Jafar Jafarov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalhan:Sanchit.html">Sanchit Kalhan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makarychev:Konstantin.html">Konstantin Makarychev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makarychev:Yury.html">Yury Makarychev</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.05697">PDF</a><br/><b>Abstract: </b>In the Correlation Clustering problem, we are given a complete weighted graph
$G$ with its edges labeled as "similar" and "dissimilar" by a noisy binary
classifier. For a clustering $\mathcal{C}$ of graph $G$, a similar edge is in
disagreement with $\mathcal{C}$, if its endpoints belong to distinct clusters;
and a dissimilar edge is in disagreement with $\mathcal{C}$ if its endpoints
belong to the same cluster. The disagreements vector, $\text{dis}$, is a vector
indexed by the vertices of $G$ such that the $v$-th coordinate $\text{dis}_v$
equals the weight of all disagreeing edges incident on $v$. The goal is to
produce a clustering that minimizes the $\ell_p$ norm of the disagreements
vector for $p\geq 1$. We study the $\ell_p$ objective in Correlation Clustering
under the following assumption: Every similar edge has weight in the range of
$[\alpha\mathbf{w},\mathbf{w}]$ and every dissimilar edge has weight at least
$\alpha\mathbf{w}$ (where $\alpha \leq 1$ and $\mathbf{w}&gt;0$ is a scaling
parameter). We give an
$O\left((\frac{1}{\alpha})^{\frac{1}{2}-\frac{1}{2p}}\cdot
\log\frac{1}{\alpha}\right)$ approximation algorithm for this problem.
Furthermore, we show an almost matching convex programming integrality gap.
</p></div>
    </summary>
    <updated>2021-08-14T22:49:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.05696</id>
    <link href="http://arxiv.org/abs/2108.05696" rel="alternate" type="text/html"/>
    <title>Correlation Clustering with Asymmetric Classification Errors</title>
    <feedworld_mtime>1628899200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jafarov:Jafar.html">Jafar Jafarov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalhan:Sanchit.html">Sanchit Kalhan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makarychev:Konstantin.html">Konstantin Makarychev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makarychev:Yury.html">Yury Makarychev</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.05696">PDF</a><br/><b>Abstract: </b>In the Correlation Clustering problem, we are given a weighted graph $G$ with
its edges labeled as "similar" or "dissimilar" by a binary classifier. The goal
is to produce a clustering that minimizes the weight of "disagreements": the
sum of the weights of "similar" edges across clusters and "dissimilar" edges
within clusters. We study the correlation clustering problem under the
following assumption: Every "similar" edge $e$ has weight
$\mathbf{w}_e\in[\alpha \mathbf{w}, \mathbf{w}]$ and every "dissimilar" edge
$e$ has weight $\mathbf{w}_e\geq \alpha \mathbf{w}$ (where $\alpha\leq 1$ and
$\mathbf{w}&gt;0$ is a scaling parameter). We give a $(3 + 2 \log_e (1/\alpha))$
approximation algorithm for this problem. This assumption captures well the
scenario when classification errors are asymmetric. Additionally, we show an
asymptotically matching Linear Programming integrality gap of $\Omega(\log
1/\alpha)$.
</p></div>
    </summary>
    <updated>2021-08-14T22:48:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.05666</id>
    <link href="http://arxiv.org/abs/2108.05666" rel="alternate" type="text/html"/>
    <title>A coefficient related to splay-to-root traversal, correct to thousands of decimal places</title>
    <feedworld_mtime>1628899200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uacute=nlaing:Colm_=Oacute=.html">Colm Ó Dúnlaing</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.05666">PDF</a><br/><b>Abstract: </b>This paper takes another look at the cost of traversing a binary tree using
repeated splay-to-root. This was shown to cost $O(n)$ (in rotations) by Tarjan
and later, in different ways, by Elmasry and others.
</p>
<p>It would be interesting to know the minimal possible coefficient implied by
the $O(n)$ cost; call this coefficient $\beta$. In this paper we define a
related coefficient $\alpha$ describing the cost of splay-to-root traversal on
maximal (i.e., complete) binary trees, and show that $\beta \geq 2 + \alpha$.
We give the first 3009 digits of $\alpha$, including the decimal point, and
show that every digit is correct.
</p>
<p>We make two conjectures: first, that $\beta = 2 + \alpha$, and second, that
$\alpha$ is irrational.
</p></div>
    </summary>
    <updated>2021-08-14T22:48:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.05611</id>
    <link href="http://arxiv.org/abs/2108.05611" rel="alternate" type="text/html"/>
    <title>Grounded L-graphs are polynomially $\chi$-bounded</title>
    <feedworld_mtime>1628899200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davies:James.html">James Davies</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krawczyk:Tomasz.html">Tomasz Krawczyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McCarty:Rose.html">Rose McCarty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Walczak:Bartosz.html">Bartosz Walczak</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.05611">PDF</a><br/><b>Abstract: </b>A grounded L-graph is the intersection graph of a collection of "L" shapes
whose topmost points belong to a common horizontal line. We prove that every
grounded L-graph with clique number $\omega$ has chromatic number at most
$17\omega^4$. This improves the doubly-exponential bound of McGuinness and
generalizes the recent result that the class of circle graphs is polynomially
$\chi$-bounded. We also survey $\chi$-boundedness problems for grounded
geometric intersection graphs and give a high-level overview of recent
techniques to obtain polynomial bounds.
</p></div>
    </summary>
    <updated>2021-08-14T22:51:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-08-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.05581</id>
    <link href="http://arxiv.org/abs/2108.05581" rel="alternate" type="text/html"/>
    <title>On the Fine-Grained Complexity of the Unbounded SubsetSum and the Frobenius Problem</title>
    <feedworld_mtime>1628899200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klein:Kim=Manuel.html">Kim-Manuel Klein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.05581">PDF</a><br/><b>Abstract: </b>Consider positive integral solutions $x \in \mathbb{Z}^{n+1}$ to the equation
$a_0 x_0 + \ldots + a_n x_n = t$. In the so called unbounded subset sum
problem, the objective is to decide whether such a solution exists, whereas in
the Frobenius problem, the objective is to compute the largest $t$ such that
there is no such solution.
</p>
<p>In this paper we study the algorithmic complexity of the unbounded subset
sum, the Frobenius problem and a generalization of the problems. More
precisely, we study pseudo-polynomial time algorithms with a running time that
depends on the smallest number $a_0$ or respectively the largest number $a_n$.
For the parameter $a_0$, we show that all considered problems are
subquadratically equivalent to $(min,+)$-convolution, a fundamental algorithmic
problem from the area of fine-grained complexity. By this equivalence, we
obtain hardness results for the considered problems (based on the assumption
that an algorithm with a subquadratic running time for $(min,+)$-convolution
does not exist) as well as algorithms with improved running time. The proof for
the equivalence makes use of structural properties of solutions, a technique
that was developed in the area of integer programming.
</p>
<p>In case of the complexity of the problems parameterized by $a_n$, we present
improved algorithms. For example we give a quasi linear time algorithm for the
Frobenius problem as well as a hardness result based on the strong exponential
time hypothesis.
</p></div>
    </summary>
    <updated>2021-08-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.05495</id>
    <link href="http://arxiv.org/abs/2108.05495" rel="alternate" type="text/html"/>
    <title>Space-Efficient Huffman Codes Revisited</title>
    <feedworld_mtime>1628899200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grabowski:Szymon.html">Szymon Grabowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=ouml=ppl:Dominik.html">Dominik Köppl</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.05495">PDF</a><br/><b>Abstract: </b>Canonical Huffman code is an optimal prefix-free compression code whose
codewords enumerated in the lexicographical order form a list of binary words
in non-decreasing lengths. Gagie et al. (2015) gave a representation of this
coding capable to encode or decode a symbol in constant worst case time. It
uses $\sigma \lg \ell_{\text{max}} + o(\sigma) + O(\ell_{\text{max}}^2)$ bits
of space, where $\sigma$ and $\ell_{\text{max}}$ are the alphabet size and
maximum codeword length, respectively. We refine their representation to reduce
the space complexity to $\sigma \lg \ell_{\text{max}} (1 + o(1))$ bits while
preserving the constant encode and decode times. Our algorithmic idea can be
applied to any canonical code.
</p></div>
    </summary>
    <updated>2021-08-14T22:38:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.05433</id>
    <link href="http://arxiv.org/abs/2108.05433" rel="alternate" type="text/html"/>
    <title>Learning to Hash Robustly, with Guarantees</title>
    <feedworld_mtime>1628899200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Andoni:Alexandr.html">Alexandr Andoni</a>, Daniel Beaglehole <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.05433">PDF</a><br/><b>Abstract: </b>The indexing algorithms for the high-dimensional nearest neighbor search
(NNS) with the best worst-case guarantees are based on the randomized Locality
Sensitive Hashing (LSH), and its derivatives. In practice, many heuristic
approaches exist to "learn" the best indexing method in order to speed-up NNS,
crucially adapting to the structure of the given dataset.
</p>
<p>Oftentimes, these heuristics outperform the LSH-based algorithms on real
datasets, but, almost always, come at the cost of losing the guarantees of
either correctness or robust performance on adversarial queries, or apply to
datasets with an assumed extra structure/model. In this paper, we design an NNS
algorithm for the Hamming space that has worst-case guarantees essentially
matching that of theoretical algorithms, while optimizing the hashing to the
structure of the dataset (think instance-optimal algorithms) for performance on
the minimum-performing query. We evaluate the algorithm's ability to optimize
for a given dataset both theoretically and practically. On the theoretical
side, we exhibit a natural setting (dataset model) where our algorithm is much
better than the standard theoretical one. On the practical side, we run
experiments that show that our algorithm has a 1.8x and 2.1x better recall on
the worst-performing queries to the MNIST and ImageNet datasets.
</p></div>
    </summary>
    <updated>2021-08-14T22:48:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.02197</id>
    <link href="http://arxiv.org/abs/2108.02197" rel="alternate" type="text/html"/>
    <title>Singularly Near Optimal Leader Election in Asynchronous Networks</title>
    <feedworld_mtime>1628899200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kutten:Shay.html">Shay Kutten</a>, William K. Moses Jr., <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pandurangan:Gopal.html">Gopal Pandurangan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peleg:David.html">David Peleg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.02197">PDF</a><br/><b>Abstract: </b>This paper concerns designing distributed algorithms that are {\em singularly
optimal}, i.e., algorithms that are {\em simultaneously} time and message {\em
optimal}, for the fundamental leader election problem in {\em asynchronous}
networks.
</p>
<p>Kutten et al. (JACM 2015) presented a singularly near optimal randomized
leader election algorithm for general {\em synchronous} networks that ran in
$O(D)$ time and used $O(m \log n)$ messages (where $D$, $m$, and $n$ are the
network's diameter, number of edges and number of nodes, respectively) with
high probability.\footnote{Throughout, "with high probability" means "with
probability at least $1-1/n^c$, for constant $c$."} Both bounds are near
optimal (up to a logarithmic factor), since $\Omega(D)$ and $\Omega(m)$ are the
respective lower bounds for time and messages for leader election even for
synchronous networks and even for (Monte-Carlo) randomized algorithms. On the
other hand, for general asynchronous networks, leader election algorithms are
only known that are either time or message optimal, but not both. Kutten et al.
(DISC 2020) presented a randomized asynchronous leader election algorithm that
is singularly near optimal for \emph{complete networks}, but left open the
problem for general networks.
</p>
<p>This paper shows that singularly near optimal (up to polylogarithmic factors)
bounds can be achieved for general {\em asynchronous} networks. We present a
randomized singularly near optimal leader election algorithm that runs in $O(D
+ \log^2n)$ time and $O(m\log^2 n)$ messages with high probability. Our result
is the first known distributed leader election algorithm for asynchronous
networks that is near optimal with respect to both time and message complexity
and improves over a long line of results including the classical results of
Gallager et al. (ACM TOPLAS, 1983), Peleg (JPDC, 1989), and Awerbuch (STOC 89).
</p></div>
    </summary>
    <updated>2021-08-14T22:38:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2762</id>
    <link href="https://theorydish.blog/2021/08/13/random-2021-starts-on-monday/" rel="alternate" type="text/html"/>
    <title>RANDOM 2021 Starts on Monday</title>
    <summary>Call for Participation: APPROX/RANDOM 2021 The 25th International Workshop on Randomization and Computation (RANDOM 2021) and the 24th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2021) will be starting on Monday August 16!  The conferences will be held as parallel virtual conferences, August 16-18, 2021. RANDOM 2021 focuses on applications of randomness to computational and combinatorial problems while APPROX 2021 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems. To learn more about the conferences and program, visit: APPROX: https://approxconference.wordpress.com/approx-2021/RANDOM: https://randomconference.com/random-2021-home/ In addition to an exciting program of live talks and discussion (to complement pre-recorded talks), the conference will feature two invited talks, by Jelani Nelson (UC Berkeley) and Vera Traub (ETH Zurich), as well as a social event with trivia and a cartoon caption contest! Registration is only $10 for general audience members.  You can register here: https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience Hope to see you at the conference!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Call for Participation: APPROX/RANDOM 2021</p>



<p>The 25th International Workshop on Randomization and Computation (RANDOM 2021) and the 24th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2021) will be starting on Monday August 16!  The conferences will be held as parallel virtual conferences, August 16-18, 2021. RANDOM 2021 focuses on applications of randomness to computational and combinatorial problems while APPROX 2021 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems.</p>



<p>To learn more about the conferences and program, visit:</p>



<p>APPROX: <a href="https://approxconference.wordpress.com/approx-2021/" rel="noreferrer noopener" target="_blank">https://approxconference.wordpress.com/approx-2021/</a><br/>RANDOM: <a href="https://randomconference.com/random-2021-home/" rel="noreferrer noopener" target="_blank">https://randomconference.com/random-2021-home/</a></p>



<p>In addition to an exciting program of live talks and discussion (to complement pre-recorded talks), the conference will feature two invited talks, by Jelani Nelson (UC Berkeley) and Vera Traub (ETH Zurich), as well as a social event with trivia and a cartoon caption contest!</p>



<p>Registration is only $10 for general audience members.  You can register here: <a href="https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience" rel="noreferrer noopener" target="_blank">https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience</a></p>



<p>Hope to see you at the conference!</p></div>
    </content>
    <updated>2021-08-13T23:18:23Z</updated>
    <published>2021-08-13T23:18:23Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-08-15T06:22:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5730</id>
    <link href="https://www.scottaaronson.com/blog/?p=5730" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5730#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5730" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Stephen Wiesner (1942-2021)</title>
    <summary xml:lang="en-US">These have not been an auspicious few weeks for Jewish-American-born theoretical physicists named Steve who made epochal contributions to human knowledge in the late 1960s, and who I had the privilege to get to know a bit when they were old. This morning, my friend and colleague Or Sattath brought me the terrible news that […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p/>



<p/>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" src="https://www.scottaaronson.com/wiesner-sm.jpg"/>Photo credit: Lev Vaidman</figure></div>



<p>These have not been an auspicious few weeks for Jewish-American-born theoretical physicists named Steve who made epochal contributions to human knowledge in the late 1960s, and who I had the privilege to get to know a bit when they were old.</p>



<p>This morning, my friend and colleague <a href="https://orsattath.wordpress.com/about/">Or Sattath</a> brought me the terrible news that <a href="https://en.wikipedia.org/wiki/Stephen_Wiesner">Stephen Wiesner</a> has passed away in Israel.  [Because people have asked: I’ve now also heard directly from Wiesner’s daughter Sarah.]</p>



<p>Decades ago, Wiesner left academia, embraced Orthodox Judaism, moved from the US to Israel, and took up work there as a construction laborer—believing (or so he told me) that manual labor was good for the soul.  In the late 1960s, however, Wiesner was still a graduate student in physics at Columbia University, when he wrote <a href="http://users.cms.caltech.edu/~vidick/teaching/120_qcrypto/wiesner.pdf">Conjugate Coding</a>: arguably the foundational document of the entire field of quantum information science.  <s>Famously, this paper was so far ahead of its time that it was rejected over and over from journals, taking nearly 15 years to get published.</s>  (Fascinatingly, Gilles Brassard tells me that this isn’t true: it was rejected <em>once</em>, from IEEE Transactions on Information Theory, and then Wiesner simply shelved it.)  When it finally appeared, in 1983, it was in <em><a href="https://dl.acm.org/newsletter/sigact">SIGACT News</a></em>—a venue that I know and love, where I’ve published too, but that’s more like the house newsletter for theoretical computer scientists than an academic journal.</p>



<p>But it didn’t matter.  By the early 1980s, Wiesner’s ideas had been successfully communicated to <a href="https://en.wikipedia.org/wiki/Charles_H._Bennett_(physicist)">Charlie Bennett</a> and <a href="https://en.wikipedia.org/wiki/Gilles_Brassard">Gilles Brassard</a>, who refashioned them into the first scheme for <a href="https://en.wikipedia.org/wiki/Quantum_key_distribution">quantum key distribution</a>—what we now call <a href="https://en.wikipedia.org/wiki/BB84">BB84</a>.  Even as Bennett and Brassard received scientific acclaim for the invention of quantum cryptography—including, a few years ago, the <a href="https://en.wikipedia.org/wiki/Wolf_Prize">Wolf Prize</a> (often considered second only to the Nobel Prize), at a ceremony in the Knesset in Jerusalem that I attended—the two B’s were always careful to acknowledge their massive intellectual debt to Steve Wiesner.</p>



<hr class="wp-block-separator"/>



<p>Let me explain what Wiesner does in the Conjugate Coding paper.  As far as I know, this is the first paper ever to propose that quantum information—what Wiesner called “polarized light” or “spin-1/2 particles” but we now simply call <a href="https://en.wikipedia.org/wiki/Qubit">qubits</a>—works differently than classical bits, in ways that could <em>actually be useful</em> for achieving cryptographic tasks that are impossible in a classical world.  What could enable these cryptographic applications, wrote Wiesner, is the fact that there’s no physical means for an attacker or eavesdropper to <em>copy</em> an unknown qubit, to produce a second qubit in the same quantum state.  This observation—now called the <a href="https://en.wikipedia.org/wiki/No-cloning_theorem">No-Cloning Theorem</a>—would only be named and published in 1982, but Wiesner treats it in his late-1960s manuscript as just obvious background.</p>



<p>Wiesner went further than these general ideas, though, to propose an explicit scheme for <a href="https://en.wikipedia.org/wiki/Quantum_money">quantum money</a> that would be physically impossible to counterfeit—a scheme that’s still of enormous interest half a century later (I teach it every year in my <a href="https://www.scottaaronson.com/qclec.pdf">undergraduate course</a>).  In what we now call the Wiesner money scheme, a central bank prints “quantum bills,” each of which contains a classical serial number as well as a long string of qubits.  Each qubit is prepared in one of four possible quantum states:</p>



<ul><li>|0⟩,</li><li>|1⟩,</li><li>|+⟩ = (|0⟩+|1⟩)/√2, or</li><li>|-⟩ = (|0⟩-|1⟩)/√2.</li></ul>



<p>The bank, in a central database, stores the serial number of every bill in circulation, as well as the preparation instructions for each of the bill’s qubits.  If you want to <em>verify</em> a bill as genuine—this, as Wiesner knew, is the big drawback—you have to bring it back to the bank.  The bank, using its secret knowledge of how each qubit was prepared, measures each qubit in the appropriate basis—the {<span style="font-size: revert; color: initial;">|0⟩,|1⟩</span>} basis for <span style="font-size: revert; color: initial;">|0⟩ or |1⟩</span> qubits, the {<span style="font-size: revert; color: initial;">|+⟩,|-⟩</span>} basis for <span style="font-size: revert; color: initial;">|+⟩ or |-⟩</span> qubits—and checks that it gets the expected outcomes.  If even one qubit yields the wrong outcome, the bill is rejected as counterfeit.</p>



<p>Now consider the situation of a counterfeiter, who holds a quantum bill but lacks access to the bank’s secret database.  When the counterfeiter tries to copy the bill, they won’t know the right basis in which to measure each qubit—and if they make the wrong choice, then it’s not only that they fail to make a copy; it’s that the measurement destroys even the <em>original</em> copy!  For example, measuring a <span style="font-size: revert; color: initial;">|+⟩ or |-⟩</span> qubit in the {<span style="font-size: revert; color: initial;">|0⟩,|1⟩</span>} basis will randomly collapse the qubit to either <span style="font-size: revert; color: initial;">|0⟩ or |1⟩</span>—so that, when the bank later measures the same qubit in the correct {<span style="font-size: revert; color: initial;">|+⟩,|-⟩</span>} basis, it will see the wrong outcome, and realize that the bill has been compromised, with 1/2 probability (with the probability increasing to nearly 1 as we repeat across hundreds or thousands of qubits).</p>



<p>Admittedly, the handwavy argument above, which Wiesner offered, is far from a security proof by cryptographers’ standards.  In 2011, I <a href="https://cstheory.stackexchange.com/questions/11363/rigorous-security-proof-for-wiesners-quantum-money">pointed that out on StackExchange</a>.  My post, I’m happy to say, spurred Molina, Vidick, and Watrous to write a <a href="https://arxiv.org/abs/1202.4010">beautiful 2012 paper</a>, where they rigorously proved for the first time that in Wiesner’s money scheme, no counterfeiter consistent with the laws of quantum mechanics can turn a single n-qubit bill into two bills that both pass the bank’s verification with success probability greater than (3/4)<sup>n</sup> (and this is tight).  But the intuition was already clear enough to Wiesner in the 1960s.</p>



<p>In 2003—when I was already a PhD student in quantum information, but incredibly, had never heard of Stephen Wiesner or his role in founding my field—I rediscovered the idea of quantum states |ψ⟩ that you could store, measure, and feed into a quantum computer, but that would be <em>usefully uncopyable</em>.  (My main interest was in whether you could create “unpiratable quantum software programs.”)  Only in 2006, at the University of Waterloo, did Michele Mosca and his students make the connection for me to quantum money, Stephen Wiesner, and his Conjugate Coding paper, which I then read with amazement—along with a <a href="https://static.aminer.org/pdf/PDF/000/120/546/quantum_cryptography_or_unforgeable_subway_tokens.pdf">comparably amazing followup work</a> by Bennett, Brassard, Breidbart, and Wiesner.</p>



<p>But it was clear that there was still a great deal to do.  Besides unpiratable software, Wiesner and his collaborators had lacked the tools in the early 1980s seriously to tackle the problem of secure quantum money that <em>anybody</em> could verify, not only the bank that had created the money.  I realized that, if such a thing was possible at all, then just like unpiratable software, it would require cryptographic hardness assumptions, a restriction to polynomial-time counterfeiters, and (hence) ideas from quantum computational complexity.  The No-Cloning Theorem couldn’t do the job on its own.</p>



<p>That realization led to my 2009 paper <a href="https://arxiv.org/abs/1110.5353">Quantum Copy-Protection and Quantum Money</a>, and from there, to the “modern renaissance” of Wiesner’s old idea of quantum money, with well over a hundred papers (e.g., <a href="https://arxiv.org/abs/1203.4740">my 2012 paper with Paul Christiano</a>, Farhi et al.’s <a href="https://arxiv.org/abs/0912.3823">quantum state restoration paper</a>, their <a href="https://arxiv.org/abs/1004.5127">quantum money from knots paper</a>, Mark Zhandry’s 2017 <a href="https://arxiv.org/abs/1711.02276">quantum lightning paper</a>, Dmitry Gavinsky’s <a href="https://arxiv.org/abs/1109.0372">improvement of Wiesner’s scheme</a> wherein the money is verified by classical communication with the bank, Broduch et al.’s <a href="https://arxiv.org/abs/1404.1507">adaptive attack</a> on Wiesner’s original scheme, my <a href="https://arxiv.org/abs/1711.01053">shadow tomography paper</a> proving the necessity for the bank to keep a giant database in information-theoretic quantum money schemes like Wiesner’s, Daniel Kane’s <a href="https://arxiv.org/abs/1809.05925">strange scheme based on modular forms</a>…).  The purpose of many of these papers was either to break the quantum money schemes proposed in previous papers, or to patch the schemes that were previously broken.</p>



<p>After all this back-and-forth, spanning more than a decade, I’d say that Wiesner’s old idea of quantum money is now in good enough theoretical shape that the main obstacle to its practical realization is merely the “engineering difficulty”—namely, how to get the qubits in a bill, sitting in your pocket or whatever, to maintain their quantum coherence for more than a few nanoseconds!  (Or possibly a few hours, if you’re willing to schlep a cryogenic freezer everywhere you go.)  It’s precisely because quantum key distribution doesn’t have this storage problem—because there the qubits are simply sent across a channel and then immediately measured on arrival—that QKD is actually practical today, although the market for it has proven to be extremely limited so far.</p>



<p>In the meantime, while the world waits for the quantum error-correction that could keep qubits alive indefinitely, there’s Bitcoin.  The latter perversely illustrates just how immense the demand for quantum money might someday be: the staggering lengths to which people will go, diverting the electricity to power whole nations into mining rigs, to get around our current inability to realize Wiesner’s elegant quantum-mechanical solution to the same problem.  When I first learned about Bitcoin, shortly after its invention, it was in the context of: “here’s something I’d better bring up in my lectures on quantum money, in order to explain how much better WiesnerCoin could eventually be, when it’s the year 2200 or whatever and we all have quantum computers wired up by a quantum Internet!”  It never occurred to me that I should forget about the year 2200, liquidate my life savings, and immediately buy up all the Bitcoin I could.</p>



<hr class="wp-block-separator"/>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" src="https://www.scottaaronson.com/wiesnaar-sm.jpg"/>Photo credit: Or Sattath</figure></div>



<p>In his decades as a construction laborer, Wiesner had (as far as I know) no Internet presence; many of my colleagues didn’t even realize he was still alive.  Even then, though, Wiesner never turned his back <em>so</em> far on his previous life, his academic life, that the quantum information faculty at Hebrew University in Jerusalem couldn’t entice him to participate in some seminars there.  Those seminars are where I had the privilege to meet and talk to him several times over the last decade.  He was thoughtful and kind, listening with interest as I told him how I and others were trying to take quantum money into the modern era by making it publicly verifiable.</p>



<p>I also vividly remember a conversation in 2013 where Steve shared his fears about the American physics establishment and military-industrial complex, and passionately urged me to</p>



<ol><li>quit academia and get a “real job,” and</li><li>flee the US immediately and move my family to Israel, because of a wave of fascism and antisemitism that was about to sweep the US, just like with Germany in the 1930s.</li></ol>



<p>I politely nodded along, pointing out that my Israeli wife and I had considered living in Israel but the job opportunities were better in US, silently wondering when Steve had gone <em>completely</em> off his rocker.  Today, Steve’s urgent warning about an impending fascist takeover of the US seems … uh, <em>slightly</em> less crazy than in 2013?  Maybe, just like with quantum money, Wiesner was simply too far ahead of his time to sound sane.</p>



<p>Wiesner also talked to me about his father, <a href="https://en.wikipedia.org/wiki/Jerome_Wiesner">Jerome Wiesner</a>, who was a legendary president of MIT—still spoken about in reverent tones when I taught there—as well as the chief science advisor to John F. Kennedy.  One of JFK’s most famous decisions was to override the elder Wiesner’s fervent opposition to sending humans to the moon (Wiesner thought it a waste of money, as robots could do the same science for vastly cheaper).</p>



<p>While I don’t know all the details (I hope someone someday researches it and writes a book), Steve Wiesner made it clear to me that he did not get along with his famous father <em>at all</em>—in fact they became estranged.  Steve told me that his embrace of Orthodox Judaism was, at least in part, a reaction against everything his father had stood for, including militant scientific atheism.  I suppose that in the 1960s, millions of young Americans defied their parents via sex, drugs, and acoustic guitar; only a tiny number did so by donning <a href="https://en.wikipedia.org/wiki/Tzitzit">tzitzit</a> and moving to Israel to pray and toil with their hands.  The two groups of rebels did, however, share a tendency to grow long beards.</p>



<p>Wiesner leaves behind a daughter, Sarah, who I haven’t yet had the honor to meet.  I’ll say more about his family if they choose to share with me.</p>



<p>Wiesner’s unique, remarkable, <em>uncloneable</em> life trajectory raises the question: who are the young Stephen Wiesners of our time?  Will we be faster to recognize their foresight than Wiesner’s contemporaries were to recognize his?</p>



<hr class="wp-block-separator"/>



<p>Feel free to share any other memories of Stephen Wiesner or his influence in the comments.</p>



<hr class="wp-block-separator"/>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Aug. 14):</span></strong> See also <a href="https://orsattath.wordpress.com/2021/08/14/stephen-wiesner/">Or Sattath’s memorial post</a>, which (among other things) points out something that my narrative missed: namely, besides quantum money, Wiesner <em>also</em> invented <a href="https://en.wikipedia.org/wiki/Superdense_coding">superdense coding</a> in 1970, although he and Bennett only published the idea 22 years later (!).</p>



<p>And I have more photos!  Here’s <a href="https://www.scottaaronson.com/wiesner2.jpg">Wiesner with an invention of his</a> and <a href="https://www.scottaaronson.com/wiesner3.jpg">another photo</a> (thanks to his daughter Sarah).  Here’s <a href="https://www.scottaaronson.com/wiesner1970.jpg">another photo from 1970</a> and <a href="https://www.scottaaronson.com/wiesnernotes1970.jpg">Charlie Bennett’s handwritten notes</a> (!) after first meeting Wiesner in 1970 (thanks to Charlie Bennett).</p></div>
    </content>
    <updated>2021-08-13T20:54:29Z</updated>
    <published>2021-08-13T20:54:29Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-08-15T01:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/119</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/119" rel="alternate" type="text/html"/>
    <title>TR21-119 |  Visible Rank and Codes with Locality | 

	Omar Alrabiah, 

	Venkatesan Guruswami</title>
    <summary>We propose a framework to study the effect of local recovery requirements of codeword symbols on the dimension of linear codes, based on a combinatorial proxy that we call "visible rank." The locality constraints of a linear code are stipulated by a matrix $H$ of $\star$'s and $0$'s (which we call a "stencil"), whose rows correspond to the local parity checks (with the $\star$'s indicating the support of the check). The visible rank of $H$ is the largest $r$ for which there is a $r \times r$ submatrix in $H$ with a unique generalized diagonal of $\star$'s. The visible rank yields a field-independent combinatorial lower bound on the rank of $H$ and thus the co-dimension of the code. 

We point out connections of the visible rank to other notions in the literature such as unique restricted graph matchings, matroids, spanoids, and min-rank. In particular, we prove a rank-nullity type theorem relating visible rank to the rank of an associated construct called symmetric spanoid, which was introduced by Dvir, Gopi, Gu, and Wigderson [DGWW]. Using this connection and a construction of appropriate stencils, we answer a question posed in [DGGW] and demonstrate that symmetric spanoid rank cannot improve the currently best known $\widetilde{O}(n^{(q-2)/(q-1)})$ upper bound on the dimension of $q$-query locally correctable codes (LCCs) of length $n$. This also pins down the efficacy of visible rank as a proxy for the dimension of LCCs. 

We also study the $t$-Disjoint Repair Group Property ($t$-DRGP) of codes where each codeword symbol must belong to $t$ disjoint check equations. It is known that  linear codes with $2$-DRGP must have co-dimension $\Omega(\sqrt{n})$ (which is matched by a simple product code construction). We show that there are stencils corresponding to $2$-DRGP with visible rank as small as $O(\log n)$. However, we show the second tensor of any $2$-DRGP stencil has visible rank $\Omega(n)$, thus recovering the $\Omega(\sqrt{n})$ lower bound for $2$-DRGP. For $q$-LCC, however, the $k$'th tensor power for $k\le n^{o(1)}$ is unable to improve the $\widetilde{O}(n^{(q-2)/(q-1)})$ upper bound on the dimension of $q$-LCCs by a polynomial factor. Inspired by this and as a notion of intrinsic interest, we define the notion of visible capacity of a stencil as the limiting visible rank of high tensor powers, analogous to Shannon capacity, and pose the question whether there can be large gaps between visible capacity and algebraic rank.</summary>
    <updated>2021-08-13T14:04:34Z</updated>
    <published>2021-08-13T14:04:34Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-15T06:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/118</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/118" rel="alternate" type="text/html"/>
    <title>TR21-118 |  Efficient multivariate low-degree tests via interactive oracle proofs of proximity for polynomial codes | 

	Sarah Bordage, 

	Daniel Augot, 

	Jade Nardi</title>
    <summary>We consider the proximity testing problem for error-correcting codes which consist in evaluations of multivariate polynomials either of bounded individual degree or bounded total degree. Namely, given an
oracle function $f : L^m \rightarrow \mathbb F_q$, where $L\subset \mathbb F_q$, a verifier     distinguishes whether $f$ is the evaluation of a low-degree polynomial or is far (in relative Hamming distance) from being one, by making only a few queries to $f$.  This topic has been studied in the context of locally testable codes, interactive proofs, probabilistically checkable proofs, and interactive oracle proofs.
We present the first interactive oracle proofs of proximity (IOPP) for tensor products of Reed-Solomon codes (evaluation of polynomials with bounds on individual degrees) and for Reed-Muller codes (evaluation of polynomials with a bound on the total
degree).
        
Such low-degree polynomials play a central role in constructions of probabilistic proof systems and succinct non-interactive arguments of knowledge with zero-knowledge. For these applications, highly-efficient multivariate low-degree tests are
desired, but prior probabilistic proofs of proximity required super-linear proving time. In contrast, for multivariate codes of length $N$, our constructions admit a prover running in time linear in $N$ and a verifier which is logarithmic in $N$.
        
      For fixed constant  number of variables $m$, the efficiency parameters of our IOPPs for multivariate codes compare well, all things equal,  with those of the IOPP for Reed-Solomon codes of [Ben-Sasson et al., ICALP 2018] from which they are directly inspired.</summary>
    <updated>2021-08-13T13:52:54Z</updated>
    <published>2021-08-13T13:52:54Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-15T06:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/117</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/117" rel="alternate" type="text/html"/>
    <title>TR21-117 |  Simplicity Meets Near-Optimal Rate: Non-malleable Codes and Non-malleable Two-source Extractors via Rate Boosters | 

	Divesh Aggarwal, 

	Bhavana Kanukurthi, 

	SaiLakshmiBhavana Obbattu, 

	Maciej Obremski, 

	Sruthi Sekar</title>
    <summary>At ITCS 2010, Dziembowski, Pietrzak, and Wichs introduced Non-malleable Codes (NMCs). Non-malleability is one of the strongest and most challenging notions of security considered in cryptography and protects against tampering attacks. In the context of coding schemes, non-malleability requires that it be infeasible to tamper the codeword of a message into the codeword of a related message. A natural and well-studied model of tampering is the $2$-split-state model where the codeword consists of two independently tamperable states. As with standard error-correcting codes, it is of great importance to build codes with high rates. Cheraghchi and Guruswami (ITCS 2014) showed that one cannot obtain NMCs in the $2$-split state model with a rate better than $1/2$.  Since its inception, this area has witnessed huge strides leading to the construction of a constant-rate NMC in the $2$-split state model due to Aggarwal and Obremski (FOCS 2020). However, the rate of this construction -- roughly $1/1,500,000$ -- is nowhere close to the best achievable rate of $1/2$! In this work, we dramatically improve this status quo by building a rate booster that converts any augmented non-malleable code into an augmented non-malleable code with a rate of $1/3$. Using similar, but simpler techniques we also obtain rate boosters that convert any unbalanced (with sources of unequal length) non-malleable $2$-source extractor into an unbalanced non-malleable $2$-source extractor with rate $1/2$.
  
The beauty of our construction lies in its simplicity. In particular, if we apply our rate booster to the non-malleable code construction by Aggarwal, Dodis, and Lovett (STOC 2014), then all we need is one instance of the inner-product extractor, one instance of a seeded extractor, and an affine-evasive function for the construction to work.

Further, as an application of our $1/3$-rate augmented NMC (which we also prove to be leakage resilient), we give an extremely simple computational binding and statistical hiding non-malleable commitment scheme using only standard assumptions, and with a communication cost of $41$ times the length of the message to be committed, which is optimal up to a constant factor.</summary>
    <updated>2021-08-13T13:49:56Z</updated>
    <published>2021-08-13T13:49:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-15T06:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/08/13/postdoc-at-unsw-sydney-apply-by-september-13-2021/</id>
    <link href="https://cstheory-jobs.org/2021/08/13/postdoc-at-unsw-sydney-apply-by-september-13-2021/" rel="alternate" type="text/html"/>
    <title>postdoc at UNSW Sydney (apply by September 13, 2021)</title>
    <summary>A 2-year postdoc position is available at UNSW Sydney. The position is part of a new research project on “Improved algorithms via random sampling” (Serge Gaspers, Fedor Fomin, Daniel Lokshtanov) funded by the Australian Research Council. It revolves around the design and analysis of parameterized, moderately exponential, randomized, and approximation algorithms for graph problems. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A 2-year postdoc position is available at UNSW Sydney.</p>
<p>The position is part of a new research project on “Improved algorithms via random sampling” (Serge Gaspers, Fedor Fomin, Daniel Lokshtanov) funded by the Australian Research Council. It revolves around the design and analysis of parameterized, moderately exponential, randomized, and approximation algorithms for graph problems.</p>
<p>Website: <a href="https://www.cse.unsw.edu.au/~sergeg/contact.html#postdocs">https://www.cse.unsw.edu.au/~sergeg/contact.html#postdocs</a><br/>
Email: serge.gaspers@unsw.edu.au</p></div>
    </content>
    <updated>2021-08-13T05:13:27Z</updated>
    <published>2021-08-13T05:13:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-08-15T06:20:57Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2021/08/13/relative-risk/</id>
    <link href="http://benjamin-recht.github.io/2021/08/13/relative-risk/" rel="alternate" type="text/html"/>
    <title>Relative risk is more informative than effectiveness.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The past few weeks have sent a tremendous amount of disappointing news about the Delta variant and the possible waning effectiveness of vaccines. <a href="https://www.washingtonpost.com/health/2021/07/30/provincetown-covid-outbreak-vaccinated/">An outbreak among partygoers in Provincetown found a high number of infected individuals who had already been fully vaccinated.</a> <a href="https://nymag.com/intelligencer/2021/08/breakthrough-covid-19-cases-may-be-a-bigger-problem.html">News stories and anecdotes about “breakthrough infections” abound.</a> <a href="https://apnews.com/article/health-coronavirus-pandemic-79959d313428d98ab8aa905bbe287ba0">And the CDC has confusingly made an about face on its recommendations about face coverings for the vaccinated.</a> This has fed a growing anxiety that the vaccines have not delivered what was promised.</p>

<p>The Pfizer and Moderna vaccine, <a href="https://www.pfizer.com/news/press-release/press-release-detail/pfizer-and-biontech-conclude-phase-3-study-covid-19-vaccine">heralded as “95% effective”</a>, led many to hope that they could eradicate this coronavirus. However, and unfortunately, from the beginning, there has been a lack of clarity among not only the media but among health professionals themselves around what “effectiveness” means. Indeed, <a href="https://www.npr.org/sections/goatsandsoda/2021/08/11/1026190062/covid-delta-variant-transmission-cdc-chickenpox">misunderstandings of vaccine effectiveness persist in media reporting on the pandemic</a>.</p>

<p>Pfizer ran a randomized control trial where they took a diverse pool of sixty thousand people and randomly gave half of the people the vaccine and half a placebo injection with no effect at all. Effectiveness measures the percent reduction of infection in the vaccinated group when compared against the control group. A vaccine effectiveness of 95% means that Pfizer observed 95% fewer infections amongst the vaccinated than the unvaccinated.</p>

<p>An identical way of stating the result of the trial is that 20 times as many people who received the placebo developed symptomatic COVID-19 as those that received the vaccine. In the language of risk, we would say that the vaccinated group had 20 times less risk of symptomatic COVID-19 infection than the control group.</p>

<p>While “95% effective” is numerically the same as “20-fold risk reduction,” it leads to more confusion. It is far too common for “95% effective” to be incorrectly interpreted as a 5% chance of getting infected at all, which is not the case. If the message had been that vaccines reduce risk by a factor of 20, then we would be equipped to both better understand the power of these vaccines and better plan for moving into an open, but increasingly vaccinated world.</p>

<p>20-fold risk reduction allows one to consider their everyday experiences and make judgement calls. Hanging out with friends outside was already very low risk. If everyone is vaccinated, the risk after vaccination becomes effectively zero. On the other hand, going to a packed, sweaty dance club and sharing drinks with strangers was very high risk before vaccination, so vaccination makes it 20 times less risky but still risky.</p>

<p>The Delta variant itself is more infectious than previous strains and can partially evade immunity. This further reduces vaccine effectiveness. But, like with the clubbing example, behavioral changes also change effectiveness. The virus is not the same today as it was in the Fall of 2020, but we are also not the same people.</p>

<p>Clinical studies are done in the best of circumstances, and most medical professionals expect the effectiveness to decrease in the general population. Scientists do their best to ensure that the population of individuals in a study are representative of all of the people in the world. They perform complex matching and outreach to ensure characteristic diversity in the pool of subjects. But one of the hardest factors to control for is the psychological and behavioral changes in the broader population over time.</p>

<p>In particular, people are taking more risks now than they did in 2020, and the risk of infection has increased for all, vaccinated or not. Just imagine you get vaccinated and then increase your risk by a factor of four. For example, you stop staying at home on Zoom all day, and go back to the office, bars, and restaurants. You start meeting with all sorts of people you don’t know very well. Then your risk of infection is now a fifth, not a twentieth of what it was. And further imagine that a new variant comes along that is twice as infectious as the old one. Now your risk reduction is down to 2.5, which would amount to a total “effectiveness”—combining the vaccine, your behavior, and the variant—of only 60%. But 60% is still better than most seasonal flu shots.</p>

<p>Perhaps this is the hardest part about where we are in the pandemic. Everyone wants to return to normal and never think about coronaviruses again. But a <a href="https://www.newyorker.com/science/annals-of-medicine/coexisting-with-the-coronavirus">preponderance of evidence indicates that SARS-CoV-2 will become endemic and will circulate like other common viruses</a>. It is likely that all of us will become immune to COVID-19 in one way or another, though, because of the vaccines, the disease will cause much less death and suffering than it did before. But even with such dramatic reductions in mortality, any COVID-19-associated deaths in the United States feel like too many for a society that has been terrorized and torn apart by the pandemic.</p>

<p>And given that children do not yet have a vaccine available, parents worry that their children remain at risk. But we accept much more deadly risks in our lives. It is uncomfortable to accept that a 3 year old has a similar risk of severe COVID-19 as a vaccinated 40 year old. Doing a cost benefit analysis about your child is emotionally impossible. I understand how any risk, no matter how small, can feel intolerable. Unfortunately, that same 3 year old is <a href="https://www.nytimes.com/2021/04/22/opinion/covid-vaccine-kids.html">more vulnerable to death or serious injury by driving, swimming, or even eating</a>. The risks entailed with COVID-19 do not seem to be much different than those of just growing up. Since we want nothing more than for our kids to be safe, we delude ourselves by never conceptualizing the risks of their ordinary activities. We live our lives as though these risks are zero. Though it seems impossible to imagine this now, in time, we’ll accept the remaining risk from COVID-19 as well.</p>

<p><em>Many thanks to Sarah Dean, Jordan Ellenberg, Eric Jonas, Lauren Kroiz, Deb Raji, Lawrence Recht, Chris Re, and Isaac Sparks for reading drafts of this post and offering insightful comments and suggestions.</em></p></div>
    </summary>
    <updated>2021-08-13T00:00:00Z</updated>
    <published>2021-08-13T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2021-08-14T22:54:05Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4087249606769943482</id>
    <link href="http://blog.computationalcomplexity.org/feeds/4087249606769943482/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/recognizing-faces.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4087249606769943482" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4087249606769943482" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/recognizing-faces.html" rel="alternate" type="text/html"/>
    <title>Recognizing Faces</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I sometimes have trouble recognizing faces, matching faces to people I've interacted with in the past. It's not a disease like <a href="https://en.wikipedia.org/wiki/Prosopagnosia">prosopagnosia</a>, I can certainly tell the difference between faces and have no trouble with people I work with directly. But if I haven't seen someone in a while, I may not recognize them or confuse them for someone else. It's especially bad out of context, say running into a professor in my campus on the streets of Frankfurt. It's gotten worse with age but I've had challenges my whole life.</p><p>I have my coping mechanisms. I start a conversation to get enough clues to figure out who I'm talking to. I'll google an image before I'm supposed to meet someone I haven't seen in a while. Sometimes I'll just say "Remind me how to pronounce your name again". Sometimes I'll just say something embarrassing thinking the person I'm talking to is someone else.</p><p>Name tags are useful, if it isn't obvious you are looking at them. Zoom has been great--everyone's name is just there. I worry that 18 months of zoom meetings means I've lost much of my coping ability, much the way I can no longer navigate by maps the way I used to.</p><p>We have technological solutions but mostly unable to make use of them. Through the magic of machine learning, computers have gotten extremely good at recognizing faces. Nevertheless Google Googles actively prevented their one killer app, telling you who you were looking at, for privacy reasons. Perhaps they could limit it to people in your contacts with pictures you uploaded. It would only recognize people you already know.</p><p>I know I'm not alone, and I'm writing this post so others won't feel alone. And next time you see me and I look confused, remind me of your name.</p></div>
    </content>
    <updated>2021-08-12T14:16:00Z</updated>
    <published>2021-08-12T14:16:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-15T01:18:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8184</id>
    <link href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/" rel="alternate" type="text/html"/>
    <title>Replica Method for the Machine Learning Theorist: Part 2 of 2</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Blake Bordelon, Haozhe Shan, Abdul Canatar, Boaz Barak, Cengiz Pehlevan See part 1 of this series, and pdf version of both parts. See also all seminar posts. In the previous post we described the outline of the replica method, and outlined the analysis per this figure: Specifically, we reduced the task of evaluating the expectation … <a class="more-link" href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/">Continue reading <span class="screen-reader-text">Replica Method for the Machine Learning Theorist: Part 2 of 2</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h4>Blake Bordelon, Haozhe Shan, Abdul Canatar, Boaz Barak, Cengiz Pehlevan</h4>



<p>See <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-1-of-2/">part 1</a> of this series, and <a href="https://boazbarak.org/Papers/replica.pdf">pdf version of both parts</a>. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a>.</p>



<p>In the previous post we described the outline of the replica method, and outlined the analysis per this figure:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/eVOI8EA.png"/></figure>



<p>Specifically, we reduced the task of evaluating the expectation of a (potentially modified) log partition function to evaluating expectation over <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> replicas which in turn amount to</p>



<p><img alt="\left&lt; \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) \right) \right&gt;_{\mathcal D} = \int\exp(-nN \mathcal{F}(Q))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D+%3D+%5Cint%5Cexp%28-nN+%5Cmathcal%7BF%7D%28Q%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix of _overlaps_ (inner products between pairs of replicas), and <img alt="\mathcal{F}(Q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D%28Q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is some nice analytical function depending on <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the log probability obtaining this overlap <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Then since this is an integral of exponentials, it turns out to be dominated by the maximizer <img alt="Q^\star" class="latex" src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, arriving at</p>



<p><img alt="\lim_{N \rightarrow \infty}\left&lt; Z^n \right&gt; = \exp(-n N \mathcal F(Q^\star ))." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN+%5Crightarrow+%5Cinfty%7D%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cexp%28-n+N+%5Cmathcal+F%28Q%5E%5Cstar+%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>reducing our desired quantity <img alt="&lt;-\log Z&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%3C-%5Clog+Z%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="N \mathcal F(Q^\star)" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Cmathcal+F%28Q%5E%5Cstar%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and so if we’re lucky, all we need to do is run Mathemtica or Sympy to find the maximizer of <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> over the space of all?? (not really: see below) <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrices.</p>



<h2>IV. Constraints on <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>: Replica Symmetry and Replica Symmetry Breaking</h2>



<p>Unfortunately, the description of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> above is a gross simplification. <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> cannot be any matrix — it needs to satisfy particular constraints. In particular, it cannot be a matrix that appears with probability tending to zero with <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the overlap matrix of a tuple of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> replicas from the Gibbs distribution.<br/>Hence, we need to understand the space of potential matrices <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that could arise from the probability distribution, and <img alt="Q^\star" class="latex" src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the global minimum under these constraints.</p>



<p>The most important constraint on <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the <b>replica symmetry</b> (RS), or the lack thereof (<b>replica symmetry breaking</b>, or <b>RSB</b>). Recall that <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> encodes the overlap between <img alt="\{x^{(a)}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bx%5E%7B%28a%29%7D%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where each element is a Gibbs random variable. On a high level, the structure of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> describes the geometry of the Gibbs distribution. An in depth description of the relationship between the two is beyond the scope of this post (check out <a href="http://michel.talagrand.net/challenge/volume1.pdf">Mean Field Models for Spin Glasses</a> by Michel Talagrand). We will give some intuitions that apply in the zero-temperature limit.</p>



<h3>A. What is the symmetry ansatz and when is it a good idea?</h3>



<p>The <b>replica symmetric ansatz</b> studies the following special form of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix</p>



<p><img alt="Q_{ab} = (q-q_0) \delta_{ab} + q_0" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%28q-q_0%29+%5Cdelta_%7Bab%7D+%2B+q_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\delta_{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the Kroneker delta. In other words, this ansatz corresponds to the guess that if we pick <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> random replicas <img alt="x^{(1)},\ldots, x^{(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2C+x%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then they will satisfy that <img alt="\| x^{(a)} \|^2 \approx q" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7C+x%5E%7B%28a%29%7D+%5C%7C%5E2+%5Capprox+q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for all <img alt="a=1,\ldots n" class="latex" src="https://s0.wp.com/latex.php?latex=a%3D1%2C%5Cldots+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and <img alt="x^{(a)} \cdot x^{(b)} \approx q_0" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%28a%29%7D+%5Ccdot+x%5E%7B%28b%29%7D+%5Capprox+q_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="a \neq b" class="latex" src="https://s0.wp.com/latex.php?latex=a+%5Cneq+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>This ansatz is especially natural for problems with unique minimizers for a fixed problem instance <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>In such a problem we might imagine that the <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> replicas are all random vectors that are have the same correlation with the true minimizer <img alt="x^{(0)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%280%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and since they are random and in high dimension, this correlation explains their correlation with one another (see example below).</p>



<p>&gt;<b>What have we done?</b> It is worthwhile to pause and take stock of what we have done here. We have reduced computing <img alt="\langle \log Z \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> into finding an expression for <img alt="\langle Z^n \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and then reduced this to computing <img alt="\mathbb{E} \exp(- n N \mathcal{F} (Q))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+%5Cexp%28-+n+N+%5Cmathcal%7BF%7D+%28Q%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> whre the expectation is taken over the induced distribution of the <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> overlap matrix. Now for every fixed <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we reduce the task to optimizing over just two parameters <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="q_0" class="latex" src="https://s0.wp.com/latex.php?latex=q_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Once we find the matrix <img alt="Q^\star" class="latex" src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that optimizes this bound, we obtain the desired quantity by taking <img alt="\lim_{n \rightarrow 0} \tfrac{1}{n}\log \exp(-n N \mathcal{F}(Q^\star) = N \mathcal{F}(Q^\star)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Crightarrow+0%7D+%5Ctfrac%7B1%7D%7Bn%7D%5Clog+%5Cexp%28-n+N+%5Cmathcal%7BF%7D%28Q%5E%5Cstar%29+%3D+N+%5Cmathcal%7BF%7D%28Q%5E%5Cstar%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h3>B. An illustration:</h3>



<p>Annealed langevin dynamics on a convex and non-convex objective below illustrate how the geometry of the learning problem influences the structure of the overlap matrix <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h4>A convex problem</h4>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"/>
</div></figure>



<h4>A non-convex problem</h4>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"/>
</div></figure>



<p>We see that even in low-dimensional problems, the structure of the loss landscape influences the resulting <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrices. Since the replica method works only in high dimensions, these animations cannot be taken too seriously as a justification of the symmetry ansatz, but below we discuss in what kinds of models we could expect the symmetry ansatz to be a good idea.</p>



<h3>C. Replica Symmetry in High Dimensions</h3>



<p>We will now discuss a simple model where the replica symmetry ansatz is especially natural. For a fixed problem instance <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, suppose that the <img alt="x_a" class="latex" src="https://s0.wp.com/latex.php?latex=x_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> vectors are distributed in a point cloud about some mean vector <img alt="\mu \in\mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p><img alt="x_a = \mu + \epsilon_a" class="latex" src="https://s0.wp.com/latex.php?latex=x_a+%3D+%5Cmu+%2B+%5Cepsilon_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\epsilon_a" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are zero-mean noise independently sampled across different replicas with covariance <img alt="\left&lt; \epsilon_{a,i} \epsilon_{b,j} \right&gt; = \frac{\sigma^2}{N} \delta_{ab}\delta_{ij}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cepsilon_%7Ba%2Ci%7D+%5Cepsilon_%7Bb%2Cj%7D+%5Cright%3E+%3D+%5Cfrac%7B%5Csigma%5E2%7D%7BN%7D+%5Cdelta_%7Bab%7D%5Cdelta_%7Bij%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This is equivalent to stipulating a Gibbs measure with energy <img alt="\beta H(x) = - \log p_{\epsilon}(x-\mu)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+H%28x%29+%3D+-+%5Clog+p_%7B%5Cepsilon%7D%28x-%5Cmu%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="p_\epsilon(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cepsilon%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the distribution of each noise variable. In this case, the <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix has elements</p>



<p><img alt="Q_{ab} = |\mu|^2 + \mu^\top \epsilon_a + \mu^\top \epsilon_b + \epsilon_a^\top \epsilon_b" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%7C%5Cmu%7C%5E2+%2B+%5Cmu%5E%5Ctop+%5Cepsilon_a+%2B+%5Cmu%5E%5Ctop+%5Cepsilon_b+%2B+%5Cepsilon_a%5E%5Ctop+%5Cepsilon_b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>By the central limit theorem, these sums of independently sampled random variables are approximately Gaussian (remember <img alt="N \to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>), so we can estimate how <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> behaves in the large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit</p>



<p><img alt="\left&lt; Q_{ab} \right&gt; = |\mu|^2 + \sigma^2 \delta_{ab} \ , \ \text{Var} \ Q_{ab} = O(1/N)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Q_%7Bab%7D+%5Cright%3E+%3D+%7C%5Cmu%7C%5E2+%2B+%5Csigma%5E2+%5Cdelta_%7Bab%7D+%5C+%2C+%5C+%5Ctext%7BVar%7D+%5C+Q_%7Bab%7D+%3D+O%281%2FN%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This implies that in the thermodynamic <img alt="N\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit, the <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix concentrates around a replica symmetric structure. Note that the emergence of this RS structure relied on the fact that high dimensional random vectors are approximately orthogonal. For many supervised learning problems such as least squares fitting, this toy model is actually relevant by specifically taking <img alt="\epsilon \sim \mathcal N(0,\sigma^2/N)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Csim+%5Cmathcal+N%280%2C%5Csigma%5E2%2FN%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h2>V. Example Simple Problem: Learning Curve Phase Transition in Least Squares Fitting</h2>



<p>To show these tools in action we will first study the simplest possible example with that has an interesting outcome. We will study the generalization performance of ridge regression on Gaussian distributed random features. In particular we will study a thermodynamic limit where the number of samples <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the number of features <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are both tending to infinity <img alt="P,N \to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=P%2CN+%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> but with finite ratio <img alt="\alpha = P/N" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+P%2FN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We will observe a phase transition the point <img alt="\alpha = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where the learning problem transitions from over-parameterized (<img alt="P &lt; N" class="latex" src="https://s0.wp.com/latex.php?latex=P+%3C+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) to under-parameterized (<img alt="P&gt;N" class="latex" src="https://s0.wp.com/latex.php?latex=P%3EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>). In the presence of noise this leads to an overfitting peak which can be eliminated through explicit regularization.</p>



<h3>A. Some References</h3>



<p><a href="https://iopscience.iop.org/article/10.1088/0305-4470/22/12/016">Hertz, Krogh, and Thorbergsson</a> first studied this problem and noted the phase transition at <img alt="\alpha = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. <a href="https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031034">Advani and Ganguli</a> examine this model as a special case of M-estimation. Analysis of this model can also be obtained as a special case of kernel regression, for which general learning curves were obtained by <a href="https://arxiv.org/abs/2006.13198">Canatar, Bordelon, and Pehlevan</a> with the replica method. Similar overfitting peaks were recently observed in nonlinear two layer neural networks by <a href="https://www.pnas.org/content/116/32/15849">Belkin, Hsu, Ma, and Mandal</a> and modeled with the replica method by <a href="http://proceedings.mlr.press/v119/d-ascoli20a.html">d’Ascoli, Refinetti, Biroli, and, Krzakala </a> allowing them to <a href="https://arxiv.org/abs/2006.03509">clarify the two possible types of overfitting peaks</a> in random feature models. This problem can also be studied with tools from random matrix theory as in the work of <a href="https://arxiv.org/abs/1908.05355">Mei and Montanari</a> and several others.</p>



<h3>B. Problem Setup</h3>



<p>Our problem instance is a dataset <img alt="\mathcal D = \{ (x^\mu, y^\mu) \}_{\mu=1}^P" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D+%3D+%5C%7B+%28x%5E%5Cmu%2C+y%5E%5Cmu%29+%5C%7D_%7B%5Cmu%3D1%7D%5EP&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with <img alt="x^\mu \in \mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> drawn i.i.d. from a Gaussian distribution <img alt="x^\mu_k \sim \mathcal N(0,1/N )" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%5Cmu_k+%5Csim+%5Cmathcal+N%280%2C1%2FN+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The target values <img alt="y^\mu" class="latex" src="https://s0.wp.com/latex.php?latex=y%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are generated by a noisy linear teacher</p>



<p><img alt="y^\mu = \sum_{k=1}^N w_k^* x_k^\mu + \sigma \epsilon^\mu" class="latex" src="https://s0.wp.com/latex.php?latex=y%5E%5Cmu+%3D+%5Csum_%7Bk%3D1%7D%5EN+w_k%5E%2A+x_k%5E%5Cmu+%2B+%5Csigma+%5Cepsilon%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="||w^*||^2 = N" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%7Cw%5E%2A%7C%7C%5E2+%3D+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and noise is Gaussian distributed <img alt="\epsilon^\mu \sim \mathcal N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%5Cmu+%5Csim+%5Cmathcal+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We will compute, not the generalization error for a particular problem instance <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, but the <i>average</i> performance over random datasets! The energy function we study is the ridge regression loss function</p>



<p><img alt="H(w,\mathcal D) = \frac{1}{\lambda} \sum_{\mu=1}^P \left( \sum_{k=1}^N w_k x_k^\mu - y^\mu \right)^2 + \sum_k w_k^2 " class="latex" src="https://s0.wp.com/latex.php?latex=H%28w%2C%5Cmathcal+D%29+%3D+%5Cfrac%7B1%7D%7B%5Clambda%7D+%5Csum_%7B%5Cmu%3D1%7D%5EP+%5Cleft%28+%5Csum_%7Bk%3D1%7D%5EN+w_k+x_k%5E%5Cmu+-+y%5E%5Cmu+%5Cright%29%5E2+%2B+%5Csum_k+w_k%5E2+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>The ridge parameter <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> controls the trade-off between training accuracy and regularization of the weight vectors. When <img alt="\lambda \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the training data are fit perfectly while the <img alt="\lambda \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit gives <img alt="w = 0" class="latex" src="https://s0.wp.com/latex.php?latex=w+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the minimizer of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The <img alt="\beta \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit of the Gibbs distribution corresponds to studying the performance of the ridge regression solution which minimizes <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The generalization error is an average over possible test points drawn from the same distribution<br/><img alt="E_g = \left&lt; \left( \sum_k (w_k-w^*_k) x_k \right)^2 \right&gt;_x = \frac{1}{N} \sum_{k=1}^N (w_k-w_k^*)^2" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%3D+%5Cleft%3C+%5Cleft%28+%5Csum_k+%28w_k-w%5E%2A_k%29+x_k+%5Cright%29%5E2+%5Cright%3E_x+%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bk%3D1%7D%5EN+%28w_k-w_k%5E%2A%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>C. Partition Function</h3>



<p>We introduce a partition function for the Gibbs distribution on <img alt="H(w,\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28w%2C%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><img alt="Z(\mathcal D) = \int dw \exp\left( - \frac{\beta}{2 \lambda} \sum_{\mu=1}^P ( w^\top x^\mu - y^\mu )^2 + \frac{\beta }{2}||w||^2 \right) ." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%29+%3D+%5Cint+dw+%5Cexp%5Cleft%28+-+%5Cfrac%7B%5Cbeta%7D%7B2+%5Clambda%7D+%5Csum_%7B%5Cmu%3D1%7D%5EP+%28+w%5E%5Ctop+x%5E%5Cmu+-+y%5E%5Cmu+%29%5E2+%2B+%5Cfrac%7B%5Cbeta+%7D%7B2%7D%7C%7Cw%7C%7C%5E2+%5Cright%29+.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>D. Replicated Partition Function</h3>



<p>We can rewrite the integral through a simple change of variables <img alt="\Delta = w-w^*" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta+%3D+w-w%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> since <img alt="y^\mu = w^{* \top} x^\mu + \sigma \epsilon^\mu" class="latex" src="https://s0.wp.com/latex.php?latex=y%5E%5Cmu+%3D+w%5E%7B%2A+%5Ctop%7D+x%5E%5Cmu+%2B+%5Csigma+%5Cepsilon%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. <img alt="\Delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> represents the discrepancy between the learned weights <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the target weights <img alt="w^*" class="latex" src="https://s0.wp.com/latex.php?latex=w%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We will now replicate and average over the training data <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, ie compute <img alt="\left&lt; Z^n \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p><img alt="\left&lt; Z(\mathcal{D}, J)^n \right&gt;_{\mathcal D} = \int \prod_{a=1}^n d\Delta_a \left&lt; \exp\left( - \frac{\beta}{2\lambda} \sum_{\mu=1}^P \sum_{a=1}^n ( \Delta_a \cdot x^\mu - \sigma \epsilon^\mu )^2 - \frac{\beta}{2} \sum_{a=1}^n ||\Delta_a + w^*||^2 \right) \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%28%5Cmathcal%7BD%7D%2C+J%29%5En+%5Cright%3E_%7B%5Cmathcal+D%7D+%3D+%5Cint+%5Cprod_%7Ba%3D1%7D%5En+d%5CDelta_a+%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Cfrac%7B%5Cbeta%7D%7B2%5Clambda%7D+%5Csum_%7B%5Cmu%3D1%7D%5EP+%5Csum_%7Ba%3D1%7D%5En+%28+%5CDelta_a+%5Ccdot+x%5E%5Cmu+-+%5Csigma+%5Cepsilon%5E%5Cmu+%29%5E2+-+%5Cfrac%7B%5Cbeta%7D%7B2%7D+%5Csum_%7Ba%3D1%7D%5En+%7C%7C%5CDelta_a+%2B+w%5E%2A%7C%7C%5E2+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><strong>Warning:</strong> Notice that by writing these integrals, we are implicitly assuming that <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is an integer. Eventually, we need to take <img alt="n \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit to obtain the generalization error <img alt="E_g" class="latex" src="https://s0.wp.com/latex.php?latex=E_g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> from <img alt="\left&lt; \log Z \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. After computation of <img alt="\left&lt; Z^n \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at integer <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we will get an analytic expression of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which we will allow us to non-rigorously take <img alt="n \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>The randomness from the dataset <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is present in the first term only appears through mean-zero Gaussian variables <img alt="\gamma_a^\mu = \Delta_a \cdot x^\mu - \epsilon^\mu \sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_a%5E%5Cmu+%3D+%5CDelta_a+%5Ccdot+x%5E%5Cmu+-+%5Cepsilon%5E%5Cmu+%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which have covariance structure</p>



<p><img alt="\left&lt; \gamma_a^\mu \gamma_b^\nu \right&gt; = \delta_{\mu \nu} \left[ \frac{1}{N} \Delta_a \cdot \Delta_b + \sigma^2 \right] = \delta_{\mu \nu} \left[ Q_{ab} + \sigma^2 \right] \implies \left&lt; \gamma \gamma^\top \right&gt; = Q + \sigma^2 1 1^\top \in \mathbb{R}^{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cgamma_a%5E%5Cmu+%5Cgamma_b%5E%5Cnu+%5Cright%3E+%3D+%5Cdelta_%7B%5Cmu+%5Cnu%7D+%5Cleft%5B+%5Cfrac%7B1%7D%7BN%7D+%5CDelta_a+%5Ccdot+%5CDelta_b+%2B+%5Csigma%5E2+%5Cright%5D+%3D+%5Cdelta_%7B%5Cmu+%5Cnu%7D+%5Cleft%5B+Q_%7Bab%7D+%2B+%5Csigma%5E2+%5Cright%5D+%5Cimplies+%5Cleft%3C+%5Cgamma+%5Cgamma%5E%5Ctop+%5Cright%3E+%3D+Q+%2B+%5Csigma%5E2+1+1%5E%5Ctop+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>where <img alt="1 \in \mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=1+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the vector of all ones and we introduced overlap order parameters <img alt="Q_{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> defined as</p>



<p><img alt="Q_{ab} = \frac{1}{N} \Delta_a \cdot \Delta_b \ ." class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%5Cfrac%7B1%7D%7BN%7D+%5CDelta_a+%5Ccdot+%5CDelta_b+%5C+.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>The average over the randomness in the dataset <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is therefore converted into a routine Gaussian integral. Exploiting the independence over each data point, we break the average into a product of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> averages.</p>



<p><img alt="\left&lt; \exp\left( -\frac{\beta}{2\lambda} \sum_{a,\mu} \left( \gamma_{a}^\mu\right)^2 \right) \right&gt;_{\{\gamma_a^\mu\}} = \left&lt; \exp\left( -\frac{\beta}{2\lambda} \sum_{a} \gamma_{a}^2 \right) \right&gt;_{\{\gamma_a\}}^P" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cexp%5Cleft%28+-%5Cfrac%7B%5Cbeta%7D%7B2%5Clambda%7D+%5Csum_%7Ba%2C%5Cmu%7D+%5Cleft%28+%5Cgamma_%7Ba%7D%5E%5Cmu%5Cright%29%5E2+%5Cright%29+%5Cright%3E_%7B%5C%7B%5Cgamma_a%5E%5Cmu%5C%7D%7D+%3D+%5Cleft%3C+%5Cexp%5Cleft%28+-%5Cfrac%7B%5Cbeta%7D%7B2%5Clambda%7D+%5Csum_%7Ba%7D+%5Cgamma_%7Ba%7D%5E2+%5Cright%29+%5Cright%3E_%7B%5C%7B%5Cgamma_a%5C%7D%7D%5EP&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Each average is a multivariate Gaussian integral of the form<br/><img alt="\int \frac{d\gamma_1 d\gamma_2 ... d\gamma_n}{\sqrt{\left( 2\pi \right)^n \det(Q+\sigma^2 I)}} \exp\left( -\frac{1}{2} \sum_{ab} \gamma_a \gamma_b \left( Q + \sigma^2 11^\top \right)^{-1}_{ab} - \frac{\beta}{2 \lambda} \sum_{a} \gamma_a^2 \right) = \det\left(I + \frac{\beta}{\lambda} Q + \frac{\beta}{\lambda} \sigma^2 11^\top \right)^{-1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+%5Cfrac%7Bd%5Cgamma_1+d%5Cgamma_2+...+d%5Cgamma_n%7D%7B%5Csqrt%7B%5Cleft%28+2%5Cpi+%5Cright%29%5En+%5Cdet%28Q%2B%5Csigma%5E2+I%29%7D%7D+%5Cexp%5Cleft%28+-%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bab%7D+%5Cgamma_a+%5Cgamma_b+%5Cleft%28+Q+%2B+%5Csigma%5E2+11%5E%5Ctop+%5Cright%29%5E%7B-1%7D_%7Bab%7D+-+%5Cfrac%7B%5Cbeta%7D%7B2+%5Clambda%7D+%5Csum_%7Ba%7D+%5Cgamma_a%5E2+%5Cright%29+%3D+%5Cdet%5Cleft%28I+%2B+%5Cfrac%7B%5Cbeta%7D%7B%5Clambda%7D+Q+%2B+%5Cfrac%7B%5Cbeta%7D%7B%5Clambda%7D+%5Csigma%5E2+11%5E%5Ctop+%5Cright%29%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This integral can be derived by routine integration of Gaussian functions, which we derive in the Appendix.</p>



<h3>E. Enforcing Order Parameter Definition</h3>



<p>To enforce the definition of the order parameters, we insert delta-functions into the expression for <img alt="\left&lt; Z^n \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which we write as Fourier integrals over dual order parameters <img alt="\hat{Q}_{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7BQ%7D_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><img alt="\delta( N Q_{ab} - \Delta_a \cdot \Delta_b) = \frac{1}{2\pi} \int d\hat{Q}_{ab} \exp\left(i \hat{Q}_{ab} ( N Q_{ab} - \Delta_a \cdot \Delta_b)\right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%28+N+Q_%7Bab%7D+-+%5CDelta_a+%5Ccdot+%5CDelta_b%29+%3D+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint+d%5Chat%7BQ%7D_%7Bab%7D+%5Cexp%5Cleft%28i+%5Chat%7BQ%7D_%7Bab%7D+%28+N+Q_%7Bab%7D+-+%5CDelta_a+%5Ccdot+%5CDelta_b%29%5Cright%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This trick is routine and is derived in the Appendix of this post.</p>



<p>After integration over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\Delta_a" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we are left with an expression of the form</p>



<p><img alt="\left&lt; Z^n \right&gt; = \int \prod_{ab} dQ_{ab} \prod_{ab} d\hat{Q}_{ab} \exp\left( - P G_E(Q) + i N \sum_{ab} Q_{ab} \hat{Q}_{ab} - N G_S(\hat{Q}) \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint+%5Cprod_%7Bab%7D+dQ_%7Bab%7D+%5Cprod_%7Bab%7D+d%5Chat%7BQ%7D_%7Bab%7D+%5Cexp%5Cleft%28+-+P+G_E%28Q%29+%2B+i+N+%5Csum_%7Bab%7D+Q_%7Bab%7D+%5Chat%7BQ%7D_%7Bab%7D+-+N+G_S%28%5Chat%7BQ%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="G_E" class="latex" src="https://s0.wp.com/latex.php?latex=G_E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a function which arises from the average over <img alt="\gamma_a^\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_a%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="G_S" class="latex" src="https://s0.wp.com/latex.php?latex=G_S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is calculated through integration over the <img alt="\Delta_a" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> variables.</p>



<p><strong>Warning:</strong> The functions <img alt="G_E" class="latex" src="https://s0.wp.com/latex.php?latex=G_E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="G_S" class="latex" src="https://s0.wp.com/latex.php?latex=G_S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> have complicated formulas and we omit them here to focus on the conceptual steps in the replica method. Interested readers can find explicit expressions for these functions in the references above.</p>



<h3>F. Replica Symmetry</h3>



<p>To make progress on the integral above, we will make the replica symmetry assumption, leveraging the fact that the ridge regression loss is convex and has unique minimizer for <img alt="\lambda &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Based on our simulations and arguments above, we will assume that the <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\hat{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrices satisfy <i>replica symmetry</i><br/><img alt="{Q}_{ab} = q \delta_{ab} + q_0 \ , \ \hat{Q}_{ab} = \hat{q} \delta_{ab} + \hat{q}_0 " class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D_%7Bab%7D+%3D+q+%5Cdelta_%7Bab%7D+%2B+q_0+%5C+%2C+%5C+%5Chat%7BQ%7D_%7Bab%7D+%3D+%5Chat%7Bq%7D+%5Cdelta_%7Bab%7D+%2B+%5Chat%7Bq%7D_0+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>G. Saddle Point Equations and Final Result</h3>



<p>After the replica symmetry ansatz, the replicated partition function has the form</p>



<p><img alt="\left&lt; Z^n \right&gt; = \int dq d\hat{q} dq_0 d\hat{q}_0 \exp( - n N \mathcal F(q,\hat{q},q_0,\hat{q}_0))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint+dq+d%5Chat%7Bq%7D+dq_0+d%5Chat%7Bq%7D_0+%5Cexp%28+-+n+N+%5Cmathcal+F%28q%2C%5Chat%7Bq%7D%2Cq_0%2C%5Chat%7Bq%7D_0%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>In the <img alt="N \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit, this integral is dominated by the order parameters <img alt="q, \hat{q},q_0,\hat{q}_0" class="latex" src="https://s0.wp.com/latex.php?latex=q%2C+%5Chat%7Bq%7D%2Cq_0%2C%5Chat%7Bq%7D_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which satisfy the saddle point equations</p>



<p><img alt="\frac{\partial \mathcal F}{\partial q} = 0 \ , \ \frac{\partial \mathcal F}{\partial \hat{q}} = 0 \ , \ \frac{\partial \mathcal F}{\partial q_0} = 0 \ , \ \frac{\partial \mathcal F}{\partial \hat{q}_0} = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+q%7D+%3D+0+%5C+%2C+%5C+%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+%5Chat%7Bq%7D%7D+%3D+0+%5C+%2C+%5C+%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+q_0%7D+%3D+0+%5C+%2C+%5C+%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+%5Chat%7Bq%7D_0%7D+%3D+0+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><strong>Warning</strong>:Notice that <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is small (we are working in <img alt="n\to0" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Cto0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit to study <img alt="\log Z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) but <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is large (we are studying the “thermodynamic” <img alt="N\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit). The order of taking these limits matters. It is important that we take <img alt="N \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> first before taking <img alt="n \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> so that, at finite value of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the integral for <img alt="\left&lt; Z^n \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is dominated by the saddle point of <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>We can solve the saddle point equations symbolically with Mathematica (see <a href="https://www.dropbox.com/s/m0pje9mr0gp0x1n/saddle_point_demo.nb?dl=1">this notebook</a>) in the <img alt="\beta \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit. We notice that <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> must scale like <img alt="O(1/\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%2F%5Cbeta%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\hat{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> must scale like <img alt="O(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Cbeta%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. After factoring out the dependence on the temperature, we can compute the saddle point conditions through partial differentiation.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Ps8WGh6.png"/></figure>



<p>This symbolically gives us the order parameters at the saddle point. For example, the overlap parameter <img alt="q = \frac{1}{2}[1-\lambda-\alpha + \sqrt{(1-\lambda-\alpha)^2 + 4\lambda} ]" class="latex" src="https://s0.wp.com/latex.php?latex=q+%3D+%5Cfrac%7B1%7D%7B2%7D%5B1-%5Clambda-%5Calpha+%2B+%5Csqrt%7B%281-%5Clambda-%5Calpha%29%5E2+%2B+4%5Clambda%7D+%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. After solving the saddle point equations, the generalization error can be written entirely in terms of the first order parameter <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at the saddle point. For replica <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the generalization error is merely <img alt="||\Delta_a||^2 = ||w_a - w^*||^2 = Q_{aa} = q+q_0" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%7C%5CDelta_a%7C%7C%5E2+%3D+%7C%7Cw_a+-+w%5E%2A%7C%7C%5E2+%3D+Q_%7Baa%7D+%3D+q%2Bq_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Thus</p>



<p><img alt="E_g = q+q_0 = \frac{(q+\lambda)^2 + \sigma^2 \alpha}{(q+\lambda + \alpha)^2 - \alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%3D+q%2Bq_0+%3D+%5Cfrac%7B%28q%2B%5Clambda%29%5E2+%2B+%5Csigma%5E2+%5Calpha%7D%7B%28q%2B%5Clambda+%2B+%5Calpha%29%5E2+-+%5Calpha%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Where <img alt="q + \lambda = \frac{1}{2} \left[ 1 + \lambda - \alpha + \sqrt{(1+\lambda-\alpha)^2 + 4\lambda\alpha} \right]" class="latex" src="https://s0.wp.com/latex.php?latex=q+%2B+%5Clambda+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%5B+1+%2B+%5Clambda+-+%5Calpha+%2B+%5Csqrt%7B%281%2B%5Clambda-%5Calpha%29%5E2+%2B+4%5Clambda%5Calpha%7D+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at the saddle point.</p>



<h3>H. Noise Free Estimation</h3>



<p>When <img alt="\sigma^2, \lambda \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2%2C+%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> the generalization error decreases linearly with <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>: <img alt="E_g = 1-\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%3D+1-%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\alpha &lt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3C+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="E_g=0" class="latex" src="https://s0.wp.com/latex.php?latex=E_g%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\alpha &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3E+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This indicates the target weights are perfectly estimated when the number of samples equals the number of features <img alt="P \to N" class="latex" src="https://s0.wp.com/latex.php?latex=P+%5Cto+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. A finite ridge parameter <img alt="\lambda &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> increases the generalization error when noise is zero <img alt="\sigma^2=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Asymptotically, the generalization error scales like <img alt="E_g \sim \frac{\lambda^2}{\alpha^2}" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Cfrac%7B%5Clambda%5E2%7D%7B%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for large <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h3>I. Phase transition and overfitting peaks</h3>



<p>In the presence of noise <img alt="\sigma^2 &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the story is different. In this case, the generalization error exhibits a peak at <img alt="\alpha \approx 1+\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+1%2B%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> before falling at a rate <img alt="E_g \sim \sigma^2/\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Csigma%5E2%2F%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at large <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In this regime, accurate estimation requires reducing the variance of the estimator by increasing the number of samples.</p>



<p>In small <img alt="\lambda \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit, the order parameter behaves like <img alt="q+\lambda \sim 1-\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=q%2B%5Clambda+%5Csim+1-%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\alpha&lt;1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3C1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="q+\lambda \sim \lambda" class="latex" src="https://s0.wp.com/latex.php?latex=q%2B%5Clambda+%5Csim+%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\alpha &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3E+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/nmDihyt.png"/></figure>



<p>The free energy <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> exhibits a discontinuous first derivative as <img alt="\alpha \to 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, a phenomenon known as <a href="https://en.wikipedia.org/wiki/Phase_transition#Ehrenfest_classification">first-order phase transition</a>. Let <img alt="\mathcal F^*" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be the value of the free energy at the saddle point <img alt="(Q^\star, \hat{q}^*, q_0^*, \hat{q}_0^*)" class="latex" src="https://s0.wp.com/latex.php?latex=%28Q%5E%5Cstar%2C+%5Chat%7Bq%7D%5E%2A%2C+q_0%5E%2A%2C+%5Chat%7Bq%7D_0%5E%2A%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Then we find</p>



<p><img alt="\frac{\partial \mathcal F^*}{\partial \alpha} \sim \frac{\sigma^2}{2\lambda} \Theta(\alpha - 1) + \mathcal{O}_\lambda(1) \ , \ (\lambda \to 0, \alpha \to 1 )" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+%5Cmathcal+F%5E%2A%7D%7B%5Cpartial+%5Calpha%7D+%5Csim+%5Cfrac%7B%5Csigma%5E2%7D%7B2%5Clambda%7D+%5CTheta%28%5Calpha+-+1%29+%2B+%5Cmathcal%7BO%7D_%5Clambda%281%29+%5C+%2C+%5C+%28%5Clambda+%5Cto+0%2C+%5Calpha+%5Cto+1+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>which indicates a discontinous first derivative in the <img alt="\lambda \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit as <img alt="\alpha \to 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We plot this free energy <img alt="\mathcal F^*(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F%5E%2A%28%5Calpha%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for varying values of <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, showing that as <img alt="\lambda\to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> a discontinuity in the free energy occurs at <img alt="\alpha \to 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The non-zero ridge parameter <img alt="\lambda &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> prevents the strict phase transition at <img alt="\alpha = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/f4t9Tun.png"/></figure>



<h3>J. Putting it all together</h3>



<p>Using the analysis of the saddle point, we are now prepared to construct a full picture of the possibilities. A figure below from <a href="https://arxiv.org/abs/2006.13198">this paper</a> provies all of the major insights. We plot experimental values of generalization error <img alt="E_g" class="latex" src="https://s0.wp.com/latex.php?latex=E_g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in a <img alt="N=800" class="latex" src="https://s0.wp.com/latex.php?latex=N%3D800&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> dimensional problem to provide a comparison with the replica prediction.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Vy7GwUs.png"/></figure>



<p>( a ) When <img alt="\lambda = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the generalization error either falls like <img alt="1-\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=1-%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> if noise is zero, or it exhibits a divergence at <img alt="\alpha \to 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> if noise is non-zero.</p>



<p>( b ) When noise is zero, increasing the explicit ridge <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> increases the generalization error. At large <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="E_g \sim \frac{\lambda^2}{\alpha^2}" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Cfrac%7B%5Clambda%5E2%7D%7B%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>( c ) When there is noise, explicit regularization can prevent the overfitting peak and give optimal generalization. At large <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="E_g \sim \frac{\sigma^2}{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Cfrac%7B%5Csigma%5E2%7D%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>( d ) In the <img alt="(\lambda,\sigma^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> plane, there are multiple possibilities for the learning curve <img alt="E_g(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=E_g%28%5Calpha%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Monotonic learning curves <img alt="E_g'(\alpha) &lt;0" class="latex" src="https://s0.wp.com/latex.php?latex=E_g%27%28%5Calpha%29+%3C0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are guaranteed provided <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is sufficiently large compared to <img alt="\sigma^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. If regularization is too small, then two-critical points can exist in the learning curve, ie two values <img alt="\alpha^*" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="E_g'(\alpha^*) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=E_g%27%28%5Calpha%5E%2A%29+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (sample wise double descent). For very large noise, a single local maximum exists in the learning curve <img alt="E_g(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=E_g%28%5Calpha%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which is followed by monotonic decreasing error.</p>



<h2>VI. Example Problem 2: Spiked Matrix Recovery</h2>



<p>_Detailed calculations can be found in this excellent <a href="https://meisong541.github.io/jekyll/update/2019/08/04/Replica_method_1.html">introduction of the problem by Song Mei</a>._</p>



<p>Suppose we have a <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>-by-<img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> rank-1 matrix, <img alt="\lambda \mathbf u \mathbf u ^T" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cmathbf+u+%5Cmathbf+u+%5ET&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where <img alt="\mathbf u" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a norm-1 column vector constituting the signal that we would like to recover. The input <img alt="\mathbf A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> we receive is corrupted by symmetric Gaussian i.i.d. noise, i.e.,</p>



<p><img alt="\mathbf A = \lambda \mathbf{uu}^T + \mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A+%3D+%5Clambda+%5Cmathbf%7Buu%7D%5ET+%2B+%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="W_{ij}=W_{ji}\sim \mathcal N(0, 1/N), W_{ii}\sim \mathcal N (0, 2/N)" class="latex" src="https://s0.wp.com/latex.php?latex=W_%7Bij%7D%3DW_%7Bji%7D%5Csim+%5Cmathcal+N%280%2C+1%2FN%29%2C+W_%7Bii%7D%5Csim+%5Cmathcal+N+%280%2C+2%2FN%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (<img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is drawn from a Gaussian Orthogonal Ensemble). At large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, eigenvalues of <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are distributed uniformly on a unit disk in the complex plane. Thus, the best estimate (which we call <img alt="\mathbf v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) of<br/><img alt="\mathbf u" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> from <img alt="\mathbf A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the eigenvector associated with the largest eigenvalue. In other words</p>



<p><img alt="\mathbf v = \arg \max_{\mathbf x\in \mathbb S^{N-1}} \mathbf{x}^T \mathbf{A} \mathbf {x}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v+%3D+%5Carg+%5Cmax_%7B%5Cmathbf+x%5Cin+%5Cmathbb+S%5E%7BN-1%7D%7D+%5Cmathbf%7Bx%7D%5ET+%5Cmathbf%7BA%7D+%5Cmathbf+%7Bx%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>The <i>observable</i> of interest is how well the estimate, <img alt="\mathbf v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, matches <img alt="\mathbf u" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, as measured by <img alt="(\mathbf v \cdot \mathbf u)^2" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+v+%5Ccdot+%5Cmathbf+u%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We would like to know its average over different <img alt="\mathbf{W}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>In the problem setup, <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a constant controlling the signal-to-noise ratio. Intuitively, the larger <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is, the better the estimate should be (when averaged over <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>). This is indeed true. Remarkably, for large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="\mathbf v \cdot \mathbf u" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v+%5Ccdot+%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is almost surely <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\lambda &lt;1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3C1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. For <img alt="\lambda \geq 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, it grows quickly as <img alt="1-\lambda^{-2}" class="latex" src="https://s0.wp.com/latex.php?latex=1-%5Clambda%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This discontinuity at <img alt="\lambda=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a <b>phase transition</b>. This dependence on <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be derived using the replica method.<br/><img src="https://i.imgur.com/ysQ4W3Y.png"/></p>



<p>In the simulations above, we see two trend with increasing <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. First, the average curve approaches the theory, which is good. In addition, trial-to-trial variability (as reflected by the error bars) shrinks. This reflects the fact that our observable is indeed self-averaging.</p>



<p>Here, we give a brief overview of how the steps of a replica calculation can be set up and carried out.</p>



<h3>Step 1</h3>



<p>Here, <img alt="\mathbf{W}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the problem parameter (<img alt="\mathcal P" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) that we average over. The minimized function is</p>



<p><img alt="H(\mathbf x, \mathbf W) = -\mathbf x^T (\lambda \mathbf u \mathbf u^T + \mathbf W) \mathbf{x} = -\lambda (\mathbf x \cdot \mathbf{u })^2 - \mathbf{x}^T \mathbf{W} \mathbf{x}." class="latex" src="https://s0.wp.com/latex.php?latex=H%28%5Cmathbf+x%2C+%5Cmathbf+W%29+%3D+-%5Cmathbf+x%5ET+%28%5Clambda+%5Cmathbf+u+%5Cmathbf+u%5ET+%2B+%5Cmathbf+W%29+%5Cmathbf%7Bx%7D+%3D+-%5Clambda+%28%5Cmathbf+x+%5Ccdot+%5Cmathbf%7Bu+%7D%29%5E2+-+%5Cmathbf%7Bx%7D%5ET+%5Cmathbf%7BW%7D+%5Cmathbf%7Bx%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This energy function already contains a “source term” for our observable of interest. Thus, the vanilla partition function will be used as the augmented partition function. In addition, this function does not scale with <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. To introduce the appropriate <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> scaling, we add an <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> factor to <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, yielding the partition function</p>



<p><img alt="Z(\mathbf W, \lambda) = \int_{\mathbb S^{N-1}}d \mathbf x \exp \left( -\beta N H(\mathbf x, \mathbf W) \right)." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathbf+W%2C+%5Clambda%29+%3D+%5Cint_%7B%5Cmathbb+S%5E%7BN-1%7D%7Dd+%5Cmathbf+x+%5Cexp+%5Cleft%28+-%5Cbeta+N+H%28%5Cmathbf+x%2C+%5Cmathbf+W%29+%5Cright%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>It follows that (again using angular brackets to denote average over <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>)</p>



<p><img alt="\langle (\mathbf x \cdot \mathbf u)^2 \rangle = \frac{1}{\beta N}\frac{d}{d\lambda }\langle \log Z(\mathbf W, \lambda) \rangle \Big|_{\lambda=0}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%28%5Cmathbf+x+%5Ccdot+%5Cmathbf+u%29%5E2+%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Cfrac%7Bd%7D%7Bd%5Clambda+%7D%5Clangle+%5Clog+Z%28%5Cmathbf+W%2C+%5Clambda%29+%5Crangle+%5CBig%7C_%7B%5Clambda%3D0%7D.+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>Since we are ultimately interested in this observable for the best estimate, <img alt="\mathbf v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, at the large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, limit, we seek to compute</p>



<p><img alt="\lim_{N\rightarrow \infty}\mathbb E [(\mathbf v \cdot \mathbf u)^2] =\lim_{N\rightarrow \infty} \lim_{\beta\rightarrow \infty} \frac{1}{\beta N}\frac{d}{d\lambda }\langle \log Z(\mathbf W, \lambda) \rangle ." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN%5Crightarrow+%5Cinfty%7D%5Cmathbb+E+%5B%28%5Cmathbf+v+%5Ccdot+%5Cmathbf+u%29%5E2%5D+%3D%5Clim_%7BN%5Crightarrow+%5Cinfty%7D+%5Clim_%7B%5Cbeta%5Crightarrow+%5Cinfty%7D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Cfrac%7Bd%7D%7Bd%5Clambda+%7D%5Clangle+%5Clog+Z%28%5Cmathbf+W%2C+%5Clambda%29+%5Crangle+.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Why don’t we evaluate the derivative only at <img alt="\lambda=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>? Because <img alt="\lambda (\mathbf x \cdot \mathbf u)^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%28%5Cmathbf+x+%5Ccdot+%5Cmathbf+u%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not a source term that we introduced. Another way to think about it is that this result needs to be a function of <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, so of course we don’t just evaluate it at one value.</p>



<h3>Step 2</h3>



<p>Per the replica trick, we need to compute</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/7f2JdBt.png"/></figure>



<p><strong>Warning:</strong> Hereafter, our use of “<img alt="\langle Z^n\rangle =" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>” is loose. When performing integrals, we will ignore the constants generated and only focus on getting the exponent right. This is because we will eventually take <img alt="\log" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of <img alt="\langle Z^n \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and take the derivative w.r.t. <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. A constant in <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in front of the integral expression for <img alt="\langle Z^n\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> does not affect this integral. This is often the case in replica calculations.</p>



<p>where <img alt="D\mathbf x" class="latex" src="https://s0.wp.com/latex.php?latex=D%5Cmathbf+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a uniform measure on <img alt="\mathbb S^{N-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+S%5E%7BN-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="D\mathbf{W}" class="latex" src="https://s0.wp.com/latex.php?latex=D%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the probability measure for <img alt="\mathbf{W}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, as described above.</p>



<p>We will not carry out the calculation in detail in this note as the details are problem-specific. But the overall workflow is rather typical of replica calculations:</p>



<p>1. Integrate over <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This can be done by writing <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the sum of a Gaussian i.i.d. matrix with its own transpose. The integral is then over the i.i.d. matrix and thus a standard Gaussian integral. After this step, we obtain an expression that no longer contains <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>,a major simplification.<br/>2. Introduce the order parameter <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. After the last integral, the exponent only depends on <img alt="\mathbf x^{(a)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> through <img alt="\mathbf u \cdot \mathbf x^{(a)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u+%5Ccdot+%5Cmathbf+x%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\mathbf{x^{(a)}} \cdot \mathbf{x^{(b)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%5E%7B%28a%29%7D%7D+%5Ccdot+%5Cmathbf%7Bx%5E%7B%28b%29%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. These dot products can be described by a matrix <img alt="\mathbf Q \in \mathbb R^{N+1 \times N+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q+%5Cin+%5Cmathbb+R%5E%7BN%2B1+%5Ctimes+N%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where we define <img alt="Q_{0,a}=Q_{a,0}=\mathbf{u} \cdot \mathbf{x}^{(a)}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7B0%2Ca%7D%3DQ_%7Ba%2C0%7D%3D%5Cmathbf%7Bu%7D+%5Ccdot+%5Cmathbf%7Bx%7D%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="Q_{a\geq 1, b\geq1}=\mathbf x^{(a)} \cdot \mathbf x^{(b)}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Ba%5Cgeq+1%2C+b%5Cgeq1%7D%3D%5Cmathbf+x%5E%7B%28a%29%7D+%5Ccdot+%5Cmathbf+x%5E%7B%28b%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>3. Replace the integral over <img alt="\mathbf x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with one over <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. A major inconvenience of the integral over <img alt="\mathbf x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is that it is not over the entire real space but over a hypersphere. However, we can demand <img alt="\mathbf x^{(a)} \in \mathbb S^{N-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x%5E%7B%28a%29%7D+%5Cin+%5Cmathbb+S%5E%7BN-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> by requiring <img alt="Q_{aa}=1" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Baa%7D%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Now, we rewrite the exponent in terms of <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and integrate over <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> instead, but we add many Dirac delta functions to enforce the definition of <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We get an expression in the form</p>



<p><img alt="\langle Z^n\rangle = \int d\mathbf Q \exp (f(Q)) \prod_{i=1}^N \delta (\mathbf u \cdot \mathbf x^{(i)} - Q_{0i}) \prod _{1\leq i \leq j \leq N} \delta (\mathbf x^{(j)} u \cdot \mathbf x^{(i)} - Q_{ji})." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D+%5Cint+d%5Cmathbf+Q+%5Cexp+%28f%28Q%29%29+%5Cprod_%7Bi%3D1%7D%5EN+%5Cdelta+%28%5Cmathbf+u+%5Ccdot+%5Cmathbf+x%5E%7B%28i%29%7D+-+Q_%7B0i%7D%29+%5Cprod+_%7B1%5Cleq+i+%5Cleq+j+%5Cleq+N%7D+%5Cdelta+%28%5Cmathbf+x%5E%7B%28j%29%7D+u+%5Ccdot+%5Cmathbf+x%5E%7B%28i%29%7D+-+Q_%7Bji%7D%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>4. After some involved simplifications, we have</p>



<p><img alt="\langle Z^n\rangle = \int d \mathbf Q \exp(N g(\mathbf Q) + C)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D+%5Cint+d+%5Cmathbf+Q+%5Cexp%28N+g%28%5Cmathbf+Q%29+%2B+C%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>where <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> does not depend on <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="g(\mathbf Q)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cmathbf+Q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is <img alt="O(1)" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. By the saddle point method,</p>



<p><img alt="\langle Z^n\rangle = \max_\mathbf{Q} \exp(N g(\mathbf Q))," class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D+%5Cmax_%5Cmathbf%7BQ%7D+%5Cexp%28N+g%28%5Cmathbf+Q%29%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\mathbf{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> needs to satisfy the various constraints we proposed (e.g., its diagonal is all <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and it is symmetric).</p>



<h3>Step 3</h3>



<p>The optimization over <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not trivial. Hence, we make some guesses about the structure of <img alt="\mathbf Q^\star" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which maximizes the exponent. This is where the *replica symmetry (RS) ansatz* comes in. Since the indices of <img alt="\mathbf{x}^{(a)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are arbitrary, one guess is that for all <img alt="a,b\geq 1" class="latex" src="https://s0.wp.com/latex.php?latex=a%2Cb%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> <img alt="Q_{a\neq b}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Ba%5Cneq+b%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> has the same value ,<img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In addition, for all <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="Q_{0,a}=\mu" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7B0%2Ca%7D%3D%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This is the RS ansatz — it assumes an equivalency between replicas. Rigorously showing whether this is indeed the case is challenging, but we can proceed with this assumption and see if the results are correct.</p>



<p>The maximization of <img alt="g(\mathbf{Q})" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cmathbf%7BQ%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is now over two scalars, <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Writing the maximum as <img alt="\mu^*" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="Q^\star" class="latex" src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and using the replica identity</p>



<p><img alt="\langle \log Z \rangle = \lim_{n\rightarrow 0} \log \langle Z^n \rangle /n= \lim_{ n \rightarrow 0} Ng(\mu^*, Q^\star)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle+%3D+%5Clim_%7Bn%5Crightarrow+0%7D+%5Clog+%5Clangle+Z%5En+%5Crangle+%2Fn%3D+%5Clim_%7B+n+%5Crightarrow+0%7D+Ng%28%5Cmu%5E%2A%2C+Q%5E%5Cstar%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Setting the derivative of <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> w.r.t. them to zero yields two solutions.</p>



<p><strong>Bad Math Warning:</strong> Maximizing <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> w.r.t. <img alt="\mu,q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%2Cq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> requires checking that the solutions are indeed global minima, a laborious effort that has done for some models. We will assume them to be global minima.</p>



<p>For each solution, we can compute <img alt="\lim_{N\rightarrow \infty} \lim_{\beta\rightarrow \infty} \frac{1}{\beta N}\langle \log Z \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN%5Crightarrow+%5Cinfty%7D+%5Clim_%7B%5Cbeta%5Crightarrow+%5Cinfty%7D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Clangle+%5Clog+Z+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which will become what we are looking for after being differentiated w.r.t. <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We obtain an expression of <img alt="\langle \log Z \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The two solutions yield <img alt="\langle \log Z \rangle= \lambda + 1/\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle%3D+%5Clambda+%2B+1%2F%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\langle \log Z \rangle= 2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, respectively. Differentiating each w.r.t. <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to get <img alt="\langle (v \cdot u)^2 \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%28v+%5Ccdot+u%29%5E2+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we have <img alt="1 - \lambda^{-2}" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+%5Clambda%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/ciDxSHD.png"/></figure>



<p>Which one is correct? We decide by checking whether the solutions (black line and blue line above) are sensible (“physical”). It can be verified that <img alt="\lim_{N\rightarrow \infty} \lim_{\beta\rightarrow \infty} \frac{1}{\beta N}\langle \log Z \rangle=\langle \lambda_\text{max} \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN%5Crightarrow+%5Cinfty%7D+%5Clim_%7B%5Cbeta%5Crightarrow+%5Cinfty%7D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Clangle+%5Clog+Z+%5Crangle%3D%5Clangle+%5Clambda_%5Ctext%7Bmax%7D+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which is the largest eigenvalue of <img alt="\mathbf A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Clearly, it should be non-decreasing as a function of <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Thus, for <img alt="\lambda \leq 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cleq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we choose the <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> solution, and for <img alt="\lambda \geq 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> the <img alt="\lambda + 1 / \lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%2B+1+%2F+%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> solution. Thus, <img alt="\langle (v \cdot u)^2 \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%28v+%5Ccdot+u%29%5E2+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="1-\lambda^{-2}" class="latex" src="https://s0.wp.com/latex.php?latex=1-%5Clambda%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in the two regimes, respectively.</p>



<h2>Appendix: Gaussian Integrals and Delta Function Representation</h2>



<p>We frequently encounter Gaussian integrals when using the replica method and it is often convenient to rely on basic integration results which we provide in this Appendix.</p>



<h4>Single Variable</h4>



<p>The simplest Gaussian integral is the following one dimensional integral</p>



<p><img alt="I(a) = \int_{-\infty}^\infty \exp\left( -\frac{a}{2} x^2 \right) dx" class="latex" src="https://s0.wp.com/latex.php?latex=I%28a%29+%3D+%5Cint_%7B-%5Cinfty%7D%5E%5Cinfty+%5Cexp%5Cleft%28+-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We can calculate the square of this quantity by changing to polar coordinates <img alt="x=r\cos\phi, y = r \sin \phi" class="latex" src="https://s0.wp.com/latex.php?latex=x%3Dr%5Ccos%5Cphi%2C+y+%3D+r+%5Csin+%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/><img alt="I(a)^2 = \int_{\mathbb{R}^2} \exp\left(-\frac{a}{2} \left( x^2 + y^2 \right) \right) dx dy = \int_{0}^{2\pi} d\phi \int_{0}^{\infty} r \exp\left( - \frac{a}{2} r^2 \right) dr = 2 \pi a^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=I%28a%29%5E2+%3D+%5Cint_%7B%5Cmathbb%7BR%7D%5E2%7D+%5Cexp%5Cleft%28-%5Cfrac%7Ba%7D%7B2%7D+%5Cleft%28+x%5E2+%2B+y%5E2+%5Cright%29+%5Cright%29+dx+dy+%3D+%5Cint_%7B0%7D%5E%7B2%5Cpi%7D+d%5Cphi+%5Cint_%7B0%7D%5E%7B%5Cinfty%7D+r+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+r%5E2+%5Cright%29+dr+%3D+2+%5Cpi+a%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We thus conclude that <img alt="I(a) = \sqrt{2\pi / a}" class="latex" src="https://s0.wp.com/latex.php?latex=I%28a%29+%3D+%5Csqrt%7B2%5Cpi+%2F+a%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Thus we find that the function <img alt="\sqrt{\frac{a}{2\pi}} e^{-\frac{a}{2} x^2 }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+e%5E%7B-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a normalized function over <img alt="(-\infty, \infty)" class="latex" src="https://s0.wp.com/latex.php?latex=%28-%5Cinfty%2C+%5Cinfty%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This integral can also be calculated with Mathematica or Sympy. Below is the result in Sympy.</p>



<p><code>from sympy import *<br/>from sympy.abc import a, b, x, y<br/>x = Symbol('x')<br/>integrate( exp( -a/2 <i> x</i>*2 ) , (x, -oo,oo))<br/></code></p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/k8OcjPa.png"/></figure>



<p>This agrees with our result since we were implicitly assuming <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> real and positive (<img alt="\arg a = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Carg+a+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>).</p>



<p>We can generalize this result to accomodate slightly more involved integrals which contain both quadratic and linear terms in the exponent. This exercise reduces to the previous case through simple completion of the square</p>



<p><img alt="I(a,b)=\int_{-\infty}^{\infty} \exp\left(-\frac{a}{2} x^2 \pm bx \right) dx = \exp\left( \frac{b^2}{2a} \right) \int_{-\infty}^{\infty} \exp\left( - \frac{a}{2} \left[ x \mp \frac{b}{a} \right]^2 \right) dx = \exp\left( \frac{b^2}{2a} \right) \sqrt{2\pi/a}" class="latex" src="https://s0.wp.com/latex.php?latex=I%28a%2Cb%29%3D%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+%5Cexp%5Cleft%28-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+bx+%5Cright%29+dx+%3D+%5Cexp%5Cleft%28+%5Cfrac%7Bb%5E2%7D%7B2a%7D+%5Cright%29+%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+%5Cleft%5B+x+%5Cmp+%5Cfrac%7Bb%7D%7Ba%7D+%5Cright%5D%5E2+%5Cright%29+dx+%3D+%5Cexp%5Cleft%28+%5Cfrac%7Bb%5E2%7D%7B2a%7D+%5Cright%29+%5Csqrt%7B2%5Cpi%2Fa%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We can turn this equality around to find an expression</p>



<p><img alt="\exp\left( \frac{b^2}{2a} \right) = \sqrt{\frac{a}{2\pi}} \int \exp\left( -\frac{a}{2} x^2 \pm b x \right) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+%5Cfrac%7Bb%5E2%7D%7B2a%7D+%5Cright%29+%3D+%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+%5Cint+%5Cexp%5Cleft%28+-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+b+x+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Viewed in this way, this formula allows one to transform a term quadratic in <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in the exponential function into an integral involving a term linear in <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This is known as the <a href="https://en.wikipedia.org/wiki/Hubbard%E2%80%93Stratonovich_transformation">Hubbard-Stratanovich</a> transformation. Taking <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be imaginary (<img alt="b=ik" class="latex" src="https://s0.wp.com/latex.php?latex=b%3Dik&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for real <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>), we find an alternative expression of a Gaussian function</p>



<p><img alt="\exp\left( - \frac{k^2}{2a} \right) = \sqrt{\frac{a}{2\pi}} \int \exp\left( -\frac{a}{2} x^2 \pm i k x \right) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+-+%5Cfrac%7Bk%5E2%7D%7B2a%7D+%5Cright%29+%3D+%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+%5Cint+%5Cexp%5Cleft%28+-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+i+k+x+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h4>Delta Function Integral Representation</h4>



<p>A delta function <img alt="\delta(z)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be considered as a limit of a normalized mean-zero Gaussian function with variance taken to zero</p>



<p><img alt="\delta(z) = \lim_{a \to 0} \sqrt{\frac{1}{2\pi a}} \exp\left( - \frac{1}{2 a} z^2 \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%28z%29+%3D+%5Clim_%7Ba+%5Cto+0%7D+%5Csqrt%7B%5Cfrac%7B1%7D%7B2%5Cpi+a%7D%7D+%5Cexp%5Cleft%28+-+%5Cfrac%7B1%7D%7B2+a%7D+z%5E2+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We can now use the <a href="https://en.wikipedia.org/wiki/Hubbard%E2%80%93Stratonovich_transformation">Hubbard-Stratanovich</a> trick to rewrite the Gaussian function</p>



<p><img alt="\exp\left( - \frac{1}{2 a} z^2 \right) = \sqrt{\frac{a}{2\pi}} \int \exp\left( - \frac{a}{2} x^2 \pm i z x \right) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+-+%5Cfrac%7B1%7D%7B2+a%7D+z%5E2+%5Cright%29+%3D+%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+%5Cint+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+i+z+x+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Thus we can relate the delta function to an integral</p>



<p><img alt="\delta(z) = \lim_{a \to 0} \frac{1}{2\pi} \int \exp\left( - \frac{a}{2} x^2 \pm i x z \right) dx = \frac{1}{2\pi} \int \exp\left( \pm i x z \right) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%28z%29+%3D+%5Clim_%7Ba+%5Cto+0%7D+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+i+x+z+%5Cright%29+dx+%3D+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint+%5Cexp%5Cleft%28+%5Cpm+i+x+z+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This trick is routinely utilized to represent delta functions with integrals over exponential functions during a replica calculation. In particular, this identity is often used to enforce defintions of the order parameters <img alt="Q_{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in the problem. For example, in the least squares problem where <img alt="Q_{ab} = \frac{1}{N}\Delta_a \cdot \Delta_b" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%5Cfrac%7B1%7D%7BN%7D%5CDelta_a+%5Ccdot+%5CDelta_b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> we used</p>



<p><img alt="\delta(NQ_{ab} - \Delta_{a} \cdot \Delta_b) = \frac{1}{2\pi } \int d\hat{Q}_{ab} \exp\left( i N Q_{ab} \hat{Q}_{ab} - i \hat{Q}_{ab} \Delta_{a} \cdot \Delta_b \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%28NQ_%7Bab%7D+-+%5CDelta_%7Ba%7D+%5Ccdot+%5CDelta_b%29+%3D+%5Cfrac%7B1%7D%7B2%5Cpi+%7D+%5Cint+d%5Chat%7BQ%7D_%7Bab%7D+%5Cexp%5Cleft%28+i+N+Q_%7Bab%7D+%5Chat%7BQ%7D_%7Bab%7D+-+i+%5Chat%7BQ%7D_%7Bab%7D+%5CDelta_%7Ba%7D+%5Ccdot+%5CDelta_b+%5Cright%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h4>Multivariate Gaussian integrals</h4>



<p>We commonly encounter integrals of the following form<br/><img alt="I(M) = \int_{\mathbb{R}^n} \exp\left( - \frac{1}{2} \sum_{ab} M_{ab} x_a x_b \right) dx_1 dx_2 ... dx_n" class="latex" src="https://s0.wp.com/latex.php?latex=I%28M%29+%3D+%5Cint_%7B%5Cmathbb%7BR%7D%5En%7D+%5Cexp%5Cleft%28+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bab%7D+M_%7Bab%7D+x_a+x_b+%5Cright%29+dx_1+dx_2+...+dx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where matrix <img alt="M_{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=M_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is symmetric and positive definite. An example is the data average in the least squares problem studied in this blog post where <img alt="M = (Q + \sigma^2 11^\top)^{-1} + \beta I" class="latex" src="https://s0.wp.com/latex.php?latex=M+%3D+%28Q+%2B+%5Csigma%5E2+11%5E%5Ctop%29%5E%7B-1%7D+%2B+%5Cbeta+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We can reduce this to a collection of one dimensional problems by computing the eigendecomposition of <img alt="M = \sum_{\rho} \lambda_\rho u_\rho u_\rho^\top" class="latex" src="https://s0.wp.com/latex.php?latex=M+%3D+%5Csum_%7B%5Crho%7D+%5Clambda_%5Crho+u_%5Crho+u_%5Crho%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. From this decomposition, we introduce variables</p>



<p><img alt="z_\rho = \sum_{a=1}^n u_{\rho,a} x_a" class="latex" src="https://s0.wp.com/latex.php?latex=z_%5Crho+%3D+%5Csum_%7Ba%3D1%7D%5En+u_%7B%5Crho%2Ca%7D+x_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>The transformation from <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is orthogonal so the determinant of the Jacobian has absolute value one. After changing variables, we therefore obtain the following decoupled integrals</p>



<p><img alt="I(M) = \int_{\mathbb{R}^n} \exp\left( -\frac{1}{2} \sum_{\rho} \lambda_\rho z_\rho^2 \right) dz_1...dz_n = \prod_{\rho=1}^n \int \exp\left( -\frac{\lambda_\rho}{2 } z_\rho^2 \right) dz_\rho = \prod_{\rho=1}^n \sqrt{\frac{2\pi}{\lambda_\rho}}" class="latex" src="https://s0.wp.com/latex.php?latex=I%28M%29+%3D+%5Cint_%7B%5Cmathbb%7BR%7D%5En%7D+%5Cexp%5Cleft%28+-%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7B%5Crho%7D+%5Clambda_%5Crho+z_%5Crho%5E2+%5Cright%29+dz_1...dz_n+%3D+%5Cprod_%7B%5Crho%3D1%7D%5En+%5Cint+%5Cexp%5Cleft%28+-%5Cfrac%7B%5Clambda_%5Crho%7D%7B2+%7D+z_%5Crho%5E2+%5Cright%29+dz_%5Crho+%3D+%5Cprod_%7B%5Crho%3D1%7D%5En+%5Csqrt%7B%5Cfrac%7B2%5Cpi%7D%7B%5Clambda_%5Crho%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Using the fact that the determinant is the product of eigenvalues <img alt="\det M = \prod_{\rho} \lambda_\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdet+M+%3D+%5Cprod_%7B%5Crho%7D+%5Clambda_%5Crho&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we have the following expression for the multivariate Gaussian integral</p>



<p><img alt="I(M) = \left(2 \pi \right)^{n/2} \det\left( M \right)^{-1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=I%28M%29+%3D+%5Cleft%282+%5Cpi+%5Cright%29%5E%7Bn%2F2%7D+%5Cdet%5Cleft%28+M+%5Cright%29%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p></div>
    </content>
    <updated>2021-08-11T16:06:56Z</updated>
    <published>2021-08-11T16:06:56Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-08-15T06:21:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8163</id>
    <link href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-1-of-2/" rel="alternate" type="text/html"/>
    <title>Replica Method for the Machine Learning Theorist: Part 1 of 2</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Blake Bordelon, Haozhe Shan, Abdul Canatar, Boaz Barak, Cengiz Pehlevan [Boaz’s note: Blake and Haozhe were students in the ML theory seminar this spring; in that seminar we touched on the replica method in the lecture on inference and statistical physics but here Blake and Haozhe (with a little help from the rest of us) … <a class="more-link" href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-1-of-2/">Continue reading <span class="screen-reader-text">Replica Method for the Machine Learning Theorist: Part 1 of 2</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h4>Blake Bordelon, Haozhe Shan, Abdul Canatar, Boaz Barak, Cengiz Pehlevan</h4>



<p><em>[Boaz’s note: Blake and Haozhe were students in <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">the ML theory seminar</a> this spring; in that seminar we touched on the replica method in the <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">lecture on inference and statistical physics</a> but here Blake and Haozhe (with a little help from the rest of us) give a great overview of the method and its relations to ML. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a>.]</em></p>



<p>See also: <a href="https://boazbarak.org/Papers/replica.pdf">PDF version of both parts</a> and <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/">part 2 of this post</a>.</p>



<h2>I. Analysis of Optimization Problems with Statistical Physics</h2>



<p>In computer science and machine learning, we are often interested in solving optimization problems of the form</p>



<p><img alt="\min_{x \in \mathcal{S}} H(x,\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bx+%5Cin+%5Cmathcal%7BS%7D%7D+H%28x%2C%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is an objective function which depends on our decision variables <img alt="x \in \mathcal S" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal+S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as well as on a set of problem-specific parameters <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Frequently, we encounter problems relevant to machine learning, where <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a random variable. The replica method is a useful tool to analyze <em>large problems</em> and their <em>typical</em> behavior over the distribution of <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Here are a few examples of problems that fit this form:</p>



<ol><li>In supervised learning, <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> may be a training loss, <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> a set of neural network weights and <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> the data points and their labels</li><li>We may want to find the most efficient way to visit all nodes on a graph. In this case <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> describes nodes and edges of the graph, <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a representation of the set of chosen edges, and <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be the cost of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> if <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> encodes a valid path and <img alt="\infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (or a very large number ) if it doesn’t encode a valid path.</li><li>Satisfiability: <img alt="x \in \{0,1\}^N" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B0%2C1%5C%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a collection booleans which must satisfy a collection of constraints. In this case the logical constraints (clauses) are the parameters <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. <img alt="H(x)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be the number of constraints violated by <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li><li>Recovery of structure in noisy data: <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is our guess of the structure and <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are instances of the observed noisy data. For example PCA attempts to identify the directions of maximal variation in the data. With the replica method, we could ask how the accuracy of the estimated top eigenvector degrades with noise.</li></ol>



<h2>II. The Goal of the Replica Method</h2>



<p>The <strong>replica method</strong> is a way to calculate the value of some statistic (<strong>observable</strong> in physics-speak) <img alt="O(x)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a “typical” minimizer of <img alt="H(x,\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=H%28x%2C%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a “typical” value for the parameters (which are also known in physics-speak as the <strong>disorder</strong>).</p>



<p>In Example 1 (supervised learning), the observable may be the generalization error of a chosen algorithm (e.g. a linear classifier) on a given dataset. For Example 2 (path), this could be the cost of the best path. For Example 3 (satisfiability), the observable might be whether or not a solution exists at all for the problem. In Example 4 (noisy data), the observable might be the quality of decoded data (distance from ground truth under some measure).</p>



<p>An observable like generalization error obviously depends on <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the problem instance. However, can we say something more general about this <em>type of problem</em>? In particular, if <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> obeys some probability distribution, is it possible to characterize the the typical observable over different problem instances <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>?</p>



<p>For instance, in Example 1, we can draw all of our training data from a distribution. For each random sample of data points <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we find the set of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which minimize <img alt="H(x, \mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28x%2C+%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and compute a generalization error. We then repeat this procedure many times and average the results. Sometimes, there are multiple <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that minimize <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for a given sample of <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>; this requires averaging the observable over all global minima for each <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> first, before averaging over different <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>To give away a “spolier”, towards the end of this note, we will see how to use the replica method to give accurate predictions of performance for noisy least square fitting and spiked matrix recovery.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/BKjGG3O.png"/></figure>



<p><em>Generalization gap in least squares ridge regression, figure taken from <a href="https://arxiv.org/abs/2006.13198">Canatar, Bordelon, and Pehlevan</a></em></p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Jrth8N3.png"/></figure>



<p><em>Performance (agreement with planted signal) as function of signal strength for spiked matrix recovery, as the dimension grows, the experiment has stronger agreement with theory. See also <a href="https://meisong541.github.io/jekyll/update/2019/08/04/Replica_method_1.html">Song Mei’s exposition</a></em></p>



<h3>A. What do we actually do?</h3>



<p>Now that we are motivated, let’s see what quantities the replica method attempts to obtain. In general, given some observable <img alt="O(x,\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28x%2C%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the average of <img alt="O" class="latex" src="https://s0.wp.com/latex.php?latex=O&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> over a minimizer <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> chosen at random from the set <img alt="\text{arg} \min H(x,\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Barg%7D+%5Cmin+H%28x%2C%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and take the average of this quantity over the choice of the disorder <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>In other words, we want to compute the following quantity:</p>



<p><img alt="\text{Desired quantity } = \mathbb{E}_{\mathcal{D}} \mathbb{E}_{x \in \text{arg}\min H(x,\mathcal{D})} \left[ O(x, \mathcal{D}) \right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Cmathbb%7BE%7D_%7B%5Cmathcal%7BD%7D%7D+%5Cmathbb%7BE%7D_%7Bx+%5Cin+%5Ctext%7Barg%7D%5Cmin+H%28x%2C%5Cmathcal%7BD%7D%29%7D+%5Cleft%5B+O%28x%2C+%5Cmathcal%7BD%7D%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>The above equation has two types of expectation- over the disorder <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and over the minimizers <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>The physics convention is to</p>



<ul><li>use <img alt="\left&lt; f \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+f+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for the expectation of a function <img alt="f(\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> over the disorder <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></li><li>use <img alt="\int g(x) \mu(x) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+g%28x%29+%5Cmu%28x%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for the expectation of a function <img alt="g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> over <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> chosen according to some measure <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li></ul>



<p>Using this notation, we can write the above as</p>



<p><img alt="\text{Desired quantity } = \left&lt; \int p^*(x;\mathcal D) O(x,\mathcal D) dx \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Cleft%3C+%5Cint+p%5E%2A%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\left&lt; \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> denotes an average over the probability measure for problem parameters <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and <img alt="p^*(x;\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p%5E%2A%28x%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a uniform distribution over the set of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that minimize <img alt="H(x,\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=H%28x%2C%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with zero probability mass placed on sub-optimal points.</p>



<p>The ultimate goal of the replica method is to express</p>



<p><img alt="\text{Desired quantity } = \text{solution of optimization on constant number of variables}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Ctext%7Bsolution+of+optimization+on+constant+number+of+variables%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>but it will take some time to get there.</p>



<h3>B. The concept of “self-averaging” and concentration</h3>



<p>Above, we glossed over an important distinction between the “typical” value of <img alt="f(\mathcal D) = \int p^*(x; \mathcal D) O(x,\mathcal D) dx" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal+D%29+%3D+%5Cint+p%5E%2A%28x%3B+%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the *average* value of <img alt="f(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This is OK only when we have <em>concentration</em> in the sense that with high probability over the choice of <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="f(\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is close to its expected value. We define this as the property that with probability at least <img alt="1-\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1-%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the quantity <img alt="f(\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is within a <img alt="1\pm \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1%5Cpm+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> multiplicative factor of its expectation, whwere <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is some quantity that goes to zero as the system size grows. A quantity <img alt="f(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that concentrates in this sense is called <strong>self averaging</strong>.</p>



<p>For example, suppose that <img alt="X= \sum_{i=1}^n X_i" class="latex" src="https://s0.wp.com/latex.php?latex=X%3D+%5Csum_%7Bi%3D1%7D%5En+X_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where each <img alt="X_i" class="latex" src="https://s0.wp.com/latex.php?latex=X_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> equals <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with probabilty <img alt="1/2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with probability <img alt="1/2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> independently. Standard Chernoff bounds show that with high probability <img alt="X \in [n/2 \pm O(\sqrt{n})]" class="latex" src="https://s0.wp.com/latex.php?latex=X+%5Cin+%5Bn%2F2+%5Cpm+O%28%5Csqrt%7Bn%7D%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> or <img alt="\tfrac{X}{\mathbb{E} X} \in \left(1 + O(\tfrac{1}{\sqrt n})\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7BX%7D%7B%5Cmathbb%7BE%7D+X%7D+%5Cin+%5Cleft%281+%2B+O%28%5Ctfrac%7B1%7D%7B%5Csqrt+n%7D%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Hence <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a self averaging quantity.</p>



<p>In contrast the random variable <img alt="Y=2^X" class="latex" src="https://s0.wp.com/latex.php?latex=Y%3D2%5EX&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not self averaging. Since <img alt="Y = \prod_{i=1}^n 2^{X_i}" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cprod_%7Bi%3D1%7D%5En+2%5E%7BX_i%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and these random variables are independent, we know that <img alt="\mathbb{E} Y = \prod_{i=1}^n \mathbb{E} 2^{X_i} = \left(\tfrac{1}{2} 2^1 + \tfrac{1}{2} 2^0 \right)^n = (3/2)^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+Y+%3D+%5Cprod_%7Bi%3D1%7D%5En+%5Cmathbb%7BE%7D+2%5E%7BX_i%7D+%3D+%5Cleft%28%5Ctfrac%7B1%7D%7B2%7D+2%5E1+%2B+%5Ctfrac%7B1%7D%7B2%7D+2%5E0+%5Cright%29%5En+%3D+%283%2F2%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. However, with high probability a typical value of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> will be of the form <img alt="2^{n/2 \pm O(\sqrt{n})} = \sqrt{2}^{n \pm O(\sqrt n)}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bn%2F2+%5Cpm+O%28%5Csqrt%7Bn%7D%29%7D+%3D+%5Csqrt%7B2%7D%5E%7Bn+%5Cpm+O%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Since <img alt="\sqrt{2} &lt; 3/2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%7D+%3C+3%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> we see that the typical value of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is exponentially smaller than the expected value of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>The example above is part of a more general pattern. Often even if a variable <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not self averaging, the variable <img alt="X = \log Y" class="latex" src="https://s0.wp.com/latex.php?latex=X+%3D+%5Clog+Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> will be self-averaging. Hence if we are interested in the typical value of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the quantity <img alt="\exp\left( \mathbb{E} [\log Y] \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+%5Cmathbb%7BE%7D+%5B%5Clog+Y%5D+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is more representative than the quantity <img alt="\mathbb{E}[Y]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BY%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h3>C. When is using the replica method a good idea?</h3>



<p>Suppose that we want to compute a quantity of the form above. When is it a good idea to use the replica method to do so?<br/>Generally, we would want it to satisfy the following conditions:</p>



<ol><li>The learning problem is high dimensional with a large budget of data. The replica method describes a <em>thermodynamic limit</em> where the system size and data budget are taken to infinity with some fixed ratio between the two quantities. Such a limit is obviously never achieved in reality, but in practice sufficiently large learning problems can be accurately modeled by the method.</li><li>The loss or the constraints are convenient functions of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Typically <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> will be a low degree polynomial or a sum of local functions (each depending on small number of variables) in <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li><li>Averages over the disorder in <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are tractable analytically. That is, we can compute marginals of the distribution over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li><li>The statistic that we are interested in is self-averaging.</li></ol>



<p>If the above conditions aren’t met, it is unlikely that this problem will gain much analytical insight from the replica method.</p>



<h2>III. The Main Conceptual Steps Behind Replica Calculations</h2>



<p>We now describe the conceptual steps that are involved in calculating a quantity using the replica method.<br/>They are also outlined in this figure:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/eVOI8EA.png"/></figure>



<h3>Step 1:”Softening” Constraints with the Gibbs Measure</h3>



<p>The uniform measure on minimizers <img alt="p^*(x;\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p%5E%2A%28x%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is often difficult to work with. To aid progress, we can think of it as a special case of what is known as the <a href="https://en.wikipedia.org/wiki/Gibbs_measure">Gibbs measure</a>, defined as<br/><img alt="p_\beta(x;\mathcal D)dx= \frac{1}{Z(\mathcal D)}\exp \left( -\beta H(x,\mathcal{D})\right)dx" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28x%3B%5Cmathcal+D%29dx%3D+%5Cfrac%7B1%7D%7BZ%28%5Cmathcal+D%29%7D%5Cexp+%5Cleft%28+-%5Cbeta+H%28x%2C%5Cmathcal%7BD%7D%29%5Cright%29dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="Z(\mathcal D) = \int \exp\left(-\beta {H}(x, \mathcal D) \right)dx" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%29+%3D+%5Cint+%5Cexp%5Cleft%28-%5Cbeta+%7BH%7D%28x%2C+%5Cmathcal+D%29+%5Cright%29dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the normalization factor, or <strong>partition function</strong>. <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is called the <strong>inverse temperature</strong>, a name from thermodynamics. It is easy to see that when <img alt="\beta\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (i.e., when the temperature tends to the absolute zero), the Gibbs measure converges to a uniform distribution on the minimizers of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>: <img alt="p_{\beta} \to p^*" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7B%5Cbeta%7D+%5Cto+p%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Hence we can write</p>



<p><img alt="\text{Desired quantity } = \left&lt; \int p^*(x;\mathcal D) O(x,\mathcal D) dx \right&gt;{\mathcal D} = \left&lt; \lim_{\beta \rightarrow \infty} \int p_\beta(x;\mathcal D) O(x,\mathcal D) dx \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Cleft%3C+%5Cint+p%5E%2A%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E%7B%5Cmathcal+D%7D+%3D+%5Cleft%3C+%5Clim_%7B%5Cbeta+%5Crightarrow+%5Cinfty%7D+%5Cint+p_%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Physicists often exchange the order of limits and expectations at will, which generally makes sense in this setting, and so assume</p>



<p><img alt="\text{Desired quantity } = \lim_{\beta \rightarrow \infty} \left&lt; \int p_\beta(x;\mathcal D) O(x,\mathcal D) dx \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Clim_%7B%5Cbeta+%5Crightarrow+%5Cinfty%7D+%5Cleft%3C+%5Cint+p_%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Thus general approach taken in the replica method is to derive an expression for the average observable for any <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and then take the <img alt="\beta \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit. The quantity <img alt="\int p_\beta(x;\mathcal D) O(x,\mathcal D) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+p_%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is also known as the <em>thermal average</em> of <img alt="O" class="latex" src="https://s0.wp.com/latex.php?latex=O&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, since it is taken with respect to the Gibbs distribution at some positive temperature.</p>



<p>To compute the thermal average of <img alt="O" class="latex" src="https://s0.wp.com/latex.php?latex=O&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we define the following <em>augmented partition function</em>:</p>



<p><img alt="Z(\mathcal D, J) = \int_{S} \exp\left( -\beta H(x,\mathcal D) + J O(x; \mathcal D) \right) dx." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%2C+J%29+%3D+%5Cint_%7BS%7D+%5Cexp%5Cleft%28+-%5Cbeta+H%28x%2C%5Cmathcal+D%29+%2B+J+O%28x%3B+%5Cmathcal+D%29+%5Cright%29+dx.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>One can then check that</p>



<p><img alt="\frac{d}{dJ} \log Z(\mathcal{D}, J)\Big|_{J=0}= \frac{1}{Z} \int O(x,\mathcal D) \exp(- \beta H(x,\mathcal D) ) dx = \int p\beta(x;\mathcal D) O(x,\mathcal D) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7BdJ%7D+%5Clog+Z%28%5Cmathcal%7BD%7D%2C+J%29%5CBig%7C_%7BJ%3D0%7D%3D+%5Cfrac%7B1%7D%7BZ%7D+%5Cint+O%28x%2C%5Cmathcal+D%29+%5Cexp%28-+%5Cbeta+H%28x%2C%5Cmathcal+D%29+%29+dx+%3D+%5Cint+p%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Hence our desired quantity can be obtained as</p>



<p><img alt="\text{Desired quantity } = \lim_{\beta \to \infty} \frac{\partial}{\partial J} \left&lt; \log Z(\mathcal D, J) \right&gt;_{\mathcal D}(0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Clim_%7B%5Cbeta+%5Cto+%5Cinfty%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+J%7D+%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+J%29+%5Cright%3E_%7B%5Cmathcal+D%7D%280%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>or (assuming we can again exchange limits at will):</p>



<p><img alt="\text{Desired quantity } = \lim_{\epsilon \to 0} \tfrac{1}{\epsilon}\left[ \lim_{\beta \to \infty} \left&lt; \log Z(\mathcal D, \epsilon) \right&gt;{\mathcal D} - \lim_{\beta \to \infty} \left&lt; \log Z(\mathcal D, 0) \right&gt;_{\mathcal D}\right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Clim_%7B%5Cepsilon+%5Cto+0%7D+%5Ctfrac%7B1%7D%7B%5Cepsilon%7D%5Cleft%5B+%5Clim_%7B%5Cbeta+%5Cto+%5Cinfty%7D+%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+%5Cepsilon%29+%5Cright%3E%7B%5Cmathcal+D%7D+-+%5Clim_%7B%5Cbeta+%5Cto+%5Cinfty%7D+%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+0%29+%5Cright%3E_%7B%5Cmathcal+D%7D%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We see that ultimately computing the desired quantity reduces to computing quantities of the form</p>



<p><img alt="\left&lt; \log Z'(\mathcal{D}) \right&gt;_{\mathcal D}\;\; (\star)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z%27%28%5Cmathcal%7BD%7D%29+%5Cright%3E_%7B%5Cmathcal+D%7D%5C%3B%5C%3B+%28%5Cstar%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>for the original or modified partition function <img alt="Z'" class="latex" src="https://s0.wp.com/latex.php?latex=Z%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Hence our focus from now on will be on computing (<img alt="\star" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>).<br/>Averaging over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is known as “configurational average” or <strong>quenched average</strong>. All together, we obtain the observable, first thermal averaged to get <img alt="O^*(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=O%5E%2A%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (averaged over <img alt="p^*(x;\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p%5E%2A%28x%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) and then quenched averaged over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<blockquote class="wp-block-quote"><p><img alt="&#x26A0;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a0.png" style="height: 1em;"/> <em>What Concentrates?</em>: It is not just an algebraic convenience to average <img alt="\log Z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> instead of averaging <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> itself. When the system size <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is large, <img alt="\frac{1}{N} \log Z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D+%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> concentrates around its average. Thus, the typical behavior of the system can be understood by studying the quenched average <img alt="\frac{1}{N} \left&lt; \log Z \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D+%5Cleft%3C+%5Clog+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> The partition function <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> itself often does not concentrate and in general the values <img alt="\frac{1}{N} \log \left&lt; Z \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D+%5Clog+%5Cleft%3C+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (known as the “annealed average”) and <img alt="\frac{1}{N}\left&lt; \log Z \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D%5Cleft%3C+%5Clog+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (known as the “quenched average”) could differ subtantially. For more information, please consult <a href="https://web.stanford.edu/~montanar/RESEARCH/book.html">Mezard and Montanari’s</a> excellent book, Chapter 5.</p></blockquote>



<h3>Step 2: The Replica Trick</h3>



<p>Hereafter, we use <img alt="\langle \cdot \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Ccdot+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to denote the average and drop the dependence of <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. To compute <img alt="\left&lt; \log Z(\mathcal D, J) \right&gt;{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+J%29+%5Cright%3E%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we use an identity <img alt="\left&lt; \log Z \right&gt; = \lim_{n \to 0} \frac{1}{n} \log \left&lt; Z^n \right&gt;." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z+%5Cright%3E+%3D+%5Clim_%7Bn+%5Cto+0%7D+%5Cfrac%7B1%7D%7Bn%7D+%5Clog+%5Cleft%3C+Z%5En+%5Cright%3E.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>For the limit to make sense, <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> should be any real number. However, the expression for <img alt="Z^n" class="latex" src="https://s0.wp.com/latex.php?latex=Z%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is only easily computable for natural numbers. <img alt="&#x26A0;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a0.png" style="height: 1em;"/> This step is non-rigorous: we will obtain an expression for <img alt="\log \langle Z^n \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+%5Clangle+Z%5En+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for natural number <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and then take the <img alt="n \rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit after the fact.</p>



<p>Recall that under the Gibbs distribution <img alt="p_\beta(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the probability density on state <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is equal to <img alt="\exp(-\beta H(x,\mathcal D) )/Z(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+H%28x%2C%5Cmathcal+D%29+%29%2FZ%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Denote by <img alt="p_\beta(\mathcal D)^n" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> the probability distribution over a tuple <img alt="\vec{x} = (x^{(1)},\ldots,x^{(n)})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%3D+%28x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> independent samples (also known as <strong>replicas</strong>) chosen from <img alt="p_\beta(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Since the partition function <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is an integral (or sum in the discrete case) of the form <img alt="\int \exp(-\beta H(x;\mathcal D)) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+%5Cexp%28-%5Cbeta+H%28x%3B%5Cmathcal+D%29%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we can write <img alt="Z(\mathcal D)^n" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the integral of <img alt="\prod_{a=1}^n \exp(-\beta H(x^{(a)};\mathcal D))= \exp\left( - \beta \sum_{a=1}^n H(x^{(a)}, \mathcal D)\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cprod_%7Ba%3D1%7D%5En+%5Cexp%28-%5Cbeta+H%28x%5E%7B%28a%29%7D%3B%5Cmathcal+D%29%29%3D+%5Cexp%5Cleft%28+-+%5Cbeta+%5Csum_%7Ba%3D1%7D%5En+H%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="x^{(1)},\ldots,x^{(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are independent variables.</p>



<p>Now since each <img alt="x^{(a)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is weighed with a factor of <img alt="\exp(-\beta H(x^{(a)});\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+H%28x%5E%7B%28a%29%7D%29%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, this expression can be shown as equal to taking expectation of some exponential function <img alt="\exp( \sum_{a=1}^n G(x^{(a)}; \mathcal D))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%3B+%5Cmathcal+D%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> over a tuple <img alt="(x^{(1)},\ldots,x^{(n)})" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> independent samples of <strong>replicas</strong> all coming from the same Gibbs distribution <img alt="p_\beta(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> corresponding to the same instance <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>(The discussion on <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is just for intuition – we will not care about the particular form of this <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, since soon average it over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.)</p>



<p>Hence</p>



<p><img alt="\left&lt; Z^n \right&gt; = \left&lt; \int_{\vec{x} \sim p_*(\mathcal D)^n} \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) dx \right) \right&gt;_{\mathcal D}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cleft%3C+%5Cint_%7B%5Cvec%7Bx%7D+%5Csim+p_%2A%28%5Cmathcal+D%29%5En%7D+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+dx+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>Step 3: The Order Parameters</h3>



<p>The above expression is an expectation of an integral, and so we can switch the order of summation, and write it also as</p>



<p><img alt="\left&lt; Z^n \right&gt; = \int_{\vec{x} \sim p_*(\mathcal D)^n} \left&lt; \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) \right) \right&gt;_{\mathcal D} dx." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint_%7B%5Cvec%7Bx%7D+%5Csim+p_%2A%28%5Cmathcal+D%29%5En%7D+%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D+dx.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>It turns out that for natural energy functions (for example when <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is quadratic in <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> such as when it corresponds to Mean Squared Error loss), for any tuple of <img alt="x^{(1)},\ldots,x^{(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the expectation over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of <img alt="\exp\left( - \beta \sum_{a=1}^n H(x^{(a)}, \mathcal D) \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+-+%5Cbeta+%5Csum_%7Ba%3D1%7D%5En+H%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> only depends on the angles between the <img alt="x^(a)" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%28a%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>‘s.<br/>That is, rather than depending on all of these <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>-dimensional vectors, it only depends on the <img alt="n^2" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> coefficients <img alt="Q_{ab} =\frac{1}{N} x^{(a)}\cdot x^{(b)}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D%5Cfrac%7B1%7D%7BN%7D+x%5E%7B%28a%29%7D%5Ccdot+x%5E%7B%28b%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix Q is known as the <strong>overlap matrix</strong> or <strong>order parameters</strong> and one can often find a nice analytical function <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> whose values are bounded (independently of <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) such that</p>



<p><img alt="\left&lt; \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) \right) \right&gt;_{\mathcal D} = \exp(-nN \mathcal{F}(Q))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D+%3D+%5Cexp%28-nN+%5Cmathcal%7BF%7D%28Q%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Hence we can replace the integral over <img alt="x^{(1)},\ldots,x^{(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with an integral over <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and write</p>



<p><img alt="\left&lt; Z^n \right&gt; = \int dQ \exp\left(- n N \mathcal F(Q) \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint+dQ+%5Cexp%5Cleft%28-+n+N+%5Cmathcal+F%28Q%29+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where the measure <img alt="dQ" class="latex" src="https://s0.wp.com/latex.php?latex=dQ&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the one induced by the overlap distribution of a tuple <img alt="\vec{x} \sim p_\beta(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%5Csim+p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> taken for a random choice of the parameters <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Since <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> only ranges over a small (<img alt="n^2" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> dimensional set), at the large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit, the integral is dominated by the maximum of its integrand (“method of steepest descent” / “saddle point method”). Let <img alt="Q^*" class="latex" src="https://s0.wp.com/latex.php?latex=Q%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be the global minimum of <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (within some space of matrices). We have</p>



<p><img alt="\lim_{N \rightarrow \infty}\left&lt; Z^n \right&gt; = \exp(-n N \mathcal F(Q^* ))." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN+%5Crightarrow+%5Cinfty%7D%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cexp%28-n+N+%5Cmathcal+F%28Q%5E%2A+%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Once we arrive at this expression, the configurational average of <img alt="-\log Z" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is simply <img alt="N \mathcal F(Q^*)" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Cmathcal+F%28Q%5E%2A%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. These steps constitute the replica method. The ability to compute the configurational average by creating an appropriate <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is one of the factors determining whether the replica method can be used. For example, in the supervised learning example, <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is almost always assumed to be quadratic in <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>; cross-entropy loss, for instance, is generally not amendable.</p>



<blockquote class="wp-block-quote"><p><img alt="&#x26A0;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a0.png" style="height: 1em;"/> <em>Bad Math Warning</em>: there are three limits, <img alt="\beta \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="N \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and <img alt="n \rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In replica calculations, we assume that we take take these limits in whichever order that is arithmetically convenient.</p></blockquote>



<p><strong>Coming up:</strong> In <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/">part two of this blog post</a>, we will explain the replica symmetric assumption (or “Ansatz” in Physics-speak) on the order parameters <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and then demonstrate how to use the replica method for two simple examples: <em>least squares regression</em> and <em>spiked matrix model</em>.</p></div>
    </content>
    <updated>2021-08-11T15:00:11Z</updated>
    <published>2021-08-11T15:00:11Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-08-15T06:21:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-7046389272941836220</id>
    <link href="http://processalgebra.blogspot.com/feeds/7046389272941836220/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=7046389272941836220" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7046389272941836220" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7046389272941836220" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/08/the-first-movie.html" rel="alternate" type="text/html"/>
    <title>The First: A Movie</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I had the great pleasure to watch <a href="https://pantar.com/" target="_blank">Ali Hossaini</a> and <a href="http://www.lucavigano.com/" target="_blank">Luca Viganò</a>'s short movie "<a href="https://www.nationalgallery.org.uk/national-gallery-x/the-ai-gallery" target="_blank">The First</a>" that has been released by National Gallery X. "The First" is a coproduction of <a href="https://www.tas.ac.uk/" target="_blank">UKRI TAS Hub</a>, <a href="https://rusi.org/" target="_blank">RUSI </a>and <a href="https://www.nationalgallery.org.uk/national-gallery-x" target="_blank">National Gallery X</a>. It was produced as a keynote for <a href="https://www.tas.ac.uk/bigeventscpt/trusting-machines/" target="_blank">Trusting Machines?</a>, a conference on how to develop trustworthy AI. </p><p>For the little that it may be worth, I strongly recommend the movie. Do watch also the <a href="https://vimeo.com/577412746" target="_blank">conversation</a> between National Gallery X co-director Ali Hossaini and Luca Viganò, possibly before enjoying a second viewing of the movie. You can also read the paper "<a href="https://arxiv.org/pdf/1807.06078.pdf" target="_blank">Gnirut: The Trouble With Being Born Human In An Autonomous World</a>" mentioned in that conversation.  </p><p>I fully subscribe to Luca Viganò's vision of using artistic tools to explain computer science concepts to the public, whose members will have to make use of the technological artifacts based on those concepts. Indeed, we live in a world in which technologists will increasingly have to be great humanists. IMHO, we are lucky to have people like Luca Viganò, who is also a playwright, paving the way in connecting "<a href="https://en.wikipedia.org/wiki/The_Two_Cultures" target="_blank">The Two Cultures</a>". (In case any of you is interested, I recommend Luca Viganò's  <a href="https://us02web.zoom.us/rec/play/3uNdz1F0g1JCAir0C10Y3_jVs7k6fJgrIwr5RomnzBJGTjqBMOYv6SJgD2jINwMTuBvx-CSsHclof5Vn.29XNpJA7FKbfL5dG?autoplay=true&amp;startTime=1614787555000" target="_blank">GSSI+ICE-TCS webinar</a>.)</p><p> </p></div>
    </content>
    <updated>2021-08-11T09:38:00Z</updated>
    <published>2021-08-11T09:38:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-08-11T09:39:22Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/08/10/relandscaping</id>
    <link href="https://11011110.github.io/blog/2021/08/10/relandscaping.html" rel="alternate" type="text/html"/>
    <title>Relandscaping</title>
    <summary>We recently redid our front yard, after spending too long with a boring flat weedy mostly-unused and water-thirsty lawn and after our neighbor took out the only part of it that we actually cared for, a liquidambar tree between our yards. This week’s WADS/CCCG conference, with its four-hour time difference from Pacific, gave me an excuse to catch it in the early morning light:</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We recently redid our front yard, after spending too long with a boring flat weedy mostly-unused and water-thirsty lawn and after our neighbor took out the only part of it that we actually cared for, a liquidambar tree between our yards. This week’s WADS/CCCG conference, with its four-hour time difference from Pacific, gave me an excuse to catch it in the early morning light:</p>

<p style="text-align: center;"><img alt="My house with its old front lawn" src="https://www.ics.uci.edu/~eppstein/pix/frontyard/frontyard-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p>I don’t have a photo of the old front, so for comparison here’s a similar angle snagged from Google Street View.</p>

<p style="text-align: center;"><img alt="My house with its old front lawn" src="https://www.ics.uci.edu/~eppstein/pix/frontyard/lawn.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>

<p>More for my own later benefit than because I imagine anyone else cares:</p>

<p>The frontmost tree is a <a href="https://en.wikipedia.org/wiki/Parkinsonia_florida">palo verde</a>; a neighbor across the street has a much bigger one, a little messy but quite pretty, especially when covered with its yellow flowers. The tree in the back corner with the blue bistro table under it is some kind of mesquite, I think maybe a black mesquite; the flagstone path makes a loop around it. The two tallest bushes, forming a quadrilateral with the two trees, are <a href="https://en.wikipedia.org/wiki/Feijoa_sellowiana">pineapple guavas / feijoas</a>.</p>

<p>Behind the palo verde and against the house are some <a href="https://en.wikipedia.org/wiki/Kangaroo_paw">kangaroo paws</a>. The closest medium-sized bush on the bottom right (and elsewhere) is <a href="https://en.wikipedia.org/wiki/Salvia_yangii">Russian sage</a>. There are also some <a href="https://en.wikipedia.org/wiki/Buddleja_davidii">butterfly bushes</a>, and several other low bushes and flowers whose names I already lost track of or didn’t get. I think the tall bunchgrass may be a <a href="https://en.wikipedia.org/wiki/Lomandra">lomandra</a>, and the low blue ones <a href="https://en.wikipedia.org/wiki/Festuca_glauca">blue fescue</a>.</p>

<p>Lining the path to the door is <a href="https://en.wikipedia.org/wiki/Curio_repens">senecio / blue chalksticks</a>, and there’s more of it around the base of the mesquite. The groundcover in front is <a href="https://en.wikipedia.org/wiki/Dymondia">dymondia / silver carpet</a> (well, and some crabgrass and spurge, but let’s not count that), with some <a href="https://11011110.github.io/blog/2021/08/10/Myoporum parvifolium">myoporum parvifolium</a> behind it (the low spreading groundcover with white flowers); there’s more myoporum barely visible near the porch bench.</p>

<p>The neighbor ended up replacing the liquidambar (at the far left of the Street View shot) with a <a href="https://11011110.github.io/blog/2021/08/10/Podocarpus henkelii">podocarpus henkelii</a> and some bottlebrushes.</p></div>
    </content>
    <updated>2021-08-10T21:24:00Z</updated>
    <published>2021-08-10T21:24:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-08-11T04:43:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19041</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/08/10/p-vs-np-proof-claims/" rel="alternate" type="text/html"/>
    <title>P vs NP Proof Claims</title>
    <summary>Ken Ribet once was sent a freebie book that he looked at and decided he didn’t want, so took it to a second hand bookstore on his lunch break, sold it, and bought lunch with the proceeds. On the way back to the math department he realized he’d turned theorems into coffee. IAS page Robbert […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
Ken Ribet once was sent a freebie book that he looked at and decided he didn’t want, so took it to a second hand bookstore on his lunch break, sold it, and bought lunch with the proceeds. On the way back to the math department he realized he’d turned theorems into coffee.<br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p/><p>
</p><p/>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/08/10/p-vs-np-proof-claims/dijkgraafias/" rel="attachment wp-att-19043"><img alt="" class="alignright size-full wp-image-19043" height="144" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/DijkgraafIAS.jpg?resize=120%2C144&amp;ssl=1" width="120"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">IAS <a href="https://www.ias.edu/scholars/dijkgraaf">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Robbert Dijkgraaf is a mathematical physicist and the current Director of the Institute for Advanced Study in Princeton. He is also a down-to-earth communicator of mathematics and a <a href="https://www.ias.edu/news/dijkgraaf-blackhole-mural">spacy</a> surrealist artist. He wrote a guest <a href="https://www.quantamagazine.org/the-subtle-art-of-the-mathematical-conjecture-20190507/">column</a> for <em>Quanta</em> two years ago titled, “The Subtle Art of the Mathematical Conjecture.”</p>
<p>
Today I talk again about what I feel is a meta-error in all the claims to resolve our favorite conjecture, <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Or <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if you are so inclined.</p>
<p>
I have an issue with them that goes beyond well-noted <a href="https://www.scottaaronson.com/blog/?p=458">advice</a> by Scott Aaronson on how to tell claimed proofs of <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are wrong. It is not about whether they are incorrect. They all are incorrect. And I believe that they will continue to be so into the future.</p>
<p>
My problem with these claims is: they are always all or nothing. </p>
<p>
The claims never improve what we know about <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> versus <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. They always resolve the entire question. There is no partial result, no improvement of what we know. They always solve the conjecture and are ready to get the $1,000,000 dollars. Of course the prize money is not in any jeopardy. </p>
<p>
</p><p/><h2> Climbing a Mountain </h2><p/>
<p/><p>
This is where Dijkgraaf’s article comes in. He talks first about mountain climbing (as we have <a href="https://rjlipton.wpcomstaging.com/2010/02/02/climbing-mountains-and-proving-theorems/">also</a> <a href="https://rjlipton.wpcomstaging.com/2012/12/08/mounting-or-solving-open-problems/">done</a>) rather than art:</p>
<blockquote><p><b> </b> <em> Mountain climbing is a beloved metaphor for mathematical research. … [T]he role of these highest peaks is played by the great conjectures—sharply formulated statements that are most likely true but for which no conclusive proof has yet been found.</em></p><em>
</em><p><em>
The highest summits are not conquered in a single effort. Climbing expeditions carefully lay out base camps and fixed ropes, then slowly work their way to the peak. Similarly, in mathematics one often needs to erect elaborate structures to attack a major problem. A direct assault is seen as foolish and naive. These auxiliary mathematical constructions can sometimes take centuries to build and in the end often prove to be more valuable than the conquered theorem itself. The scaffold then becomes a permanent addition to the architecture of mathematics. </em>
</p></blockquote>
<p/><p>
What strikes me and Ken about the <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> versus <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> claims we see so often is that they try to reach the summit in one bound. There is no scaffolding, no rope, no new handhold. The reader learns little.</p>
<p>
The term that most needs expounding is <em>base camp</em>. A base camp is not at the bottom. The main <a href="https://en.wikipedia.org/wiki/Everest_base_camps">base camps</a> for Mount Everest are halfway up to the summit. Getting to a base camp takes substantial work by itself. <em>Building</em> a base camp certainly does. But getting to one is essential. This is what is missing for <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> versus <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
</p><p/><h2> P&lt;NP Base Camps </h2><p/>
<p/><p>
Let’s look at <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Suppose you claim that you can show that CLIQUE requires super polynomial time. This is what you need to do to prove <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This is way beyond anything we can imagine proving. </p>
<p>
Suppose rather you claimed that CLIQUE requires <img alt="{m^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> deterministic time where <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the number of edges. This would be the best result ever on the difficulty of CLIQUE. It would easily be the best paper in complexity theory in decades. Would win prizes of all kinds. </p>
<p>
It is even worse. If one could prove that CLIQUE is not in deterministic time 	</p>
<p align="center"><img alt="\displaystyle  m\log\log\log m " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m%5Clog%5Clog%5Clog+m+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>that would also be a major result. Forget about proving a lower bound of 	</p>
<p align="center"><img alt="\displaystyle  m^{1000} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m%5E%7B1000%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>and more that is needed to solve <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Just the above would be major.</p>
<p>
If we skirt around some technical issues with time on Turing machines, we can pitch our camp right at going beyond linear time. Or certainly linear size circuits. Nobody knows a language in <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that does not have linear size circuits. Proving one would bring enough renown for anyone.</p>
<p>
</p><p/><h2> P=NP Base Camps </h2><p/>
<p/><p>
Let’s look at <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Most regard this as the far side of the mountain but please bear with us to the end. Suppose you claim that CLIQUE is in polynomial time. </p>
<p>
The usual paper of this type claims it is doable by some straightforward polynomial time algorithm. The method might use linear programming in some standard manner. This might lead to a <em>practical</em> algorithm, but none of those has ever been observed in the wild. Even worse, any proof that CLIQUE can be resolved in time 	</p>
<p align="center"><img alt="\displaystyle  n^{C} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n%5E%7BC%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>would be also the best result ever on this problem. This applies even if <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an unknown constant, or is equal to some astronomically large value. Think <a href="https://en.wikipedia.org/wiki/Graham%27s_number">Graham’s number</a> or the <a href="https://en.wikipedia.org/wiki/Skewes%27s_number">Skewes</a> number: 	</p>
<p align="center"><img alt="\displaystyle 10^{10^{10^{502}}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+10%5E%7B10%5E%7B10%5E%7B502%7D%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Nor do we need <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be constant. Say it is <img alt="{O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or some polynomial in <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This would be a <em>quasi-polynomial</em> time algorithm. Here a really big campsite was built by László Babai, who <a href="https://www.quantamagazine.org/graph-isomorphism-vanquished-again-20170114/">proved</a> that Graph Isomorphism is in quasi-polynomial time. His <a href="http://people.cs.uchicago.edu/~laci/17groups/version2.1.pdf">paper</a> has a wealth of ideas that might be extended.</p>
<p>
But what really might be attractive about this side is that you can make progress without “shaking the Earth.” These results would be in the direction of <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> but not opposed by as many factors:</p>
<ul>
<li>
<em>Refute the Strong Exponential Time Hypothesis</em> (SETH). That is, find an algorithm in time <img alt="{2^{O(cn)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7BO%28cn%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with <img alt="{c &lt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for CNF-SAT. We have written about SETH <a href="https://rjlipton.wpcomstaging.com/2015/06/01/puzzling-evidence/">here</a>. <p/>
</li><li>
<em>Refute the Unique Games Conjecture</em> (UGC). Unlike SETH, this technically does not imply <img alt="{\mathsf{P &lt; NP}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> But refuting it does reduce the reach of <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-hardness. A large special case of UGC was, however, <a href="https://ieeexplore.ieee.org/document/8555140">proved</a> three years ago.
</li></ul>
<p>
Or prove that they cannot both be true simultaneously. My old <a href="https://rjlipton.wpcomstaging.com/2010/05/05/unique-games-a-three-act-play/">post</a> on UGC covered a sense in which there is no “SETH for UGC.” </p>
<p>
</p><p/><h2> But … </h2><p/>
<p/><p>
The trouble with our insight is that in the past, sometimes a full conjecture has been solved. That is, partial progress did not happen first—the mountain was scaled in one go. Or at least a lot of it, from a relatively low base camp. For <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> there is an essence of “polynomial” versus “exponential” that is sharply defined in other ways, for instance Mikhail Gromov’s theorem about growth rates in groups, which we wrote about <a href="https://rjlipton.wpcomstaging.com/2013/06/20/three-theorems-about-growth/">here</a>.</p>
<p>
Ken and I have differed on how long and steep and sudden the final ascent was for the <a href="https://en.wikipedia.org/wiki/Four_color_theorem">Four Color Theorem</a> and <a href="https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem">Fermat’s Last Theorem</a> (FLT). The proof of the former by Kenneth Appel and Wolfgang Haken was a watershed for its use of computers, but drew on ideas that had gone through the non-computer proofs-and-refutations process. Andrew Wiles’s announcement of FLT was a shock even with a three-day buildup of his lectures at a conference in 1993 having hinted it to several attendees. But he drew on partial progress that had been ramped up by Ribet and others since the mid-1980s.</p>
<p>
Maybe if Évariste Galois had beaten Niels Abel to showing the unsolvability of the quintic, his invention of group theory for the proof used today would have been a single bound. But Abel got a big lift from Paolo Ruffini’s 500 pages of work in 1799. (Évariste is the same name as <a href="https://appellationmountain.net/baby-name-of-the-day-everest/">Everest</a>, go figure.)</p>
<p>
The proof of the Boolean Sensitivity Conjecture two years ago by Hao Huang was short and sudden. But along lines remarked also in Dijkgraaf’s article, perhaps it was “more of a foothill.” Or maybe a base camp for harder problems, such as improving the upper bound from quartic to cubic or quadratic, as we discussed <a href="https://rjlipton.wpcomstaging.com/2019/07/12/tools-and-sensitivity/">here</a> and <a href="https://rjlipton.wpcomstaging.com/2019/07/25/discrepancy-games-and-sensitivity/">here</a>.</p>
<p>
This leads Ken into a historical daydream, taking over from here.</p>
<p>
</p><p/><h2> A Fermat Fantasy </h2><p/>
<p/><p>
Pierre de Fermat famously <a href="https://www.maths-et-tiques.fr/index.php/detentes/la-conjecture-de-fermat">wrote</a> the following in French in the margin of his copy of the famous book on arithmetic by Diophantus:</p>
<blockquote><p><b> </b> <em> Un cube n’est jamais la somme de deux cubes, une puissance quatrième n’est jamais la somme de deux puissances quatriémes et plus généralement aucune puissance supérieure à 2 n’est la somme de deux puissances analogues. J’ai trouvé une merveilleuse démonstration de cette proposition, mais la marge est trop étroite pour la contenir. </em>
</p></blockquote>
<p/><p>
I (Ken) think he could just as easily have written the following—and in place of the margin being too narrow, he could have given a more reasonable excuse, one I know all too well:</p>
<blockquote><p><b> </b> <em> Un cube n’est jamais la somme de moins que trois cubes, une puissance quatrième n’est jamais la somme de moins que quatre puissances quatriémes, et plus généralement aucune puissance n’est la somme de un moindre nombre de puissances analogues. J’ai trouvé une merveilleuse démonstration de cette proposition, que je rédigerai après avoir traité onze nouveaux cas de triche aux échecs en ligne. </em>
</p></blockquote>
<p/><p>
The stronger statement here is that no cube can be a nontrivial sum of fewer than three cubes (such as <img alt="{6^3 = 3^3 + 4^3 + 5^3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B6%5E3+%3D+3%5E3+%2B+4%5E3+%2B+5%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>), no fourth power a sum of fewer than four other like powers, and so on. This also was a hallowed conjecture that stood for centuries, <a href="https://en.wikipedia.org/wiki/Euler's_sum_of_powers_conjecture">named for</a> the giant Leonhard Euler no less. Well, if Pierre had just changed a few of his words, then <em>this</em> is what we would have known as FLT. Call it EFLT. It could have been just as worthy. That there were reservations known to Euler, even as he lent his name to it, might have made it all the more Olympian. </p>
<p>
Then what we actually know as FLT would have been a hard-work base camp for EFLT. Would we have seen the vast number of unsuccessful FLT proofs directed at EFLT instead? By people claiming to climb this peak in one bound—without first trying to prove that a sum of <b>just two</b> <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-th powers cannot be a higher <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-th power, for all <img alt="{n \geq 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? Well, there would have been a problem with that, one we <a href="https://rjlipton.wpcomstaging.com/2015/09/03/open-problems-that-might-be-easy/">discussed</a> in connection with solutions that might be easy after all:</p>
<p/><p align="center"><img alt="\displaystyle  \begin{array}{rcl}  144^5 &amp;=&amp; 27^5 + 84^5 + 110^5 + 133^5.\\ 20615673^4 &amp;=&amp; 2682440^4 + 15365639^4 + 18796760^4. \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++144%5E5+%26%3D%26+27%5E5+%2B+84%5E5+%2B+110%5E5+%2B+133%5E5.%5C%5C+20615673%5E4+%26%3D%26+2682440%5E4+%2B+15365639%5E4+%2B+18796760%5E4.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Do you have your own issues with these claimed proofs? Or, do you see other cases of people having suddenly scaled a mountain in one stride?</p>
<p/></font></font></div>
    </content>
    <updated>2021-08-10T19:50:43Z</updated>
    <published>2021-08-10T19:50:43Z</published>
    <category term="All Posts"/>
    <category term="graph isomorphism"/>
    <category term="History"/>
    <category term="P=NP"/>
    <category term="Proofs"/>
    <category term="base camp"/>
    <category term="claimed proofs"/>
    <category term="conjectures"/>
    <category term="Euler"/>
    <category term="Fermat"/>
    <category term="mountain"/>
    <category term="Robbert Dijkgraaf"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-08-15T06:20:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5706</id>
    <link href="https://www.scottaaronson.com/blog/?p=5706" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5706#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5706" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Yet more mistakes in papers</title>
    <summary xml:lang="en-US">In my last post, I came down pretty hard on the blankfaces: people who relish their power to persist in easily-correctable errors, to the detriment of those subject to their authority. The sad truth, though, is that I don’t obviously do better than your average blankface in my ability to resist falsehoods on early encounter […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>In my <a href="https://www.scottaaronson.com/blog/?p=5675">last post</a>, I came down pretty hard on the blankfaces: people who relish their power to persist in easily-correctable errors, to the detriment of those subject to their authority.  The sad truth, though, is that <em>I</em> don’t obviously do better than your average blankface in my ability to resist falsehoods on early encounter with them.  As one of many examples that readers of this blog might know, I didn’t think covid seemed like a big deal in early February 2020—although by mid-to-late February 2020, I’d repented of my doofosity.  If I have <em>any</em> tool with which to unblank my face, then it’s only my extreme self-consciousness when confronted with evidence of my own stupidities—the way I’ve trained myself over decades in science to see error-correction as a or even <em>the</em> fundamental virtue.</p>



<p>Which brings me to today’s post.  Continuing what’s become a <em>Shtetl-Optimized</em> tradition—see <a href="https://www.scottaaronson.com/blog/?p=2072">here from 2014</a>, <a href="https://www.scottaaronson.com/blog/?p=2854">here from 2016</a>, <a href="https://www.scottaaronson.com/blog/?p=3256">here from 2017</a>—I’m going to fess up to two serious mistakes in research papers on which I was a coauthor.</p>



<hr class="wp-block-separator"/>



<p>In 2015, Andris Ambainis and I had a STOC paper entitled <a href="https://arxiv.org/abs/1411.5729">Forrelation: A Problem that Optimally Separates Quantum from Classical Computing</a>.  We gave two main results there:</p>



<ol><li>A Ω((√N)/log(N)) lower bound on the randomized query complexity of my “Forrelation” problem, which was known to be solvable with only a single quantum query.</li><li>A proposed way to take any k-query quantum algorithm that queries an N-bit string, and simulate it using only O(N<sup>1-1/2k</sup>) classical randomized queries.</li></ol>



<p>Later, <a href="https://arxiv.org/abs/2008.07003">Bansal and Sinha</a> and independently <a href="https://arxiv.org/abs/2008.10223">Sherstov, Storozhenko, and Wu</a> showed that a k-query generalization of Forrelation, which I’d also defined, requires ~Ω(N<sup>1-1/2k</sup>) classical randomized queries, in line with my and Andris’s conjecture that k-fold Forrelation <em>optimally</em> separates quantum and classical query complexities.</p>



<p>A couple months ago, alas, my former grad school officemate <a href="https://www.cse.cuhk.edu.hk/~andrejb/">Andrej Bogdanov</a>, along with Tsun Ming Cheung and Krishnamoorthy Dinesh, emailed me and Andris to say that they’d discovered an error in result 2 of our paper (result 1, along with the Bansal-Sinha and Sherstov-Storozhenko-Wu extensions of it, remained fine).  So, adding our own names, we’ve now posted a <a href="https://eccc.weizmann.ac.il/report/2021/115/">preprint on ECCC</a> that explains the error, while also showing how to recover our result for the special case k=1: that is, any 1-query quantum algorithm really can be simulated using only O(√N) classical randomized queries.</p>



<p>Read the preprint if you really want to know the details of the error, but to summarize it in my words: Andris and I used a trick that we called “variable-splitting” to handle variables that have way more influence than average on the algorithm’s acceptance probability.  Alas, variable-splitting fails to take care of a situation where there are a bunch of variables that are non-influential individually, but that on some unusual input string, can “conspire” in such a way that their signs all line up and their contribution overwhelms those from the other variables.  A single mistaken inequality fooled us into thinking such cases were handled, but an explicit counterexample makes the issue obvious.</p>



<p>I <em>still</em> conjecture that my original guess was right: that is, I conjecture that any problem solvable with k quantum queries is solvable with O(N<sup>1-1/2k</sup>) classical randomized queries, so that k-fold Forrelation is the extremal example, and so that no problem has constant quantum query complexity but linear randomized query complexity.  More strongly, I reiterate the conjecture that any bounded degree-d real polynomial, p:{0,1}<sup>N</sup>→[0,1], can be approximated by querying only O(N<sup>1-1/d</sup>) input bits drawn from some suitable distribution.  But proving these conjectures, if they’re true, will require a new algorithmic idea.</p>



<hr class="wp-block-separator"/>



<p>Now for the second <em>mea culpa</em>.  Earlier this year, my student Sabee Grewal and I posted a short preprint on the arXiv entitled <a href="https://arxiv.org/abs/2102.10458">Efficient Learning of Non-Interacting Fermion Distributions</a>.  In it, we claimed to give a classical algorithm for reconstructing any “free fermionic state” |ψ⟩—that is, a state of n identical fermionic particles, like electrons, each occupying one of m&gt;n possible modes, that can be produced using only “fermionic beamsplitters” and no interaction terms—and for doing so in polynomial time and using a polynomial number of samples (i.e., measurements of where all the fermions are, given a copy of |ψ⟩).  Alas, after trying to reply to confused comments from readers and reviewers (albeit, none of them <em>exactly</em> putting their finger on the problem), Sabee and I were able to figure out that we’d done no such thing.</p>



<p>Let me explain the error, since it’s actually really interesting.  In our underlying problem, we’re trying to find a collection of unit vectors, call them |v<sub>1</sub>⟩,…,|v<sub>m</sub>⟩, in C<sup>n</sup>.  Here, again, n is the number of fermions and m&gt;n is the number of modes.  By measuring the “2-mode correlations” (i.e., the probability of finding a fermion in both mode i and mode j), we can figure out the approximate value of |⟨v<sub>i</sub>|v<sub>j</sub>⟩|—i.e., the absolute value of the inner product—for any i≠j.  From that information, we want to recover |v<sub>1</sub>⟩,…,|v<sub>m</sub>⟩ themselves—or rather, their relative configuration in n-dimensional space, isometries being irrelevant.</p>



<p>It seemed to me and Sabee that, if we knew ⟨v<sub>i</sub>|v<sub>j</sub>⟩ for all i≠j, then we’d get linear equations that iteratively constrained each |v<sub>j</sub>⟩ in terms of ⟨v<sub>i</sub>|v<sub>j</sub>⟩ for j&lt;i, so all we’d need to do is solve those linear systems, and then (crucially, and this was the main work we did) show that the solution would be <em>robust</em> with respect to small errors in our estimates of each ⟨v<sub>i</sub>|v<sub>j</sub>⟩.  It seemed further to us that, while it was true that the measurements only revealed |⟨v<sub>i</sub>|v<sub>j</sub>⟩| rather than ⟨v<sub>i</sub>|v<sub>j</sub>⟩ itself, the “phase information” in ⟨v<sub>i</sub>|v<sub>j</sub>⟩ was manifestly irrelevant, as it in any case depended on the irrelevant global phases of |v<sub>i</sub>⟩ and |v<sub>j</sub>⟩ themselves.</p>



<p>Alas, it turns out that the phase information <em>does</em> matter.  As an example, suppose I told you only the following about three unit vectors |u⟩,|v⟩,|w⟩ in R<sup>3</sup>:</p>



<p>|⟨u|v⟩| = |⟨u|w⟩| = |⟨v|w⟩| = 1/2.</p>



<p>Have I thereby determined these vectors up to isometry?  Nope!  In one class of solution, all three vectors belong to the same plane, like so:</p>



<p>|u⟩=(1,0,0),<br/>|v⟩=(1/2,(√3)/2,0),<br/>|w⟩=(-1/2,(√3)/2,0).</p>



<p>In a completely different class of solution, the three vectors <em>don’t</em> belong to the same plane, and instead look like three edges of a tetrahedron meeting at a vertex:</p>



<p>|u⟩=(1,0,0),<br/>|v⟩=(1/2,(√3)/2,0),<br/>|w⟩=(1/2,1/(2√3),√(2/3)).</p>



<p>These solutions correspond to different sign choices for |⟨u|v⟩|, |⟨u|w⟩|, and |⟨v|w⟩|—choices that <em>collectively</em> matter, even though each of them is individually irrelevant.</p>



<p>It follows that, even in the special case where the vectors are all real, the 2-mode correlations are <em>not </em>enough information to determine the vectors’ relative positions.  (Well, it takes some more work to convert this to a counterexample that could actually arise in the fermion problem, but that work can be done.)  And alas, the situation gets even gnarlier when, as for us, the vectors can be complex.</p>



<p>Any possible algorithm for our problem will have to solve a system of <em>non</em>linear equations (albeit, a massively overconstrained system that’s guaranteed to have a solution), and it will have to use <em>3-mode</em> correlations (i.e., statistics of <em>triples</em> of fermions), and quite possibly 4-mode correlations and above.</p>



<p>But now comes the good news!  Googling revealed that, for reasons having nothing to do with fermions or quantum physics, problems <em>extremely</em> close to ours had already been studied in classical machine learning.  The key term here is <a href="https://en.wikipedia.org/wiki/Determinantal_point_process">“Determinantal Point Processes”</a> (DPPs).  A DPP is a model where you specify an m×m matrix A (typically symmetric or Hermitian), and then the probabilities of various events are given by the determinants of various principal minors of A.  Which is <em>precisely</em> what happens with fermions!  In terms of the vectors |v<sub>1</sub>⟩,…,|v<sub>m</sub>⟩ that I was talking about before, to make this connection we simply let A be the m×m <em>covariance matrix</em>, whose (i,j) entry equals ⟨v<sub>i</sub>|v<sub>j</sub>⟩.</p>



<p>I first learned of this remarkable correspondence between fermions and DPPs a decade ago, from a talk on DPPs that <a href="https://www.cs.washington.edu/people/faculty/taskar">Ben Taskar</a> gave at MIT.  Immediately after the talk, I made a mental note that Taskar was a rising star in theoretical machine learning, and that his work would probably be relevant to me in the future.  While researching this summer, I was devastated to learn that Taskar died of heart failure in 2013, in his mid-30s and only a couple of years after I’d heard him speak.</p>



<p>The most relevant paper for me and Sabee was called <a href="https://www.alexkulesza.com/pubs/spmap_laa14.pdf">An Efficient Algorithm for the Symmetric Principal Minor Assignment Problem</a>, by Rising, Kulesza, and Taskar.  Using a combinatorial algorithm based on minimum spanning trees and chordless cycles, this paper <em>nearly</em> solves our problem, except for two minor details:</p>



<ol><li>It doesn’t do an error analysis, and</li><li>It considers complex <em>symmetric</em> matrices, whereas our matrix A is <a href="https://en.wikipedia.org/wiki/Hermitian_matrix">Hermitian</a> (i.e., it equals its <em>conjugate</em> transpose, not its transpose).</li></ol>



<p>So I decided to email <a href="https://www.alexkulesza.com/">Alex Kulezsa</a>, one of Taskar’s surviving collaborators who’s now a research scientist at Google NYC, to ask his thoughts about the Hermitian case.  Alex kindly replied that they’d been meaning to study that case—a reviewer had even asked about it!—but they’d ran into difficulties and didn’t know what it was good for.  I asked Alex whether he’d like to join forces with me and Sabee in tackling the Hermitian case, which (I told him) was enormously relevant in quantum physics.  To my surprise and delight, Alex agreed.</p>



<p>So we’ve been working on the problem together, making progress, and I’m optimistic that we’ll have <em>some</em> nice result.  By using the 3-mode correlations, at least “generically” we can recover the entries of the matrix A <em>up to complex conjugation</em>, but further ideas will be needed to resolve the complex conjugation ambiguity, to whatever extent it actually matters.</p>



<p>In short: on the negative side, there’s much more to the problem of learning a fermionic state than we’d realized.  But on the positive side, there’s much more to the problem than we’d realized!  As with the simulation of k-query quantum algorithms, my coauthors and I would welcome any ideas.  And I apologize to anyone who was misled by our premature (and hereby retracted) claims.</p>



<hr class="wp-block-separator"/>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Aug. 11):</span></strong> Here’s a third bonus retraction, which I thank my colleague <a href="https://www.markwilde.com/">Mark Wilde</a> for bringing to my attention.  Way back in 2005, in my <a href="https://arxiv.org/abs/quant-ph/0502072">NP-complete Problems and Physical Reality</a> survey article, I “left it as an exercise for the reader” to prove that BQP<sub>CTC</sub>, or quantum polynomial time augmented with Deutschian closed timelike curves, is contained in a complexity class called SQG (Short Quantum Games).  While it turns out to be <em>true</em> that BQP<sub>CTC</sub> ⊆ SQG—as follows from <a href="https://arxiv.org/abs/0808.2669">my and Watrous’s 2008 result</a> that BQP<sub>CTC</sub> = PSPACE, combined with <a href="https://arxiv.org/abs/1011.2787">Gutoski and Wu’s 2010 result</a> that SQG = PSPACE—it’s not something for which I could possibly have had a correct proof back in 2005.  I.e., it was a harder exercise than I’d intended!</p></div>
    </content>
    <updated>2021-08-10T19:02:42Z</updated>
    <published>2021-08-10T19:02:42Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Embarrassing Myself"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-08-15T01:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/116</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/116" rel="alternate" type="text/html"/>
    <title>TR21-116 |  Quantum Meets the Minimum Circuit Size Problem | 

	Nai-Hui Chia, 

	Chi-Ning  Chou, 

	Jiayu Zhang, 

	Ruizhe Zhang</title>
    <summary>In this work, we initiate the study of the Minimum Circuit Size Problem (MCSP) in the quantum setting. MCSP is a problem to compute the circuit complexity of Boolean functions. It is a fascinating problem in complexity theory---its hardness is mysterious, and a better understanding of its hardness can have surprising implications to many fields in computer science.

We first define and investigate the basic complexity-theoretic properties of minimum quantum circuit size problems for three natural objects: Boolean functions, unitaries, and quantum states. We show that these problems are not trivially in NP but in QCMA (or have QCMA protocols). Next, we explore the relations between the three quantum MCSPs and their variants. We discover that some reductions that are not known for classical MCSP exist for quantum MCSPs for unitaries and states, e.g., search-to-decision reduction and self-reduction. Finally, we systematically generalize results known for classical MCSP to the quantum setting (including quantum cryptography, quantum learning theory, quantum circuit lower bounds, and quantum fine-grained complexity) and also find new connections to tomography and quantum gravity. Due to the fundamental differences between classical and quantum circuits, most of our results require extra care and reveal properties and phenomena unique to the quantum setting. Our findings could be of interest for future studies, and we post several open problems for further exploration along this direction.</summary>
    <updated>2021-08-10T18:30:50Z</updated>
    <published>2021-08-10T18:30:50Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-15T06:20:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-480455417739937086</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/480455417739937086/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=480455417739937086" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/480455417739937086" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/480455417739937086" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2021/08/queues-with-small-advice.html" rel="alternate" type="text/html"/>
    <title>Queues with Small Advice</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have had papers rejected, with comments of the form that the results seem too easy, and are at the level of a homework assignment.  Generally, I think these reviewers miss the point.  The fact that the results seem easy may be because the point isn't the derivation but the conception and framing of the problem.  I actually think that generally it's an interesting subclass of good papers that can be and are turned into homework assignments.</p><p>A new-ish paper of mine, Queues with Small Advice, was recently accepted to the very new SIAM Conference on Applied and Computational Discrete Algorithms (<a href="https://www.siam.org/conferences/cm/conference/acda21">ACDA21</a>), which took place July 19-21.  This conference focuses on algorithms with a close tie to applications.  Some people unfamiliar with theory conferences might think that algorithms work would naturally be tied to applications, but I've generally found that algorithmic work tied to applications is more negatively reviewed in theory conferences.  Indeed, that type of work is much more likely to receive comments of the form that the results seem too easy, and are at the level of a homework assignment.  So perhaps this new conference will fill an important role and hole in the current slate of theory conferences. </p><p>In any case, I actually do think this paper is in some ways easy (in that the analysis is readily approachable with standard tools), and parts of it would, I believe, make a great homework assignment.  The goal was to show the potential power of using even very simple advice, such as from machine-learning algorithms, in queueing systems.  This seems to me to be a very understudied topic, and fits into the recently growing theme of <a href="https://arxiv.org/abs/2006.09123">Algorithms with Predictions</a>.  (The paper was rejected previously from a conference, where the most negative review said "Very accessible and well written paper, which certainly provides motivation to consider problems of this type." but also said "The mathematical analysis in this paper is fairly standard, and in that sense not novel... the paper is interesting, but not advancing sufficiently the state of the art.")  </p><p>The paper focuses on the case of 1 bit of advice -- essentially, is the job "short" or "long".  I think this type is advice is a good approach to look at for queueing -- it corresponds naturally to putting a job at the front of the queue, or the back.  And it may be easier for machine-learning algorithms to generate accurately.  Simple is often good in practice.  </p><p>Rather than describe the paper further, I'll go ahead and turn it directly into a collection of homework problems.  Feel free to use them or variations you come up with;  hopefully, the students won't find the paper for answers. I personally would be thrilled if one outcome of this paper was that prediction-based problems of this form made their way into problem sets.  (Although, serious question:  who still teaches queueing theory any more?)  </p><p><b>Section 1:  One-Bit Advice (Single Queue)</b></p><p>a)  Consider the standard M/M/1 queue, with Poisson arrivals at rate λ, and exponentially distributed service times of mean 1;  the expected time a job spends in the queue in equilibrium is 1/(1-λ).  Now suppose each job comes with one bit advice;  if the job has service time greater than T, the bit is 1, and if it is smaller than T, the bit is 0.  A "big" job goes to the end of the queue, a "small" job goes to the front.  (Assume the queue is non-preemptive.)  Find the expected time for a job in this queue in equilibrium, as a function of T and λ.</p><p>b)  What is the optimal value for T (as a function of λ)? </p><p>c)  Repeat parts a and b, but this time with a preemptive queue.  Does preemption help or hurt performance?</p><p>Harder variation:  The above questions, but with an M/G/1 queue (that is, for a general, given service distribution);  derive a formula for the expected time in the system, where the formula may involve terms based on the service distribution.</p><p>Easier variation:  Write a simulation, experimentally determine the best threshold, and the improvements from one bit of advice.  Different service time distributions can be tried.  </p><p><b>Section 2:  One-Bit Advice with Predictions (Single Queue)</b></p><p>Where would possibly get a bit of advice in real life?  Perhaps from a machine learning predictor.  But in that case, the advice might turn out to be wrong.  What if our bit of advice is just right most of the time?</p><p>a)  Consider the (non-preemptive) M/M/1 queue variation from Section 1 part a above, but now the advice is correct with some probability p.  Find the expected time for a job in this queue in equilibrium, as a function of p, T, and λ.</p><p>b)  Repeat part a with a preemptive queue.</p><p>Harder variations:  The above questions, but have the probability the advice is correct depend on the size of the job.  A particularly fun example is when the "predicted service time" for a job with true time x is exponentially distributed with mean x, and the prediction bit is 1 if the predicted time is larger than T, and 0 otherwise.  Also, one can again consider general service times.  </p><p>Easier variation:  Again, write a simulation and derive experimental results/insights.  </p><p><b>Section 3:  One-Bit Advice with Prediction (Power of 2 Choices)</b>  <i>[harder, grad student level;  needs to know fluid limit models;  I'd stick with sections 1 and 2!]</i></p><p>a)  Derive fluid limit equations for a collection of N queues, where there are two types of jobs:  "large" jobs arrive as a Poisson stream of rate λ₁N and have exponentially distributed service times with mean μ₁ and "small" jobs arrive as a Poisson stream of rate λ₂N and have exponentially distributed service times of mean μ₂.  Each job comes with a bit of advice determining whether it is large or small, but large jobs are mislabelled with probability p₁ and small jobs are mislabelled with probability p₂.  An incoming job selects a queue using "the power of two choices" -- it is up to you to describe how a job determines what is the better of the two choices (there are multiple possibilities) and how jobs are processed within a queue (non-preemptive is suggested).   </p><p>[Hint:  the queue state can be represented by the number of jobs that are labelled short that are waiting, the number of jobs that are labelled long that are waiting, and the type of the job currently being serviced.]  </p><p>b)  Compare fluid limit results to simulations for 1000 queues to see if your equations seem accurate.  </p><p><br/></p></div>
    </content>
    <updated>2021-08-09T20:01:00Z</updated>
    <published>2021-08-09T20:01:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2021-08-10T13:41:51Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/one-shot-top-k/</id>
    <link href="https://differentialprivacy.org/one-shot-top-k/" rel="alternate" type="text/html"/>
    <title>One-shot DP Top-k mechanisms</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the last <a href="https://differentialprivacy.org/exponential-mechanism-bounded-range/"><em>blog post</em></a>, we showed that the exponential mechanism enjoys improved composition bounds over general pure DP mechanisms due to a property called <strong>bounded range</strong>.  For this post, we will present another useful, and somewhat surprising, property of the exponential mechanism in its application of top-\(k\) selection.</p>

<h2 id="differentially-private-top-k-selection">Differentially Private Top-\(k\) Selection</h2>

<p>We will focus on datasets that are a vector of counts \(h = (h_1, \cdots, h_d) \in \mathbb{N}^d\), which consist of counts \(h_i\) for elements from a universe \(\mathcal{U}\) where \(|\mathcal{U}| = d\).  Let’s assume that a user’s data can modify each count by at most 1, yet can change all \(d\) counts, i.e. the \(\ell_\infty\)-sensitivity is 1 and the \(\ell_0\)-sensitivity is \(d\).  The task here is to return the top-\(k\) elements from the input counts in a differentially private way.</p>

<p>For top-\(1\), this is simply returning the element with the max count, and this is precisely the problem that the exponential mechanism is set up to solve.  Let’s write out the exponential mechanism \(M^{(1)}: \mathbb{N}^d \to [d]\) for this instance:
\[
\mathbb{P}[M^{(1)}(h) = i] = \frac{e^{ \varepsilon h_i }}{\sum_{j \in [d] } e^{ \varepsilon h_j } }, \qquad \forall i \in [d].
\]
For those wondering why this formula omits the factor of \(1/2\) in the exponent, we are using the <a href="https://dongjs.github.io/2020/02/10/ExpMech.html">stronger result</a> of the exponential mechanism which replaces global sensitivity with the range of the loss function \(\ell(i,h) = - h_i\), which is \(1\) in this case.  Recall from the last blog post that the exponential mechanism is \(\varepsilon^2/8\)-CDP.</p>

<p>Hence, to generalize this to top-\(k\) selection, we can simply iteratively apply this exponential mechanism by removing the <em>discovered</em> element from each previous round.  That is, we write \(M^{(k)}: \mathbb{N}^d \to [d]^k\) as the following for any outcome \( (i_1, i_2, \cdots, i_k) \in [d]^k\),</p>

<p>\[
\mathbb{P}[M^{(k)}(h) = (i_1, i_2, \cdots, i_k)] \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
<a name="eq:peelingEM"/>
\[\qquad = \frac{e^{ \varepsilon h_{i_1} }}{\sum_{j \in [d] } e^{ \varepsilon h_j } } \cdot  \frac{ e^{\varepsilon h_{i_2} } }{\sum_{j\in [d]\setminus \{ i_1\}} e^{\varepsilon h_j } } \cdot \cdots \cdot \frac{ e^{ \varepsilon h_{i_2} } }{\sum_{j\in [d]\setminus \{ i_1, \cdots, i_{k-1}\}} e^{ \varepsilon h_j } }. 
\tag{1}
\]</p>

<p>We can then apply composition to conclude that \(M^{(k)}\) is \(k \varepsilon^2/8\)-CDP.</p>

<h2 id="gumbel-noise-and-the-exponential-mechanism">Gumbel Noise and the Exponential Mechanism</h2>

<p>As we discussed in our last post, we can implement the exponential mechanism by adding <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel</a> noise to each count and reporting the noisy max element.  A Gumbel random variable \(X \sim \text{Gumbel}(\beta) \), parameterized by scale parameter \(\beta&gt;0\), has the following density function
<a name="eq:GumbelDensity"/>
\[
p(x;\beta) = \frac{1}{\beta} \exp\left( - x/\beta - e^{-x/\beta} \right), \qquad \forall x \in \mathbb{R}.
\tag{2}
\]</p>

<p>Hence, we can write the exponential mechanism in the following way
\[
M^{(1)}(h) = \arg\max \{ h_i + X_i : i \in [d] \}, \qquad \{X_i \} \stackrel{i.i.d.}{\sim} \text{Gumbel}(1/\varepsilon).
\]</p>

<p>We can then extend this to top-\(k\) by repeatedly adding independent Gumbel noise to each count and removing the discovered element for the next round.  However, something that would significantly improve run time would be to add Gumbel noise to each count <em>once</em> and then take the elements with the top-\(k\) noisy counts.  We could then add only \(d\) many noise terms, rather than \(O(d^k)\) noise terms if we were to iteratively run \(k\) different exponential mechanisms.  The question is, does this one-shot top-\(k\) Gumbel noise mechanism ensure the same level of privacy?</p>

<p>Let’s denote the one-shot Gumbel mechanism as \(\tilde{M}^{(k)}\).  At first glance, it does not seem like the one-shot Gumbel mechanism \(\tilde{M}^{(k)}\) should be just as private as the iterative exponential mechanism \(M^{(k)}\), but it turns out they are exactly the same mechanism!  The following result is due to <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a>.</p>

<blockquote>
  <p><strong>Theorem 1</strong>
For any input vector of counts \(h \in \mathbb{N}^d\), the one-shot Gumbel mechanism \(\tilde{M}^{(k)}(h)\) and iteratively applying the exponential mechanism \(M^{(k)}(h)\) are equal in distribution.</p>
</blockquote>

<p><em>Proof.</em> 
Recall the distribution of the iterative exponential mechanism \(M^{(k)}(h)\) from <a href="https://differentialprivacy.org/feed.xml#eq:peelingEM">(1)</a>.
Now we consider the one-shot Gumbel mechanism \(\tilde{M}^{(k)}(h)\) where we use the density of \(X \sim \) Gumbel\( (1/\varepsilon)\) from <a href="https://differentialprivacy.org/feed.xml#eq:GumbelDensity">(2)</a>.<br/>
\[
\mathbb{P}[\tilde{M}^{(k)}(h) = (i_1, \cdots, i_k)] \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
\[
\qquad = \int_{-\infty}^\infty p(u_1 - h_{i_1}) \int_{-\infty}^{u_1}  p(u_2 - h_{i_2}) \cdots \int_{-\infty}^{u_{k-1}} p(u_k - h_k) 
\]
<a name="eq:integral"/>
\[
\qquad \qquad \cdot \prod_{j \in [d] \setminus \{i_1, \cdots, i_k \} } \mathbb{P}[ X &lt; u_k - h_j]du_k \cdots du_2 du_1.
\tag{3}
\]
Note that we have 
\[
\mathbb{P}[X &lt; y] = \exp\left( - \exp\left( -\varepsilon y \right) \right).
\]
Let’s focus on the inner integral over \(u_k\) in <a href="https://differentialprivacy.org/feed.xml#eq:integral">(3)</a>.<br/>
\[
\int_{-\infty}^{u_{k-1}} p(u_k - h_{i_k} )\prod_{j \in [d] \setminus \{i_1, \cdots, i_k \} } \mathbb{P}[X &lt; u_k - h_j ]du_k \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
\[
\quad = \int_{-\infty}^{u_{k-1}} \varepsilon \cdot  \exp\left( - \varepsilon (u_k - h_{i_k}) - e^{ -\varepsilon (u_k - h_{i_k}) } \right) \cdot  \exp\left( -e^{-\varepsilon u_k}  \sum_{j \in [d] \setminus \{i_1, \cdots, i_k \} } e^{\varepsilon h_j}  \right) du_k 
\]
\[
\quad  = \varepsilon e^{\varepsilon h_{i_k}}  \int_{-\infty}^{u_{k-1}} \exp\left( -\varepsilon u_k - e^{-\varepsilon u_k} \left( e^{\varepsilon h_{i_k}} + \sum_{j \in [d] \setminus \{i_1, \cdots i_k \} } e^{\varepsilon h_j} \right) \right) du_k \qquad
\]
<a name="eq:lastLine"/>
\[
\qquad =  \varepsilon e^{\varepsilon h_{i_k}}  \int_{-\infty}^{u_{k-1}} \exp\left( -\varepsilon u_k - e^{-\varepsilon u_k} \left(\sum_{j \in [d] \setminus \{i_1, \cdots i_{k-1} \} } e^{\varepsilon h_j} \right) \right) du_k. \qquad \qquad
\tag{4}
\]
We now integrate with a \(v\)-substitution,
\[
v =e^{-\varepsilon u_{k}} \sum_{j \in [d] \setminus \{i_1, \cdots i_{k-1} \} } e^{\varepsilon h_j}<br/>
\]
\[
dv = - \varepsilon \sum_{j \in [d] \setminus \{i_1, \cdots i_{k-1} \} } e^{\varepsilon h_j}  \cdot e^{-\varepsilon u_{k}} du_{k}.
\]</p>

<p>Continuing with <a href="https://differentialprivacy.org/feed.xml#eq:lastLine">(4)</a>, we get
\[
\int_{-\infty}^{u_{k-1}} p(u_k - h_{i_k} )\prod_{j \in [d] \setminus \{i_1, \cdots, i_k \} } \mathbb{P}[X &lt; u_k - h_j ]du_k \qquad \qquad \qquad \qquad \qquad
\]
\[
\qquad = \frac{e^{\varepsilon h_{i_k} }}{\sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j}} \cdot \exp\left( - e^{-\varepsilon u_{k-1}} \cdot \sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j} \right)
\]
\[
\qquad = \frac{e^{\varepsilon h_{i_k} }}{\sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j}}  \cdot \prod_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \} } \mathbb{P}[X &lt; u_{k-1} - h_j ] .
\] 
Note how this line has the last term in the expression for \(M^{(k)}(h)\) in <a href="https://differentialprivacy.org/feed.xml#eq:peelingEM">(1)</a>, which is independent of \(u_{k-1}\) and can hence be pulled out of the larger integral in <a href="https://differentialprivacy.org/feed.xml#eq:integral">(3)</a>.  By induction, we have
\[
\mathbb{P}[\tilde{M}^{(k)}(h) = (i_1, \cdots, i_k)] \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
\[
\qquad  = \frac{e^{\varepsilon h_{i_1} }}{\sum_{j \in [d]} e^{\varepsilon h_j}}  \cdot \frac{e^{\varepsilon h_{i_2} }}{\sum_{j \in [d] \setminus \{ i_1\}} e^{\varepsilon h_j}}  \cdot \cdots \cdot \frac{e^{\varepsilon h_k }}{\sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j}} 
\]
\[
\qquad = \mathbb{P}[M^{(k)}(h) =(i_1, \cdots, i_k)].
\] ∎</p>

<p>So that’s great!  We can now run the one-shot Gumbel mechanism for top-\(k\) and still get the improved composition bounds of the exponential mechanism.  In addition to this achieving better runtime, this analysis can help with proving top-\(k\) DP algorithms over a large domain universe despite giving access to only the true top-\(\bar{k}\) items and their counts where \(\bar{k} &gt; k \), see <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a> for more details.</p>

<h2 id="report-noisy-max-for-dp-top-k">Report Noisy Max for DP Top-\(k\)</h2>

<p>We now turn to comparing this algorithm to some natural alternatives.  As we discussed in the last post, there is a family of mechanisms, report noisy max (RNM) mechanisms, that ensure differential privacy for the selection problem, and hence the top-\(1\) problem.  We showed that the exponential mechanism is equivalent to RNM with Gumbel noise, there is also RNM with Laplace and with Exponential noise, the last being the recently discovered <em>permute-and-flip</em> mechanism <a href="https://arxiv.org/abs/2010.12603" title="Ryan McKenna, Daniel Sheldon. Permute-and-Flip: A new mechanism for differentially private selection . NeurIPS 2020."><strong>[MS20]</strong></a> <a href="https://arxiv.org/abs/2105.07260" title="Zeyu Ding, Daniel Kifer, Sayed M. Saghaian N. E., Thomas Steinke, Yuxin Wang, Yingtai Xiao, Danfeng Zhang. The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with Exponential Noise. 2021."><strong>[DKSSWXZ21]</strong></a>.</p>

<p>To then use RNM mechanisms for top-\(k\), we can again iteratively apply them and use composition to get the overall privacy guarantee.  However, it turns out that you can also use the Laplace noise version of RNM in one-shot <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a>.</p>

<p>We can compare the relative noise that is added to each count in both the Laplace and Gumbel versions.  Since <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a> gives their privacy guarantee in terms of approximate \((\varepsilon,\delta )\)-DP, we will now make the comparison there.  We first look at the standard deviation for Laplace \( \sigma_{\text{Lap}}\) (using Theorem 2.2 in <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a>).
\[
\sigma_{\text{Lap}} =  \frac{8 \sqrt{2k \ln(d/\delta)}}{\varepsilon}.
\]
Note that the one-shot Laplace mechanism returns counts as well as the indices of the top-\(k\), both of which use Laplace noise with standard deviation \(\sigma_{\text{Lap}}\), so we will also include Laplace noise to the discovered elements in the Gumbel version. That is, we add Gumbel noise with scale \(\sqrt{k}/\varepsilon’\) for the discovery portion and Laplace noise with scale \(2\sqrt{k}/\varepsilon’\) for obtaining their counts, resulting in standard deviation noise \(\sigma_{\text{Gumb}}’\) and \(\sigma_{\text{Lap}}’\), respectively.<br/>
\[
\sigma_{\text{Gumb}}’  = \frac{\pi \sqrt{k} }{\sqrt{6}\varepsilon’}, \qquad 
\sigma_{\text{Lap}}’ = \frac{2\sqrt{2k}}{\varepsilon’}.
\]
Recall that adding this scale of Gumbel noise and Laplace noise will ensure \(\tfrac{\varepsilon’^2}{8} \)-CDP each, so combining will ensure \(\tfrac{\varepsilon’^2}{4}\)-CDP.  We could also use Gaussian noise to return the counts since we are using CDP, but we will analyze it with Laplace noise for comparison.  To ensure \((\varepsilon,\delta)\)-DP, we use the CDP to DP conversion from Lemma 3.5 in <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a> and solve for \(\varepsilon’\).  Hence, we get for any \(\delta&gt;0\)
\[
\varepsilon’^2/4 = \left( \sqrt{\ln(1/\delta) + \varepsilon} - \sqrt{\ln(1/\delta)} \right)^2 
\]
\[ \implies \varepsilon’ = 2 \sqrt{\ln(1/\delta)} \left( \sqrt{1 + \tfrac{\varepsilon}{\ln(1/\delta)}} - 1 \right).
\]</p>

<p>Let’s consider a typical privacy setting where \(\varepsilon &lt;  \ln(1/\delta)\), and use the inequality \(\sqrt{1+x} \geq 1 + x/4\) for \(0&lt;x&lt;1\).  Here is a short proof of this inequality:
\[
(1 + x/4)^2 = 1 + x/2  + x^2/16  \leq 1 + x/2 + x/2 = 1 + x.<br/>
\]
Note that the privacy guarantee for one-shot Laplace noise only holds when \(\varepsilon &lt; 0.2\) and \(\delta &lt; 0.05\) as stated in Theorem 2.2 in <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a>.  In this case, we have 
\[
\varepsilon’ \geq 2 \sqrt{\ln(1/\delta)} \left( 1 + 1/4 \cdot \tfrac{\varepsilon}{\ln(1/\delta)} - 1 \right) = 1/2 \cdot \tfrac{\varepsilon}{\sqrt{\ln(1/\delta)}}.
\]
Plugging \(\varepsilon’\) into the standard deviation of Gumbel and Laplace, we get
\[
\sigma_{\text{Gumb}}’  \leq \frac{2\pi\sqrt{k \ln(1/\delta)}}{\sqrt{6}\cdot \varepsilon}, \qquad \sigma_{\text{Lap}}’ \leq \frac{4\sqrt{2k\ln(1/\delta)}}{\varepsilon}.
\]</p>

<p>Putting this together, we can show that we add significantly less noise for the discovery and releasing noisy count phases, 
\[
\sigma_{\text{Gumb}}’\leq \sigma_{\text{Lap}}/4 ,\qquad  \sigma_{\text{Lap}}’ \leq \sigma_{\text{Lap}}/2.
\]
Note that these bounds can be improved further with similar analysis.</p>

<p>Although it has not been studied yet whether the permute-and-flip mechanism \(M_{\text{PF}} \) can also ensure DP in one shot by using Exponential noise, we briefly discuss whether it can be bounded range for a similar parameter as the Exponential Mechanism, and hence achieve similar composition bounds.  Consider running permute-and-flip on two items \(\{1,2 \}\) with a monotonic quality score \(q: \mathcal{X} \times \{1,2 \} \to \mathbb{R} \) whose sensitivity is 1.  Let \(x, x’ \in \mathcal{X} \) be neighbors where 
\[
q(x,1) = q(x,2) = 0
\]
\[
q(x’,1) = 0 , \quad q(x’,2) = 1.
\]
Hence, permute-and-flip will return outcome \(1\) or \(2\) with half probability each on dataset \(x\), while with dataset \(x’\) outcome \(1\) occurs with probability \(1/2 \cdot  e^{-\varepsilon}\) and outcome \(2\) occurs with probability \( 1/2 + 1/2 \cdot (1 - e^{-\varepsilon})\).  We can then compute the bounded range parameter \(\alpha\) as
\[
\frac{\mathbb{P}[M_{\text{PF}}(x’) = 2 ] }{\mathbb{P}[M_{\text{PF}}(x) = 2 ]}\leq e^{\alpha}\frac{\mathbb{P}[M_{\text{PF}}(x’) = 1 ]}{\mathbb{P}[M_{\text{PF}}(x) = 1 ]} \implies  \alpha \geq \varepsilon + \ln(2 - e^{-\varepsilon} ).
\]
Note that with \(\varepsilon \gg 1\), we get \(\alpha \) close to \(\varepsilon\), which would be the same bounded range parameter as the exponential mechanism.  However, with \(\varepsilon&lt; 1\), we get \(\alpha\) close to \(2\varepsilon\).  This example provides a lower bound on the BR parameter for permute-and-flip.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We have looked at the top-\(k\) selection problem subject to differential privacy and although there are many different mechanisms to use, the exponential mechanism stands out for several reasons:</p>
<ol>
  <li>The exponential mechanism is \(\varepsilon\)-DP and \( \varepsilon^2/8\)-CDP and hence gets improved composition.</li>
  <li>Iteratively applying the exponential mechanism for top-\(k\) can be implemented by adding Gumbel noise to each count and returning the elements with the top-\(k\) noisy counts in one-shot.</li>
  <li>The one-shot Gumbel mechanism returns a ranked list of \(k\) elements, rather than a set of \(k\) elements.</li>
</ol></div>
    </summary>
    <updated>2021-08-09T17:00:00Z</updated>
    <published>2021-08-09T17:00:00Z</published>
    <author>
      <name>Ryan Rogers</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-08-14T22:54:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1648249705477846335</id>
    <link href="http://blog.computationalcomplexity.org/feeds/1648249705477846335/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/combing-two-posts-blankface-scott-aa.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/1648249705477846335" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/1648249705477846335" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/combing-two-posts-blankface-scott-aa.html" rel="alternate" type="text/html"/>
    <title>Combing two posts: Blankface (Scott Aa) and Is Science Slowing Down? (Scott Al)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(I also posted this to the Less Wrong Website. At least I tried to- I don't quite know if or when it will appear there as its my first post there.) </p><p>Some papers result from taking two papers and combining them. Perhaps nobody else had read both of them so you can say something new! Or (looking over this post) it may guide people to two really good papers, or in this case two really good posts. </p><p>This blog will draw from two excellent blog posts.</p><p>Scott Aaronson  blogged on  his website Aug 2, 2021 about <a href="https://www.scottaaronson.com/blog/?p=5675#comments">blankfaces</a>, people who let stupid or undefined rules dictate what you can do  without apology (see his post for a better explanation). One example that struck me I quote</p><p><i>No, I never applied for that grant. I spend two hours struggling to log in to a web portal designed by the world's top blankfaces until I finally gave up in despair. </i></p><p><i><br/></i></p><p>Scott Alexander blogged  on LessWrong on Nov 26, 2018 about <a href="https://www.lesswrong.com/posts/v7c47vjta3mavY3QC/is-science-slowing-down">Is science slowing down?</a> which answers with an emphatic <i>yes.</i> His point is science-per-researcher is much less than it used to be, and he has graphs and stats to prove it (see his post for the evidence and some speculation as to why this is) One of the reasons he gave struck me which I quote</p><p><i>Certain features of the modern academic system like undepaid PhD's, interminably long postdocs, endless grant writing drudgery, and clueless funders have lowered productivity. The 1930's academic system was ineed 25x more effective at getting researchers to actually do good research.</i></p><p>(A commenter reminded me that Scott Alexander himself dismisses this reason. I do not.) </p><p>(I note that he gives other reasons as well, most notably for our field that the low hanging fruit is gone. Our lack of progress on P vs NP is likely that its a hard problem, rather than the reason above. Of course, if its solved tomorrow by an outsider without funding, I will happily be proven wrong.) </p><p>Scott Alexander hits upon two types of blankfaces (without using the term).</p><p><i>Grant writing drudgery</i>: the rules for how to submit get more and more detailed an onerous. This is  what Scott Aaronson was alluding to. There are other ways its drudgery as well. </p><p><i>Clueless Funders</i>: the people deciding who gets funded might not know the area (actually in my experience the grant I've reviews have been quite good and the problem is more not enough money to award all that are deserving.) </p><p>SO I pose the following non-rhetorically as always</p><p>1) How big a factor is the slowing down of science that blankfaces get in the way?</p><p>2) What can we do about it?</p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><i><br/></i></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-08-09T01:50:00Z</updated>
    <published>2021-08-09T01:50:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-15T01:18:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/</id>
    <link href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/" rel="alternate" type="text/html"/>
    <title>School on Modern Directions in Discrete Optimization</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">September 13-17, 2021 Online https://www.him.uni-bonn.de/programs/future-programs/future-trimester-programs/discrete-optimization/discrete-optimization-school/ Aims and Scope: The school provides an introduction to some of the main topics of the trimester program on discrete optimization. The lectures will address the interface between tropical geometry and discrete optimization; recent developments in continuous optimization with applications to combinatorial problems; topics in approximation algorithms; and fixed parameter … <a class="more-link" href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/">Continue reading <span class="screen-reader-text">School on Modern Directions in Discrete Optimization</span></a></div>
    </summary>
    <updated>2021-08-08T14:26:42Z</updated>
    <published>2021-08-08T14:26:42Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-08-15T06:21:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/115</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/115" rel="alternate" type="text/html"/>
    <title>TR21-115 |  On quantum versus classical query complexity | 

	Scott Aaronson, 

	Andris Ambainis, 

	Andrej Bogdanov, 

	Krishnamoorthy Dinesh, 

	Cheung Tsun Ming</title>
    <summary>Aaronson and Ambainis (STOC 2015, SICOMP 2018) claimed that the acceptance probability of every quantum algorithm that makes $q$ queries to an $N$-bit string can be estimated to within $\epsilon$ by a randomized classical algorithm of query complexity $O_q((N/\epsilon^2)^{1-1/2q})$.  We describe a flaw in their argument but prove that the dependence on $N$ in this upper bound is correct for one-query quantum algorithms ($q = 1$).</summary>
    <updated>2021-08-08T00:29:27Z</updated>
    <published>2021-08-08T00:29:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-15T06:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1563</id>
    <link href="https://ptreview.sublinear.info/2021/08/workshop-on-algorithms-for-large-data-we-found-waldo-and-so-can-you/" rel="alternate" type="text/html"/>
    <title>Workshop on Algorithms for Large Data: We found WALD(O), and so can you!</title>
    <summary>Ainesh Bakshi, Rajesh Jayaram, and Samson Zhou are organizing a 3-day Workshop on Algorithms for Large Data (nicely abbreviated as WALD(O), the O standing for Online), featuring many talks which should be of interest to the readers of this blog, as well as an open problems and a poster sessions, and a junior/senior lunch. As […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Ainesh Bakshi, Rajesh Jayaram, and Samson Zhou are organizing a 3-day <a href="https://waldo2021.github.io/">Workshop on Algorithms for Large Data</a> (nicely abbreviated as WALD(O), the O standing for Online), featuring many talks which should be of interest to the readers of this blog, as well as an open problems and a poster sessions, and a junior/senior lunch. As the organizers describe it:</p>



<blockquote class="wp-block-quote"><p>This workshop aims to foster collaborations between researchers across multiple disciplines through a set of central questions and techniques for algorithm design for large data. We will focus on topics such as sublinear algorithms, randomized numerical linear algebra, streaming and sketching, and learning and testing.</p></blockquote>



<p>The workshop will take place on <strong>August 23 — August 25</strong> (ET). Attendance is free, but <a href="https://docs.google.com/forms/d/1VMtDFay1MoiKMAErfkg2ZkAswQQdNWhiDQUKGtPBrzA/viewform">registration</a> is required by <strong>August 20th</strong>. More details at <a href="https://waldo2021.github.io/">https://waldo2021.github.io/</a></p></div>
    </content>
    <updated>2021-08-07T07:10:15Z</updated>
    <published>2021-08-07T07:10:15Z</published>
    <category term="Announcement"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-08-14T22:54:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1560</id>
    <link href="https://ptreview.sublinear.info/2021/08/new-for-july-2021/" rel="alternate" type="text/html"/>
    <title>New for July 2021</title>
    <summary>This month saw three papers appear online, together covering a rather broad range of topics: testing of regular languages, distribution testing under differential privacy, and local testability from high-dimensional expanders. Let’s dive in! Property Testing of Regular Languages with Applications to Streaming Property Testing of Visibly Pushdown Languages, by Gabriel Bathie and Tatiana Starikovskaya (paper). […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This month saw three papers appear online, together covering a rather broad range of topics: testing of regular languages, distribution testing under differential privacy, and local testability from high-dimensional expanders. Let’s dive in!</p>



<p><strong>Property Testing of Regular Languages with Applications to Streaming Property Testing of Visibly Pushdown Languages</strong>, by Gabriel Bathie and Tatiana Starikovskaya (<a href="https://drops.dagstuhl.de/opus/frontdoor.php?source_opus=14188">paper</a>). Let \(L\in \Sigma^\ast\) be a regular language recognized by an automation with \(m\) states and \(k\) connected components: given as input a word \(u\in \Sigma^n\), what is the query complexity to test membership to \(L\) in Hamming distance? Edit distance? Or, more generally, <em>weighted</em> edit distance, where each letter of the word \(u\) comes with a weight? In this paper, the authors focus on non-adaptive, one-sided errors testing algorithms, for which they show an upper bound of \(q=O(k m \log(m/\varepsilon)/\varepsilon)\) queries (with running time \(O(m^2 q)\)), which they complement by a query complexity lower bound of \(\Omega(\log(1/\varepsilon)/\epsilon)\), thus matching the upper bound for languages recognized by constant-size automata. The guarantee for the upper bound is with respected to weighted edit distance, and thus implies the same upper bound for testing with respect to Hamming distance. <br/>To conclude, the authors use an existing connection to streaming property testing to obtain new algorithms for property testing of visibly pushdown languages (VPL) in the <em>streaming</em> model, along with a new lower bound in that model.</p>



<p><strong>High dimensional expansion implies amplified local testability</strong>, by Tali Kaufman and Izhar Oppenheim (<a href="https://arxiv.org/abs/2107.10488">arXiv</a>). This paper sets out to show that codes that arise from high-dimensional expanders are locally testable (membership to the code can be tested using very few queries). To do so, the authors define a new notion of <em>high-dimensional expanding system</em> (HDE system), as well as that of <em>amplified</em> local testability, a stronger notion than local testability; they then prove that a code based on a HDE system satisfies this stronger notion. Moreover, they show that many well-known families of codes are, in fact, HDE system codes, and therefore satisfy this stronger notion of local testability as well.</p>



<p>Finally, a survey on differential privacy, with a foray into distribution testing:</p>



<p><strong>Differential Privacy in the Shuffle Model: A Survey of Separations</strong>, by Albert Cheu (<a href="https://arxiv.org/abs/2107.11839">arXiv</a>). If you are familiar with differential privacy (DP), you may recall that there are several notions of DP, each meant to address a different “threat model” (depending on whom you trust with your data). <em>Shuffle DP</em> is one of them, intermediate between “central” DP and the more stringent “local” DP. Long story short: with shuffle DP, the tradeoff between privacy and accuracy can be strictly in-between what’s achievable in central and local DP, and that’s the case for one of the usual suspects of distribution testing, uniformity testing (<em>“I want to test if the data uniformly distributed, but now, with privacy of that data in mind”</em>). The survey discusses what is known about this in Sections 3.3 and 6, and what the implications are; but there are quite a few questions left unanswered… Long story short: a very good introduction to shuffle privacy, and to open problems in that area!</p></div>
    </content>
    <updated>2021-08-07T06:57:24Z</updated>
    <published>2021-08-07T06:57:24Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-08-14T22:54:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-551003207026816768</id>
    <link href="http://processalgebra.blogspot.com/feeds/551003207026816768/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=551003207026816768" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/551003207026816768" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/551003207026816768" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/08/interview-with-concur-2021-tot-award.html" rel="alternate" type="text/html"/>
    <title>Interview with CONCUR 2021 ToT Award Recipients: Uwe Nestmann and Benjamin Pierce</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am pleased to re-post <a href="https://www.imperial.ac.uk/people/n.yoshida" target="_blank">Nobuko Yoshida</a>'s splendid <a href="http://mrg.doc.ic.ac.uk/concur-tot/" target="_blank">interview</a> with CONCUR 2021 Test-of-Time Award recipients <a href="https://www.mtv.tu-berlin.de/nestmann/" target="_blank">Uwe Nestmann</a> and <a href="https://www.cis.upenn.edu/~bcpierce/" target="_blank">Benjamin Pierce</a>. I thoroughly enjoyed reading it and learnt much from the many pearls of wisdom that pepper the interview. </p><p>Thanks to Benjamin and Uwe for their answers and to Nobuko for conducting such an inspiring interview. Enjoy!<br/></p><p><br/></p></div>
    </content>
    <updated>2021-08-06T21:36:00Z</updated>
    <published>2021-08-06T21:36:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-08-11T09:39:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/</id>
    <link href="https://cstheory-events.org/2021/08/06/workshop-on-algorithms-for-large-data-online-2021/" rel="alternate" type="text/html"/>
    <title>Workshop on Algorithms for Large Data (Online) 2021</title>
    <summary>August 23-25, 2021 Online https://waldo2021.github.io/ Registration deadline: August 20, 2021 This workshop aims to foster collaborations between researchers across multiple disciplines through a set of central questions and techniques for algorithm design for large data. We will focus on topics such as sublinear algorithms, randomized numerical linear algebra, streaming and sketching, and learning and testing.</summary>
    <updated>2021-08-06T21:33:33Z</updated>
    <published>2021-08-06T21:33:33Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-08-15T06:21:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2754</id>
    <link href="https://theorydish.blog/2021/08/06/average-case-fine-grained-hardness-part-iii/" rel="alternate" type="text/html"/>
    <title>Average-Case Fine-Grained Hardness, Part III</title>
    <summary>Continuing our previous discussion, I will show another application of the new recipe described in the previous post (i.e., constructing a “good” polynomial for a problem of interest), which will establish average-case hardness of a problem related to the orthogonal vector (OV) problem. (Recall the OV problem: Given , , where each for , decide if there are such that . The reader can look up the worst-case hardness of the OV problem in the first post of the series.) The motivating question is can we show average-case hardness for counting the number of orthogonal pairs in the OV problem by constructing a “good” polynomial? (One motivation is that such average-case hardness result could serve as the source of reductions for proving average-case hardness for many other problems, because OV is a main source of fine-grained hardness.) There is a good reason to believe the answer is no. Specifically, an -time algorithm for counting orthogonal pairs for average-case OV instances (here “average-case” is Erdős–Rényi random input model, and is a constant that depends on the parameter of the input model) was given in [DLW20], while constructing a “good” polynomial would prove average-case hardness for any constant assuming randomized SETH. This [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Continuing our previous discussion, I will show another application of the new recipe described in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a> (i.e., constructing a “good” polynomial for a problem of interest), which will establish average-case hardness of a problem related to the orthogonal vector (OV) problem. (Recall the OV problem: Given <img alt="X=\{x_1,\dots,x_n\}" class="latex" src="https://s0.wp.com/latex.php?latex=X%3D%5C%7Bx_1%2C%5Cdots%2Cx_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="Y=\{y_1,\dots,y_n\}" class="latex" src="https://s0.wp.com/latex.php?latex=Y%3D%5C%7By_1%2C%5Cdots%2Cy_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where each <img alt="x_i,y_i\in\{0,1\}^{d}" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%2Cy_i%5Cin%5C%7B0%2C1%5C%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="d=\omega(\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D%5Comega%28%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, decide if there are <img alt="x_i,y_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%2Cy_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that <img alt="\langle x_i,y_j\rangle=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i%2Cy_j%5Crangle%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The reader can look up the worst-case hardness of the OV problem in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">first post</a> of the series.)</p>



<p>The motivating question is can we show average-case hardness for counting the number of orthogonal pairs in the OV problem by constructing a “good” polynomial? (One motivation is that such average-case hardness result could serve as the source of reductions for proving average-case hardness for many other problems, because OV is a main source of fine-grained hardness.) There is a good reason to believe the answer is no. Specifically, an <img alt="O(n^{2-\delta})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2-%5Cdelta%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-time algorithm for counting orthogonal pairs for average-case OV instances (here “average-case” is Erdős–Rényi random input model, and <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a constant that depends on the parameter of the input model) was given in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a>, while constructing a “good” polynomial would prove <img alt="\Omega(n^{2-\varepsilon})" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28n%5E%7B2-%5Cvarepsilon%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> average-case hardness for any constant <img alt="\varepsilon&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon%3E0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> assuming randomized SETH. This indicates that sometimes constructing a “good” polynomial might be a little too ambitious goal.</p>



<p>Instead, can we first come up with a nice combinatorial problem that encodes OV on a slightly larger binary input space and then construct a “good” polynomial for counting solutions of this combinatorial problem? (The motivation is that since this combinatorial problems encodes OV, it is at least as hard as OV for worst case, and moreover, if we can construct a “good” polynomial for counting solutions of this combinatorial problem, by the new recipe in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>, counting solutions of this combinatorial problem for average case is (almost) as hard as that for worst case. Therefore, counting solutions of this combinatorial problem for average case is at least as hard as OV for worst case. More importantly, this combinatorial problem could serve as the source of reductions thanks to its combinatorial structure.) The factored OV problem introduced in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a> gives a positive answer to this question. Analogously, they proposed factored variants for many other flagship fine-grained hard problems. By reductions to these factored problems, they managed to prove average-case fine-grained hardness for many natural combinatorial problems such as counting regular expression matchings (the featured image of this post is the web of reductions in their paper).</p>



<p>Next, for the purpose of exposition, I briefly sketch the high-level idea behind the factored OV problem (without even explicitly describing its combinatorial interpretation, since I will not show any reduction from this problem). </p>



<p><strong>Counting solutions for factored OV.</strong>  Given an OV instance <img alt="X,Y" class="latex" src="https://s0.wp.com/latex.php?latex=X%2CY&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="d=o((\log n/\log\log n)^2)" class="latex" src="https://s0.wp.com/latex.php?latex=d%3Do%28%28%5Clog+n%2F%5Clog%5Clog+n%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (actually, <img alt="d=o(\log^2 n/\log\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog%5E2+n%2F%5Clog%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> would also work, and the choice here is for simplicity), we encode <img alt="x\in\{0,1\}^d" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as <img alt="\textrm{Enc}(x):=\textrm{LONG}(x[1:\sqrt{d}])\circ \textrm{LONG}(x[\sqrt{d}+1:2\sqrt{d}])\circ\dots\circ\textrm{LONG}(x[d-\sqrt{d}+1:d])" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%3A%3D%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29%5Ccirc+%5Ctextrm%7BLONG%7D%28x%5B%5Csqrt%7Bd%7D%2B1%3A2%5Csqrt%7Bd%7D%5D%29%5Ccirc%5Cdots%5Ccirc%5Ctextrm%7BLONG%7D%28x%5Bd-%5Csqrt%7Bd%7D%2B1%3Ad%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,<br/>where <img alt="x[i:j]" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Bi%3Aj%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> represents the subvector (block) of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from the <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-th to the <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-th coordinate, and <img alt="\textrm{LONG}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the long code encoding (specifically, <img alt="\textrm{LONG}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> maps a <img alt="\sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-dimensional binary vector <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to a <img alt="2^{\sqrt{d}}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Csqrt%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-dimensional vector binary vector <img alt="\textrm{LONG}(x')" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of which all the coordinates are zero except the coordinate indexed by <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>). Namely, <img alt="\textrm{Enc}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the concatenation of the long code encoding of each block of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and thus, <img alt="\textrm{Enc}(x)\in\{0,1\}^{\sqrt{d}\cdot 2^{\sqrt{d}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%5Cin%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%5Ccdot+2%5E%7B%5Csqrt%7Bd%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and for our choice of <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we have that <img alt="\textrm{Enc}(x)\in\{0,1\}^{n^{o(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29%5Cin%5C%7B0%2C1%5C%7D%5E%7Bn%5E%7Bo%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Therefore, by taking such encoding for the vectors in the OV instance, we blow up the size of input (and hence weaken the worst-case hardness of OV) very mildly.</p>



<p>The key advantage of such encoding is that the indicator function <img alt="\mathbf{1}(x[1:\sqrt{d}]\perp y[1:\sqrt{d}])" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%5Cperp+y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (which outputs <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if the two vectors are orthogonal and <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> otherwise) can be represented as the sum of degree-<img alt="2" class="latex" src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> monomials, of which the variables are the coordinates of <img alt="\textrm{LONG}(x[1:\sqrt{d}])" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="\textrm{LONG}(y[1:\sqrt{d}])" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Indeed, we can first enumerate all the orthogonal pairs of vectors <img alt="v_1,v_2\in \{0,1\}^{\sqrt{d}}" class="latex" src="https://s0.wp.com/latex.php?latex=v_1%2Cv_2%5Cin+%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and we check whether <img alt="\textrm{LONG}(x[1:\sqrt{d}])" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28x%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has value <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> at coordinate <img alt="v_1" class="latex" src="https://s0.wp.com/latex.php?latex=v_1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="\textrm{LONG}(y[1:\sqrt{d}])" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BLONG%7D%28y%5B1%3A%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has value <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> at coordinate <img alt="v_2" class="latex" src="https://s0.wp.com/latex.php?latex=v_2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, by taking product of these two coordinates, and then, we take the sum of all the products.</p>



<p>Using this approach, for each <img alt="x\in X, y\in Y" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin+X%2C+y%5Cin+Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, for each <img alt="i\in[\sqrt{d}]" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cin%5B%5Csqrt%7Bd%7D%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we get a degree-<img alt="2" class="latex" src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> polynomial that computes <img alt="\mathbf{1}(x[(i-1)\sqrt{d}+1:i\sqrt{d}]\perp y[(i-1)\sqrt{d}+1:i\sqrt{d}])" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5B%28i-1%29%5Csqrt%7Bd%7D%2B1%3Ai%5Csqrt%7Bd%7D%5D%5Cperp+y%5B%28i-1%29%5Csqrt%7Bd%7D%2B1%3Ai%5Csqrt%7Bd%7D%5D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on input <img alt="\textrm{Enc}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="\textrm{Enc}(y)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The product of these polynomials obviously computes <img alt="\mathbf{1}(x\perp y)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B1%7D%28x%5Cperp+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and moreover, this product is a <img alt="2\sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite polynomial (<img alt="d'" class="latex" src="https://s0.wp.com/latex.php?latex=d%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite polynomial is defined in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>), where each part corresponds to the coordinates of the long code encoding of a block (subvector) of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. If we sum up these <img alt="2\sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite polynomials for all pairs <img alt="x\in X, y\in Y" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin+X%2C+y%5Cin+Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we get a <img alt="2\sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite polynomial that precisely counts orthogonal pairs for the OV instance. We can let the field size of this polynomial be a prime <img alt="p&gt;n^2" class="latex" src="https://s0.wp.com/latex.php?latex=p%3En%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (<img alt="n^2" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a trivial upper bound of the number of orthogonal pairs) such that the output of this polynomial is indeed the number of orthogonal pairs.</p>



<p>Notice that (i) Since the encoding only blows up the input size mildly, the worst-case hardness of OV (almost) carries over to evaluating this polynomial. (ii) Since <img alt="d=o((\log n/\log\log n)^2)" class="latex" src="https://s0.wp.com/latex.php?latex=d%3Do%28%28%5Clog+n%2F%5Clog%5Clog+n%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, the <img alt="2\sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite polynomial is a “good” polynomial (defined in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a>). It follows from our new recipe in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">previous post</a> that evaluating this polynomial on binary input for average case (here “average case” means Erdős–Rényi random input model) is (almost) as hard as worst case. (iii) Last but not least, evaluating this polynomial on any <img alt="z\in\{0,1\}^{\sqrt{d}\cdot 2^{\sqrt{d}}}" class="latex" src="https://s0.wp.com/latex.php?latex=z%5Cin%5C%7B0%2C1%5C%7D%5E%7B%5Csqrt%7Bd%7D%5Ccdot+2%5E%7B%5Csqrt%7Bd%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (not just <img alt="\textrm{Enc}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7BEnc%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="x\in\{0,1\}^d" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>) can be interpreted as counting solutions for a combinatorial problem, which is the factored OV problem in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a>. As I mentioned earlier, I will not explain the combinatorial interpretation in details. The takeaway is that counting solutions of such factored problem is average-case fine-grained hard, and its combinatorial structure allows possible reductions to other natural combinatorial problems.</p>



<p>Finally, I mention two broad research directions in this area: (i) design cryptographic primitives, e.g., one-way functions, based on these fine-grained average-case hardness results (or show complexity barriers) and (ii) prove fine-grained average-case hardness for decision problems (or design more efficient algorithms).</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk. </p></div>
    </content>
    <updated>2021-08-06T15:33:00Z</updated>
    <published>2021-08-06T15:33:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Junyao Zhao</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-08-15T06:22:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/08/05/predicting-weighted-ranks</id>
    <link href="https://11011110.github.io/blog/2021/08/05/predicting-weighted-ranks.html" rel="alternate" type="text/html"/>
    <title>Predicting weighted ranks</title>
    <summary>This week’s events have included Olympic sport climbing, for the first time. The NBC livestream of the women’s qualification suffered a bit of an embarrassment, though, as the computer display of the results incorrectly showed some competitors as being guaranteed to qualify when they were not. (Rest of post contains spoilers; don’t click if you don’t want to know about the outcomes of the event.)</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This week’s events have included Olympic sport climbing, for the first time. The NBC livestream of the women’s qualification suffered a bit of an embarrassment, though, as the computer display of the results incorrectly showed some competitors as being guaranteed to qualify when they were not. (Rest of post contains spoilers; don’t click if you don’t want to know about the outcomes of the event.)</p>

<p>For example, the screenshot below shows Viktoriia Meshkova as a qualifier, with Janja Garnbret still to climb, but Meshkova was actually eliminated after Garnbret’s climb. The commentators noticed the problem and had to tell the audience not to pay attention to that part of the display. What went wrong, and how could it have been done correctly?</p>

<p style="text-align: center;"><img alt="Ranking with one climb to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/5.jpg" width="80%"/></p>

<h1 id="background">Background</h1>

<p>The important things to know about this event are:</p>

<ul>
  <li>
    <p>It involves three disciplines, speed, bouldering, and lead, in that order. One discipline finishes before the next one starts (in fact there is a long rest period between each two disciplines).</p>
  </li>
  <li>
    <p>The scores from each discipline are turned into orderings, giving each of the competitors a number from 1 to 20, before combining them into a single outcome.
Each of 20 competitors gets a rank from 1 to 20 in each of the three disciplines.</p>
  </li>
  <li>
    <p>These numbers are multiplied and the eight competitors with the <a href="https://11011110.github.io/blog/2020/07/16/comparing-multi-sport.html">smallest product of ranks</a> advance to the final round.</p>
  </li>
  <li>
    <p>In lead climbing, the ranks are based on how high each competitor climbs, with ties broken by time, so any competitor who has not yet climbed can slot into the ranking in any position.</p>
  </li>
</ul>

<p>In screens like the one shown above, the livestream showed the standings of the competitor, ordered by their product of ranks: the product of two ranks for competitors who had not yet climbed and the product of all three current ranks for competitors who had already climbed. It predicted qualification for competitors who had already climbed and were in the top eight in this ordering. This seems reasonable, at first glance: the ordering among the people who have already climbed is set, and the people who have not yet climbed can only go down in the ordering, so they will stay in the top eight.</p>

<p>But the ordering among competitors who had already climbed is <em>not</em> set. In the example shown above, before Janja Garnbret climbed, Meshkova was ahead of two other climbers, Aleksandra Mirosław and Anouck Jaubert, both of whom had done well in speed and badly in lead. Garnbret climbed better than Meshkova, bumping Meshkova’s lead ranking down from fourth to fifth. That hurt Meshkova’s combined score a lot more than it hurt Mirosław’s and Jaubert’s, because Mirosław and Jaubert already had very high ranks in lead. At the end of the competition, Meshkova was behind Mirosław and Jaubert, and out of the competition.</p>

<h1 id="analysis">Analysis</h1>

<p>We can formulate this as an algorithms problem: Suppose we have \(n\) competitors, numbered \(1\dots n\), each with a weight \(w_i\) (their combined score from the previous disciplines). We have selected an ordering on a set \(X\) of the competitors, while the remaining set \(Y\) have yet to be ordered. The eventual ordering on \(X\cup Y\) must be consistent with the ordering we already know on \(X\). If competitor \(i\) ends up in position \(p_i\), they get a combined score \(w_i\cdot p_i\) and a combined rank based on sorting these combined scores. (To keep the terminology from being confused, I’ll stick to “ordering” for the result of a single discipline, and “ranking” for the combined result of the whole competition.) What we want to know, for each competitor number \(i\), is: what is the maximum possible combined rank \(r_i\)? Before formulating an algorithm for this problem, let’s do some analysis to find simplifying assumptions that can make the algorithm fast.</p>

<p>For the climbing competition, \(n=20\) and we only care whether \(r_i\le 8\) or \(r_i&gt;8\): is competitor \(i\) guaranteed a spot in the final or not? More generally, we can ask the same question for any \(n\) and any threshold on the combined rank. We can also ask this question regardless of whether competitor \(i\) has already competed (that is, whether \(i\in X\)): if not, it’s safe to assume that they will end up last in the ordering, because that’s the slot that will give them the maximum combined rank.</p>

<p>For each competitor \(j\) in the set \(Y\) of not-yet-ordered competitors, it’s always safe to assume that \(j\) will slot in somewhere above \(i\) in the final ordering. Slotting in immediately above \(i\) is always worse for \(i\) than slotting in anywhere below \(i\), because it hurts the ranking for \(i\) more than it hurts anyone else. Therefore, it can only cause \(i\) to go down among the combined rankings of competitors who are already ordered. Once we make this assumption, we know the final position \(p_i\) of competitor \(i\) in our ordering, and therefore we also know the final combined score \(w_i\cdot p_i\). This assumption lets us determine which final scores beat \(i\), and (because the orderings are also fixed by this assumption) determines which of the competitors ordered later than \(i\) in \(X\) end up beating \(i\). What remains to be determined is which of the competitors ordered earlier than \(i\) in \(X\) might beat \(i\), and which of the competitors in \(Y\) might <span style="white-space: nowrap;">beat \(i\).</span></p>

<p>We don’t know how many competitors in \(Y\) beat \(i\), but we can guess; there are only \(\vert Y\vert\lt n\) possibilities to try. Suppose that we guess that this number is \(k\). If our guess is correct, then we can safely assume that these \(k\) better competitors are the ones in \(Y\) with the \(k\) smallest weights. Any other outcome that puts \(k\) competitors in \(Y\) ahead of \(i\) can be swapped to an outcome that puts these \(k\) competitors ahead, without changing the ranks of any competitors <span style="white-space: nowrap;">in \(X\).</span></p>

<p>Once we know which \(k\) competitors in \(Y\) beat \(i\), we also know how good a position \(p_j\) each of these competitors will need to attain, to beat \(i\). We can assign these competitors to these positions, breaking ties by moving some up into higher positions, determining where they all slot into the overall ordering. Among all assignments of positions to these competitors that puts them ahead of \(i\), this is the one that hurts the other competitors of \(i\) the least. Once this is done, we can safely assign all of the remaining competitors in \(Y\) to an ordering that slots them in just ahead of \(i\), again hurting \(i\) while causing the least hurt to the competitors <span style="white-space: nowrap;">of \(i\).</span></p>

<p>With the ordering of competitors completely determined by the choice of \(k\), all we need to do is try all choices of \(k\) and test which ones put the largest number of competitors ahead <span style="white-space: nowrap;">of \(i\)!</span></p>

<p>There is a complication here with tiebreaks that I am not handling. If two competitors get the same combined score, the one who is ahead in two of the three disciplines gets the higher combined rank. If three competitors get the same combined score, and they have a cyclic ordering on tiebreaks, I don’t know what happens, and I suspect the rules don’t cover that situation. To simplify things I will just assume that all tiebreaks go against the candidate (so we might not guarantee qualification until the ties are resolved).</p>

<h1 id="algorithm">Algorithm</h1>

<p>Based on these simplifications, our algorithm for computing the maximum combined rank \(r_i\) of competitor \(i\) performs the following steps:</p>

<ul>
  <li>
    <p>If \(i\) is not already in \(X\), add it to \(X\) with a position after all of the other competitors <span style="white-space: nowrap;">in \(X\).</span></p>
  </li>
  <li>
    <p>Loop through all choices of \(k\) in \(0\dots\vert Y\vert\). For each choice:</p>

    <ul>
      <li>
        <p>Determine, for each \(j\) in the \(k\) smallest-weight competitors in \(Y\), the position \(p_j\) that \(j\) would need to obtain to <span style="white-space: nowrap;">beat \(i\)</span></p>
      </li>
      <li>
        <p>While any two of these competitors have the same value for \(p_j\), decrement one of these two values. If this causes any value to become non-positive, continue the outer loop with the next choice <span style="white-space: nowrap;">of \(k\).</span></p>
      </li>
      <li>
        <p>Place the competitors in \(Y\) that are not among the \(k\) smallest immediately above \(i\) in the ordering</p>
      </li>
      <li>
        <p>Place the \(k\) smallest-weight competitors into positions \(p_j\), preserving the ordering of the remaining competitors.</p>
      </li>
      <li>
        <p>In the resulting ordering, determine how many competitors <span style="white-space: nowrap;">beat \(i\)</span></p>
      </li>
    </ul>
  </li>
  <li>
    <p>Return the maximum number of competitors beating \(i\) in all of the orderings that have been examined</p>
  </li>
</ul>

<p>With some care it should be possible to do all of this in time \(O(nk)\). <a href="https://11011110.github.io/blog/assets/2021/rank-product/qualify.py">My implementation</a> is slower because I was more interested in getting it to work than in optimizing it, and for \(n=20\) it is blazingly fast regardless.</p>

<h1 id="outcomes">Outcomes</h1>

<p>With all that in mind, and assuming that (somehow) I’ve implemented this correctly, let’s look at what this algorithm predicts for the actual data.</p>

<p style="text-align: center;"><img alt="Ranking with five climbs to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/2.jpg" width="80%"/></p>

<p>At this stage of the qualifications, five competitors are left to climb. NBC predicted that Seo, Raboutou, and Pilz had already qualified, and my implementation agrees for Seo and Raboutou (both can finish at worst 8th) but it was incorrect for Pilz, who could drop to 9th if Mirosław miraculously finished 2nd and Garnbret 3rd. More surprising to me, Garnbret was still not an automatic qualifier: if she finished 20th, enough other competitors could better her score to put her into 9th place.</p>

<p style="text-align: center;"><img alt="Ranking with three climbs to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/3.jpg" width="80%"/></p>

<p>After two more climbs, the faulty NBC algorithm claims that five climbers have qualified. My program agrees for Seo (worst rank 6), Nonaka (worst rank 5), Raboutou (worst rank 7), but still not Pilz (worst rank 9) or Jaubert (worst rank 10). My code now thinks Garnbret has locked in a rank of at worst 7th, qualifying without even climbing yet.</p>

<p style="text-align: center;"><img alt="Ranking with two climbs to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/4.jpg" width="80%"/></p>

<p>Shauna Coxsey, climbing with an injured knee, has climbed into the middle of the pack, falling off the top ten ranking, and her place has been taken by Kyra Condie. Garnbret is now at worst 6th and should have been marked as qualified. Noguchi is not quite guaranteed yet, with a scenario in which she could rank 9th. Seo is now at worst 5th, Nonaka at worst 4th, Raboutou at worst 5th, and Pilz at worst 8th (now guaranteeing her spot). But Jaubert and Mirosław each could end 9th; at most one of Jaubert, Mirosław, or Noguchi will be eliminated, but we can’t yet guarantee any of their spots.</p>

<p style="text-align: center;"><img alt="Ranking with one climb to go in the 2021 Olympic women's sport climbing qualifying event" src="https://11011110.github.io/blog/assets/2021/rank-product/5.jpg" width="80%"/></p>

<p>Until now, all of the “Q” markings shown on the livestream, while mathematically incorrect in many cases, were at least correct in hindsight: the people marked that way did end up qualifying. This one, though, a repeat of the first image in this post, gets it wrong in practice as well as in theory. Meshkova is marked as qualifying, but did not. Coxsey has moved back into the top ten. Mirosław and Jaubert could still have ended up 9th (under different scenarios, obviously) and should not have been marked as qualifying. Seo is at worst 4th, Nonaka is at most 3rd, Noguch is at most 4th, Raboutou is at most 5th, and Pilz is at most 6th.</p>

<p>And the final ranking:</p>

<p style="text-align: center;"><img alt="Final ranking in the 2021 Olympic women's sport climbing qualifying event, top ten" src="https://11011110.github.io/blog/assets/2021/rank-product/6a.jpg" width="80%"/></p>

<p style="text-align: center;"><img alt="Final ranking in the 2021 Olympic women's sport climbing qualifying event, bottom ten" src="https://11011110.github.io/blog/assets/2021/rank-product/6b.jpg" width="80%"/></p>

<h1 id="the-future">The future</h1>

<p>I think the moral of the story is that this ranking system is too hard to understand and a little too random (with players trading places too much depending on what other players do). To some extent this is unavoidable (a version of <a href="https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem">Arrow’s impossibility theorem for rank aggregation</a>), but the scuttlebutt seems to be that this system will be replaced for future competitions. Which, sadly, makes all of this algorithm design a little redundant…</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106707759220927820">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-08-05T23:23:00Z</updated>
    <published>2021-08-05T23:23:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-08-11T04:43:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5445216218770736629</id>
    <link href="http://blog.computationalcomplexity.org/feeds/5445216218770736629/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/pole-vault-live-blogging.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/5445216218770736629" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/5445216218770736629" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/pole-vault-live-blogging.html" rel="alternate" type="text/html"/>
    <title>Pole Vault Live Blogging</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As I write this I'm watching the women's pole vault final in the Olympics. Of the 15 women who made the finals, only four remain after two heights.</p><p>To expand on my <a href="https://twitter.com/fortnow/status/1421510617878437891">tweet</a>, I find the pole vault the purest of the Olympic Sports. No electronic monitors and timers, no biased judges, no video review. No points deducted for bad form or failing to stick the landing. No disqualification for a false start or stepping over a line. Either you clear the bar without knocking it down, or you don't.</p><p>The high jump has similar properties, but just not as cool looking.</p><p>All four made the third height. Now onto 4.85 meters. An American, a Greek, a Brit and a Russian (sorry I meant member of the Russian Olympic Committee).</p><p>Back in the day, the TV coverage was rather limited. We'd only see the Americans and the medal winners with too much time spend on human interest backgrounds. Now in the streaming world I can watch every competitor. The good and the bad. Live as it happens.</p><p>The Russian Anzhelika Sidorova just cleared 4.85 on her first attempt. So did the Brit Holly Bradshaw and the American Katie Nageotte. The Greek Katerina Stefanidi missed her first attempt but decided to pass on the rest. All now go to 4.90 but Stefanidi only gets two attempts while the rest get three.</p><p>Stefanidi missed her first attempt at 4.90. She gets one attempt left.</p><p>Sidorova and Bradshaw fail to even reach the bar. Nageotte can't clear the bar.</p><p>Now the moment that means everything for Stefanidi. Her last attempt. Make it or the rest get the medals. Stefaidi fails to get a good plant and doesn't get into the air at all. Her Olympics are over.</p><p>Second attempt for the others. Sidorva and Bardshaw knock down the bar. Nageotte clears the bar, putting her in prime position. Go USA!</p><p>Imagine if we judged research papers this way. Either they get into a conference or they don't. Wait, that is they way they happen, although not always without biased judging.</p><p>Sidorova is passing on her last attempt at 4.90. Bradshaw goes for it but hits the bar. She has to settle for Bronze.</p><p>Bar is now at 4.95 meters. </p><p>Sidorova gets only one attempt at 4.95. If she makes it, she takes the lead, if she misses, she gets the silver. </p><p>Sidorova doesn't clear and the gold goes to the American Katie Nageotte! </p><p>Just for excitement Nageotte is going for 5.01 meters, which would be her first over five meters in competition. In the men's pole vault, the Swede Armand Duplantis (great pole vault name!) easily won the gold. He moved the bar to 6.19 meters to break his own world record. Came all so close in his first attempt but failed to clear. </p><p>Nageotte is just too excited winning the gold to focus enough to make a serious attempt at 5.01. Can't blame her.</p><p>Thus ends the best sport in the Olympics.</p><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-UPu6__YpuCw/YQvp7AHcn7I/AAAAAAAB9pA/W4zoC8Jo9OM8pVQ7NOrPKIpTLrIqAWyewCLcBGAsYHQ/s797/polevault.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="229" src="https://1.bp.blogspot.com/-UPu6__YpuCw/YQvp7AHcn7I/AAAAAAAB9pA/W4zoC8Jo9OM8pVQ7NOrPKIpTLrIqAWyewCLcBGAsYHQ/w400-h229/polevault.png" width="400"/></a></div></div>
    </content>
    <updated>2021-08-05T13:44:00Z</updated>
    <published>2021-08-05T13:44:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-15T01:18:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21878</id>
    <link href="https://gilkalai.wordpress.com/2021/08/05/let-me-tell-you-about-three-of-my-recent-papers/" rel="alternate" type="text/html"/>
    <title>Let me tell you about three of my recent papers</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Let me tell you briefly about three of my papers that were recently accepted for publication. Relative Leray numbers via spectral sequences with Roy Meshulam, Helly-type problems with Imre Bárány, and Statistical aspects of quantum supremacy experiments with Yosi … <a href="https://gilkalai.wordpress.com/2021/08/05/let-me-tell-you-about-three-of-my-recent-papers/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><img alt="michaelrabin" class="alignnone size-full wp-image-21893" height="403" src="https://gilkalai.files.wordpress.com/2021/08/michaelrabin.jpg" width="605"/></p>
<p> </p>
<p>Let me tell you briefly about three of my papers that were recently accepted for publication. <a href="http://Let F be a fixed field and let X be a simplicial complex on the vertex set V. The Leray number L(X;F) is the minimal d such that for all i&#x2265;d and S&#x2282;V, the induced complex X[S] satisfies H~i(X[S];F)=0. Leray numbers play a role in formulating and proving topological Helly type theorems. For two complexes X,Y on the same vertex set V, define the relative Leray number LY(X;F) as the minimal d such that H~i(X[V&#x2216;&#x3C3;];F)=0 for all i&#x2265;d and &#x3C3;&#x2208;Y. In this paper we extend the topological colorful Helly theorem to the relative setting. Our main tool is a spectral sequence for the intersection of complexes indexed by a geometric lattice.">Relative Leray numbers via spectral sequences</a> with Roy Meshulam, <a href="https://gilkalai.files.wordpress.com/2021/08/hpaug05.pdf">Helly-type problems</a> with Imre Bárány, and <a href="https://arxiv.org/abs/2008.05177">Statistical aspects of quantum supremacy experiments</a> with Yosi Rinott and Tomer Shoham.</p>
<h3><a href="http://Let F be a fixed field and let X be a simplicial complex on the vertex set V. The Leray number L(X;F) is the minimal d such that for all i&#x2265;d and S&#x2282;V, the induced complex X[S] satisfies H~i(X[S];F)=0. Leray numbers play a role in formulating and proving topological Helly type theorems. For two complexes X,Y on the same vertex set V, define the relative Leray number LY(X;F) as the minimal d such that H~i(X[V&#x2216;&#x3C3;];F)=0 for all i&#x2265;d and &#x3C3;&#x2208;Y. In this paper we extend the topological colorful Helly theorem to the relative setting. Our main tool is a spectral sequence for the intersection of complexes indexed by a geometric lattice.">Relative Leray numbers via spectral sequences</a></h3>
<blockquote>
<p><span style="color: #ff0000;"><em>We extend the topological colorful Helly theorem to the relative setting. Our main tool is a spectral sequence for the intersection of complexes indexed by a geometric lattice. </em></span></p>
</blockquote>
<p>Roy and I have a<a href="https://scholar.google.com/scholar?hl=iw&amp;as_sdt=0%2C5&amp;q=kalai+and+meshulam&amp;btnG="> long term project</a> of studying topological Helly type theorems. Often, results from convexity give a simple and strong manifestation of theorems from topology: For example, Helly’s theorem manifests the nerve theorem from algebraic topology, and Radon’s theorem can be regarded as an early “linear” version of the Borsuk–Ulam theorem. We have a few more “linear” theorems in need of topologizing on our list. Actually the paper <a href="https://londmathsoc.onlinelibrary.wiley.com/doi/full/10.1112/mtk.12103">already appeared in Mathematika</a> on June 26, 2021. It is dedicated to our dear teacher, colleague and friend  Michael O. Rabin. </p>
<blockquote>
<h3><span style="color: #993300;"> Dedicated to Michael O. Rabin, a trailblazing mathematician and computer scientist</span></h3>
</blockquote>
<h3><a href="https://gilkalai.files.wordpress.com/2021/08/hpaug05.pdf">Helly type problems</a>, to appear in the Bulletin of the American Mathematical Society </h3>
<blockquote>
<p><span style="color: #ff0000;"><em>We present a variety of problems in the interface between combinatorics and geometry around the theorems of Helly, Radon, Carath ́eodory, and Tverberg. Through these problems we describe the fascinating area of Helly-type theorems, and explain some of its main themes and goals.</em></span></p>
</blockquote>
<p>Imre and I have long term common interest in Helly-type problems and often discussed it since we first met in 1982.  We wrote a first joint paper in 2016 and last year we wrote two additional papers with Attila Por.  Last year Imre wrote a great book  “Combinatorial convexity” (AMS, 2021, in press) largely devoted to Helly-type theorems. As for me, I plan on gradually writing on open problems related to my areas of interest. (See <a href="https://www.renyi.hu/conferences/erdos100/slides/kalai.pdf">these slides</a> for some problems.)   </p>
<h3><a href="https://arxiv.org/abs/2008.05177">Statistical aspects of quantum supremacy experiments</a></h3>
<p>Yosi Rinott, Tomer Shoham and I started this project about a year an a half ago. Our paper have now been accepted to Statistical Science where you can <a href="https://www.e-publications.org/ims/submission/STS/user/submissionFile/47360?confirm=ed13d436">download the accepted version</a> along <a href="https://imstat.org/journals-and-publications/statistical-science/statistical-science-future-papers/">many other future papers</a>. This is my second paper in Statistical Science. The first one was “<a href="https://projecteuclid.org/journals/statistical-science/volume-14/issue-2/Solving-the-Bible-Code-Puzzle/10.1214/ss/1009212243.full">Solving the bible code puzzle</a>” with Brendan McKay, Dror Bar-Nathan and Maya Bar-Hillel, that appeared in 1999.     </p>
<blockquote>
<p><span style="color: #ff0000;"><em>In quantum computing, a demonstration of quantum supremacy (or quantum advantage) consists of presenting a task, possibly of no practical value, whose computation is feasible on a quantum device, but cannot be performed by classical computers in any feasible amount of time. The notable claim of quantum supremacy presented by Google’s team in 2019 consists of demonstrating the ability of a quantum circuit to generate, albeit with considerable noise, bitstrings from a distribution that is considered hard to simulate on classical computers. Very </em></span><span style="color: #ff0000;"><em>recently, in 2020, a quantum supremacy claim was presented by a group from the University of Science and Technology of China, using a different technology and generating a different distribution, but sharing some statistical principles with Google’s demonstration. </em></span></p>
<p><span style="color: #ff0000;"><em>Verifying that the generated data is indeed from the claimed distribution and assessing the circuit’s noise level and its fidelity is a statistical undertaking. The objective of this paper is to explain the relations between quantum computing and some of the statistical aspects involved in demonstrating quantum supremacy in terms that are accessible to statisticians, computer scientists, and mathematicians. Starting with the statistical modeling and analysis in Google’s demonstration, which we explain, we study various estimators of the fidelity, and different approaches to testing the distributions generated by the quantum computer. We propose different noise models, and discuss their implications. A preliminary study of the Google data, focusing mostly on circuits of 12 and 14 qubits is given in different parts of the paper</em></span></p>
</blockquote>


<p/></div>
    </content>
    <updated>2021-08-05T11:18:43Z</updated>
    <published>2021-08-05T11:18:43Z</published>
    <category term="Combinatorics"/>
    <category term="Convexity"/>
    <category term="Geometry"/>
    <category term="Quantum"/>
    <category term="Statistics"/>
    <category term="Imre B&#xE1;r&#xE1;ny"/>
    <category term="Michael O. Rabin"/>
    <category term="Roy Meshulam"/>
    <category term="Tomer Shoham"/>
    <category term="Yosi Rinott"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-08-15T06:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19028</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/" rel="alternate" type="text/html"/>
    <title>Turning the Tables on Cheating?</title>
    <summary>Colonel Stok: Do you play chess? Harry Palmer: Yes, but I prefer a game with a better chance of cheating. Sleuth source Michael Caine played Harry Palmer in the movie Funeral in Berlin. This was released in 1966—long before computers could play terrific chess. Perhaps he would have a different answer today? Today I thought […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Colonel Stok: Do you play chess?</em></font></p><font color="#0044cc"><em>
</em><p><em>
Harry Palmer: Yes, but I prefer a game with a better chance of cheating.</em><br/>
<font color="#000000"/></p><font color="#000000">
<p/><p/>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/sleuth-1972-screencaps-michael-caine-5575427-550-330/" rel="attachment wp-att-19031"><img alt="" class="alignright size-full wp-image-19031" height="100" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/Sleuth-1972-Screencaps-michael-caine-5575427-550-330.jpg?resize=154%2C100&amp;ssl=1" width="154"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><i>Sleuth</i> <a href="https://www.storypick.com/memorable-michael-caine-roles/">source</a></font></td>
</tr>
</tbody>
</table>
<p/><p>
Michael Caine played <a href="https://en.wikipedia.org/wiki/Harry_Palmer">Harry Palmer</a> in the movie <a href="https://www.quotes.net/mquote/35048">Funeral in Berlin</a>. This was released in 1966—long before computers could play terrific chess. Perhaps he would have a different answer today?</p>
<p>
Today I thought we might look at chess cheating in a way that complements what Ken does.<br/>
<span id="more-19028"/></p>
<p>
Of course Ken refers to Ken Regan. He is one of the world experts at detecting chess cheating. Detection is based solely on the statistics of move choice. The only data given to Ken about the games are the moves that were played and the overall time allowance. But this ignores the players and equipment on the scene. People can cheat in ways that are closer to issues in computer security and protocols.</p>
<p>
Caine knows a lot about the latter. He is a doyen of <a href="https://filmsane.com/5-of-my-favorite-michael-caine-heist-movies/">heist movies</a> and those increasingly involve security. In the 1966 comedy <a href="https://en.wikipedia.org/wiki/Gambit_(1966_film)">Gambit</a> it is as simple as working around an alarm, but the 1969 version of <a href="https://en.wikipedia.org/wiki/The_Italian_Job">The Italian Job</a> has a switch of computer data reels and jamming traffic cameras. His latest movie <a href="https://en.wikipedia.org/wiki/Tenet_(film)">Tenet</a> centers on an algorithm for inverting time and entropy on Earth. He is also a fan of chess. He tangles with his co-star Laurence Olivier <a href="http://www.chess-in-the-cinema.de/showfilm.php?filmfile=7258.txt&amp;pfad=7079">amid</a> chess sets in the movie <a href="https://en.wikipedia.org/wiki/Sleuth_(1972_film)">Sleuth</a>. Most of all, his character in 2009’s <a href="https://en.wikipedia.org/wiki/Harry_Brown_(film)">Harry Brown</a> is a chess player, who <a href="https://www.dailymotion.com/video/xqr41b">discourses</a> on the <a href="https://www.chessgames.com/perl/chessgame?gid=1044731">17th</a> (not 7th as said) game of the 1972 championship between Bobby Fischer and Boris Spassky. </p>
<p>
</p><p/><h2> Cheating at Chess—the Easy Way </h2><p/>
<p/><p>
Derren Brown is an <a href="https://derrenbrown.co.uk">illusionist</a>—a magician. He claims that he is a weak chess player. But he had Britain’s Channel 4 broadcast him playing nine strong players, including two grandmasters. Yet he won the match <b>5-4</b>. </p>
<p>
This is how he did it. He used an ancient <a href="https://en.wikipedia.org/wiki/Cheating_in_chess#Simultaneous_games">trick</a>. Say he plays two games: one against Alice and one against Bob. He plays black against Alice and white against Bob. After he gets Alice’s first move he plays that exact move against Bob. Then after he gets Bob’s move he plays that one against Alice. And so on.</p>
<p>
Suppose he loses both games. Thus Alice wins and Bob wins. But that means that Bob’s answer to Alices’ move was a winner and so on. This is impossible and so he must win at least one of the games. Thus he wins one game unless both end in a tie. </p>
<p>
The illusion is, how could he win five games against nine players given two were grandmasters? Brown played one of the nine games for real—he won that one. The “table trick” only works for an even number of games. It is not, of course, a new <a href="https://en.chessbase.com/post/the-magical-che-experiment">idea</a>: </p>
<blockquote><p><b> </b> <em> Alekhine and his twice world championship challenger Bogoljubov were once challenged separately by a relative patzer to games of correspondence chess at money odds. In effect, of course, the anonymous opportunist was playing in neither game. The story goes that the two players, who were friends away from the board, met up one day and latched on to what was happening. </em>
</p></blockquote>
<p/><p>
The imitation trick is a real potential issue in <em>Basque chess</em>, where two players play two games simultaneously, one as white and one as black. By copying each other’s moves, they would always tie. An early <a href="https://en.chessbase.com/post/che-magazine-basque-che-does-it-work-for-you-">description</a> noted the issue but Ken has not been able to find how the rules forbid it. </p>
<p>
A similar situation <a href="https://www.chess.com/article/view/chess-arbiters">happened</a> recently in a real tournament—a world championship qualifier, no less. Two games at adjacent tables played almost twenty of the same opening moves. The chief arbiter—someone Ken corresponds with several times a week—moved one of the games to a different area. International Master Danny Rensch, who heads the major online playing site <a href="https://www.chess.com/">Chess.com</a>, made a <a href="https://youtu.be/-6QX53BmDbg">video</a> “Are You Copying Me?” of the incident. None of the four players involved was cheating, but this illustrates the kind of people dynamics one needs to watch for.</p>
<p>
</p><p/><h2> Cheating at Chess—the Too Easy Way </h2><p/>
<p/><p>
This is to cheat by consulting a computer program that is stronger than all human players, such as the free program <a href="https://en.wikipedia.org/wiki/Stockfish_(chess)">Stockfish</a>, without anything impeding one’s ability to access the program’s recommendations during the game. This is often the case in online chess without sophisticated measures to detect the access. </p>
<p>
Ken’s statistical model can still judge the moves, but this is after the fact. We would like to <em>prevent</em> cheating. This hasn’t happened. Already last year, Ken was <a href="https://www.theguardian.com/sport/2020/oct/16/chesss-cheating-crisis-paranoia-has-become-the-culture">quoted</a> in the UK Guardian newspaper saying, “The pandemic has brought me as much work in a single day as I have had in a year previously.” A Wall Street Journal <a href="https://www.wsj.com/articles/the-real-queens-gambit-catching-chess-cheaters-11607439491">story</a> that featured Ken also noted:</p>
<blockquote><p><b> </b> <em> The data showed something curious. More people were playing chess. Yet the fair play violations were surging even faster than the number of overall games. “Which makes us think that there has been an uptick in the rate of cheating,” said Gerard Le-Marechal, head of cheat detection for Chess.com. </em>
</p></blockquote>
<p/><p>
This year has brought no letup—see Ken’s statement prefacing a non-chess <a href="https://rjlipton.wpcomstaging.com/2021/07/22/the-reach-of-dichotomy/">post</a> that June and July were the worst. Chess.com and Lichess and other playing sites have the final say but Ken is often used for both early warning (his “screening” step is agile and gives officials an informative snapshot of an entire tournament) and for explaining verdicts afterwards, since his model is transparent and not compromised by divulging explanations.</p>
<p>
But again, this is after the fact. I recall a <a href="https://rjlipton.wpcomstaging.com/2016/05/20/making-public-information-secret/">post</a> we wrote about ways computer security is like “closing the barn door after the horse has already left.” The open question that I find interesting is not how cheaters can be detected, but is there some way to make it hard for them to cheat at all. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/08/04/turning-the-tables-on-cheating/cainebbb/" rel="attachment wp-att-19037"><img alt="" class="aligncenter size-full wp-image-19037" height="279" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/CaineBBB.jpg?resize=295%2C279&amp;ssl=1" width="295"/></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2">ARPANET history <a href="https://computer.howstuffworks.com/arpanet.htm#pt1">source</a><br/>
</font>
</td>
</tr>
</tbody></table>
<p>
Of course, this is not just about chess, or other games forced online by the pandemic. It extends to administering courses and tests, at a time when the prospect of a “normal” in-person Fall semester is being roiled by the surge we’ve <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">previewed</a> and <a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/">tracked</a> on this blog. </p>
<p>
</p><p/><h2> Cheating at Chess—the Harder Way </h2><p/>
<p/><p>
The number of ways people have cheated at in-person chess is legion. Wikipedia has a long <a href="https://en.wikipedia.org/wiki/Cheating_in_chess">list</a>. Ken put the ways in cases he’d encountered to a Dr. Seuss rhyme midway through his 2014 TEDx Buffalo <a href="https://www.youtube.com/watch?v=9W3D8xVAKao">talk</a>. </p>
<p>
On March 15, 2020, the New York Times published an <a href="https://www.nytimes.com/2020/03/15/sports/chess-cheating.html">article</a> on tech in chess cheating. It drew analogy to the Houston Astros scandal, including the same example we just <a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/">covered</a> of whether José Altuve was wired for his series-winning home run in 2019. It touched on online chess and quotes Le-Marechal but <em>showed no inkling of</em> the impending pandemic and its effect on chess. Its first sentence about chess alludes to the 1978 incident in which Viktor Korchnoi alleged that Anatoly Karpov could receive coded information about their match games via the flavor of yogurt delivered to the table. </p>
<p>
I, Ken writing this part, have been part of discussions of how a yogurt <em>spoon</em> dropped audibly could be one of myriad possible signals from the audience. The pandemic caused this year’s Tata Steel tournament to be played <a href="https://www.dutchnews.nl/news/2021/01/no-women-at-tata-steel-chess-this-year-but-game-is-growing-in-popularity/">without</a> audience, while some other elite events are played in an “<a href="https://www.chess.com/news/view/bilbao-chess-outside-in-a-glass-cube">aquarium</a>” with one-way glass. But that does not work for larger-scale Open tournaments. Jamming RF signals is generally illegal. I agree with those recommending that an illusionist like Brown—someone with an eagle eye for watching people—be employed to help the arbiters at large events.</p>
<p>
Yet for all the ways and means out there, it is still <em>hard</em> to cheat at in-person chess. Its state is one that organizers of <em>online chess</em> would gladly reach if they could. FIDE has promoted a <a href="https://en.chessbase.com/post/the-rise-of-hybrid-chess">hybrid</a> form in which players travel to regional rooms watched by arbiters, but this is hard to manage on large scale. Dick and I have debated all year what to do for online chess, and we’ve converged on two poles of answers.</p>
<p>
</p><p/><h2> Way #1: Standardized Playing Tabletops </h2><p/>
<p/><p>
The paradox, noted this week by International Master Nisha Mohota in her recent <a href="https://youtu.be/hpLiG1UgIus">video</a> on cheating, is that the popularity of chess online has burgeoned during the pandemic. But this also enhances the following dilemma:</p>
<ul>
<li>
Having a second camera—side view supplementing screen view—has been an effective measure. <p/>
</li><li>
But requiring even one camera has been an acknowledged obstacle to expanding the reach of chess tournaments. <p/>
</li><li>
And it takes extra human resources to monitor two video feeds per player.
</li></ul>
<p>
Online education has also <a href="https://www.erasmusmagazine.nl/en/2021/01/15/students-will-have-to-use-phone-as-a-second-camera-in-proctored-online-exams/">recognized</a> the importance of a second camera, <a href="https://www.theabr.org/announcements/remote-exam-information">requiring</a> one in some cases. Yet allowing the user to control how the side camera is positioned may allow circumventions, and mandating one connotes distrust and negativity in a bare sense. </p>
<p>
My suggestion is to try to turn around the negative aspect into a positive by marketing a standardized and hopefully-inexpensive “Online Tabletop Arena.” It would have three walls to feel like an alcove. The walls, one with a side camera built in, would limit hand movements as well as sight lines. Standardization would lessen stigma and help monitoring. It could also be used for online test taking.</p>
<p>
</p><p/><h2> Way #2: Give In </h2><p/>
<p/><p>
The real import of our mentioned security <a href="https://rjlipton.wpcomstaging.com/2016/05/20/making-public-information-secret/">post</a> is to stop trying to stick thumbs in all the dam holes. Mohota in her video laments kids being exposed to chess programs and advocates training without them, but that strikes us as trying to close ten thousand barn doors while a million free horses are out there. </p>
<p>
So let’s give in: Allow the players to use computers freely. The more, the merrier. But as in the 1967 Caine-as-Harry-Palmer movie <a href="https://en.wikipedia.org/wiki/Billion_Dollar_Brain">Billion Dollar Brain</a>, we reward the humans for how they <em>disobey</em> the computer calling the shots.</p>
<p>
One way to implement this would be to have the chess playing site appoint one unknown (say, randomly selected) strong chess program <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as the official scorer of all games. A player’s score for a won or drawn game would be proportional to the total difference from <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in Ken’s metrics. Perhaps credit could be given also for a valiantly lost game.</p>
<p>
This scheme would directly reward players for the amount of <em>non-</em>cheating they do. Or put more positively, human ingenuity apart from computers would bring the reward. The ability to sleuth strategy beyond computer moves was already <a href="https://www.psychologytoday.com/us/blog/seeing-what-others-dont/201710/the-age-centaurs">demonstrated</a> in so-called <a href="https://en.wikipedia.org/wiki/Advanced_chess">freestyle</a> tournaments held in 2007-08 and 2014. A particularly nice example of playing a sacrifice that the computer does not like was executed at turn 18 by Magnus Carlsen in his World Cup <a href="https://www.chessbomb.com/arena/2021-fide-world-cup/08-01-Fedoseev_Vladimir-Carlsen_Magnus">win</a> today over Vladimir Fedoseev.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We admit the tabletop suggestion is far from electrifying, but has anyone come up with better? As always we welcome suggestions from our readers, or pointers to forum discussions that you agree with.</p>
<p/></font></font></div>
    </content>
    <updated>2021-08-04T21:38:36Z</updated>
    <published>2021-08-04T21:38:36Z</published>
    <category term="All Posts"/>
    <category term="chess"/>
    <category term="detection"/>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="trick"/>
    <category term="cheating"/>
    <category term="Derren Brown"/>
    <category term="fraud-detection"/>
    <category term="Michael Caine"/>
    <category term="online chess"/>
    <category term="pandemic"/>
    <category term="security"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-08-15T06:20:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/</id>
    <link href="https://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/" rel="alternate" type="text/html"/>
    <title>Complexity Postdoctoral Fellowship at Santa Fe Institute (apply by October 24, 2021)</title>
    <summary>Unique among postdocs, these 3-year fellowships offer the opportunity to join a collaborative community that nurtures creative, transdisciplinary thought in pursuit of key insights about the complex systems that matter most for science and society. Benefits include research/collaboration funds, paid family leave, and a professional leadership &amp; development program. Website: https://santafe.edu/news-center/news/apply-now-santa-fe-institute-postdoctoral-fellowships-2021 Email: sfifellowship@santafe.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Unique among postdocs, these 3-year fellowships offer the opportunity to join a collaborative community that nurtures creative, transdisciplinary thought in pursuit of key insights about the complex systems that matter most for science and society. Benefits include research/collaboration funds, paid family leave, and a professional leadership &amp; development program.</p>
<p>Website: <a href="https://santafe.edu/news-center/news/apply-now-santa-fe-institute-postdoctoral-fellowships-2021">https://santafe.edu/news-center/news/apply-now-santa-fe-institute-postdoctoral-fellowships-2021</a><br/>
Email: sfifellowship@santafe.edu</p></div>
    </content>
    <updated>2021-08-02T22:29:27Z</updated>
    <published>2021-08-02T22:29:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-08-15T06:20:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5675</id>
    <link href="https://www.scottaaronson.com/blog/?p=5675" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5675#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5675" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">On blankfaces</title>
    <summary xml:lang="en-US">For years, I’ve had a private term I’ve used with my family. To give a few examples of its use: No, I never applied for that grant. I spent two hours struggling to log in to a web portal designed by the world’s top blankfaces until I finally gave up in despair. No, I paid […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>For years, I’ve had a private term I’ve used with my family.  To give a few examples of its use:</p>



<blockquote class="wp-block-quote"><p>No, I never applied for that grant. I spent two hours struggling to log in to a web portal designed by the world’s top blankfaces until I finally gave up in despair.</p></blockquote>



<blockquote class="wp-block-quote"><p>No, I paid for that whole lecture trip out of pocket; I never got the reimbursement they promised.  Their blankface administrator just kept sending me back the form, demanding more and more convoluted bank details, until I finally got the hint and dropped it.</p></blockquote>



<blockquote class="wp-block-quote"><p>No, my daughter Lily isn’t allowed in the swimming pool there.  She easily passed their swim test last year, but this year the blankface lifeguard made up a new rule on the spot that she needs to retake the test, so Lily took it again and passed even <em>more</em> easily, but then the lifeguard said she didn’t like the stroke Lily used, so she failed her and didn’t let her retake it.  I complained to their blankface athletic director, who launched an ‘investigation.’  The outcome of the ‘investigation’ was that, regardless of the ground truth about how well Lily can swim, their blankface lifeguard said she’s not allowed in the pool, so being blankfaces themselves, they’re going to stand with the lifeguard.</p></blockquote>



<blockquote class="wp-block-quote"><p>Yeah, the kids spend the entire day indoors, breathing each other’s stale, unventilated air, then they finally go outside and they aren’t allowed on the playground equipment, because of the covid risk from them touching it.  Even though we’ve known for more than a year that covid is an airborne disease.  Everyone I’ve talked there agrees that I have a point, but they say their hands are tied.  I haven’t yet located the blankface who actually made this decision and stands by it.</p></blockquote>



<p>What exactly is a blankface?  He or she is often a mid-level bureaucrat, but not every bureaucrat is a blankface, and not every blankface is a bureaucrat.  A blankface is anyone who enjoys wielding the power entrusted in them to make others miserable by acting like a cog in a broken machine, rather than like a human being with courage, judgment, and responsibility for their actions.  A blankface meets every appeal to facts, logic, and plain compassion with the same repetition of rules and regulations and the same blank stare—a blank stare that, more often than not, conceals a contemptuous smile.</p>



<p>The longer I live, the more I see blankfacedness as one of the fundamental evils of the human condition.  Yes, it contains large elements of stupidity, incuriosity, malevolence, and bureaucratic indifference, but it’s not reducible to any of those.  After enough experience, the first two questions you ask about any organization are:</p>



<ol><li>Who are the blankfaces here?</li><li>Who are the people I can talk with to get around the blankfaces?</li></ol>



<p>As far as I can tell, blankfacedness cuts straight across conventional political ideology, gender, and race.  (Age, too, except that I’ve never once encountered a blankfaced child.)  Brilliance and creativity do seem to offer some protection against blankfacedness—possibly because the smarter you are, the harder it is to justify idiotic rules to yourself—but even there, the protection is far from complete.</p>



<hr class="wp-block-separator"/>



<p>Twenty years ago, all the conformists in my age cohort were obsessed with the <em>Harry Potter</em> books and movies—holding parties where they wore wizard costumes, etc.  I decided that the <em>Harry Potter</em> phenomenon was a sort of collective insanity: from what I could tell, the stories seemed like startlingly puerile and unoriginal mass-marketed wish-fulfillment fantasies.</p>



<p>Today, those same conformists in my age cohort are more likely to condemn the <em>Harry Potter</em> series as Problematically white, male, and cisnormative, and J. K. Rowling herself as a monstrous bigot whose acquaintances’ acquaintances should be shunned.  Naturally, then, there was nothing for me to do but finally read the series!  My 8-year-old daughter Lily and I have been partner-reading it for half a year; we’re just finishing book 5.  (<em>After</em> we’ve finished the series, we might start on <em><a href="http://www.hpmor.com/">Harry Potter and the Methods of Rationality</a></em> … which, I confess, I’ve also never read.)</p>



<p>From book 5, I learned something extremely interesting.  The most despicable villain in the <em>Harry Potter</em> universe is not Lord Voldemort, who’s mostly just a faraway cipher and abstract embodiment of pure evil, no more hateable than an earthquake.  Rather, it’s <a href="https://en.wikipedia.org/wiki/Dolores_Umbridge">Dolores Jane Umbridge</a>, the toadlike Ministry of Magic bureaucrat who takes over Hogwarts school, forces out Dumbledore as headmaster, and terrorizes the students with increasingly draconian “Educational Decrees.”  Umbridge’s decrees are mostly aimed at punishing Harry Potter and his friends, who’ve embarrassed the Ministry by telling everyone the truth that Voldemort has returned and by readying themselves to fight him, thereby defying the Ministry’s head-in-the-sand policy.</p>



<p>Anyway, I’ll say this for <em>Harry Potter</em>: Rowling’s portrayal of Umbridge is so spot-on and merciless that, for anyone who knows the series, I could simply <em>define</em> a blankface to be anyone sufficiently Umbridge-like.</p>



<hr class="wp-block-separator"/>



<p>This week I <em>also</em> finished reading <em><a href="https://www.amazon.com/Premonition-Pandemic-Story-Michael-Lewis-ebook/dp/B08V91YY8R">The Premonition</a></em>, the thrilling account of the runup to covid by <a href="https://en.wikipedia.org/wiki/Michael_Lewis">Michael Lewis</a> (who also wrote <em><a href="https://www.amazon.com/Big-Short-Inside-Doomsday-Machine/dp/0393338827">The Big Short</a></em>, <em><a href="https://en.wikipedia.org/wiki/Moneyball">Moneyball</a></em>, etc).  Lewis tells the stories of a few individuals scattered across US health and government bureaucracies who figured out over the past 20 years that the US was breathtakingly unprepared for a pandemic, and who struggled against official indifference, mostly unsuccessfully, to try to fix that.  As covid hit the US in early 2020, these same individuals frantically tried to pull the fire alarms, even as the Trump White House, the CDC, and state bureaucrats all did everything in their power to block and sideline them.  We all know the results.</p>



<p>It’s no surprise that, in Lewis’s telling, Trump and his goons come in for world-historic blame: however terrible you thought they were, they were worse.  It seems that John Bolton, in particular, gleefully took an ax to everything the two previous administrations had done to try to prepare the federal government for pandemics—after Tom Bossert, the one guy in Trump’s inner circle who’d actually taken pandemic preparation seriously, was forced out for contradicting Trump about Russia and Ukraine.</p>



<p>But the left isn’t spared either.  The most compelling character in <em>The Premonition</em> is <a href="https://en.m.wikipedia.org/wiki/Charity_Dean">Charity Dean</a>, who escaped from the Christian fundamentalist sect in which she was raised to put herself through medical school and become a crusading public-health officer for Santa Barbara County.  Lewis relates with relish how, again and again, Dean startled the bureaucrats around her by taking matters into her own hands in her war against pathogens—e.g., slicing into a cadaver herself to take samples when the people whose job it was wouldn’t do it.</p>



<p>In 2019, Dean moved to Sacramento to become California’s next chief public health officer, but then Governor Gavin Newsom blocked her expected promotion, instead recruiting someone from the outside named Sonia Angell, who had no infectious disease experience but to whom Dean would have to report.  Lewis reports the following as the reason:</p>



<blockquote class="wp-block-quote"><p>“It was an optics problem,” says a senior official in the Department of Health and Human Services.  “Charity was too young, too blond, too Barbie.  They wanted a person of color.”  Sonia Angell identified as Latina.</p></blockquote>



<p>After it became obvious that the White House and the CDC were both asleep at the wheel, the competent experts’ Plan B was to get California to set a national standard, one that would shame all the other states into acting, by telling the truth about covid and by aggressively testing, tracing, and isolating.  And here comes the tragedy: Charity Dean spent from mid-January till mid-March trying to do exactly that, and Sonia Angell blocked her.  Angell—who comes across as a real-life Dolores Umbridge—banned Dean from using the word “pandemic,” screamed at her for her insubordination, and systematically shut her out of meetings.  Angell’s stated view was that, until and unless the CDC said that there was a pandemic, <em>there was no pandemic</em>—regardless of what hospitals across California might be reporting to the contrary.</p>



<p>As it happens, California <em>was</em> the first state to move aggressively against covid, on March 19—basically because as the bodies started piling up, Dean and her allies finally managed to maneuver around Angell and get the ear of Governor Newsom directly.  Had the response started earlier, the US might have had an outcome more in line with most industrialized countries.  Half of the 630,000 dead Americans might now be alive.</p>



<p>Sonia Angell fully deserves to have her name immortalized by history as one of the blankest of blankfaces.  But of course, Angell was far from alone.  Robert Redfield, Trump’s CDC director, was a blankface extraordinaire.  Nancy Messonnier, who lied to stay in Trump’s good graces, was a blankface too.  The entire CDC and FDA seem to have teemed with blankfaces.  As for Anthony Fauci, he became a national hero, maybe even deservedly so, merely by <em>not being 100%</em> a blankface, when basically every other “expert” in the US with visible power was.  Fauci cleared a depressingly low bar, one that the people profiled by Lewis cleared at Simone-Biles-like heights.</p>



<p>In March 2020, the fundamental question I had was: where are the supercompetent rule-breaking American heroes from the disaster movies?  What’s taking them so long?  <em>The Premonition</em> satisfyingly answers that question.  It turns out that the heroes did exist, scattered across the American health bureaucracy.  They were screaming at the top of their lungs.  But they were outvoted by the critical mass of blankfaces that’s become one of my country’s defining features.</p>



<hr class="wp-block-separator"/>



<p>Some people will object that the term “blankface” is dehumanizing.  The reason I disagree is that a blankface is someone who freely chose to dehumanize <em>themselves</em>: to abdicate their human responsibility to see what’s right in front of them, to <em>act like</em> malfunctioning pieces of electronics even though they, like all of us, were born with the capacity for empathy and reason.</p>



<p>With many other human evils and failings, I have a strong inclination toward mercy, because I understand how someone could’ve succumbed to the temptation—indeed, I worry that I myself might’ve succumbed to it “but for the grace of God.”  But here’s the thing about blankfaces: in all my thousands of dealings with them, not once was I ever given cause to wonder whether I might have done the same in their shoes.  It’s like,<em> of course</em> I wouldn’t have!  Even if I were forced (by my own higher-ups, an intransigent computer system, or whatever else) to foist some bureaucratic horribleness on an innocent victim, I’d be sheepish and apologetic about it.  I’d acknowledge the farcical absurdity of what I was making the other person do, or declaring that they couldn’t do.  Likewise, even if I were useless in a crisis, at least I’d <em>get out of the way</em> of the people trying to solve it.  How could I live with myself otherwise?</p>



<p>The fundamental mystery of the blankfaces, then, is how they can be so alien and yet so common.</p>



<hr class="wp-block-separator"/>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Aug. 3):</span></strong> Surprisingly many people seem to have read this post, and come away with the notion that a “blankface” is simply anyone who’s a stickler for rules and formalized procedures.  They’ve then tried to refute me with examples of where it’s <em>good</em> to be a stickler, or where I in particular would believe that it’s good.</p>



<p>But no, that’s not it at all.</p>



<p>Rules can be either good or bad.  All things considered, I’d probably rather be on a plane piloted by a robotic stickler for safety rules, than by someone who ignored the rules at his or her discretion.  And as I said in the post, in the first months of covid, it was ironically the <em>anti</em>-blankfaces who were screaming for rules, regulations, and lockdowns; the blankfaces wanted to continue as though nothing had changed!</p>



<p>Also, “blankface” (just like “homophobe” or “antisemite”) is a serious accusation.  I’d never call anyone a blankface merely for sticking with a defensible rule when it turned out, in hindsight, that the rule could’ve been relaxed.</p>



<p>Here’s how to tell a blankface: suppose you see someone enforcing or interpreting a rule in a way that strikes you as <em>obviously</em> absurd.  And suppose you point it out to them.</p>



<p>Do they say “I disagree, here’s why it actually <em>does</em> make sense”?  They might be mistaken but they’re not a blankface.</p>



<p>Do they say “tell me about it, it makes <em>zero</em> sense, but it’s above my pay grade to change”?  You might wish they were more dogged or courageous but again they’re not a blankface.</p>



<p>Or do they ignore all your arguments and just restate the original rule—seemingly angered by what they understood as a challenge to their authority, and delighted to reassert it?  <em>That’s</em> the blankface.</p></div>
    </content>
    <updated>2021-08-02T20:30:48Z</updated>
    <published>2021-08-02T20:30:48Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-08-15T01:21:06Z</updated>
    </source>
  </entry>
</feed>
