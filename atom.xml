<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-04-29T03:21:51Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6025884855508893027</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6025884855508893027/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6025884855508893027" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6025884855508893027" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html" rel="alternate" type="text/html"/>
    <title>x3   + y3 + z3 = 33  has a solution in Z. And its big!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Consider the following problem:<br/>
<br/>
Given k, a natural number, determine if there exists x,y,z INTEGERS such that x<sup>3</sup>+y<sup>3</sup>+z<sup>3</sup>=k.<br/>
<br/>
It is not obvious that this problem is decidable (I think it is but have not been able to find an exact statement to that affect; however, if it was not solvable, I would know that, hence it is solvable. If you know a ref give it in the comments.)<br/>
<br/>
<br/>
If k≡ 4,5 mod 9  then mod arguments easily show there is no solution. <a href="https://arxiv.org/pdf/1604.07746.pdf">Huisman</a> showed that if k≤ 1000, k≡1,2,3,6,7,8 mod 9 and max(|x|,|y|,|z|) ≤ 10<sup>15</sup> and k is NOT one of<br/>
<br/>
33, 42, 114, 165, 390, 579, 627, 633, 732, 795, 906, 921, 975<br/>
<br/>
then there was a solution. For those on the list it was unknown.<br/>
<br/>
Recently <a href="https://people.maths.bris.ac.uk/~maarb/papers/cubesv1.pdf">Booker</a> (not Cory Booker, the candidate for prez, but Andrew Booker who I assume is a math-computer science person and is not running for prez) showed that<br/>
<br/>
x<sup>3</sup> + y<sup>3</sup> + z<sup>3</sup> =33<br/>
<br/>
DOES have a solution in INTEGERS. It is<br/>
<br/>
x= 8,866,128,975,287,528<br/>
<br/>
y=-8,778,405,442,862,239<br/>
<br/>
z=-2,736,111,468,807,040<br/>
<br/>
<br/>
does that make us more likely or less likely to think that<br/>
<br/>
x<sup>3</sup> + y<sup>3</sup> + z<sup>3</sup> =42<br/>
<br/>
has a solution? How about =114, etc, the others on the list?<br/>
<br/>
Rather than say what I think is true (I have no idea) here is what I HOPE is true: that the resolution of these problems leads to some mathematics of interest.<br/>
<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-04-29T02:26:00Z</updated>
    <published>2019-04-29T02:26:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-04-29T02:26:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11965</id>
    <link href="http://arxiv.org/abs/1904.11965" rel="alternate" type="text/html"/>
    <title>Performance of a Quantum Annealer for Ising Ground State Computations on Chimera Graphs</title>
    <feedworld_mtime>1556496000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Michael Juenger, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lobe:Elisabeth.html">Elisabeth Lobe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mutzel:Petra.html">Petra Mutzel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reinelt:Gerhard.html">Gerhard Reinelt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rendl:Franz.html">Franz Rendl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rinaldi:Giovanni.html">Giovanni Rinaldi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stollenwerk:Tobias.html">Tobias Stollenwerk</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11965">PDF</a><br/><b>Abstract: </b>Quantum annealing is getting increasing attention in combinatorial
optimization. The quantum processing unit by D-Wave is constructed to
approximately solve Ising models on so-called Chimera graphs. Ising models are
equivalent to quadratic unconstrained binary optimization (QUBO) problems and
maximum cut problems on the associated graphs. We have tailored branch-and-cut
as well as semidefinite programming algorithms for solving Ising models for
Chimera graphs to provable optimality and use the strength of these approaches
for comparing our solution values to those obtained on the current quantum
annealing machine D-Wave 2000Q. This allows for the assessment of the quality
of solutions produced by the D-Wave hardware. It has been a matter of
discussion in the literature how well the D-Wave hardware performs at its
native task, and our experiments shed some more light on this issue.
</p></div>
    </summary>
    <updated>2019-04-29T01:26:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11946</id>
    <link href="http://arxiv.org/abs/1904.11946" rel="alternate" type="text/html"/>
    <title>Retracting Graphs to Cycles</title>
    <feedworld_mtime>1556496000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haney:Samuel.html">Samuel Haney</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liaee:Mehraneh.html">Mehraneh Liaee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maggs:Bruce_M=.html">Bruce M. Maggs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panigrahi:Debmalya.html">Debmalya Panigrahi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rajaraman:Rajmohan.html">Rajmohan Rajaraman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sundaram:Ravi.html">Ravi Sundaram</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11946">PDF</a><br/><b>Abstract: </b>We initiate the algorithmic study of retracting a graph into a cycle in the
graph, which seeks a mapping of the graph vertices to the cycle vertices, so as
to minimize the maximum stretch of any edge, subject to the constraint that the
restriction of the mapping to the cycle is the identity map. This problem has
its roots in the rich theory of retraction of topological spaces, and has
strong ties to well-studied metric embedding problems such as minimum bandwidth
and 0-extension.
</p>
<p>Our first result is an O(min{k, sqrt{n}})-approximation for retracting any
graph on n nodes to a cycle with k nodes. We also show a surprising connection
to Sperner's Lemma that rules out the possibility of improving this result
using natural convex relaxations of the problem. Nevertheless, if the problem
is restricted to planar graphs, we show that we can overcome these integrality
gaps using an exact combinatorial algorithm, which is the technical centerpiece
of the paper. Building on our planar graph algorithm, we also obtain a
constant-factor approximation algorithm for retraction of points in the
Euclidean plane to a uniform cycle.
</p></div>
    </summary>
    <updated>2019-04-29T01:24:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11895</id>
    <link href="http://arxiv.org/abs/1904.11895" rel="alternate" type="text/html"/>
    <title>On analog quantum algorithms for the mixing of Markov chains</title>
    <feedworld_mtime>1556496000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakraborty:Shantanav.html">Shantanav Chakraborty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Luh:Kyle.html">Kyle Luh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roland:J=eacute=r=eacute=mie.html">Jérémie Roland</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11895">PDF</a><br/><b>Abstract: </b>The problem of sampling from the stationary distribution of a Markov chain
finds widespread applications in a variety of fields. The time required for a
Markov chain to converge to its stationary distribution is known as the
classical mixing time. In this article, we deal with analog quantum algorithms
for mixing. First, we provide an analog quantum algorithm that given a Markov
chain, allows us to sample from its stationary distribution in a time that
scales as the sum of the square root of the classical mixing time and the
square root of the classical hitting time. Our algorithm makes use of the
framework of interpolated quantum walks and relies on Hamiltonian evolution in
conjunction with von Neumann measurements. We also make novel inroads into a
different notion for quantum mixing: the problem of sampling from the limiting
distribution of quantum walks, defined in a time-averaged sense. In this
scenario, the quantum mixing time is defined as the time required to sample
from a distribution that is close to this limiting distribution. This notion of
quantum mixing has been explored for a handful of specific graphs and the
derived upper bound for this quantity has been faster than its classical
counterpart for some graphs while being slower for others. In this article,
using several results in random matrix theory, we prove an upper bound on the
quantum mixing time of Erd\"os-Renyi random graphs: graphs of $n$ nodes where
each edge exists with probability $p$ independently. For example for dense
random graphs, where $p$ is a constant, we show that the quantum mixing time is
$\mathcal{O}(n^{3/2 + o(1)})$. Consequently, this allows us to obtain an upper
bound on the quantum mixing time for \textit{almost all graphs}, i.e.\ the
fraction of graphs for which this bound holds, goes to one in the asymptotic
limit.
</p></div>
    </summary>
    <updated>2019-04-29T01:29:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11810</id>
    <link href="http://arxiv.org/abs/1904.11810" rel="alternate" type="text/html"/>
    <title>Improving the complexity of Parys' recursive algorithm</title>
    <feedworld_mtime>1556496000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lehtinen:Karoliina.html">Karoliina Lehtinen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schewe:Sven.html">Sven Schewe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wojtczak:Dominik.html">Dominik Wojtczak</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11810">PDF</a><br/><b>Abstract: </b>Parys has recently proposed a quasi-polynomial version of Zielonka's
recursive algorithm for solving parity games. In this brief note we suggest a
variation of his algorithm that improves the complexity to meet the
state-of-the-art complexity of broadly $2^{O((\log n)(\log c))}$, while
providing polynomial bounds when the number of colours is logarithmic.
</p></div>
    </summary>
    <updated>2019-04-29T01:30:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11807</id>
    <link href="http://arxiv.org/abs/1904.11807" rel="alternate" type="text/html"/>
    <title>Dynamic MCMC Sampling</title>
    <feedworld_mtime>1556496000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Weiming.html">Weiming Feng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Kun.html">Kun He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Xiaoming.html">Xiaoming Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yin:Yitong.html">Yitong Yin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11807">PDF</a><br/><b>Abstract: </b>The Markov chain Monte Carlo (MCMC) methods are the primary tools for
sampling from graphical models, e.g. Markov random fields (MRF). Traditional
MCMC sampling algorithms are focused on a classic static setting, where the
input is fixed. In this paper we study the problem of sampling from an MRF when
the graphical model itself is changing dynamically with time. The problem is
well motivated by the growing volume and velocity of data in today's
applications of the MCMC methods.
</p>
<p>For the two major MCMC approaches, respectively for the approximate and
perfect sampling, namely, the Gibbs sampling and the coupling from the past
(CFTP), we give dynamic versions for the respective MCMC sampling algorithms.
On MRF with $n$ variables and bounded maximum degrees, these dynamic sampling
algorithms can maintain approximate or perfect samples, while the MRF is
dynamically changing. Furthermore, our algorithms are efficient with
$\tilde{O}(n)$ space cost, and $\tilde{O}(\log^2n)$ incremental time cost upon
each local update to the input MRF, as long as certain decay conditions are
satisfied in each step by natural couplings of the corresponding single-site
chains. These decay conditions were well known in the literature of couplings
for rapid mixing of Markov chains, and now for the first time, are used to
imply efficient dynamic sampling algorithms. Consequently, we have efficient
dynamic sampling algorithms for the following models:
</p>
<p>(1) general MRF satisfying the Dobrushin-Shlosman condition (for approximate
sampling);
</p>
<p>(2) Ising model with temperature $\beta$ where $\mathrm{e}^{-2|\beta|}&gt;
1-\frac{2}{\Delta+1}$ (for both approximate and perfect samplings);
</p>
<p>(3) hardcore model with fugacity $\lambda&lt;\frac{2}{\Delta-2}$ (for both
approximate and perfect samplings);
</p>
<p>(4) proper $q$-coloring with: $q&gt;2\Delta$ (for approximate sampling); or
$q&gt;2\Delta^2+3\Delta$ (for perfect sampling).
</p></div>
    </summary>
    <updated>2019-04-29T01:28:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11777</id>
    <link href="http://arxiv.org/abs/1904.11777" rel="alternate" type="text/html"/>
    <title>Tight Bounds for Online Weighted Tree Augmentation</title>
    <feedworld_mtime>1556496000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Naor:Joseph.html">Joseph Naor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Umboh:Seeun_William.html">Seeun William Umboh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williamson:David_P=.html">David P. Williamson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11777">PDF</a><br/><b>Abstract: </b>The Weighted Tree Augmentation problem (WTAP) is a fundamental problem in
network design. In this paper, we consider this problem in the online setting.
We are given an $n$-vertex spanning tree $T$ and an additional set $L$ of edges
(called links) with costs. Then, terminal pairs arrive one-by-one and our task
is to maintain a low-cost subset of links $F$ such that every terminal pair
that has arrived so far is $2$-edge-connected in $T \cup F$. This online
problem was first studied by Gupta, Krishnaswamy and Ravi (SICOMP 2012) who
used it as a subroutine for the online survivable network design problem. They
gave a deterministic $O(\log^2 n)$-competitive algorithm and showed an
$\Omega(\log n)$ lower bound on the competitive ratio of randomized algorithms.
The case when $T$ is a path is also interesting: it is exactly the online
interval set cover problem, which also captures as a special case the parking
permit problem studied by Meyerson (FOCS 2005). The contribution of this paper
is to give tight results for online weighted tree and path augmentation
problems. The main result of this work is a deterministic $O(\log
n)$-competitive algorithm for online WTAP, which is tight up to constant
factors.
</p></div>
    </summary>
    <updated>2019-04-29T01:21:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11757</id>
    <link href="http://arxiv.org/abs/1904.11757" rel="alternate" type="text/html"/>
    <title>The Potential of Restarts for ProbSAT</title>
    <feedworld_mtime>1556496000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lorenz:Jan=Hendrik.html">Jan-Hendrik Lorenz</a>, Julian Nickerl <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11757">PDF</a><br/><b>Abstract: </b>This work analyses the potential of restarts for probSAT, a quite successful
algorithm for k-SAT, by estimating its runtime distributions on random 3-SAT
instances that are close to the phase transition. We estimate an optimal
restart time from empirical data, reaching a potential speedup factor of 1.39.
Calculating restart times from fitted probability distributions reduces this
factor to a maximum of 1.30. A spin-off result is that the Weibull distribution
approximates the runtime distribution for over 93% of the used instances well.
A machine learning pipeline is presented to compute a restart time for a
fixed-cutoff strategy to exploit this potential. The main components of the
pipeline are a random forest for determining the distribution type and a neural
network for the distribution's parameters. ProbSAT performs statistically
significantly better than Luby's restart strategy and the policy without
restarts when using the presented approach. The structure is particularly
advantageous on hard problems.
</p></div>
    </summary>
    <updated>2019-04-29T01:21:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11668</id>
    <link href="http://arxiv.org/abs/1904.11668" rel="alternate" type="text/html"/>
    <title>The minimum cost query problem on matroids with uncertainty areas</title>
    <feedworld_mtime>1556496000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Arturo I. Merino, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soto:Jos=eacute=_A=.html">José A. Soto</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11668">PDF</a><br/><b>Abstract: </b>We study the minimum weight basis problem on matroid when elements' weights
are uncertain. For each element we only know a set of possible values (an
uncertainty area) that contains its real weight. In some cases there exist
bases that are uniformly optimal, that is, they are minimum weight bases for
every possible weight function obeying the uncertainty areas. In other cases,
computing such a basis is not possible unless we perform some queries for the
exact value of some elements.
</p>
<p>Our main result is a polynomial time algorithm for the following problem.
Given a matroid with uncertainty areas and a query cost function on its
elements, find the set of elements of minimum total cost that we need to
simultaneously query such that, no matter their revelation, the resulting
instance admits a uniformly optimal base. We also provide combinatorial
characterizations of all uniformly optimal bases, when one exists; and of all
sets of queries that can be performed so that after revealing the corresponding
weights the resulting instance admits a uniformly optimal base.
</p></div>
    </summary>
    <updated>2019-04-29T01:26:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11606</id>
    <link href="http://arxiv.org/abs/1904.11606" rel="alternate" type="text/html"/>
    <title>Approximation Algorithms for Min-Distance Problems</title>
    <feedworld_mtime>1556496000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dalirrooyfard:Mina.html">Mina Dalirrooyfard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Virginia_Vassilevska.html">Virginia Vassilevska Williams</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vyas:Nikhil.html">Nikhil Vyas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wein:Nicole.html">Nicole Wein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Yinzhan.html">Yinzhan Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Yuancheng.html">Yuancheng Yu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11606">PDF</a><br/><b>Abstract: </b>We study fundamental graph parameters such as the Diameter and Radius in
directed graphs, when distances are measured using a somewhat unorthodox but
natural measure: the distance between $u$ and $v$ is the minimum of the
shortest path distances from $u$ to $v$ and from $v$ to $u$. The center node in
a graph under this measure can for instance represent the optimal location for
a hospital to ensure the fastest medical care for everyone, as one can either
go to the hospital, or a doctor can be sent to help.
</p>
<p>By computing All-Pairs Shortest Paths, all pairwise distances and thus the
parameters we study can be computed exactly in $\tilde{O}(mn)$ time for
directed graphs on $n$ vertices, $m$ edges and nonnegative edge weights.
Furthermore, this time bound is tight under the Strong Exponential Time
Hypothesis [Roditty-Vassilevska W. STOC 2013] so it is natural to study how
well these parameters can be approximated in $O(mn^{1-\epsilon})$ time for
constant $\epsilon&gt;0$. Abboud, Vassilevska Williams, and Wang [SODA 2016] gave
a polynomial factor approximation for Diameter and Radius, as well as a
constant factor approximation for both problems in the special case where the
graph is a DAG. We greatly improve upon these bounds by providing the first
constant factor approximations for Diameter, Radius and the related
Eccentricities problem in general graphs. Additionally, we provide a hierarchy
of algorithms for Diameter that gives a time/accuracy trade-off.
</p></div>
    </summary>
    <updated>2019-04-29T01:23:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11601</id>
    <link href="http://arxiv.org/abs/1904.11601" rel="alternate" type="text/html"/>
    <title>Tight Approximation Algorithms for Bichromatic Graph Diameter and Related Problems</title>
    <feedworld_mtime>1556496000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dalirrooyfard:Mina.html">Mina Dalirrooyfard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Virginia_Vassilevska.html">Virginia Vassilevska Williams</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vyas:Nikhil.html">Nikhil Vyas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wein:Nicole.html">Nicole Wein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11601">PDF</a><br/><b>Abstract: </b>Some of the most fundamental and well-studied graph parameters are the
Diameter (the largest shortest paths distance) and Radius (the smallest
distance for which a "center" node can reach all other nodes). The natural and
important $ST$-variant considers two subsets $S$ and $T$ of the vertex set and
lets the $ST$-diameter be the maximum distance between a node in $S$ and a node
in $T$, and the $ST$-radius be the minimum distance for a node of $S$ to reach
all nodes of $T$. The bichromatic variant is the special case in which $S$ and
$T$ partition the vertex set.
</p>
<p>In this paper we present a comprehensive study of the approximability of $ST$
and Bichromatic Diameter, Radius, and Eccentricities, and variants, in graphs
with and without directions and weights. We give the first nontrivial
approximation algorithms for most of these problems, including time/accuracy
trade-off upper and lower bounds. We show that nearly all of our obtained
bounds are tight under the Strong Exponential Time Hypothesis (SETH), or the
related Hitting Set Hypothesis.
</p>
<p>For instance, for Bichromatic Diameter in undirected weighted graphs with $m$
edges, we present an $\tilde{O}(m^{3/2})$ time $5/3$-approximation algorithm,
and show that under SETH, neither the running time, nor the approximation
factor can be significantly improved while keeping the other unchanged.
</p></div>
    </summary>
    <updated>2019-04-29T01:20:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-29T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/063</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/063" rel="alternate" type="text/html"/>
    <title>TR19-063 |  Efficient Black-Box Identity Testing for Free Group Algebra | 

	Abhranil Chatterjee, 

	Vikraman Arvind, 

	Partha Mukhopadhyay, 

	Rajit Datta</title>
    <summary>Hrubeš and Wigderson [HW14] initiated the study of
  noncommutative arithmetic circuits with division computing a
  noncommutative rational function in the free skew field, and
  raised the question of rational identity testing. It is now known
  that the problem can be solved in deterministic polynomial time in
  the white-box model for noncommutative formulas with
  inverses, and in randomized polynomial time in the black-box
  model [GGOW16, IQS18, DM18], where the running time is
  polynomial in the size of the formula. 

  The complexity of identity testing of noncommutative rational
  functions remains open in general (when the formula size
  is not polynomially bounded). We solve the problem for a natural
  special case. We consider polynomial expressions in the free group
  algebra $\mathbb{F}\langle X, X^{-1}\rangle$ where $X=\{x_1, x_2, \ldots, x_n\}$, a
  subclass of rational expressions of inversion height one. Our main
  results are the following.

1. Given a degree $d$ expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ as a black-box, we obtain a randomized $\text{poly}(n,d)$ algorithm to check
  whether $f$ is an identically zero expression or not. We obtain this
  by generalizing the Amitsur-Levitzki theorem [AL50] to
  $\mathbb{F}\langle X, X^{-1}\rangle$. This also yields a deterministic identity testing algorithm (and even
  an expression reconstruction algorithm) that is polynomial time in
  the sparsity of the input expression.

2. Given an expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ of degree at most
  $D$, and sparsity $s$, as black-box, we can check whether $f$ is
  identically zero or not in randomized $\text{poly}(n,\log s, \log D)$
  time.</summary>
    <updated>2019-04-28T15:46:25Z</updated>
    <published>2019-04-28T15:46:25Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-04-29T03:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11446</id>
    <link href="http://arxiv.org/abs/1904.11446" rel="alternate" type="text/html"/>
    <title>Quantum Walk Sampling by Growing Seed Sets</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Apers:Simon.html">Simon Apers</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11446">PDF</a><br/><b>Abstract: </b>This work describes a new algorithm for creating a superposition over the
edge set of a graph, encoding a quantum sample of the random walk stationary
distribution. The algorithm requires a number of quantum walk steps scaling as
$\widetilde{O}(m^{1/3} \delta^{-1/3})$, with $m$ the number of edges and
$\delta$ the random walk spectral gap. This improves on existing strategies by
initially growing a classical seed set in the graph, from which a quantum walk
is then run. The algorithm leads to a number of improvements: (i) it provides a
new bound on the setup cost of quantum walk search algorithms, (ii) it yields a
new algorithm for $st$-connectivity, and (iii) it allows to create a
superposition over the isomorphisms of an $n$-node graph in time
$\widetilde{O}(2^{n/3})$, surpassing the $\Omega(2^{n/2})$ barrier set by index
erasure.
</p></div>
    </summary>
    <updated>2019-04-28T23:24:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11440</id>
    <link href="http://arxiv.org/abs/1904.11440" rel="alternate" type="text/html"/>
    <title>Distributed Detection of Cliques in Dynamic Networks</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Matthias Bonne, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Censor=Hillel:Keren.html">Keren Censor-Hillel</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11440">PDF</a><br/><b>Abstract: </b>This paper provides an in-depth study of the fundamental problems of finding
small subgraphs in distributed dynamic networks. While some problems are
trivially easy to handle, such as detecting a triangle that emerges after an
edge insertion, we show that, perhaps somewhat surprisingly, other problems
exhibit a wide range of complexities in terms of the trade-offs between their
round and bandwidth complexities. In the case of triangles, which are only
affected by the topology of the immediate neighborhood, some end results are:
</p>
<p>\begin{itemize}
</p>
<p>\item The bandwidth complexity of $1$-round dynamic triangle detection or
listing is $\Theta(1)$.
</p>
<p>\item The bandwidth complexity of $1$-round dynamic triangle membership
listing is $\Theta(1)$ for node/edge deletions, $\Theta(n^{1/2})$ for edge
insertions, and $\Theta(n)$ for node insertions.
</p>
<p>\item The bandwidth complexity of $1$-round dynamic triangle membership
detection is $\Theta(1)$ for node/edge deletions, $O(\log n)$ for edge
insertions, and $\Theta(n)$ for node insertions.
</p>
<p>\end{itemize}
</p>
<p>Most of our upper and lower bounds are \emph{tight}. Additionally, we provide
almost always tight upper and lower bounds for larger cliques.
</p></div>
    </summary>
    <updated>2019-04-28T23:24:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11395</id>
    <link href="http://arxiv.org/abs/1904.11395" rel="alternate" type="text/html"/>
    <title>On the Complexity of Local Graph Transformations</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scheideler:Christian.html">Christian Scheideler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Setzer:Alexander.html">Alexander Setzer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11395">PDF</a><br/><b>Abstract: </b>We consider the problem of transforming a given graph $G_s$ into a desired
graph $G_t$ by applying a minimum number primitives from a particular set of
local graph transformation primitives. These primitives are local in the sense
that each node can apply them based on local knowledge and by affecting only
its $1$-neighborhood. Although the specific set of primitives we consider makes
it possible to transform any (weakly) connected graph into any other (weakly)
connected graph consisting of the same nodes, they cannot disconnect the graph
or introduce new nodes into the graph, making them ideal in the context of
supervised overlay network transformations. We prove that computing a minimum
sequence of primitive applications (even centralized) for arbitrary $G_s$ and
$G_t$ is NP-hard, which we conjecture to hold for any set of local graph
transformation primitives satisfying the aforementioned properties. On the
other hand, we show that this problem admits a polynomial time algorithm with a
constant approximation ratio.
</p></div>
    </summary>
    <updated>2019-04-28T23:20:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11323</id>
    <link href="http://arxiv.org/abs/1904.11323" rel="alternate" type="text/html"/>
    <title>Performance Prediction for Coarse-Grained Locking</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Vitaly Aksenov Dan Alistarh Petr Kuznetsov <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11323">PDF</a><br/><b>Abstract: </b>A standard design pattern found in many concurrent data structures, such as
hash tables or ordered containers, is an alternation of parallelizable sections
that incur no data conflicts and critical sections that must run sequentially
and are protected with locks. A lock can be viewed as a queue that arbitrates
the order in which the critical sections are executed, and a natural question
is whether we can use stochastic analysis to predict the resulting throughput.
As a preliminary evidence to the affirmative, we describe a simple model that
can be used to predict the throughput of coarse-grained lock-based algorithms.
We show that our model works well for CLH lock, and we expect it to work for
other popular lock designs such as TTAS, MCS, etc.
</p></div>
    </summary>
    <updated>2019-04-28T23:22:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11285</id>
    <link href="http://arxiv.org/abs/1904.11285" rel="alternate" type="text/html"/>
    <title>Detecting and Counting Small Patterns in Planar Graphs in Subexponential Parameterized Time</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nederlof:Jesper.html">Jesper Nederlof</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11285">PDF</a><br/><b>Abstract: </b>We present an algorithm that takes as input an $n$-vertex planar graph $G$
and a $k$-vertex pattern graph $P$, and computes the number of (induced) copies
of $P$ in $G$ in $2^{O(k/\log k)}n^{O(1)}$ time. If $P$ is a matching,
independent set, or connected bounded maximum degree graph, the runtime reduces
to $2^{\tilde{O}(\sqrt{k})}n^{O(1)}$.
</p>
<p>While our algorithm counts all copies of $P$, it also improves the fastest
algorithms that only detect copies of $P$. Before our work, no $2^{O(k/\log
k)}n^{O(1)}$ time algorithms for detecting unrestricted patterns $P$ were
known, and by a result of Bodlaender et al. [ICALP 2016] a $2^{o(k/\log
k)}n^{O(1)}$ time algorithm would violate the Exponential Time Hypothesis
(ETH). Furthermore, it was only known how to detect copies of a fixed connected
bounded maximum degree pattern $P$ in $2^{\tilde{O}(\sqrt{k})}n^{O(1)}$ time
probabilistically. For counting problems, it was a repeatedly asked open
question whether $2^{o(k)}n^{O(1)}$ time algorithms exist that count even
special patterns such as independent sets, matchings and paths in planar
graphs. The above results resolve this question in a strong sense by giving
algorithms for counting versions of problems with running times equal to the
ETH lower bounds for their decision versions.
</p>
<p>Generally speaking, our algorithm counts copies of $P$ in time proportional
to its number of non-isomorphic separations of order $\tilde{O}(\sqrt{k})$. The
algorithm introduces a new recursive approach to construct families of balanced
cycle separators in planar graphs that have limited overlap inspired by methods
from Fomin et al. [FOCS 2016], a new `efficient' inclusion-exclusion based
argument and uses methods from Bodlaender et al. [ICALP 2016].
</p></div>
    </summary>
    <updated>2019-04-28T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11263</id>
    <link href="http://arxiv.org/abs/1904.11263" rel="alternate" type="text/html"/>
    <title>Stochastic rounding and reduced-precision fixed-point arithmetic for solving neural ODEs</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Michael.html">Michael Hopkins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mikaitis:Mantas.html">Mantas Mikaitis</a>, Dave R. Lester, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Furber:Steve.html">Steve Furber</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11263">PDF</a><br/><b>Abstract: </b>Although double-precision floating-point arithmetic currently dominates
high-performance computing, there is increasing interest in smaller and simpler
arithmetic types. The main reasons are potential improvements in energy
efficiency and memory footprint and bandwidth. However, simply switching to
lower-precision types typically results in increased numerical errors. We
investigate approaches to improving the accuracy of lower-precision arithmetic
types, using examples in an important domain for numerical computation in
neuroscience: the solution of Ordinary Differential Equations (ODEs). The
Izhikevich neuron model is used to demonstrate that rounding has an important
role in producing accurate spike timings from explicit ODE solution algorithms.
In particular, stochastic rounding consistently results in smaller errors
compared to single-precision floatingpoint and fixed-point arithmetic with
round-tonearest across a range of neuron behaviours and ODE solvers. A
computationally much cheaper alternative is also investigated, inspired by the
concept of dither that is a widely understood mechanism for providing
resolution below the LSB in digital signal processing. These results will have
implications for the solution of ODEs in other subject areas, and should also
be directly relevant to the huge range of practical problems that are
represented by Partial Differential Equations (PDEs).
</p></div>
    </summary>
    <updated>2019-04-28T23:23:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11244</id>
    <link href="http://arxiv.org/abs/1904.11244" rel="alternate" type="text/html"/>
    <title>On adaptive algorithms for maximum matching</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Falko Hegerfeld, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kratsch:Stefan.html">Stefan Kratsch</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11244">PDF</a><br/><b>Abstract: </b>In the fundamental Maximum Matching problem the task is to find a maximum
cardinality set of pairwise disjoint edges in a given undirected graph. The
fastest algorithm for this problem, due to Micali and Vazirani, runs in time
$\mathcal{O}(\sqrt{n}m)$ and stands unbeaten since 1980. It is complemented by
faster, often linear-time, algorithms for various special graph classes.
Moreover, there are fast parameterized algorithms, e.g., time
$\mathcal{O}(km\log n)$ relative to tree-width $k$, which outperform
$\mathcal{O}(\sqrt{n}m)$ when the parameter is sufficiently small.
</p>
<p>We show that the Micali-Vazirani algorithm, and in fact any algorithm
following the phase framework of Hopcroft and Karp, is adaptive to beneficial
input structure. We exhibit several graph classes for which such algorithms run
in linear time $\mathcal{O}(n+m)$. More strongly, we show that they run in time
$\mathcal{O}(\sqrt{k}m)$ for graphs that are $k$ vertex deletions away from any
of several such classes, without explicitly computing an optimal or approximate
deletion set; before, most such bounds were at least $\Omega(km)$. Thus, any
phase-based matching algorithm with linear-time phases obliviously interpolates
between linear time for $k=\mathcal{O}(1)$ and the worst case of
$\mathcal{O}(\sqrt{n}m)$ when $k=\Theta(n)$. We complement our findings by
proving that the phase framework by itself still allows $\Omega(\sqrt{n})$
phases, and hence time $\Omega(\sqrt{n}m)$, even on paths, cographs, and
bipartite chain graphs.
</p></div>
    </summary>
    <updated>2019-04-28T23:23:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11229</id>
    <link href="http://arxiv.org/abs/1904.11229" rel="alternate" type="text/html"/>
    <title>Finding Hexahedrizations for Small Quadrangulations of the Sphere</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Verhetsel:Kilian.html">Kilian Verhetsel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pellerin:Jeanne.html">Jeanne Pellerin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Remacle:Jean=Fran=ccedil=ois.html">Jean-François Remacle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11229">PDF</a><br/><b>Abstract: </b>This paper tackles the challenging problem of constrained hexahedral meshing.
An algorithm is introduced to build combinatorial hexahedral meshes whose
boundary facets exactly match a given quadrangulation of the topological
sphere. This algorithm is the first practical solution to the problem. It is
able to compute small hexahedral meshes of quadrangulations for which the
previously known best solutions could only be built by hand or contained
thousands of hexahedra. These challenging quadrangulations include the
boundaries of transition templates that are critical for the success of general
hexahedral meshing algorithms.
</p>
<p>The algorithm proposed in this paper is dedicated to building combinatorial
hexahedral meshes of small quadrangulations and ignores the geometrical
problem. The key idea of the method is to exploit the equivalence between quad
flips in the boundary and the insertion of hexahedra glued to this boundary.
The tree of all sequences of flipping operations is explored, searching for a
path that transforms the input quadrangulation Q into a new quadrangulation for
which a hexahedral mesh is known. When a small hexahedral mesh exists, a
sequence transforming Q into the boundary of a cube is found; otherwise, a set
of pre-computed hexahedral meshes is used.
</p>
<p>A novel approach to deal with the large number of problem symmetries is
proposed. Combined with an efficient backtracking search, it allows small
shellable hexahedral meshes to be found for all even quadrangulations with up
to 20 quadrangles. All 54,943 such quadrangulations were meshed using no more
than 72 hexahedra. This algorithm is also used to find a construction to fill
arbitrary domains, thereby proving that any ball-shaped domain bounded by n
quadrangles can be meshed with no more than 78 n hexahedra. This very
significantly lowers the previous upper bound of 5396 n.
</p></div>
    </summary>
    <updated>2019-04-28T23:25:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11134</id>
    <link href="http://arxiv.org/abs/1904.11134" rel="alternate" type="text/html"/>
    <title>Tell Me What I Need to Know: Succinctly Summarizing Data with Itemsets</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mampaey:Michael.html">Michael Mampaey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tatti:Nikolaj.html">Nikolaj Tatti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vreeken:Jilles.html">Jilles Vreeken</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11134">PDF</a><br/><b>Abstract: </b>Data analysis is an inherently iterative process. That is, what we know about
the data greatly determines our expectations, and hence, what result we would
find the most interesting. With this in mind, we introduce a well-founded
approach for succinctly summarizing data with a collection of itemsets; using a
probabilistic maximum entropy model, we iteratively find the most interesting
itemset, and in turn update our model of the data accordingly. As we only
include itemsets that are surprising with regard to the current model, the
summary is guaranteed to be both descriptive and non-redundant. The algorithm
that we present can either mine the top-$k$ most interesting itemsets, or use
the Bayesian Information Criterion to automatically identify the model
containing only the itemsets most important for describing the data. Or, in
other words, it will `tell you what you need to know'. Experiments on synthetic
and benchmark data show that the discovered summaries are succinct, and
correctly identify the key patterns in the data. The models they form attain
high likelihoods, and inspection shows that they summarize the data well with
increasingly specific, yet non-redundant itemsets.
</p></div>
    </summary>
    <updated>2019-04-28T23:22:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11037</id>
    <link href="http://arxiv.org/abs/1904.11037" rel="alternate" type="text/html"/>
    <title>Counting the Number of Crossings in Geometric Graphs</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Duque:Frank.html">Frank Duque</a>, Ruy Fabila-Monroy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hern=aacute=ndez=V=eacute=lez:C=eacute=sar.html">César Hernández-Vélez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hidalgo=Toscano:Carlos.html">Carlos Hidalgo-Toscano</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11037">PDF</a><br/><b>Abstract: </b>A geometric graph is a graph whose vertices are points in general position in
the plane and its edges are straight line segments joining these points. In
this paper we give an $O(n^2 \log n)$ algorithm to compute the number of pairs
of edges that cross in a geometric graph on $n$ points. For layered, and convex
geometric graphs the algorithm takes $O(n^2)$ time.
</p></div>
    </summary>
    <updated>2019-04-28T23:25:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.11026</id>
    <link href="http://arxiv.org/abs/1904.11026" rel="alternate" type="text/html"/>
    <title>Efficient Nearest-Neighbor Query and Clustering of Planar Curves</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aronov:Boris.html">Boris Aronov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Omrit.html">Omrit Filtser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Horton:Michael.html">Michael Horton</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Katz:Matthew_J=.html">Matthew J. Katz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sheikhan:Khadijeh.html">Khadijeh Sheikhan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.11026">PDF</a><br/><b>Abstract: </b>We study two fundamental problems dealing with curves in the plane, namely,
the nearest-neighbor problem and the center problem. Let $\mathcal{C}$ be a set
of $n$ polygonal curves, each of size $m$. In the nearest-neighbor problem, the
goal is to construct a compact data structure over $\mathcal{C}$, such that,
given a query curve $Q$, one can efficiently find the curve in $\mathcal{C}$
closest to $Q$. In the center problem, the goal is to find a curve $Q$, such
that the maximum distance between $Q$ and the curves in $\mathcal{C}$ is
minimized. We use the well-known discrete Frechet distance function, both
under~$L_\infty$ and under $L_2$, to measure the distance between two curves.
</p>
<p>For the nearest-neighbor problem, despite discouraging previous results, we
identify two important cases for which it is possible to obtain practical
bounds, even when $m$ and $n$ are large. In these cases, either $Q$ is a line
segment or $\mathcal{C}$ consists of line segments, and the bounds on the size
of the data structure and query time are nearly linear in the size of the input
and query curve, respectively. The returned answer is either exact under
$L_\infty$, or approximated to within a factor of $1+\varepsilon$ under~$L_2$.
We also consider the variants in which the location of the input curves is only
fixed up to translation, and obtain similar bounds, under $L_\infty$.
</p>
<p>As for the center problem, we study the case where the center is a line
segment, i.e., we seek the line segment that represents the given set as well
as possible. We present near-linear time exact algorithms under $L_\infty$,
even when the location of the input curves is only fixed up to translation.
Under $L_2$, we present a roughly $O(n^2m^3)$-time exact algorithm.
</p></div>
    </summary>
    <updated>2019-04-28T23:25:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1904.10454</id>
    <link href="http://arxiv.org/abs/1904.10454" rel="alternate" type="text/html"/>
    <title>Normalizers and permutational isomorphisms in simply-exponential time</title>
    <feedworld_mtime>1556409600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiebking:Daniel.html">Daniel Wiebking</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1904.10454">PDF</a><br/><b>Abstract: </b>We show that normalizers and permutational isomorphisms of permutation groups
given by generating sets can be computed in time simply exponential in the
degree of the groups. The result is obtained by exploiting canonical forms for
permutation groups (up to permutational isomorphism).
</p></div>
    </summary>
    <updated>2019-04-28T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-04-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/</id>
    <link href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/" rel="alternate" type="text/html"/>
    <title>New York Area Theory Day (Spring 2019)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">May 10, 2019 Columbia University http://www.cs.columbia.edu/theory/s19-tday.html The New York Area Theory Day, co-organized by Columbia, IBM, and NYU, is a semi-annual conference aiming to bring together researchers in the New York Metropolitan area. It usually features a few hour long talks on recent advances in theoretical computer science. The speakers this time are Sepehr Assadi, … <a class="more-link" href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/">Continue reading <span class="screen-reader-text">New York Area Theory Day (Spring 2019)</span></a></div>
    </summary>
    <updated>2019-04-26T22:05:01Z</updated>
    <published>2019-04-26T22:05:01Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-04-29T03:21:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4238</id>
    <link href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 2: Constructing Pseudorandom Sets</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Today we will see how to use the analysis of the multiplicative weights algorithm in order to construct pseudorandom sets. The method will yield constructions that are optimal in terms of the size of the pseudorandom set, but not very … <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 Today we will see how to use the analysis of the multiplicative weights algorithm in order to construct pseudorandom sets. </p>
<p>
The method will yield constructions that are optimal in terms of the size of the pseudorandom set, but not very efficient, although there is at least one case (getting an “almost pairwise independent” pseudorandom generator) in which the method does something that I am not sure how to replicate with other techniques. </p>
<p>
Mostly, the point of this post is to illustrate a concept that will reoccur in more interesting contexts: that we can use an online optimization algorithm in order to construct a combinatorial object satisfying certain desired properties. The idea is to run a game between a “builder” against an “inspector,” in which the inspector runs the online optimization algorithm with the goal of finding a violated property in what the builder is building, and the builder plays the role of the adversary selecting the cost functions, with the advantage that it gets to build a piece of the construction after seeing what property the “inspector” is looking for. By the regret analysis of the online optimization problem, if the builder did well at each round against the inspector, then it will do well also against the “offline optimum” that looks for a violated property after seeing the whole construction. For example, the construction of graph sparsifiers by Allen-Zhu, Liao and Orecchia can be cast in this framework.</p>
<p>
(In some other applications, it will be the “builder” that runs the algorithm and the “inspector” who plays the role of the adversary. This will be the case of the Frieze-Kannan weak regularity lemma and of the Impagliazzo hard-core lemma. In those cases we capitalize on the fact that we know that there is a very good offline optimum, and we keep going for as long as the adversary is able to find violated properties in what the builder is constructing. After a sufficiently large number of rounds, the regret experienced by the algorithm would exceed the general regret bound, so the process must terminate in a small number of rounds. I have been told that this is just the “dual view” of what I described in the previous paragraph.)</p>
<p>
But, back the pseudorandom sets: if <img alt="{{\cal C} = \{ C_1,\ldots,C_N \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+C%7D+%3D+%5C%7B+C_1%2C%5Cldots%2CC_N+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal C} = \{ C_1,\ldots,C_N \}}"/> is a collection of boolean functions <img alt="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_i+%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}"/>, for example the functions computed by circuits of a certain type and a certain size, then a multiset <img alt="{S\subseteq \{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S\subseteq \{ 0,1 \}^n}"/> is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-pseudorandom for <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/> if, for every <img alt="{C_i \in \cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_i+%5Cin+%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_i \in \cal C}"/>, we have </p>
<p align="center"><img alt="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC_i%28s%29+%3D+1+%5D+%7C+%5Cleq+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon "/></p>
<p> That is, sampling uniformly from <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, which we can do with <img alt="{\log_2 |S|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log_2 |S|}"/> random bits, is as good as sampling uniformly from <img alt="{\{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n}"/>, which requires <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits, as far as the functions in <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/> are concerned.</p>
<p>
It is easy to use Chernoff bounds and union bounds to argue that there is such a set of size <img alt="{O(N/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N/\epsilon^2)}"/>, so that we can sample from it using only <img alt="{\log N + 2\log \frac 1 \epsilon + O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+N+%2B+2%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log N + 2\log \frac 1 \epsilon + O(1)}"/> random bits.</p>
<p>
We will prove this result (while also providing an “algorithm” for the construction) using multiplicative weights.</p>
<p>
<span id="more-4238"/></p>
<p>
First of all, possibly by changing <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> to <img alt="{2N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2N}"/>, we may assume that for every function <img alt="{C \in {\cal C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%5Cin+%7B%5Ccal+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C \in {\cal C}}"/> the function <img alt="{1-C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-C}"/> is also in <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/>. This simplifies things a bit because then the pseudorandom condition is equivalent to just</p>
<p/><p align="center"><img alt="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+C%5Cin+%7B%5Ccal+C%7D+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC%28s%29+%3D+1+%5D+%5Cgeq+-+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon "/></p>
<p>
We will make up an “experts” setup in which there is an expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> for each function <img alt="{C_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_i}"/>. Thus, the algorithm, at each step, comes up with a probability distribution <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> over the functions, which we can think of as a “probabilistic function.” At time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, the adversary chooses a string <img alt="{s_t \in \{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t+%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t \in \{ 0,1 \}^n}"/> and defines the cost function </p>
<p align="center"><img alt="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+%5Csum_%7Bi%3D1%7D%5EN+x%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) "/></p>
<p> where the adversary chooses an <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> such that <img alt="{f_t(x_t) \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x_t) \geq 0}"/>. At this point, the reader should try, without reading ahead, to establish: </p>
<ol>
<li> That such a choice of <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> is always possible;
</li><li> That the cost function is of the form <img alt="{f_t(x) = \langle \ell_t , x\rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Clangle+%5Cell_t+%2C+x%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x) = \langle \ell_t , x\rangle}"/>, where the loss vector <img alt="{\ell_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t}"/> satisfies <img alt="{|\ell_t (i) | \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\ell_t (i) | \leq 1}"/>, so that the regret after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps is <img alt="{\leq 2 \sqrt{T \ln N}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+2+%5Csqrt%7BT+%5Cln+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\leq 2 \sqrt{T \ln N}}"/>;
</li><li> That the sequence <img alt="{s_1,\ldots,s_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_1%2C%5Cldots%2Cs_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_1,\ldots,s_T}"/> of choices by the adversary determines a <img alt="{2\sqrt {\frac {\ln N}{T}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Csqrt+%7B%5Cfrac+%7B%5Cln+N%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\sqrt {\frac {\ln N}{T}}}"/>-pseudorandom multiset for <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/>, and, in particular, we get an <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-pseudorandom multiset of cardinality <img alt="{4 \frac {\ln N}{\epsilon^2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4+%5Cfrac+%7B%5Cln+N%7D%7B%5Cepsilon%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{4 \frac {\ln N}{\epsilon^2}}"/>
</li></ol>
<p> For the first point, note that for a random <img alt="{s \sim \{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s \sim \{ 0,1 \}^n}"/> we have </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7Bs%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s%29+%5Cright%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 "/></p>
<p> so there is an <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> such that </p>
<p align="center"><img alt="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 "/></p>
<p> For the second point we just have to inspect the definition, and for the last point we have, by construction </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 "/></p>
<p> so the regret bound is </p>
<p align="center"><img alt="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cgeq+-+%7B%5Crm+Regret%7D_T+%5Cgeq+-+2+%5Csqrt%7BT%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} "/></p>
<p> which, after dividing by <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>, is </p>
<p align="center"><img alt="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i+%3A+%5C+%5C+%5C+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} "/></p>
<p align="center"><img alt="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i%3A+%5C+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+%5CPr_%7Bs%5Cin+%5C%7B+s_1%2C%5Cldots%2Cs_T+%5C%7D+%7D+%5BC_i+%28s%29+%3D+1+%5D+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} "/></p>
<p> Consider now the application of constructing a small-support distribution over <img alt="{\{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n}"/> that is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-almost-pairwise-independent, meaning that if <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> is a random string sampled according to this distribution, then, for every <img alt="{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i,j}"/>, the marginal <img alt="{(s_i,s_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28s_i%2Cs_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(s_i,s_j)}"/> is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-close to the uniform distribution over <img alt="{\{ 0,1 \}^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^2}"/> in total variation distance. This is the same thing as asking for a small-support distribution that is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-pseudorandom for all functions <img alt="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}"/> that depend on only two input variables. There are only <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> such functions, so the above construction gives us a pseudorandom distribution that is uniform over a set of size <img alt="{O(\epsilon^{-2} \ln n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+%5Cln+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} \ln n)}"/>, meaning that the distribution can be sampled using <img alt="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n+%2B+2+%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}"/> random bits. Furthermore the algorithm can be implemented to run in time <img alt="{n^{O(1)} / \epsilon^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BO%281%29%7D+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{O(1)} / \epsilon^2}"/>. The only tricky step is how to find the string <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> at each step. For a string <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>, the loss <img alt="{f (x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf+%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f (x_t)}"/> obtained by choosing <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> as the “reference string” is a polynomial of degree 2 in the bits of <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>, so we can find a no-worse-than-average <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> using the method of conditional expectations. I am not sure if there is a more standard way of doing this construction, perhaps one in which the bit <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> of the <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>-th string in the sample space can be generated in time <img alt="{(\log n)^{O(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\log n)^{O(1)}}"/>. The standard approach is to combine a small-bias generator with a linear family of pairwise independent hash functions, but even using Ta-Shma’s construction of small-bias generators we would not get the correct dependency on <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>. This framework can “derandomize Chernoff bounds” in other settings as well, such as randomized rounding of packing and covering integer linear programs, and it is basically the same thing as the method of “pessimistic estimators” described in the Motwani-Raghavan book on randomized algorithms. </p>
<p/></div>
    </content>
    <updated>2019-04-26T01:00:06Z</updated>
    <published>2019-04-26T01:00:06Z</published>
    <category term="theory"/>
    <category term="multiplicative weights"/>
    <category term="online optimization"/>
    <category term="Pseudorandomness"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-04-29T03:20:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=351</id>
    <link href="https://tcsplus.wordpress.com/2019/04/25/tcs-talk-wednesday-may-1st-chris-peikert-university-of-michigan/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 1st — Chris Peikert, University of Michigan</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Chris Peikert from University of Michigan will speak about “Noninteractive Zero Knowledge for NP from Learning With Errors” (abstract below). Please make sure you reserve a spot for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong><a href="http://web.eecs.umich.edu/~cpeikert/">Chris Peikert</a></strong> from University of Michigan will speak about “<em>Noninteractive Zero Knowledge for NP from Learning With Errors</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We finally close the long-standing problem of constructing a noninteractive zero-knowledge (NIZK) proof system for any NP language with security based on the Learning With Errors (LWE) problem, and thereby on worst-case lattice problems. Our proof system instantiates a framework developed in a series of recent works for soundly applying the Fiat—Shamir transform using a hash function family that is <em>correlation intractable</em> for a suitable class of relations. Previously, such hash families were based either on “exotic” assumptions (e.g., indistinguishability obfuscation or optimal hardness of ad-hoc LWE variants) or, more recently, on the existence of circularly secure fully homomorphic encryption. However, none of these assumptions are known to be implied by LWE or worst-case hardness.</p>
<p>Our main technical contribution is a hash family that is correlation intractable for arbitrary size-<img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" title="S"/> circuits, for any polynomially bounded <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" title="S"/>, based on LWE (with small polynomial approximation factors). Our construction can be instantiated in two possible “modes,” yielding a NIZK that is either computationally sound and statistically zero knowledge in the common random string model, or vice-versa in the common reference string model.</p>
<p>(This is joint work with Sina Shiehian. Paper: <a href="https://eprint.iacr.org/2019/158" rel="noopener" target="_blank">https://eprint.iacr.org/2019/158</a>)</p></blockquote></div>
    </content>
    <updated>2019-04-25T20:01:09Z</updated>
    <published>2019-04-25T20:01:09Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-04-29T03:21:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1084</id>
    <link href="http://corner.mimuw.edu.pl/?p=1084" rel="alternate" type="text/html"/>
    <title>Call for Participation: HALG 2019 (Highlights of Algorithms)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">------------------------------------------------------------------- 4rd Highlights of Algorithms conference (HALG 2019) Copenhagen, June 14-16, 2019 http://highlightsofalgorithms.org/ The Highlights of Algorithms conference is a forum for presenting the highlights of recent developments in algorithms and for discussing potential further advances in this area. The … <a href="http://corner.mimuw.edu.pl/?p=1084">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>-------------------------------------------------------------------<br/>
4rd Highlights of Algorithms conference (HALG 2019)<br/>
Copenhagen, June 14-16, 2019<br/>
<a href="http://highlightsofalgorithms.org/" rel="noopener noreferrer" target="_blank">http://highlightsofalgorithms.org/</a></p>
<p>The Highlights of Algorithms conference is a forum for presenting the<br/>
highlights of recent developments in algorithms and for discussing<br/>
potential further advances in this area. The conference will provide a<br/>
broad picture of the latest research in algorithms through a series of<br/>
invited talks, as well as the possibility for all researchers and<br/>
students to present their recent results through a series of short<br/>
talks and poster presentations. Attending the Highlights of Algorithms<br/>
conference will also be an opportunity for networking and meeting<br/>
leading researchers in algorithms.<br/>
-------------------------------------------------------------------</p>
<p>PROGRAM</p>
<p>The conference will begin on Friday, June 14, at 9:00 and end on<br/>
Sunday, June 16, at 18:00. A detailed schedule and a list of all<br/>
accepted short contributions can be found at:<br/>
<a href="http://2018.highlightsofalgorithms.org/programme" rel="noopener noreferrer" target="_blank">2018.highlightsofalgorithms.org/programme</a>.</p>
<p>-------------------------------------------------------------------</p>
<p>REGISTRATION</p>
<p>Please register on our webpage<br/>
     <a href="http://highlightsofalgorithms.org/registration" rel="noopener noreferrer" target="_blank">http://highlightsofalgorithms.org/registration</a><br/>
We have done our best to keep registration fees at a minimum:</p>
<p>Early registration (by April 29, 2019)<br/>
- academic rate (incl. postdocs): 160€<br/>
- student rate: 115€</p>
<p>Regular registration will be 50€ more expensive.</p>
<p>The organizers strongly recommend that you book your hotel as soon as possible.</p>
<p>-------------------------------------------------------------------</p>
<p>CONFERENCE VENUE</p>
<p>The conference will take place at the H.C. Ørsted Institute of the<br/>
University of Copenhagen.<br/>
The address is: Universitetsparken 5, DK-2100 Copenhagen.</p>
<p>-------------------------------------------------------------------</p>
<p>INVITED SPEAKERS</p>
<p>Survey speakers:<br/>
Monika Henzinger (University of Vienna)<br/>
Thomas Vidick (California Institute of Technology)<br/>
Laszlo Vegh (London School of Economics)<br/>
James Lee (University of Washington)<br/>
Timothy Chan (University of Illinois at Urbana-Champaign)<br/>
Sergei Vassilvitskii (Google, New York)</p>
<p>Invited talks:<br/>
Martin Grohe (RWTH Aachen University)<br/>
Josh Alman (MIT)<br/>
Nima Anari (Stanford University)<br/>
Michal Koucký (Charles University)<br/>
Naveen Garg (IIT Delhi)<br/>
Vera Traub (University of Bonn)<br/>
Rico Zenklusen (ETH Zurich)<br/>
Shayan Oveis Gharan (University of Washington)<br/>
Greg Bodwin (MIT)<br/>
Cliff Stein (Columbia University)<br/>
Sungjin Im (University of California at Merced)<br/>
C. Seshadhriy (University of California, Santa Cruz)<br/>
Shay Moran (Technion)<br/>
Bundit Laekhanukit (Shanghai University of Finance and Economics)<br/>
Sebastien Bubeck (Microsoft Research, Redmond)<br/>
Sushant Sachdeva (University of Toronto)<br/>
Kunal Talwar (Google Brain)<br/>
Moses Charikar (Stanford University)<br/>
Shuichi Hirahara (University of Tokyo)</p>
<p>------------------------------------------------------------------</p></div>
    </content>
    <updated>2019-04-25T12:36:21Z</updated>
    <published>2019-04-25T12:36:21Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2019-04-28T23:29:05Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8078353386966881451</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8078353386966881451/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8078353386966881451" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8078353386966881451" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html" rel="alternate" type="text/html"/>
    <title>Geo-Centric Complexity</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">An interesting discussion during Dagstuhl last month about the US-centric view of theory. Bad enough that all talks and papers in an international venue are in English but we also have<br/>
<ul>
<li><a href="https://en.wiktionary.org/wiki/Manhattan_distance">Manhattan Distance</a>. How are foreigners supposed to know about the structure of streets in New York? What's wrong with grid distance?</li>
<li><a href="https://en.wikipedia.org/wiki/Las_Vegas_algorithm">Las Vegas Algorithms</a>. I found this one a little unfair, after all Monte Carlo algorithms came first. Still today might not Macau algorithms make sense?</li>
<li><a href="https://en.wikipedia.org/wiki/Arthur%E2%80%93Merlin_protocol">Arthur-Merlin Games</a>. A British reference by a Hungarian living in the US (László Babai who also coined Las Vegas algorithms). Still the Chinese might not know the fables. Glad the Europeans don't remember the <a href="https://blog.computationalcomplexity.org/2017/04/alice-and-bob-and-pat-and-vanna.html">Pat and Vanna</a> terminology I used in my first STOC talk. </li>
<li>Alice and Bob. The famous pair of cryptographers but how generically American can you get. Why not Amare and Bhati?</li>
</ul>
<div>
I have two minds here. We shouldn't alienate or confuse those who didn't grow up in an Anglo-American culture. On the other hand, I hate to have to try and make all terminology culturally neutral, you'd just end up with technical and ugly names, like P and NP.</div></div>
    </content>
    <updated>2019-04-25T12:12:00Z</updated>
    <published>2019-04-25T12:12:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-04-29T02:26:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17389</id>
    <link href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/" rel="alternate" type="text/html"/>
    <title>Are Natural Mathematical Problems Bad Problems?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">One unique aspect of the conference “Visions in Mathematics Towards 2000” (see the previous post) was that there were several discussion sessions where speakers and other participants presented some thoughts about mathematics (or some specific areas), discussed and argued.  In … <a href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One unique aspect of the conference “Visions in Mathematics Towards 2000” (see the <a href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/">previous post</a>) was that there were several discussion sessions where speakers and other participants presented some thoughts about mathematics (or some specific areas), discussed and argued.  In the lectures themselves you could also see a large amount of audience participation and discussions which was very nice.</p>
<p>Let me draw your attention to  one question raised and discussed in one of the discussion sessions.</p>
<h3><a href="https://youtu.be/Fme_r-nE4CI?t=1400">3.4 Discussion on Geometry with introduction by M. Gromov</a></h3>
<p/>
<p>Now, lets skip a lot of interesting staff and move <a href="https://youtu.be/Fme_r-nE4CI?t=1400">to minute 23:20</a> where Noga Alon asked Misha Gromov to elaborate a statement from his <a href="https://youtu.be/gd6EB2Zk6OE">opening lecture of the conference</a> that  the densest packing problem in <img alt="R^3" class="latex" src="https://s0.wp.com/latex.php?latex=R%5E3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R^3"/> is not interesting.  In what follows Misha Gromov passionately argued that natural problems are bad problems (or are even stupid questions), and a lovely discussion emerged (in 25:00 Yuval Neeman commented about cosmology in response to Connes’s earlier remarks but then around 27:00 Vitali asked Misha to name some bad problems in geometry and the discussion resumed.) Misha made several lovely provocative further comments: he rejected the claim that this is a matter of taste, and argued that people make conjectures when they absolutely have no right to do so.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png"><img alt="" class="alignnone size-full wp-image-17390" height="361" src="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png?w=640&amp;h=361" width="640"/></a></p>
<p><strong><span style="color: #ff0000;"> Misha argues passionately that natural problems are stupid problems</span></strong></p>
<p>Actually one problem that Misha mentioned in his lecture as interesting (see also Gromov’s proceedings paper <a href="https://www.ihes.fr/~gromov/wp-content/uploads/2018/08/SpacesandQuestions.pdf">Spaces and questions),</a> and that was raised both by him and by me is to prove an exponential upper bound for the number of simplicial 3-spheres with n facets. I remember that we talked about it in the conference and Misha was certain that the problem could be solved for shellable spheres while I was confident that the case of shellable spheres would be as hard as the general case.  He was right! This goes back to works of physicists Durhuus and Jonsson see this paper <a href="https://arxiv.org/abs/0902.0436">On locally constructible spheres and balls</a> by Bruno Benedetti and  Günter M. Ziegler.</p>
<h5>(Disclaimer: I asked quite a few questions that were both unnatural and stupid and made several conjectures when I had no right to do so.)</h5></div>
    </content>
    <updated>2019-04-25T09:42:26Z</updated>
    <published>2019-04-25T09:42:26Z</published>
    <category term="Combinatorics"/>
    <category term="Conferences"/>
    <category term="Open discussion"/>
    <category term="What is Mathematics"/>
    <category term="Misha Gromov"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-04-29T03:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4236</id>
    <link href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 1: Multiplicative Weights</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The multiplicative weights or hedge algorithm is the most well known and most frequently rediscovered algorithm in online optimization. The problem it solves is usually described in the following language: we want to design an algorithm that makes the best … <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 The <em>multiplicative weights</em> or <em>hedge</em> algorithm is the most well known and most frequently rediscovered algorithm in online optimization. </p>
<p>
The problem it solves is usually described in the following language: we want to design an algorithm that makes the best possible use of the advice coming from <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> self-described experts. At each time step <img alt="{t=1,2,\ldots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C2%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1,2,\ldots}"/>, the algorithm has to decide with what probability to follow the advice of each of the experts, that is, the algorithm has to come up with a probability distribution <img alt="{x_t = (x_t(1),\ldots,x_t(n))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%3D+%28x_t%281%29%2C%5Cldots%2Cx_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t = (x_t(1),\ldots,x_t(n))}"/> where <img alt="{x_t (i) \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t (i) \geq 0}"/> and <img alt="{\sum_{i=1}^n x_t(i)=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{i=1}^n x_t(i)=1}"/>. After the algorithm makes this choice, it is revealed that following the advice of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> leads to loss <img alt="{\ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t (i)}"/>, so that the expected loss of the algorithm at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is <img alt="{\sum_{i=1}^n x_t(i) \ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{i=1}^n x_t(i) \ell_t (i)}"/>. A loss can be negative, in which case its absolute value can be interpreted as a profit.</p>
<p>
After <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps, the algorithm “regrets” that it did not just always follow the advice of the expert that, with hindsight, was the best one, so that the regret of the algorithm after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps is </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t%28i%29+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bi%3D1%2C%5Cldots%2Cn%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) "/></p>
<p>
This corresponds to the instantiation of the framework we described in the previous post to the special case in which the set of feasible solutions <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is the set <img alt="{\Delta \subseteq {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Delta \subseteq {\mathbb R}^n}"/> of probability distributions over the sample space <img alt="{\{ 1,\ldots,n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 1,\ldots,n\}}"/> and in which the loss functions <img alt="{f_t (x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t (x)}"/> are linear functions of the form <img alt="{f_t (x) = \sum_i x(i) \ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+x%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t (x) = \sum_i x(i) \ell_t (i)}"/>. In order to bound the regret, we also have to bound the “magnitude” of the loss functions, so in the following we will assume that for all <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> and all <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> we have <img alt="{| \ell_t (i) | \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{| \ell_t (i) | \leq 1}"/>, and otherwise we can scale everything by a known upper bound on <img alt="{\max_{t,i} |\ell_t |}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bt%2Ci%7D+%7C%5Cell_t+%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\max_{t,i} |\ell_t |}"/>.</p>
<p>
We now describe the algorithm.</p>
<p>
The algorithm maintains at each step <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> a vector of <em>weights</em> <img alt="{w_t = (w_t(1),\ldots,w_t(n))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_t+%3D+%28w_t%281%29%2C%5Cldots%2Cw_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_t = (w_t(1),\ldots,w_t(n))}"/> which is initialized as <img alt="{w_1 := (1,\ldots,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_1+%3A%3D+%281%2C%5Cldots%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_1 := (1,\ldots,1)}"/>. The algorithm performs the following operations at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>: </p>
<ul>
<li> <img alt="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_t+%28i%29+%3A%3D+w_%7Bt-1%7D+%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_%7Bt-1%7D+%28i%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}"/>
</li><li> <img alt="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%3A%3D+%5Cdisplaystyle+%5Cfrac+%7Bw_t+%28i%29+%7D%7B%5Csum_%7Bj%3D1%7D%5En+w_t%28j%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}"/>
</li></ul>
<p>
That is, the weight of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is <img alt="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-%5Cepsilon+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}"/>, and the probability <img alt="{x_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t(i)}"/> of following the advice of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is proportional to the weight. The parameter <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon&gt;0}"/> is hardwired into the algorithm and we will optimize it later. Note that the algorithm gives higher weight to experts that produced small losses (or negative losses of large absolute value) in the past, and thus puts higher probability on such experts.</p>
<p>
We will prove the following bound.</p>
<blockquote><p><b>Theorem 1</b> <em> Assuming that for all <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> and <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> we have <img alt="{| \ell_t(i) | \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{| \ell_t(i) | \leq 1}"/>, for every <img alt="{0 &lt; \epsilon &lt; 1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%3C+%5Cepsilon+%3C+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 &lt; \epsilon &lt; 1/2}"/>, after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps the multiplicative weight algorithm experiences a regret that is always bounded as </em></p><em>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell%5E2+_t+%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+%5Cleq+%5Cepsilon+T+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} "/></p>
<p> In particular, if <img alt="{T &gt; 4 \ln n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3E+4+%5Cln+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T &gt; 4 \ln n}"/>, by setting <img alt="{\epsilon = \sqrt{\frac{\ln n}{T}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+%5Csqrt%7B%5Cfrac%7B%5Cln+n%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon = \sqrt{\frac{\ln n}{T}}}"/> we achieve a regret bound </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
<span id="more-4236"/></p>
<p>
We will start by giving a short proof of the above theorem. </p>
<p>
For each time step <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, define the quantity</p>
<p/><p align="center"><img alt="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_t+%3A%3D+%5Csum_%7Bi%3D1%7D%5En+w_t%28i%29+%5C+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . "/></p>
<p> We want to prove that, roughly speaking, the only way for an adversary to make the algorithm incur a large loss is to produce a sequence of loss functions such that <em>even the best expert incurs a large loss</em>. The proof will work by showing that if the algorithm incurs a large loss after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps, then <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small, and that if <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small, then even the best expert incurs a large loss.</p>
<p>
Let us define </p>
<p align="center"><img alt="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L%5E%2A+%3D+%5Cmin_%7Bi+%3D+1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) "/></p>
<p> to be the loss of the best expert. Then we have</p>
<blockquote><p><b>Lemma 2 (If <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small, then <img alt="{L^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L^*}"/> is large)</b> <em> </em></p><em>
<p align="center"><img alt="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cgeq+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
<em>Proof:</em>  Let <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> be an index such that <img alt="{L^* = \sum_{t=1}^T \ell_t (j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A+%3D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L^* = \sum_{t=1}^T \ell_t (j)}"/>. Then we have </p>
<p align="center"><img alt="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%7D+%5Cgeq+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29%7D+%3D+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} "/></p>
<p> <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<blockquote><p><b>Lemma 3 (If the loss of the algorithm is large then <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small)</b> <em> </em></p><em>
<p align="center"><img alt="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cleq+n+%5Cprod_%7Bt%3D1%7D%5En+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t+%2C+%5Cell%5E2_t+%5Crangle%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) "/></p>
</em><p><em> where <img alt="{\ell_t^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t^2}"/> is the vector whose <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>-th coordinate is <img alt="{\left( \ell_t (i)\right)^2 }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%28+%5Cell_t+%28i%29%5Cright%29%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left( \ell_t (i)\right)^2 }"/> </em></p></blockquote>
<p/><p>
<em>Proof:</em>  Since we know that <img alt="{W_1 = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_1+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_1 = n}"/>, it is enough to prove that, for every <img alt="{t=1,\ldots, T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C%5Cldots%2C+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1,\ldots, T}"/>, we have <a name="eq.lemma.two"/></p><a name="eq.lemma.two">
<p align="center"><img alt="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7Bt%2B1%7D+%5Cleq+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t%2C+%5Cell_t%5E2+%5Crangle+%29+%5Ccdot+W_t++%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)"/></p>
</a><p><a name="eq.lemma.two"/> And we see that </p>
<p align="center"><img alt="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7BW_%7Bt%2B1%7D%7D%7BW_t%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_%7Bt%2B1%7D%28i%29%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} "/></p>
<p align="center"><img alt="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t+%28i%29+%7D+%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} "/></p>
<p align="center"><img alt="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } "/></p>
<p align="center"><img alt="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+%28+1+-+%5Cepsilon+%5Cell_t+%28i%29+%2B+%5Cepsilon%5E2+%5Cell_t%5E2%28i%29+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) "/></p>
<p align="center"><img alt="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+1+-+%5Cepsilon+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle "/></p>
<p> where we used the definitions of our quantities and the fact that <img alt="{e^{-z} \leq 1-z+z^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-z%7D+%5Cleq+1-z%2Bz%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{-z} \leq 1-z+z^2}"/> for <img alt="{|z| \leq 1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|z| \leq 1/2}"/>. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
Using the fact that <img alt="{1-z \leq e^{-z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-z+%5Cleq+e%5E%7B-z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-z \leq e^{-z}}"/> for all <img alt="{|z| \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|z| \leq 1}"/>, the above lemmas can be restated as </p>
<p align="center"><img alt="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cleq+%5Cln+n+-+%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET+%5Cepsilon+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+%2B+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+x_t%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cgeq+-+%5Cepsilon+L%5E%2A+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* "/></p>
<p> which together imply </p>
<p align="center"><img alt="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+-+L%5E%2A+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell%5E2_t+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle "/></p>
<p> as desired.</p>
<p>
Personally, I find all of the above very unsatisfactory, because both the algorithm and the analysis, but especially the analysis, seem to come out of nowhere. In fact, I never felt that I actually understood this analysis until I saw it presented as a special case of the <em>Follow The Regularized Leader</em> framework that we will discuss in a future post. (We will actually prove a slightly weaker bound, but with a much more satisfying proof.)</p>
<p>
Here is, however, a story of how a statistical physicist might have invented the algorithm and might have come up with the analysis. Let’s call the loss caused by expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> after <img alt="{t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t-1}"/> steps the <em>energy</em> of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>: </p>
<p align="center"><img alt="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_t%28i%29+%3D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) "/></p>
<p> Note that we have defined it in such a way that the algorithm knows <img alt="{E_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_t(i)}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. Our offline optimum is the energy of the lowest energy expert at time <img alt="{T+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T+1}"/>, that, is, the energy of the <em>ground state</em> at time <img alt="{T+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T+1}"/>. When we have a collection of numbers <img alt="{E_t(1),\ldots, E_t(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_t%281%29%2C%5Cldots%2C+E_t%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_t(1),\ldots, E_t(n)}"/>, a nice lower bound to their minimum is </p>
<p align="center"><img alt="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_i+E_t%28i%29+%5Cgeq+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } "/></p>
<p> which is true for every <img alt="{\epsilon &gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt;0}"/>. The right-hand side above is the <em>free energy</em> at temperature <img alt="{\frac 1 \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac 1 \epsilon}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. This seems like the kind of expression that we could use to bound the offline optimum, so let’s give it a name </p>
<p align="center"><img alt="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_t+%3A%3D+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } "/></p>
<p> In terms of coming up with an algorithm, all that we have got to work with at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> are the losses of the experts at times <img alt="{1,\ldots,t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2C%5Cldots%2Ct-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1,\ldots,t-1}"/>. If the adversary chooses to make one of the experts consistently much better than the others, it is clear that, in order to get any reasonable regret bound, the algorithm will have to put much of the probability mass in most of the steps on that expert. This suggests that the <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> should put higher probability on experts that have done well in the first <img alt="{t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t-1}"/> steps, that is <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> should put higher probability on “lower-energy” experts. When we have a system in which, at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, state <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> has energy <img alt="{E_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_t(i)}"/>, a standard distribution that puts higher probability on lower energy states is the <em>Gibbs distribution</em> at temperature <img alt="{1/\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/\epsilon}"/>, defined as </p>
<p align="center"><img alt="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7B+e%5E%7B-%5Cepsilon+E_t+%28i%29%7D+%7D%7B%5Csum_j+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } "/></p>
<p> where the denominator above is also called the <em>partition function</em> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> </p>
<p align="center"><img alt="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Z_t+%3A%3D+%5Csum_%7Bj%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } "/></p>
<p> So far we have “rediscovered” our multiplicative weights algorithm, and the quantity <img alt="{W_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_t}"/> that we had in our analysis gets interpreted as the partition function <img alt="{Z_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z_t}"/>. The fact that <img alt="{\Phi_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi_{T+1}}"/> bounds the offline optimum suggests that we should use <img alt="{\Phi_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi_t}"/> as a potential function, and aim for an analysis involving a telescoping sum. Indeed some manipulations (the same as in the short proof above, but which are now more mechanical) give that the loss of the algorithm at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is </p>
<p align="center"><img alt="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7Bt%2B1%7D+-+%5CPhi_%7Bt%7D+%2B+%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle "/></p>
<p> which telescopes to give </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7BT%2B1%7D+-+%5CPhi_1+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle "/></p>
<p> Recalling that </p>
<p align="center"><img alt="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_1+%3D+-+%5Cfrac+1+%7B%5Cepsilon%7D+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_%7BT%2B1%7D+%5Cleq+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) "/></p>
<p> we have again </p>
<p align="center"><img alt="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+%5Cright%29+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle "/></p>
<p> As mentioned above, we will give a better story when we get to the <em>Follow The Regularized Leader</em> framework. In the next post, we will discuss complexity-theory consequences of the result we just proved. </p></div>
    </content>
    <updated>2019-04-25T06:44:54Z</updated>
    <published>2019-04-25T06:44:54Z</published>
    <category term="theory"/>
    <category term="multiplicative weights"/>
    <category term="online optimization"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-04-29T03:20:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15798</id>
    <link href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/" rel="alternate" type="text/html"/>
    <title>Why Check A Proof?</title>
    <summary>Why check another’s proof? [Russell and Whitehead ] Bertrand Russell and Alfred Whitehead were not primarily trying to mechanize mathematics in writing their famous book. They wanted to assure precision and certainty in proofs while minimizing the axioms and rules they rest on. They cared more about checking proofs than generating theorems. By the way: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Why check another’s proof?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/russellwhitehead1900s/" rel="attachment wp-att-15809"><img alt="" class="alignright size-medium wp-image-15809" height="203" src="https://rjlipton.files.wordpress.com/2019/04/russellwhitehead1900s.png?w=300&amp;h=203" width="300"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[Russell and Whitehead ]</font></td>
</tr>
</tbody>
</table>
<p>
Bertrand Russell and Alfred Whitehead were not primarily trying to mechanize mathematics in writing their famous book. They wanted to assure precision and certainty in proofs while minimizing the axioms and rules they rest on. They cared more about checking proofs than generating theorems. By the way: They are listed in the order Whitehead and Russell on the book. See <a href="https://thonyc.wordpress.com/2016/05/19/bertrand-russell-did-not-write-principia-mathematica/">this</a> for a discussion about the importance of the order.</p>
<p><a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/unknown-120/" rel="attachment wp-att-15804"><img alt="" class="aligncenter size-full wp-image-15804" src="https://rjlipton.files.wordpress.com/2019/04/unknown-1.jpeg?w=600"/></a></p>
<p>
Today Ken and I thought we would add a few more thoughts on why proofs get checked.</p>
<p>
We discussed those who <em>claim</em> proofs in our previous <a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/">post</a>. Once a proof is claimed, it needs people to check it. This is not as fraught as the <a href="https://en.wikipedia.org/wiki/Replication_crisis">replication crisis</a> in other sciences where “proof” is a statement of statistical significance whose most intensive check needs repeating the experiment. </p>
<p>
If you do a Google search on “why check proofs” you get lots of hits on using automated proof checkers. Coming on eleven decades after the publication of Russell and Whitehead’s three-volume <a href="https://en.wikipedia.org/wiki/Principia_Mathematica">opus</a> <em>Principia Mathematica</em>, these are still in their formative years. We <a href="https://rjlipton.wordpress.com/2013/07/14/surely-you-are-joking/">covered</a> a major system of this kind some years ago. </p>
<p>
We are personally more interested in what motivates us <em>humans</em> to check proofs. We believe that there are various factors that make it less or more likely to find a good human checker. So today we will try to list some of them. </p>
<p>
</p><p/><h2> Why Check A Proof? </h2><p/>
<p/><p>
One of the questions that was raised by some commenters to our recent post is: <i>Why should I check your proof?</i></p>
<p>
This is a critical question. If their is no reason to check your proof, then your result will not get checked. It is almost a tautology. We like this question and thought we could suggest several ways to increase the likelihood that one will check another person’s proof. </p>
<p>
So lets assume that Alice is claiming some new theorem <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> and we ponder whether Bob will spend time checking it.</p>
<p>
</p><p/><h3> Bob has to </h3><p/>
<p>This happens when Bob is required to check her proof. This can happen if Bob is a referee of her paper. It could also be when Bob is hired to do this task. It usually is a weak reason for making someone do the checking. In real life we think that it is unlikely to be a strong motivator.</p>
<p>
</p><p/><h3> Bob wants to </h3><p/>
<p>This happens when Bob feels that he will benefit from checking. The main type of situation here is: Alice’s theorem <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> uses some new method or trick. If Bob believes that this method can be used in his work, in his research, in his future papers, then he is strongly motivated.</p>
<p>
We are all very self-centered in our research. If we think we could in the future use your method we are likely to spent time and energy on your proof. Thus if Bob is convinced that Alice has a some new ideas, he is much more likely to spent the time checking her theorem. This means that Alice should—if possible–explain that her proof of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> uses something new. Proofs that are “just technical inductions” are very unlikely to get Bob to read them. In many areas some authors have stated things like: <i>The proof is a careful induction…</i> This is not a good idea. </p>
<p>
</p><p/><h3> Bob needs to </h3><p/>
<p>This happens when Bob has some “skin” in the game. A classic situation is when Bob has an earlier result that is affected by Alice’s new theorem. If <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is stronger than Bob’s previous result, then he is motivated to check her theorem. Or if <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> shows that his earlier theorem is false, this is a very strong motivation. Or perhaps Alice has proven a lemma that enables Bob to push something through.</p>
<p>
</p><p/><h2> Skin in the Game </h2><p/>
<p/><p>
Often we have situations where you do have skin in the game. An old <a href="https://rjlipton.wordpress.com/2009/09/27/surprises-in-mathematics-and-theory/">example</a> that comes to mind is from group theory. The problem is a natural question about a class of groups: Let <img alt="{B(m,n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%28m%2Cn%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B(m,n)}"/> be the class of groups that are generated by <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> elements and all elements in the group satisfy, <img alt="{x^{n} = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Bn%7D+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^{n} = 1}"/>. Sergei Adian and Pyotr Novikov proved that <img alt="{B(m, n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%28m%2C+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B(m, n)}"/> is infinite for <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> odd, <img alt="{{n \ge 4381}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Bn+%5Cge+4381%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{n \ge 4381}}"/> by a long complex combinatorial proof in 1968. This is a famous result. </p>
<p>
Shortly after another group theorist, John Britton, claimed an alternative proof in 1970. Unfortunately, Adian later discovered that Britton’s proof was wrong. I do not have first-hand information, but I was told that Adian was motivated by wanting to have <i>the</i> proof. He worked hard until he discovered an unrepairable bug in Britton’s 300-page monograph. The proof was unsalvageable.</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/yau/" rel="attachment wp-att-15802"><img alt="" class="aligncenter size-medium wp-image-15802" height="300" src="https://rjlipton.files.wordpress.com/2019/04/yau.jpg?w=199&amp;h=300" width="199"/></a></p>
<p>
A much newer example is from a recent book by Shing-Tung Yau, <a href="https://yalebooks.yale.edu/book/9780300235906/shape-life">The Shape of a Life</a>. He is a famous geometry expert and has made many important contributions to many areas of mathematics. We will probably discuss his book in detail in the future, but for today it has a neat example of “skin in the game”. He writes about an enumeration problem of counting how many curves lie on a certain manifold—a century old problem. One group used a clever trick to get the number 	</p>
<p align="center"><img alt="\displaystyle  317,206,375. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++317%2C206%2C375.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  317,206,375. "/></p>
<p>However another group discovered via a different method that the count was 	</p>
<p align="center"><img alt="\displaystyle  2,682,549,425. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%2C682%2C549%2C425.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  2,682,549,425. "/></p>
<p>Somewhat a different count—not even close. Clearly, both sets of authors were heavily motivated to check their work. And within a month the larger count was found to be wrong and the first was correct.</p>
<p>
</p><p/><h2> A<del datetime="2019-04-24T13:55:30-04:00">n</del> <del datetime="2019-04-24T13:55:11-04:00">Un</del>resolved Claim </h2><p/>
<p/><p>
This is from the wonderful P vs NP <a href="https://www.win.tue.nl/~gwoegi/P-versus-NP.htm">pages</a> of Gerhard Woeginger. It was pointed out to us by the commenter <i>gentzen</i>. Quoting Woeginger’s page, including its use of “showed”:</p>
<blockquote><p><b> </b> <em> In February 2016, Mathias Hauptmann showed that P is not equal to NP. Hauptmann starts from the assumption that P equals <img alt="{\Sigma_{2}^{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Sigma_{2}^{p}}"/>, proves a new variant of the Union Theorem of McCreight and Meyer for <img alt="{\Sigma_{2}^{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Sigma_{2}^{p}}"/>, and eventually derives a contradiction. This implies P not equal to NP. </em>
</p></blockquote>
<p/><p>
Woeginger gives a link to Hauptmann’s <a href="http://arxiv.org/abs/1602.04781">paper</a>, “On Alternation and the Union Theorem,” and thanks two people who communicated this to him. </p>
<p>
The union <a href="https://people.csail.mit.edu/meyer/meyer-mccreight.pdf">theorem</a> of Albert Meyer and Edward McCreight is the classic theorem that shows how to encode many complexity classes into one. Hauptmann’s idea is not unreasonable. He makes an assumption that P=NP and tries to use it to improve the union theorem. This is a nice idea: Make a strong assumption and then try to improve a deep result. The hope is that this will lead to a contradiction. His abstract ends by saying, “Hence the assumption <img alt="{P = \Sigma_{2}^{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP+%3D+%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P = \Sigma_{2}^{p}}"/> cannot hold.” We do not know if this paper has received a thorough reading. <b>Update:</b> We have learned that a pair of experts reviewed the argument and found that part of it implied a contradiction to the deterministic time hierarchy theorem, while another part relativizes in a way that would yield a false statement under certain oracles.</p>
<p>
</p><p/><h2> A Resolved Claim </h2><p/>
<p/><p>
Hauptmann is a colleague of Norbert Blum at the University of Bonn. Two years ago, Blum claimed to prove P <img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/> NP by making technical improvements on a well-known circuit-based attack from the 1980s and 1990s. He has had a long track record of expertise and reliability in this area and his <a href="http://arxiv.org/trackback/1708.03486">paper</a> was read right away. </p>
<p>
The reading was helped by his paper being well-organized, straightforward, and relatively short—the crucial segment was under ten pages. The news broke while we were preparing a post on the August 2017 total solar eclipse in the US. In the 24–48 hours it took us to modify our <a href="https://rjlipton.wordpress.com/2017/08/17/on-the-edge-of-eclipses-and-pnp/">post</a>, we were already able to draw on several accounts by first-responder readers and check those accounts ourselves against the paper. </p>
<p>
The error was triangulated in an interesting way. It was first observed that if Blum’s attack could succeed by the means and premises stated, then it would extend to prove something else that is known not to be true. Once this was ascertained, a closer reading was able to zero in on the exact technical point of error. Blum soon acknowledged this and that the breach was unfixable. The attempt still combines circuit theory and graph theory in ways a student can benefit from learning about, and this furnished its own incentive to read it.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We appreciate the comments on the previous post and hope this adds some additional insights.</p>
<p>
[added update about Hauptmann’s paper]</p></font></font></div>
    </content>
    <updated>2019-04-24T14:32:25Z</updated>
    <published>2019-04-24T14:32:25Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="check proofs"/>
    <category term="conjecture"/>
    <category term="motivation"/>
    <category term="P&#x2260;NP"/>
    <category term="skin in game"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-04-29T03:20:52Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/04/23/euler-characteristics-nonmanifold</id>
    <link href="https://11011110.github.io/blog/2019/04/23/euler-characteristics-nonmanifold.html" rel="alternate" type="text/html"/>
    <title>Euler characteristics of non-manifold polycubes</title>
    <summary>From a block of cubes, remove two non-adjacent and non-opposite cubes. The resulting polycube has a boundary that is not a manifold: between the two removed cubes, there is an edge shared by four squares, but a two-dimensional manifold can only have two faces per edge. Nevertheless, we can compute its Euler characteristic as the number of vertices () minus the number of edges () plus the number of square faces (). , the same number we would expect for the Euler characteristic of a topological sphere! What does it mean?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>From a  block of cubes, remove two non-adjacent and non-opposite cubes. The resulting polycube has a boundary that is not a <a href="https://en.wikipedia.org/wiki/Manifold">manifold</a>: between the two removed cubes, there is an edge shared by four squares, but a two-dimensional manifold can only have two faces per edge. Nevertheless, we can compute its Euler characteristic as the number of vertices () minus the number of edges () plus the number of square faces (). , the same number we would expect for the Euler characteristic of a topological sphere! What does it mean?</p>

<p style="text-align: center;"><img alt="Removing two non-adjacent and non-opposite cubes from a 2x2 block of cubes" src="https://11011110.github.io/blog/assets/2019/nonmanifold-polycube.svg"/></p>

<p>Any finite union of cubes of the integer lattice (not even necessarily connected) has as its boundary a set of vertices, edges, and squares, with each edge incident to an even number of squares. We can define the Euler characteristic to be the number of vertices minus edges plus squares, in the usual way. But we can also compute it in a different, more intrinsic and topological, way. For any  in the range , define the “shrunken interior” of the polycube to be the set of points of the interior farther than  from the boundary, and define the “shrunken exterior” in the same way. Then the shrunken interior and shrunken exterior both have (possibly disconnected) 2-manifolds as boundaries. We can define their Euler characteristics in the standard way from any cell decomposition of these boundaries (it doesn’t matter which cell decomposition we choose). Then the Euler characteristic of the polycube is the average of the Euler characteristics of the shrunken interior and shrunken exterior!</p>

<p>In the case of the mutilated  block, the shrunken interior and shrunken exterior are both topological balls (ignoring the puncture at infinity as it doesn’t have a boundary), so the average of their Euler characteristics is the Euler characteristic of a sphere, as we calculated.</p>

<p>There’s probably a simpler and more conceptual way of doing it, but here’s an explanation for why the Euler characteristic of the polycube boundary is the average of the Euler characteristics of the interior and exterior. Form a cell complex on the boundary of the interior and exterior, together, in the following way: expand each square of the polycube boundary to a cuboid with thickness 0.1, expand each edge into a cylinder with diameter 0.2 (big enough to enclose all the intersections of two expanded squares), and expand each vertex into a sphere with diameter 0.3 (big enough to enclose all the intersections of two cylinders but small enough that no two of these spheres touch). Remove the union of these expanded shapes from the space, and consider what’s left. It has the same topology as the union of the shrunken interior and exterior, and its boundary is now naturally divided up into cells: offset squares patches on the sides of each expanded square face, cylindrical patches on each expanded edge, and spherical patches on each expanded vertex, with curves where two patches meet.</p>

<p>Let’s calculate the Euler characteristic of this cell complex. Each square of the polycube leads to two offset square patches, so the  squares contribute   to the Euler characteristic. Each edge  of the polycube might be adjacent to two or four squares; call this number . Then the cylinder around  includes  surface patches between pairs of squares and  curves connecting them to the square patches. The patches count  and the curves count  for a total contribution to the Euler characteristic of .</p>

<p>Finally, each vertex of the polycube becomes a sphere, subdivided by the patches and curves of the complex. These spheres also contain all the vertices of the complex. The Euler characteristic of a subdivided sphere would be , but the vertex spheres have some parts of their subdivision removed. Where each edge cylinder or expanded square comes into the sphere, a patch of surfaces is removed, and the curves between these removed patches are also removed. An edge  with degree  contributes to the removal of  curves and  patches (one for itself and  for each adjacent square). So if there are  vertices in the polycube, the  contribution from the Euler characteristics of the subdivided spheres is modified by subtracting  for each incident edge. The total modification at both endpoints of each edge is . The  that we calculated here is cancelled by the  on the cylinder for , and we are left with a total modification of  where  is the number of polycube edges.</p>

<p>Putting all the pieces of this calculation together, the complex we have constructed on the union of the shrunken interior and exterior has Euler characteristic . Therefore, the Euler characteristic of the polycube boundary itself, , equals the average of the characteristics of the interior and exterior. The same reasoning shows more generally that whenever you have a finite cell complex embedded into , dividing space up into chambers, the Euler characteristic of the complex equals half the sum of Euler characteristics of the manifolds bounding shrunken chambers.</p>

<p>Although Euler characteristics of 2-manifolds embedded without boundary in  are always even, this averaging method can produce non-manifold surfaces with odd Euler characteristic. For instance, consider mutilating the  block in a different way, by removing two opposite cubes. The interior and exterior of the resulting polycube are both connected, but the interior is a solid torus and the exterior is a ball. So the Euler characteristic of the polycube should be the average of the torus and sphere, . And if we actually calculate it we get .</p>

<p>As this example shows, it’s possible for a polycube to have different topologies of surface on the interior and exterior, and it’s also possible to have different numbers of surfaces: for instance, two cubes attached vertex-to-vertex produce two interior surfaces but only one exterior. For cell complexes in , there appears to be no restriction on which combinations of surfaces are possible. But for cell complexes in other spaces (other 3-manifolds than Euclidean space) it may be possible to embed 2-manifolds with odd Euler characteristic. When this happens, the number of odd chambers of a cell complex must always be even. For, the parity of the sum of the Euler characteristics of the chambers must be even, in order to be able to divide by two and get an integer as the Euler characteristic of the cell complex.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/101978143052398446">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-04-23T16:37:00Z</updated>
    <published>2019-04-23T16:37:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-04-23T23:43:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17325</id>
    <link href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/" rel="alternate" type="text/html"/>
    <title>An Invitation to a Conference: Visions in Mathematics towards 2000</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Let me invite you to a conference. The conference took place in 1999 but only recently the 57 videos of the lectures and the discussion sessions are publicly available. (I thank Vitali Milman for telling me about it.) One novel … <a href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Let me invite you to a conference. The conference took place in 1999 but only recently <a href="https://www.youtube.com/playlist?list=PLP0YToNcfAwLBd8yibTtjv3aHfcbT4GBA">the 57 videos of the lectures and the discussion sessions are publicly available.</a> (I thank Vitali Milman for telling me about it.) One novel idea of Vitali Milman was to hold discussion sessions and they were quite interesting. (But, I am biased, I like discussions.) I will invite you to one of the heated discussions in the next post. There were very many nice talks and very many nice visions. And it is fun to watch the videos and judge the ideas in the perspective of time.</p>
<p>The proceedings appeared as GAFA special volumes, but alas the articles are not electronically available even to GAFA’s subscribers. Let me encourage both Birkhauser and the contributors to make them more available. My talk was: <a href="https://youtu.be/Wjg1_QwjUos">An invitation to Tverberg’s theorem</a>, and my own contribution to the Proceedings <a href="http://www.ma.huji.ac.il/~kalai/VIS.pdf">Combinatorics with a Geometric Flavor </a>is probably the widest scope survey article I ever wrote.  At the end of each section I added a brief philosophical thought about mathematics and those are collected in the post “<a href="https://gilkalai.wordpress.com/2008/10/12/about-mathematics/">about mathematics</a>“.</p>
<p>Two more things: The conference (and few others organized by Vitali) was  in Tel Aviv with a few days at the dead see and this worked very nicely.  Vitali also organized in the mid 90s another very successful geometry conference unofficially celebrating Gromov’s 50th birthday with, among others, a very nice lecture by Gregory Perelman. If videos will become available I will be delighted to invite you to that conference as well. Update from Vitali: It was also the week of Jeff Cheeger’s 50th birthday which was also celebrated. Grisha Perelman gave an absolutely excellent talk  talk on works of Cheeger.  Lectures were not videotaped.</p>
<p/>
<p><span style="color: #ff0000;">Avi Wigderson’s lecture</span></p>
<p><span style="color: #ff0000;">Are so called “natural questions” good for mathematics. Specifically is Kepler’s questions about the densest packing of unit balls in 3-space interesting? Watch a discussion of Misha Gromov, Noga Alon, Laci Lovasz and others. (next post)</span></p>
<h2/>
<h2><a href="https://gilkalai.files.wordpress.com/2019/04/vis99-p1.png"><img alt="" class="alignnone size-full wp-image-17362" height="363" src="https://gilkalai.files.wordpress.com/2019/04/vis99-p1.png?w=640&amp;h=363" width="640"/></a></h2>
<p><span style="color: #ff0000;">We were all so much younger!  (And in that old millennium,  we were also all men <img alt="&#x1F626;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f626.png" style="height: 1em;"/> )</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/misha5.png"><img alt="" class="alignnone size-full wp-image-17366" height="614" src="https://gilkalai.files.wordpress.com/2019/04/misha5.png?w=640&amp;h=614" width="640"/></a></p>
<p><span style="color: #ff0000;">Misha Gromov argues passionately that natural problems are bad problems (see next post)</span></p>
<p>Pictures of most participants ad two slides are below</p>
<p><span id="more-17325"/><br/>
<a href="https://gilkalai.files.wordpress.com/2019/04/vim1.png"><img alt="VIM1.png" class="alignnone size-full wp-image-17372" src="https://gilkalai.files.wordpress.com/2019/04/vim1.png?w=640" style="font-size: 12px;"/></a><br/>
<a href="https://gilkalai.files.wordpress.com/2019/04/vim2.png"><img alt="VIM2.png" class="alignnone size-full wp-image-17373" src="https://gilkalai.files.wordpress.com/2019/04/vim2.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim3.png"><img alt="VIM3.png" class="alignnone size-full wp-image-17374" src="https://gilkalai.files.wordpress.com/2019/04/vim3.png?w=640"/></a></p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/vis.png"><img alt="" class="alignnone size-full wp-image-17387" height="373" src="https://gilkalai.files.wordpress.com/2019/04/vis.png?w=640&amp;h=373" width="640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim5.png"><img alt="VIM5.png" class="alignnone size-full wp-image-17376" src="https://gilkalai.files.wordpress.com/2019/04/vim5.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim6.png"><img alt="VIM6.png" class="alignnone size-full wp-image-17377" src="https://gilkalai.files.wordpress.com/2019/04/vim6.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim7.png"><img alt="VIM7.png" class="alignnone size-full wp-image-17378" src="https://gilkalai.files.wordpress.com/2019/04/vim7.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim4.png"><img alt="VIM4.png" class="alignnone size-full wp-image-17375" src="https://gilkalai.files.wordpress.com/2019/04/vim4.png?w=640"/></a></p></div>
    </content>
    <updated>2019-04-23T15:40:47Z</updated>
    <published>2019-04-23T15:40:47Z</published>
    <category term="Combinatorics"/>
    <category term="Conferences"/>
    <category term="What is Mathematics"/>
    <category term="Vitali Milman"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-04-29T03:20:48Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8139226761048070665</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8139226761048070665/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/quiz-show-scandalsadmissions.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8139226761048070665" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8139226761048070665" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/quiz-show-scandalsadmissions.html" rel="alternate" type="text/html"/>
    <title>Quiz Show Scandals/Admissions Scandal/Stormy Daniels/Beer names:being  a lawyer would drive me nuts!!!!!!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">0) Charles van Doren (see <a href="https://en.wikipedia.org/wiki/Charles_Van_Doren">here</a>) passed away recently. For those who don't know he he was (prob most of you) he was one of the contestants involved in RIGGED quiz shows in the 1950's.  While there was a Grand Jury Hearing about Quiz Shows being rigged, nobody went to jail since TV was new and it was not clear if rigging quiz shows was illegal. Laws were then passed to make them it illegal.<br/>
<br/>
So why are today's so-called reality shows legal? I ask non-rhetorically.<br/>
<br/>
(The person he beat in a rigged game show- Herb Stempel (see <a href="https://en.wikipedia.org/wiki/Herb_Stempel">here</a>) is still alive.)<br/>
<br/>
1) The college admissions scandal. I won't restate the details and how awful it is since you can get that elsewhere and I doubt I can add much to it.  One thing I've heard in the discussions about it is a question that is often posted rhetorically but I want to pose for real:<br/>
<br/>
There are people whose parents give X dollars to a school and they get admitted even though they are not qualified. Why is that legal?<br/>
<br/>
I ask that question without an ax to grind and without anger. Why is out-right bribery of this sort legal?<br/>
<br/>
Possibilities:<br/>
<br/>
a) Its transparent. So being honest about bribery makes it okay?<br/>
<br/>
b) My question said `even though they are not qualified' - what if they explicitly or implicitly said `having parents give money to our school is one of our qualifications'<br/>
<br/>
c) The money they give is used to fund scholarships for students who can't afford to go. This is an argument for why its not immoral, not why its not illegal.<br/>
<br/>
But here is my question: Really, what is the legal issue here? It still seems like bribery.<br/>
<br/>
2) Big Oil gives money to congressman Smith, who then votes against a carbon tax. This seems like outright bribery<br/>
<br/>
Caveat:<br/>
<br/>
a) If Congressman Smith is normally a anti-regulation then he could say correctly that he was given the money because they agree with his general philosophy, so it's  not bribery.<br/>
<br/>
b) If Congressman smith is normally pro-environment and has no problem with voting for taxes then perhaps it is bribery.<br/>
<br/>
3) John Edwards a while back and Donald Trump now are claiming (not quite) that the money used to pay off their mistress to be quiet is NOT a campaign contribution, but was to keep the affair from his wife. (I don't think Donald Trump has admitted the affair so its harder to know what his defense is). But lets take a less controversial example of `what is a campaign contribution'<br/>
<br/>
I throw a party for my wife's 50th birthday and I invite Beto O'Rourke and many voters and some Dem party big-wigs to the party. The party costs me $50,000.  While I claim it's for my wife's bday it really is for Beto to make connections to voters and others. So is that a campaign contribution?<br/>
<br/>
4) The creators of HUGE ASS BEER are suing GIANT ASS BEER for trademark infringement. I am not making this up- see <a href="https://thetakeout.com/huge-giant-ass-beer-lawsuit-new-orleans-1832989913">here</a><br/>
<br/>
---------------------------------------------------------<br/>
<br/>
All of these cases involve ill defined questions (e.g., `what is a bribe'). And the people arguing either side are not unbiased. The cases also illustrate why I prefer mathematics: nice clean questions that (for the most part) have answers. We may have our biases as to which way they go, but if it went the other way we would not sue in a court of law.</div>
    </content>
    <updated>2019-04-23T03:24:00Z</updated>
    <published>2019-04-23T03:24:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-04-29T02:26:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4233</id>
    <link href="https://lucatrevisan.wordpress.com/2019/04/22/the-more-things-change-the-more-they-stay-the-same/" rel="alternate" type="text/html"/>
    <title>The more things change the more they stay the same</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">From a 1981 (!!) New York Times Article titled “Changing San Francisco is foreseen as a haven for wealthy and childless”: A major reason for the exodus of the middle class from San Francisco, demographers say, is the high cost … <a href="https://lucatrevisan.wordpress.com/2019/04/22/the-more-things-change-the-more-they-stay-the-same/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>From a 1981 (!!) <a href="https://www.nytimes.com/1981/06/09/us/changing-san-francisco-is-foreseen-as-a-haven-for-wealthy-and-childless.html">New York Times Article</a> titled “Changing San Francisco is foreseen as a haven for wealthy and childless”:</p>
<blockquote><p>
A major reason for the exodus of the middle class from San Francisco, demographers say, is the high cost of housing, the highest in the mainland United States. Last month, the median cost of a dwelling in the San Francisco Standard Metropolitan Statistical Area was $129,000, according to the Federal Home Loan Bank Board in Washington, D.C. The comparable figure for New York, Newark and Jersey City was $90,400, and for Los Angeles, the second most expensive city, $118,400.</p>
<p>”This city dwarfs anything I’ve ever seen in terms of housing prices,” said Mr. Witte. Among factors contributing to high housing cost, according to Mr. Witte and others, is its relative scarcity, since the number of housing units has not grown significantly in a decade; the influx of Asians, whose first priority is usually to buy a home; the high incidence of adults with good incomes and no children, particularly homosexuals who pool their incomes to buy homes, and the desirability of San Francisco as a place to live.
</p></blockquote>
<p>$129,000 in 1981 dollars is $360,748 in 2019 dollars.</p></div>
    </content>
    <updated>2019-04-23T01:52:32Z</updated>
    <published>2019-04-23T01:52:32Z</published>
    <category term="history"/>
    <category term="San Francisco"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-04-29T03:20:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4230</id>
    <link href="https://lucatrevisan.wordpress.com/2019/04/22/online-optimization-post-0-definitions/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 0: Definitions</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Online convex optimization deals with the following setup: we want to design an algorithm that, at each discrete time step , comes up with a solution , where is a certain convex set of feasible solution. After the algorithm has … <a href="https://lucatrevisan.wordpress.com/2019/04/22/online-optimization-post-0-definitions/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 Online convex optimization deals with the following setup: we want to design an algorithm that, at each discrete time step <img alt="{t=1,2,\ldots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C2%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1,2,\ldots}"/>, comes up with a solution <img alt="{x_t \in K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%5Cin+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t \in K}"/>, where <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is a certain convex set of feasible solution. After the algorithm has selected its solution <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/>, a convex cost function <img alt="{f_t : K \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t : K \rightarrow {\mathbb R}}"/>, coming from a known restricted set of admissible cost functions <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/>, is revealed, and the algorithm pays the loss <img alt="{f_t (x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t (x_t)}"/>. </p>
<p>
Again, the algorithm has to come up with a solution <em>without knowing what cost functions it is supposed to be optimizing</em>. Furthermore, we will think of the sequence of cost functions <img alt="{f_1,f_2, \ldots,f_t,\ldots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2Cf_2%2C+%5Cldots%2Cf_t%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,f_2, \ldots,f_t,\ldots}"/> not as being fixed in advanced and unknown to the algorithm, but as being dynamically generated by an adversary, after seeing the solutions provided by the algorithm. (This resilience to adaptive adversaries will be important in most of the applications.)</p>
<p>
The <em>offline optimum</em> after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps is the total cost that the best possible fixed solution would have incurred when evaluated against the cost functions seen by the algorithm, that is, it is a solution to </p>
<p align="center"><img alt="\displaystyle  \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) "/></p>
<p>
The <em>regret</em> after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps is the difference between the loss suffered by the algorithm and the offline optimum, that is, </p>
<p/><p align="center"><img alt="\displaystyle  {\rm Regret}_T = \sum_{t=1}^T f_t (x_t) - \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_t%29+-+%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T = \sum_{t=1}^T f_t (x_t) - \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) "/></p>
<p>
The remarkable results that we will review give algorithms that achieve regret</p>
<p/><p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq O_{K, {\cal F}} (\sqrt T) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+O_%7BK%2C+%7B%5Ccal+F%7D%7D+%28%5Csqrt+T%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq O_{K, {\cal F}} (\sqrt T) "/></p>
<p> that is, for fixed <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> and <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/>, the regret-per-time-step goes to zero with the number of steps, as <img alt="{O\left( \frac 1 {\sqrt T} \right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%5Cleft%28+%5Cfrac+1+%7B%5Csqrt+T%7D+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O\left( \frac 1 {\sqrt T} \right)}"/>. It is intuitive that our bounds will have to depend on how big is the “diameter” of <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> and how large is the “magnitude” and “smoothness” of the functions <img alt="{f\in {\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\in {\cal F}}"/>, but depending on how we choose to formalize these quantities we will be led to define different algorithms. </p>
<p/></div>
    </content>
    <updated>2019-04-22T21:35:44Z</updated>
    <published>2019-04-22T21:35:44Z</published>
    <category term="theory"/>
    <category term="online optimization"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-04-29T03:20:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15778</id>
    <link href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/" rel="alternate" type="text/html"/>
    <title>P=NP Proofs</title>
    <summary>Advice to claimers The Claimers The Claimers are a gang on the hit AMC television series The Walking Dead. They are the main antagonists in the second half of the zombie-apocalypse show’s Season 4. According to Wikipedia’s description, they “live by the philosophy of ‘claiming’.” Today Ken and I discuss issues about ‘claiming’ and give […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Advice to claimers</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/claimers3/" rel="attachment wp-att-15790"><img alt="" class="alignright size-medium wp-image-15790" height="191" src="https://rjlipton.files.wordpress.com/2019/04/claimers3.png?w=300&amp;h=191" width="300"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://the-walking-dead-tvseries.fandom.com/wiki/The_Claimers">The Claimers</a></font></td>
</tr>
</tbody>
</table>
<p>
The Claimers are a gang on the hit AMC television <a href="https://en.wikipedia.org/wiki/The_Walking_Dead_(TV_series)">series</a> <em>The Walking Dead</em>. They are the main antagonists in the second half of the zombie-apocalypse show’s Season 4. According to Wikipedia’s <a href="https://en.wikipedia.org/wiki/The_Walking_Dead_(season_4)#The_Claimers">description</a>, they “live by the philosophy of ‘claiming’.”</p>
<p>
Today Ken and I discuss issues about ‘claiming’ and give advice on how to present your claims—or not.</p>
<p>
Yes, this post is about our own “claimers” in complexity theory. It is especially about those who claim to have a solution to P=NP. We will not give any names today. You know who you are.</p>
<p>
The TV Claimers meet a grisly end. We will not say any more about it. We want to be nice. But we would like to not keep seeing the same level of zombie claims raised again and again.</p>
<p>
<b>Please: Do not stop reading.</b> Yes we know that it is likely that no claimer really has such a proof. However, our suggestions apply to all of us when we have a non-trivial result. Especially a result that has been open, even if the result is not a major open problem. So please keep reading today.</p>
<p>
</p><p/><h2> So You Can Prove P=NP </h2><p/>
<p/><p>
This is a list of ideas for anyone who claims to have solved P=NP or some similar hard open problem in mathematics. There are already lots of suggestions online about what you should do, so this is just a list of additional thoughts. We hope they are helpful.</p>
<p>
</p><p/><h3> You are being pretty arrogant </h3><p/>
<p/><p>
In order to succeed in mathematics research one has to be a bit arrogant. It is quite difficult to prove new things without some swagger. However, proving or resolving P=NP requires a very non-humble attitude. I think many claimers have not thought how arrogant they are being. The P=NP problem is a huge open problem. Thousands and thousands of researchers have spent years thinking about it. Why do you, the claimer, think you see the light and we remain in the dark?</p>
<p>
It might be useful for the claimers to ponder: <i>Why did I succeed where all others have failed?</i> It might be useful to be a bit humble and at least think what did they see that we all missed? If they can say something like:</p>
<blockquote><p><b> </b> <em> The reason I succeeded in finding an algorithm for P=NP is that I noticed that <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\dots}"/> No one else seems to see that this insight is very powerful. It is very useful since it implies <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\dots}"/> </em>
</p></blockquote>
<p>
</p><p/><h3> Working alone </h3><p/>
<p/><p>
I think the vast majority of claimers of P=NP or other big results have almost always worked alone. This is okay, but the average number of authors these days of a theory paper is pretty large. So any paper that is sole authored, perhaps, leads the community think it is unusual—and is wrong. Another point is that being part of a team may help control the arrogance. It can also be invaluable in detecting errors. </p>
<p>
</p><p/><h3> Show them the money </h3><p/>
<p/><p>
There is an advantage in “proving” P=NP over other major problems. There is the Clay prize of a million dollars. I wonder if claimers could use the prize money in some interesting way. How about saying: If you read the my proof and repair it or make it more readable and it is correct, then you get something. A certain dollar amount. Or a percentage of the prize. Or—you get the idea.</p>
<p>
</p><p/><h3> The role of code </h3><p/>
<p/><p>
Many claimers have also supplied working code for their algorithm. That is they also supply a program that claims to solve some NP-complete problem. I have several thoughts about this. In some cases it seems that it could be possible to have code that works for small size problems, but not in the general case. This seems to be possible for the claims by some that they can solve the Traveling Salesman Problem, for example. Their algorithm could be correct for small instances.</p>
<p>
Mathematics is filled with surprises like: This effect works for all values of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> less than some bound. If the claimers give a program, our expectation based on experience is that it may work for small cases but will probably fail in general. </p>
<p>
The last point is that working code could actually be valuable. If the code can be used to solve SAT problems how about using your program to enter a SAT contest and win it. A win, or even a good showing, would help tremendously in convincing people to read the paper. Or use the code to break some known cryptosystem. That would also convince people that they need to read your paper.</p>
<p>
</p><p/><h2> Writing Your Paper </h2><p/>
<p/><p>
Okay we all dream about solving a major open problem. Or even a minor one. Here we give an outline of how to write up such a paper. </p>
<p/><h3> How to write up the proof </h3><p/>
<p>
I would suggest that you not have any statements about why P=NP is an important problem. None. No history of the problem. No literature survey is needed. None. You goal is to get an expert to read and believe the proof. They will just skip over the above. Also please no statements of how your algorithm that solves P=NP is going to change the world. Just give us the proof.</p>
<p>
</p><p/><h3> How to get them to read the proof </h3><p/>
<p/><p>
This is really hard. Hard. I have read a number of claimers’ papers. I try to be helpful. However, many of us do not have the time to look at such papers. Years ago, before Fermat’s Last Theorem was solved, a famous mathematician once made up a post-card that looked like this:</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/postcard/" rel="attachment wp-att-15784"><img alt="" class="aligncenter size-medium wp-image-15784" height="221" src="https://rjlipton.files.wordpress.com/2019/04/postcard.png?w=300&amp;h=221" width="300"/></a></p>
<p>
I think that we all have a mental version of this card. There are definitely ways to help induce someone to read a paper and its proof. Look at some recent top theory papers. Even the authors of these papers, often well known authors, work hard to motivate potential readers. The authors often do several things:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>They often sketch the proof.</em> By leaving out details they may help get a reader interested. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>They often explain the new trick—or tricks.</em> The goal here is to explain some new insight that is used in the proof. We are very self-oriented: If I see that your new trick could be useful in my research that is a huge motivator for me to understand the proof. People are very excited about a strong result, but they are even more excited about a new trick. Explain what is new in your proof. If there is nothing new, no new trick or method, then hmmmm<img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/></p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>They often first prove a weaker result.</em> That is, they show that their method can already make progress. If you could prove that the zeta function has all its nontrivial zeros on the critical line, that would be apocalyptic—the famous Riemann Hypothesis, of course. But if you could merely prove that there is no zero in some new region, then that would still be <em>wonderful</em>. And also probably more believable. If you could prove that there is no zero <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> with its real part <img alt="{0.99999}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.99999%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.99999}"/> that would be huge. If the proof of this is simpler, then use it to get readers excited about your full result. </p>
<p>
An observation related to the last point is: </p>
<blockquote><p><b> </b> <em> <i>Why do all claims of progress on P=NP give a polynomial time bound?</i> </em>
</p></blockquote>
<p>How about just getting a better bound of say <img alt="{2^{n/10}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%2F10%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{n/10}}"/> for the Traveling Salesman Problem? Or a better bound for factoring? Or a better bound for your favorite problem?</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>I hope these points help. One last pointer is to double-check dependencies. If your proof relies on a result by someone else, make sure the result really gives what you need. Terms may be defined differently from what you expect, or you may really need a feature of the proof rather than the mere statement. A mis-attributed result can become “undead.”</p></font></font></div>
    </content>
    <updated>2019-04-22T03:25:39Z</updated>
    <published>2019-04-22T03:25:39Z</published>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="claimed proofs"/>
    <category term="claims"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-04-29T03:20:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/062</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/062" rel="alternate" type="text/html"/>
    <title>TR19-062 |  Quantum Lower Bounds for Approximate Counting via Laurent Polynomials | 

	Scott Aaronson, 

	Robin Kothari, 

	William Kretschmer, 

	Justin Thaler</title>
    <summary>This paper proves new limitations on the power of quantum computers to solve approximate counting---that is, multiplicatively estimating the size of a nonempty set $S\subseteq [N]$.

Given only a membership oracle for $S$, it is well known that approximate counting takes $\Theta(\sqrt{N/|S|})$ quantum queries. But what if a quantum algorithm is also given "QSamples"---i.e., copies of the state $|S\rangle = \sum_{i\in S}|i\rangle$---or even the ability to apply reflections about $|S\rangle$? Our first main result is that, even then, the algorithm needs either $\Theta(\sqrt{N/|S|})$ queries or else $\Theta(\min\{|S|^{1/3},\sqrt{N/|S|}\})$ reflections or samples. We also give matching upper bounds.

We prove the lower bound using a novel generalization of the polynomial method of Beals et al. to Laurent polynomials, which can have negative exponents. We lower-bound Laurent polynomial degree using two methods: a new "explosion argument" and a new formulation of the dual polynomials method.

Our second main result rules out the possibility of a black-box Quantum Merlin-Arthur (or QMA) protocol for proving that a set is large. We show that, even if Arthur can make $T$ quantum queries to the set $S$, and also receives an $m$-qubit quantum witness from Merlin in support of $S$ being large, we have $Tm=\Omega(\min\{|S|,\sqrt{N/|S|}\})$. This resolves the open problem of giving an oracle separation between SBP and QMA.

Note that QMA is "stronger" than the queries+QSamples model in that Merlin's witness can be anything, rather than just the specific state $|S\rangle$, but also "weaker" in that Merlin's witness cannot be trusted. Intriguingly, Laurent polynomials also play a crucial role in our QMA lower bound, but in a completely different manner than in the queries+QSamples lower bound. This suggests that the "Laurent polynomial method" might be broadly useful in complexity theory.</summary>
    <updated>2019-04-21T13:57:29Z</updated>
    <published>2019-04-21T13:57:29Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-04-29T03:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/061</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/061" rel="alternate" type="text/html"/>
    <title>TR19-061 |  A Quantum Query Complexity Trichotomy for Regular Languages | 

	Daniel Grier, 

	Luke Schaeffer, 

	Scott Aaronson</title>
    <summary>We present a trichotomy theorem for the quantum query complexity of regular languages. Every regular language has quantum query complexity $\Theta(1)$, $\tilde{\Theta}(\sqrt n)$, or $\Theta(n)$. The extreme uniformity of regular languages prevents them from taking any other asymptotic complexity. This is in contrast to even the context-free languages, which we show can have query complexity $\Theta(n^c)$ for all computable $c \in [1/2,1]$. Our result implies an equivalent trichotomy for the approximate degree of regular languages, and a dichotomy---either $\Theta(1)$ or $\Theta(n)$---for sensitivity, block sensitivity, certificate complexity, deterministic query complexity, and randomized query complexity.

The heart of the classification theorem is an explicit quantum algorithm which decides membership in any star-free language in $\tilde{O}(\sqrt n)$ time. This well-studied family of the regular languages admits many interesting characterizations, for instance, as those languages expressible as sentences in first-order logic over the natural numbers with the less-than relation. Therefore, not only do the star-free languages capture functions such as OR, they can also express functions such as ``there exist a pair of 2's such that everything between them is a 0."  

Thus, we view the algorithm for star-free languages as a nontrivial generalization of Grover's algorithm which extends the quantum quadratic speedup to a much wider range of string-processing algorithms than was previously known.  We show a variety of applications---new quantum algorithms for dynamic constant-depth Boolean formulas, balanced parentheses nested constantly many levels deep, binary addition, a restricted word break problem, and path-discovery in narrow grids---all obtained as immediate consequences of our classification theorem.</summary>
    <updated>2019-04-21T13:56:47Z</updated>
    <published>2019-04-21T13:56:47Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-04-29T03:20:45Z</updated>
    </source>
  </entry>
</feed>
