<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-01-31T04:22:49Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10888</id>
    <link href="http://arxiv.org/abs/1901.10888" rel="alternate" type="text/html"/>
    <title>A common lines approach for ab-initio modeling of cyclically-symmetric molecules</title>
    <feedworld_mtime>1548892800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pragier:Gabi.html">Gabi Pragier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shkolnisky:Yoel.html">Yoel Shkolnisky</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10888">PDF</a><br/><b>Abstract: </b>One of the challenges in single particle reconstruction in cryo-electron
microscopy is to find a three-dimensional model of a molecule using its
two-dimensional noisy projection-images. In this paper, we propose a robust
"angular reconstitution" algorithm for molecules with $n$-fold cyclic symmetry,
that estimates the orientation parameters of the projections-images. Our
suggested method utilizes self common lines which induce identical lines within
the Fourier transform of each projection-image. We show that the location of
self common lines admits quite a few favorable geometrical constraints, thus
allowing to detect them even in a noisy setting. In addition, for molecules
with higher order rotational symmetry, our proposed method exploits the fact
that there exist numerous common lines between any two Fourier transformed
projection-images of such molecules, thus allowing to determine their relative
orientation even under high levels of noise. The efficacy of our proposed
method is demonstrated using numerical experiments conducted on real data.
</p></div>
    </summary>
    <updated>2019-01-31T02:36:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10848</id>
    <link href="http://arxiv.org/abs/1901.10848" rel="alternate" type="text/html"/>
    <title>Comparing Election Methods Where Each Voter Ranks Only Few Candidates</title>
    <feedworld_mtime>1548892800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bentert:Matthias.html">Matthias Bentert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Skowron:Piotr.html">Piotr Skowron</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10848">PDF</a><br/><b>Abstract: </b>Election rules are formal processes that aggregate voters preferences,
typically to select a single candidate, called the winner. Most of the election
rules studied in the literature require the voters to rank the candidates from
the most to the least preferred one. This method of eliciting preferences is
impractical when the number of candidates to be ranked is large. We ask how
well certain election rules (focusing on positional scoring rules and the
Minimax rule) can be approximated from partial preferences collected through
one of the following procedures: (i) randomized-we ask each voter to rank a
random subset of $\ell$ candidates, and (ii) deterministic-we ask each voter to
provide a ranking of her $\ell$ most preferred candidates (the $\ell$-truncated
ballot). We establish theoretical bounds on the approximation ratios and we
complement our theoretical analysis with computer simulations. We find that
mostly (apart from the cases when the preferences have no or very little
structure) it is better to use the randomized approach. While we obtain fairly
good approximation guarantees for the Borda rule already for $\ell = 2$, for
approximating the Minimax rule one needs to ask each voter to compare a larger
set of candidates in order to obtain good guarantees.
</p></div>
    </summary>
    <updated>2019-01-31T02:27:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10789</id>
    <link href="http://arxiv.org/abs/1901.10789" rel="alternate" type="text/html"/>
    <title>Optimal Minimal Margin Maximization with Boosting</title>
    <feedworld_mtime>1548892800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gr=oslash=nlund:Allan.html">Allan Grønlund</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Larsen:Kasper_Green.html">Kasper Green Larsen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mathiasen:Alexander.html">Alexander Mathiasen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10789">PDF</a><br/><b>Abstract: </b>Boosting algorithms produce a classifier by iteratively combining base
hypotheses. It has been observed experimentally that the generalization error
keeps improving even after achieving zero training error. One popular
explanation attributes this to improvements in margins. A common goal in a long
line of research, is to maximize the smallest margin using as few base
hypotheses as possible, culminating with the AdaBoostV algorithm by (R{\"a}tsch
and Warmuth [JMLR'04]). The AdaBoostV algorithm was later conjectured to yield
an optimal trade-off between number of hypotheses trained and the minimal
margin over all training points (Nie et al. [JMLR'13]). Our main contribution
is a new algorithm refuting this conjecture. Furthermore, we prove a lower
bound which implies that our new algorithm is optimal.
</p></div>
    </summary>
    <updated>2019-01-31T02:31:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10759</id>
    <link href="http://arxiv.org/abs/1901.10759" rel="alternate" type="text/html"/>
    <title>Manifold-based B-splines on unstructured meshes</title>
    <feedworld_mtime>1548892800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Qiaoling.html">Qiaoling Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takacs:Thomas.html">Thomas Takacs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cirak:Fehmi.html">Fehmi Cirak</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10759">PDF</a><br/><b>Abstract: </b>We introduce new manifold-based splines that are able to exactly reproduce
B-splines on unstructured surface meshes. Such splines can be used in
isogeometric analysis (IGA) to represent smooth surfaces of arbitrary topology.
Since prevalent computer-aided design (CAD) models are composed of
tensor-product B-spline patches, any IGA suitable construction should be able
to reproduce B-splines. To achieve this goal, we focus on univariate
manifold-based constructions that can reproduce B-splines. The manifold-based
splines are constructed by smoothly blending together polynomial interpolants
defined on overlapping charts. The proposed constructions automatically
reproduce B-splines in regular parts of the mesh, with no extraordinary
vertices, and polynomial basis functions in the remaining parts of the mesh. We
study and compare analytically and numerically the finite element convergence
of several univariate constructions. The obtained results directly carry over
to the tensor-product case.
</p></div>
    </summary>
    <updated>2019-01-31T02:36:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10722</id>
    <link href="http://arxiv.org/abs/1901.10722" rel="alternate" type="text/html"/>
    <title>Faster queries for longest substring palindrome after block edit</title>
    <feedworld_mtime>1548892800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Funakoshi:Mitsuru.html">Mitsuru Funakoshi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakashima:Yuto.html">Yuto Nakashima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeda:Masayuki.html">Masayuki Takeda</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10722">PDF</a><br/><b>Abstract: </b>Palindromes are important objects in strings which have been extensively
studied from combinatorial, algorithmic, and bioinformatics points of views.
Manacher [J. ACM 1975] proposed a seminal algorithm that computes the longest
substring palindromes (LSPals) of a given string in O(n) time, where n is the
length of the string. In this paper, we consider the problem of finding the
LSPal after the string is edited. We present an algorithm that uses O(n) time
and space for preprocessing, and answers the length of the LSPals in O(\ell +
\log \log n) time, after a substring in T is replaced by a string of arbitrary
length \ell. This outperforms the query algorithm proposed in our previous work
[CPM 2018] that uses O(\ell + \log n) time for each query.
</p></div>
    </summary>
    <updated>2019-01-31T02:27:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10698</id>
    <link href="http://arxiv.org/abs/1901.10698" rel="alternate" type="text/html"/>
    <title>Online Pandora's Boxes and Bandits</title>
    <feedworld_mtime>1548892800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esfandiari:Hossein.html">Hossein Esfandiari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajiaghayi:MohammadTaghi.html">MohammadTaghi Hajiaghayi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lucier:Brendan.html">Brendan Lucier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitzenmacher:Michael.html">Michael Mitzenmacher</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10698">PDF</a><br/><b>Abstract: </b>We consider online variations of the Pandora's box problem (Weitzman. 1979),
a standard model for understanding issues related to the cost of acquiring
information for decision-making. Our problem generalizes both the classic
Pandora's box problem and the prophet inequality framework. Boxes are presented
online, each with a random value and cost drew jointly from some known
distribution. Pandora chooses online whether to open each box given its cost,
and then chooses irrevocably whether to keep the revealed prize or pass on it.
We aim for approximation algorithms against adversaries that can choose the
largest prize over any opened box, and use optimal offline policies to decide
which boxes to open (without knowledge of the value inside). We consider
variations where Pandora can collect multiple prizes subject to feasibility
constraints, such as cardinality, matroid, or knapsack constraints. We also
consider variations related to classic multi-armed bandit problems from
reinforcement learning. Our results use a reduction-based framework where we
separate the issues of the cost of acquiring information from the online
decision process of which prizes to keep. Our work shows that in many
scenarios, Pandora can achieve a good approximation to the best possible
performance.
</p></div>
    </summary>
    <updated>2019-01-31T02:20:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10633</id>
    <link href="http://arxiv.org/abs/1901.10633" rel="alternate" type="text/html"/>
    <title>Computing runs on a trie</title>
    <feedworld_mtime>1548892800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ryo Sugahara, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakashima:Yuto.html">Yuto Nakashima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeda:Masayuki.html">Masayuki Takeda</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10633">PDF</a><br/><b>Abstract: </b>A maximal repeat, or run, in a string, is a periodically maximal substring
whose smallest period is at most half the length of the substring. In this
paper, we consider runs that correspond to a path on a trie, or in other words,
on a rooted edge-labeled tree where the endpoints of the path must be a
descendant/ancestor of the other. For a trie with $n$ edges, we show that the
number of runs is less than $n$. We also show an $O(n\sqrt{\log n}\log \log n)$
time and $O(n)$ space algorithm for counting and finding the shallower endpoint
of all runs. We further show an $O(n\sqrt{\log n}\log^2\log n)$ time and $O(n)$
space algorithm for finding both endpoints of all runs.
</p></div>
    </summary>
    <updated>2019-01-31T02:32:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10475</id>
    <link href="http://arxiv.org/abs/1901.10475" rel="alternate" type="text/html"/>
    <title>Efficient n-to-n Collision Detection for Space Debris using 4D AABB Trees (Extended Report)</title>
    <feedworld_mtime>1548892800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bak:Stanley.html">Stanley Bak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hobbs:Kerianne.html">Kerianne Hobbs</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10475">PDF</a><br/><b>Abstract: </b>Collision detection algorithms are used in aerospace, swarm robotics,
automotive, video gaming, dynamics simulation and other domains. As many
applications of collision detection run online, timing requirements are imposed
on the algorithm runtime: algorithms must, at a minimum, keep up with the
passage of time. In practice, this places a limit on the number of objects, n,
that can be tracked at the same time. In this paper, we improve the scalability
of collision detection, effectively raising the limit n for online object
tracking.
</p>
<p>The key to our approach is the use of a four-dimensional axis-aligned
bounding box (AABB) tree, which stores each object's three-dimensional
occupancy region in space during a one-dimensional interval of time. This
improves efficiency by permitting per-object variable times steps. Further, we
describe partitioning strategies that can decompose the 4D AABB tree search
into several smaller-dimensional problems that can be solved in parallel. We
formalize the collision detection problem and prove our algorithm's
correctness. We demonstrate the feasibility of online collision detection for
an orbital space debris application, using publicly available data on the full
catalog of n=16848 objects provided by www.space-track.org.
</p></div>
    </summary>
    <updated>2019-01-31T02:32:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7426</id>
    <link href="https://windowsontheory.org/2019/01/30/black-hole-paradoxes-a-conservative-yet-radical-journey/" rel="alternate" type="text/html"/>
    <title>Black hole paradoxes: A conservative yet radical journey</title>
    <summary>Guest post by Abhishek Anand and Noah Miller from the physics and computation seminar. In 2013, Harlow and Hayden drew an unexpected connection between theoretical computer science and theoretical physics as they proposed a potential resolution to the famous black hole Firewall paradox using computational complexity arguments. This blog post attempts to lay out the […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Guest post by Abhishek Anand and Noah Miller from <a href="https://www.boazbarak.org/fall18seminar/">the physics and computation seminar</a>.</p>



<p>In 2013, Harlow and Hayden drew an unexpected connection between theoretical computer science and theoretical physics as they proposed a potential resolution to the famous black hole Firewall paradox using computational complexity arguments. This blog post attempts to lay out the Firewall paradox and other peculiar (at first) properties associated with black holes that make them such intriguing objects to study. This post is inspired by Scott Aaronson’s [1] and Daniel Harlow’s [2] excellent notes on the same topic. The notes accompanying <a href="https://windowsontheory.org/2019/01/20/the-firewall-paradox-in-context/">this post</a> provides a thorough and self-contained introduction to theoretical physics from a CS perspective. Furthermore, for a quick and intuitive summary of the Firewall paradox and it’s link to computational complexity, refer to <a href="https://windowsontheory.org/2018/08/22/black-holes-paradoxes-and-computational-complexity/">this</a> blog post by Professor Barak last summer.</p>



<h2>Black holes and conservative radicalism</h2>



<p>Black holes are fascinating objects. Very briefly, they are regions of spacetime where the matter-energy density is so high and hence, where the gravitational effects are so strong that no particle (not even light!) can escape from it. More specifically, we define a particular distance called the “Schwarzschild radius” and anything that enters within the Schwarzschild radius, (also known as the “event horizon,”) cannot ever escape from the black hole. General relativity predicts that this particle is bound to hit the “singularity,” where spacetime curvature becomes infinite. In the truest sense of the word, they represent the “edge cases” of our Universe. Hence, perhaps, it is fitting that physicists believe that through thought experiments at these edges cases, they can investigate the true behavior of the laws that govern our Universe.</p>



<p>Once you know that such an object exists, many questions arise: what would it look it from the outside? Could we already be within the event horizon of a future black hole? How much information does it store? Would something special be happening at the Schwarzschild radius? How would the singularity manifest physically?</p>



<p>The journey of trying to answer these questions can aptly be described by the term “radical conservatism.” This is a phrase that has become quite popular in the physics community. A “radical conservative” would be someone that tries to modify as few laws of physics as possible (that’s the conservative part) and through their dogmatic refusal to modify these laws and go wherever their reasoning leads (that’s the radical part) is able to derive amazing things. We radically use the given system of beliefs to lead to certain conclusions (sometimes paradoxes!) and then conservatively update the system of beliefs to resolve the created paradox and iterate. We shall go through a few such cycles and end at the Firewall paradox. Let’s begin with the first problem: how much information does a black hole store?</p>



<h2>Entropy of a black hole</h2>



<p>A black hole is a physical object. Hence, it could be able to store some information. But how much? In other words, what should the entropy of a black hole be? There are two simple ways of looking at this problem:</p>



<ul><li><strong>0:</strong> The no-hair theorem postulates that an outside observer can measure a small number of quantities which completely characterize the black hole. There’s the mass of the black hole, which is its most important quantity. Interestingly, if the star was spinning before it collapsed, the black hole will also have some angular momentum, and its equator will bulge out a bit. Hence, the black hole is also characterized by an angular momentum vector. Also, if the object had some net charge, the black hole would also have that net charge. This means that if two black holes were created due to a book and a pizza, respectively, with the same mass, charge and angular momentum, there would settle down to the “same” black hole with no observable difference. If an outside observer knows these quantities, they will now know everything about the black hole. So, in this view, we should expect for the entropy of a black hole to be 0.</li><li><strong>Unbounded:</strong> But maybe that’s not entirely fair. After all, the contents of the star should somehow be contained in the singularity, hidden behind the horizon. As we saw above, all of the specific details of the star from before the collapse do not have any effect on the properties of the resulting black hole. The only stuff that matters it the total mass, total angular momentum, and the total charge. That leaves an infinite number of possible objects that could all have produced the same black hole: a pizza or a book or a PlayStation and so on. So actually, perhaps, we should expect the entropy of a black hole to be unbounded.</li></ul>



<p>The first answer troubled Jacob Bekenstein. He was a firm believer in the Second Law of Thermodynamics: the total entropy of an isolated system can never decrease over time. However, if the entropy of a black hole is 0, it provides with a way to reduce the entropy of any system: just dump objects with non-zero entropy into the black hole. </p>



<p>Bekenstein drew connections between the area of the black hole and its entropy. For example, the way in which a black hole’s area could only increase (according to classical general relativity) seemed reminiscent of entropy. Moreover, when two black holes merge, the area of the final black hole will always exceed the sum of the areas of the two original black holes This is surprising as for two spheres, the area/radius of the merged sphere, is always less than the sum of the areas/radii of two individual spheres: </p>



<p><img alt="(r_1^3 + r_2^3)^{\frac{1}{3}} &lt; r_1 + r_2" class="latex" src="https://s0.wp.com/latex.php?latex=%28r_1%5E3+%2B+r_2%5E3%29%5E%7B%5Cfrac%7B1%7D%7B3%7D%7D+%3C+r_1+%2B+r_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(r_1^3 + r_2^3)^{\frac{1}{3}} &lt; r_1 + r_2"/></p>



<p> Most things we’re used to, like a box of gas, have an entropy that scales linearly with its volume. However, black holes are not like most things. He predicted that entropy of a black hole should be proportional to its area, A and not its volume. We now believe that Bekenstein was right and it turns out that the entropy of the black hole can be written as:</p>



<p><img alt="S=\frac{kA}{4l^2_p}" class="latex" src="https://s0.wp.com/latex.php?latex=S%3D%5Cfrac%7BkA%7D%7B4l%5E2_p%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S=\frac{kA}{4l^2_p}"/></p>



<p>where <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> is Boltzmann constant and <img alt="l_p" class="latex" src="https://s0.wp.com/latex.php?latex=l_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="l_p"/> is the Planck-length, a length scale where physicists believe quantum mechanics breaks down and a quantum theory of gravity will be required. Interestingly, it seems as though the entropy of the black hole is (one-fourth times) the number of Planck-length-sized squares it would take to tile the horizon area. (Perhaps, the microstates of the black hole are “stored” on the horizon?) Using “natural units” where we set all constants to 1, we can write this as<br/> </p>



<p><img alt="S=\frac{A}{4}" class="latex" src="https://s0.wp.com/latex.php?latex=S%3D%5Cfrac%7BA%7D%7B4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S=\frac{A}{4}"/></p>



<p>which is very pretty. Even though this number of not infinite, it is very large. Here are some numerical estimates from [2]. The entropy of the universe (minus all the black holes) mostly comes from cosmic microwave background radiation and is about <img alt="10^{87}" class="latex" src="https://s0.wp.com/latex.php?latex=10%5E%7B87%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10^{87}"/> in some units. Meanwhile, in the same units, the entropy of a solar mass black hole is <img alt="10^{78}" class="latex" src="https://s0.wp.com/latex.php?latex=10%5E%7B78%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10^{78}"/>. The entropy of our sun, as it is now, is a much smaller <img alt="10^{60}" class="latex" src="https://s0.wp.com/latex.php?latex=10%5E%7B60%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10^{60}"/>. The entropy of the supermassive black hole in the center of our galaxy is <img alt="10^{88}" class="latex" src="https://s0.wp.com/latex.php?latex=10%5E%7B88%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10^{88}"/>, <strong>larger than the rest of the universe combined </strong>(minus black holes). The entropy of any of the largest known supermassive black holes would be <img alt="10^{96}" class="latex" src="https://s0.wp.com/latex.php?latex=10%5E%7B96%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10^{96}"/>. Hence, there is a simple “argument” which suggests that black holes are the most efficient information storage devices in the universe: if you wanted to store a lot of information in a region smaller than a black hole horizon, it would probably have to be so dense that it would just be a black hole anyway.</p>



<p>However, this resolution to “maintain” the second law of thermodynamics leads to a radical conclusion: if a black hole has non-zero entropy, it must have a non-zero temperature and hence, must emit thermal radiation. This troubled Hawking.</p>



<h2>Hawking radiation and black hole evaporation</h2>



<p>Hawking did a semi-classical computation looking at energy fluctuations near the horizon and actually found that black holes do radiate! They emit energy in the form of very low-energy particles. This is a unique feature of what happens to black holes when you take quantum field theory into account and is very surprising. However, the Hawking radiation from any actually existing black hole is far too weak to have been detected experimentally.</p>



<p>One simplified way to understand the Hawking radiation is by thinking about highly coupled modes (think “particles”) being formed continuously near the horizon. As this formation must conserve the conservation of energy, one of these particles has negative energy and one of the particles has the same energy but with a positive sign and hence, they are maximally entangled (if you know the energy of one of the particles, you know the energy of the other one): we will be referring to this as short-range entanglement. The one with negative energy falls into the black hole while the one with positive energy comes out as Hawking radiation. The maximally-entangled state of the modes looks like:</p>



<p><img alt="\sum_{\mathbf{k}} f(\mathbf{k}) |\mathbf{k}\rangle_{\rm in} |\mathbf{k}\rangle_{\rm out} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7B%5Cmathbf%7Bk%7D%7D+f%28%5Cmathbf%7Bk%7D%29+%7C%5Cmathbf%7Bk%7D%5Crangle_%7B%5Crm+in%7D+%7C%5Cmathbf%7Bk%7D%5Crangle_%7B%5Crm+out%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{\mathbf{k}} f(\mathbf{k}) |\mathbf{k}\rangle_{\rm in} |\mathbf{k}\rangle_{\rm out} "/></p>



<p>Here is a cartoon that represents the process:</p>



<ul class="wp-block-gallery columns-1 is-cropped"><li class="blocks-gallery-item"><figure><img alt="" class="wp-image-7431" src="https://windowsontheory.files.wordpress.com/2019/01/partnermode2.png?w=600"/>A cartoon of the Hawking partner modes. The shaded region shows the width of the Gaussian wavepackets. The outgoing mode redshifts and spreads.</figure></li></ul>



<p>Because energetic particles are leaving the black hole and negative energy particles are adding to it, the black hole itself will actually shrink, which would never happen classically! And, eventually a black-hole will disappear. In fact, the time of evaporation of the black hole scales polynomially in the radius of the black hole, as <img alt="R^3" class="latex" src="https://s0.wp.com/latex.php?latex=R%5E3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R^3"/>. The black holes that we know about are simply too big and would be shrinking too slowly. A stellar-mass black hole would take <img alt="10^{67}" class="latex" src="https://s0.wp.com/latex.php?latex=10%5E%7B67%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="10^{67}"/> years to disappear from Hawking radiation.</p>



<p>However, the fact that black holes disappear does not play nicely with another core belief in physics: reversibility.</p>



<h2>Unitary evolution and thermal radiation</h2>



<p>A core tenet of quantum mechanics is <em>unitary evolution</em>: every operation that happens to a quantum state must be reversible (invertible). That is: if we know the final state and the set and order of operations performed, we should be able to invert the operations and get back the initial state. No information is lost. However, something weird happens with an evaporating black hole. First, let us quickly review pure and mixed quantum states. A <em>pure state</em> is a quantum state that can be described by a single ket vector while a mixed state represents a classical (probabilistic) mixture of pure states and can be expressed using density matrices. For example, in both, the pure state <img alt="|\psi\langle = |1\rangle + |0\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Clangle+%3D+%7C1%5Crangle+%2B+%7C0%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\langle = |1\rangle + |0\rangle"/> and mixed state <img alt="\rho = |1\rangle\langle1| +  |0\rangle\langle0|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho+%3D+%7C1%5Crangle%5Clangle1%7C+%2B++%7C0%5Crangle%5Clangle0%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho = |1\rangle\langle1| +  |0\rangle\langle0|"/> would one measure <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1"/> half the time and <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> 50% half the time. However, in the later one would not observe any quantum effects (think interference patterns of the double-slit experiment).</p>



<p>People outside of the black hole will not be able to measure the objects (quantum degrees of freedom) that are inside the black hole. They will only be able to perform measurements on a subset of the information: the one available outside of the event horizon. So, the state they would measure would be a mixed state. A simple example to explain what this means is that if the state of the particles near the horizon is:</p>



<p><img alt="|\Psi\rangle_{init} = \frac{|0\rangle_A|0\rangle_B +|1\rangle_A|1\rangle_B} {\sqrt{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5CPsi%5Crangle_%7Binit%7D+%3D+%5Cfrac%7B%7C0%5Crangle_A%7C0%5Crangle_B+%2B%7C1%5Crangle_A%7C1%5Crangle_B%7D+%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\Psi\rangle_{init} = \frac{|0\rangle_A|0\rangle_B +|1\rangle_A|1\rangle_B} {\sqrt{2}}"/></p>



<p>tracing over the qubit A leaves us with the state and density matrix:</p>



<p><br/> <img alt="|\Psi\rangle_{obs} = \frac{|0\rangle_{B}\langle0| + |1\rangle_{B}\langle1|}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5CPsi%5Crangle_%7Bobs%7D+%3D+%5Cfrac%7B%7C0%5Crangle_%7BB%7D%5Clangle0%7C+%2B+%7C1%5Crangle_%7BB%7D%5Clangle1%7C%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\Psi\rangle_{obs} = \frac{|0\rangle_{B}\langle0| + |1\rangle_{B}\langle1|}{2}"/>,</p>



<p><img alt="\rho_{obs} = \frac{1}{2} \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho_%7Bobs%7D+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cbegin%7Bpmatrix%7D+1+%26+0+%5C%5C+0+%26+1+%5Cend%7Bpmatrix%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho_{obs} = \frac{1}{2} \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}"/></p>



<p>which is a classical mixed state (50% of times results in 1 and 50% of times results in 0). The non-diagonal entries of the density matrix encode the “quantum inference” of the quantum state. Here, are they are, in some sense we have lost the “quantum” aspect of the information.</p>



<p>In fact, Hawking went and traced over the field degrees of freedom that were hidden behind the event horizon, and found something surprising: the mixed state was thermal! It acted “as if” it is being emitted by some object with temperature “T” which does not depend on what formed the black hole and solely depends on the mass of the black hole. Now, we have the information paradox:</p>



<ul><li><strong>Physics perspective</strong>: Now, once the black hole evaporates, we are left with this mixed thermal there is no way to precisely reconstruct the initial state that formed the black hole: the black hole has taken away information! Once the black hole is gone, the information of what went into the black hole is gone for good. Nobody living in the post-black-hole universe could figure out exactly what went into the black hole, even if they had full knowledge of the radiation. Another way to derive a contradiction is that the process of black hole evaporation when combined with the disappearance of the black hole, imply that a pure state has evolved into a mixed state, something which is impossible via unitary time evolution! Pure states only become mixed states whenever we decide to perform a partial trace; they never become mixed because of Schrodinger’s equation which governs the evolution of quantum states.</li><li><strong>CS perspective: </strong>We live in a world where only invertible functions are allowed. However, we are given this exotic function – the black hole – which seems to be a genuine random one-to-many function. There is no way to determine the input deterministically given the output of the function.</li></ul>



<p>What gives? If the process of black hole evaporation is truly “non-unitary,” it would be a first for physics. We have no way to make sense of quantum mechanics without the assumption of unitary operations and reversibility; hence, it does not seem very conservative to get ride of it. </p>



<p>Physicists don’t know exactly how information is conserved, but they think that if they assume that it does, it will help them figure out something about quantum gravity. Most physicists believe that the process of black hole evaporation should indeed be unitary. The information of what went into the black hole is being released via the radiation in way too subtle for us to currently understand. What does this mean?</p>



<ul><li><strong>Physics perspective</strong>: Somehow, after the black hole is gone, the final state we observe, after tracing over the degrees of freedom taken away by the black hole, is pure and encodes all information about what went inside the black hole. That is: <img alt="Tr_{inside} (|\Psi\rangle_{init}\langle\Psi|) = pure" class="latex" src="https://s0.wp.com/latex.php?latex=Tr_%7Binside%7D+%28%7C%5CPsi%5Crangle_%7Binit%7D%5Clangle%5CPsi%7C%29+%3D+pure&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Tr_{inside} (|\Psi\rangle_{init}\langle\Psi|) = pure"/></li><li><strong>CS perspective</strong>: Somehow, the exotic black hole function seems random but actually is pseudo-random as well as injective and given the output and enough time, we can decode it and determine the input (think hash functions!).</li></ul>



<p>However, this causes yet another unwanted consequence: the violation of the no-cloning theorem!</p>



<h2>Xeroxing problem and black hole complementarity</h2>



<p>The no-cloning theorem simply states that an arbitrary quantum state cannot be copied. In other words, if you have one qubit representing some initial state, no matter what operations you do, you cannot end up with two qubits with the same state you started with. How do our assumptions violate this?</p>



<p>Say you are outside the black hole and send in a qubit with some information (input to the function). You collect the radiation corresponding to the qubit (output of the function) that came out. Now you decode this radiation (output) to determine the state of infalling matter (input). Aha! You have violated the no-cloning theorem as you have two copies of the same state: one inside and one outside the black hole.</p>



<p>So wait, again, what gives?</p>



<p>One possible resolution is to postulate that the inside of the black hole just does not exist. However, that doesn’t seem very conservative. According to Einstein’s theory of relativity, locally speaking, there is nothing particularly special about the horizon: hence, one should be able to cross the horizon and move towards the singularity peacefully.</p>



<p>The crucial observation is that for the person who jumped into the black hole, the outside universe may as well not exist; they can not escape. Extending this further, perhaps, somebody on the outside does not believe the interior of the black hole exists and somebody on the inside does not believe the exterior exists and they are both right. This hypothesis, formulated in the early 1990s, has been given the name of <em>Black Hole Complementarity.</em> The word “complementarity” comes from the fact that two observers give different yet complementary views of the world.</p>



<p>In this view, according to someone on the outside, instead of entering the black hole at some finite time, the infalling observer will instead be stopped at some region very close to the horizon, which is quite hot when you get up close. Then, the Hawking radiation coming off of the horizon will hit the observer on its way out, carrying the information about them which has been plastered on the horizon. So the outside observer, who is free to collect this radiation, should be able to reconstruct all the information about the person who went in. Of course, that person will have burned up near the horizon and will be dead.</p>



<p>And from the infalling observer’s perspective, however, they were able to pass peacefully through the black hole and sail on to the singularity. So from their perspective, they live, while from the outside it looks like they died. However, no contradiction can be reached, because nobody has access to both realities.</p>



<p>But why is that? Couldn’t the outside observer see the infalling observer die and then rocket themselves straight into the black hole themselves to meet the alive person once again before they hit the singularity, thus producing a contradiction?</p>



<p>The core idea is that it must take some time for the infalling observer to “thermalize” (equilibriate) on the horizon: enough time for the infalling observer to reach the singularity and hence become completely inaccessible. Calculations do show this to be true. In fact, we can already sense a taste of complexity theory even in this argument: we are assuming that some process is slower than some other process.</p>



<p>In summary, according to the BHC worldview, the information outside the horizon is redundant with the information inside the horizon.</p>



<p>But, in 2012, a new paradox, the Firewall paradox, was introduced by AMPS [3]. This paradox seems to be immune to BHC: the paradox exists even if we assume everything we have discussed till now. The physics principle we violate, in this case, is the monogamy of entanglement.</p>



<h2>Monogamy of entanglement and Page time</h2>



<p>Before we state the Firewall paradox, we must introduce two key concepts.</p>



<h3>Monogamy of entanglement</h3>



<p>Monogamy of entanglement is a statement about the maximum entanglement a particle can share with other particles. More precisely, if two particles A and B are maximally entangled with each other, they cannot be at all entanglement with a third particle C. Two maximally entangled particles have saturated both of their “entanglement quotas\”. In order for them to have correlations with other particles, they must decrease their entanglement with each other.</p>



<p>Monogamy of entanglement can be understood as a static version of the no-cloning theorem. Here is a short proof sketch of why polygamy of entanglement implies the violation of no-cloning theorem.</p>



<p>Let’s take a short detour to explain quantum teleportation:</p>



<p>Say you have three particles A, B, and C with A and B maximally entangled (Bell pair), and C is an arbitrary quantum state:</p>



<p><img alt="|\Phi^+\rangle_{AB} = \frac{1}{\sqrt{2}} (|0\rangle_A \otimes |0\rangle_{B} + |1\rangle_A \otimes |1\rangle_{B})" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5CPhi%5E%2B%5Crangle_%7BAB%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D+%28%7C0%5Crangle_A+%5Cotimes+%7C0%5Crangle_%7BB%7D+%2B+%7C1%5Crangle_A+%5Cotimes+%7C1%5Crangle_%7BB%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\Phi^+\rangle_{AB} = \frac{1}{\sqrt{2}} (|0\rangle_A \otimes |0\rangle_{B} + |1\rangle_A \otimes |1\rangle_{B})"/></p>



<p><img alt="|\psi\rangle_C = \alpha |0\rangle_C + \beta|1\rangle_C" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle_C+%3D+%5Calpha+%7C0%5Crangle_C+%2B+%5Cbeta%7C1%5Crangle_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle_C = \alpha |0\rangle_C + \beta|1\rangle_C"/></p>



<p>We can write their total state as:</p>



<p><img alt="|\psi \rangle_{C}\otimes |\Phi ^{+}\rangle_{AB}=(\alpha |0\rangle_{C}+\beta |1\rangle_{C})\otimes {\frac {1}{\sqrt {2}}}(|0\rangle_{A}\otimes |0\rangle_{B}+|1\rangle_{A}\otimes |1\rangle_{B})" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi+%5Crangle_%7BC%7D%5Cotimes+%7C%5CPhi+%5E%7B%2B%7D%5Crangle_%7BAB%7D%3D%28%5Calpha+%7C0%5Crangle_%7BC%7D%2B%5Cbeta+%7C1%5Crangle_%7BC%7D%29%5Cotimes+%7B%5Cfrac+%7B1%7D%7B%5Csqrt+%7B2%7D%7D%7D%28%7C0%5Crangle_%7BA%7D%5Cotimes+%7C0%5Crangle_%7BB%7D%2B%7C1%5Crangle_%7BA%7D%5Cotimes+%7C1%5Crangle_%7BB%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi \rangle_{C}\otimes |\Phi ^{+}\rangle_{AB}=(\alpha |0\rangle_{C}+\beta |1\rangle_{C})\otimes {\frac {1}{\sqrt {2}}}(|0\rangle_{A}\otimes |0\rangle_{B}+|1\rangle_{A}\otimes |1\rangle_{B})"/></p>



<p>Re-arranging and pairing A and C, the state simplifies to:</p>



<p><img alt="|\psi \rangle {C}\otimes \ |\Phi ^{+}\rangle {AB}\ = \frac {1}{2}{\Big \lbrack }\ |\Phi ^{+}\rangle {AC}\otimes (\alpha |0\rangle {B}+\beta |1\rangle {B})\ +\ |\Phi ^{-}\rangle {AC}\otimes (\alpha |0\rangle {B}-\beta |1\rangle {B})\ +\ |\Psi ^{+}\rangle {AC}\otimes (\beta |0\rangle {B}+\alpha |1\rangle {B})\ +\ |\Psi ^{-}\rangle {AC}\otimes (\beta |0\rangle {B}-\alpha |1\rangle {B}){\Big \rbrack }" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi+%5Crangle+%7BC%7D%5Cotimes+%5C+%7C%5CPhi+%5E%7B%2B%7D%5Crangle+%7BAB%7D%5C+%3D+%5Cfrac+%7B1%7D%7B2%7D%7B%5CBig+%5Clbrack+%7D%5C+%7C%5CPhi+%5E%7B%2B%7D%5Crangle+%7BAC%7D%5Cotimes+%28%5Calpha+%7C0%5Crangle+%7BB%7D%2B%5Cbeta+%7C1%5Crangle+%7BB%7D%29%5C+%2B%5C+%7C%5CPhi+%5E%7B-%7D%5Crangle+%7BAC%7D%5Cotimes+%28%5Calpha+%7C0%5Crangle+%7BB%7D-%5Cbeta+%7C1%5Crangle+%7BB%7D%29%5C+%2B%5C+%7C%5CPsi+%5E%7B%2B%7D%5Crangle+%7BAC%7D%5Cotimes+%28%5Cbeta+%7C0%5Crangle+%7BB%7D%2B%5Calpha+%7C1%5Crangle+%7BB%7D%29%5C+%2B%5C+%7C%5CPsi+%5E%7B-%7D%5Crangle+%7BAC%7D%5Cotimes+%28%5Cbeta+%7C0%5Crangle+%7BB%7D-%5Calpha+%7C1%5Crangle+%7BB%7D%29%7B%5CBig+%5Crbrack+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi \rangle {C}\otimes \ |\Phi ^{+}\rangle {AB}\ = \frac {1}{2}{\Big \lbrack }\ |\Phi ^{+}\rangle {AC}\otimes (\alpha |0\rangle {B}+\beta |1\rangle {B})\ +\ |\Phi ^{-}\rangle {AC}\otimes (\alpha |0\rangle {B}-\beta |1\rangle {B})\ +\ |\Psi ^{+}\rangle {AC}\otimes (\beta |0\rangle {B}+\alpha |1\rangle {B})\ +\ |\Psi ^{-}\rangle {AC}\otimes (\beta |0\rangle {B}-\alpha |1\rangle {B}){\Big \rbrack }"/></p>



<p>which means that if one does a Bell pair measurement on A and C, based on the measurement outcome, we know exactly which state B is projected to and by using rotations can make the state of B equal to the initial state of C. Hence, we teleported quantum information from C to B.</p>



<p>Now, assume that A was maximally entangled to both B and D. Then by doing the same procedure, we could teleport quantum information from C to both B and D and hence, violate the no-cloning theorem!</p>



<h3>Page time</h3>



<p>Named after Don Page, the “Page time” refers to the time when the black hole has emitted enough of its energy in the form of Hawking radiation that its entropy has (approximately) halved. Now the question is, what’s so special about the Page time?</p>



<p>First note that the rank of the density matrix is closely related to its purity (or mixedness). For example, a completely mixed state is the diagonal matrix:</p>



<p><img alt="\rho_{obs} = \frac{1}{4} \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho_%7Bobs%7D+%3D+%5Cfrac%7B1%7D%7B4%7D+%5Cbegin%7Bpmatrix%7D+1+%26+0+%26+0+%26+0%5C%5C+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+0+%26+1+%5Cend%7Bpmatrix%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho_{obs} = \frac{1}{4} \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{pmatrix}"/></p>



<p>which has maximal rank (<img alt="2^{2}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{2}"/>). Furthermore, a completely pure state <img alt="|\Psi\rangle \langle\Psi|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5CPsi%5Crangle+%5Clangle%5CPsi%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\Psi\rangle \langle\Psi|"/> can always be represented as (if we just change the basis and make the first column/row represent <img alt="|\Psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5CPsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\Psi\rangle"/>):</p>



<p><img alt="\rho_{obs} =  \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0  \end{pmatrix}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho_%7Bobs%7D+%3D++%5Cbegin%7Bpmatrix%7D+1+%26+0+%26+0+%26+0%5C%5C+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0++%5Cend%7Bpmatrix%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho_{obs} =  \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0  \end{pmatrix}"/><br/></p>



<p>which has rank 1.</p>



<p>Imagine we have watched a black hole form and begin emitting Hawking radiation. Say we start collecting this radiation. The density matrix of the radiation will have the form:</p>



<p><img alt="\rho_{obs} = \sum_{i=1}^{2^{n-k}} p_i |\Psi\rangle_{i}\langle\Psi|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho_%7Bobs%7D+%3D+%5Csum_%7Bi%3D1%7D%5E%7B2%5E%7Bn-k%7D%7D+p_i+%7C%5CPsi%5Crangle_%7Bi%7D%5Clangle%5CPsi%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho_{obs} = \sum_{i=1}^{2^{n-k}} p_i |\Psi\rangle_{i}\langle\Psi|"/></p>



<p>where <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> is the total number of qubits in our initial state, <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> is the number of qubits outside (in form of radiation), and <img alt="p_i" class="latex" src="https://s0.wp.com/latex.php?latex=p_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_i"/> is the probability of each state. We are simply tracing over the degrees of freedom inside the black hole (as there are <img alt="n-k" class="latex" src="https://s0.wp.com/latex.php?latex=n-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n-k"/> degrees inside the black hole, dimensionality of this space is <img alt="2^{n-k}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bn-k%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{n-k}"/>).</p>



<p>Don Page proposed the following graph of what he thought entanglement entropy of this density matrix should look like. It is fittingly called the “Page curve.”</p>



<figure class="wp-block-image"><img alt="" class="wp-image-7432" src="https://windowsontheory.files.wordpress.com/2019/01/pagecurve.png?w=600"/>The Page Curve</figure>



<ul><li>If <img alt="k&lt;\frac{n}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=k%3C%5Cfrac%7Bn%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k&lt;\frac{n}{2}"/>, rank(<img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/>) = <img alt="2^{k}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{k}"/>, as there are enough terms in the sum to get a maximally ranked matrix. And hence, we get maximally mixed states. In the beginning, the radiation we collect at early times will still remain heavily entangled with the degrees of freedom near the black hole, and as such the state will look mixed to us because we can not yet observe all the complicated entanglement. As more and more information leaves the black hole in the form of Hawking radiation, we are “tracing out” fewer and fewer of the near-horizon degrees of freedom. The dimension of our density matrix grows bigger and bigger, and because the outgoing radiation is still so entangled with the near-horizon degrees of freedom, the density matrix will still have off-diagonal terms which are essentially zero. Hence, the state entropy increases linearly.</li><li>But if <img alt="k&gt;\frac{n}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=k%3E%5Cfrac%7Bn%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k&gt;\frac{n}{2}"/>, by the same argument, rank(<img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/>) = <img alt="2^{n-k} &lt; 2^{k}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bn-k%7D+%3C+2%5E%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{n-k} &lt; 2^{k}"/>. Hence, the density matrix becomes more and more pure. Once the black hole’s entropy has reduced by half, the dimension of the Hilbert space we are tracing out finally becomes smaller than the dimension of the Hilbert space we are not tracing out. The off-diagonal terms spring into our density matrix, growing in size and number as the black hole continues to shrink. Finally, once the black hole is gone, we can easily see that all the resulting radiation is in a pure state.</li></ul>



<p>The entanglement entropy of the outgoing radiation finally starts decreasing, as we are finally able to start seeing entanglements between all this seemingly random radiation we have painstakingly collected. Some people like to say that if one could calculate the Page curve from first principles, the information paradox would be solved. Now we are ready to state the firewall paradox.</p>



<h2>The Firewall Paradox</h2>



<p>Say Alice collects all the Hawking radiation coming out of a black hole. At maybe, about <img alt="1.5" class="latex" src="https://s0.wp.com/latex.php?latex=1.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1.5"/> times the Page time, Alice is now able to see significant entanglement in all the radiation she has collected. Alice then dives into the black hole and sees an outgoing Hawking mode escaping. Given the Page curve, we know that knowing this outgoing mode must decrease the entropy of our observed mixed state. In other words, it must make our observed density matrix purer. And hence, be entangled with the particles we have already collected.</p>



<p>(Another way to think about this: let’s say that a random quantum circuit at the horizon scrambles the information in a non-trivial yet injective way in order for radiation particles to encode the information regarding what went inside the black hole. The output qubits of the circuit must be highly entangled due to the random circuit.)</p>



<ul class="wp-block-gallery columns-1 is-cropped"><li class="blocks-gallery-item"><figure><img class="wp-image-7455" src="https://windowsontheory.files.wordpress.com/2019/01/alice2-1.png?w=796"/>Alice diving into the black hole after the Page time to see the outgoing mode emerge.</figure></li></ul>



<p>However, given our discussion on Hawking radiation about short-range entanglement, the outgoing mode must be maximally entangled with an in-falling partner mode. This contradicts monogamy of entanglement! The outgoing mode cannot be entangled both with the radiation Alice has already collected and also maximally entangled with the nearby infalling mode!</p>



<p>So, to summarize, what did we do? We started with the existence of black holes and through our game of conservative radicalism, modified how physics works around them in order to make sure the following dear Physics principles are not violated by these special objects:</p>



<ul><li>Second Law of Thermodynamics</li><li>Objects with entropy emit thermal radiation</li><li>Unitary evolution and reversibility</li><li>No-cloning theorem</li><li>Monogamy of entanglement</li></ul>



<p>And finally, ended with the Firewall paradox.</p>



<p>So, for the last time in this blog post, what gives?</p>



<ul><li><strong>Firewall solution:</strong> The first solution to the paradox is the existence of a <em>firewall</em> at the horizon. The only way to not have the short-range entanglement discussed is if there is very high energy density at the horizon. However, this violates the “no-drama” theorem and Einstein’s equivalence principle of general relativity which states that locally there should be nothing special about the horizon. If firewalls did exist, an actual wall of fire could randomly appear out of nowhere in front of us right now if a future black hole would have its horizon near us. Hence, this solution is not very popular.</li><li><strong>Remnant solution:</strong> One possible resolution would be that the black hole never “poofs” but some quantum gravity effect we do not yet understand stabilizes it instead, allowing for some Planck-sized object to stick around? Such an object would be called a “remnant.” The so-called “remnant solution” to the information paradox is not a very popular one. People don’t like the idea of a very tiny, low-mass object holding an absurdly large amount of information.</li><li><strong>No unitary evolution:</strong> Perhaps, black holes are special objects which actually lose information! This would mean that black hole physics (the quantum theory of gravity) would be considerably different compared to quantum field theory.</li><li><strong>Computational complexity solution?</strong>: Can anyone ever observe this violation? And if not, does that resolve the paradox? This will be covered in our next blog post by Parth and Chi-Ning.</li></ul>



<h2>References</h2>



<ol><li>Scott Aaronson. The complexity of quantum states and transformations: from quantum money to black holes.arXiv preprintarXiv:1607.05256, 2016.</li><li>Daniel Harlow. Jerusalem lectures on black holes and quantum information. Reviews of Modern Physics, 88(1):015002, 2016.</li><li>Ahmed Almheiri, Donald Marolf, Joseph Polchinski, and JamesSully. Black holes: complementarity or firewalls? Journal of HighEnergy Physics, 2013(2):62, 2013.</li></ol></div>
    </content>
    <updated>2019-01-30T05:58:42Z</updated>
    <published>2019-01-30T05:58:42Z</published>
    <category term="physics"/>
    <category term="cs229r"/>
    <category term="quantum"/>
    <author>
      <name>bishk97</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-01-31T04:21:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10453</id>
    <link href="http://arxiv.org/abs/1901.10453" rel="alternate" type="text/html"/>
    <title>Simulating the DNA String Graph in Succinct Space</title>
    <feedworld_mtime>1548806400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=iacute=az=Dom=iacute=nguez:Diego.html">Diego Díaz-Domínguez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gagie:Travis.html">Travis Gagie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10453">PDF</a><br/><b>Abstract: </b>Converting a set of sequencing reads into a lossless compact data structure
that encodes all the relevant biological information is a major challenge. The
classical approaches are to build the string graph or the de Bruijn graph. Each
has advantages over the other depending on the application. Still, the ideal
setting would be to have an index of the reads that is easy to build and can be
adapted to any type of biological analysis. In this paper, we propose a new
data structure we call rBOSS, which gets close to that ideal. Our rBOSS is a de
Bruijn graph in practice, but it simulates any length up to k and can compute
overlaps of size at least m between the labels of the nodes, with k and m being
parameters. If we choose the parameter k equal to the size of the reads, then
we can simulate a complete string graph. As most BWT-based structures, rBOSS is
unidirectional, but it exploits the property of the DNA reverse complements to
simulate bi-directionality with some time-space trade-offs. We implemented a
genome assembler on top of rBOSS to demonstrate its usefulness. Our
experimental results show that using k = 100, rBOSS can assemble 185 MB of
reads in less than 15 minutes and using 110 MB in total. It produces contigs of
mean sizes over 10,000, which is twice the size obtained by using a pure de
Bruijn graph of fixed length k.
</p></div>
    </summary>
    <updated>2019-01-30T23:22:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10387</id>
    <link href="http://arxiv.org/abs/1901.10387" rel="alternate" type="text/html"/>
    <title>A Pseudo-Deterministic RNC Algorithm for General Graph Perfect Matching</title>
    <feedworld_mtime>1548806400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anari:Nima.html">Nima Anari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vazirani:Vijay_V=.html">Vijay V. Vazirani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10387">PDF</a><br/><b>Abstract: </b>The difficulty of obtaining an NC perfect matching algorithm has led
researchers to study matching vis-a-vis clever relaxations of the class NC. In
this vein, recently [GG15] gave a pseudo-deterministic RNC algorithm for
finding a perfect matching in a bipartite graph, i.e., an RNC algorithm with
the additional requirement that on the same graph, it should return the same
unique perfect matching for almost all choices of random bits. In this paper,
we give an analogous algorithm for general, not necessarily bipartite, graphs.
More generally, we give a pseudo-deterministic RNC algorithm for finding a
minimum weight perfect matching when the edge weights are polynomially bounded.
</p>
<p>Our algorithm builds on [AV18], whose result used planarity of input graphs
critically; in fact, in three different ways. The main challenge was to adapt
these steps to general graphs by exploiting the leeway that seeking a
pseudo-deterministic RNC algorithm and not an NC algorithm gives us.
</p></div>
    </summary>
    <updated>2019-01-30T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10330</id>
    <link href="http://arxiv.org/abs/1901.10330" rel="alternate" type="text/html"/>
    <title>Canonisation and Definability for Graphs of Bounded Rank Width</title>
    <feedworld_mtime>1548806400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grohe:Martin.html">Martin Grohe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neuen:Daniel.html">Daniel Neuen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10330">PDF</a><br/><b>Abstract: </b>We prove that the combinatorial Weisfeiler-Leman algorithm of dimension
$(3k+4)$ is a complete isomorphism test for the class of all graphs of rank
width at most $k$. Rank width is a graph invariant that, similarly to tree
width, measures the width of a certain style of hierarchical decomposition of
graphs; it is equivalent to clique width. It was known that isomorphism of
graphs of rank width $k$ is decidable in polynomial time (Grohe and Schweitzer,
FOCS 2015), but the best previously known algorithm has a running time
$n^{f(k)}$ for a non-elementary function $f$. Our result yields an isomorphism
test for graphs of rank width $k$ running in time $n^{O(k)}$. Another
consequence of our result is the first polynomial time canonisation algorithm
for graphs of bounded rank width. Our second main result is that fixed-point
logic with counting captures polynomial time on all graph classes of bounded
rank width.
</p></div>
    </summary>
    <updated>2019-01-30T23:32:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10317</id>
    <link href="http://arxiv.org/abs/1901.10317" rel="alternate" type="text/html"/>
    <title>On the Complexity of Computing the Topology of Real Algebraic Space Curves</title>
    <feedworld_mtime>1548806400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Kai.html">Kai Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Jin=San.html">Jin-San Cheng</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10317">PDF</a><br/><b>Abstract: </b>In this paper, we present a deterministic algorithm to find a strong generic
position for an algebraic space curve. We modify our existing algorithm for
computing the topology of an algebraic space curve and analyze the bit
complexity of the algorithm. It is $\tilde{\mathcal {O}} (N^{20})$, where
$N=\max\{d,\tau\}$, $d, \tau$ are the degree bound and the bit size bound of
the coefficients of the defining polynomials of the algebraic space curve. To
our knowledge, this is the best bound among the existing work. It gains the
existing results at least $N^2$.
</p></div>
    </summary>
    <updated>2019-01-30T23:31:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10165</id>
    <link href="http://arxiv.org/abs/1901.10165" rel="alternate" type="text/html"/>
    <title>Fully-functional bidirectional Burrows-Wheeler indexes</title>
    <feedworld_mtime>1548806400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cunial:Fabio.html">Fabio Cunial</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Belazzougui:Djamal.html">Djamal Belazzougui</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10165">PDF</a><br/><b>Abstract: </b>Given a string $T$ on an alphabet of size $\sigma$, we describe a
bidirectional Burrows-Wheeler index that takes $O(|T|\log{\sigma})$ bits of
space, and that supports the addition \emph{and removal} of one character, on
the left or right side of any substring of $T$, in constant time. Previously
known data structures that used the same space allowed constant-time addition
to any substring of $T$, but they could support removal only from specific
substrings of $T$. We also describe an index that supports bidirectional
addition and removal in $O(\log{\log{|T|}})$ time, and that occupies a number
of words proportional to the number of left and right extensions of the maximal
repeats of $T$. We use such fully-functional indexes to implement
bidirectional, frequency-aware, variable-order de Bruijn graphs in small space,
with no upper bound on their order, and supporting natural criteria for
increasing and decreasing the order during traversal.
</p></div>
    </summary>
    <updated>2019-01-30T23:36:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10084</id>
    <link href="http://arxiv.org/abs/1901.10084" rel="alternate" type="text/html"/>
    <title>A Parallel Projection Method for Metric Constrained Optimization</title>
    <feedworld_mtime>1548806400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Cameron Ruggles, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Veldt:Nate.html">Nate Veldt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gleich:David_F=.html">David F. Gleich</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10084">PDF</a><br/><b>Abstract: </b>Many clustering applications in machine learning and data mining rely on
solving metric-constrained optimization problems. These problems are
characterized by $O(n^3)$ constraints that enforce triangle inequalities on
distance variables associated with $n$ objects in a large dataset. Despite its
usefulness, metric-constrained optimization is challenging in practice due to
the cubic number of constraints and the high-memory requirements of standard
optimization software. Recent work has shown that iterative projection methods
are able to solve metric-constrained optimization problems on a much larger
scale than was previously possible, thanks to their comparatively low memory
requirement. However, the major limitation of projection methods is their slow
convergence rate. In this paper we present a parallel projection method for
metric-constrained optimization which allows us to speed up the convergence
rate in practice. The key to our approach is a new parallel execution schedule
that allows us to perform projections at multiple metric constraints
simultaneously without any conflicts or locking of variables. We illustrate the
effectiveness of this execution schedule by implementing and testing a parallel
projection method for solving the metric-constrained linear programming
relaxation of correlation clustering. We show numerous experimental results on
problems involving up to 2.9 trillion constraints.
</p></div>
    </summary>
    <updated>2019-01-30T23:33:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10045</id>
    <link href="http://arxiv.org/abs/1901.10045" rel="alternate" type="text/html"/>
    <title>Online Algorithms for Constructing Linear-size Suffix Trie</title>
    <feedworld_mtime>1548806400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hendrian:Diptarama.html">Diptarama Hendrian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takagi:Takuya.html">Takuya Takagi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10045">PDF</a><br/><b>Abstract: </b>The suffix trees are fundamental data structures for various kinds of string
processing. The suffix tree of a string $T$ of length $n$ has $O(n)$ nodes and
edges, and the string label of each edge is encoded by a pair of positions in
$T$. Thus, even after the tree is built, the input text $T$ needs to be kept
stored and random access to $T$ is still needed. The linear-size suffix tries
(LSTs), proposed by Crochemore et al. [Linear-size suffix tries, TCS
638:171-178, 2016], are a `stand-alone' alternative to the suffix trees.
Namely, the LST of a string $T$ of length $n$ occupies $O(n)$ total space, and
supports pattern matching and other tasks in the same efficiency as the suffix
tree without the need to store the input text $T$. Crochemore et al. proposed
an offline algorithm which transforms the suffix tree of $T$ into the LST of
$T$ in $O(n \log \sigma)$ time and $O(n)$ space, where $\sigma$ is the alphabet
size. In this paper, we present two types of online algorithms which `directly'
construct the LST, from right to left, and from left to right, without
constructing the suffix tree as an intermediate structure. The right-to-left
construction algorithm works in $O(n \log \sigma)$ time and $O(n)$ space and
the left-to-right construction algorithm works in $O(n (\log \sigma + \log n /
\log \log n))$ time and $O(n)$ space.
</p></div>
    </summary>
    <updated>2019-01-30T23:30:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.10026</id>
    <link href="http://arxiv.org/abs/1901.10026" rel="alternate" type="text/html"/>
    <title>Heterogeneous Network Motifs</title>
    <feedworld_mtime>1548806400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rossi:Ryan_A=.html">Ryan A. Rossi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmed:Nesreen_K=.html">Nesreen K. Ahmed</a>, Aldo Carranza, David Arbour, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rao:Anup.html">Anup Rao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Sungchul.html">Sungchul Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koh:Eunyee.html">Eunyee Koh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.10026">PDF</a><br/><b>Abstract: </b>Many real-world applications give rise to large heterogeneous networks where
nodes and edges can be of any arbitrary type (e.g., user, web page, location).
Special cases of such heterogeneous graphs include homogeneous graphs,
bipartite, k-partite, signed, labeled graphs, among many others. In this work,
we generalize the notion of network motifs to heterogeneous networks. In
particular, small induced typed subgraphs called typed graphlets (heterogeneous
network motifs) are introduced and shown to be the fundamental building blocks
of complex heterogeneous networks. Typed graphlets are a powerful
generalization of the notion of graphlet (network motif) to heterogeneous
networks as they capture both the induced subgraph of interest and the types
associated with the nodes in the induced subgraph. To address this problem, we
propose a fast, parallel, and space-efficient framework for counting typed
graphlets in large networks. We discover the existence of non-trivial
combinatorial relationships between lower-order ($k-1$)-node typed graphlets
and leverage them for deriving many of the $k$-node typed graphlets in $o(1)$
constant time. Thus, we avoid explicit enumeration of those typed graphlets.
Notably, the time complexity matches the best untyped graphlet counting
algorithm. The experiments demonstrate the effectiveness of the proposed
framework in terms of runtime, space-efficiency, parallel speedup, and
scalability as it is able to handle large-scale networks.
</p></div>
    </summary>
    <updated>2019-01-30T23:35:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1483</id>
    <link href="https://theorydish.blog/2019/01/29/deadline-approaching-2019-godel-prize/" rel="alternate" type="text/html"/>
    <title>Deadline approaching – 2019 Gödel Prize</title>
    <summary>A reminder that the February 15th deadline is approaching (see https://theorydish.blog/2018/12/18/2019-godel-prize/). It is safe to assume that the paper you are excited about was not nominated by others. Also, from personal experience, it is a great feeling to know that your nomination succeeded. Worse case, your (moderately) hard work you will earn you the privilege to complain about the committee’s stupidity (which, again from personal experience, could be satisfying as well 😉 )</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A reminder that the February 15th deadline is approaching (see <a href="https://theorydish.blog/2018/12/18/2019-godel-prize/" rel="nofollow">https://theorydish.blog/2018/12/18/2019-godel-prize/</a>). It is safe to assume that the paper you are excited about was not nominated by others. Also, from personal experience, it is a great feeling to know that your nomination succeeded. Worse case, your (moderately) hard work you will earn you the privilege to complain about the committee’s stupidity (which, again from personal experience, could be satisfying as well <img alt="&#x1F609;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" style="height: 1em;"/> )</p></div>
    </content>
    <updated>2019-01-29T23:25:08Z</updated>
    <published>2019-01-29T23:25:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-01-31T04:22:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15603</id>
    <link href="https://rjlipton.wordpress.com/2019/01/29/primes-and-polynomials/" rel="alternate" type="text/html"/>
    <title>Primes And Polynomials</title>
    <summary>A result on the prime divisors of polynomial values Cropped from source Issai Schur was a mathematician who obtained his doctorate over a hundred years ago. He was a student of the great group theorist Ferdinand Frobenius. Schur worked in various areas and proved many deep results, including some theorems in basic number theory. Today […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>A result on the prime divisors of polynomial values</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/01/issaischur.jpg"><img alt="" class="alignright wp-image-15604" height="180" src="https://rjlipton.files.wordpress.com/2019/01/issaischur.jpg?w=120&amp;h=180" width="120"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="https://books.google.com/books?id=XLNJDylP53QC&amp;pg=PA89&amp;lpg=PA89&amp;dq=Issai+Schur:+Ramanujan%E2%80%99s+German+Contemporary&amp;source=bl&amp;ots=JwM2fktiyS&amp;sig=ACfU3U0kPjUEsHm3z5xtZTpLYjEW89mc5Q&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwiG5pftg5TgAhWCiOAKHZTXAlEQ6AEwC3oECAQQAQ#v=onepage&amp;q=Issai%20Schur%3A%20Ramanujan%E2%80%99s%20German%20Contemporary&amp;f=false">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Issai Schur was a mathematician who obtained his doctorate over a hundred years ago. He was a student of the great group theorist Ferdinand Frobenius. Schur worked in various areas and proved many deep results, including some theorems in basic number theory.</p>
<p>
Today we discuss a nice lemma due to Schur. Actually, it’s a theorem.</p>
<p>
There are many things named after Schur. Wikipedia has compiled a <a href="https://en.wikipedia.org/wiki/List_of_things_named_after_Issai_Schur">list</a> of them:</p>
<blockquote><p><b> </b> <em> <i>Frobenius-Schur indicator Herz-Schur multiplier Jordan-Schur theorem Lehmer-Schur algorithm Schur algebra Schur class Schur complement method Schur complement Schur decomposition Schur functor Schur index Schur multiplier Schur number Schur orthogonality relations Schur polynomial Schur product theorem Schur test Schur-convex function Schur-Horn theorem Schur-Weyl duality Schur-Zassenhaus theorem Schur’s inequality Schur’s lemma (from Riemannian geometry) Schur’s lemma Schur’s property Schur’s theorem</i> </em>
</p></blockquote>
<p/><p>
This lists two Schur lemmas. But when you click on Wikipedia’s “Schur’s lemma” <a href="https://en.wikipedia.org/wiki/Schur's_lemma_(disambiguation)">page</a>, there are three Schur lemmas. No, wait—there are four, including one called “Schur’s test.” But the result we are interested in is classed as a <em>theorem</em>. Wikipedia’s single <a href="https://en.wikipedia.org/wiki/Schur's_theorem">page</a> for “Schur’s theorem” lists not just <em>five</em> but <em>six</em> Schur’s theorems. This is one higher than the count eventually reached in Monty Python’s famous “Spanish Inquisition” <a href="http://www.montypython.net/scripts/spanish.php">skit</a>. We want the sixth one, which is quite useful—like a lemma.</p>
<p>
</p><p/><h2> The Result </h2><p/>
<p/><p>
Here is Schur’s lemma—no, theorem—published in 1912.</p>
<blockquote><p><b>Theorem 1</b> <em> Let <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f(x)}"/> be a non-constant polynomial with integer coefficients. Then <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f}"/> has infinitely many prime divisors. </em>
</p></blockquote>
<p/><p>
Here <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> has infinitely many prime divisors means that the number of primes that arise as divisors of the values <img alt="{f(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(n)}"/> as <img alt="{n=0,1,2,3,\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D0%2C1%2C2%2C3%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=0,1,2,3,\dots}"/> is infinite. Note, this works even if the polynomial is reducible or contains some trivial factors. Thus 	</p>
<p align="center"><img alt="\displaystyle  f(x) = 10x^{3} + 100, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+10x%5E%7B3%7D+%2B+100%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(x) = 10x^{3} + 100, "/></p>
<p>is fine. As is <img alt="{x^{2}-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B2%7D-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^{2}-1}"/>.</p>
<p>
Schur’s theorem is quite fun, even just to prove. Here is a nice version from a <a href="https://pdfs.semanticscholar.org/5eb7/4363199754164fdf7bbc77925a98c1eb435b.pdf">paper</a> by Ram Murty, whose work on extensions of Euclid’s famous proof of the infinitude of primes we <a href="https://rjlipton.wordpress.com/2018/07/11/you-cannot-do-that/">covered</a> last summer. We follow Murty’s proof.</p>
<p>
<em>Proof:</em>  Suppose <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/> is an integral polynomial of least degree for which the statement fails, 	</p>
<p align="center"><img alt="\displaystyle  f(x) = a_{n}x^{n} + a_{n-1}x^{n-1} + \cdots + a_{1}x + a_{0}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+a_%7Bn%7Dx%5E%7Bn%7D+%2B+a_%7Bn-1%7Dx%5E%7Bn-1%7D+%2B+%5Ccdots+%2B+a_%7B1%7Dx+%2B+a_%7B0%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(x) = a_{n}x^{n} + a_{n-1}x^{n-1} + \cdots + a_{1}x + a_{0}, "/></p>
<p>where <img alt="{a_n &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_n+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_n &gt; 0}"/>. If <img alt="{a_{0}=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7B0%7D%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_{0}=0}"/>, then <img alt="{f(x)/x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%2Fx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)/x}"/> is an integral polynomial of lower degree for which it must fail, so we have <img alt="{a_{0}\neq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7B0%7D%5Cneq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_{0}\neq 0}"/>. By hypothesis, <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> has only finitely many prime divisors, which we can represent as <img alt="{p_{1}, \dots, p_{r}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7B1%7D%2C+%5Cdots%2C+p_%7Br%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{1}, \dots, p_{r}}"/>. For each natural number <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/>, define <img alt="{P = a_0 \cdot p_1 \cdots p_r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP+%3D+a_0+%5Ccdot+p_1+%5Ccdots+p_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P = a_0 \cdot p_1 \cdots p_r}"/> and </p>
<p align="center"><img alt="\displaystyle  Q_{m} = f(P^{m}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q_%7Bm%7D+%3D+f%28P%5E%7Bm%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q_{m} = f(P^{m}). "/></p>
<p>Now <img alt="{P &gt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P &gt; 1}"/> because <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/> can take the value <img alt="{+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+1}"/> or <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/> only finitely often, and all sufficiently large <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> give <img alt="{Q_m &gt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_m+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_m &gt; 1}"/> for the same reason. Because <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> includes <img alt="{a_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_0}"/> as a factor, <img alt="{Q_m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_m}"/> is divisible by <img alt="{a_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_0}"/>, giving </p>
<p align="center"><img alt="\displaystyle  f(P^m) = a_0(K + 1) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28P%5Em%29+%3D+a_0%28K+%2B+1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(P^m) = a_0(K + 1) "/></p>
<p>for some integer <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> that by choice of <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> is divisible by all of <img alt="{p_1,\dots,p_r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_1%2C%5Cdots%2Cp_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_1,\dots,p_r}"/>. But then as in Euclid’s proof, <img alt="{K+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K+1}"/> is co-prime to all those primes, so it must furnish a prime divisor of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> that is not among them. This is a contradiction. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
This proof was simple but clever. Here is a concrete version for the polynomial <img alt="{x^{2} + a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B2%7D+%2B+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^{2} + a}"/>, which may help in understanding the proof: Say <img alt="{f(x)=x^{2} + a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%3Dx%5E%7B2%7D+%2B+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)=x^{2} + a}"/> is divisible by only <img alt="{p_{1},\dots,p_{r}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7B1%7D%2C%5Cdots%2Cp_%7Br%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{1},\dots,p_{r}}"/> say. Let <img alt="{P=p_{1} \cdots p_{r}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%3Dp_%7B1%7D+%5Ccdots+p_%7Br%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P=p_{1} \cdots p_{r}.}"/> The trick is to look at 	</p>
<p align="center"><img alt="\displaystyle  f(aPt) = (aPt)^{2} + a = ag(t), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28aPt%29+%3D+%28aPt%29%5E%7B2%7D+%2B+a+%3D+ag%28t%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(aPt) = (aPt)^{2} + a = ag(t), "/></p>
<p>where 	</p>
<p align="center"><img alt="\displaystyle  g(t) = a(Pt)^{2} + 1. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28t%29+%3D+a%28Pt%29%5E%7B2%7D+%2B+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(t) = a(Pt)^{2} + 1. "/></p>
<p>For some <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> large enough there exists a prime <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> that divides <img alt="{g(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(n)}"/>. But this prime divides <img alt="{f(aPn)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28aPn%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(aPn)}"/> and so <img alt="{p = p_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3D+p_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p = p_{i}}"/> for some <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>. But then <img alt="{g(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(n)}"/> modulo <img alt="{p_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{i}}"/> is equal to <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> which is a contradiction.</p>
<p>
</p><p/><h2> An Application </h2><p/>
<p/><p>
As the proof hints, Schur’s theorem is a proper extension of Euclid’s, which is just the case <img alt="{f(x) = x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%3D+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x) = x}"/>. Here is a less-obvious application:</p>
<blockquote><p><b>Theorem 2</b> <em> Suppose that <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f(x)}"/> is a polynomial with integer coefficients. Assume that <img alt="{f(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f(n)}"/> is a perfect square for all integers <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> large enough. Then <img alt="{f(x)=g(x)^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%3Dg%28x%29%5E%7B2%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f(x)=g(x)^{2}}"/> for some polynomial <img alt="{g(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{g(x)}"/>. </em>
</p></blockquote>
<p/><p>
There are many proofs of this theorem. One uses the famous Hilbert Irreducibility theorem, which we also <a href="https://rjlipton.wordpress.com/2018/06/08/hilberts-irreducibility-theorem/">discussed</a> last summer. Another proof uses Schur’s lemma and the fact that if <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/> and <img alt="{g(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(x)}"/> are products of irreducible integer polynomials that are collectively distinct then the ideal generated by <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> and <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> contains a positive integer—namely, their <a href="https://en.wikipedia.org/wiki/Resultant">resultant</a> <img alt="{R(f,g)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28f%2Cg%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(f,g)}"/>. Again following Murty’s paper: </p>
<p>
<em>Proof:</em>  We can factor <img alt="{f(x)=g(x)^{2}h(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%3Dg%28x%29%5E%7B2%7Dh%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)=g(x)^{2}h(x)}"/>, where <img alt="{h(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h(x)}"/> is a product of irreducible integer polynomials, and the goal is to show that <img alt="{h(x) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h(x) = 1}"/>. If <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> has positive degree, then by Schur’s theorem, there are infinitely many prime divisors <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> of <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/>. The square-freeness of <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> implies that it shares no factors with its derivative <img alt="{h'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h'}"/>, so <img alt="{r = R(h,h') &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+R%28h%2Ch%27%29+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = R(h,h') &gt; 0}"/>. We need only take <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> dividing <img alt="{h(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h(n)}"/> for some <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> large enough that <img alt="{f(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(n)}"/> is a perfect square but such that <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> does not divide <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>. Then the order of <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> dividing <img alt="{f(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(n)}"/> must be even, which implies that the order of <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> dividing <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> must be even. So <img alt="{p^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p^2}"/> divides <img alt="{h(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h(n)}"/>, but that implies that <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> divides <img alt="{h'(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%27%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h'(n)}"/>. Since <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> belongs to the ideal generated by <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> and <img alt="{h'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h'}"/> in <img alt="{\mathbb{Z}[x]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%5Bx%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}[x]}"/>, <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> divides <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> too, but this contradicts the choice of <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/>. So <img alt="{h(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h(x)}"/> must be a constant. Since the notion of “irreducible” with <img alt="{\mathbb{Z}[x]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%5Bx%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}[x]}"/> applies to constants, any square dividing <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> is already part of <img alt="{g(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(x)}"/>, so <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> must be a product of distinct primes. This forestalls <img alt="{p^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p^2}"/> dividing <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> in the above, so we must have <img alt="{h = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h = 1}"/>. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We did mention Schur’s long list of things named after him. Did people name things after others more often years ago? Or is naming them still common-place?</p>
<p/></font></font></div>
    </content>
    <updated>2019-01-29T22:45:22Z</updated>
    <published>2019-01-29T22:45:22Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Oldies"/>
    <category term="primes"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="David Hilbert"/>
    <category term="Euclid's argument"/>
    <category term="Issai Schur"/>
    <category term="polynomials"/>
    <category term="Ram Murty"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-01-31T04:21:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/01/29/simplifying-task-milestone</id>
    <link href="https://11011110.github.io/blog/2019/01/29/simplifying-task-milestone.html" rel="alternate" type="text/html"/>
    <title>Simplifying task-milestone diagrams</title>
    <summary>In my graph algorithms class last week, I covered critical path scheduling, as motivation for the linear-time algorithms for computing shortest and longest paths in directed acyclic graphs. In this scheduling problem, you are given a system of tasks, each with a predicted time to perform it, and constraints that some tasks should be done before others. Assuming that you have enough people working together to perform tasks in parallel, how quickly can you get everything done?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In my graph algorithms class last week, I covered critical path scheduling, as motivation for the linear-time algorithms for computing shortest and longest paths in directed acyclic graphs. In this scheduling problem, you are given a system of tasks, each with a predicted time to perform it, and constraints that some tasks should be done before others. Assuming that you have enough people working together to perform tasks in parallel, how quickly can you get everything done?</p>

<p>An optimal solution can be found by scheduling each task to start at a time given by the longest sequence of tasks leading up to it such that each consecutive pair in the sequence must be performed sequentially. The total length of the resulting schedule equals the length of the <a href="https://en.wikipedia.org/wiki/Critical_path_method">critical path</a>, the longest sequence of sequential tasks in the whole system. The example below could represent subtasks of a software project: design, implement, and test the software (A, B, and C), develop test cases (D), and document the results (E). Its critical path could be one of ABC, DE, AE, or DC, depending on the lengths of each task.</p>

<p style="text-align: center;"><img alt="Five tasks with ordering constraints" src="https://11011110.github.io/blog/assets/2019/pert1.svg"/></p>

<p>It’s easy enough to solve this problem directly, but to convert it to a path problem we need to have lengths on edges rather than times on vertices. It’s also convenient to have a single starting vertex for all the paths.
Therefore, <a href="https://en.wikipedia.org/wiki/Critical_path_method">Wikipedia’s article on the critical path method</a> uses a different kind of graph, which I’ll call a “task-milestone diagram” to distinguish it from the one above.
In this diagram, the vertices represent milestones, single points in time. The tasks to be performed are represented by edges, and the time to perform a task becomes the length of its edge. The goal of the scheduling problem now becomes one of choosing a time for each milestone, with enough time between pairs of milestones to perform each task.
The longest-path schedule, in which we place each milestone at a time given by the longest path to it from the start milestone, solves this problem optimally.</p>

<p>To convert a system of tasks and ordering constraints (without milestones) into an equivalent task-milestone diagram, make two new milestones for each task, one for when it starts and one for when it ends. Turn each task into an edge between its two milestones. Transform each ordering constraint (saying that task X should be performed earlier than task Y) into a length-zero edge from the end of task X to the beginning of task Y. Add two more milestones, for the start and end of the whole project. And add more length-zero edges from the project start to the start of each task that has no predecessors, and from the end of each task that has no successors to the project end milestone.</p>

<p style="text-align: center;"><img alt="Expanding each task vertex into two milestone vertices connected by a task edge" src="https://11011110.github.io/blog/assets/2019/pert2.svg"/></p>

<p>In the resulting graph, the paths from the start to the end milestone consist of the same sequences of tasks as we had before: ABC, DE, AE, or DC, separated by length-zero edges.
And for computational purposes the expansion doesn’t blow up the input size enough to cause any problems. But if we want to use this diagram for visualizing the resulting schedule, it’s a little confusing because of all of those extra length-zero edges. They don’t really represent tasks; they’re just there to make sure that the paths connect the tasks in the correct order. Do we really need so many of them?</p>

<p>There are a couple of simple rules that can be used to reduce the number of length-zero edges:</p>

<ul>
  <li>If two vertices both have only zero-length edges going out of them, and both have the same set of outgoing neighbors, they can be merged into a single vertex. Symmetrically, if two vertices both have only zero-length edges coming into them, and both have the same set of incoming neighbors, they can be merged.</li>
  <li>If a vertex has only one edge going out of it, of length zero, it can be merged with its outgoing neighbor. Symmetrically, if a vertex has only one edge coming into it, of length zero, it can be merged with its incoming neighbor.</li>
</ul>

<p>For instance, the first rule will merge the milestones for the starts of tasks A and D, and the second rule will merge the resulting vertex into the start vertex. Repeatedly applying these rules produces the following simplified graph. Note that its start-to-end paths, ABC, DE, AE, and DC, are exactly the same as the potential critical paths that we started with.</p>

<p style="text-align: center;"><img alt="Path-preserving simplification of the task-milestone diagram" src="https://11011110.github.io/blog/assets/2019/pert3.svg"/></p>

<p>If we assume that the task edges all have non-negative length (as they do in the scheduling application) and that we only care about longest paths, there are even more simplifications that we can perform. These ones might change the set of all paths in the graph (as identified by their sequences of tasks) but they preserve the identities of the longest paths:</p>

<ul>
  <li>
    <p>If a zero-length edge goes from task X to task Y, and the graph contains another path between the same two tasks, remove the edge.</p>
  </li>
  <li>
    <p>If a zero-length edge goes from task X to Y, every other edge into Y or out of X has length zero, and every incoming neighbor of task Y has a path to every outgoing neighbor of task X, then merge X and Y into a single vertex.</p>
  </li>
</ul>

<p>In our example, the bottom edge meets the conditions of the second rule. Applying that rule produces an even simpler task-milestone diagram:</p>

<p style="text-align: center;"><img alt="Additional simplification preserving longest paths" src="https://11011110.github.io/blog/assets/2019/pert4.svg"/></p>

<p>Our example has no instances of the first rule, but when it is used it removes paths from the graph. The removed paths can never be longest paths, because any path through the removed edge can be made longer by replacing that edge by a different path from X to Y.
When we perform the second rule, we may introduce new paths that were not already present, from a predecessor of Y, through the merged vertex, to a successor of X. For instance, the new graph has a path through only the two tasks AC, which was not one of the four paths we started with. But because these new paths replace portions of existing paths by two length-zero edges, they can never be the longest path, and the resulting compacted diagram can safely be used for scheduling.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/101502692118389818">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-01-29T16:22:00Z</updated>
    <published>2019-01-29T16:22:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-01-30T00:50:41Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-6555947.post-6062056358171439265</id>
    <link href="http://blog.geomblog.org/feeds/6062056358171439265/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.geomblog.org/2019/01/fat-session-2-systems-and-measurement.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/6555947/posts/default/6062056358171439265" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/6555947/posts/default/6062056358171439265" rel="self" type="application/atom+xml"/>
    <link href="http://feedproxy.google.com/~r/TheGeomblog/~3/G-UqSGr3bSg/fat-session-2-systems-and-measurement.html" rel="alternate" type="text/html"/>
    <title>FAT* Session 2: Systems and Measurement.</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Building systems that have fairness properties and monitoring systems that do A/B testing on us.<br/><br/><a href="https://algorithmicfairness.wordpress.com/2019/01/28/fat-papers-systems-and-measurement/">Session 2 of FAT*</a>: my opinionated summary.<img alt="" height="1" src="http://feeds.feedburner.com/~r/TheGeomblog/~4/G-UqSGr3bSg" width="1"/></div>
    </content>
    <updated>2019-01-29T06:48:00Z</updated>
    <published>2019-01-29T06:48:00Z</published><feedburner:origlink xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">http://blog.geomblog.org/2019/01/fat-session-2-systems-and-measurement.html</feedburner:origlink>
    <author>
      <name>Suresh Venkatasubramanian</name>
      <email>noreply@blogger.com</email>
      <uri>https://plus.google.com/112165457714968997350</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-6555947</id>
      <category term="research"/>
      <category term="community"/>
      <category term="miscellaneous"/>
      <category term="soda"/>
      <category term="conferences"/>
      <category term="data-mining"/>
      <category term="socg"/>
      <category term="blogosphere"/>
      <category term="publishing"/>
      <category term="clustering"/>
      <category term="teaching"/>
      <category term="jobs"/>
      <category term="funding"/>
      <category term="humor"/>
      <category term="awards"/>
      <category term="outreach"/>
      <category term="stoc"/>
      <category term="cs.CG"/>
      <category term="focs"/>
      <category term="nsf"/>
      <category term="reviewing"/>
      <category term="socg-2010"/>
      <category term="fairness"/>
      <category term="academy"/>
      <category term="latex"/>
      <category term="stoc2017"/>
      <category term="theoryfest"/>
      <category term="workshops"/>
      <category term="acm"/>
      <category term="writing"/>
      <category term="conf-blogs"/>
      <category term="cs.DS"/>
      <category term="cs.LG"/>
      <category term="geometry"/>
      <category term="p-vs-nc"/>
      <category term="advising"/>
      <category term="sabbatical"/>
      <category term="simons foundation"/>
      <category term="announcement"/>
      <category term="big-data"/>
      <category term="deadline"/>
      <category term="jeff phillips"/>
      <category term="streaming"/>
      <category term="books"/>
      <category term="large-data"/>
      <category term="p-vs-np"/>
      <category term="cra"/>
      <category term="cstheory"/>
      <category term="focs2010"/>
      <category term="icdm"/>
      <category term="math.PR"/>
      <category term="memorial"/>
      <category term="personal"/>
      <category term="posters"/>
      <category term="potd"/>
      <category term="rajeev motwani"/>
      <category term="shonan"/>
      <category term="socg2012"/>
      <category term="software"/>
      <category term="stoc2012"/>
      <category term="GIA"/>
      <category term="SDM"/>
      <category term="alenex"/>
      <category term="alenex2011"/>
      <category term="arxiv"/>
      <category term="career"/>
      <category term="complexity"/>
      <category term="cs.CC"/>
      <category term="deolalikar"/>
      <category term="distributions"/>
      <category term="madalgo"/>
      <category term="nips"/>
      <category term="sdm2011"/>
      <category term="shape"/>
      <category term="talks"/>
      <category term="technology"/>
      <category term="theory.SE"/>
      <category term="travel"/>
      <category term="video"/>
      <category term="8f-cg"/>
      <category term="DBR"/>
      <category term="ICS"/>
      <category term="LISPI"/>
      <category term="acceptances"/>
      <category term="bibtex"/>
      <category term="bregman"/>
      <category term="cfp"/>
      <category term="clustering-book"/>
      <category term="column"/>
      <category term="combinatorial geometry"/>
      <category term="current-distance"/>
      <category term="ecml-pkdd"/>
      <category term="empirical"/>
      <category term="esa"/>
      <category term="focs2012"/>
      <category term="focs2014"/>
      <category term="fwcg"/>
      <category term="game theory"/>
      <category term="godel"/>
      <category term="graphs"/>
      <category term="implementation"/>
      <category term="journals"/>
      <category term="kernels"/>
      <category term="misc"/>
      <category term="models"/>
      <category term="obituary"/>
      <category term="productivity"/>
      <category term="programming"/>
      <category term="society"/>
      <category term="soda2011"/>
      <category term="topology"/>
      <category term="turing"/>
      <category term="tv"/>
      <category term="women-in-theory"/>
      <category term=".02"/>
      <category term="IMA"/>
      <category term="MOOC"/>
      <category term="PPAD"/>
      <category term="accountability"/>
      <category term="active-learning"/>
      <category term="aggregator"/>
      <category term="algorithms"/>
      <category term="ams"/>
      <category term="analco"/>
      <category term="barriers"/>
      <category term="beamer"/>
      <category term="blogging"/>
      <category term="candes"/>
      <category term="civil rights"/>
      <category term="classification"/>
      <category term="coding-theory"/>
      <category term="coffee"/>
      <category term="conjecture"/>
      <category term="cosmos"/>
      <category term="counting"/>
      <category term="cricket"/>
      <category term="cs.DC"/>
      <category term="dagstuhl"/>
      <category term="databuse"/>
      <category term="dimacs"/>
      <category term="dimensionality-reduction"/>
      <category term="distributed-learning"/>
      <category term="double-blind review"/>
      <category term="duality"/>
      <category term="eda"/>
      <category term="embarrassing"/>
      <category term="expanders"/>
      <category term="experiments"/>
      <category term="fake-news"/>
      <category term="fat*"/>
      <category term="fatml"/>
      <category term="fellowships"/>
      <category term="focs2013"/>
      <category term="fonts"/>
      <category term="gct"/>
      <category term="ggplot"/>
      <category term="gpu"/>
      <category term="graph minors"/>
      <category term="gt.game-theory"/>
      <category term="guest-post"/>
      <category term="guitar"/>
      <category term="hangouts"/>
      <category term="hirsch"/>
      <category term="history"/>
      <category term="ipe"/>
      <category term="ita"/>
      <category term="jmm"/>
      <category term="k-12"/>
      <category term="knuth"/>
      <category term="machine-learning"/>
      <category term="massive"/>
      <category term="math.ST"/>
      <category term="media"/>
      <category term="memes"/>
      <category term="metoo"/>
      <category term="metrics"/>
      <category term="morris"/>
      <category term="movies"/>
      <category term="multicore"/>
      <category term="music"/>
      <category term="narrative"/>
      <category term="networks"/>
      <category term="nih"/>
      <category term="parallelism"/>
      <category term="partha niyogi"/>
      <category term="polymath"/>
      <category term="polymath research"/>
      <category term="polytopes"/>
      <category term="postdocs"/>
      <category term="privacy"/>
      <category term="quant-ph"/>
      <category term="quantum"/>
      <category term="randomness"/>
      <category term="review"/>
      <category term="sampling"/>
      <category term="seminars"/>
      <category term="social-networking"/>
      <category term="soda2014"/>
      <category term="students"/>
      <category term="sublinear"/>
      <category term="submissions"/>
      <category term="summer-school"/>
      <category term="superbowl"/>
      <category term="surveys"/>
      <category term="svn"/>
      <category term="television"/>
      <category term="traffic"/>
      <category term="twitter"/>
      <category term="utah"/>
      <category term="wads"/>
      <category term="white elephant"/>
      <category term="xkcd"/>
      <author>
        <name>Suresh Venkatasubramanian</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="http://blog.geomblog.org/" rel="alternate" type="text/html"/>
      <link href="http://www.blogger.com/feeds/6555947/posts/default?alt=atom&amp;start-index=26&amp;max-results=25&amp;redirect=false" rel="next" type="application/atom+xml"/>
      <link href="http://feeds.feedburner.com/TheGeomblog" rel="self" type="application/atom+xml"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <subtitle>Ruminations on computational geometry, algorithms, theoretical computer science and life</subtitle>
      <title>The Geomblog</title>
      <updated>2019-01-29T06:48:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=334</id>
    <link href="https://tcsplus.wordpress.com/2019/01/28/tcs-talk-wednesday-february-6-ran-canetti-bu-and-tau/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, February 6 — Ran Canetti, BU and TAU</title>
    <summary>The new season of TCS+ is about to start! Our first talk for Spring will take place next Wednesday, February 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Ran Canetti from BU and TAU will speak about “Fully Bideniable Interactive Encryption” (abstract below). Please make sure you […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The new season of TCS+ is about to start! Our first talk for Spring will take place next Wednesday, February 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Ran Canetti</strong> from BU and TAU will speak about “<em>Fully Bideniable Interactive Encryption</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: While standard encryption guarantees secrecy of the encrypted plaintext only against an attacker that has no knowledge of the communicating parties’ keys and randomness of encryption, deniable encryption [Canetti et al., Crypto’96] provides the additional guarantee that the plaintext remains secret even in face of entities that attempt to coerce (or bribe) the communicating parties to expose their internal states, including the plaintexts, keys and randomness. To achieve this guarantee, deniable encryption equips the parties with faking algorithms which allow them to generate fake keys and randomness that make the ciphertext appear consistent with any plaintext of the parties’ choice. To date, however, only partial results were known: Either deniability against coercing only the sender, or against coercing only the receiver [Sahai-Waters, STOC ‘14] or schemes satisfying weaker notions of deniability [O’Neil et al., Crypto ‘11].</p>
<p>In this paper we present the first fully bideniable interactive encryption scheme, thus resolving the 20-years-old open problem. Our scheme also provides an additional and new guarantee: Even if the sender claims that one plaintext was used and the receiver claims a different one, the adversary has no way of figuring out who is lying – the sender, the receiver, or both. This property, which we call off-the-record deniability, is useful when the parties don’t have means to agree on what fake plaintext to claim, or when one party defects against the other. Our protocol has three messages, which is optimal [Bendlin et al., Asiacrypt’11], and needs a globally available reference string. We assume subexponential indistinguishability obfuscation (IO) and one-way functions.</p>
<p>Joint work with Sunoo Park and Oxana Poburinnaya.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2019-01-28T16:23:59Z</updated>
    <published>2019-01-28T16:23:59Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-01-31T04:22:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/01/28/first-airoyoung-phd-school/</id>
    <link href="https://cstheory-events.org/2019/01/28/first-airoyoung-phd-school/" rel="alternate" type="text/html"/>
    <title>First AIROYoung PhD School</title>
    <summary>March 26-29, 2019 Rome, Italy https://workshop.airoyoung.org/2019#phd-school This is a three-days PhD School with theoretical classes and lab sessions on cutting-edge topics arising in Optimization and Simulation</summary>
    <updated>2019-01-28T11:59:55Z</updated>
    <published>2019-01-28T11:59:55Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-01-31T04:22:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-6555947.post-8169216749697822290</id>
    <link href="http://blog.geomblog.org/feeds/8169216749697822290/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.geomblog.org/2019/01/fat-blogging.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/6555947/posts/default/8169216749697822290" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/6555947/posts/default/8169216749697822290" rel="self" type="application/atom+xml"/>
    <link href="http://feedproxy.google.com/~r/TheGeomblog/~3/TROyy-Os39k/fat-blogging.html" rel="alternate" type="text/html"/>
    <title>FAT* blogging</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I'll be blogging about each session of papers from the FAT* Conference. So as not to clutter your feed, the posts will be housed at the fairness blog that I co-write along with Sorelle Friedler and Carlos Scheidegger.<br/><br/>The first post is on <a href="https://algorithmicfairness.wordpress.com/2019/01/27/fat-papers-framing-and-abstraction/">Session 1: Framing and Abstraction</a>.<img alt="" height="1" src="http://feeds.feedburner.com/~r/TheGeomblog/~4/TROyy-Os39k" width="1"/></div>
    </content>
    <updated>2019-01-28T04:09:00Z</updated>
    <published>2019-01-28T04:09:00Z</published>
    <category scheme="http://www.blogger.com/atom/ns#" term="conf-blogs"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="fat*"/><feedburner:origlink xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">http://blog.geomblog.org/2019/01/fat-blogging.html</feedburner:origlink>
    <author>
      <name>Suresh Venkatasubramanian</name>
      <email>noreply@blogger.com</email>
      <uri>https://plus.google.com/112165457714968997350</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-6555947</id>
      <category term="research"/>
      <category term="community"/>
      <category term="miscellaneous"/>
      <category term="soda"/>
      <category term="conferences"/>
      <category term="data-mining"/>
      <category term="socg"/>
      <category term="blogosphere"/>
      <category term="publishing"/>
      <category term="clustering"/>
      <category term="teaching"/>
      <category term="jobs"/>
      <category term="funding"/>
      <category term="humor"/>
      <category term="awards"/>
      <category term="outreach"/>
      <category term="stoc"/>
      <category term="cs.CG"/>
      <category term="focs"/>
      <category term="nsf"/>
      <category term="reviewing"/>
      <category term="socg-2010"/>
      <category term="fairness"/>
      <category term="academy"/>
      <category term="latex"/>
      <category term="stoc2017"/>
      <category term="theoryfest"/>
      <category term="workshops"/>
      <category term="acm"/>
      <category term="writing"/>
      <category term="conf-blogs"/>
      <category term="cs.DS"/>
      <category term="cs.LG"/>
      <category term="geometry"/>
      <category term="p-vs-nc"/>
      <category term="advising"/>
      <category term="sabbatical"/>
      <category term="simons foundation"/>
      <category term="announcement"/>
      <category term="big-data"/>
      <category term="deadline"/>
      <category term="jeff phillips"/>
      <category term="streaming"/>
      <category term="books"/>
      <category term="large-data"/>
      <category term="p-vs-np"/>
      <category term="cra"/>
      <category term="cstheory"/>
      <category term="focs2010"/>
      <category term="icdm"/>
      <category term="math.PR"/>
      <category term="memorial"/>
      <category term="personal"/>
      <category term="posters"/>
      <category term="potd"/>
      <category term="rajeev motwani"/>
      <category term="shonan"/>
      <category term="socg2012"/>
      <category term="software"/>
      <category term="stoc2012"/>
      <category term="GIA"/>
      <category term="SDM"/>
      <category term="alenex"/>
      <category term="alenex2011"/>
      <category term="arxiv"/>
      <category term="career"/>
      <category term="complexity"/>
      <category term="cs.CC"/>
      <category term="deolalikar"/>
      <category term="distributions"/>
      <category term="madalgo"/>
      <category term="nips"/>
      <category term="sdm2011"/>
      <category term="shape"/>
      <category term="talks"/>
      <category term="technology"/>
      <category term="theory.SE"/>
      <category term="travel"/>
      <category term="video"/>
      <category term="8f-cg"/>
      <category term="DBR"/>
      <category term="ICS"/>
      <category term="LISPI"/>
      <category term="acceptances"/>
      <category term="bibtex"/>
      <category term="bregman"/>
      <category term="cfp"/>
      <category term="clustering-book"/>
      <category term="column"/>
      <category term="combinatorial geometry"/>
      <category term="current-distance"/>
      <category term="ecml-pkdd"/>
      <category term="empirical"/>
      <category term="esa"/>
      <category term="focs2012"/>
      <category term="focs2014"/>
      <category term="fwcg"/>
      <category term="game theory"/>
      <category term="godel"/>
      <category term="graphs"/>
      <category term="implementation"/>
      <category term="journals"/>
      <category term="kernels"/>
      <category term="misc"/>
      <category term="models"/>
      <category term="obituary"/>
      <category term="productivity"/>
      <category term="programming"/>
      <category term="society"/>
      <category term="soda2011"/>
      <category term="topology"/>
      <category term="turing"/>
      <category term="tv"/>
      <category term="women-in-theory"/>
      <category term=".02"/>
      <category term="IMA"/>
      <category term="MOOC"/>
      <category term="PPAD"/>
      <category term="accountability"/>
      <category term="active-learning"/>
      <category term="aggregator"/>
      <category term="algorithms"/>
      <category term="ams"/>
      <category term="analco"/>
      <category term="barriers"/>
      <category term="beamer"/>
      <category term="blogging"/>
      <category term="candes"/>
      <category term="civil rights"/>
      <category term="classification"/>
      <category term="coding-theory"/>
      <category term="coffee"/>
      <category term="conjecture"/>
      <category term="cosmos"/>
      <category term="counting"/>
      <category term="cricket"/>
      <category term="cs.DC"/>
      <category term="dagstuhl"/>
      <category term="databuse"/>
      <category term="dimacs"/>
      <category term="dimensionality-reduction"/>
      <category term="distributed-learning"/>
      <category term="double-blind review"/>
      <category term="duality"/>
      <category term="eda"/>
      <category term="embarrassing"/>
      <category term="expanders"/>
      <category term="experiments"/>
      <category term="fake-news"/>
      <category term="fat*"/>
      <category term="fatml"/>
      <category term="fellowships"/>
      <category term="focs2013"/>
      <category term="fonts"/>
      <category term="gct"/>
      <category term="ggplot"/>
      <category term="gpu"/>
      <category term="graph minors"/>
      <category term="gt.game-theory"/>
      <category term="guest-post"/>
      <category term="guitar"/>
      <category term="hangouts"/>
      <category term="hirsch"/>
      <category term="history"/>
      <category term="ipe"/>
      <category term="ita"/>
      <category term="jmm"/>
      <category term="k-12"/>
      <category term="knuth"/>
      <category term="machine-learning"/>
      <category term="massive"/>
      <category term="math.ST"/>
      <category term="media"/>
      <category term="memes"/>
      <category term="metoo"/>
      <category term="metrics"/>
      <category term="morris"/>
      <category term="movies"/>
      <category term="multicore"/>
      <category term="music"/>
      <category term="narrative"/>
      <category term="networks"/>
      <category term="nih"/>
      <category term="parallelism"/>
      <category term="partha niyogi"/>
      <category term="polymath"/>
      <category term="polymath research"/>
      <category term="polytopes"/>
      <category term="postdocs"/>
      <category term="privacy"/>
      <category term="quant-ph"/>
      <category term="quantum"/>
      <category term="randomness"/>
      <category term="review"/>
      <category term="sampling"/>
      <category term="seminars"/>
      <category term="social-networking"/>
      <category term="soda2014"/>
      <category term="students"/>
      <category term="sublinear"/>
      <category term="submissions"/>
      <category term="summer-school"/>
      <category term="superbowl"/>
      <category term="surveys"/>
      <category term="svn"/>
      <category term="television"/>
      <category term="traffic"/>
      <category term="twitter"/>
      <category term="utah"/>
      <category term="wads"/>
      <category term="white elephant"/>
      <category term="xkcd"/>
      <author>
        <name>Suresh Venkatasubramanian</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="http://blog.geomblog.org/" rel="alternate" type="text/html"/>
      <link href="http://www.blogger.com/feeds/6555947/posts/default?alt=atom&amp;start-index=26&amp;max-results=25&amp;redirect=false" rel="next" type="application/atom+xml"/>
      <link href="http://feeds.feedburner.com/TheGeomblog" rel="self" type="application/atom+xml"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <subtitle>Ruminations on computational geometry, algorithms, theoretical computer science and life</subtitle>
      <title>The Geomblog</title>
      <updated>2019-01-29T06:48:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=600</id>
    <link href="https://emanueleviola.wordpress.com/2019/01/27/selling-your-town-to-the-marijuana-industry/" rel="alternate" type="text/html"/>
    <title>Selling your town to the marijuana industry</title>
    <summary>I vowed to quit with marijuana, but I just can’t.  It’s addictive. We can go back to 2016, when voters were hit with legalese that can only be described as a trap.  Basically, under the mask of legalizing the consumption of marijuana, the ballot question was really about opening recreational pot shops around the corner.  […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: justify;">I vowed to quit with marijuana, but I just can’t.  It’s addictive.</p>
<p style="text-align: justify;">We can go back to 2016, when voters were hit with legalese that can only be described as a trap.  Basically, under the mask of legalizing the consumption of marijuana, the ballot question was really about opening recreational pot shops around the corner.  No doubt many, many people voted for legalization without knowledge of this and with no desire to have pot shops in their town.  What exultation must have come from the lawyers working for the industry, when their masterstroke made it to the fine print:</p>
<h3 style="text-align: justify;">A town voting to legalize marijuana <del>may</del> MUST open pot shops.</h3>
<p style="text-align: justify;">At the same time, the administration of Newton changed.  Councilors who liked the place the way it is and wanted to protect it lost to others who wanted it more vibrant.  The new councilors and the new mayor sided with the marijuana industry.</p>
<p style="text-align: justify;">The way in which they eventually won is sinister.  The context was that everybody in Newton wants at least some restriction on the number of marijuana stores.  But don’t take my word for this claim: even the pro-pot councilors believe so, and in fact almost unanimously they put a question on the ballot about restricting the number of stores.  At the same time, many people in Newton wanted zero stores.  In another masterstroke of the saga, the councilors were able to put one group against the other.  They added another question about having zero stores, following a massive, grassroots petition which however should have put the question at a different time. Then they forced the people who wanted zero stores to vote against restricting the number of stores. This is genius.  Also, if it isn’t illegal I believe it should be.  And in perfect coup style, media outlets censored several pieces explaining the situation to the voters. The end result was what the administration had always wanted: no restriction on the number of stores. Ignore the alarms of the doctors, the police officers, and the people.  What do they know about what’s best for Newton? A joint is like a pint of beer!  Except beer does not give you permanent brain damage. Whatever, the bottom line is that the revenue will do good things for the city! Oh yes, the revenue.  Newton has 1 billion dollars in deficit.  You read well, 1 billion.  For decades we will have a fraction of the city budget wiped out to repay that. I guess they can say we are so desperately in debt that we should rake in every penny we can zone in town.  But I think a more accurate perspective is that even in their wildest dreams, cannabis sales won’t make a dent in that.  And maybe they should spend a couple of minutes thinking about the dozens of other ways we can bring money to the city without bringing the drugs.</p>
<p style="text-align: justify;">Executing their sophisticated plan cost in the neighborhood of $100k, mostly spent on a political strategy group which helped win the election.  To add insult to injury, key members of this marijuana combine, including the political strategists and those who funded them, don’t live in Newton but in towns where recreational pot stores are banned.  The marijuana combine is effectively carving out suburban Boston in areas where it’s good to live and areas where it’s good to sell pot.</p>
<p style="text-align: justify;">As is well known, nobody has any problem with legalizing marijuana consumption.  Moreover, there is absolutely no problem with buying this stuff over the internet, or stocking up at out-of-the-way stores.  Well, absolutely no problem except one.  The money wouldn’t go into the pockets of X, Y, and Z.</p></div>
    </content>
    <updated>2019-01-28T01:31:42Z</updated>
    <published>2019-01-28T01:31:42Z</published>
    <category term="Uncategorized"/>
    <category term="marijuana"/>
    <author>
      <name>Emanuele</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>By Emanuele Viola</subtitle>
      <title>Thoughts</title>
      <updated>2019-01-31T04:22:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/012</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/012" rel="alternate" type="text/html"/>
    <title>TR19-012 |  Multi-pseudodeterministic algorithms | 

	Oded Goldreich</title>
    <summary>In this work, dedicated to Shafi Goldwasser, we consider a relaxation of the notion of pseudodeterministic algorithms, which was put forward by Gat and Goldwasser ({\em ECCC}, TR11--136, 2011). 


Pseudodeterministic algorithms are randomized algorithms that solve search problems by almost always providing the same canonical solution (per each input). 
Multi-pseudodeterministic algorithms relax the former notion by allowing the algorithms to output one of a bounded number  of canonical solutions (per each input). 
We show that efficient multi-seudodeterministic algorithms can solve natural problems that are not solveable by efficient pseudodeterministic algorithms, present a composition theorem regarding multi-pseudodeterministic algorithms,
and relate them to other known notions.</summary>
    <updated>2019-01-27T17:25:14Z</updated>
    <published>2019-01-27T17:25:14Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-01-31T04:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/011</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/011" rel="alternate" type="text/html"/>
    <title>TR19-011 |  Sampling Graphs without Forbidden Subgraphs and Almost-Explicit Unbalanced Expanders | 

	Benny Applebaum, 

	Eliran Kachlon</title>
    <summary>We initiate the study of the following hypergraph sampling problem: Sample a $d$-uniform hypergraph over $n$ vertices and $m$ hyperedges from some pseudorandom distribution $\mathcal{G}$ conditioned on not having some small predefined $t$-size hypergraph $H$ as a subgraph. The algorithm should run in $\mathrm{poly}(n)$-time even when the size of the subgraph $H$ is super-constant.

We solve the problem by carefully designing a sampling algorithm for $k$-wise independent hypergraphs $\mathcal{G}$ that supports efficient testing for subgraph-freeness. We use our algorithm to obtain the first probabilistic construction of constant-degree polynomially-unbalanced expander graphs whose failure probability is negligible in $n$ (i.e., $n^{-\omega(1)}$). In particular, given constants $d&gt;c$, we output a bipartite graph that has $n$ left nodes, $n^c$ right nodes with right-degree of $d$ so that any right set of size at most $n^{\Omega(1)}$ expands by factor of $\Omega(d)$. This result is extended to the setting of unique expansion as well.

We argue that such an ``almost-explicit'' construction can be employed in many useful settings, and present applications in coding theory (batch codes and LDPC codes), pseudorandomness (low-bias generators and randomness extractors) and cryptography. Notably, we show that our constructions yield a collection of polynomial-stretch locally-computable cryptographic pseudorandom generators based on Goldreich's one-wayness assumption resolving a long-standing open problem.</summary>
    <updated>2019-01-27T17:16:23Z</updated>
    <published>2019-01-27T17:16:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-01-31T04:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/010</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/010" rel="alternate" type="text/html"/>
    <title>TR19-010 |   Stoquastic PCP vs. Randomness | 

	Alex Bredariol Grilo, 

	Dorit Aharonov</title>
    <summary>The derandomization of MA, the probabilistic version of NP, is a long standing open question. In this work, we connect this problem to a variant of another major problem: the quantum PCP conjecture. Our connection goes through the surprising quantum characterization of MA by Bravyi and Terhal. They proved the MA-completeness of the problem of deciding whether the groundenergy of a uniform stoquastic local Hamiltonian is zero or inverse polynomial. We show that the gapped version of this problem, i.e. deciding if a given uniform stoquastic local Hamiltonian is frustration-free or has energy at least some constant $\varepsilon$, is in NP. Thus, if there exists a gap-amplification procedure for uniform stoquastic Local Hamiltonians (in analogy to the gap amplification procedure for constraint satisfaction problems in the original PCP theorem), then MA = NP (and vice versa). Furthermore, if this gap amplification procedure exhibits some additional (natural) properties, then P = RP. We feel this work opens up a rich set of new directions to explore, which might lead to progress on both quantum PCP and derandomization. As a small side result, we also show that deciding if commuting stoquastic Hamiltonian is frustration free is in NP.</summary>
    <updated>2019-01-27T12:32:57Z</updated>
    <published>2019-01-27T12:32:57Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-01-31T04:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/009</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/009" rel="alternate" type="text/html"/>
    <title>TR19-009 |  The Fine-Grained Complexity of Strengthenings of First-Order Logic | 

	Jiawei Gao, 

	Russell Impagliazzo</title>
    <summary>The class of model checking for first-order formulas on sparse graphs has a complete problem with respect to fine-grained reductions, Orthogonal Vectors (OV) [GIKW17]. This paper studies extensions of this class or more lenient parameterizations. We consider classes obtained by allowing function symbols;
first-order on ordered structures; adding various notions of transitive closure operations; and stratifications of first-order properties by quantifier depth and variable complexity, rather than number of quantifiers. For some of these classes, OV is still a complete problem, in that significant improvement for the entire class is equivalent to significant improvement for OV algorithms.  For these classes, we can also use the improved OV algorithm of [AWY16, CW16] to get moderate improvements on algorithms for the entire class. For other classes, we show that model checking becomes harder than for first-order, under well-studied conjectures such as SETH.  For other classes, we show hardness follows from weaker assumptions than SETH. 

Surprisingly, whether an extension increases the complexity of model checking seems independent of whether it increases the expressive power of the logic. For example, adding function symbols does not change which problems are expressible by first-order, but does increase the time for model checking under SETH. On the other hand, adding an ordering does not change the fine-grained complexity
of model checking, although it increases the logic's expressive power.</summary>
    <updated>2019-01-27T12:24:10Z</updated>
    <published>2019-01-27T12:24:10Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-01-31T04:20:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-25562705.post-2559156841800187715</id>
    <link href="http://aaronsadventures.blogspot.com/feeds/2559156841800187715/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=2559156841800187715" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/2559156841800187715" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/2559156841800187715" rel="self" type="application/atom+xml"/>
    <link href="http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html" rel="alternate" type="text/html"/>
    <title>Algorithmic Unfairness Without Any Bias Baked In</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Discussion of (un)fairness in machine learning hit mainstream political discourse this week, when Representative Alexandria Ocasio-Cortez discussed the possibility of algorithmic bias, and was clumsily "called out" by Ryan Saavedra on twitter: <br/><center><blockquote class="twitter-tweet"><div dir="ltr" lang="en">Socialist Rep. Alexandria Ocasio-Cortez (D-NY) claims that algorithms, which are driven by math, are racist <a href="https://t.co/X2veVvAU1H">pic.twitter.com/X2veVvAU1H</a>— Ryan Saavedra (@RealSaavedra) <a href="https://twitter.com/RealSaavedra/status/1087627739861897216?ref_src=twsrc%5Etfw">January 22, 2019</a></div></blockquote></center>It was gratifying to see the number of responses pointing out how wrong he was --- awareness of algorithmic bias has clearly become pervasive! But most of the pushback focused on the possibility of bias being "baked in" by the designer of the algorithm, or because of latent bias embedded in the data, or both:  <br/><center><blockquote class="twitter-tweet"><div dir="ltr" lang="en">You know algorithms are written by people right? And that the data they are trained on is made and selected by people? And that the problems algorithms solve are decided...again...by people? And that people can be and many times are racist? Ok now you do the math</div>— kade (@onekade) <a href="https://twitter.com/onekade/status/1087853353000939521?ref_src=twsrc%5Etfw">January 22, 2019</a></blockquote></center>Bias in the data is certainly a problem, especially when labels are gathered by human beings. But its far from being the only problem. In this post, I want to walk through a very simple example in which the algorithm designer is being entirely reasonable, there are no human beings injecting bias into the labels, and yet the resulting outcome is "unfair".   Here is the (toy) scenario -- the specifics aren't important. High school students are applying to college, and each student has some innate "talent" $I$, which we will imagine is normally distributed, with mean 100 and standard deviation 15: $I \sim N(100,15)$. The college would like to admit students who are sufficiently talented --- say one standard deviation above the mean (so, it would like to admit students with $I \geq 115$). The problem is that talent isn't directly observable. Instead, the college can observe <i>grades</i> $g$ and <i>SAT scores $s$</i>, which are a noisy estimate of talent. For simplicity, lets imagine that both grades and SAT scores are independently and normally distributed, centered at a student's talent level, and also with standard deviation 15: $g \sim N(I, 15)$, $s \sim N(I, 15)$.<br/><br/>In this scenario, the college has a simple, optimal decision rule: It should run a linear regression to try and predict student talent from grades and SAT scores, and then it should admit the students whose <i>predicted </i>talent is at least 115. This is indeed "driven by math" --- since we assumed everything was normally distributed here, this turns out to correspond to the Bayesian optimal decision rule for the college.<br/><br/>Ok. Now lets suppose there are two populations of students, which we will call Reds and Blues. Reds are the majority population, and Blues are a small minority population --- the Blues's only make up about 1% of the student body. But the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. And there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions, as described above.<br/><br/>But there is one difference: the Blues have a bit more money than the Reds, so they each take the SAT twice, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds. Here are the grades and exam scores for the two populations, plotted:<br/><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-B7-EMI0LIZ8/XEzQO2OBAsI/AAAAAAAAQ3o/PFO_wd6igLgwpA5OCQ1Ux0N7B9A5s3mcACLcBGAs/s1600/gradesexams.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="261" src="https://1.bp.blogspot.com/-B7-EMI0LIZ8/XEzQO2OBAsI/AAAAAAAAQ3o/PFO_wd6igLgwpA5OCQ1Ux0N7B9A5s3mcACLcBGAs/s400/gradesexams.png" width="400"/></a></div><div class="separator" style="clear: both; text-align: center;"/>So what is the effect of this when we use our reasonable inference procedure? First, lets consider what happens when we learn two different regression models: one for the Blues, and a different one for the Reds. We don't see much difference:<br/><br/>The Red classifier makes errors approximately 11% of the time. The Blue classifier does about the same --- it makes errors about 10.4% of the time. This makes sense: the Blues artificially inflated their SAT score distribution without increasing their talent, and the classifier picked up on this and corrected for it. In fact, it is even a little more accurate!<br/><br/>And since we are interested in fairness, lets think about the <i>false negative rate</i> of our classifiers. "False Negatives" in this setting are the people who are qualified to attend the college ($I &gt; 115$), but whom the college mistakenly rejects. These are really the people who have come to harm as a result of the classifier's mistakes. And the False Negative <i>Rate</i> is the probability that a randomly selected qualified person is mistakenly rejected from college --- i.e. the probability that a randomly selected student is harmed by the classifier. We should want that the false negative rates are approximately equal across the two populations: this would mean that the burden of harm caused by the classifier's mistakes is not disproportionately borne by one population over the other. This is one reason why the difference between false negative rates across different populations has become a standard fairness metric in algorithmic fairness --- <a href="http://papers.nips.cc/paper/6373-equality-of-opportunity-in-supervised-learning">sometimes referred to as "equal opportunity."</a><br/><br/>So how do we fare on this metric? Not so badly! The Blue model has a false negative rate of 50% on the blues, and the Red model has a false negative rate of 47% on the reds --- so the difference between these two is a satisfyingly small 3%.<br/><br/>But you might reasonably object: because we have learned separate models for the Blues and the Reds, we are <i>explicitly </i>making admissions decisions as a function of a student's color! This might sound like a form of discrimination, baked in by the algorithm designer --- and if the two populations represent e.g. racial groups, then its explicitly illegal in a number of settings, including lending.<br/><br/>So what happens if we don't allow our classifier to see group membership, and just train one classifier on the whole student body? The gap in false negative rates between the two populations balloons to 12.5%, and the overall error rate ticks up. This means if you are a qualified member of the Red population, you are substantially more likely to be mistakenly rejected by our classifier than if you are a qualified member of the Blue population.<br/><br/>What happened? There wasn't any malice anywhere in this data pipeline. Its just that the Red population was much larger than the Blue population, so when we trained a classifier to minimize its average error over the entire student body, it naturally fit the Red population --- which contributed much more to the <i>average</i>. But this means that the classifier was no longer compensating for the artificially inflated SAT scores of the Blues, and so was making a disproportionate number of errors on them --- all in their favor.<br/><br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="https://4.bp.blogspot.com/-W3rfiRIJUC0/XEzciybyBfI/AAAAAAAAQ4A/lmABxaYed28K77up20emGgc3TMr8D05QQCLcBGAs/s1600/classifier.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="262" src="https://4.bp.blogspot.com/-W3rfiRIJUC0/XEzciybyBfI/AAAAAAAAQ4A/lmABxaYed28K77up20emGgc3TMr8D05QQCLcBGAs/s400/classifier.png" width="400"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The combined admissions rule takes everyone above the black line. Since the Blues are shifted up relative to the Reds, they are admitted at a disproportionately higher rate. </td></tr></tbody></table><div class="separator" style="clear: both; text-align: center;"/><br/><br/>This is the kind of thing that happens <i>all the time</i>: whenever there are two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations, simply because they contribute more to average error. Depending on the nature of the distribution difference, this can be either to the benefit or the detriment of the minority population. And not only does this not involve any explicit human bias, either on the part of the algorithm designer or the data gathering process, <i>it is exacerbated if we artificially force the algorithm to be group blind</i>. Well intentioned "fairness" regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time.<br/><br/><br/><br/><br/></div>
    </content>
    <updated>2019-01-26T22:19:00Z</updated>
    <published>2019-01-26T22:19:00Z</published>
    <author>
      <name>Aaron Roth</name>
      <email>noreply@blogger.com</email>
      <uri>https://plus.google.com/111805394598997130229</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-25562705</id>
      <category term="game theory"/>
      <category term="news"/>
      <author>
        <name>Aaron Roth</name>
        <email>noreply@blogger.com</email>
      </author>
      <link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <title>Adventures in Computation</title>
      <updated>2019-01-28T13:54:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7420</id>
    <link href="https://windowsontheory.org/2019/01/26/introduction-to-amp-and-the-replica-trick/" rel="alternate" type="text/html"/>
    <title>Introduction to AMP and the Replica Trick</title>
    <summary>(This post from the lecture by Yueqi Sheng) In this post, we will talk about detecting phase transitions using Approximate-Message-Passing (AMP), which is an extension of Belief-Propagation to “dense” models. We will also discuss the Replica Symmetric trick, which is a heuristic method of analyzing phase transitions. We focus on the Rademacher spiked Wigner model (defined below), […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>(This post from the lecture by <span class="qu"><span class="gD">Yueqi </span></span><span class="qu"><span class="gD">Sheng)</span></span></em></p>
<p>In this post, we will talk about detecting phase transitions using<br/>
Approximate-Message-Passing (AMP), which is an extension of<br/>
Belief-Propagation to “dense” models. We will also discuss the Replica<br/>
Symmetric trick, which is a heuristic method of analyzing phase<br/>
transitions. We focus on the Rademacher spiked Wigner model (defined<br/>
below), and show how both these methods yield the same phrase transition<br/>
in this setting.</p>
<p>The Rademacher spiked Wigner model (RSW) is the following. We are given<br/>
observations <img alt="Y = \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7Dxx%5ET+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y = \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W"/> where<br/>
<img alt="x \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \{\pm 1\}^n"/> (sampled uniformly) is the true signal and <img alt="W" class="latex" src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W"/> is a<br/>
Gaussian-Orthogonal-Ensemble (GOE) matrix:<br/>
<img alt="W_{i, j} \sim \mathbb{N}(0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+j%7D+%5Csim+%5Cmathbb%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W_{i, j} \sim \mathbb{N}(0, 1)"/> for <img alt="i \neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i+%5Cneq+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i \neq j"/> and<br/>
<img alt="W_{i, i} \sim \mathbb{N}(0, 2)" class="latex" src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+i%7D+%5Csim+%5Cmathbb%7BN%7D%280%2C+2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W_{i, i} \sim \mathbb{N}(0, 2)"/>. Here <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> is the signal to noise<br/>
ratio. The goal is to approximately recover <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>.</p>
<p>The question here is: how small can <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> be such that it is<br/>
impossible to recover anything reasonably correlated with the<br/>
ground-truth <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>? And what do the approximate-message-passing algorithm<br/>
(or the replica method) have to say about this?</p>
<p>To answer the first question, one can think of the task here is to<br/>
distinguish <img alt="Y \sim \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%5Csim+%5Cfrac%7B%5Clambda%7D%7Bn%7Dxx%5ET+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y \sim \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W"/> vs<br/>
<img alt="Y \sim W" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%5Csim+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y \sim W"/>. One approach to distinguishing these distributions is to<br/>
look at the spectrum of the observation matrix <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/>. (In fact, it turns<br/>
out that this is an asymptotically optimal distinguisher [1]). The spectrum of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> behaves as ([2]):</p>
<ul>
<li>When <img alt="\lambda \leq 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cleq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda \leq 1"/>, the empirical distribution of eigenvalues in<br/>
spiked model still follows the semicircle law, with the top<br/>
eigenvalues <img alt="\approx 2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\approx 2"/><p/>
</li>
<li>
<p>When <img alt="\lambda &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda &gt; 1"/>, we start to see an eigenvalue <img alt="&gt; 2" class="latex" src="https://s0.wp.com/latex.php?latex=%3E+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&gt; 2"/> in the<br/>
planted model.</p>
</li>
</ul>
<h1>Approximate message passing</h1>
<p>This section approximately follows the exposition in [3].</p>
<p>First, note that in the Rademacher spiked Wigner model, the posterior<br/>
distribution of the signal <img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma"/> conditioned on the observation <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/><br/>
is: <img alt="\Pr[\sigma | Y] \propto \Pr[Y | \sigma] \propto \prod_{i \neq j} \exp(\lambda Y_{i, j} \sigma_i \sigma_j /2 )" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma+%7C+Y%5D+%5Cpropto+%5CPr%5BY+%7C+%5Csigma%5D+%5Cpropto+%5Cprod_%7Bi+%5Cneq+j%7D+%5Cexp%28%5Clambda+Y_%7Bi%2C+j%7D+%5Csigma_i+%5Csigma_j+%2F2+%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[\sigma | Y] \propto \Pr[Y | \sigma] \propto \prod_{i \neq j} \exp(\lambda Y_{i, j} \sigma_i \sigma_j /2 )"/> This<br/>
defines a graphical-model (or “factor-graph”), over which we can perform<br/>
Belief-Propogation to infer the posterior distribution of <img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma"/>.<br/>
However, in this case the factor-graph is dense (the distribution is a<br/>
product of potentials <img alt="\exp(\lambda Y_{i, j} \sigma_i\sigma_j)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Clambda+Y_%7Bi%2C+j%7D+%5Csigma_i%5Csigma_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(\lambda Y_{i, j} \sigma_i\sigma_j)"/> for all<br/>
pairs of <img alt="i, j" class="latex" src="https://s0.wp.com/latex.php?latex=i%2C+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i, j"/>).</p>
<p>In the previous <a href="https://windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/">blog post</a>, we saw belief propagation works great when the underlying interaction<br/>
graph is sparse. Intuitively, this is because <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is locally tree like,<br/>
which allows us to assume each messages are independent random<br/>
variables. In dense model, this no longer holds. One can think of dense<br/>
model as each node receive a weak signal from all its neighbors.</p>
<p>In the dense model setting, a class of algorithms called Approximate<br/>
message passing (AMP) is proposed as an alternative of BP. We will<br/>
define AMP for RWM in terms of its state evolution.</p>
<h2>State evolution of AMP for Rademacher spiked Wigner model</h2>
<p>Recall that in BP, we wish to infer the posterior distributon of<br/>
<img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma"/>, and the messages we pass between nodes correspond to marginal<br/>
probability distribution over values on nodes. In our setting, since the<br/>
distributions are over <img alt="\{\pm 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}"/>, we can represent distributions by<br/>
their expected values. Let <img alt="m^t_{u \to v} \in [-1, 1]" class="latex" src="https://s0.wp.com/latex.php?latex=m%5Et_%7Bu+%5Cto+v%7D+%5Cin+%5B-1%2C+1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^t_{u \to v} \in [-1, 1]"/> denote the<br/>
message from <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u"/> to <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> at time <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/>. That is, <img alt="m_{u \to v}" class="latex" src="https://s0.wp.com/latex.php?latex=m_%7Bu+%5Cto+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m_{u \to v}"/> corresponds<br/>
to the expected value <img alt="{{\mathbb{E}}}[\sigma_u]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[\sigma_u]"/>.</p>
<p>To derive the BP update rules, we want to compute the expectation<br/>
<img alt="{{\mathbb{E}}}[\sigma_v]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_v%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[\sigma_v]"/> of a node <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/>, given the<br/>
messages <img alt="{{\mathbb{E}}}[\sigma_u]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[\sigma_u]"/> for <img alt="u \neq v" class="latex" src="https://s0.wp.com/latex.php?latex=u+%5Cneq+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u \neq v"/>. We can<br/>
do this using the posterior distribution of the RWM, <img alt="\Pr[\sigma | Y]" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma+%7C+Y%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[\sigma | Y]"/>,<br/>
which we computed above.<br/>
<img alt="\displaystyle \Pr[\sigma_v = 1 | Y, \{\sigma_u\}_{u \neq v}] = \frac{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) - \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) + \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr%5B%5Csigma_v+%3D+1+%7C+Y%2C+%5C%7B%5Csigma_u%5C%7D_%7Bu+%5Cneq+v%7D%5D+%3D+%5Cfrac%7B+%5Cprod_u+%5Cexp%28%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+-+%5Cprod_u+%5Cexp%28-%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%7D%7B+%5Cprod_u+%5Cexp%28%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%2B+%5Cprod_u+%5Cexp%28-%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \Pr[\sigma_v = 1 | Y, \{\sigma_u\}_{u \neq v}] = \frac{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) - \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) + \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }"/></p>
<p>And similarly for <img alt="\Pr[\sigma_v = -1 | Y, \{\sigma_u\}_{u \neq v}]" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma_v+%3D+-1+%7C+Y%2C+%5C%7B%5Csigma_u%5C%7D_%7Bu+%5Cneq+v%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[\sigma_v = -1 | Y, \{\sigma_u\}_{u \neq v}]"/>.<br/>
From the above, we can take expectations over <img alt="\sigma_u" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_u&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_u"/>, and express<br/>
<img alt="{{\mathbb{E}}}[\sigma_v]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_v%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[\sigma_v]"/> in terms of<br/>
<img alt="\{{{\mathbb{E}}}[\sigma_u]\}_{u \neq v}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D%5C%7D_%7Bu+%5Cneq+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{{{\mathbb{E}}}[\sigma_u]\}_{u \neq v}"/>. Doing this (and<br/>
using the heuristic assumption that the distribution of <img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma"/> is a<br/>
product distribution), we find that the BP state update can be written<br/>
as:<br/>
<img alt="m^{t}_{u \to v} = f(\sum_{w \neq v}f^{-1}(A_{w, u} m^{t - 1}_{w \to u}))" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+f%28%5Csum_%7Bw+%5Cneq+v%7Df%5E%7B-1%7D%28A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw+%5Cto+u%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t}_{u \to v} = f(\sum_{w \neq v}f^{-1}(A_{w, u} m^{t - 1}_{w \to u}))"/><br/>
where the interaction matrix <img alt="A_{w, u} = \lambda Y_{w, u}" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7Bw%2C+u%7D+%3D+%5Clambda+Y_%7Bw%2C+u%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A_{w, u} = \lambda Y_{w, u}"/>, and<br/>
<img alt="f(x) = tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+tanh%28x%29+%3D+%5Cfrac%7B%5Cexp%28x%29+-+%5Cexp%28-x%29%7D%7B%5Cexp%28x%29+%2B+%5Cexp%28x%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(x) = tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(x)}"/>.</p>
<p>Now, Taylor expanding <img alt="f^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7B-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f^{-1}"/> around <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>, we find<br/>
<img alt="m^{t}_{u \to v} = f\left( (\sum_{w \neq v} A_{w, u} m^{t - 1}_{w \to u}) + O(1/\sqrt{n}) \right)" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+f%5Cleft%28+%28%5Csum_%7Bw+%5Cneq+v%7D+A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw+%5Cto+u%7D%29+%2B+O%281%2F%5Csqrt%7Bn%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t}_{u \to v} = f\left( (\sum_{w \neq v} A_{w, u} m^{t - 1}_{w \to u}) + O(1/\sqrt{n}) \right)"/><br/>
since the terms <img alt="A_{w, u}" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7Bw%2C+u%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A_{w, u}"/> are of order <img alt="O(1/\sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%2F%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(1/\sqrt{n})"/>.</p>
<p>At this point, we could try dropping the “non-backtracking” condition<br/>
<img alt="w \neq v" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Cneq+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w \neq v"/> from the above sum (since the node <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> contributes at most<br/>
<img alt="O(1/\sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%2F%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(1/\sqrt{n})"/> to the sum anyway), to get the state update:<br/>
<img alt="m^{t}_{u} = f\left( \sum_{w} A_{w, u} m^{t - 1}_{w}) \right)" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu%7D+%3D+f%5Cleft%28+%5Csum_%7Bw%7D+A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t}_{u} = f\left( \sum_{w} A_{w, u} m^{t - 1}_{w}) \right)"/> (note the messages no longer<br/>
depend on receiver – so we write <img alt="m_u" class="latex" src="https://s0.wp.com/latex.php?latex=m_u&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m_u"/> in place of <img alt="m_{u \to v}" class="latex" src="https://s0.wp.com/latex.php?latex=m_%7Bu+%5Cto+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m_{u \to v}"/>).<br/>
However, this simplification turns out not to work for estimating the<br/>
signal. The problem is that the “backtracking” terms which we added<br/>
amplify over two iterations.</p>
<p>In AMP, we simply perform the above procedure, except we add a<br/>
correction term to account for the backtracking issue above. Given <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u"/>,<br/>
for all <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/>, the AMP update is:<br/>
<img alt="m^{t}_{u \to v} = m^{t}_u = f(\sum_{w}A_{w, u} m^{t - 1}_{w}) + [\text{some correction term}]" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+m%5E%7Bt%7D_u+%3D+f%28%5Csum_%7Bw%7DA_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw%7D%29+%2B+%5B%5Ctext%7Bsome+correction+term%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t}_{u \to v} = m^{t}_u = f(\sum_{w}A_{w, u} m^{t - 1}_{w}) + [\text{some correction term}]"/></p>
<p>The correction term corresponds to error introduced by the backtracking<br/>
terms. Suppose everything is good until step <img alt="t - 2" class="latex" src="https://s0.wp.com/latex.php?latex=t+-+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t - 2"/>. We will examine<br/>
the influence of backtracking term to a node <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> through length 2 loops.<br/>
At time <img alt="t - 1" class="latex" src="https://s0.wp.com/latex.php?latex=t+-+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t - 1"/>, <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> exert <img alt="Y_{v, u}m^{t - 2}_v" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7Bv%2C+u%7Dm%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y_{v, u}m^{t - 2}_v"/> additional influence to<br/>
each of it’s neighbor <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u"/>. At time <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/>, <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> receive roughly<br/>
<img alt="Y_{u, v}^2m^{t - 2}_v" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7Bu%2C+v%7D%5E2m%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y_{u, v}^2m^{t - 2}_v"/>. Since <img alt="Y_{u, v}^2" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7Bu%2C+v%7D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y_{u, v}^2"/> has magnitude<br/>
<img alt="\approx \frac{1}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+%5Cfrac%7B1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\approx \frac{1}{n}"/> and we need to sum over all of <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/>’s neighbors,<br/>
this error term is to large to ignore. To characterize the exact form of<br/>
correction, we simply do a taylor expansion</p>
<p><img alt="m^{t}_v = \sum_{u}f(Y_{u, v}m^{t - 1}_u) = \sum_{u}f(Y_{u, v} \left(\sum_{w}f(Y_{w, u}m^{t - 2}_w) - f(Y_{u, v}m^{t - 2}_w)\right) )\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - Y_{u, v}f'(m^{t - 1}_u)m^{t - 2}_v\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - \frac{1}{n}\sum_{u}f'(m^{t - 1}_u)m^{t - 2}_v" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_v+%3D+%5Csum_%7Bu%7Df%28Y_%7Bu%2C+v%7Dm%5E%7Bt+-+1%7D_u%29+%3D+%5Csum_%7Bu%7Df%28Y_%7Bu%2C+v%7D+%5Cleft%28%5Csum_%7Bw%7Df%28Y_%7Bw%2C+u%7Dm%5E%7Bt+-+2%7D_w%29+-+f%28Y_%7Bu%2C+v%7Dm%5E%7Bt+-+2%7D_w%29%5Cright%29+%29%5C%5C+%5Capprox+%5Csum_u+f%28Y_%7Bu%2C+v%7D+m%5E%7Bt+-+1%7D_u%29+-+Y_%7Bu%2C+v%7Df%27%28m%5E%7Bt+-+1%7D_u%29m%5E%7Bt+-+2%7D_v%5C%5C+%5Capprox+%5Csum_u+f%28Y_%7Bu%2C+v%7D+m%5E%7Bt+-+1%7D_u%29+-+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bu%7Df%27%28m%5E%7Bt+-+1%7D_u%29m%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t}_v = \sum_{u}f(Y_{u, v}m^{t - 1}_u) = \sum_{u}f(Y_{u, v} \left(\sum_{w}f(Y_{w, u}m^{t - 2}_w) - f(Y_{u, v}m^{t - 2}_w)\right) )\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - Y_{u, v}f'(m^{t - 1}_u)m^{t - 2}_v\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - \frac{1}{n}\sum_{u}f'(m^{t - 1}_u)m^{t - 2}_v"/></p>
<h2>State evolution of AMP</h2>
<p>In this section we attempt to obtain the phase transition of Rademacher<br/>
spiked Wigner model via looking at <img alt="m^{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{\infty}"/>.</p>
<p>We assume that each message could be written as a sum of signal term and<br/>
noise term. <img alt="m^t = \mu_t x + \sigma_t g" class="latex" src="https://s0.wp.com/latex.php?latex=m%5Et+%3D+%5Cmu_t+x+%2B+%5Csigma_t+g&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^t = \mu_t x + \sigma_t g"/> where<br/>
<img alt="g \sim \mathbb{N}(0, I)" class="latex" src="https://s0.wp.com/latex.php?latex=g+%5Csim+%5Cmathbb%7BN%7D%280%2C+I%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g \sim \mathbb{N}(0, I)"/>. To the dynamics of AMP (and find its phase<br/>
transition), we need to look at how the signal <img alt="\mu_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_t"/> and noise<br/>
<img alt="\sigma_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_t"/> evolves with <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/>.</p>
<p>We do the following simplification: ignore the correction term and<br/>
assume each time we obtain an independent noise <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g"/>.</p>
<p><img alt="m^{t} = Yf(m^{t - 1}) = (\frac{\lambda}{n}x^Tx + \frac{1}{\sqrt{n}}W)f(m^{t - 1}) = \frac{\lambda}{n} &lt; f(m^{t - 1}), x &gt; x + \frac{1}{\sqrt{n}} Wf(m^{t - 1})" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D+%3D+Yf%28m%5E%7Bt+-+1%7D%29+%3D+%28%5Cfrac%7B%5Clambda%7D%7Bn%7Dx%5ETx+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW%29f%28m%5E%7Bt+-+1%7D%29+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3C+f%28m%5E%7Bt+-+1%7D%29%2C+x+%3E+x+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7D+Wf%28m%5E%7Bt+-+1%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t} = Yf(m^{t - 1}) = (\frac{\lambda}{n}x^Tx + \frac{1}{\sqrt{n}}W)f(m^{t - 1}) = \frac{\lambda}{n} &lt; f(m^{t - 1}), x &gt; x + \frac{1}{\sqrt{n}} Wf(m^{t - 1})"/></p>
<p>Here, we see that <img alt="\mu_t = \frac{\lambda}{n}&lt; f(m^{t - 1}), x&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_t+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D%3C+f%28m%5E%7Bt+-+1%7D%29%2C+x%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_t = \frac{\lambda}{n}&lt; f(m^{t - 1}), x&gt;"/><br/>
and <img alt="\sigma_t = \frac{1}{\sqrt{n}}Wf(m^{t - 1})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_t+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DWf%28m%5E%7Bt+-+1%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_t = \frac{1}{\sqrt{n}}Wf(m^{t - 1})"/>.</p>
<p><em>Note that <img alt="\mu_{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_%7Bt%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_{t}"/> is essentially proportional to overlap between<br/>
ground truth and current belief</em>, since the function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/> keeps the<br/>
magnitude of the current beliefs bounded.</p>
<p><img alt="\frac{\lambda}{n} &lt;f(m^{t - 1}), x&gt;= \frac{\lambda}{n} &lt;f(\mu_{t - 1}x + \sigma_{t - 1}g), x&gt; \approx\lambda {{\mathbb{E}}}_{X \sim unif(\pm 1), G\sim \mathbb{N}(0, 1)}[X f(\mu_{t - 1}X + \sigma_{t - 1}G)] = \lambda {{\mathbb{E}}}_G[f(\mu_{t - 1} + \sigma_{t - 1}G)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3Cf%28m%5E%7Bt+-+1%7D%29%2C+x%3E%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3Cf%28%5Cmu_%7Bt+-+1%7Dx+%2B+%5Csigma_%7Bt+-+1%7Dg%29%2C+x%3E+%5Capprox%5Clambda+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BX+%5Csim+unif%28%5Cpm+1%29%2C+G%5Csim+%5Cmathbb%7BN%7D%280%2C+1%29%7D%5BX+f%28%5Cmu_%7Bt+-+1%7DX+%2B+%5Csigma_%7Bt+-+1%7DG%29%5D+%3D+%5Clambda+%7B%7B%5Cmathbb%7BE%7D%7D%7D_G%5Bf%28%5Cmu_%7Bt+-+1%7D+%2B+%5Csigma_%7Bt+-+1%7DG%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{\lambda}{n} &lt;f(m^{t - 1}), x&gt;= \frac{\lambda}{n} &lt;f(\mu_{t - 1}x + \sigma_{t - 1}g), x&gt; \approx\lambda {{\mathbb{E}}}_{X \sim unif(\pm 1), G\sim \mathbb{N}(0, 1)}[X f(\mu_{t - 1}X + \sigma_{t - 1}G)] = \lambda {{\mathbb{E}}}_G[f(\mu_{t - 1} + \sigma_{t - 1}G)]"/></p>
<p>For the noise term, each coordinate of <img alt="\sigma_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_t"/> is a gaussian random<br/>
variable with <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> mean and variance</p>
<p><img alt="\frac{1}{n} \sum_v f(m^{t - 1})_v^2 \approx {{\mathbb{E}}}_{X, G}[f(\mu_{t - 1}X + \sigma_{t - 1}G)^2] = {{\mathbb{E}}}_{G}[f(\mu_{t - 1} + \sigma_{t - 1}G)^2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn%7D+%5Csum_v+f%28m%5E%7Bt+-+1%7D%29_v%5E2+%5Capprox+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BX%2C+G%7D%5Bf%28%5Cmu_%7Bt+-+1%7DX+%2B+%5Csigma_%7Bt+-+1%7DG%29%5E2%5D+%3D+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BG%7D%5Bf%28%5Cmu_%7Bt+-+1%7D+%2B+%5Csigma_%7Bt+-+1%7DG%29%5E2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{n} \sum_v f(m^{t - 1})_v^2 \approx {{\mathbb{E}}}_{X, G}[f(\mu_{t - 1}X + \sigma_{t - 1}G)^2] = {{\mathbb{E}}}_{G}[f(\mu_{t - 1} + \sigma_{t - 1}G)^2]"/></p>
<p>It was shown in [4] that we can introduce a new<br/>
parameter <img alt="\gamma_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_t"/> s.t.<br/>
<img alt="\gamma_t = \lambda^2 {{\mathbb{E}}}[f(\gamma_{t - 1} + \sqrt{\gamma_{t - 1}}G)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_t+%3D+%5Clambda%5E2+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5Bf%28%5Cgamma_%7Bt+-+1%7D+%2B+%5Csqrt%7B%5Cgamma_%7Bt+-+1%7D%7DG%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_t = \lambda^2 {{\mathbb{E}}}[f(\gamma_{t - 1} + \sqrt{\gamma_{t - 1}}G)]"/><br/>
As <img alt="t \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t \to \infty"/>, turns out <img alt="\mu_t = \frac{\gamma_t}{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_t+%3D+%5Cfrac%7B%5Cgamma_t%7D%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_t = \frac{\gamma_t}{\lambda}"/> and<br/>
<img alt="\sigma_t^2 = \frac{\sigma_t}{\lambda^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_t%5E2+%3D+%5Cfrac%7B%5Csigma_t%7D%7B%5Clambda%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_t^2 = \frac{\sigma_t}{\lambda^2}"/>. To study the behavior of<br/>
<img alt="m^t" class="latex" src="https://s0.wp.com/latex.php?latex=m%5Et&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^t"/> as <img alt="t \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t \to \infty"/>, it is enough to track the evolution of<br/>
<img alt="\gamma_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_t"/>.</p>
<p>This heuristic analysis of AMP actually gives a phase transition at<br/>
<img alt="\lambda = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda = 1"/> (in fact, the analysis of AMP can be done rigorously as in [5]):</p>
<ul>
<li>For <img alt="\lambda &lt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3C+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda &lt; 1"/>: If <img alt="\gamma_t \approx 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_t+%5Capprox+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_t \approx 0"/>, <img alt="|\gamma_t + \sqrt{\gamma_t}G| &lt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cgamma_t+%2B+%5Csqrt%7B%5Cgamma_t%7DG%7C+%3C+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\gamma_t + \sqrt{\gamma_t}G| &lt; 1"/> w.h.p., thus we have <img alt="\gamma_{t + 1} \approx \lambda^2 (\gamma_t) &lt; \gamma_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bt+%2B+1%7D+%5Capprox+%5Clambda%5E2+%28%5Cgamma_t%29+%3C+%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_{t + 1} \approx \lambda^2 (\gamma_t) &lt; \gamma_t"/>. Taking <img alt="t \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t \to \infty"/>, we have <img alt="\gamma_{\infty} = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7B%5Cinfty%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_{\infty} = 0"/>, which means there AMP solution has no overlap with the ground truth.<p/>
</li>
<li>
<p>For <img alt="\lambda &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda &gt; 1"/>: In this case, AMP’s solution has some correlation with the ground truth.</p>
</li>
</ul>
<p><img alt="screenshot 2019-01-26 13.49.39" class="alignnone size-full wp-image-7422" src="https://windowsontheory.files.wordpress.com/2019/01/screenshot-2019-01-26-13.49.39.png?w=600"/></p>
<p>(Figure from [6])</p>
<h1>Replica symmetry trick</h1>
<p>Another way of obtaining the phase transition is via a non-rigorous<br/>
analytic method called the replica method. Although non-rigorous, this<br/>
method from statistical physics has been used to predict the fixed point<br/>
of many message passing algorithms and has the advantage of being easy<br/>
to simulate. In our case, we will see that we obtain the same phase<br/>
transition temperature as AMP above. The method is non-rigorous due to<br/>
several assumptions made during the computation.</p>
<h2>Outline of replica method</h2>
<p>Recall that we are interested in minizing the free energy of a given<br/>
system <img alt="f(\beta, Y) = \frac{1}{\beta n} \log Z(\beta, Y)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%2C+Y%29+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+n%7D+%5Clog+Z%28%5Cbeta%2C+Y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta, Y) = \frac{1}{\beta n} \log Z(\beta, Y)"/> where <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> is<br/>
the partition function as before:<br/>
<img alt="Z(\beta, Y) = \sum_{x \in \{\pm 1\}^n} exp(-\beta H(Y, x))" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%2C+Y%29+%3D+%5Csum_%7Bx+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En%7D+exp%28-%5Cbeta+H%28Y%2C+x%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\beta, Y) = \sum_{x \in \{\pm 1\}^n} exp(-\beta H(Y, x))"/> and<br/>
<img alt="H(Y, x) = -&lt;Y, x^Tx&gt; = -xYx^T = -\sum_{i, j} Y_{i, j}x_ix_j" class="latex" src="https://s0.wp.com/latex.php?latex=H%28Y%2C+x%29+%3D+-%3CY%2C+x%5ETx%3E+%3D+-xYx%5ET+%3D+-%5Csum_%7Bi%2C+j%7D+Y_%7Bi%2C+j%7Dx_ix_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H(Y, x) = -&lt;Y, x^Tx&gt; = -xYx^T = -\sum_{i, j} Y_{i, j}x_ix_j"/>.</p>
<p>In replica method, <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> is not fixed but a random variable. The<br/>
assumption is that as <img alt="n \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n \to \infty"/>, free energy doesn’t vary with <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/><br/>
too much, so we will look at the mean of <img alt="f_Y" class="latex" src="https://s0.wp.com/latex.php?latex=f_Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_Y"/> to approximate free<br/>
energy of the system.</p>
<p><img alt="f(\beta) = \lim_{n \to \infty}\frac{1}{\beta n}{{\mathbb{E}}}_{Y}[\log Z(\beta, Y)]" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn+%5Cto+%5Cinfty%7D%5Cfrac%7B1%7D%7B%5Cbeta+n%7D%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BY%7D%5B%5Clog+Z%28%5Cbeta%2C+Y%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) = \lim_{n \to \infty}\frac{1}{\beta n}{{\mathbb{E}}}_{Y}[\log Z(\beta, Y)]"/></p>
<p><img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> is called the free energy density and the goal now is to<br/>
compute the free energy density as a function of only <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/> , the<br/>
temperature of the system.</p>
<p>The <strong>replica method</strong> is first proposed as a simplification of the<br/>
computation of <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/></p>
<p>It is a generally hard problem to compute <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> in a clear way. A<br/>
naive attempt of approximate <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> is to simply pull the log out<br/>
<img alt="g(\beta) = \frac{1}{\beta n}\log {{\mathbb{E}}}_Y[Z(\beta, Y)]" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+n%7D%5Clog+%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(\beta) = \frac{1}{\beta n}\log {{\mathbb{E}}}_Y[Z(\beta, Y)]"/><br/>
Unfortunately <img alt="g(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(\beta)"/> and <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> are quite different quantities,<br/>
at least when temperature is low. Intuitively, <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> is looking at<br/>
system with a fixed <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> while in <img alt="g(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(\beta)"/>, <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> are allowed to<br/>
fluctuate together. When the temperature is high, <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> doesn’t play a big<br/>
roll in system thus they could be close. However, when temperature is<br/>
low, there could be a problems. Let <img alt="\beta \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta \to \infty"/>,<br/>
<img alt="f(\beta) \approx \int_Y (\beta x_Y Y x_Y)\mu(Y) dY" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%5Capprox+%5Cint_Y+%28%5Cbeta+x_Y+Y+x_Y%29%5Cmu%28Y%29+dY&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) \approx \int_Y (\beta x_Y Y x_Y)\mu(Y) dY"/>,<br/>
<img alt="g(\beta) \approx \log \int_Y exp(\beta x_J Y x_Y)\mu(Y)dY \approx \beta x^* Yx^*" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29+%5Capprox+%5Clog+%5Cint_Y+exp%28%5Cbeta+x_J+Y+x_Y%29%5Cmu%28Y%29dY+%5Capprox+%5Cbeta+x%5E%2A+Yx%5E%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(\beta) \approx \log \int_Y exp(\beta x_J Y x_Y)\mu(Y)dY \approx \beta x^* Yx^*"/>.</p>
<p>While <img alt="{{\mathbb{E}}}_X[\log(f(X))]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_X%5B%5Clog%28f%28X%29%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}_X[\log(f(X))]"/> is hard to compute,<br/>
<img alt="{{\mathbb{E}}}[f(X)^r]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5Bf%28X%29%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[f(X)^r]"/> is a much easier quantity. The<br/>
replica trick starts from rewriting <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> with moments of <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/>:<br/>
Recall that <img alt="x^r \approx 1 + r \log x" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Er+%5Capprox+1+%2B+r+%5Clog+x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^r \approx 1 + r \log x"/> for <img alt="r \approx 0" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Capprox+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \approx 0"/> and<br/>
<img alt="\ln(1 + x)\approx x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cln%281+%2B+x%29%5Capprox+x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ln(1 + x)\approx x"/>, using this we can rewrite <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(x)"/> in the following<br/>
way:</p>
<p><strong>Claim 1.</strong> <em>Let <img alt="f_r(\beta) = \frac{1}{r \beta n}\ln[{{\mathbb{E}}}_Y[Z(\beta, Y)^r]]" class="latex" src="https://s0.wp.com/latex.php?latex=f_r%28%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br+%5Cbeta+n%7D%5Cln%5B%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5Er%5D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_r(\beta) = \frac{1}{r \beta n}\ln[{{\mathbb{E}}}_Y[Z(\beta, Y)^r]]"/></em><br/>
<em>Then, <img alt="f(\beta) = \lim_{r \to 0}f_r(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Br+%5Cto+0%7Df_r%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) = \lim_{r \to 0}f_r(\beta)"/></em></p>
<p>The idea of replica method is quite simple</p>
<ul>
<li>Define a function <img alt="f(r, \beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(r, \beta)"/> for <img alt="r \in \mathbb{Z}_+" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Cmathbb%7BZ%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \in \mathbb{Z}_+"/> s.t. <img alt="f(r, \beta) = f_r(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29+%3D+f_r%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(r, \beta) = f_r(\beta)"/> for all such <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/>.<p/>
</li>
<li>
<p>Extend <img alt="f(r, \beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(r, \beta)"/> analytically to all <img alt="r \in {{\mathbb{R}}}_+" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cin+%7B%7B%5Cmathbb%7BR%7D%7D%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \in {{\mathbb{R}}}_+"/> and take the limit of <img alt="r \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \to 0"/>.</p>
</li>
</ul>
<p>The second step may sound crazy, but for some unexplained reason, it has<br/>
been surprisingly effective at making correct predictions.</p>
<p>The term replica comes from the way used to compute<br/>
<img alt="{{\mathbb{E}}}[Z^r]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[Z^r]"/> in Claim 1. We expand the <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/>-th moment<br/>
in terms of <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> replicas of the system</p>
<p><img alt="Z(\beta, Y)^r = (\sum_x exp(-\beta H(Y, x)))^r = \sum_{x^1, \cdots, x^r} \Pi_{k = 1}^r exp(-\beta H(Y, x^i))" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%2C+Y%29%5Er+%3D+%28%5Csum_x+exp%28-%5Cbeta+H%28Y%2C+x%29%29%29%5Er+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+%5CPi_%7Bk+%3D+1%7D%5Er+exp%28-%5Cbeta+H%28Y%2C+x%5Ei%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\beta, Y)^r = (\sum_x exp(-\beta H(Y, x)))^r = \sum_{x^1, \cdots, x^r} \Pi_{k = 1}^r exp(-\beta H(Y, x^i))"/></p>
<h2>For Rademacher spiked Wigner model</h2>
<p>In this section, we will see how one can apply the replica trick to<br/>
obtain phase transition in the Rademacher spiked Wigner model. Recall<br/>
that given a hidden <img alt="a \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=a+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a \in \{\pm 1\}^n"/>, the observable<br/>
<img alt="Y = \frac{\lambda}{n}a^Ta + \frac{1}{\sqrt n} W" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7Da%5ETa+%2B+%5Cfrac%7B1%7D%7B%5Csqrt+n%7D+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y = \frac{\lambda}{n}a^Ta + \frac{1}{\sqrt n} W"/> where<br/>
<img alt="W_{i, j} \sim \mathcal{N}(0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+j%7D+%5Csim+%5Cmathcal%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W_{i, j} \sim \mathcal{N}(0, 1)"/> and <img alt="W_{i, i} \sim \mathcal{N}(0, 2)" class="latex" src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+i%7D+%5Csim+%5Cmathcal%7BN%7D%280%2C+2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W_{i, i} \sim \mathcal{N}(0, 2)"/>.<br/>
We are interested in finding the smallest <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> where we can still<br/>
recover a solution with some correlation to the ground truth <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a"/>. <em>Note<br/>
that <img alt="\{W_{i, i}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BW_%7Bi%2C+i%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{W_{i, i}\}"/> is not so important here as <img alt="x_i^2" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i^2"/> doesn’t carry<br/>
any information in this case.</em></p>
<p>Given by the posterior <img alt="{{\mathbb{P}}}[x|Y]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BP%7D%7D%7D%5Bx%7CY%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{P}}}[x|Y]"/>, the system we<br/>
set up corresponding to Rademacher spiked Wigner model is the following:</p>
<ul>
<li>the system consists of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> particles and the interactions between<br/>
each particle are give by <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/><p/>
</li>
<li>
<p>the signal to noise ratio <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> as the inverse temperature<br/>
<img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/>.</p>
</li>
</ul>
<p>Following the steps above, we begin by computing<br/>
<img alt="f(r, \beta) = \frac{1}{r\beta n}\ln{{\mathbb{E}}}_Y[Z^r]" class="latex" src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(r, \beta) = \frac{1}{r\beta n}\ln{{\mathbb{E}}}_Y[Z^r]"/><br/>
for <img alt="r \in \mathbb{Z}_+" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Cmathbb%7BZ%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \in \mathbb{Z}_+"/>: Denote <img alt="X^k = (x^k)^Tx^k" class="latex" src="https://s0.wp.com/latex.php?latex=X%5Ek+%3D+%28x%5Ek%29%5ETx%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X^k = (x^k)^Tx^k"/> where <img alt="x^k" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^k"/> is the<br/>
<img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>th replica of the system.</p>
<p><img alt="{{\mathbb{E}}}_Y[Z^r] = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta \sum_k &lt;Y, X^k&gt; \mu(Y) dY\\ = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta &lt;Y, \sum_k X^k&gt;) \mu(Y) dY" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D+%3D+%5Cint_Y+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cbeta+%5Csum_k+%3CY%2C+X%5Ek%3E+%5Cmu%28Y%29+dY%5C%5C+%3D+%5Cint_Y+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cbeta+%3CY%2C+%5Csum_k+X%5Ek%3E%29+%5Cmu%28Y%29+dY&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}_Y[Z^r] = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta \sum_k &lt;Y, X^k&gt; \mu(Y) dY\\ = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta &lt;Y, \sum_k X^k&gt;) \mu(Y) dY"/></p>
<p>We then simplify the above expression with a technical claim.</p>
<p><strong>Claim 2.</strong><em> Let <img alt="Y = A + \frac{1}{\sqrt{n}}W" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+A+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y = A + \frac{1}{\sqrt{n}}W"/> where <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> is a fixed matrix and</em><br/>
<em><img alt="W" class="latex" src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W"/> is the GOE matrix defined as above. Then,</em><br/>
<em><img alt="\int_Y exp(\beta&lt;Y, X&gt;) \mu(Y) dY = exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta}{2} &lt;A, X&gt;)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint_Y+exp%28%5Cbeta%3CY%2C+X%3E%29+%5Cmu%28Y%29+dY+%3D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%7D%7B2%7D+%3CA%2C+X%3E%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\int_Y exp(\beta&lt;Y, X&gt;) \mu(Y) dY = exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta}{2} &lt;A, X&gt;)"/></em><br/>
<em>for some constant <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> depending on distribution of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/>.</em></p>
<p>Denote <img alt="X = \sum_k X^k" class="latex" src="https://s0.wp.com/latex.php?latex=X+%3D+%5Csum_k+X%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X = \sum_k X^k"/>. Apply Claim 2 with<br/>
<img alt="A = \frac{\beta}{n}a^Ta" class="latex" src="https://s0.wp.com/latex.php?latex=A+%3D+%5Cfrac%7B%5Cbeta%7D%7Bn%7Da%5ETa&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A = \frac{\beta}{n}a^Ta"/>, we have<br/>
<img alt="{{\mathbb{E}}}_Y[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta^2}{2n} &lt;a^Ta, X&gt;)" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D+%3Ca%5ETa%2C+X%3E%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}_Y[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta^2}{2n} &lt;a^Ta, X&gt;)"/><br/>
To understand the term inside exponent better, we can rewrite the inner<br/>
sum in terms of overlap between replicas:</p>
<p><img alt="{{\|{X}\|_{F}}}^2 = \sum_{i, j}X_{i, j}^2 = \sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)^2 =\sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)(\sum_{l = 1}^r x^l_ix^l_j)\\ = \sum_{k, l} (\sum_{i = 1}^n x^k_ix^{l}_i)^2 = \sum_{k, l} &lt;x^k, x^l&gt;^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%3D+%5Csum_%7Bi%2C+j%7DX_%7Bi%2C+j%7D%5E2+%3D+%5Csum_%7Bi%2C+j%7D%28%5Csum_%7Bk+%3D+1%7D%5Er+x%5Ek_ix%5Ek_j%29%5E2+%3D%5Csum_%7Bi%2C+j%7D%28%5Csum_%7Bk+%3D+1%7D%5Er+x%5Ek_ix%5Ek_j%29%28%5Csum_%7Bl+%3D+1%7D%5Er+x%5El_ix%5El_j%29%5C%5C+%3D+%5Csum_%7Bk%2C+l%7D+%28%5Csum_%7Bi+%3D+1%7D%5En+x%5Ek_ix%5E%7Bl%7D_i%29%5E2+%3D+%5Csum_%7Bk%2C+l%7D+%3Cx%5Ek%2C+x%5El%3E%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\|{X}\|_{F}}}^2 = \sum_{i, j}X_{i, j}^2 = \sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)^2 =\sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)(\sum_{l = 1}^r x^l_ix^l_j)\\ = \sum_{k, l} (\sum_{i = 1}^n x^k_ix^{l}_i)^2 = \sum_{k, l} &lt;x^k, x^l&gt;^2"/></p>
<p>where the last equality follows from rearranging and switch the inner<br/>
and outer summations.</p>
<p>Using a similar trick, we can view the other term as</p>
<p><img alt="&lt;a^Ta, X&gt; = \sum_{i, j}\sum_{k = 1}^rx^k_ix^k_ja_ia_j = \sum_{k = 1}^r (\sum_{i = 1}^n a_ix^k_i)^2 = \sum_{k}&lt;a, x^k&gt;^2" class="latex" src="https://s0.wp.com/latex.php?latex=%3Ca%5ETa%2C+X%3E+%3D+%5Csum_%7Bi%2C+j%7D%5Csum_%7Bk+%3D+1%7D%5Erx%5Ek_ix%5Ek_ja_ia_j+%3D+%5Csum_%7Bk+%3D+1%7D%5Er+%28%5Csum_%7Bi+%3D+1%7D%5En+a_ix%5Ek_i%29%5E2+%3D+%5Csum_%7Bk%7D%3Ca%2C+x%5Ek%3E%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&lt;a^Ta, X&gt; = \sum_{i, j}\sum_{k = 1}^rx^k_ix^k_ja_ia_j = \sum_{k = 1}^r (\sum_{i = 1}^n a_ix^k_i)^2 = \sum_{k}&lt;a, x^k&gt;^2"/></p>
<p>Note that <img alt="Q_{k, l} = &lt;x^k, x^l&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+%3Cx%5Ek%2C+x%5El%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_{k, l} = &lt;x^k, x^l&gt;"/> represents overlaps between the<br/>
<img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> and <img alt="l" class="latex" src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="l"/>th replicas and <img alt="Q_k = &lt;a, x^k&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=Q_k+%3D+%3Ca%2C+x%5Ek%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_k = &lt;a, x^k&gt;"/> represents the<br/>
overlaps between the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>th replica and the ground truth vector.</p>
<p>In the end, we get for any integer <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/>, (Equation 1):</p>
<p><img alt="\displaystyle f(r, \beta) = \frac{1}{r\beta n}\ln(\sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2)) \label{e:1}\\ = \frac{1}{r\beta n} \ln(\sum_{Q}\nu_{x^k}(Q)exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f%28r%2C+%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln%28%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29%29+%5Clabel%7Be%3A1%7D%5C%5C+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D+%5Cln%28%5Csum_%7BQ%7D%5Cnu_%7Bx%5Ek%7D%28Q%29exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle f(r, \beta) = \frac{1}{r\beta n}\ln(\sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2)) \label{e:1}\\ = \frac{1}{r\beta n} \ln(\sum_{Q}\nu_{x^k}(Q)exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2))"/></p>
<p>Our goal becomes to approximate this quantity. Intuitively, if we think<br/>
of <img alt="Q_{k, l}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_{k, l}"/> as indices on a <img alt="(r + 1) \times (r + 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28r+%2B+1%29+%5Ctimes+%28r+%2B+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(r + 1) \times (r + 1)"/> matrices, <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/>,<br/>
with <img alt="Q(i,i) = 1" class="latex" src="https://s0.wp.com/latex.php?latex=Q%28i%2Ci%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q(i,i) = 1"/>, then <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/> is the average of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> i.i.d matrices. So we<br/>
expect <img alt="Q_{j, k} \in [\pm \frac{1}{n}]" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bj%2C+k%7D+%5Cin+%5B%5Cpm+%5Cfrac%7B1%7D%7Bn%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_{j, k} \in [\pm \frac{1}{n}]"/> for <img alt="j \neq k" class="latex" src="https://s0.wp.com/latex.php?latex=j+%5Cneq+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j \neq k"/> w.h.p. In the<br/>
remaining part, We find the correct <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/> via rewriting Equation 1.</p>
<p>Observe that by introducing a new variable <img alt="Z_{k, l}" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7Bk%2C+l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{k, l}"/> for <img alt="k \neq l" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cneq+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k \neq l"/> and<br/>
using the property of gaussian intergal (Equation 4):</p>
<p><img alt="\label{e:4} exp(\frac{\beta^2}{n}Q_{k, l}^2) = \sqrt{\frac{n}{4\pi}}\int_{Z_{k, l}} exp(-\frac{n}{4}Z_{k, l}^2 + \beta Q_{k, l}Z_{k, l})dZ_k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clabel%7Be%3A4%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7DQ_%7Bk%2C+l%7D%5E2%29+%3D+%5Csqrt%7B%5Cfrac%7Bn%7D%7B4%5Cpi%7D%7D%5Cint_%7BZ_%7Bk%2C+l%7D%7D+exp%28-%5Cfrac%7Bn%7D%7B4%7DZ_%7Bk%2C+l%7D%5E2+%2B+%5Cbeta+Q_%7Bk%2C+l%7DZ_%7Bk%2C+l%7D%29dZ_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\label{e:4} exp(\frac{\beta^2}{n}Q_{k, l}^2) = \sqrt{\frac{n}{4\pi}}\int_{Z_{k, l}} exp(-\frac{n}{4}Z_{k, l}^2 + \beta Q_{k, l}Z_{k, l})dZ_k"/></p>
<p><img alt="\exp(\frac{\beta^2}{2n}Q_k^2) = \sqrt{\frac{1}{8\pi n}}\int_{Z_k}exp(-(2n)Z_k^2 + 2\beta Q_{k}Z_k)dZ_k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7DQ_k%5E2%29+%3D+%5Csqrt%7B%5Cfrac%7B1%7D%7B8%5Cpi+n%7D%7D%5Cint_%7BZ_k%7Dexp%28-%282n%29Z_k%5E2+%2B+2%5Cbeta+Q_%7Bk%7DZ_k%29dZ_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(\frac{\beta^2}{2n}Q_k^2) = \sqrt{\frac{1}{8\pi n}}\int_{Z_k}exp(-(2n)Z_k^2 + 2\beta Q_{k}Z_k)dZ_k"/><br/>
Replace each <img alt="exp(\frac{\beta^2}{n}Q_{k, l}^2)" class="latex" src="https://s0.wp.com/latex.php?latex=exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7DQ_%7Bk%2C+l%7D%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="exp(\frac{\beta^2}{n}Q_{k, l}^2)"/> by a such integral, we<br/>
have (Equation 2):</p>
<p><img alt="\begin{gathered} {{\mathbb{E}}}[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2) \label{e:2}\\ = C\sum_{x^1, \cdots, x^r} \exp(\beta^2 n)\int_{Z_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Z_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \beta \sum_{k \neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k) dZ \\ =C\exp(\beta^n) \int_{Y_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Y_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k\neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kY_k Q_k)) dY \label{e:2}\end{gathered}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bgathered%7D+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29+%5Clabel%7Be%3A2%7D%5C%5C+%3D+C%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+%5Cexp%28%5Cbeta%5E2+n%29%5Cint_%7BZ_%7Bk%2C+l%7D%7Dexp%28-%5Cfrac%7Bn%7D%7B4%7D%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7D%5E2+-+%5Cfrac%7Bn%7D%7B2%7D%5Csum_k+Z_k%5E2+%2B+%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DY_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kZ_k+Q_k%29+dZ+%5C%5C+%3DC%5Cexp%28%5Cbeta%5En%29+%5Cint_%7BY_%7Bk%2C+l%7D%7Dexp%28-%5Cfrac%7Bn%7D%7B4%7D%5Csum_%7Bk+%5Cneq+l%7DY_%7Bk%2C+l%7D%5E2+-+%5Cfrac%7Bn%7D%7B2%7D%5Csum_k+Z_k%5E2+%2B+%5Cln%28%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk%5Cneq+l%7DY_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kY_k+Q_k%29%29+dY+%5Clabel%7Be%3A2%7D%5Cend%7Bgathered%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\begin{gathered} {{\mathbb{E}}}[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2) \label{e:2}\\ = C\sum_{x^1, \cdots, x^r} \exp(\beta^2 n)\int_{Z_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Z_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \beta \sum_{k \neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k) dZ \\ =C\exp(\beta^n) \int_{Y_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Y_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k\neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kY_k Q_k)) dY \label{e:2}\end{gathered}"/></p>
<p>where <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> is the constant given by introducing gaussian intergals.</p>
<p>To compute the integral in (Equation 2), we need to cheat a little bit and take<br/>
<img alt="n \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n \to \infty"/> before letting <img alt="r \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \to 0"/>. Note that free energy density<br/>
is defined as<br/>
<img alt="f(\beta) = \lim_{n \to \infty}\lim_{r \to 0}\frac{1}{r\beta n}\ln {{\mathbb{E}}}_Y[Z(\beta, Y)^r]" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn+%5Cto+%5Cinfty%7D%5Clim_%7Br+%5Cto+0%7D%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln+%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) = \lim_{n \to \infty}\lim_{r \to 0}\frac{1}{r\beta n}\ln {{\mathbb{E}}}_Y[Z(\beta, Y)^r]"/><br/>
This is the second assumption made in the replica method and it is<br/>
commonly believed that switching the order is okay here. Physically,<br/>
this is plausible because we believe intrinsic physical quantities<br/>
should not depend on the system size.</p>
<p>Now the <a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace method</a> tells us when <img alt="n \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n \to \infty"/>, the integral in (Equation 2) is dominated by the max of the exponent.</p>
<p><strong>Theorem 1 (Laplace Method).</strong> <em>Let <img alt="h(x): {{\mathbb{R}}}^n \to {{\mathbb{R}}}" class="latex" src="https://s0.wp.com/latex.php?latex=h%28x%29%3A+%7B%7B%5Cmathbb%7BR%7D%7D%7D%5En+%5Cto+%7B%7B%5Cmathbb%7BR%7D%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h(x): {{\mathbb{R}}}^n \to {{\mathbb{R}}}"/>, </em><em>then </em></p>
<p><em><img alt="\int e^{nh(x)} \approx e^{nh(x^*)}(\frac{2\pi}{n})^{\frac{d}{2}}\frac{1}{\sqrt{det(H)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+e%5E%7Bnh%28x%29%7D+%5Capprox+e%5E%7Bnh%28x%5E%2A%29%7D%28%5Cfrac%7B2%5Cpi%7D%7Bn%7D%29%5E%7B%5Cfrac%7Bd%7D%7B2%7D%7D%5Cfrac%7B1%7D%7B%5Csqrt%7Bdet%28H%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\int e^{nh(x)} \approx e^{nh(x^*)}(\frac{2\pi}{n})^{\frac{d}{2}}\frac{1}{\sqrt{det(H)}}"/></em></p>
<p><em>where <img alt="x^* = argmax_x \{h(x)\}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A+%3D+argmax_x+%5C%7Bh%28x%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^* = argmax_x \{h(x)\}"/> and <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> is the Hessian of <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h"/> evaluated at the point <img alt="x^*" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^*"/>.</em></p>
<p>Fix a pair of <img alt="k, l" class="latex" src="https://s0.wp.com/latex.php?latex=k%2C+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k, l"/> and apply Laplace method with<br/>
<img alt="h(Z_{k, l}) = -\frac{1}{2}\sum_{0 \leq k &lt; l \leq r}Z_{k, l}^2 + \frac{1}{n}\ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k))" class="latex" src="https://s0.wp.com/latex.php?latex=h%28Z_%7Bk%2C+l%7D%29+%3D+-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7B0+%5Cleq+k+%3C+l+%5Cleq+r%7DZ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B1%7D%7Bn%7D%5Cln%28%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kZ_k+Q_k%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h(Z_{k, l}) = -\frac{1}{2}\sum_{0 \leq k &lt; l \leq r}Z_{k, l}^2 + \frac{1}{n}\ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k))"/><br/>
what’s left to do is to find the critical point of <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h"/>. Taking the<br/>
derivatives gives<br/>
<img alt="-Y_{k, l} + \frac{A(Z_{k, l})\beta Q_{k, l}}{n A(Z_{k, l})} = 0" class="latex" src="https://s0.wp.com/latex.php?latex=-Y_%7Bk%2C+l%7D+%2B+%5Cfrac%7BA%28Z_%7Bk%2C+l%7D%29%5Cbeta+Q_%7Bk%2C+l%7D%7D%7Bn+A%28Z_%7Bk%2C+l%7D%29%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-Y_{k, l} + \frac{A(Z_{k, l})\beta Q_{k, l}}{n A(Z_{k, l})} = 0"/><br/>
where<br/>
<img alt="A(Z_{k, l}) = \sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + \beta\sum_kY_k Q_k)" class="latex" src="https://s0.wp.com/latex.php?latex=A%28Z_%7Bk%2C+l%7D%29+%3D+%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+%5Cbeta%5Csum_kY_k+Q_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A(Z_{k, l}) = \sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + \beta\sum_kY_k Q_k)"/>.</p>
<p>We now need to find a saddle point of <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h"/> where the hessian is PSD. To<br/>
do that, we choose to assume the order of the replicas does not matter,<br/>
which is refer to as the replica symmetry case. <sup id="fnref-7420-1"><a class="jetpack-footnote" href="https://windowsontheory.org/feed/#fn-7420-1">1</a></sup> One simplest form<br/>
of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> is the following: <img alt="\forall k, l &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cforall+k%2C+l+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\forall k, l &gt; 0"/>, <img alt="Z_{k, l} = y" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7Bk%2C+l%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{k, l} = y"/> and<br/>
<img alt="Z_{k} = y" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7Bk%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{k} = y"/> for some <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/>. This also implies that <img alt="Q_{k, l} = q" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_{k, l} = q"/> for some<br/>
<img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and <img alt="y =\frac{\beta}{n} q" class="latex" src="https://s0.wp.com/latex.php?latex=y+%3D%5Cfrac%7B%5Cbeta%7D%7Bn%7D+q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y =\frac{\beta}{n} q"/></p>
<p>Plug this back in to Equation 2 gives: (Equation 3)</p>
<p><img alt="\label{e:3} {{\mathbb{E}}}[Z^r] = C\exp(\beta n)\exp(-\frac{n}{2}(\frac{r^2 - r}{2})y^2 - \frac{n^2}{2} + \ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + 2y\beta \sum_k Q_k))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clabel%7Be%3A3%7D+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D+%3D+C%5Cexp%28%5Cbeta+n%29%5Cexp%28-%5Cfrac%7Bn%7D%7B2%7D%28%5Cfrac%7Br%5E2+-+r%7D%7B2%7D%29y%5E2+-+%5Cfrac%7Bn%5E2%7D%7B2%7D+%2B+%5Cln%28%5Csum_%7Bx%5Ei%7D%5Cexp%28y%5Cbeta%5Csum_%7Bk+%5Cneq+l%7DQ_%7Bk%2C+l%7D+%2B+2y%5Cbeta+%5Csum_k+Q_k%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\label{e:3} {{\mathbb{E}}}[Z^r] = C\exp(\beta n)\exp(-\frac{n}{2}(\frac{r^2 - r}{2})y^2 - \frac{n^2}{2} + \ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + 2y\beta \sum_k Q_k))"/></p>
<p>To obtain <img alt="f(r, \beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(r, \beta)"/>, we only need to deal with the last term in<br/>
(Equation 3) as <img alt="r \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \to 0"/>. Using the fact that <img alt="Q_{k, l} = y" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_{k, l} = y"/> for all<br/>
<img alt="k, l" class="latex" src="https://s0.wp.com/latex.php?latex=k%2C+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k, l"/> and using the same trick of introducing new gaussain integral as<br/>
in (Equation 4) we have<br/>
<img alt="\lim_{r \to 0}\frac{1}{r}\ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + n\beta \sum_k Q_k)) = -\beta + {{\mathbb{E}}}_{z \sim \mathcal{N}(0, 1)}[\log(2cosh(y\beta + \sqrt{y\beta}z))]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Br+%5Cto+0%7D%5Cfrac%7B1%7D%7Br%7D%5Cln%28%5Csum_%7Bx%5Ei%7D%5Cexp%28y%5Cbeta%5Csum_%7Bk+%5Cneq+l%7DQ_%7Bk%2C+l%7D+%2B+n%5Cbeta+%5Csum_k+Q_k%29%29+%3D+-%5Cbeta+%2B+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7Bz+%5Csim+%5Cmathcal%7BN%7D%280%2C+1%29%7D%5B%5Clog%282cosh%28y%5Cbeta+%2B+%5Csqrt%7By%5Cbeta%7Dz%29%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lim_{r \to 0}\frac{1}{r}\ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + n\beta \sum_k Q_k)) = -\beta + {{\mathbb{E}}}_{z \sim \mathcal{N}(0, 1)}[\log(2cosh(y\beta + \sqrt{y\beta}z))]"/></p>
<p>Using the fact that we want the solution to minimizes free energy,<br/>
taking the derivative of the current <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/> w.r.t. <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> gives<br/>
<img alt="\frac{y}{\beta} = n{{\mathbb{E}}}_z[tanh(y\beta + \sqrt{y\beta}z)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7By%7D%7B%5Cbeta%7D+%3D+n%7B%7B%5Cmathbb%7BE%7D%7D%7D_z%5Btanh%28y%5Cbeta+%2B+%5Csqrt%7By%5Cbeta%7Dz%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{y}{\beta} = n{{\mathbb{E}}}_z[tanh(y\beta + \sqrt{y\beta}z)]"/><br/>
which matches the fixed point of AMP. Plug in <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> will give us<br/>
<img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/>. The curve of <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> looks like the Figure below, where<br/>
the solid line is the curve of <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> with the given <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and the<br/>
dotted line is the curve given by setting all variables <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>.</p>
<p><img alt="screenshot 2019-01-26 13.54.49" class="alignnone size-full wp-image-7423" src="https://windowsontheory.files.wordpress.com/2019/01/screenshot-2019-01-26-13.54.49.png?w=600"/></p>
<p> </p>
<p><strong>References</strong></p>
<div>[1] Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and sub-optimality of pca for spiked random matrices and synchronization.</div>
<div>arXiv preprint arXiv:1609.05573, 2016.</div>
<div/>
<div>[2] D. Feral and S. Pech e. The Largest Eigenvalue of Rank One Deformation of Large Wigner Matrices. Communications in Mathematical Physics, 272:185–228, May 2007.</div>
<div/>
<div>[3] Afonso S Bandeira, Amelia Perry, and Alexander S Wein. Notes on computational-to-statistical gaps: predictions using statistical physics. arXiv preprint arXiv:1803.11132, 2018.</div>
<div/>
<div>[4] Yash Deshpande, Emmanuel Abbe, and Andrea Montanari. Asymptotic mutual information for thebinary stochastic block model. In</div>
<div>Information Theory (ISIT), 2016 IEEE International Symposium on, pages 185–189. IEEE, 2016.</div>
<div/>
<div>[5] Adel Javanmard and Andrea Montanari. State evolution for general approximate message passing algorithms, with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115–144, 2013.</div>
<div/>
<div>[6] A. Perry, A. S. Wein, and A. S. Bandeira. Statistical limits of spiked tensor models.</div>
<div>ArXiv e-prints, December 2016.</div>
<div class="footnotes">
<hr/>
<ol>
<li id="fn-7420-1">
Turns out for this problem, replica symmetry is the only case. We<br/>
will not talk about replica symmetry breaking here, which<br/>
intuitively means we partition replicas into groups and re-curse. <a href="https://windowsontheory.org/feed/#fnref-7420-1">↩</a>
</li>
</ol>
</div></div>
    </content>
    <updated>2019-01-26T19:10:18Z</updated>
    <published>2019-01-26T19:10:18Z</published>
    <category term="physics"/>
    <author>
      <name>preetum</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-01-31T04:21:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7413</id>
    <link href="https://windowsontheory.org/2019/01/25/quantum-circuits-and-their-role-in-demonstrating-quantum-supremacy/" rel="alternate" type="text/html"/>
    <title>Quantum circuits and their role in demonstrating quantum supremacy</title>
    <summary>There’s a lot of discussion and (possibly well-deserved) hype nowadays about quantum computation and its potential for computation at speeds we simply can’t reach with the classical computers we’re used to today. The excitement about this has been building for years, even decades, but it’s only very recently that we’ve really been approaching a solid […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>There’s a lot of discussion and (possibly well-deserved) hype nowadays about quantum computation and its potential for computation at speeds we simply can’t reach with the classical computers we’re used to today. The excitement about this has been building for years, even decades, but it’s only very recently that we’ve really been approaching a solid proof that quantum computers do have an <a href="https://en.wikipedia.org/wiki/Quantum_supremacy">advantage</a> over classical computers. </p>



<p>What’s rather tricky about showing such a result is that, rather than a direct argument about the capability of quantum computers, what we really need to demonstrate is the incapability of classical computers to achieve tasks that can be done with quantum computers. </p>



<p>One of the major leaps forward in demonstrating quantum supremacy was taken by Terhal and DiVincenzo in their 2008 paper “<a href="https://arxiv.org/abs/quant-ph/0205133">Adaptive quantum computation, constant depth quantum circuits and arthur-merlin games</a>“. Their approach was to appeal to a complexity-theoretic argument: they gave evidence that there exists a certain class of quantum circuits that cannot be simulated classically by proving that if a classical simulation existed, certain complexity classes strongly believed to be distinct would collapse to the same class. While this doesn’t quite provide a proof of quantum supremacy – since the statement about the distinction between complexity classes upon which it hinges is not a proven fact – because the complexity statement appears overwhelmingly likely to be true, so too does the proposed existence of non-classically-simulatable quantum circuits. The Terhal and DiVincenzo paper is a complex and highly technical one, but in this post I hope to explain a little bit and give some intuition for the major points. </p>



<p>Now, let’s start at the beginning. What is a <a href="https://en.wikipedia.org/wiki/Quantum_circuit">quantum circuit</a>? I’m going to go ahead and assume you already know what a <a href="https://en.wikipedia.org/wiki/Circuit_(computer_science)">classical circuit</a> is – the extension to a quantum circuit is rather straightforward: it’s a circuit in which all gates are <a href="https://en.wikipedia.org/wiki/Quantum_logic_gate">quantum gates</a>, where a quantum gate can be thought of as a classical gate whose output is, rather than a deterministic function of the inputs, instead a probability distribution over all possible outputs given the size of the inputs. For example, given two single-bit inputs, a classical AND gate outputs 0 or 1 deterministically given the inputs. A quantum AND gate on the analogous single-<a href="https://en.wikipedia.org/wiki/Qubit">qubit</a> inputs would output 0 with some probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> and 1 with some probability <img alt="1-p" class="latex" src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1-p"/>. Similarly, a classical AND gate on two 4-bit inputs outputs the bitwise AND, while the quantum analog has associated with it a 4-qubit output: some probability distribution over all 4-bit binary strings. A priori there is no particular string that is the “output” of the computation by the quantum gate; it’s only after taking a <a href="https://en.wikipedia.org/wiki/Measurement_in_quantum_mechanics">quantum measurement</a> of the output that we get an actual string that we can think of as the outcome of the computation done by the gate. The actual string we “observe” upon taking the measurement follows the probability distribution computed by the gate on its inputs. In this way, a quantum circuit can then be thought of as producing, via a sequence of probabilistic classical gates (i.e., quantum gates) some probability distribution over possible outputs given the input lengths. It’s not hard to see that in this way, we can compose circuits: suppose we have a quantum circuit <img alt="c_1" class="latex" src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_1"/> and another quantum circuit <img alt="c_2" class="latex" src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_2"/>. Let <img alt="c_1" class="latex" src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_1"/> have an input of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> qubits and an output of <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> qubits; suppose we measure <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> of the output qubits of <img alt="c_1" class="latex" src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_1"/> – then we can feed the remaining <img alt="m-k" class="latex" src="https://s0.wp.com/latex.php?latex=m-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m-k"/> unmeasured qubits as inputs into <img alt="c_2" class="latex" src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_2"/>(assuming that those <img alt="m-k" class="latex" src="https://s0.wp.com/latex.php?latex=m-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m-k"/> qubits do indeed constitute a valid input to <img alt="c_2" class="latex" src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_2"/>). </p>



<p>Consider, then, the following sort of quantum circuit: it’s a composition of <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/> quantum circuits, such that after each <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>-th circuit we take a measurement some of its output qubits (so that the remaining unmeasured qubits become inputs to the <img alt="(i+1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i+1)"/>-th circuit), and then the structure of the <img alt="(i+1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i+1)"/>-th circuit is dependent on this measurement. That is, it’s as though, given a quantum circuit, we’re checking every so often at intermediate layers over the course of the circuit’s computation what the value of some of the variables are (leaving the rest to keep going along through the circuit to undergo more computational processing), and based on what we measure is the current computed value, the remainder of the circuit “adapts” in a way determined by that measurement. Aptly enough, this is called an “adaptive circuit”. But since the “downstream” structure of the circuit depends on the outcomes of all the measurements made “upstream”, each adaptive circuit actually comprises a family of circuits, each of which is specified by the sequence of intermediate measurement outcomes. That is, we can alternatively characterize an adaptive circuit as a set of ordinary quantum circuits that is parameterized by a list of measurement outcomes. Terhal and DiVincenzo call this way of viewing an adaptive circuit, as a family of circuits parametrized by a sequence of measurement values, a “non-adaptive circuit” – since we replace the idea that the circuit “adapts” to intermediate measurements with the idea that there are just many regular circuits, one for each possible sequence of measurements. It’s this non-adaptive circuit concept that’ll be our main object of study going forward.</p>



<h2>Simulating quantum circuits</h2>



<p>Now, the result we wanted to demonstrate about quantum circuits had to do with their efficient simulatability by classical circuits – and so we should establish some notion of what we mean when we talk about an “efficient simulation”. </p>



<p>Terhal and DiVincenzo offer the following notion of a classical simulation – which in their paper they call an “efficient density computation”: consider a quantum circuit with some output of length <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> qubits. Recall that to actually obtain an output value, we need to take a measurement of the circuit output – imagine doing this in disjoint subsets of cubits at a time. That is, we can break up the <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> qubits into <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> disjoint subsets and consider the entire output measurement as a process of taking <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> measurements, subset by subset. An efficient density computation exists if there’s a classical procedure for computing, in time polynomial in the width and depth of the quantum circuit, the conditional probability distribution over the set of possible measurement outcomes of a particular subset of qubits, given any subset of the <img alt="k-1" class="latex" src="https://s0.wp.com/latex.php?latex=k-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k-1"/> other measurement outcomes. Intuitively, this is a good notion of what a classical simulation should consist of, or at least what data it should contain, since if you know the conditional probabilities given any (possibly empty) subset of the other measurements, you can just flip coins for the outputs according to the conditional probabilities as a way of actually exhibiting a “working” simulation.</p>



<p>It’s with this notion of simulation, along with our concept of an adaptive quantum circuit as a family of regular circuits parameterized by a sequence of intermediate measurement outcomes, we may now arrive at the main result of Terhal and DiVincenzo’s paper. Recall that what we wanted to show from the very beginning is that there exists some quantum circuit that can’t be simulated classically. The argument for this proceeds like a proof by contradiction: suppose the contrary, and that all quantum circuits can be simulated classically. We want to show that we can find, then, a quantum circuit which, if it were possible to be simulated classically (as per our assumption), we’d wind up with some strange consequences that we believe are false, leading us to conclude that those circuits probably can’t be simulated classically.</p>



<p>Thus, we shall now exhibit such a quantum circuit whose classical simulatability leads (as far as we believe) to a contradiction. Consider a special case of adaptive quantum circuits, considered as a parameterized family of regular circuits, in which the circuit’s output distribution is independent of the intermediate measurement outcomes; that is, the case in which the entire family of circuits corresponding to an adaptive circuit is logically the same – that is, is the same logical circuit on input qubits independent of intermediate measurements. I’d like to point out, just for clarification’s sake, the subtlety here, which makes this consideration non-redundant, and not simply a reduction of an adaptive quantum circuit (again, thought of as a family) to a single fixed circuit (i.e., a family of one): the situation in which the family is reduced to a single fixed circuit occurs when the<em> structure of the circuit</em> is independent of the intermediate measurement outcomes. If the structure were independent of the measurements, then no matter what we observed in the measurements, we’d get the same circuit – hence a trivial family of one. What we’re considering instead is the case in which the <em>structure</em> of the circuit is still dependent on the intermediate measurements (and so the circuit is still adaptive), but where the <em>distribution over the possible outputs of the circuit</em> is identical no matter what the intermediate measurements are. In this case, the circuit can still be considered as a parameterized and in general non-trivial family of circuits, but for which each member produces the same distribution over outputs – hence, a family of potentially <em>structurally</em> different circuits, but which are <em>logically</em> identical.</p>



<p>Suppose there’s some set of such circuits that’s universal – that is, that’s sufficient to implement all polynomial-time quantum computations. (This is a reasonable assumption to make, since there do in fact exist <a href="https://en.wikipedia.org/wiki/Quantum_logic_gate\#Universal_quantum_gates">universal quantum gate sets.</a> But now if a simulation of the kind we defined (an efficient density computation) existed for every circuit in this set, then we could calculate the outcome probability of any polynomial-depth quantum circuit, since any polynomial-depth quantum circuit could be realized as some composition of circuits in this universal set (and in particular as a composition of particular family members of each adaptive circuit in the universal set), and an efficient density computation, as we mentioned above, precisely gives us a way to compute the output distribution. </p>



<p>But now here is where our believed contradiction lies: </p>



<p><em>Theorem:</em> Suppose there exists a universal set of adaptive quantum circuits whose output distributions are independent of intermediate measurements. If there is an efficient density computation for each family member of each adaptive circuit in this universal set, then for the polynomial hierarchy PH we have PH = BPP = BQP. </p>



<p>The proof goes something like this: if we can do our desired efficient density computations (as we assumed, for the sake of contradiction, we could for all quantum circuits), this is equivalent to being able to determine the acceptance probability of a quantum computation, which was shown in the paper “<a href="https://arxiv.org/abs/quant-ph/9812056">Determining Acceptance Possibility for a Quantum Computation is Hard for the Polynomial Hierarchy”</a> by Fenner, Green, Homer and Pruim to be equivalent to the class <img alt="\text{coC=P}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BcoC%3DP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{coC=P}"/>. Thus, we have that <img alt="\text{coC=P} \subseteq \text{BPP}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BcoC%3DP%7D+%5Csubseteq+%5Ctext%7BBPP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{coC=P} \subseteq \text{BPP}"/>. But it’s known that <img alt="\text{PH} \subseteq \text{BPP}^{\text{coC=P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%5Csubseteq+%5Ctext%7BBPP%7D%5E%7B%5Ctext%7BcoC%3DP%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{PH} \subseteq \text{BPP}^{\text{coC=P}}"/> and so <img alt="\text{PH} \subseteq \text{BPP}^{\text{BPP}} = \text{BPP}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%5Csubseteq+%5Ctext%7BBPP%7D%5E%7B%5Ctext%7BBPP%7D%7D+%3D+%5Ctext%7BBPP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{PH} \subseteq \text{BPP}^{\text{BPP}} = \text{BPP}"/>. That is, we have <img alt="\text{PH} = \text{BPP} = \text{BQP}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%3D+%5Ctext%7BBPP%7D+%3D+%5Ctext%7BBQP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{PH} = \text{BPP} = \text{BQP}"/>, and so the polynomial hierarchy would collapse to <img alt="\Sigma^P_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CSigma%5EP_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Sigma^P_2"/> since <img alt="\text{BPP} \subseteq \Sigma^P_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BBPP%7D+%5Csubseteq+%5CSigma%5EP_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{BPP} \subseteq \Sigma^P_2"/> (for more on these more obscure complexity classes, see <a href="https://complexityzoo.uwaterloo.ca/Complexity_Zoo">here</a>). Again, this is our “contradiction”: while it hasn’t been quite proven, it is widely believed, with strong supporting evidence, that the polynomial hierarchy does not collapse as would be the case if all quantum circuits were classically simulatable. Thus this provides a strong argument that not all quantum circuits are classically simulatable, which was precisely what we were looking to demonstrate.</p>



<h2>Conclusion</h2>



<p>Terhal and DiVincenzo actually go even further and show that there is a certain class of constant-depth quantum circuits that are unlikely to be simulatable by classical circuits – this, indeed, seems to provide even stronger evidence for quantum supremacy. This argument, which is somewhat more complex, uses the idea of teleportation and focuses on a particular class of circuits implementable by a certain restricted set of quantum gates. If you’re interested, I highly recommend reading their paper, where this is explained. </p>



<h2>Recommended reading</h2>



<ul><li>Terhal, Barbara and David DiVincenzo.  “Adptive quantum computation, constant depth quantum circuits and arthur-merlin games.” <em>Quantum Info. Comput.</em> 4, 2 (2004); 134-145.</li><li>Bravyil, Sergey, David Gosset, and Robert König. “Quantum advantage with shallow circuits.” <em>Science</em> 362, 6412 (2018); 308-311.</li><li>Harrow, Aram Wettroth and Ashley Montanaro. “Quantum computational supremacy.” <em>Nature</em> 549 (2017): 203-209.</li><li>Boixo, Sergio et. al. “Characterizing quantum supremacy in near-term devices.” <em>Nature Physics</em> 14 (2018); 595-600.</li></ul>



<h2>References</h2>



<p>\begin{enumerate}</p>



<ul><li>Terhal, Barbara and David DiVincenzo.  “Adptive quantum computation, constant depth quantum circuits and arthur-merlin games.” <em>Quantum Info. Comput.</em> 4, 2 (2004); 134-145.</li><li>Fenner, Stephen et. al. “Determining Acceptance Possibility for a Quantum Computation is Hard for the Polynomial Hierarchy.” <a href="https://arxiv.org/abs/quant-ph/9812056" rel="nofollow">https://arxiv.org/abs/quant-ph/9812056</a>. (1998).</li><li>Bravyil, Sergey, David Gosset, and Robert König. “Quantum advantage with shallow circuits.” <em>Science</em> 362, 6412 (2018); 308-311.</li></ul></div>
    </content>
    <updated>2019-01-25T23:42:09Z</updated>
    <published>2019-01-25T23:42:09Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>hksorens</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-01-31T04:21:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/25/postdoc-at-uc-san-diego-apply-by-march-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/25/postdoc-at-uc-san-diego-apply-by-march-1-2019/" rel="alternate" type="text/html"/>
    <title>postdoc at UC San Diego (apply by March 1, 2019)</title>
    <summary>We are looking for strong theory candidates working in the areas of machine learning, optimization, high dimensional statistics, privacy, fairness, and broadly interpreted data science. The postdoc is part of the Data Science fellows program at UCSD. Website: http://dsfellows.ucsd.edu/ Email: shachar.lovett@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are looking for strong theory candidates working in the areas of machine learning, optimization, high dimensional statistics, privacy, fairness, and broadly interpreted data science. The postdoc is part of the Data Science fellows program at UCSD.</p>
<p>Website: <a href="http://dsfellows.ucsd.edu/">http://dsfellows.ucsd.edu/</a><br/>
Email: shachar.lovett@gmail.com</p></div>
    </content>
    <updated>2019-01-25T19:46:17Z</updated>
    <published>2019-01-25T19:46:17Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-31T04:21:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7411</id>
    <link href="https://windowsontheory.org/2019/01/25/looking-a-postdoc-opportunity/" rel="alternate" type="text/html"/>
    <title>Looking a postdoc opportunity?</title>
    <summary>This is the season that people are applying for postdoc positions. Unlike student and faculty hiring, which each have a fairly fixed schedule, postdoc availability can change from time to time, with new opportunities opening up all the time. So, I encourage everyone looking for a postdoc position to periodically check out https://cstheory-jobs.org/ . (For […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is the season that people are applying for postdoc positions. Unlike student and faculty hiring, which each have a fairly fixed schedule, postdoc availability can change from time to time, with new opportunities opening up all the time. So, I encourage everyone looking for a postdoc position to periodically check out <a href="https://cstheory-jobs.org/">https://cstheory-jobs.org/</a> . </p>



<p>(For example, a new ad was just posted on Wednesday by <a href="https://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/">Venkat Guruswami and Pravesh Kothari </a> )</p></div>
    </content>
    <updated>2019-01-25T17:51:37Z</updated>
    <published>2019-01-25T17:51:37Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>windowsontheory</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-01-31T04:21:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7382539649899855346</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7382539649899855346/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/the-paradigm-shift-in-fintech.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7382539649899855346" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7382539649899855346" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/the-paradigm-shift-in-fintech.html" rel="alternate" type="text/html"/>
    <title>The Paradigm Shift in FinTech Computation and the need for a Computational Toolkit (Guest Post by Evangelos Georgiadis)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The Paradigm Shift in FinTech Computation and the need for a Computational Toolkit<br/>
<br/>
(Guest Post by <a href="http://www.mathcognify.com/">Evangelos Georgiadis</a>)<br/>
<br/>
We are experiencing a paradigm shift in finance as we are entering the era of algorithmic FinTech computation. (**And another yet to come. See **Future** below.)  This era is marked by a shift in the role played by the theoretical computer scientist. In the not so distant past, the (financial) economist had the ultimate stamp of approval  for how to study financial models, pricing models, mechanism design, etc. The economist was the ultimate gatekeeper of ideas and models, whereas the main role of the computer scientist was to turn these ideas or models into working code; in a sense, an obedient beaver/engineer. (In finance, the theoretical computer scientist more often than not wears the hat of the quant.)<br/>
<br/>
In today's era, the role of the theoretical computer scientist has been elevated from the obedient engineer to the creative architect not only of models and mechanism designs but also of entire ecosystems. One example is blockchain based ecosystems. In the light of this promotion from obedient engineer to architect, we might need to re-hash the notion of 'sharing blame', as originally and elegantly voiced in <a href="https://www.technologyreview.com/s/408851/on-quants/">On Quants</a> by Professor Daniel W. Stroock, when things go wrong.)<br/>
<br/>
The role change is also coupled by a shift in emphasis of computation that in turn necessitates a deeper understanding of (what this author would refer to as) distributed yet pragmatic complexity based crypto systems' that attempt to redefine 'trust' in terms of distributed computation.<br/>
<br/>
This change necessitates an ability to think in terms of approximation (and lower/upper bounds)  or other good-enough solutions that work on all inputs,  rather than merely easy instances of  problem types that usually lead to clean, exact formulas or solutions.  Additionally, looking through the lens of approximation algorithms enables a different and often more insightful metric for dealing with intrinsically hard problems (for which often no exact or clean solutions exist.) Computer Scientists are trained in this way; however, financial economists are not.   Might the economists actually get in the way?<br/>
<br/>
Our tentative response: The economists are valuable and the solution to the dilemma is to equip them with the right 'computational toolkit'. Ideally, such a toolkit comprises computational tools and algorithms that enable automation of certain computational tasks which otherwise would necessitate more granular understanding at the level of a theoretical computer scientist (or mathematician)<br/>
OR be too cumbersome to perform by hand even for the expert.<br/>
<br/>
Essentially, a toolkit even for the theoretical computer scientist that frees her from clerical work and enables computation to scale from clean cases, such as n=1, to pathological (yet far more realistic) cases, such as n=100000, all the way to the advanced and rather important (agnostic case or) symbolic case when n=k -- without much pain or agony.<br/>
<br/>
The existence of such a toolkit would in turn do justice to the definition of FinTech Computation, which entails applying advanced computational techniques not necessarily information techniques) to financial computation. in fact, this author is part of building such an infrastructure solution which<br/>
necessitates the underlying programming language [R-E-CAS-T] to have intrinsic hybrid capabilities -- symbolic as well as numeric.<br/>
<br/>
One step towards this  "automation" conquest is shown in <a href="https://arxiv.org/pdf/1808.05255v2.pdf">A combinatorial-probabilistic analysis of bitcoin attacks</a> with Doron Zeilberger.  The work illustrates an algorithmic risk analysis of the bitcoin protocol via symbolic computation, as opposed to the meticulous, yet more laborious by hand conquest shown by the European duo in <a href="https://arxiv.org/abs/1702.02867">Double spend races</a> Heavy usage of the "Wilf-Zeilberger algorithmic proof theory" one of the cornerstones in applied symbolic computation, enabled automated recurrence discovery and algorithmic derivation of higher-order asymptotics. For example, in terms of asymptotics tools: the ability to internalize a very dense body of mathematics, such as the G.D. Birkhoff and W.J. Trjitzinsky method, symbolically, automates the process of computing asymptotics of solutions of recurrence equations; a swiss army knife for any user.<br/>
<br/>
&lt;**Future**&gt;<br/>
<br/>
What does the future entail for FinTech Computation ?<br/>
<br/>
[My two <a href="https://en.bitcoin.it/wiki/Satoshi_(unit)">satoshis</a> on this]<br/>
<br/>
Where are we headed in terms of type of computation ?<br/>
<br/>
Blockchain based systems, even though some of us (including this author) have noticed fundamental flaws, seem to still have momentum, at least, judging from recent news articles about companies becoming blockchain technology friendly.  Ranging from (of course) exchanges such as our friends at <a href="https://www.binance.com/en">Binance</a> and <a href="https://www.bitmex.com/">BitMEX</a>, we have major smartphone makers such as <a href="https://www.bloomberg.com/news/articles/2018-04-15/samsung-jumps-on-blockchain-bandwagon-to-manage-its-supply-chain">Samsung</a>, <a href="https://www.bloomberg.com/news/articles/2018-03-21/huawei-said-to-be-in-talks-to-build-blockchain-ready-smartphone">Huawei</a>, and <a href="https://www.cnbc.com/2018/10/23/htc-launches-blockchain-phone-exodus-1-to-be-sold-in-cryptocurrency.html">HTC</a>. The favorable sentiment towards blockchain technology is shared even amongst top tier U.S. banks.<br/>
 Can one deduce success or failure momentum from the citation count distribution of the paper that laid grounds to this technology ? <a href="https://bitcoin.org/bitcoin.pdf">Bitcoin: A Peer-to-Peer Electronic Cash System</a>)<br/>
<br/>
If we look at crypto(currencies), one of many challenges for these blockchain based systems is the high maintenance cost.  Certainly in terms of energy consumption when it comes to the process of mining -- whether Proof-of-Work (PoW) is replaced by Proof-of-Stake (PoS) or some other more energy efficient consensus variant. (This author is aware of various types of optimizations that have been used.)<br/>
A few questions that have bugged this author every since ...<br/>
<br/>
a) Is there a natural way to formalize the notion of energy consumption for consensus mechanisms?<br/>
<br/>
b) What about formalizing an energy-efficient mechanism design ?)<br/>
<br/>
(The idea of savings when PoW is replaced by PoS as intended by our friends at the <a href="https://spectrum.ieee.org/computing/networks/ethereum-plans-to-cut-its-absurd-energy-consumption-by-99-percent">Ethereum Foundation</a> has been around for some time but the point of this author is, the value of 0.99*X (where X is a <a href="https://link.springer.com/chapter/10.1007/978-1-4684-6686-7_28">supernatural number</a>  [a la Don E. Knuth style]), is still a big quantity; too big for an environmentalist ?)<br/>
<br/>
So, what comes next ?<br/>
<br/>
[... the satoshis are still on the table.]<br/>
<br/>
Daniel Kane has brought to my attention that quantum computation -- the seemingly next paradigm shift in which again the role of TCS seem  inextricably interwoven --  may lead to blockchain based systems being replaced by less expensive (at least in terms of energy consumption) quantum based systems. (Crypto might get replaced by Quantum (money). :-)) One such pioneering approach is masterfully articulated by Daniel Kane in "<a href="https://arxiv.org/abs/1809.05925">Quantum Money from Modular Forms</a>.</div>
    </content>
    <updated>2019-01-25T16:44:00Z</updated>
    <published>2019-01-25T16:44:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>https://plus.google.com/101693130490639305932</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-01-30T11:38:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/24/faculty-at-uc-san-diego-apply-by-february-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/24/faculty-at-uc-san-diego-apply-by-february-15-2019/" rel="alternate" type="text/html"/>
    <title>faculty at UC San Diego (apply by February 15, 2019)</title>
    <summary>Apply to be an Assistant Professor, with a focus on Algorithmic Approaches to Socially Innovative Product Architectures and Business Models. This is a joint search by the Computer Science department and the Rady business school. Website: https://apol-recruit.ucsd.edu/JPF01987 Email: slovett@ucsd.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Apply to be an Assistant Professor, with a focus on Algorithmic Approaches to Socially Innovative Product Architectures and Business Models. This is a joint search by the Computer Science department and the Rady business school.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/JPF01987">https://apol-recruit.ucsd.edu/JPF01987</a><br/>
Email: slovett@ucsd.edu</p></div>
    </content>
    <updated>2019-01-24T18:10:56Z</updated>
    <published>2019-01-24T18:10:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-31T04:21:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7530001663397385709</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7530001663397385709/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/machine-learning-and-wind-turbines.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7530001663397385709" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7530001663397385709" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/machine-learning-and-wind-turbines.html" rel="alternate" type="text/html"/>
    <title>Machine Learning and Wind Turbines</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="separator" style="clear: both; text-align: center;">
<a href="https://3.bp.blogspot.com/-IwXAr_ptgYo/XEYxZWFiAzI/AAAAAAABmPw/6t_lphNZigA3sXSix1MpcmKNANZFiGkawCLcBGAs/s1600/Turbines.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="180" src="https://3.bp.blogspot.com/-IwXAr_ptgYo/XEYxZWFiAzI/AAAAAAABmPw/6t_lphNZigA3sXSix1MpcmKNANZFiGkawCLcBGAs/s320/Turbines.png" width="320"/></a></div>
<br/>
My daughter Molly spent six weeks on an environmental program in China last summer. When she got back she had to do a report on machine learning and wind turbines used for clean energy generation. What does machine learning have to do with wind turbines? Plenty it turns out and it tell us a lot about the future of programming.<br/>
<br/>
Sudden changes in wind can cause damage to the blades of the turbine. Maintenance is very expensive especially for turbines in the sea and a broken turbine generates no electricity. To catch these changes ahead of time you can mount a Lidar on top of the turbine.<br/>
<br/>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://4.bp.blogspot.com/-9LFv8FGZNG8/XEYxYFyCYDI/AAAAAAABmPs/TLY7-bfPWhMWjRLl8Jn9ixXrTBa_sAk7ACLcBGAs/s1600/Turbine%2BLidar.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="180" src="https://4.bp.blogspot.com/-9LFv8FGZNG8/XEYxYFyCYDI/AAAAAAABmPs/TLY7-bfPWhMWjRLl8Jn9ixXrTBa_sAk7ACLcBGAs/s320/Turbine%2BLidar.jpg" width="320"/></a></div>
<br/>
The Lidar can detect wind gusts from about 100 meters ahead, giving about 10 seconds to react. In that time you can rotate the blades, or the whole turbine itself to minimize any damage. Here's a <a href="https://www.youtube.com/watch?v=j5zLp6UuC70">video</a> describing the situation.<br/>
<br/>
<center>

</center>
<br/>
How do you do the computations to convert the Lidar data into accurate representations of wind gusts and then how to best adjust for them? You could imagine some complex fluid dynamics computation, which gets even more complex when you several wind turbines in front of each other. Instead you can use the massive amount of data you have collected by sensors on the turbine and the Lidar information and train a neural network. Training takes a long time but a trained network can quickly determine a good course of action. Now neural networks can always make mistakes but unlike self-driving cars, a mistake won't kill anyone, just possibly cause more damage. Since on average you can save considerable maintenance costs, using ML here is a big win.<br/>
<br/>
I've obviously over simplified the above but I really like this example. This is not an ML solution to a standard AI question like image recognition or playing chess. Rather we are using ML to make a difficult computation tractable mostly by using ML on available data and that changes how we think about programming complex tasks.</div>
    </content>
    <updated>2019-01-24T13:58:00Z</updated>
    <published>2019-01-24T13:58:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>https://plus.google.com/101693130490639305932</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>https://plus.google.com/101693130490639305932</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-01-30T11:38:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1054</id>
    <link href="http://corner.mimuw.edu.pl/?p=1054" rel="alternate" type="text/html"/>
    <title>Prophet inequality and auction design</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Suppose you want to sell a car and there are 10 agents willing to buy it. You are not sure how much they could pay but for each of them you know a probability distribution of how high the offer … <a href="http://corner.mimuw.edu.pl/?p=1054">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Suppose you want to sell a car and there are 10 agents willing to buy it. You are not sure how much they could pay but for each of them you know a probability distribution of how high the offer will be. For example, a car salon would always pay 10K  but some person might offer 5K or 15K with equal probability. The best you could do is to first negotiate with all of them and then pick the highest bid. Unfortunately, you cannot do so - after seeing each offer you must irrevocably choose either to sell the car or to refuse the offer. What is the best strategy to maximize your revenue in this case?</p>
<p>In turns out that this is a well-studied optimization problem with a simple strategy that guarantees you can (on expectation) earn at least half as much as a hypothetical prophet, who knows all the bids in advance. This result is known as the prophet inequality. What is more surprising, this strategy would work even if you are a car retailer and want to sell five cars. Moreover, you might want to have some constraints, for example you do not want to sell two cars to buyers from the same city, or have multiple kinds of cars with different evaluations, and you can always guarantee an expected revenue comparable to the one of the prophets.</p>
<p>This problem not only exploits beautiful math but also has important applications in internet ad display. Actually, whenever you type a query into a web search engine, the ad system performs this kind of car-selling game with the ad suppliers, who offer different bids for their ad to be displayed to you.</p>
<p>Here is a link to our recent work with new developments in this theory: <a href="http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f790.pdf">http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f790.pdf</a></p>
<p>Michał Włodarczyk</p></div>
    </content>
    <updated>2019-01-24T10:21:58Z</updated>
    <published>2019-01-24T10:21:58Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Renata Czarniecka</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2019-01-30T23:38:20Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/" rel="alternate" type="text/html"/>
    <title>Postdoctoral Fellow at Carnegie Mellon University (apply by February 28, 2019)</title>
    <summary>Postdoctoral Position at the computer science department, CMU. Hosted by Venkatesan Gurusawami and Pravesh Kothari. Start Date: Fall 2019 (Flexible) Application Deadline: Feb 28, 2019 (earlier applications encouraged). To apply, send CV, a research statement and arrange for 2 letters of recommendation to be set to be sent to bcook@cs.cmu.edu with subject line mentioning “CMU […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Postdoctoral Position at the computer science department, CMU. Hosted by Venkatesan Gurusawami and Pravesh Kothari. Start Date: Fall 2019 (Flexible)<br/>
Application Deadline: Feb 28, 2019 (earlier applications encouraged).</p>
<p>To apply, send CV, a research statement and arrange for 2 letters of recommendation to be set to be sent to bcook@cs.cmu.edu with subject line mentioning “CMU theory postdoc.”</p>
<p>Website: <a href="https://www.cs.princeton.edu/~kothari/theory-postdoc.html">https://www.cs.princeton.edu/~kothari/theory-postdoc.html</a><br/>
Email: kothari@cs.princeton.edu</p></div>
    </content>
    <updated>2019-01-23T22:24:07Z</updated>
    <published>2019-01-23T22:24:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-31T04:21:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019-2/</id>
    <link href="https://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019-2/" rel="alternate" type="text/html"/>
    <title>Postdoc at Charles University in Prague (apply by February 28, 2019)</title>
    <summary>Several one‐year post‐doc positions are available at the Computer Science Institute of Charles University, with a possibility of one-year extension. The positions are in areas of algorithms, cryptography, computational complexity, combinatorics and graph theory. Starting date is in Fall 2019, and can be negotiated. Website: https://iuuk.mff.cuni.cz/~koucky/iuuk-postdocs.html Email: koucky@iuuk.mff.cuni.cz</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Several one‐year post‐doc positions are available at the Computer Science Institute of Charles University, with a possibility of one-year extension. The positions are in areas of algorithms, cryptography, computational complexity, combinatorics and graph theory. Starting date is in Fall 2019, and can be negotiated.</p>
<p>Website: <a href="https://iuuk.mff.cuni.cz/~koucky/iuuk-postdocs.html">https://iuuk.mff.cuni.cz/~koucky/iuuk-postdocs.html</a><br/>
Email: koucky@iuuk.mff.cuni.cz</p></div>
    </content>
    <updated>2019-01-22T12:38:31Z</updated>
    <published>2019-01-22T12:38:31Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-31T04:21:12Z</updated>
    </source>
  </entry>
</feed>
