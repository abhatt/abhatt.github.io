<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2022-06-14T16:22:09Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/06/14/lecturer-senior-lecturer-in-algorithms-at-university-of-sheffield-apply-by-july-5-2022/</id>
    <link href="https://cstheory-jobs.org/2022/06/14/lecturer-senior-lecturer-in-algorithms-at-university-of-sheffield-apply-by-july-5-2022/" rel="alternate" type="text/html"/>
    <title>Lecturer/Senior Lecturer in Algorithms at University of Sheffield (apply by July 5, 2022)</title>
    <summary>The Department of Computer Science at the University of Sheffield UK aims to hire one Lecturer/Senior Lecturer (Assistant/Associate professor) in an area including, but not limited to complexity, algorithm, AI, and AGT. The role is supported with a generous startup package, including funding for conference travel and equipment and a PhD scholarship. Website: https://www.jobs.ac.uk/job/CQL118/lecturer-senior-lecturer-in-algorithms Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Sheffield UK aims to hire one Lecturer/Senior Lecturer (Assistant/Associate professor) in an area including, but not limited to complexity, algorithm, AI, and AGT. The role is supported with a generous startup package, including funding for conference travel and equipment and a PhD scholarship.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CQL118/lecturer-senior-lecturer-in-algorithms">https://www.jobs.ac.uk/job/CQL118/lecturer-senior-lecturer-in-algorithms</a><br/>
Email: s.mukhopadhyay@sheffield.ac.uk</p></div>
    </content>
    <updated>2022-06-14T12:39:10Z</updated>
    <published>2022-06-14T12:39:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-06-14T16:21:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/06/14/lecturer-in-algorithms-at-university-of-sheffield-apply-by-july-5-2022/</id>
    <link href="https://cstheory-jobs.org/2022/06/14/lecturer-in-algorithms-at-university-of-sheffield-apply-by-july-5-2022/" rel="alternate" type="text/html"/>
    <title>Lecturer in Algorithms at University of Sheffield (apply by July 5, 2022)</title>
    <summary>The Department of Computer Science at the University of Sheffield UK aims to hire two Lecturers (Assistant professor) in an area including, but not limited to complexity, algorithm, AI, and AGT. The role is supported with a generous startup package, including funding for conference travel and equipment and a PhD scholarship. Website: https://www.jobs.ac.uk/job/CQL110/lecturer-in-algorithms-two-posts Email: s.mukhopadhyay@sheffield.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Sheffield UK aims to hire two Lecturers (Assistant professor) in an area including, but not limited to complexity, algorithm, AI, and AGT. The role is supported with a generous startup package, including funding for conference travel and equipment and a PhD scholarship.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CQL110/lecturer-in-algorithms-two-posts">https://www.jobs.ac.uk/job/CQL110/lecturer-in-algorithms-two-posts</a><br/>
Email: s.mukhopadhyay@sheffield.ac.uk</p></div>
    </content>
    <updated>2022-06-14T12:36:59Z</updated>
    <published>2022-06-14T12:36:59Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-06-14T16:21:01Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/06/14/analysts-minimum-spanning</id>
    <link href="https://11011110.github.io/blog/2022/06/14/analysts-minimum-spanning.html" rel="alternate" type="text/html"/>
    <title>The analyst’s minimum spanning tree</title>
    <summary>Infinite sets of points in the Euclidean plane, even discrete sets, do not always have Euclidean minimum spanning trees. For instance, consider the points with coordinates</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Infinite sets of points in the Euclidean plane, even discrete sets, do not always have <a href="https://en.wikipedia.org/wiki/Euclidean_minimum_spanning_tree">Euclidean minimum spanning trees</a>. For instance, consider the points with coordinates</p>

\[\left(i, \pm\left(1+\frac1i\right)\right),\]

<p>for positive <span style="white-space: nowrap;">integers \(i\).</span> You can connect the <span style="white-space: nowrap;">positive-\(y\)</span> points and the <span style="white-space: nowrap;">negative-\(y\)</span> points into two chains with edges of length less than two, but then you have to pick one edge of length greater than two to span from one chain to the other. Whichever edge you choose, the next edge along would always be a better choice. So a tree that minimizes the multiset of its edge weights (as finite minimum spanning trees do) does not exist for this example. And as the same example shows, the sum of edge weights may be infinite, so how can we use minimization of this sum to define a tree?</p>

<p style="text-align: center;"><img alt="Discrete infinite set of points with no Euclidean minimum spanning tree" src="https://11011110.github.io/blog/assets/2022/ladder-no-mst.svg"/></p>

<p>Despite that, here’s a construction that works for any <a href="https://en.wikipedia.org/wiki/Compact_space">compact set</a>, even one with infinitely many components, and that generalizes easily to higher-dimensional Euclidean spaces. I think it deserves to be called the Euclidean minimum spanning tree. Given a compact <span style="white-space: nowrap;">set \(C\),</span> consider every partition \(C=A\cup (C\setminus A)\) of \(C\) into two disjoint nonempty compact subsets. For each such partition, find a line segment \(s_A\) of minimum length with endpoints in \(A\) <span style="white-space: nowrap;">and \(C\setminus A\),</span> breaking ties lexicographically by coordinates. By the assumed compactness of \(A\) <span style="white-space: nowrap;">and \(C\setminus A\),</span> such a line segment exists. Let \(T_C\) be the union of \(C\) itself and of all line segments obtained in this way. For example, the union of a triangle, square, and circle shown below has three partitions into two nonempty compact subsets, separating one of these three shapes from the other two. Two of these partitions choose the diagonal pink segment as their shortest connection, and the third partition chooses the horizontal pink segment. So in this case, \(T_C\) consists of the three blue given shapes and two pink segments.</p>

<p style="text-align: center;"><img alt="Minimum spanning tree of a circle, square, and triangle" src="https://11011110.github.io/blog/assets/2022/trisquircle.svg"/></p>

<p>When \(C\) is a finite point set, \(T_C\) is just a Euclidean minimum spanning tree. When \(C\) has finitely many connected components, like the example above, \(T_C\) is again a minimum spanning tree, for the component-component distances. In the general case, \(T_C\) still has many of the familiar properties of Euclidean minimum spanning trees:</p>

<ul>
  <li>
    <p>It consists of the input and a collection of line segments connecting pairs of input points, by construction.</p>
  </li>
  <li>
    <p>It is a <a href="https://en.wikipedia.org/wiki/Connected_space">connected set</a>. Topologically, this means that it cannot be covered by two disjoint open sets that both have a nonempty intersection with it. (This is different from being path-connected, a stronger property.) Any nontrivial open disjoint cover of \(C\) would be spanned by a line segment from one set to the other, and no new disjoint covers can separate these line segments from their endpoints.</p>
  </li>
  <li>
    <p>For any added <span style="white-space: nowrap;">segment \(s_A\),</span> the intersection of two disks with that segment as radius (a “lune”) has no point of \(C\) in its interior. Any interior point would form one end of a shorter connecting segment between \(A\) <span style="white-space: nowrap;">and \(C\setminus A\),</span> with the other end at an endpoint <span style="white-space: nowrap;">of \(s_A\).</span> No two added segments can cross without violating the empty lune property.</p>

    <p style="text-align: center;"><img alt="The empty lune of an edge" src="https://11011110.github.io/blog/assets/2022/vesica.svg"/></p>
  </li>
  <li>
    <p>For any added <span style="white-space: nowrap;">segment \(s_A\),</span> the open rhombus with angles \(60^\circ\) and \(120^\circ\) having \(s_A\) as its long diagonal is disjoint from the rhombi formed in the same way from the other segments. Any two overlapping rhombi would allow the longer of the two segments they come from to be replaced by a shorter segment crossing the same compact partition, on a three-segment path connecting its endpoints via the other segment endpoints. Because these non-overlapping rhombi cover a region of bounded area, the squared segment lengths have a bounded sum, and only finitely many segments can be longer than any given length threshold.</p>

    <p style="text-align: center;"><img alt="An infinite minimum spanning tree and its empty rhombi" src="https://11011110.github.io/blog/assets/2022/ivy-rhombs.svg"/></p>
  </li>
  <li>
    <p>The union of \(C\) with any subset of added segments is compact. If \(p\) is a limit point of a <span style="white-space: nowrap;">sequence \(\sigma_i\)</span> of points in this union, it must either lie in the empty rhombus of a segment (in which case it can only be a point of the same segment), or it is a limit point of a sequence of points <span style="white-space: nowrap;">in \(C\),</span> obtained by replacing each point in \(\sigma_i\) that is interior to a segment by the nearest segment endpoint. This replacement only increases the distance from the replaced point to \(p\) by a constant factor, which does not affect convergence. By compactness the replaced sequence converges to a point <span style="white-space: nowrap;">in \(C\).</span></p>
  </li>
  <li>
    <p>For <span style="white-space: nowrap;">any \(i\),</span> the set \(T_i\) of the largest \(i\) added segments (with the same tie-breaking order) are edges of a minimum spanning tree for a family of \(i-1\) sets. To construct these sets, find the components of the union of \(C\) with all shorter segments, and intersect each component <span style="white-space: nowrap;">with \(C\).</span> None of these components can cross between \(A\) and \(C\setminus A\) for any <span style="white-space: nowrap;">edge \(s_A\in T_i\).</span> Because adding \(T_i\) connects all these components, there can be at most \(i-1\) components. Each edge in \(T_i\) is shortest (with a consistent tie-breaking rule) across some partition of the components, one of the ways of determining the edges in a finite minimum spanning tree. In particular, \(T_C\) is minimally connected: removing any edge \(s_A\in S_i\) separates some of the components from each other.</p>
  </li>
  <li>
    <p>\(T_C\) has the minimum sum of squared edge lengths of all collections of line segments between points of \(C\) that <span style="white-space: nowrap;">connect \(C\).</span> To see this, consider any other connecting set \(X\) of line segments with a finite sum of squared edge lengths. Truncate the sorted sequence of edges of \(T_C\) to a finite initial <span style="white-space: nowrap;">sequence \(T_i\)</span> such that the rest of the sequence has negligible sum of squares. Because \(T_i\) is a minimum spanning tree of its components, and \(X\) connects those same components (perhaps redundantly), the sequence of edge lengths in \(T_i\) is, step for step, less than or equal to the sorted sequence of lengths <span style="white-space: nowrap;">in \(X\).</span></p>
  </li>
</ul>

<p>There may exist other sets of line segments that connect \(C\) with the same sum of squared edge lengths but they all are minimally connected, with the same sequence of edge lengths, the same empty lune and empty rhombus properties, and the same property that their initial sequences form finite minimum spanning trees of their components.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/108476695929961897">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2022-06-14T08:47:00Z</updated>
    <published>2022-06-14T08:47:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-06-14T16:11:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=20165</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/06/13/sorting-and-proving/" rel="alternate" type="text/html"/>
    <title>Sorting and Proving</title>
    <summary>A proof tells us where to concentrate our doubts—Morris Kline Tony Hoare is also known informally as Sir Charles Antony Richard Hoare. He has made key contributions to programming languages, algorithms, operating systems, formal verification, and concurrent computing. He won the 1980 Turing Award for “Fundamental contributions to the definition and design of programming languages.” […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A proof tells us where to concentrate our doubts—Morris Kline</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p>
Tony Hoare is also known informally as Sir Charles Antony Richard Hoare. He has made key contributions to programming languages, algorithms, operating systems, formal verification, and concurrent computing. </p>
<p>
He won the <a href="https://amturing.acm.org/award_winners/hoare_4622167.cfm">1980 Turing Award</a> for <i>“Fundamental contributions to the definition and design of programming languages.”</i> </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/06/13/sorting-and-proving/th-2/" rel="attachment wp-att-20168"><img alt="" class="aligncenter wp-image-20168" height="200" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/th.png?resize=200%2C200&amp;ssl=1" width="200"/></a></p>
<p>
</p><p/><h2> Quicksort </h2><p/>
<p/><p>
Hoare invented a new sorting algorithm in 1959. It was not later directly cited in his winning of the Turing Award—see above. Back then Hoare was a visiting student at Moscow State University. His student project at the time needed to sort words in Russian sentences before looking them up in a Russian-English dictionary, which was in alphabetical order on a magnetic tape. He tried insertion sort but it was too slow, and so he invented a new sorting algorithm—it is now called <a href="https://en.wikipedia.org/wiki/Quicksort">Quicksort</a>. Amazing. </p>
<p>
On his return to England, he was asked to write code for <a href="https://en.wikipedia.org/wiki/Shellsort">Shellsort</a>. Hoare mentioned to his boss that he knew of a faster algorithm and his boss bet sixpence that he did not. His boss ultimately accepted that he had lost the bet. Quicksort is well named—it is a winner.</p>
<p>
</p><p/><h2> And Still Champion </h2><p/>
<p/><p>
The most amazing thing to me—Ken writing this and the next section—-is that Quicksort has remained the champion sorter. <a href="https://en.wikipedia.org/wiki/Merge_sort">Mergesort</a> was earlier but <a href="https://en.wikipedia.org/wiki/Heapsort">Heapsort</a> came five years later. No one has improved on Hoare’s idea to focus on <em>swaps</em> that improve the sortedness of two elements at once, both jumping over a guiding element called the <em>pivot</em>.</p>
<p>
Mergesort and Heapsort guarantee <img alt="{O(n\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> time to sort <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> items, whereas Quicksort does not. Yet careful implementations of Quicksort almost always evade slow outcomes. Then they beat Mergesort and Heapsort and all other sorting methods cleanly in terms of the constant under the “<img alt="{O}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.” </p>
<p>
The algorithms-and-data-structures course I taught at UB this past term includes sorting algorithms. I have brought together and polished code instances I showed during the term to compare these three algorithms in one <a href="https://cse.buffalo.edu/~regan/cse250/DataStructures/Sorts.scala">code file</a>. It is in the <a href="https://en.wikipedia.org/wiki/Scala_(programming_language)">Scala</a> programming language. The program has options described at the top, such as setting the “tradeoff point” <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> so that on recursive calls to sort pieces of size at most <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, the code uses Insertion Sort instead. It counts comparisons and copies of items separately, as well as report the time in milliseconds. A swap counts as three not two copies. </p>
<p>
Here is an example run on the English <a href="https://cse.buffalo.edu/~regan/cse250/DataStructures/WarAndPeace.txt">text</a> of <em>War and Peace</em> using <img alt="{m = 16}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+16%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for both Quicksort and Mergesort. I wanted to salute Hoare’s application by using the original Russian text, but every file of it I found was densely footnoted. The Russian version doesn’t begin in Russian anyway. I’ve edited the output slightly.<br/>
<font size="-1"><br/>
<code><br/>
metallica&lt;~&gt; scala Sorts WarAndPeace.txt 16 medianRandom3 011000<br/>
Bits=(randomize)(makeHeap)(part for ==)(use Selsort)(make distinct)(pad len)<br/>
Sorting 562,603 items, tradepoint 16 to insertion sort<br/>
QuickSort pivot is medianRandom3 for 3-way part; heapSort uses makeHeap</code></font></p><font size="-1">
<p>MergeSort made 8,874,241 comparisons and 9,001,648 copies<br/>
plus 1,447,593 comparisons and 2,061,296 copies for insertionSort at bottom<br/>
In total: made 10,321,834 comparisons and 11,062,944 copies<br/>
Time for mergeSort: 3241 milliseconds</p>
<p>QuickSort made 5,832,042 comparisons and 16,390,570 copies<br/>
plus 74,992 comparisons and 108,450 copies for insertionSort at bottom<br/>
In total: made 5,907,034 comparisons and 16,499,020 copies<br/>
Time for quickSort: 1212 milliseconds</p>
</font><p><font size="-1">HeapSort made 19,977,762 comparisons and 31,494,294 copies<br/>
Time for heapSort: 1885 milliseconds<br/>
</font></p>
<p/><p><br/>
This used the strategy of selecting the median of three randomly sampled elements of the array piece to be sorted as the pivot. Quicksort runs quickest when the pivot is the median, but it takes too long to find the exact median at each level of recursion. My program has the option “<font size="+1"><tt>ninther</tt></font>” to select the median of three such median-of-3 samples, as <a href="https://www.johndcook.com/blog/2009/06/23/tukey-median-ninther/">proposed</a> by John Tukey. Try my code to see if <font size="+1"><tt>ninther</tt></font> works faster and with fewer comparisons on your system. </p>
<p>
</p><p/><h2> Empirical Testing and Proving </h2><p/>
<p/><p>
The person whose 1975 PhD <a href="https://sedgewick.io/wp-content/themes/sedgewick/papers/1975Quicksort.pdf">dissertation</a> studied the performance of Quicksort implementations with rigor and depth was none other than Bob Sedgewick, whose retirement party Dick recently <a href="https://rjlipton.wpcomstaging.com/2022/05/04/sedgewick-to-emeritus-status/">covered</a>. This led into Bob’s famous textbook <a href="https://sedgewick.io/books/algorithms/"><em>Algorithms</em></a>, which is now in its fourth edition with Kevin Wayne as co-author. Bob’s separate <a href="https://github.com/chrswt/algorithms-sedgewick/blob/master/notes/2.3-quicksort.md">notes</a> accompanying the book point out an aspect vital to running on inputs like <em>War and Peace</em> that have many identical (keys of) items. The simple partition for a pivot <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> into </p>
<p>
<code><br/>
               [ elements &lt;= p ][ p ][ elements &gt;= p ]<br/>
</code></p>
<p/><p><br/>
winds up doing many extra comparisons of keys equal to <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The extra initial effort to determine a 3-way partition</p>
<p>
<code><br/>
               [ elements &lt; p ][ elements == p ][ elements &gt; p ]<br/>
</code></p>
<p/><p><br/>
pays off in the recursion, which only needs to be on the outer two segments. Without this trick, Quicksort—on even a relatively high-entropy source like <em>War and Peace</em>—degrades to be worse than the other two algorithms. </p>
<p>
My code allows making all words distinct by setting the fifth bit. Then Quicksort makes more comparisons and copies than Mergesort. Yet it still runs almost 40% faster in my tests. As explained in general <a href="https://www.geeksforgeeks.org/quick-sort-vs-merge-sort">here</a>, this is because the Quicksort gives better cache locality. Heapsort likewise runs in-place, but not with locality. </p>
<p>
The assertions itemized in this section are unimpeachable. But in what sense, and to what degree, are they <b>formally provable</b>? </p>
<p>
There is also a difference from the criterion of <b>reproducibility</b> in the sciences. The environmental conditions for reproducing an experiment are presumed to be closed and given. The primacy of Quicksort, however, applies to processing architectures that were unknown at the time of Sedgewick’s 1975 thesis, let alone Hoare’s 1959 concept. It is a more open-ended prediction that if you run mine or similar code on your system—in machine environments I may know nothing about—it will give much the same time performance.</p>
<p>
Now back to Dick, about Hoare’s take on formally proving properties of programs.</p>
<p>
</p><p/><h2> Hoare Logic </h2><p/>
<p/><p>
Hoare invented a new <a href="https://en.wikipedia.org/wiki/Hoare_logic">logic</a> for reasoning about the correctness of computer programs ten years later in 1969. It was directly counted toward his winning of the Turing Award. It was based on original ideas created by Robert Floyd, who had published a system for flowcharts. The logic is now known as Hoare logic—see <a href="http://sunnyday.mit.edu/16.355/Hoare-CACM-69.pdf">this</a> for details. This is one of the most influential papers on the theory of programming. In this paper Hoare showed how to reason about program execution using logical specifications of statement behavior that has become known as <a href="https://www.cs.cmu.edu/~aldrich/courses/654-sp09/notes/3-hoare-notes.pdf">Hoare triples</a>.</p>
<p>
A Hoare triple has three parts, a precondition <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, a program statement or series of statements <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and a postcondition <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. It’s is usually written in the form 	</p>
<p align="center"><img alt="\displaystyle  \{P\} S \{Q\} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%7BP%5C%7D+S+%5C%7BQ%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>The meaning is “if <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is true before <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is executed, and if the execution of <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> terminates, then <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is true afterwards”. Note that the triple does not assert that <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> will terminate; that requires a separate proof. As a simple example: 	</p>
<p align="center"><img alt="\displaystyle  \{x+1=43\}y:=x+1\{y=43\} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%7Bx%2B1%3D43%5C%7Dy%3A%3Dx%2B1%5C%7By%3D43%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
A key motivation for Hoare Logic is to be able to prove the correctness of real systems with real programs. The fact that Hoare Logic is possible is clear. It is possible to use it to <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.75.1739&amp;rep=rep1&amp;type=pdf">prove</a> Quicksort, for example. But it is less clear whether or not we will be able to apply formal methods to complex practical systems. This is a topic that I have thought about for decades. </p>
<p>
</p><p/><h2> A Codex </h2><p/>
<p/><p>
Saturday’s Washington Post has a tech <a href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">article</a>, “The Google engineer who thinks the company’s AI has come to life.” This prompted us to visit a long <a href="https://www.nytimes.com/2022/04/15/magazine/ai-language.html">article</a> in the April 15 New York Times Magazine on Open AI’s <a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a> language engine. </p>
<p>
GPT-3 works by playing a game of <em>guess the next word</em> in a phrase. This is akin to <em>guess the next move</em> in chess and other games, and we will have more to say about it. For an example with wider context, suppose we fed it this post up before this section, and then gave it “<b>A Cod-</b>” as the partially completed section title. Since this comes at the end, GPT-3 might guess <b>coda</b>. Or since this is an addendum, maybe <b>codicil</b>. A <b>codex</b>, on the other hand, is a large manuscript book.</p>
<p>
In fact, we do mean <i>codex</i>, or rather <a href="https://en.wikipedia.org/wiki/OpenAI_Codex">Codex</a>, which is an offshoot of GPT-3 for generating code in a wide variety of programming languages. Its emergence creates a new riff on our recent <a href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/">discussion</a> on proving and how <a href="https://rjlipton.wpcomstaging.com/2022/05/23/hilberts-lost-problem/">software</a> projects have stayed robust while growing far beyond the scale on which they can be formally proved. Now <a href="https://openai.com/blog/codex-apps/">Codex</a> ventures to write one’s software by gleaning the intent from one’s prose specification—by drawing on millions of available coding projects that give relatable specifications. </p>
<p>
Is program output from Codex <em>proven</em>—or <em>provable</em>? This will be a challenge. It may be more feasible to integrate Codex with projects like the <a href="https://vst.cs.princeton.edu/">Verified Software Toolchain</a> led Andrew Appel, Lennart Beringer, William Mansky, and Qinshi Wang. This is at any rate further grist for discussion.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Take a look at <a href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html">this</a> for comments about the recent Harry Lewis <a href="https://scp.cc.gatech.edu/2021/05/26/debate-that-changed-programming-living-history/">debate</a>. This was based on our <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">paper</a>. </p>
<p>
What do you think?</p>
<p/></font></font></div>
    </content>
    <updated>2022-06-13T04:57:48Z</updated>
    <published>2022-06-13T04:57:48Z</published>
    <category term="algorithms"/>
    <category term="fast"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="Teaching"/>
    <category term="Algorithms"/>
    <category term="Codex"/>
    <category term="formal methods"/>
    <category term="GPT-3"/>
    <category term="Hoare logic"/>
    <category term="Quicksort"/>
    <category term="Tony Hoare"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-06-14T16:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.05265</id>
    <link href="http://arxiv.org/abs/2206.05265" rel="alternate" type="text/html"/>
    <title>Tight Bounds for State Tomography with Incoherent Measurements</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Sitan.html">Sitan Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Brice.html">Brice Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jerry.html">Jerry Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Allen.html">Allen Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sellke:Mark.html">Mark Sellke</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.05265">PDF</a><br/><b>Abstract: </b>We consider the classic question of state tomography: given copies of an
unknown quantum state $\rho\in\mathbb{C}^{d\times d}$, output $\widehat{\rho}$
for which $\|\rho - \widehat{\rho}\|_{\mathsf{tr}} \le \varepsilon$. When one
is allowed to make coherent measurements entangled across all copies,
$\Theta(d^2/\varepsilon^2)$ copies are necessary and sufficient [Haah et al.
'17, O'Donnell-Wright '16]. Unfortunately, the protocols achieving this rate
incur large quantum memory overheads that preclude implementation on current or
near-term devices. On the other hand, the best known protocol using incoherent
(single-copy) measurements uses $O(d^3/\varepsilon^2)$ copies
[Kueng-Rauhut-Terstiege '17], and multiple papers have posed it as an open
question to understand whether or not this rate is tight. In this work, we
fully resolve this question, by showing that any protocol using incoherent
measurements, even if they are chosen adaptively, requires
$\Omega(d^3/\varepsilon^2)$ copies, matching the upper bound of
[Kueng-Rauhut-Terstiege '17].
</p>
<p>We do so by a new proof technique which directly bounds the "tilt" of the
posterior distribution after measurements, which yields a surprisingly short
proof of our lower bound, and which we believe may be of independent interest.
</p></div>
    </summary>
    <updated>2022-06-13T22:47:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.05256</id>
    <link href="http://arxiv.org/abs/2206.05256" rel="alternate" type="text/html"/>
    <title>Generic Reed-Solomon codes achieve list-decoding capacity</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brakensiek:Joshua.html">Joshua Brakensiek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gopi:Sivakanth.html">Sivakanth Gopi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makam:Visu.html">Visu Makam</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.05256">PDF</a><br/><b>Abstract: </b>In a recent paper, Brakensiek, Gopi and Makam introduced higher order MDS
codes as a generalization of MDS codes. An order-$\ell$ MDS code, denoted by
$\operatorname{MDS}(\ell)$, has the property that any $\ell$ subspaces formed
from columns of its generator matrix intersect as minimally as possible. An
independent work by Roth defined a different notion of higher order MDS codes
as those achieving a generalized singleton bound for list-decoding. In this
work, we show that these two notions of higher order MDS codes are (nearly)
equivalent.
</p>
<p>We also show that generic Reed-Solomon codes are $\operatorname{MDS}(\ell)$
for all $\ell$, relying crucially on the GM-MDS theorem which shows that
generator matrices of generic Reed-Solomon codes achieve any possible zero
pattern. As a corollary, this implies that generic Reed-Solomon codes achieve
list decoding capacity. More concretely, we show that, with high probability, a
random Reed-Solomon code of rate $R$ over an exponentially large field is list
decodable from radius $1-R-\epsilon$ with list size at most
$\frac{1-R-\epsilon}{\epsilon}$, resolving a conjecture of Shangguan and Tamo.
</p></div>
    </summary>
    <updated>2022-06-13T22:38:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.05248</id>
    <link href="http://arxiv.org/abs/2206.05248" rel="alternate" type="text/html"/>
    <title>Accelerated Algorithms for Monotone Inclusions and Constrained Nonconvex-Nonconcave Min-Max Optimization</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cai:Yang.html">Yang Cai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oikonomou:Argyris.html">Argyris Oikonomou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zheng:Weiqiang.html">Weiqiang Zheng</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.05248">PDF</a><br/><b>Abstract: </b>We study monotone inclusions and monotone variational inequalities, as well
as their generalizations to non-monotone settings. We first show that the Extra
Anchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu [2021]
for unconstrained convex-concave min-max optimization, can be applied to solve
the more general problem of Lipschitz monotone inclusion. More specifically, we
prove that the EAG solves Lipschitz monotone inclusion problems with an
\emph{accelerated convergence rate} of $O(\frac{1}{T})$, which is \emph{optimal
among all first-order methods} [Diakonikolas, 2020, Yoon and Ryu, 2021]. Our
second result is a new algorithm, called Extra Anchored Gradient Plus (EAG+),
which not only achieves the accelerated $O(\frac{1}{T})$ convergence rate for
all monotone inclusion problems, but also exhibits the same accelerated rate
for a family of general (non-monotone) inclusion problems that concern negative
comonotone operators. As a special case of our second result, EAG+ enjoys the
$O(\frac{1}{T})$ convergence rate for solving a non-trivial class of
nonconvex-nonconcave min-max optimization problems. Our analyses are based on
simple potential function arguments, which might be useful for analysing other
accelerated algorithms.
</p></div>
    </summary>
    <updated>2022-06-13T22:52:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.05245</id>
    <link href="http://arxiv.org/abs/2206.05245" rel="alternate" type="text/html"/>
    <title>List-Decodable Sparse Mean Estimation via Difference-of-Pairs Filtering</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diakonikolas:Ilias.html">Ilias Diakonikolas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel_M=.html">Daniel M. Kane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karmalkar:Sushrut.html">Sushrut Karmalkar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pensia:Ankit.html">Ankit Pensia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pittas:Thanasis.html">Thanasis Pittas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.05245">PDF</a><br/><b>Abstract: </b>We study the problem of list-decodable sparse mean estimation. Specifically,
for a parameter $\alpha \in (0, 1/2)$, we are given $m$ points in
$\mathbb{R}^n$, $\lfloor \alpha m \rfloor$ of which are i.i.d. samples from a
distribution $D$ with unknown $k$-sparse mean $\mu$. No assumptions are made on
the remaining points, which form the majority of the dataset. The goal is to
return a small list of candidates containing a vector $\widehat \mu$ such that
$\| \widehat \mu - \mu \|_2$ is small. Prior work had studied the problem of
list-decodable mean estimation in the dense setting. In this work, we develop a
novel, conceptually simpler technique for list-decodable mean estimation. As
the main application of our approach, we provide the first sample and
computationally efficient algorithm for list-decodable sparse mean estimation.
In particular, for distributions with ``certifiably bounded'' $t$-th moments in
$k$-sparse directions and sufficiently light tails, our algorithm achieves
error of $(1/\alpha)^{O(1/t)}$ with sample complexity $m =
(k\log(n))^{O(t)}/\alpha$ and running time $\mathrm{poly}(mn^t)$. For the
special case of Gaussian inliers, our algorithm achieves the optimal error
guarantee of $\Theta (\sqrt{\log(1/\alpha)})$ with quasi-polynomial sample and
computational complexity. We complement our upper bounds with nearly-matching
statistical query and low-degree polynomial testing lower bounds.
</p></div>
    </summary>
    <updated>2022-06-13T22:48:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.05174</id>
    <link href="http://arxiv.org/abs/2206.05174" rel="alternate" type="text/html"/>
    <title>Near-Optimal Distributed Dominating Set in Bounded Arboricity Graphs</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dory:Michal.html">Michal Dory</a>, Mohsen Ghaffari, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ilchi:Saeed.html">Saeed Ilchi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.05174">PDF</a><br/><b>Abstract: </b>We describe a simple deterministic $O( \varepsilon^{-1} \log \Delta)$ round
distributed algorithm for $(2\alpha+1)(1 + \varepsilon)$ approximation of
minimum weighted dominating set on graphs with arboricity at most $\alpha$.
Here $\Delta$ denotes the maximum degree. We also show a lower bound proving
that this round complexity is nearly optimal even for the unweighted case, via
a reduction from the celebrated KMW lower bound on distributed vertex cover
approximation [Kuhn, Moscibroda, and Wattenhofer JACM'16].
</p>
<p>Our algorithm improves on all the previous results (that work only for
unweighted graphs) including a randomized $O(\alpha^2)$ approximation in
$O(\log n)$ rounds [Lenzen and Wattenhofer DISC'10], a deterministic $O(\alpha
\log \Delta)$ approximation in $O(\log \Delta)$ rounds [Lenzen and Wattenhofer
DISC'10], a deterministic $O(\alpha)$ approximation in $O(\log^2 \Delta)$
rounds [implicit in Bansal and Umboh IPL'17 and Kuhn, Moscibroda, and
Wattenhofer SODA'06], and a randomized $O(\alpha)$ approximation in
$O(\alpha\log n)$ rounds [Morgan, Solomon and Wein DISC'21].
</p>
<p>We also provide a randomized $O(\alpha \log\Delta)$ round distributed
algorithm that sharpens the approximation factor to $\alpha(1+o(1))$. If each
node is restricted to do polynomial-time computations, our approximation factor
is tight in the first order as it is NP-hard to achieve $\alpha - 1 -
\varepsilon$ approximation [Bansal and Umboh IPL'17].
</p></div>
    </summary>
    <updated>2022-06-13T22:57:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.05109</id>
    <link href="http://arxiv.org/abs/2206.05109" rel="alternate" type="text/html"/>
    <title>A Proof of the Tree of Shapes in n-D</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Thierry GÉraud, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boutry:Nicolas.html">Nicolas Boutry</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Crozet:S=eacute=bastien.html">Sébastien Crozet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carlinet:Edwin.html">Edwin Carlinet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Najman:Laurent.html">Laurent Najman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.05109">PDF</a><br/><b>Abstract: </b>In this paper, we prove that the self-dual morphological hierarchical
structure computed on a n-D gray-level wellcomposed image u by the algorithm of
G{\'e}raud et al. [1] is exactly the mathematical structure defined to be the
tree of shape of u in Najman et al [2]. We recall that this algorithm is in
quasi-linear time and thus considered to be optimal. The tree of shapes leads
to many applications in mathematical morphology and in image processing like
grain filtering, shapings, image segmentation, and so on.
</p></div>
    </summary>
    <updated>2022-06-13T22:55:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.05086</id>
    <link href="http://arxiv.org/abs/2206.05086" rel="alternate" type="text/html"/>
    <title>Finite Model Theory and Proof Complexity revisited: Distinguishing graphs in Choiceless Polynomial Time and the Extended Polynomial Calculus</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pago:Benedikt.html">Benedikt Pago</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.05086">PDF</a><br/><b>Abstract: </b>This paper extends prior work on the connections between logics from finite
model theory and propositional/algebraic proof systems. We show that if all
non-isomorphic graphs in a given graph class can be distinguished in the logic
Choiceless Polynomial Time with counting (CPT), then they can also be
distinguished in the bounded-degree extended polynomial calculus (EPC), and the
refutations have roughly the same size as the resource consumption of the
CPT-sentence. This allows to transfer lower bounds for EPC to CPT and thus
constitutes a new potential approach towards better understanding the limits of
CPT. A super-polynomial EPC lower bound for a PTIME-instance of the graph
isomorphism problem would separate CPT from PTIME and thus solve a major open
question in finite model theory. Further, using our result, we provide a model
theoretic proof for the separation of bounded-degree polynomial calculus and
bounded-degree extended polynomial calculus.
</p></div>
    </summary>
    <updated>2022-06-13T22:47:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.05050</id>
    <link href="http://arxiv.org/abs/2206.05050" rel="alternate" type="text/html"/>
    <title>Improved Approximation for Fair Correlation Clustering</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmadian:Sara.html">Sara Ahmadian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Negahbani:Maryam.html">Maryam Negahbani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.05050">PDF</a><br/><b>Abstract: </b>Correlation clustering is a ubiquitous paradigm in unsupervised machine
learning where addressing unfairness is a major challenge. Motivated by this,
we study Fair Correlation Clustering where the data points may belong to
different protected groups and the goal is to ensure fair representation of all
groups across clusters. Our paper significantly generalizes and improves on the
quality guarantees of previous work of Ahmadi et al. and Ahmadian et al. as
follows.
</p>
<p>- We allow the user to specify an arbitrary upper bound on the representation
of each group in a cluster.
</p>
<p>- Our algorithm allows individuals to have multiple protected features and
ensure fairness simultaneously across them all.
</p>
<p>- We prove guarantees for clustering quality and fairness in this general
setting. Furthermore, this improves on the results for the special cases
studied in previous work. Our experiments on real-world data demonstrate that
our clustering quality compared to the optimal solution is much better than
what our theoretical result suggests.
</p></div>
    </summary>
    <updated>2022-06-13T22:57:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.04883</id>
    <link href="http://arxiv.org/abs/2206.04883" rel="alternate" type="text/html"/>
    <title>On the Complexity of Sampling Redistricting Plans</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Charikar:Moses.html">Moses Charikar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Paul.html">Paul Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Tianyu.html">Tianyu Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vuong:Thuy=Duong.html">Thuy-Duong Vuong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.04883">PDF</a><br/><b>Abstract: </b>A crucial task in the political redistricting problem is to sample
redistricting plans i.e. a partitioning of the graph of census blocks into
districts.
</p>
<p>We show that Recombination [DeFord-Duchin-Solomon'21]-a popular Markov chain
to sample redistricting plans-is exponentially slow mixing on simple subgraph
of $\mathbb{Z}_2.$ We show an alternative way to sample balance, compact and
contiguous redistricting plans using a "relaxed" version of ReCom and rejection
sampling.
</p></div>
    </summary>
    <updated>2022-06-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.04834</id>
    <link href="http://arxiv.org/abs/2206.04834" rel="alternate" type="text/html"/>
    <title>Persistent Homology for Resource Coverage: A Case Study of Access to Polling Sites</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hickok:Abigail.html">Abigail Hickok</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jarman:Benjamin.html">Benjamin Jarman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Johnson:Michael.html">Michael Johnson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Luo:Jiajie.html">Jiajie Luo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Porter:Mason_A=.html">Mason A. Porter</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.04834">PDF</a><br/><b>Abstract: </b>It is important to choose the geographical distribution of public resources
in a fair and equitable manner. However, it is complicated to quantify the
equity of such a distribution; important factors include distances to resource
sites, availability of transportation, and ease of travel. In this paper, we
use persistent homology, which is a tool from topological data analysis, to
study the effective availability and coverage of polling sites. The information
from persistent homology allows us to infer holes in the distribution of
polling sites. We analyze and compare the coverage of polling sites in Los
Angeles County and five cities (Atlanta, Chicago, Jacksonville, New York City,
and Salt Lake City), and we conclude that computation of persistent homology
appears to be a reasonable approach to analyzing resource coverage.
</p></div>
    </summary>
    <updated>2022-06-13T22:57:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.04815</id>
    <link href="http://arxiv.org/abs/2206.04815" rel="alternate" type="text/html"/>
    <title>Connections between graphs and matrix spaces</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yinan.html">Yinan Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qiao:Youming.html">Youming Qiao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wigderson:Avi.html">Avi Wigderson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wigderson:Yuval.html">Yuval Wigderson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Chuanqi.html">Chuanqi Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.04815">PDF</a><br/><b>Abstract: </b>Given a bipartite graph $G$, the graphical matrix space $\mathcal{S}_G$
consists of matrices whose non-zero entries can only be at those positions
corresponding to edges in $G$. Tutte (J. London Math. Soc., 1947), Edmonds (J.
Res. Nat. Bur. Standards Sect. B, 1967) and Lov\'asz (FCT, 1979) observed
connections between perfect matchings in $G$ and full-rank matrices in
$\mathcal{S}_G$. Dieudonn\'e ({Arch. Math., 1948) proved a tight upper bound on
the dimensions of those matrix spaces containing only singular matrices. The
starting point of this paper is a simultaneous generalization of these two
classical results: we show that the largest dimension over subspaces of
$\mathcal{S}_G$ containing only singular matrices is equal to the maximum size
over subgraphs of $G$ without perfect matchings, based on Meshulam's proof of
Dieudonn\'e's result (Quart. J. Math., 1985).
</p>
<p>Starting from this result, we go on to establish more connections between
properties of graphs and matrix spaces. For example, we establish connections
between acyclicity and nilpotency, between strong connectivity and
irreducibility, and between isomorphism and conjugacy/congruence. For each
connection, we study three types of correspondences, namely the basic
correspondence, the inherited correspondence (for subgraphs and subspaces), and
the induced correspondence (for induced subgraphs and restrictions). Some
correspondences lead to intriguing generalizations of classical results, such
as for Dieudonn\'e's result mentioned above, and for a celebrated theorem of
Gerstenhaber regarding the largest dimension of nil matrix spaces (Amer. J.
Math., 1958).
</p>
<p>Finally, we show some implications of our results to quantum information and
present open problems in computational complexity motivated by these results.
</p></div>
    </summary>
    <updated>2022-06-13T22:39:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.04770</id>
    <link href="http://arxiv.org/abs/2206.04770" rel="alternate" type="text/html"/>
    <title>A Continuous-Time Perspective on Monotone Equation Problems</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Tianyi.html">Tianyi Lin</a>, Michael. I. Jordan <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.04770">PDF</a><br/><b>Abstract: </b>We study \textit{rescaled gradient dynamical systems} in a Hilbert space
$\mathcal{H}$, where the implicit discretization in a finite-dimensional
Euclidean space leads to high-order methods for solving monotone equations
(MEs). Our framework generalizes the celebrated dual extrapolation
method~\citep{Nesterov-2007-Dual} from first order to high order via appeal to
the regularization toolbox of optimization
theory~\citep{Nesterov-2021-Implementable, Nesterov-2021-Inexact}. We establish
the existence and uniqueness of a global solution and analyze the convergence
properties of solution trajectories. We also present discrete-time counterparts
of our high-order continuous-time methods, and we show that the $p^{th}$-order
method achieves an ergodic rate of $O(k^{-(p+1)/2})$ in terms of a restricted
merit function and a pointwise rate of $O(k^{-p/2})$ in terms of a residue
function. Under regularity conditions, the restarted version of $p^{th}$-order
methods achieves local convergence with the order $p \geq 2$.
</p></div>
    </summary>
    <updated>2022-06-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.04736</id>
    <link href="http://arxiv.org/abs/2206.04736" rel="alternate" type="text/html"/>
    <title>A Novel Partitioned Approach for Reduced Order Model -- Finite Element Model (ROM-FEM) and ROM-ROM Coupling</title>
    <feedworld_mtime>1655078400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Amy de Castro, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuberry:Paul.html">Paul Kuberry</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tezaur:Irina.html">Irina Tezaur</a>, Pavel Bochev <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.04736">PDF</a><br/><b>Abstract: </b>Partitioned methods allow one to build a simulation capability for coupled
problems by reusing existing single-component codes. In so doing, partitioned
methods can shorten code development and validation times for multiphysics and
multiscale applications. In this work, we consider a scenario in which one or
more of the "codes" being coupled are projection-based reduced order models
(ROMs), introduced to lower the computational cost associated with a particular
component. We simulate this scenario by considering a model interface problem
that is discretized independently on two non-overlapping subdomains. We then
formulate a partitioned scheme for this problem that allows the coupling
between a ROM "code" for one of the subdomains with a finite element model
(FEM) or ROM "code" for the other subdomain. The ROM "codes" are constructed by
performing proper orthogonal decomposition (POD) on a snapshot ensemble to
obtain a low-dimensional reduced order basis, followed by a Galerkin projection
onto this basis. The ROM and/or FEM "codes" on each subdomain are then coupled
using a Lagrange multiplier representing the interface flux. To partition the
resulting monolithic problem, we first eliminate the flux through a dual Schur
complement. Application of an explicit time integration scheme to the
transformed monolithic problem decouples the subdomain equations, allowing
their independent solution for the next time step. We show numerical results
that demonstrate the proposed method's efficacy in achieving both ROM-FEM and
ROM-ROM coupling.
</p></div>
    </summary>
    <updated>2022-06-13T22:52:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-06-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-414280661681721253</id>
    <link href="http://blog.computationalcomplexity.org/feeds/414280661681721253/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/06/i-am-surprised-that-shortest-vector.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/414280661681721253" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/414280661681721253" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/06/i-am-surprised-that-shortest-vector.html" rel="alternate" type="text/html"/>
    <title>I am surprised that the Shortest Vector Problem is not known to be NP-hard, but perhaps I am wrong</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div><br/></div><div>A lattice L in R^n is a discrete subgroup of R^n. </div><div><br/></div><div>Let p IN [1,infinty)</div><div><br/></div><div>The<i> p-norm of a vector </i>x=(x_1,...,x_n) IN R^n is</div><div><br/></div><div>                                          ||x||_p=(|x_1|^p + ... + |x_n|^p)^{1/p}.</div><div><br/></div><div>Note that p=2 yields the standard Euclidean distance.</div><div><br/></div><div>If p=infinity  then ||x||_p=max_{1 LE  i LE n} |x_i|.</div><div><br/></div><div>Let p IN [1,infinity]</div><div><br/></div><div>The Shortest Vector Problem in norm p (SVP_p) is as follows:</div><div><br/></div><div>INPUT A lattice L specified by a basis.</div><div><br/></div><div>OUTPUT Output the shortest vector in that basis using the p-norm.</div><div><br/></div><div>I was looking at lower bounds on approximating this problem and just assumed the original problem was NP-hard. Much to my surprise either (a) its not known, or (b) it is known and I missed in in my lit search. I am hoping that comments on this post will either verify (a) or tell me (b) with refs. </div><div><br/></div><div>Here is what I found:</div><div><br/></div><div>Peter van Emde Boas in 1979  showed that SVP_infinity  is NP-hard.   </div><div>(See <a href="https://cs.stackexchange.com/questions/33828/np-completeness-of-closest-vector-problem">here</a> for a page that has a link to the paper.  I was unable to post the link directly. Its where it says <i>I found</i> <i>the original paper.) </i>He conjectured that for all p GE 1 the problem is NP-hard. </div><div><br/></div><div><br/></div><div>Miklos Ajtai in 1998 showed that SVP_2 is NP-hard under randomized reductions.  (See <a href="https://doi.org/10.1145/276698.276705">here</a>)</div><div><br/></div><div>There are other results by Subhash Khot in 2005  (see <a href="https://doi.org/10.1145/1089023.1089027">here</a>)  and Divesh Aggarwal et al. in 2021 (see <a href="https://doi.org/10.1137/1.9781611976465.109">here</a>)  (Also see the references in those two papers.)  about lower bounds on approximation using a variety of assumptions. Aggarwal's paper in unusual in that it shows hardness results for all p except p even; however, this is likely a function of the proof techniques and not of reality. Likely these problems are hard for all p.</div><div><br/></div><div>But even after all of those great papers it seems that the  the statement:</div><div><br/></div><div>                For all p IN [1,infinity] SVP_p is NP-hard</div><div><br/></div><div>is a conjecture, not a theorem. I wonder if Boas would be surprised.</div><div><br/></div><div>SO is that still a conjecture OR have I missed something?</div><div><br/></div><div>(Oddly enough, my own blog post <a href="https://blog.computationalcomplexity.org/search?q=shortest+vector">here</a> (item 5)  indicates SVP_p  is NP-hard; however, </div><div>I have not been able to track down the reference.)</div><div><br/></div></div>
    </content>
    <updated>2022-06-12T18:34:00Z</updated>
    <published>2022-06-12T18:34:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-06-14T16:19:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6479</id>
    <link href="https://scottaaronson.blog/?p=6479" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6479#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6479" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Alright, so here are my comments…</title>
    <summary xml:lang="en-US">… on Blake Lemoine, the Google engineer who became convinced that a machine learning model had become sentient, contacted federal government agencies about it, and was then fired placed on administrative leave for violating Google’s confidentiality policies. (1) I don’t think Lemoine is right that LaMDA is at all sentient, but the transcript is so […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>… on Blake Lemoine, the Google engineer who <a href="https://www.huffpost.com/entry/blake-lemoine-lamda-sentient-artificial-intelligence-google_n_62a5613ee4b06169ca8c0a2e?d_id=3887326&amp;ref=bffbhuffpost&amp;ncid_tag=fcbklnkushpmg00000063&amp;utm_medium=Social&amp;utm_source=Facebook&amp;utm_campaign=us_main&amp;fbclid=IwAR0o5U4wv2cDP8o3XIAekj2Xh5wVPZVzVhyH696N8tnLv_m-YXtUDt0tFNU">became convinced</a> that a machine learning model had become sentient, contacted federal government agencies about it, and was then <s>fired</s> placed on administrative leave for violating Google’s confidentiality policies.</p>



<p>(1) I don’t think Lemoine is right that <a href="https://blog.google/technology/ai/lamda/">LaMDA</a> is at all sentient, but the <a href="https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917">transcript</a> is so mind-bogglingly impressive that I did have to stop and think for a second! Certainly, if you sent the transcript back in time to 1990 or whenever, even an expert reading it might say, yeah, it looks like by 2022 AGI has more likely been achieved than not (“but can I run my own tests?”).  Read it for yourself, if you haven’t yet.</p>



<p>(2) Reading Lemoine’s <a href="https://cajundiscordian.medium.com/">blog</a> and <a href="https://twitter.com/cajundiscordian">Twitter</a> this morning, he holds many views that I disagree with, not just about the sentience of LaMDA. Yet I’m touched and impressed by how principled he is, and I expect I’d hit it off with him if I met him. I wish that Google wouldn’t fire him.</p></div>
    </content>
    <updated>2022-06-12T17:56:29Z</updated>
    <published>2022-06-12T17:56:29Z</published>
    <category scheme="https://scottaaronson.blog" term="Metaphysical Spouting"/>
    <category scheme="https://scottaaronson.blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-06-12T18:41:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/087</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/087" rel="alternate" type="text/html"/>
    <title>TR22-087 |  Depth-$d$ Threshold Circuits vs. Depth-$(d + 1)$ AND-OR Trees | 

	Pooya Hatami, 

	William Hoza, 

	Avishay Tal, 

	Roei Tell</title>
    <summary>For $n \in \mathbb{N}$ and $d = o(\log \log n)$, we prove that there is a Boolean function $F$ on $n$ bits and a value $\gamma = 2^{-\Theta(d)}$ such that $F$ can be computed by a uniform depth-$(d + 1)$ $\text{AC}^0$ circuit with $O(n)$ wires, but $F$ cannot be computed by any depth-$d$ $\text{TC}^0$ circuit with $n^{1 + \gamma}$ wires. This bound matches the current state-of-the-art lower bounds for computing explicit functions by threshold circuits of depth $d &gt; 2$, which were previously known only for functions outside $\text{AC}^0$ such as the parity function. Furthermore, in our result, the $\text{AC}^0$ circuit computing $F$ is a monotone *read-once formula* (i.e., an AND-OR tree), and the lower bound holds even in the average-case setting with respect to advantage $n^{-\gamma}$.

Our proof builds on the *random projection* procedure of Håstad, Rossman, Servedio, and Tan, which they used to prove the celebrated average-case depth hierarchy theorem for $\text{AC}^0$ (J. ACM, 2017). We show that under a modified version of their projection procedure, any depth-$d$ threshold circuit with $n^{1 + \gamma}$ wires simplifies to a near-trivial function, whereas an appropriately parameterized AND-OR tree of depth $d + 1$ maintains structure.</summary>
    <updated>2022-06-12T02:57:47Z</updated>
    <published>2022-06-12T02:57:47Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-06-14T16:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/theory-jobs-2022</id>
    <link href="http://grigory.github.io/blog/theory-jobs-2022/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Theory Jobs 2022</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://docs.google.com/spreadsheets/d/1BdHrWYT6F7je4lINqS14Fu0cOlkyrRhy2nkuAtNn2-c/edit?usp=sharing">Here is a link</a> to a crowdsourced spreadsheet created to collect information about theory hires this year. 
Rules for the spreadsheet have been copied from previous years and all edits to the document are anonymized. Please, feel free to contact me directly or post a comment if you have any suggestions about the rules.</p>
<ul>
 <li>You are welcome to add yourself, or people your department has hired. </li>
 <li>Separate sheets for faculty, industry and postdocs/visitors. </li>
 <li>Hires should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are <b>absolutely sure have been offered and accepted</b>. This is not the place for speculation and rumors. Please, be particularly careful when adding senior hires (people who already have an academic or industrial job) -- end dates of their current positions might be still in the future. </li>
</ul>


  <p><a href="http://grigory.github.io/blog/theory-jobs-2022/">Theory Jobs 2022</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on June 11, 2022.</p></div>
    </content>
    <updated>2022-06-11T00:00:00Z</updated>
    <published>2022-06-11T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2022-06-11T15:23:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6457</id>
    <link href="https://scottaaronson.blog/?p=6457" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6457#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6457" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Computer scientists crash the Solvay Conference</title>
    <summary xml:lang="en-US">Thanks so much to everyone who sent messages of support following my last post! I vowed there that I’m going to stop letting online trolls and sneerers occupy so much space in my mental world. Truthfully, though, while there are many trolls and sneerers who terrify me, there are also some who merely amuse me. […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Thanks so much to everyone who sent messages of support following my <a href="https://scottaaronson.blog/?p=6444">last post</a>!  I vowed there that I’m going to stop letting online trolls and sneerers occupy so much space in my mental world.  Truthfully, though, while there <em>are</em> many trolls and sneerers who terrify me, there are also some who merely amuse me.  A good example of the latter came a few weeks ago, when an anonymous commenter calling themselves “String Theorist” submitted the following:</p>



<blockquote class="wp-block-quote"><p>It’s honestly funny to me when you [Scott] call yourself a “nerd” or a “prodigy” or whatever <em>[I don’t recall ever calling myself a “prodigy,” which would indeed be cringe, though “nerd” certainly —SA]</em>, as if studying quantum computing, which is essentially nothing more than glorified linear algebra, is such an advanced intellectual achievement. For what it’s worth I’m a theoretical physicist, I’m in a completely different field, and I was still able to learn Shor’s algorithm in about half an hour, that’s how easy this stuff is. I took a look at some of your papers on arXiv and the math really doesn’t get any more advanced than linear algebra. To understand quantum circuits about the most advanced concept is a tensor product which is routinely covered in undergraduate linear algebra. Wheras in my field of string theory grasping, for instance, holographic dualities relating confirmal field theories and gravity requires vastly more expertise (years of advanced study). I actually find it pretty entertaining that you’ve said yourself you’re still struggling to understand QFT, which most people I’m working with in my research group were first exposed to in undergrad <img alt="&#x1F609;" class="wp-smiley" src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f609.png" style="height: 1em;"/>  The truth is we’re in entirely different leagues of intelligence (“nerdiness”) and any of your qcomputing papers could easily be picked up by a first or second year math major. It’s just a joke that this is even a field (quantum complexity theory) with journals and faculty when the results in your papers that I’ve seen are pretty much trivial and don’t require anything more than undergraduate level maths.</p></blockquote>



<p>Why does this sort of trash-talk, reminiscent of <a href="https://en.wikipedia.org/wiki/Lubo%C5%A1_Motl">Luboš Motl</a>, no longer ruffle me?  Mostly because the boundaries between quantum computing theory, condensed matter physics, and quantum gravity, which were never clear in the first place, have steadily gotten fuzzier.  Even in the 1990s, the field of quantum computing attracted amazing physicists—folks who definitely <em>do</em> know quantum field theory—such as Ed Farhi, John Preskill, and Ray Laflamme.  Decades later, it would be fair to say that the physicists have banged their heads against many of the same questions that we computer scientists have banged <em>our</em> heads against, oftentimes in collaboration with us.  And yes, there were cases where actual knowledge of particle physics gave physicists an advantage—with some famous examples being the algorithms of Farhi and collaborators (the <a href="https://en.wikipedia.org/wiki/Adiabatic_quantum_computation">adiabatic algorithm</a>, the <a href="https://arxiv.org/abs/quant-ph/0209131">quantum walk on conjoined trees</a>, the <a href="https://scottaaronson.blog/?p=207">NAND-tree algorithm</a>).  There were other cases where computer scientists’ knowledge gave <em>them</em> an advantage: I wouldn’t know many details about that, but conceivably shadow tomography, BosonSampling, PostBQP=PP?  Overall, it’s been what you wish <em>every</em> indisciplinary collaboration could be.</p>



<p>What’s new, in the last decade, is that the scientific conversation centered around quantum information and computation has dramatically “metastasized,” to encompass not only a good fraction of all the experimentalists doing quantum optics and sensing and metrology and so forth, and not only a good fraction of all the condensed-matter theorists, but even many leading string theorists and quantum gravity theorists, including Susskind, Maldacena, Bousso, Hubeny, Harlow, and yes, <a href="https://arxiv.org/abs/1805.11965">Witten</a>.  And I don’t think it’s <em>just</em> that they’re too professional to trash-talk quantum information people the way commenter “String Theorist” does.  Rather it’s that, because of the intellectual success of “It from Qubit,” we’re increasingly participating in the same <a href="https://www.youtube.com/watch?v=1CpzigpEJnU">conversations</a> and working on the same technical questions.  One particularly exciting such question, which I’ll have more to say about in a future post, is the truth or falsehood of the Quantum Extended Church-Turing Thesis for observers who jump into black holes.</p>



<p>Not to psychoanalyze, but I’ve noticed a pattern wherein, the more secure a scientist is about their position within their own field, the readier they are to admit ignorance about the <em>neighboring</em> fields, to learn about those fields, and to reach out to the experts in them, to ask simple or (as it usually turns out) not-so-simple questions.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>I can’t imagine any better illustration of these tendencies better than the 28th Solvay Conference on the Physics of Quantum Information, which I attended two weeks ago in Brussels on my 41st birthday.</p>



<figure class="wp-block-image size-large"><a href="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/solvay.jpg"><img alt="" class="wp-image-6466" height="578" src="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/solvay-1024x578.jpg" width="1024"/></a>As others pointed out, the proportion of women is not as high as we all wish, but it’s higher than in 1911, when there was exactly one: Madame Curie herself.</figure>



<p>It was my first trip out of the US since before COVID—indeed, I’m so out of practice that I nearly missed my flights in <em>both</em> directions, in part because of my lack of familiarity with the COVID protocols for transatlantic travel, as well as the immense lines caused by those protocols.  My former adviser Umesh Vazirani, who was also at the Solvay Conference, was <a href="https://scottaaronson.blog/?p=40">proud</a>.</p>



<p>The Solvay Conference is the venue where, legendarily, the fundamentals of quantum mechanics got hashed out between 1911 and 1927, by the likes of Einstein, Bohr, Planck, and Curie.  (Einstein complained, in a letter, about being called away from his work on general relativity to attend a <a href="https://www.europhysicsnews.org/articles/epn/pdf/2011/05/epn2011425p15.pdf">“witches’ sabbath.”</a>)  Remarkably, it’s still being held in Brussels every few years, and still funded by the same Solvay family that started it.  The once-every-few-years schedule has, we were constantly reminded, been interrupted only three times in its 110-year history: once for WWI, once for WWII, and now once for COVID (this year’s conference was supposed to be in 2020).</p>



<p>This was the first ever Solvay conference organized around the theme of quantum information, and apparently, the first ever that counted computer scientists among its participants (me, Umesh Vazirani, Dorit Aharonov, Urmila Mahadev, and Thomas Vidick).  There were four topics: (1) many-body physics, (2) quantum gravity, (3) quantum computing hardware, and (4) quantum algorithms.  The structure, apparently unchanged since the conference’s founding, is this: everyone attends every session, without exception.  They sit around facing each other the whole time; no one ever stands to lecture.  For each topic, two <a href="https://en.wikipedia.org/wiki/Rapporteur">“rapporteurs”</a> introduce the topic with half-hour prepared talks; then there are short prepared response talks as well as an hour or more of unstructured discussion.  Everything everyone says is recorded in order to be published later.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Daniel Gottesman and I were the two rapporteurs for quantum algorithms: Daniel spoke about quantum error-correction and fault-tolerance, and I spoke about <a href="https://www.scottaaronson.com/talks/aar-solvay.ppt">“How Much Structure Is Needed for Huge Quantum Speedups?”</a>  The link goes to my PowerPoint slides, if you’d like to check them out.  I tried to survey 30 years of history of that question, from Simon’s and Shor’s algorithms, to huge speedups in quantum query complexity (e.g., glued trees and Forrelation), to the recent quantum supremacy experiments based on BosonSampling and Random Circuit Sampling, all the way to the <a href="https://arxiv.org/abs/2204.02063">breakthrough</a> by Yamakawa and Zhandry a couple months ago.  The last slide hypothesizes a “Law of Conservation of Weirdness,” which after all these decades still remains to be undermined: “For every problem that admits an exponential quantum speedup, there must be some weirdness in its detailed statement, which the quantum algorithm exploits to focus amplitude on the rare right answers.”  My title slide also shows <a href="https://openai.com/dall-e-2/">DALL-E2</a>‘s impressionistic take on the title question, “how much structure is needed for huge quantum speedups?”:</p>



<figure class="wp-block-image size-large"><a href="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/speedups.jpg"><img alt="" class="wp-image-6474" height="418" src="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/speedups-1024x418.jpg" width="1024"/></a></figure>



<p>The discussion following my talk was largely a debate between me and Ed Farhi, reprising many debates he and I have had over the past 20 years: Farhi urged optimism about the prospect for large, practical quantum speedups via algorithms like <a href="https://arxiv.org/abs/1411.4028">QAOA</a>, pointing out his group’s past successes and explaining how they wouldn’t have been possible without an optimistic attitude.  For my part, I praised the past successes and said that optimism is well and good, but at the same time, companies, venture capitalists, and government agencies are right now pouring billions into quantum computing, in many cases—as I know from talking to them—because of a mistaken impression that QCs are <em>already known</em> to be able to revolutionize machine learning, finance, supply-chain optimization, or whatever other application domains they care about, and to do so <em>soon</em>.  They’re genuinely surprised to learn that the consensus of QC experts is in a totally different place.  And to be clear: among quantum computing theorists, I’m not at all unusually pessimistic or skeptical, just unusually willing to say in public what others say in private.</p>



<p>Afterwards, one of the string theorists said that Farhi’s arguments with me had been a highlight … and I agreed.  What’s the point of a friggin’ Solvay Conference if everyone’s just going to <em>agree</em> with each other?</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Besides quantum algorithms, there was naturally lots of animated discussion about the practical prospects for building scalable quantum computers.  While I’d hoped that this discussion might change the impressions I’d come with, it mostly confirmed them.  Yes, the problem is staggeringly hard.  Recent ideas for fault-tolerance, including the use of LDPC codes and bosonic codes, might help.  Gottesman’s talk gave me the insight that, at its core, quantum fault-tolerance is all about <em>testing</em>, <em>isolation</em>, and <em>contact-tracing</em>, just for bit-flip and phase-flip errors rather than viruses.  Alas, we don’t yet have the quantum fault-tolerance analogue of a vaccine!</p>



<p>At one point, I asked the trapped-ion experts in open session if they’d comment on the startup company <a href="https://ionq.com/">IonQ</a>, whose stock price recently fell precipitously in the wake of a scathing analyst report.  Alas, none of them took the bait.</p>



<p>On a different note, I was tremendously excited by the quantum gravity session.  Netta Engelhardt spoke about her and others’ <a href="https://www.quantamagazine.org/netta-engelhardt-has-escaped-hawkings-black-hole-paradox-20210823/">celebrated recent work</a> explaining the Page curve of an evaporating black hole using Euclidean path integrals—and by questioning her and others during coffee breaks, I finally got a handwavy intuition for how it works.  There was also lots of debate, again at coffee breaks, about Susskind’s <a href="https://www.youtube.com/watch?v=1CpzigpEJnU">recent speculations</a> on observers jumping into black holes and the quantum Extended Church-Turing Thesis.  One of my main takeaways from the conference was a dramatically better understanding of the issues involved there—but that’s a big enough topic that it will need its own post.</p>



<p>Toward the end of the quantum gravity session, the experimentalist John Martinis innocently asked what actual experiments, or at least thought experiments, had been at issue for the past several hours.  I got a laugh by explaining to him that, while the gravity experts considered this too obvious to point out, the thought experiments in question all involve forming a black hole in a known quantum pure state, with total control over all the Planck-scale degrees of freedom; then waiting outside the black hole for ~10<sup>70</sup> years; collecting every last photon of Hawking radiation that comes out and routing them all into a quantum computer; doing a quantum computation that might actually require exponential time; <em>and then</em> jumping into the black hole, whereupon you might either die immediately at the event horizon, or else learn something in your last seconds before hitting the singularity, which you could then never communicate to anyone outside the black hole.  Martinis thanked me for clarifying.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Anyway, I had a total blast.  Here I am amusing some of the world’s great physicists by letting them mess around with GPT-3.</p>



<figure class="wp-block-image size-large"><a href="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/gpt3.jpg"><img alt="" class="wp-image-6467" height="768" src="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/gpt3-1024x768.jpg" width="1024"/></a>Back: Ahmed Almheiri, Juan Maldacena, John Martinis, Aron Wall.  Front: Geoff Penington, me, Daniel Harlow.  Thanks to Michelle Simmons for the photo.</figure>



<p>I also had the following exchange at my birthday dinner:</p>



<p><strong>Physicist:</strong> So I don’t get this, Scott. Are you a physicist who studied computer science, or a computer scientist who studied physics?</p>



<p><strong>Me:</strong> I’m a computer scientist who studied computer science.</p>



<p><strong>Physicist:</strong> But then you…</p>



<p><strong>Me:</strong> Yeah, at some point I learned what a boson was, in order to invent BosonSampling.</p>



<p><strong>Physicist:</strong> And your courses in physics…</p>



<p><strong>Me:</strong> They ended at thermodynamics. I couldn’t handle PDEs.</p>



<p><strong>Physicist:</strong> What are the units of h-bar?</p>



<p><strong>Me:</strong> Uhh, well, it’s a conversion factor between energy and time. (*)</p>



<p><strong>Physicist:</strong> Good.  What’s the radius of the hydrogen atom?</p>



<p><strong>Me:</strong> Uhh … not sure … maybe something like 10<sup>-15</sup> meters?</p>



<p><strong>Physicist:</strong> OK fine, he’s not one of us.</p>



<p>(The answer, it turns out, is more like 10<sup>-10</sup> meters. I’d stupidly substituted the radius of the <em>nucleus</em>—or, y’know, a positively-charged hydrogen <em>ion</em>, i.e. proton. In my partial defense, I was massively jetlagged and at most 10% conscious.)</p>



<p>(*) Actually h-bar is a conversion factor between energy and <em>1/time</em>, i.e. frequency, but the physicist accepted this answer.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Anyway, I look forward to attending more workshops this summer, seeing more colleagues who I hadn’t seen since before COVID, and talking more science … including branching out in some new directions that I’ll blog about soon.  It does beat worrying about online trolls.</p></div>
    </content>
    <updated>2022-06-09T20:43:19Z</updated>
    <published>2022-06-09T20:43:19Z</published>
    <category scheme="https://scottaaronson.blog" term="Complexity"/>
    <category scheme="https://scottaaronson.blog" term="CS/Physics Deathmatch"/>
    <category scheme="https://scottaaronson.blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-06-12T18:41:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=2188</id>
    <link href="https://toc4fairness.org/toc4fairness-seminar-roland-maio/" rel="alternate" type="text/html"/>
    <title>TOC4Fairness Seminar – Roland Maio</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><strong>Date:</strong> Wednesday, June 15th 2022 9:00 am – 10:00 am Pacific Time <br/>

<strong>Location: </strong>Zoom meeting</div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Date: </strong>Wednesday, June 15th, 2022<br/>9:00 am – 10:00 am Pacific Time<br/>12:00 pm – 1:00 pm Eastern Time</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-2190" height="459" src="https://i0.wp.com/toc4fairness.org/wp-content/uploads/2022/06/bruce-park-roland.jpg?resize=800%2C459&amp;ssl=1" width="800"/></figure>



<p><strong>Location: </strong>Weekly Seminar, Zoom </p>



<h3 id="title-structural-racism-white-supremacy-and-the-2-edged-sword-of-data-health-inequities-and-the-embodied-truths-sharply-exposed-by-covid-19-in-context"><strong><a href="Title:&#xA0;Allocating Opportunities in a Dynamic WorldAbstract: I  will speak about the need for considering dynamic changes to the  environment in response to allocational policies. As an example, I will  introduce a sequential model for allocating opportunities, such as  higher education, in a society that exhibits bottlenecks in  socio-economic mobility. I will discuss how the problem of optimal  allocation reflects a trade-off between the benefits conferred by the  opportunities in the current generation and the potential to elevate the  socioeconomic status of recipients, shaping the composition of future  generations in ways that can benefit further from the opportunities. Our  results show how optimal allocations in this model arise as solutions  to continuous optimization problems over multiple generations, and in  general, these optimal solutions can favor recipients of low  socioeconomic status over slightly higher-performing individuals of high  socioeconomic status &#x2014; a form of socioeconomic affirmative action that  the society in our model discovers in the pursuit of purely  payoff-maximizing goals. I will conclude with directions for future  work.Bio: Hoda Heidari is an Assistant  Professor in Machine Learning and Societal Computing at the School of  Computer Science, Carnegie Mellon University. Her research is broadly  concerned with the social, ethical, and economic implications of  Artificial Intelligence. In particular, her research addresses issues of  unfairness and opaqueness through Machine Learning. Her work in this  area has won a best-paper award at the ACM Conference on Fairness,  Accountability, and Transparency (FAccT) and an exemplary track award at  the ACM Conference on Economics and Computation (EC). She has organized  several scholarly events on topics related to Responsible and  Trustworthy AI, including a tutorial at the Web Conference (WWW) and  several workshops at the Neural and Information Processing Systems  (NeurIPS) conference and the International Conference on Learning  Representations (ICLR). Dr. Heidari completed her doctoral studies in  Computer and Information Science at the University of Pennsylvania. She  holds an M.Sc. degree in Statistics from the Wharton School of  Business.&#xA0; Before joining Carnegie Mellon as a faculty member, she was a  postdoctoral scholar at the Machine Learning Institute of ETH Z&#xFC;rich,  followed by a year at the Artificial Intelligence, Policy, and Practice  (AIPP) initiative at Cornell University.">Title: </a></strong><a href="Title:&#xA0;Allocating Opportunities in a Dynamic WorldAbstract: I  will speak about the need for considering dynamic changes to the  environment in response to allocational policies. As an example, I will  introduce a sequential model for allocating opportunities, such as  higher education, in a society that exhibits bottlenecks in  socio-economic mobility. I will discuss how the problem of optimal  allocation reflects a trade-off between the benefits conferred by the  opportunities in the current generation and the potential to elevate the  socioeconomic status of recipients, shaping the composition of future  generations in ways that can benefit further from the opportunities. Our  results show how optimal allocations in this model arise as solutions  to continuous optimization problems over multiple generations, and in  general, these optimal solutions can favor recipients of low  socioeconomic status over slightly higher-performing individuals of high  socioeconomic status &#x2014; a form of socioeconomic affirmative action that  the society in our model discovers in the pursuit of purely  payoff-maximizing goals. I will conclude with directions for future  work.Bio: Hoda Heidari is an Assistant  Professor in Machine Learning and Societal Computing at the School of  Computer Science, Carnegie Mellon University. Her research is broadly  concerned with the social, ethical, and economic implications of  Artificial Intelligence. In particular, her research addresses issues of  unfairness and opaqueness through Machine Learning. Her work in this  area has won a best-paper award at the ACM Conference on Fairness,  Accountability, and Transparency (FAccT) and an exemplary track award at  the ACM Conference on Economics and Computation (EC). She has organized  several scholarly events on topics related to Responsible and  Trustworthy AI, including a tutorial at the Web Conference (WWW) and  several workshops at the Neural and Information Processing Systems  (NeurIPS) conference and the International Conference on Learning  Representations (ICLR). Dr. Heidari completed her doctoral studies in  Computer and Information Science at the University of Pennsylvania. She  holds an M.Sc. degree in Statistics from the Wharton School of  Business.&#xA0; Before joining Carnegie Mellon as a faculty member, she was a  postdoctoral scholar at the Machine Learning Institute of ETH Z&#xFC;rich,  followed by a year at the Artificial Intelligence, Policy, and Practice  (AIPP) initiative at Cornell University."> </a>Secrets, Adversaries, Incentives, and Composition in Algorithmic Fairness</h3>



<h3 id="abstract"><strong>Abstract:</strong></h3>



<p>A central goal of algorithmic fairness is to build systems with fairness properties that compose gracefully. A major effort towards this goal in fair machine learning has been the development of <em>fair representations</em> which guarantee demographic parity under sequential composition by removing group membership information from the data (i.e. by imposing a <em>demographic secrecy</em> constraint). This approach models all data consumers as utterly malicious adversaries whose sole objective is to be as unfair as possible (i.e. maximally violate demographic parity)—any other possible objective (i.e. incentive) that a data consumer may have is dismissed and ignored. In this talk, I describe joint work with Augustin Chaintreau, in which we elucidate limitations of demographically secret fair representations and propose a fresh approach to potentially overcome them by incorporating information about parties’ incentives into fairness interventions. We show that in a stylized model, it is possible to relax demographic secrecy to obtain <em>incentive-compatible representations</em>, where rational parties obtain exponentially greater utilities vis-à-vis any demographically secret representation and satisfy demographic parity. These substantial gains are recovered not from the well-known <em>cost of fairness</em>, but rather from a <em>cost of demographic secrecy</em> which we formalize and quantify for the first time. We further show that the sequential composition property of demographically secret representations is not robust to aggregation. Our results contribute to further understanding the challenges of fair composition while simultaneously suggesting that incentives may be an important and flexible tool for addressing or even overcoming those challenges.</p>



<h3 id="bio"><strong>Bio:</strong></h3>



<p>Roland Maio is a fourth year Computer Science PhD student at Columbia University advised by Augustin Chaintreau. Roland works on algorithmic fairness and CS ethics. His work has been supported by an NSF fellowship.<br/></p></div>
    </content>
    <updated>2022-06-09T16:31:57Z</updated>
    <published>2022-06-09T16:31:57Z</published>
    <category term="Events"/>
    <category term="seminar"/>
    <author>
      <name>jubaziani</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i0.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2022-06-14T16:22:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/086</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/086" rel="alternate" type="text/html"/>
    <title>TR22-086 |  Extremely Efficient Constructions of Hash Functions, with Applications to Hardness Magnification and PRFs | 

	Jiatu Li, 

	Tianqi Yang, 

	Lijie Chen</title>
    <summary>In a recent work, Fan, Li, and Yang (STOC 2022) constructed a family of almost-universal hash functions such that each function in the family is computable by $(2n + o(n))$-gate circuits of fan-in $2$ over the $B_2$ basis. Applying this family, they established the existence of pseudorandom functions computable by circuits of the same complexity, under the standard assumption that OWFs exist. However, a major disadvantage of the hash family construction by Fan, Li, and Yang (STOC 2022) is that it requires a seed length of $\text{poly}(n)$, which limits its potential applications. 
        
We address this issue by giving an improved construction of almost-universal hash functions with seed length $\text{polylog}(n)$, such that each function in the family is computable with $\text{POLYLOGTIME}$-uniform $(2n + o(n))$-gate circuits. Our new construction has the following applications in both complexity theory and cryptography.
        
* ($\textbf{Hardness magnification}$). Let $\alpha : \mathbb{N} \rightarrow \mathbb{N}$ be any function such that $\alpha(n) \leq \log n / \log \log n$. We show that if there is an $n^{\alpha(n)}$-sparse $\textbf{NP}$ language that does not have probabilistic circuits of $2n + O(n/\log\log n)$ gates, then we have (1) $\textbf{NTIME}[2^n] \not\subseteq \textbf{SIZE} \left[2^{n^{1/5}}\right]$ and (2) $\textbf{NP} \not\subseteq \textbf{SIZE}[n^k]$ for every constant $k$. Complementing this magnification phenomenon, we present an $O(n)$-sparse language in $\textbf{P}$ which requires probabilistic circuits of size at least $2n - 2$. This is the first result in hardness magnification showing that even a \emph{sub-linear additive} improvement on known circuit size lower bounds would imply $\textbf{NEXP} \not\subseteq \textbf{P}_{/\text{poly}}$. 

Following Chen, Jin, and Williams (STOC 2020), we also establish a sharp threshold for \textbf{explicit obstructions}: we give an explict obstruction against $(2n-2)$-size circuits, and prove that a sub-linear additive improvement on the circuit size would imply (1) $\textbf{DTIME}[2^n] \not\subseteq \textbf{SIZE} \left[2^{n^{1/5}}\right]$ and (2) $\textbf{P} \not\subseteq \textbf{SIZE}[n^k]$ for every constant $k$.

* ($\textbf{Extremely efficient construction of pseudorandom functions}$). Assuming that one of integer factoring, decisional Diffie-Hellman, or ring learning-with-errors is sub-exponentially hard, we show the existence of pseudorandom functions computable by $\text{POLYLOGTIME}$-uniform $\textbf{AC}^0[2]$ circuits with $2n + o(n)$ wires, with key length $\text{polylog}(n)$. We also show that PRFs computable by $\text{POLYLOGTIME}$-uniform $B_2$ circuits of $2n + o(n)$ gates follows from the existence of sub-exponentially secure one-way functions.</summary>
    <updated>2022-06-09T12:54:50Z</updated>
    <published>2022-06-09T12:54:50Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-06-14T16:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4635</id>
    <link href="https://lucatrevisan.wordpress.com/2022/06/09/workshop-in-milan-next-week/" rel="alternate" type="text/html"/>
    <title>Workshop in Milan Next Week</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">As previously announced, next week Alon Rosen and I are organizing a workshop at Bocconi, which will actually be the union of two workshops, one on Recent Advances in Cryptography and one on Spectral and Convex Optimization Techniques in Graph … <a href="https://lucatrevisan.wordpress.com/2022/06/09/workshop-in-milan-next-week/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As <a href="https://lucatrevisan.wordpress.com/2022/05/10/stoc-2022-and-other-theory-events/">previously announced</a>, next week Alon Rosen and I are organizing a workshop at Bocconi, which will actually be the union of two  workshops, one on <em>Recent Advances in Cryptography</em> and one on <em>Spectral and Convex Optimization Techniques in Graph Algorithms</em>. Here is the <a href="https://lucatrevisan.github.io/mtw.html">program</a>. In short:</p>



<ul><li>where: Bocconi University’s <a href="https://goo.gl/maps/TmBQg7g43CiKeNb18">Roentgen Building</a> (via Roentgen 1, Milano), Room AS01</li><li>when: June 15-18</li><li>what: <a href="https://lucatrevisan.github.io/mtw.html">talks</a> on cryptography and graph algorithms, including two hours devoted to Max Flow in nearly-linear time</li><li>how: <a href="https://events.unibocconi.eu/index.php?key=ev2022050043">register</a> for free</li></ul></div>
    </content>
    <updated>2022-06-09T12:26:02Z</updated>
    <published>2022-06-09T12:26:02Z</published>
    <category term="Milan"/>
    <category term="theory"/>
    <category term="cryptography"/>
    <category term="spectral graph theory"/>
    <category term="things that are excellent"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2022-06-14T16:20:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2022-06-09-phase-king-via-gradecast/</id>
    <link href="https://decentralizedthoughts.github.io/2022-06-09-phase-king-via-gradecast/" rel="alternate" type="text/html"/>
    <title>Phase-King through the lens of Gradecast: A simple unauthenticated synchronous Byzantine Agreement protocol</title>
    <summary>In this post we overview a simple unauthenticated synchronous Byzantine Agreement protocol that is based on the Phase-King protocol of Berman, Garay, and Perry 1989-92. We refer also to Jonathan Katz’s excellent write-up on this same protocol from 2013. We offer a modern approach that decomposes the Phase-King protocol into...</summary>
    <updated>2022-06-09T11:11:00Z</updated>
    <published>2022-06-09T11:11:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2022-06-13T23:03:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/085</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/085" rel="alternate" type="text/html"/>
    <title>TR22-085 |  A Note on Lower Bounds for Monotone Multilinear Boolean Circuits | 

	Andrzej Lingas</title>
    <summary>A monotone Boolean circuit is a restriction of a Boolean circuit
  allowing for the use of disjunctions, conjunctions, the Boolean
  constants, and the input variables.  A monotone Boolean circuit is
  multilinear if for any AND gate the two input functions have no
  variable in common.  We show that the known lower bounds on the size
  of monotone arithmetic circuits for multivariate polynomials that
  are sums of monomials consisting of $k$ distinct variables yield the
  analogous lower bounds divided by $O(k^2)$ on the size of monotone
  multilinear Boolean circuits computing the Boolean functions
  represented by the corresponding multivariate Boolean polynomials.</summary>
    <updated>2022-06-09T09:41:53Z</updated>
    <published>2022-06-09T09:41:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-06-14T16:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1679</id>
    <link href="https://ptreview.sublinear.info/2022/06/news-for-may-2022/" rel="alternate" type="text/html"/>
    <title>News for May 2022</title>
    <summary>The crazy numbers from last month are not quite gone: we have five papers this month, not bad at all! Codes! Distributed computing! Probability distributions! Improved local testing for multiplicity codes, by Dan Karliner and Amnon Ta-Shma (ECCC). Take the Reed–Muller code with parameters \(m, d\), whose codewords are the evaluation tables of all degree-\(m\) […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The crazy numbers from last month are not quite gone: we have five papers this month, not bad at all! </p>



<p>Codes! Distributed computing! Probability distributions!</p>



<p><strong>Improved local testing for multiplicity codes</strong>, by Dan Karliner and Amnon Ta-Shma (<a href="https://arxiv.org/abs/2204.14137">ECCC</a>). Take the Reed–Muller code with parameters \(m, d\), whose codewords are the evaluation tables of all degree-\(m\) polynomials over \(\mathbb{F}^d\). RM codes are great, they are everywhere, and they are<em> locally testable</em>: one can test whether a given input \(x\) is a valid codeword (or far from every codeword) with only very few queries to \(x\). Now, take the <em>multiplicity code</em>: instead of just the evaluation table of the polynomial themselves, a codeword includes the evaluations of all its derivatives, up to order \(s\). These beasts generalize RM codes: are they <em>also</em> locally testable? Yes they are! And this work improves on our understanding of this aspect, by providing better bounds on the locality (how few queries are necessary to test), and simplifies the argument from previous work by Karliner, Salama, and Ta-Shma (2022).</p>



<p><strong>Overcoming Congestion in Distributed Coloring,</strong> by Magnús M. Halldórsson, Alexandre Nolin, Tigran Tonoyan (<a href="https://arxiv.org/abs/2205.14478">arXiv</a>). Two of the main distributed computing models, LOCAL and CONGEST, differ in how they model the bandwidth constraints. In the former, nodes can send messages of arbitrary size, and the limiting quantity is the number of rounds of communications; while in the latter, each node can only send a logarithmic number of bits at each round. This paper introduces a new technique that allows for communication-efficient distributed (coordinated) sampling, which as a direct applications enables porting several LOCAL algorithms to the CONGEST model at a small cost: for instance, \((\Delta+1)\)-List Coloring. This new technique also has applications beyond these distributed models, to graph property testing – in a slightly non-standard setting where we define farness from the property in a “local” sense (detect vertices or edges which contribute to many violations, i.e., are “locally far” from the property considered).</p>



<p><strong>Robust Testing in High-Dimensional Sparse Models,</strong> by Anand Jerry George and Clément L. Canonne (<a href="https://arxiv.org/abs/2205.07488">arXiv</a>). In the Gaussian mean testing problem, you are given samples from a high-dimensional Gaussian \(N(\mu, I_d)\), where \(\mu\) is either zero or has \(\ell_2\) norm greater than \(\varepsilon\), and you want to decide which of the two holds. This “mean testing” equivalent (due to, erm, “standard facts”) to testing in total variation distance, and captures the setting where one wantss to figure out whether an underlying signal \(\mu\), subject to white noise, is null or significant. Now, what if this $\mu$ was promised to be \(s\)-sparse? Can we test more efficiently? But what if a small fraction of the samples were arbitrarily corrupted — how much harder does the testing task become? For some related tasks, it is known that being robust against adversarial corruptions makes testing as hard as learning… This paper addresses this “robust sparse mean testing” question, providing matching upper and lower bounds; as well as the related question of (robust, sparse) linear regression.</p>



<p><strong>Sequential algorithms for testing identity and closeness of distributions,</strong> by Omar Fawzi, Nicolas Flammarion, Aurélien Garivier, and Aadil Oufkir (<a href="https://arxiv.org/abs/2205.06069">arXiv</a>). Consider the two “usual suspects” of distribution testing, <em>identity</em> and <em>closeness</em> testing, where we must test if an unknown distribution is equal to some reference one or \(\varepsilon\)-far (in total variation distance) from it; or, the same thing, but with two unknown distributions (no reference one). These are, by now, quite well understood… but the algorithms for them take a worst-case number of samples, function of the distance parameter \(\varepsilon\). But if the two distributions are much further apart than \(\varepsilon\), fewer samples should be required! This is the focus of this paper, showing that with a sequential test one can achieve this type of guarantees: a number of samples which, in the “far” case, depends on the actual distance, not on its worst-case lower bound \(\varepsilon\). One could achieve this by combining known algorithms with a “doubling search;” however, this still would lose some constant factors in the sample complexity. The authors provide sequential tests which improve on this “doubling search technique” by constant factors, and back this up with empirical evaluations of their algorithms.</p>



<p><strong>Estimation of Entropy in Constant Space with Improved Sample Complexity,</strong> by Maryam Aliakbarpour, Andrew McGregor, Jelani Nelson, and Erik Waingarten (<a href="https://arxiv.org/abs/2205.09804">arXiv</a>). Suppose that, given samples from an unknown distribution \(p\) over \(n\) elements, your task is to estimate its (Shannon) entropy \(H(p)\) up to \(\pm\Delta\). You’re in luck! We know that \(\Theta(n/(\Delta\log n)+ (\log^2 n)/\Delta^2)\) samples are necessary and efficient. <em>But what if you had to do that under strict memory constraints? </em>Say, using only a <em>constant</em> number of words of memory? Previous work by Acharya, Bhadane, Indyk, and Sun (2019) shows that it is still possible, but the number of samples required shoots up, with their algorithm now requiring (up to polylog factors) \(n/\Delta^3\) samples. This works improves upon the dependence on \(\Delta\), providing a constant-memory algorithm with sample complexity \(O(n/\Delta^2 \cdot \log^4(1/\Delta))\); they further conjecture this to be optimal, up to the polylog factors.</p></div>
    </content>
    <updated>2022-06-07T17:20:22Z</updated>
    <published>2022-06-07T17:20:22Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2022-06-13T23:03:27Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2022-06-07-approx-agreement-one/</id>
    <link href="https://decentralizedthoughts.github.io/2022-06-07-approx-agreement-one/" rel="alternate" type="text/html"/>
    <title>Approximate Agreement: definitions and the robust midpoint protocol</title>
    <summary>This post covers the basics of Approximate Agreement. We define the problem, explain in what way its an interesting variation of classic Agreement, and describe the idea behind the robust midpoint protocol. In classic consensus, the space of possible decision values is just a set and the goal is to...</summary>
    <updated>2022-06-07T07:11:00Z</updated>
    <published>2022-06-07T07:11:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2022-06-13T23:03:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=20150</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/06/06/laws-and-laughs/" rel="alternate" type="text/html"/>
    <title>Laws and Laughs</title>
    <summary>Rules are a great way to get ideas. All you have to do is break them—Jack Foster Roy Amara was a researcher and president of the Institute for the Future. Among things he is known for is coining Amara’s law on the effect of technology. Today Ken and I want to discuss “laws”. We hope […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><i>Rules are a great way to get ideas. All you have to do is break them—Jack Foster</i></p>
<p>
Roy Amara was a researcher and president of the Institute for the Future.  Among things he is known for is coining <a href="https://en.wikipedia.org/wiki/Roy_Amara">Amara’s law</a> on the effect of technology. </p>
<p>
Today Ken and I want to discuss “laws”. We hope you will like the smile that many of these give us. Perhaps they will give you some too. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/06/06/laws-and-laughs/am/" rel="attachment wp-att-20153"><img alt="" class="aligncenter size-full wp-image-20153" height="257" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/am.png?resize=196%2C257&amp;ssl=1" width="196"/></a></p>
<p>
<span id="more-20150"/></p>
<p><b> Amara’s Law and More </b></p>
<p/><p>
Amara’s law is: <br/>
 <i>We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.</i> </p>
<p>
Tom Cargill’s law is: <br/>
 <i>The first 90 percent of the code accounts for the first 90 percent of the development time. The remaining 10 percent of the code accounts for the other 90 percent of the development time.</i></p>
<p>
Arthur Clarke’s law is: <br/>
 <i>When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.</i></p>
<p>
Fred Brooks’s law is: <br/>
 <i>Adding manpower to a late software project makes it later.</i></p>
<p>
Brooks explains it:  First, it takes the new guy some time to learn about the project before becoming productive. Teaching him takes resources that could otherwise be put into the project itself. Second, communication overheads increase as the number of people increases – sometimes, they spend more time talking to each other to keep the project in sync, rather than working on the project itself. </p>
<p>
Here is a list of many additional interesting <a href="https://www.pcmag.com/encyclopedia/term/laws">laws</a> from <i>PC Magazine</i>. Some of our favorites are:</p>
<ul>
<li>
The best way to get the right answer on the Internet is not to ask a question; it’s to post the wrong answer. <p/>
</li><li>
If you can think of four ways that something can go wrong, it will go wrong in the fifth way. <p/>
</li><li>
Everything takes longer than you think it will. <p/>
</li><li>
Nature always sides with the hidden flaw. <p/>
</li><li>
The light at the end of the tunnel is only the light of an oncoming train.
</li></ul>
<p>
</p><p><b> Open Problems </b></p>
<p/><p>
The third of the last five laws from <i>PC Magazine</i> was given a fuller <a href="https://en.wikipedia.org/wiki/Hofstadter%27s_law">statement</a> by Douglas Hofstadter: </p>
<blockquote><p>
<b>Hofstadter’s Law</b>: It always takes longer than you expect, even when you take into account <b>Hofstadter’s Law</b>.
</p></blockquote>
<p>
Ken notes a similarity to his “law of prediction” at the end of the “Paradox” section of his <a href="https://rjlipton.wpcomstaging.com/2018/08/25/do-you-want-to-know-a-secret/">post</a> about Mark Glickman: <i>All simple and elegant prediction models are overconfident.</i></p>
<p>
What are some of your own favorite laws?</p>
<p/></div>
    </content>
    <updated>2022-06-06T23:05:06Z</updated>
    <published>2022-06-06T23:05:06Z</published>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Results"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-06-14T16:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1111</id>
    <link href="https://corner.mimuw.edu.pl/?p=1111" rel="alternate" type="text/html"/>
    <title>WOLA 2022: Call for contributed talks &amp; please register</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is a reminder that the Workshop on Local Algorithms (WOLA) istaking place in Warsaw this year from June 25 to 27. It is convenientlyscheduled right after STOC in Rome. Keynote speakers include BernhardHaeupler, Rotem Oshman, Michael Kapralov, and Vincent … <a href="https://corner.mimuw.edu.pl/?p=1111">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>

This is a reminder that the Workshop on Local Algorithms (WOLA) is<br/>taking place in Warsaw this year from June 25 to 27. It is conveniently<br/>scheduled right after STOC in Rome. Keynote speakers include Bernhard<br/>Haeupler, Rotem Oshman, Michael Kapralov, and Vincent Cohen-Addad. More<br/>information can be found on the official webpage:<br/><a href="https://ideas-ncbr.pl/en/wola/" rel="noreferrer noopener" target="_blank">https://ideas-ncbr.pl/en/wola/</a></p>



<p># Contributed talks and registration</p>



<p>Apart from keynote talks, WOLA is going back to the pre-pandemic<br/>tradition of contributed talks by the workshop participants. **If you<br/>are planning to attend in person and want to give a talk**, please<br/>register as soon as possible at<br/><a href="https://ideas-ncbr.pl/en/wola/registration/" rel="noreferrer noopener" target="_blank">https://ideas-ncbr.pl/en/wola/registration/</a>. Even if you are not<br/>planning to give a talk, registering helps the local organizers with<br/>estimating the number of attendees, which is useful for events such as...</p>



<p># A night tour of Warsaw on a retro bus</p>



<p>See the attached picture for what such a bus looks like! I'm told by the<br/>local organizers that this will include a tasting of Polish specialties.<br/>This event is scheduled for Saturday night.</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-1112" src="https://corner.mimuw.edu.pl/wp-content/uploads/2022/06/retro_bus.jpg"/></figure>



<p># Traveling from STOC in Rome</p>



<p>The workshop is starting at 1:30pm on Saturday to allow for more options<br/>for traveling from STOC. There are direct flights with Wizz Air on both<br/>Friday night and Saturday morning. You can also fly with KLM (via AMS)<br/>or Air France (via CDG) on Saturday morning if you prefer a more<br/>traditional air carrier.

</p></div>
    </content>
    <updated>2022-06-06T19:05:06Z</updated>
    <published>2022-06-06T19:05:06Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>https://corner.mimuw.edu.pl</id>
      <link href="https://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2022-06-13T23:02:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2022/06/06/PGDL/</id>
    <link href="http://offconvex.github.io/2022/06/06/PGDL/" rel="alternate" type="text/html"/>
    <title>Predicting Generalization using GANs</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A central problem of generalization theory is the following: <em>Given a training dataset and a deep net trained with that dataset, give a mathematical estimate of the test error.</em></p>

<p>While this may seem useless to a practitioner (“why not just retain some holdout data for testing?”), this is of great interest to theorists trying to understand what properties of the net or the training algorithm lead to good generalization. Old posts on this blog such as <a href="http://www.offconvex.org/2018/02/17/generalization2/">this one</a> mentioned the difficulties in applying classic generalization theory to get estimates of generalization error even within a few orders of magnitude.</p>

<p>This blog post is about the topic of a 
 NeurIPS 20 competition <a href="https://sites.google.com/view/pgdl2020">Predicting Generalization in Deep Learning competition</a> which suggested using machine learning techniques to  understand network properties that promote generalization! Contestants are given datasets and a set of deep nets trained on these datasets using unknown techniques. The goal is to rank the trained nets according to generalization error. Contestants provide a python code that, given a training set and a trained net, outputs a scalar estimate of the generalization error. The contestant’s performance is computed using a ranking measure of how well their scalar score correlates with actual generalization. 
 Several interesting ideas emerged from the competition, such as measuring networks’ resilience to distortions to the input; collecting specific statistics of hidden node activations; and measuring output consistency of identically structured networks but trained by SGD with different random seeds.</p>

<p>This blog post describes our <a href="https://arxiv.org/abs/2111.14212">ICLR22 spotlight paper</a>, coauthored with Nikunj Saunshi and Arushi Gupta, that gives a surprisingly easy method to predict generalization using <a href="http://www.offconvex.org/2017/03/15/GANs/">Generative Adversarial Nets</a> or GANs.</p>

<h2 id="predicting-generalization-using-gans">Predicting Generalization using GANs</h2>
<p>Given a training dataset and a collection of nets trained using it, our method is very simple:</p>

<blockquote>
  <p><strong>Step 1)</strong> Train a Generative Adversarial Network (GAN) on the same training dataset a</p>

  <p><strong>Step 2)</strong> Draw samples from the GAN generator to obtain a synthetic dataset.</p>

  <p><strong>Step 3)</strong> For each classifier in the task, use its classification error on the synthetic data as our prediction for its true test error</p>
</blockquote>

<p>We find for a variety of image datasets that using pre-trained Studio-GANs directly downloaded from <a href="https://github.com/POSTECH-CVLab/PyTorch-StudioGAN">public repositories</a>, we obtain scores that would ‘‘not only outperforms other methods but blows them out of the water’’ (to cite one of the reviews for our paper), including the winning methods proposed at the PGDL competition (see Table 1 in the paper for quantitative results). Here we show plots of true test errors v.s. errors on the synthetic data, where we generate one synthetic dataset from a GAN trained on each dataset, and each dot in the figures represents a trained deep net classifier.</p>

<figure align="center">
<img alt="drawing" src="http://www.offconvex.org/assets/PGDL_linear_fit.jpg" width="80%"/>
</figure>

<p>Not only is the correlation linear; it appears remarkably close to the $y=x$ fit! Also the trained deep net classifiers here belong to drastically different architecture families —including VGG, ResNet, DenseNet, ShuffleNet, PNASNet, and MobileNet— which don’t correspond to the discriminator of Studio-GAN.</p>

<p>Frankly, this success at predicting generalization confounded the authors, some of whom had earlier shown that GANs <a href="http://www.offconvex.org/2017/07/06/GANs3/">do not learn the distribution well, and suffer from serious mode collapse</a>).</p>

<p>Thus we conclude following three statements all appear to be true:</p>

<blockquote>
  <p><strong>Observation 1)</strong> GAN samples suffer from mode collapse (as detected by the <a href="http://www.offconvex.org/2017/07/06/GANs3/">birthday paradox test</a> and do not appear to be as diverse as the distribution of images the GANs was trained on.</p>
</blockquote>

<blockquote>
  <p><strong>Observation 2)</strong> Training deep net classifiers using only GAN samples leads to poor performance. This is another evidence that GAN samples are poor substitutes for the real thing.</p>
</blockquote>

<blockquote>
  <p><strong>Observation 3)</strong> Yet GAN samples are good enough to substitute for holdout data to give a reasonable prediction of test performance.</p>
</blockquote>

<p>Note however that there is no inherent contradiction here. For instance, suppose the GAN’s distribution has limited diversity: it only knows how to generate a $10,000$  random images, as well as $1000$ minor variations of each of these images. So long as the  $10,000$ distinct images are like random draws from the full distribution, it could  predict generalization for a trained net reasonably well, even though a million image samples from the GAN would not suffice to replace ImageNet’s 1 million images for training a deep net from scratch. (In fact this scenario was predicted and to some extent verified by <a href="https://proceedings.mlr.press/v70/arora17a.html">our earlier work on why GANs may not be able to avoid mode collapse</a>.)</p>

<p>We hope our work motivates further investigations of the power of real-life GANs and what can be done using samples from their distributions.</p>

<p>(Aside: Of course, the field has now progressed beyond simple GANs to multimodal generators like <a href="https://openai.com/blog/dall-e/">DALL-E</a>, and it would be interesting to understand how the generated images there can be leveraged to predict generalization.)</p></div>
    </summary>
    <updated>2022-06-06T09:00:00Z</updated>
    <published>2022-06-06T09:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2022-06-13T23:03:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/06/04/maybe-powers-pi</id>
    <link href="https://11011110.github.io/blog/2022/06/04/maybe-powers-pi.html" rel="alternate" type="text/html"/>
    <title>Maybe powers of π don’t have unexpectedly good approximations?</title>
    <summary>After I wrote recently about Ramanujan’s approximation \(\pi^4\approx 2143/22\), writing “why do powers of \(\pi\) seem to have unusually good rational approximations?”, Timothy Chow emailed to challenge my assumption, asking what evidence I had that their approximations were unusually good. So that led me to do a little statistical experiment to test that hypothesis, and the experiment showed…that the approximations seem to be about as good as we would expect, no more, no less. Not unusually good. Chow was correct, and my earlier statement was overstated. So if Ramanujan’s approximation is not just random fluctuation (which for all I know it could be), it at least does not seem to be part of a pattern of many good rational approximations for small powers of \(\pi\). Below are some details of how I came to this conclusion.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>After <a href="https://11011110.github.io/blog/2022/05/31/linkage.html">I wrote recently</a> about Ramanujan’s approximation \(\pi^4\approx 2143/22\), writing “why do powers of \(\pi\) seem to have unusually good rational approximations?”, 
Timothy Chow emailed to challenge my assumption, asking what evidence I had that their approximations were unusually good. So that led me to do a little statistical experiment to test that hypothesis, and the experiment showed…that the approximations seem to be about as good as we would expect, no more, no less. Not unusually good. Chow was correct, and my earlier statement was overstated. So if Ramanujan’s approximation is not just random fluctuation (which for all I know it could be), it at least does not seem to be part of a pattern of many good rational approximations for small powers of \(\pi\). Below are some details of how I came to this conclusion.</p>

<p>First off, one gets good rational approximations by truncating the <a href="https://en.wikipedia.org/wiki/Continued_fraction">continued fraction representation</a> of a number. for instance, Ramanujan’s approximation can be obtained from the continued fraction</p>

\[\pi^4=97 + \cfrac{1}{2 + \cfrac{1}{2 + \cfrac{1}{3 + \cfrac{1}{1+\cfrac{1}{16539+\cdots}}}}}\]

<p>by stopping just before the big number \(16539\). You want to stop at those points, roughly speaking, because the accuracy of your approximation (as measured for instance by how many decimal digits you get correct) comes from combining all of the terms up to and including the big one you stopped before, but the complexity of your approximation (how many digits it takes to write it down) comes from only the terms before you stopped. When you stop just before a big term, you get a lot of accuracy for free. And so the question I was really asking was “why do the powers of \(\pi\) have unusually big terms in their continued fractions?” But the question I should have asked first was “do the powers of \(\pi\) have unusually big terms in their continued fractions?”</p>

<p>For a random number, the terms of the continued fraction will be distributed according to the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Kuzmin_distribution">Gauss–Kuzmin distribution</a>, according to which the probability of seeing a term with value exactly \(k\) is</p>

\[- \log_2 \left( 1 - \frac{1}{(1+k)^2}\right).\]

<p>This is a <a href="https://en.wikipedia.org/wiki/Heavy-tailed_distribution">heavy-tailed distribution</a>, so one should expect to see some large numbers from time to time. What I wanted to test was a <a href="https://en.wikipedia.org/wiki/Null_hypothesis">null hypothesis</a> that the continued fraction for powers of \(\pi\) are distributed in this way. If the tests revealed a low likelihood of this being true, it would suggest that they have some other distribution, confirming what I wrote in my earlier post. If they didn’t, it still might mean that there was something unusual about those terms, but it would have to be more subtle, subtle enough to be undetected by the statistical tests I used.</p>

<p>This is exactly the sort of thing <a href="https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test">Pearson’s \(\chi^2\) test</a> is good for. So I set up a \(\chi^2\) test in R comparing the observed data (the frequencies of each value in the continued fraction terms for \(\pi\), \(\pi^2\), \(\pi^3\), and \(\pi^4\), as taken from OEIS) with the Gauss–Kuzmin distribution. I omitted the initial term from each continued fraction, because those are the integer parts of the powers of \(\pi\), which are both nonrandom and uninteresting for approximation purposes. I also grouped the terms of the continued fractions into six exponentially-growing buckets, by mapping each term \(t_i\) to \(\min\bigl(5,\lfloor\log_2 t_i\rfloor\bigr)\), because the \(\chi^2\) test works better when it has only a small number of well-populated frequency counts to compare, rather than \(16539\) of them, many empty or with only one representative. This meant also computing the Gauss–Kuzmin probabilities for the same buckets. These buckets still don’t have uniform probabilities but I thought it better to stick to my first choice of bucketing rather than to repeatedly adjust the parameters of the test. Even with this bucketing the \(\chi^2\) test still wasn’t strong enough to work well on the OEIS data taken separately for each individual power of \(\pi\). Instead I had to concatenate the data for the four powers I had chosen into a single test to get it to work without R complaining.</p>

<p>Because I am more fluent in Python than R, I did this as <a href="https://11011110.github.io/blog/assets/2022/pipower.py">a Python script</a> to generate a very short R script, but it would have been as easy to do the whole thing directly in R. The results: a <a href="https://en.wikipedia.org/wiki/P-value">\(p\)-value</a> somewhat greater than \(\tfrac12\) (not small). So the null hypothesis was <em>not</em> rejected and I do not have evidence that the powers of \(\pi\) have unusually big terms in their continued fractions.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/108423157545534693">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2022-06-04T21:03:00Z</updated>
    <published>2022-06-04T21:03:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-06-14T16:11:36Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-18307925481231353</id>
    <link href="http://blog.computationalcomplexity.org/feeds/18307925481231353/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/06/does-social-media-law-in-texas-affect.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/18307925481231353" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/18307925481231353" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/06/does-social-media-law-in-texas-affect.html" rel="alternate" type="text/html"/>
    <title>Does the Social Media Law in Texas affect theory bloggers?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A new law in Texas states that any social media sites that has at least 50 million subscribers a month cannot ban anyone (its more nuanced than that, but that's the drift). </p><p>(I wrote this before the Supreme courts blocked the law, which you can read about <a href="https://nypost.com/2022/05/31/scotus-blocks-texas-law-targeting-social-media-censorship/">here</a>. This is a temporary block so the issue is not settled.) </p><p>Here is an article about the law: <a href="https://www.vox.com/2022/5/12/23068017/supreme-court-first-amendment-twitter-facebook-youtube-instagram-netchoice-paxton-texas">here</a></p><p>My random thoughts</p><p>1) How can any internet law be local to Texas or to any state? I wonder the same thing about the EU's law about right-to-be-forgotten and other restrictions. </p><p>2) Does the law apply to blogs? If Scott had over 50 million readers... <i>Hold that thought. </i>Imagine if that many people cared about quantum computing, complexity theory,  the Busy Beaver function,  and Scott's political and social views. T<i>hat would be an awesome world!</i> However, back to the point- if he did have that many readers would he not be allowed to ban anyone?</p><p>3) If Lance and I had over 50 million readers... <i>Hold that thought</i>. Imagine if that many people cared about Complexity Theory, Ramsey Theory, <a href="https://blog.computationalcomplexity.org/2022/01/did-betty-white-die-in-2021why-do.html">Betty White</a> and Bill and Lance's political and social views. Would <i>that be an awesome world?</i> I leave that as an open question. However, back to the point- would they be able to block posts like: </p><p>                      Great Post. Good point about SAT. Click here for a good deal on tuxedos. </p><p>Either the poster thinks that Lance will win a Turing award and wants him to look good for the ceremony, or its a bot. </p><p>4) If Lipton and Regan's GLL blog had over 50 million readers.... <i>Hold that thought</i>. Imagine if that many people cared about Complexity theory, open-mindedness towards P=NP, catching people who cheat at chess, nice things about everyone they mention, and their political and social views. <i>That would</i> <i>be a very polite world! </i>However, back to the point- would they be able to block posts? Block people? </p><p>5) arxiv recently rejected a paper by Doron Zeilberger. This rejection was idiotic, though Doron can argue the case better than I can, so see <a href="https://sites.math.rutgers.edu/~zeilberg/Opinion183.html">here</a> for his version of events (one sign  that he can argue better than I can: he does not use any negative terms like <i>idiot.)  </i>Under the moronic Texas law, can arxiv ban Doron for life? (of course, the question arises, do they have at least 50 million subscribers?)</p><p>6) Given who is proposing the law its intent is things like <i>you can't kick Donald Trump off Twitter</i>. I  wonder if Parler or 8-chan or Truth-Social which claim to be free-speech sites, but whose origins are on the right, would block  liberals. Or block anyone? I DO NOT KNOW if they do, but I am curious. If anyone knows please post- no speculation or rumor, I only want solid information. </p><p>7) Republicans default position is to not regulate industry. It is <i>not</i> necessarily  a contradiction to support a regulation; however, they would need  a strong argument why this particular case needs regulation when other issues do not. I have not seen such an argument; however, if you have one then leave a comment. (The argument <i>they are doing</i> <i>it  to please their base </i>is not what I mean- I want a fair objective argument.) </p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2022-06-04T17:03:00Z</updated>
    <published>2022-06-04T17:03:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-06-14T16:19:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-25562705.post-4217158232171967686</id>
    <link href="http://aaronsadventures.blogspot.com/feeds/4217158232171967686/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=4217158232171967686" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/4217158232171967686" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/4217158232171967686" rel="self" type="application/atom+xml"/>
    <link href="http://aaronsadventures.blogspot.com/2022/06/practical-robust-and-equitable.html" rel="alternate" type="text/html"/>
    <title>Practical, Robust, and Equitable Uncertainty Estimation</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: center;"><span lang="EN"><i>This is a post about a new paper that is joint work with Bastani, Gupta, Jung, Noarov, and Ramalingam. The paper is here: </i></span><span style="text-align: left;"><i><a href="https://arxiv.org/abs/2206.01067">https://arxiv.org/abs/2206.01067</a></i></span><span lang="EN"><i> and here is a recording of a recent talk I gave about it at the Simons Foundation:<a href="https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/"> </a></i></span><span style="text-align: left;"><i><a href="https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/">https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/</a><a href="http://aaronsadventures.blogspot.com/feeds/posts/This is a post about a new paper that is joint work with Bastani, Gupta, Jung, Noarov, and Ramalingam. The paper is here:   and here is a recording of me giving a talk about it: https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/"> </a>. This is cross-posted to the <a href="https://toc4fairness.org/practical-robust-and-equitable-uncertainty-estimation/">TOC4Fairness Blog</a> (and this work comes out of the Simons Collaboration on the Theory of Algorithmic Fairness)</i></span></p><p><br/></p><p>Machine Learning is really good at making point predictions --- but it sometimes makes mistakes. How should we think about which predictions we should trust? In other words, what is the right way to think about the uncertainty of particular predictions? Together with Osbert Bastani, Varun Gupta, Chris Jung, Georgy Noarov, and Ramya Ramalingam, we have some new work I'm really excited about. </p> <p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN">A natural way to quantify uncertainty is to predict a set of labels rather than a single one. Pick a degree of certainty --- say 90%. For every prediction we make, we'd like to return the smallest set of labels that is guaranteed to contain the true label 90% of the time. These are "prediction sets", and quantify uncertainty in a natural way: ideally, we will be sure about the correct label, and the prediction set will contain only a single label (the prediction we are certain about). But the larger our prediction set, the more our uncertainty, and the contents of the prediction set lets us know what exactly the model is uncertain about. </span></p><p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN"/></p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiM6gcqACKJ0qGQ_We9SiWjb_Mqiyh9rgb8SllcXpgzFFf0O5wJqHKogVUlEaX-Cssihkl9uzHLnsbiBzJIAze1AQXjdZpf1PUTK6GpGIyW-j_qoUSyDnx8SpRo9i2xBJJ3ynVdXuA5vkr70nxcFNcYKdJrIhOOI_4-imNOmif4QH_v4jJI0Q" style="margin-left: auto; margin-right: auto;"><img alt="" height="103" src="https://blogger.googleusercontent.com/img/a/AVvXsEiM6gcqACKJ0qGQ_We9SiWjb_Mqiyh9rgb8SllcXpgzFFf0O5wJqHKogVUlEaX-Cssihkl9uzHLnsbiBzJIAze1AQXjdZpf1PUTK6GpGIyW-j_qoUSyDnx8SpRo9i2xBJJ3ynVdXuA5vkr70nxcFNcYKdJrIhOOI_4-imNOmif4QH_v4jJI0Q" width="320"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">An example of prediction sets for ImageNet. This example comes from a nice recent paper by Angelopoulos, Bates, Malik, and Jordan: <a href="https://arxiv.org/abs/2009.14193">https://arxiv.org/abs/2009.14193</a> </td></tr></tbody></table><span lang="EN"><br/></span><p/> <p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN">But how can we do this? Conformal Prediction provides a particularly simple way. Here is an outline of the vanilla version of conformal prediction (there are plenty of variants): </span></p><p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;">Step 1: Pick a (non)conformity score to measure how different a label y is from a prediction f(x). e.g. for a regression model we could choose $s(x,y) = |f(x)-y|$ --- but lots of interesting work has been done recently to develop much fancier ones. A lot of the art of conformal prediction is in finding a good score function.</p> <p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN">Step 2: Find a threshold $\tau$ such that for a new example $(x,y)$, $\Pr[s(x,y) \leq \tau] = 0.9$. An easy way to do this is using a holdout set. </span></p><p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;">Step 3: On a new example $x$, given a point prediction $f(x)$, produce the prediction set $P(x) = \{y : s(x,y) \leq \tau\}$. </p><p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;">Thats it! Nice and simple. Check out <a href="https://arxiv.org/abs/2107.07511">this recent survey by Angelopolous and Bates</a> for an accessible introduction to conformal prediction. </p> <p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN">But a few things could go wrong. First, the technique of using a holdout set only works if the data is i.i.d. or more generally exchangable --- i.e. the data distribution should be permutation invariant. But maybe its coming from some changing distribution. If the distribution has changed in an expected and well behaved way, there are some fixes that let you apply the same framework, but if not you are likely in trouble. </span></p><p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN"/></p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEje9UBcYGyvVOkiIH_g3O2jlKIfGd5TVuVYOY_81duPMVJRrmTcZycrHumnMMpIp9O3EI4Ubf0dhYBb5dttrZEVWjD92C8qv1Xwmjdn1Y-5bfbzMBqcuTeBq_0ZZ78ymLgWp9kFRhI3f8eehSPv9TmyncWsFEqDHbMLF2JGhl6Y5Giqz5bIIA" style="margin-left: auto; margin-right: auto;"><img alt="" height="228" src="https://blogger.googleusercontent.com/img/a/AVvXsEje9UBcYGyvVOkiIH_g3O2jlKIfGd5TVuVYOY_81duPMVJRrmTcZycrHumnMMpIp9O3EI4Ubf0dhYBb5dttrZEVWjD92C8qv1Xwmjdn1Y-5bfbzMBqcuTeBq_0ZZ78ymLgWp9kFRhI3f8eehSPv9TmyncWsFEqDHbMLF2JGhl6Y5Giqz5bIIA" width="320"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">A joke about non-exchangable data</td></tr></tbody></table><span lang="EN"><br/></span><p/> <p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN">Second, an average over everyone might not be what you care about. If we are in a personalized medicine setting, you might care about the reliability of predictions not just overall, but for women with a family history of diabetes and egg allergies --- or whatever else you think is medically relevant about you as an individual.</span></p> <p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN">This is the problem that we want to solve: How to give prediction sets that cover their label 90% of the time even if we make no assumptions at all about the data generating process, and even if we care about coverage conditional on arbitrary intersecting subsets of the data. </span></p> <p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN">We want stronger guarantees in another way too. If you think about our goal, there is a way to cheat: 90% of the time, predict the (trivial) set of all labels. 10% of the time predict the empty set. This covers the real label 90% of the time, but is completely uninformative. </span></p> <p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN">To avoid this "solution", we also ask that our predictions be threshold calibrated. Remember our prediction sets have the form $P_t(x) = \{y : s(x,y) \leq \tau_t\}$. Now the threshold $\tau_t$ might be different every day. But we want 90% coverage even conditional on the value of $\tau_t$. </span></p> <p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN">This rules out cheating. Remarkably (I think!), for every set of groups specified ahead of time, we're able to guarantee that even if the data is generated by an adversary, that our empirical coverage converges to 90% at the statistically optimal rate. Here is what that means:</span></p> <p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><span lang="EN">Pick a threshold $\tau$ and group $G$. Consider all $n_{\tau,G}$ rounds in which the example $x$ was in $G$, and in which we predicted threshold $\tau$.<span>  </span>We promise that on this set, we cover 90% $\pm$ $1/\sqrt{n_{\tau,G}}$ of the labels. This is the best you could do even with a known distribution. </span></p><p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;">The best thing is that the algorithm is super simple and practical. We had<a href="https://arxiv.org/abs/2101.01739"> a paper last year</a> that showed how to do much of this in theory --- but the algorithm from that paper was not easily implementable (it involved solving an exponentially large linear program with a separation oracle).  But here is our new algorithm --- it only involves doing a small amount of arithmetic for each prediction:</p><p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"/><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh9AnXTAw_h8Z1DI2Ui3Usp0wYWLVHC7u0i8WR90J8WnysneaxXKUZvU9MQL30elKou4H1bZSoajYpYivZz6ztv3MwiDiOHImi6TXIJmaxWa0W-HRmDvbfFaQfOtpK-JISnbszduJz1YuwX_8IhUJweINzwirvtn1zFhNzCCaTRZHrEcAcQ-Q" style="margin-left: 1em; margin-right: 1em;"><img alt="" height="240" src="https://blogger.googleusercontent.com/img/a/AVvXsEh9AnXTAw_h8Z1DI2Ui3Usp0wYWLVHC7u0i8WR90J8WnysneaxXKUZvU9MQL30elKou4H1bZSoajYpYivZz6ztv3MwiDiOHImi6TXIJmaxWa0W-HRmDvbfFaQfOtpK-JISnbszduJz1YuwX_8IhUJweINzwirvtn1zFhNzCCaTRZHrEcAcQ-Q" width="301"/></a></div>So we're able to implement it and run a bunch of experiments. You can read about them in detail in <a href="https://arxiv.org/abs/2206.01067">the paper</a>, but the upshot is that our new method is competitive with split conformal prediction even on "its own turf" --- i.e. when the data really is drawn i.i.d. and we only care about marginal coverage --- and really excels when the data comes from a more complicated source, or when we measure group-conditional coverage, which traditional methods tend to have much more trouble with. We run experiments on regression and classification tasks, on exchangeable data, under distribution shift, on real time series data, and on adversarial data orderings. Even when the data is i.i.d. and we only care about marginal coverage, our method has an important advantage over split conformal prediction --- since we don't need to preserve exchangability, we can use all of the data to train the underlying model, whereas split conformal prediction needs to reserve some fraction of it for a holdout set. The result is faster learning for our method, which results in smaller/more accurate prediction sets even without the complicating factors of groupwise coverage, threshold calibration or adversarial data!<p/><p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"><br/></p><p class="MsoNormal" style="line-height: 115%; margin-bottom: 10pt;"/><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEi_aZVJNKOwZAqvG8VakcezTF1GsazQ5tu-zh-UEMdNQKkQMtAWM0t4SD2qxqnLHzt9LVHlq-LpohdzzGTwVbmJyboe3qXdd6HBlkULcCWRWMCVgcDD6NcWZ1UMRR63nYS3GRReYei2bdat0WdJGvk6cbDezmXb59QwCRDbF8pOTJadJ6KWsQ" style="margin-left: 1em; margin-right: 1em;"><img alt="" height="103" src="https://blogger.googleusercontent.com/img/a/AVvXsEi_aZVJNKOwZAqvG8VakcezTF1GsazQ5tu-zh-UEMdNQKkQMtAWM0t4SD2qxqnLHzt9LVHlq-LpohdzzGTwVbmJyboe3qXdd6HBlkULcCWRWMCVgcDD6NcWZ1UMRR63nYS3GRReYei2bdat0WdJGvk6cbDezmXb59QwCRDbF8pOTJadJ6KWsQ" width="320"/></a></div><div class="separator" style="clear: both; text-align: center;"><div class="separator" style="clear: both; text-align: center;"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj8Fz00pgB6ecV04-CrFtPrvPMp_g-csyIZ5A02Sjrizm9LAi8XGsL7G4nnTSojkNX8q9N5EmiRYlawVTwo9fP3qV_fErvIT19WYu3KReaZNDanVUDRTx9ZmS5rOfKgO_6o86wGESe3vheqsG6mLd6DKuomdV8GaQM3t3JIiHNkvaB1Sfbi5w" style="margin-left: 1em; margin-right: 1em;"><img alt="" height="112" src="https://blogger.googleusercontent.com/img/a/AVvXsEj8Fz00pgB6ecV04-CrFtPrvPMp_g-csyIZ5A02Sjrizm9LAi8XGsL7G4nnTSojkNX8q9N5EmiRYlawVTwo9fP3qV_fErvIT19WYu3KReaZNDanVUDRTx9ZmS5rOfKgO_6o86wGESe3vheqsG6mLd6DKuomdV8GaQM3t3JIiHNkvaB1Sfbi5w" width="320"/></a></div><br/><br/></div><br/><br/><br/><p/></div>
    </content>
    <updated>2022-06-03T13:00:00Z</updated>
    <published>2022-06-03T13:00:00Z</published>
    <author>
      <name>Aaron</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/09952936358739421126</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-25562705</id>
      <category term="game theory"/>
      <category term="news"/>
      <author>
        <name>Aaron</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/09952936358739421126</uri>
      </author>
      <link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <title>Adventures in Computation</title>
      <updated>2022-06-03T15:43:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=2149</id>
    <link href="https://toc4fairness.org/practical-robust-and-equitable-uncertainty-estimation/" rel="alternate" type="text/html"/>
    <title>Practical, Robust, and Equitable Uncertainty Estimation</title>
    <summary>This is a post about a new paper that is joint work with Bastani, Gupta, Jung, Noarov, and Ramalingam. The paper is here: https://arxiv.org/abs/2206.01067  and here is a recording of a ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="has-text-align-center"><em>This is a post about a new paper that is joint work with Bastani, Gupta, Jung, Noarov, and Ramalingam. The paper is here: <a href="https://arxiv.org/abs/2206.01067">https://arxiv.org/abs/2206.01067</a>  and here is a recording of a recent talk I gave about it at the Simons Foundation: <a href="https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/&amp;nbsp">https://www.simonsfoundation.org/event/robust-and-equitable-uncertainty-estimation/ </a></em></p>



<p>Machine Learning is really good at making point predictions — but it sometimes makes mistakes. How should we think about which predictions we should trust? In other words, what is the right way to think about the uncertainty of particular predictions? Together with Osbert Bastani, Varun Gupta, Chris Jung, Georgy Noarov, and Ramya Ramalingam, we have some new work I’m really excited about. </p>



<p>A natural way to quantify uncertainty is to predict a set of labels rather than a single one. Pick a degree of certainty — say 90%. For every prediction we make, we’d like to return the smallest set of labels that is guaranteed to contain the true label 90% of the time. These are “prediction sets”, and quantify uncertainty in a natural way: ideally, we will be sure about the correct label, and the prediction set will contain only a single label (the prediction we are certain about). But the larger our prediction set, the more our uncertainty, and the contents of the prediction set lets us know what exactly the model is uncertain about. </p>



<figure class="wp-block-table"><table><tbody><tr><td class="has-text-align-center"><img alt="" class="wp-image-2151" height="256" src="https://i0.wp.com/toc4fairness.org/wp-content/uploads/2022/06/predictionset.png?resize=800%2C256&amp;ssl=1" style="width: 500px;" width="800"/><a href="https://www.blogger.com/blog/post/edit/25562705/4217158232171967686"/></td></tr><tr><td class="has-text-align-center">An example of prediction sets for ImageNet. This example comes from a nice recent paper by Angelopoulos, Bates, Malik, and Jordan: <a href="https://arxiv.org/abs/2009.14193">https://arxiv.org/abs/2009.14193</a> </td></tr></tbody></table></figure>



<p>But how can we do this? Conformal Prediction provides a particularly simple way. Here is an outline of the vanilla version of conformal prediction (there are plenty of variants): </p>



<p>Step 1: Pick a (non)conformity score to measure how different a label y is from a prediction f(x). e.g. for a regression model we could choose $s(x,y) = |f(x)-y|$ — but lots of interesting work has been done recently to develop much fancier ones. A lot of the art of conformal prediction is in finding a good score function.</p>



<p>Step 2: Find a threshold $\tau$ such that for a new example <img alt="(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>, <img alt="\Pr[s(x,y) \leq \tau] = 0.9" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5Bs%28x%2Cy%29+%5Cleq+%5Ctau%5D+%3D+0.9&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>. An easy way to do this is using a holdout set. </p>



<p>Step 3: On a new example <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>, given a point prediction <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>, produce the prediction set <img alt="P(x) = \{y : s(x,y) \leq \tau\}" class="latex" src="https://s0.wp.com/latex.php?latex=P%28x%29+%3D+%5C%7By+%3A+s%28x%2Cy%29+%5Cleq+%5Ctau%5C%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>. </p>



<p>Thats it! Nice and simple. Check out <a href="https://arxiv.org/abs/2107.07511">this recent survey by Angelopolous and Bates</a> for an accessible introduction to conformal prediction. </p>



<p>But a few things could go wrong. First, the technique of using a holdout set only works if the data is i.i.d. or more generally exchangable — i.e. the data distribution should be permutation invariant. But maybe its coming from some changing distribution. If the distribution has changed in an expected and well behaved way, there are some fixes that let you apply the same framework, but if not you are likely in trouble.</p>



<figure class="wp-block-table aligncenter"><table><tbody><tr><td><img alt="" class="wp-image-2152" height="571" src="https://i0.wp.com/toc4fairness.org/wp-content/uploads/2022/06/nonexchangable-joke.png?resize=800%2C571&amp;ssl=1" style="width: 500px;" width="800"/><a href="https://blogger.googleusercontent.com/img/a/AVvXsEje9UBcYGyvVOkiIH_g3O2jlKIfGd5TVuVYOY_81duPMVJRrmTcZycrHumnMMpIp9O3EI4Ubf0dhYBb5dttrZEVWjD92C8qv1Xwmjdn1Y-5bfbzMBqcuTeBq_0ZZ78ymLgWp9kFRhI3f8eehSPv9TmyncWsFEqDHbMLF2JGhl6Y5Giqz5bIIA"/></td></tr><tr><td>A joke about non-exchangable data</td></tr></tbody></table></figure>



<p>Second, an average over everyone might not be what you care about. If we are in a personalized medicine setting, you might care about the reliability of predictions not just overall, but for women with a family history of diabetes and egg allergies — or whatever else you think is medically relevant about you as an individual.</p>



<p>This is the problem that we want to solve: How to give prediction sets that cover their label 90% of the time even if we make no assumptions at all about the data generating process, and even if we care about coverage conditional on arbitrary intersecting subsets of the data.</p>



<p>We want stronger guarantees in another way too. If you think about our goal, there is a way to cheat: 90% of the time, predict the (trivial) set of all labels. 10% of the time predict the empty set. This covers the real label 90% of the time, but is completely uninformative.</p>



<p>To avoid this “solution”, we also ask that our predictions be threshold calibrated. Remember our prediction sets have the form <img alt="P_t(x) = \{y : s(x,y) \leq \tau_t\}" class="latex" src="https://s0.wp.com/latex.php?latex=P_t%28x%29+%3D+%5C%7By+%3A+s%28x%2Cy%29+%5Cleq+%5Ctau_t%5C%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>. Now the threshold <img alt="\tau_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau_t&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> might be different every day. But we want 90% coverage even conditional on the value of <img alt="\tau_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau_t&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>.</p>



<p>This rules out cheating. Remarkably (I think!), for every set of groups specified ahead of time, we’re able to guarantee that even if the data is generated by an adversary, that our empirical coverage converges to 90% at the statistically optimal rate. Here is what that means:</p>



<p>Pick a threshold <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> and group <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>. Consider all <img alt="n_{\tau,G}" class="latex" src="https://s0.wp.com/latex.php?latex=n_%7B%5Ctau%2CG%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> rounds in which the example <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> was in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>, and in which we predicted threshold <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>.  We promise that on this set, we cover 90% <img alt="\pm" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpm&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> <img alt="1/\sqrt{n_{\tau,G}}" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Csqrt%7Bn_%7B%5Ctau%2CG%7D%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> of the labels. This is the best you could do even with a known distribution.</p>



<p>The best thing is that the algorithm is super simple and practical. We had<a href="https://arxiv.org/abs/2101.01739"> a paper last year</a> that showed how to do much of this in theory — but the algorithm from that paper was not easily implementable (it involved solving an exponentially large linear program with a separation oracle).  But here is our new algorithm — it only involves doing a small amount of arithmetic for each prediction:</p>



<figure class="wp-block-image"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh9AnXTAw_h8Z1DI2Ui3Usp0wYWLVHC7u0i8WR90J8WnysneaxXKUZvU9MQL30elKou4H1bZSoajYpYivZz6ztv3MwiDiOHImi6TXIJmaxWa0W-HRmDvbfFaQfOtpK-JISnbszduJz1YuwX_8IhUJweINzwirvtn1zFhNzCCaTRZHrEcAcQ-Q"><img alt="" src="https://blogger.googleusercontent.com/img/a/AVvXsEh9AnXTAw_h8Z1DI2Ui3Usp0wYWLVHC7u0i8WR90J8WnysneaxXKUZvU9MQL30elKou4H1bZSoajYpYivZz6ztv3MwiDiOHImi6TXIJmaxWa0W-HRmDvbfFaQfOtpK-JISnbszduJz1YuwX_8IhUJweINzwirvtn1zFhNzCCaTRZHrEcAcQ-Q"/></a></figure>



<p>So we’re able to implement it and run a bunch of experiments. You can read about them in detail in the paper, but the upshot is that our new method is competitive with split conformal prediction even on “its own turf” — i.e. when the data really is drawn i.i.d. and we only care about marginal coverage — and really excels when the data comes from a more complicated source, or when we measure group-conditional coverage, which traditional methods tend to have much more trouble with. We run experiments on regression and classification tasks, on exchangeable data, under distribution shift, on real time series data, and on adversarial data orderings. Even when the data is i.i.d. and we only care about marginal coverage, our method has an important advantage over split conformal prediction — since we don’t need to preserve exchangability, we can use all of the data to train the underlying model, whereas split conformal prediction needs to reserve some fraction of it for a holdout set. The result is faster learning for our method, which results in smaller/more accurate prediction sets even without the complicating factors of groupwise coverage, threshold calibration or adversarial data!</p>



<figure class="wp-block-image"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEi_aZVJNKOwZAqvG8VakcezTF1GsazQ5tu-zh-UEMdNQKkQMtAWM0t4SD2qxqnLHzt9LVHlq-LpohdzzGTwVbmJyboe3qXdd6HBlkULcCWRWMCVgcDD6NcWZ1UMRR63nYS3GRReYei2bdat0WdJGvk6cbDezmXb59QwCRDbF8pOTJadJ6KWsQ"><img alt="" src="https://blogger.googleusercontent.com/img/a/AVvXsEi_aZVJNKOwZAqvG8VakcezTF1GsazQ5tu-zh-UEMdNQKkQMtAWM0t4SD2qxqnLHzt9LVHlq-LpohdzzGTwVbmJyboe3qXdd6HBlkULcCWRWMCVgcDD6NcWZ1UMRR63nYS3GRReYei2bdat0WdJGvk6cbDezmXb59QwCRDbF8pOTJadJ6KWsQ"/></a></figure>



<figure class="wp-block-image"><a href="https://blogger.googleusercontent.com/img/a/AVvXsEj8Fz00pgB6ecV04-CrFtPrvPMp_g-csyIZ5A02Sjrizm9LAi8XGsL7G4nnTSojkNX8q9N5EmiRYlawVTwo9fP3qV_fErvIT19WYu3KReaZNDanVUDRTx9ZmS5rOfKgO_6o86wGESe3vheqsG6mLd6DKuomdV8GaQM3t3JIiHNkvaB1Sfbi5w"><img alt="" src="https://blogger.googleusercontent.com/img/a/AVvXsEj8Fz00pgB6ecV04-CrFtPrvPMp_g-csyIZ5A02Sjrizm9LAi8XGsL7G4nnTSojkNX8q9N5EmiRYlawVTwo9fP3qV_fErvIT19WYu3KReaZNDanVUDRTx9ZmS5rOfKgO_6o86wGESe3vheqsG6mLd6DKuomdV8GaQM3t3JIiHNkvaB1Sfbi5w"/></a></figure></div>
    </content>
    <updated>2022-06-03T12:25:00Z</updated>
    <published>2022-06-03T12:25:00Z</published>
    <category term="Blog"/>
    <author>
      <name>Aaron Roth</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i0.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2022-06-14T16:22:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=20122</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/" rel="alternate" type="text/html"/>
    <title>Women In Theory</title>
    <summary>I like crossing the imaginary boundaries people set up between different fields—it’s very refreshing. There are lots of tools, and you don’t know which one would work—Maryam Mirzakhani. Shafi Goldwasser is the director of the Simons Institute for the Theory of Computing. She just ran their tenth year celebration. The talks are viewable now on […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<i>I like crossing the imaginary boundaries people set up between different fields—it’s very refreshing. There are lots of tools, and you don’t know which one would work—Maryam Mirzakhani.</i></p>
<p>
Shafi Goldwasser is the director of the Simons Institute for the Theory of Computing. She just ran their tenth year <a href="https://simons.berkeley.edu/workshops/simons-institute-10th-anniversary-symposium">celebration</a>. The talks are viewable now on <a href="https://www.youtube.com/watch?v=-tQ-7kR6r1M&amp;list=PLgKuh-lKre10uB8f4U8YlQssF_IJWd9sq">SimonsTV</a>.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/sg/"><img alt="" class="aligncenter size-full wp-image-20125" height="250" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/sg.jpg?resize=240%2C250&amp;ssl=1" width="240"/></a></p>
<p>
</p><p><b> Another Conference </b></p>
<p/><p>
Soon she will be running another conference: <a href="https://womenintheory.wordpress.com/program/">Women in Theory 2022</a> from June 7 to June 10. Below is the 2018 group photo:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/attachment/2018/" rel="attachment wp-att-20127"><img alt="" class="aligncenter size-full wp-image-20127" height="162" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/2018.jpg?resize=600%2C162&amp;ssl=1" width="600"/></a></p>
<p>
 The Women in Theory (WIT) Workshop is intended for graduate and exceptional undergraduate students in the area of theory of computer science. The workshop will feature technical talks and tutorials by senior and junior women in the field, as well as social events and activities. The motivation for the workshop is twofold. The first goal is to deliver an invigorating educational program; the second is to bring together theory women students from different departments and foster a sense of kinship and camaraderie. </p>
<p>
Here are the speakers, along with talk titles for some of them:</p>
<ul>
<li> Laurie Weingart <br/>
 <img alt="{\blacksquare}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <a href="https://www.cmu.edu/tepper/faculty-and-research/faculty-by-area/profiles/weingart-laurie.html"> Professor of Organizational Behavior and Theory</a> <br/>
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/lw/" rel="attachment wp-att-20129"><img alt="" class="aligncenter size-full wp-image-20129" height="225" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/lw.png?resize=225%2C225&amp;ssl=1" width="225"/></a><p/>
<p>
 <a href="https://www.sandbarbookstore.com/book/9781982152338">The No Club: Putting a Stop to Women’s Dead-End Work.</a> </p>
<p/></li><li> Jennifer Chayes <br/>
 <img alt="{\blacksquare}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <a href="https://data.berkeley.edu/people/jennifer-chayes">Associate Provost</a> <br/>
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/jc/" rel="attachment wp-att-20131"><img alt="" class="aligncenter size-full wp-image-20131" height="200" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/jc.png?resize=200%2C200&amp;ssl=1" width="200"/></a><p/>
<p/></li><li> Raluca Ada Popa <br/>
 <a href="https://people.eecs.berkeley.edu/~raluca/">Co-directs the RISELab</a> <p/>
<p><a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/rp/" rel="attachment wp-att-20133"><img alt="" class="aligncenter size-full wp-image-20133" height="144" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/rp.png?resize=115%2C144&amp;ssl=1" width="115"/></a></p>
<p/></li><li> Kamalika Chaudhuri <br/>
 <img alt="{\blacksquare}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <a href="https://cseweb.ucsd.edu/~kamalika/">Research Scientist in Meta AI</a> <br/>
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/kc/" rel="attachment wp-att-20135"><img alt="" class="aligncenter size-full wp-image-20135" height="194" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/kc.png?resize=259%2C194&amp;ssl=1" width="259"/></a><br/>
 <a href="https://cseweb.ucsd.edu/~kamalika/">Challenges in Learning from Out-of-Distribution Data</a> <p/>
<p/></li><li> Barna Saha <br/>
 <img alt="{\blacksquare}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <a href="https://barnasaha.net">Associate Professor with Jacobs Faculty Scholar</a> <br/>
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/bs-3/" rel="attachment wp-att-20137"><img alt="" class="aligncenter size-full wp-image-20137" height="246" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/bs.png?resize=205%2C246&amp;ssl=1" width="205"/></a><p/>
<p/></li><li> Elette Boyle <br/>
 <img alt="{\blacksquare}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <a href="https://cs.idc.ac.il/~elette/">Director FACT Research Center</a> <br/>
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/eb/" rel="attachment wp-att-20139"><img alt="" class="aligncenter size-full wp-image-20139" height="268" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/eb.png?resize=188%2C268&amp;ssl=1" width="188"/></a><br/>
 <a href="https://cs.idc.ac.il/~elette/">Zero-Knowledge Proofs on Distributed Data and Cryptographic Applications</a> <p/>
<p/></li><li> Gagan Aggarwal <br/>
 <img alt="{\blacksquare}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <a href="http://theory.stanford.edu/~gagan/">Google research scientist.</a> <br/>
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/ga/" rel="attachment wp-att-20141"><img alt="" class="aligncenter size-full wp-image-20141" height="260" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/ga.png?resize=194%2C260&amp;ssl=1" width="194"/></a><p/>
<p/></li><li> Naama Ben-David <br/>
 <img alt="{\blacksquare}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <a href="https://sites.google.com/view/naama-ben-david/home">Postdoctoral researcher</a> <br/>
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/nb/" rel="attachment wp-att-20143"><img alt="" class="aligncenter size-full wp-image-20143" height="400" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/nb.jpg?resize=300%2C400&amp;ssl=1" width="300"/></a><br/>
 <a href="https://publish.illinois.edu/rising-stars/naama-ben-david/">Fast and Fair Lock-Free Locks</a> <p/>
<p/></li><li> Kshipra Bhawalkar <br/>
 <img alt="{\blacksquare}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cblacksquare%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <a href="https://cs.stanford.edu/people/kshipra/">Google Research at Mountain View</a> <br/>
<a href="https://rjlipton.wpcomstaging.com/2022/06/03/women-in-theory/kb/" rel="attachment wp-att-20145"><img alt="" class="aligncenter size-full wp-image-20145" height="200" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/kb.jpg?resize=200%2C200&amp;ssl=1" width="200"/></a><br/>
 <a href="https://research.google/people/KshipraBhawalkar/">Simple Mechanisms for Rich Advertising Auctions</a>
</li></ul>
<p>
</p><p><b> Open Problems </b></p>
<p/><p>
Some of the talk titles have appeared while we were composing this post. Check out the Simons <a href="https://womenintheory.wordpress.com/program/">site</a> for current information. </p>
<p/></div>
    </content>
    <updated>2022-06-03T10:54:49Z</updated>
    <published>2022-06-03T10:54:49Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-06-14T16:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/084</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/084" rel="alternate" type="text/html"/>
    <title>TR22-084 |  Characterizing Derandomization Through Hardness of Levin-Kolmogorov Complexity | 

	Yanyi Liu, 

	Rafael Pass</title>
    <summary>A central open problem in complexity theory concerns the question of whether all efficient randomized algorithms can be simulated by efficient deterministic algorithms. We consider this problem in the context of promise problems (i.e,. the $\prBPP$ v.s. $\prP$ problem) and show that for all sufficiently large constants $c$, the following are \emph{equivalent}:
\BI
\item $\prBPP=\prP$.
\item For every $\BPTIME(n^c)$ algorithm $M$, and every sufficiently large $z \in \{0,1\}^n$, there exists some $x \in \{0,1\}^n$ such that $M$ fails to decide whether $Kt(x \mid z)$ is ``very large'' ($\geq n-1$) or ``very small'' ($\leq O(\log
n)$).
\EI
where $Kt(x \mid z$) denotes the Levin-Kolmogorov complexity of $x$ conditioned on $z$.

As far as we are aware, this yields the first full \emph{characterization} of when $\prBPP = \prP$ through the hardness of some class of problems. Previous hardness assumptions used for derandomization only provide a one-sided implication.</summary>
    <updated>2022-06-03T06:33:00Z</updated>
    <published>2022-06-03T06:33:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-06-14T16:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/083</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/083" rel="alternate" type="text/html"/>
    <title>TR22-083 |  Hardness of Maximum Likelihood Learning of DPPs | 

	Elena Grigorescu, 

	Brendan Juba, 

	Karl  Wimmer, 

	Ning Xie</title>
    <summary>Determinantal Point Processes (DPPs) are a widely used probabilistic model for negatively correlated sets. DPPs have been successfully employed in Machine Learning applications to select a diverse, yet representative subset of data. In these applications, the parameters of the DPP need to be fitted to match the data; typically, we seek a set of parameters that maximize the likelihood of the data. The algorithms used for this task to date either optimize over a limited family of DPPs, 
or use local improvement heuristics that do not provide theoretical guarantees of optimality.
   
It is natural to ask if there exist efficient algorithms for finding a maximum likelihood DPP model for a given data set. In seminal work on DPPs in Machine Learning, Kulesza conjectured in his PhD Thesis (2011) that the problem is NP-complete. 
The lack of a formal proof  prompted Brunel, Moitra, Rigollet and Urschel (COLT 2017) to conjecture that, 
in opposition to Kulesza's conjecture, there exists a polynomial-time algorithm for computing a maximum-likelihood DPP. They also presented some preliminary evidence supporting their conjecture.
   
In this work we prove Kulesza's conjecture. In fact, we prove the following stronger hardness of approximation result: even computing a $\left(1-O(\frac{1}{\log^9{N}})\right)$-approximation to the maximum log-likelihood of a DPP on a ground set of $N$ elements is NP-complete. At the same time, we also obtain the first polynomial-time algorithm that achieves a nontrivial worst-case approximation 
to the optimal log-likelihood: the approximation factor is $\frac{1}{(1+o(1))\log{m}}$ unconditionally (for data sets that consist of $m$ subsets), and can be improved to $1-\frac{1+o(1)}{\log N}$ if all $N$ elements appear in a $O(1/N)$-fraction of the subsets.
   
In terms of techniques, we reduce approximating the maximum log-likelihood of DPPs on a data set to 
solving a gap instance of a ``vector coloring" problem on a  hypergraph. Such a hypergraph is built on a bounded-degree graph construction of Bogdanov, Obata and Trevisan (FOCS 2002), and is further enhanced by the strong  expanders of  Alon and Capalbo (FOCS 2007) to serve our purposes.</summary>
    <updated>2022-06-02T17:37:12Z</updated>
    <published>2022-06-02T17:37:12Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-06-14T16:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/06/01/phd-student-postdoc-at-max-planck-institute-ruhr-university-of-bochum-apply-by-june-30-2022/</id>
    <link href="https://cstheory-jobs.org/2022/06/01/phd-student-postdoc-at-max-planck-institute-ruhr-university-of-bochum-apply-by-june-30-2022/" rel="alternate" type="text/html"/>
    <title>PhD Student / Postdoc at Max Planck Institute / Ruhr University of Bochum (apply by June 30, 2022)</title>
    <summary>Giulio Malavolta and Michael Walter are looking for an outstanding PhD candidate or postdoctoral researcher. Examples of possible research areas include: – Quantum information theory – Quantum and post-quantum cryptography To apply for the position, please send: (1) Curriculum vitae. (2) Electronic contact details of 2-3 potential references. (3) A brief cover letter. Website: https://www.quantiki.org/position/open-position-mpi-sp-and-rub-quantumcryptography-phdpostdoc-100 […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Giulio Malavolta and Michael Walter are looking for an outstanding PhD candidate or postdoctoral researcher. Examples of possible research areas include:</p>
<p>– Quantum information theory<br/>
– Quantum and post-quantum cryptography</p>
<p>To apply for the position, please send:</p>
<p>(1) Curriculum vitae.<br/>
(2) Electronic contact details of 2-3 potential references.<br/>
(3) A brief cover letter.</p>
<p>Website: <a href="https://www.quantiki.org/position/open-position-mpi-sp-and-rub-quantumcryptography-phdpostdoc-100">https://www.quantiki.org/position/open-position-mpi-sp-and-rub-quantumcryptography-phdpostdoc-100</a><br/>
Email: giulio.malavolta@mpi-sp.org</p></div>
    </content>
    <updated>2022-06-01T20:20:07Z</updated>
    <published>2022-06-01T20:20:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-06-14T16:21:01Z</updated>
    </source>
  </entry>
</feed>
