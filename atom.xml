<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-05-16T01:23:31Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.06104</id>
    <link href="http://arxiv.org/abs/1905.06104" rel="alternate" type="text/html"/>
    <title>About a certain NP complete problem</title>
    <feedworld_mtime>1557964800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Stepan Margaryan <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06104">PDF</a><br/><b>Abstract: </b>In this article, we introduce the concept of special decomposition of a set
under the given pairs of subsets of that set, and the concept of special
covering of a set under such a decomposition. We study the conditions for
existence of special coverings of sets, under special decomposition of the set.
Such conditions of formulated problem have important applications in the field
of satisfiability of functions. Our goal is to study the relationship between
sat CNF problem and the problem of existance of special covering of te set. We
also study the relationship between classes of computational complexity by
searching for special coverings of the sets. We prove, that the decidability of
sat CNF problem, in polynomial time reduces to the problem of existence of a
special covering of a set. We also prove, that the problem of existence of a
special covering of a set, in polynomial time reduces to the decidability of
the sat CNF problem. Therefore, the mentioned problems are polynomially
equivalent. And then, the problem of existence of a special covering of a set
is NP-complete problem.
</p></div>
    </summary>
    <updated>2019-05-16T01:20:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.06084</id>
    <link href="http://arxiv.org/abs/1905.06084" rel="alternate" type="text/html"/>
    <title>Restricted Max-Min Allocation: Approximation and Integrality Gap</title>
    <feedworld_mtime>1557964800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Siu=Wing.html">Siu-Wing Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mao:Yuchen.html">Yuchen Mao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.06084">PDF</a><br/><b>Abstract: </b>Asadpour, Feige, and Saberi proved that the integrality gap of the
configuration LP for the restricted max-min allocation problem is at most $4$.
However, their proof does not give a polynomial-time approximation algorithm. A
lot of efforts have been devoted to designing an efficient algorithm whose
approximation ratio can match this upper bound for the integrality gap. In
ICALP 2018, we present a $(6 + \delta)$-approximation algorithm where $\delta$
can be any positive constant, and there is still a gap of roughly $2$. In this
paper, we narrow the gap significantly by proposing a
$(4+\delta)$-approximation algorithm where $\delta$ can be any positive
constant. The approximation ratio is with respect to the optimal value of the
configuration LP, and the running time is $\mathit{poly}(m,n)\cdot
n^{\mathit{poly}(\frac{1}{\delta})}$ where $n$ is the number of players and $m$
is the number of resources. We also improve the upper bound for the integrality
gap of the configuration LP to $3 + \frac{21}{26} \approx 3.808$.
</p></div>
    </summary>
    <updated>2019-05-16T01:20:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05765</id>
    <link href="http://arxiv.org/abs/1905.05765" rel="alternate" type="text/html"/>
    <title>Quantum Complexity of Time Evolution with Chaotic Hamiltonians</title>
    <feedworld_mtime>1557964800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balasubramanian:Vijay.html">Vijay Balasubramanian</a>, Matthew DeCross, Arjun Kar, Onkar Parrikar <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05765">PDF</a><br/><b>Abstract: </b>We study the quantum complexity of time evolution in large-$N$ chaotic
systems, with the SYK model as our main example. This complexity is expected to
increase linearly for exponential time prior to saturating at its maximum
value, and is related to the length of minimal geodesics on the manifold of
unitary operators that act on Hilbert space. Using the Euler-Arnold formalism,
we demonstrate that there is always a geodesic between the identity and the
time evolution operator $e^{-iHt}$ whose length grows linearly with time. This
geodesic is minimal until there is an obstruction to its minimality, after
which it can fail to be a minimum either locally or globally. We identify a
criterion - the Eigenstate Complexity Hypothesis (ECH) - which bounds the
overlap between off-diagonal energy eigenstate projectors and the $k$-local
operators of the theory, and use it to show that the linear geodesic will at
least be a local minimum for exponential time. We show numerically that the
large-$N$ SYK model (which is chaotic) satisfies ECH and thus has no local
obstructions to linear growth of complexity for exponential time, as expected
from holographic duality. In contrast, we also study the case with $N=2$
fermions (which is integrable) and find short-time linear complexity growth
followed by oscillations. Our analysis relates complexity to familiar
properties of physical theories like their spectra and the structure of energy
eigenstates and has implications for the hypothesized computational complexity
class separations PSPACE $\nsubseteq$ BQP/poly and PSPACE $\nsubseteq$
BQSUBEXP/subexp, and the "fast-forwarding" of quantum Hamiltonians.
</p></div>
    </summary>
    <updated>2019-05-16T01:20:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/05/15/postdoc-at-boston-college-apply-by-june-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/05/15/postdoc-at-boston-college-apply-by-june-1-2019/" rel="alternate" type="text/html"/>
    <title>Postdoc at Boston College (apply by June 1, 2019)</title>
    <summary>We invite applications for a postdoc position hosted by Hsin-Hao Su at Boston College. Areas of specific interests include but not limited to distributed graph algorithms, local algorithms, dynamic graph algorithms, gossip algorithms, and massive parallel computation algorithms. The starting date is flexible between Fall 2019 and Spring 2020. The position is for a period […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We invite applications for a postdoc position hosted by Hsin-Hao Su at Boston College. Areas of specific interests include but not limited to distributed graph algorithms, local algorithms, dynamic graph algorithms, gossip algorithms, and massive parallel computation algorithms. The starting date is flexible between Fall 2019 and Spring 2020. The position is for a period of up to two years.</p>
<p>Website: <a href="https://sites.google.com/site/distributedhsinhao/postdoc">https://sites.google.com/site/distributedhsinhao/postdoc</a><br/>
Email: suhx@bc.edu</p></div>
    </content>
    <updated>2019-05-15T19:23:45Z</updated>
    <published>2019-05-15T19:23:45Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-05-16T01:22:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4191</id>
    <link href="https://www.scottaaronson.com/blog/?p=4191" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4191#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4191" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The SSL Certificate of Damocles</title>
    <summary xml:lang="en-US">Ever since I “upgraded” this website to use SSL, it’s become completely inaccessible once every three months, because the SSL certificate expires. Several years in, I’ve been unable to find any way to prevent this from happening, and Bluehost technical support was unable to suggest any solution. The fundamental problem is that, as long as […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Ever since I “upgraded” this website to use SSL, it’s become completely inaccessible once every three months, because the SSL certificate expires.  Several years in, I’ve been unable to find any way to prevent this from happening, and Bluehost technical support was unable to suggest any solution.  The fundamental problem is that, as long as the site remains up, the Bluehost control panel tells me that there’s nothing to do, since there <em>is</em> a current certificate.  Meanwhile, though, I start getting menacing emails saying that my SSL certificate is about to expire and “you must take action to secure the site”—<em>never</em>, of course, specifying what action to take.  The only thing to do seems to be to wait for the whole site to go down, then frantically take random certificate-related actions until somehow the site goes back up.  Those actions vary each time and are not repeatable.</p>



<p>Does anyone know a simple solution to this ridiculous problem?</p>



<p>(The deeper problem, of course, is that a PhD in theoretical computer science left me utterly unqualified for the job of webmaster.  And webmasters, as it turns out, need to do a lot just to prevent anything from changing.  And since childhood, I’ve been accustomed to countless tasks that are trivial for most people being difficult for me—-if that ever stopped being the case, I’d no longer feel like myself.)</p></div>
    </content>
    <updated>2019-05-15T02:56:05Z</updated>
    <published>2019-05-15T02:56:05Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Self-Referential"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-05-15T03:23:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05679</id>
    <link href="http://arxiv.org/abs/1905.05679" rel="alternate" type="text/html"/>
    <title>List-Decodable Linear Regression</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karmalkar:Sushrut.html">Sushrut Karmalkar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kothari:Pravesh.html">Pravesh Kothari</a>, Adam Klivans <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05679">PDF</a><br/><b>Abstract: </b>We give the first polynomial-time algorithm for robust regression in the
list-decodable setting where an adversary can corrupt a greater than $1/2$
fraction of examples. For any $\alpha &lt; 1$, our algorithm takes as input a
sample $\{ (x_i,y_i)\}_{i \leq n}$ of $n$ linear equations where $\alpha n$ of
the equations satisfy $y_i = \langle x_i,\ell^*\rangle +\zeta$ for some small
noise $\zeta$ and $(1-\alpha)n$ of the equations are \emph{arbitrarily} chosen.
It outputs a list $L$ of size $O(1/\alpha)$ - a fixed constant - that contains
an $\ell$ that is close to $\ell^*$. Our algorithm succeeds whenever the
inliers are chosen from a \emph{certifiably} anti-concentrated distribution
$D$. As a special case, this yields a $(d/\alpha)^{O(1/\alpha^8)}$ time
algorithm to find a $O(1/\alpha)$ size list when the inlier distribution is a
standard Gaussian.
</p>
<p>The anti-concentration assumption on the inliers is information-theoretically
necessary. Our algorithm works for more general distributions under the
additional assumption that $\ell^*$ is Boolean valued. To solve the problem we
introduce a new framework for list-decodable learning that strengthens the
sum-of-squares `identifiability to algorithms' paradigm.
</p>
<p>In an independent work, Raghavendra and Yau [RY19] have obtained a similar
result for list-decodable regression also using the sum-of-squares method.
</p></div>
    </summary>
    <updated>2019-05-15T23:21:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05669</id>
    <link href="http://arxiv.org/abs/1905.05669" rel="alternate" type="text/html"/>
    <title>Stochastic thermodynamics of computation</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wolpert:David_H=.html">David H. Wolpert</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05669">PDF</a><br/><b>Abstract: </b>One of the major resource requirements of computers - ranging from biological
cells to human brains to high-performance (engineered) computers - is the
energy used to run them. Those costs of performing a computation have long been
a focus of research in physics, going back to the early work of Landauer. One
of the most prominent aspects of computers is that they are inherently
nonequilibrium systems. However, the early research was done when
nonequilibrium statistical physics was in its infancy, which meant the work was
formulated in terms of equilibrium statistical physics. Since then there have
been major breakthroughs in nonequilibrium statistical physics, which are
allowing us to investigate the myriad aspects of the relationship between
statistical physics and computation, extending well beyond the issue of how
much work is required to erase a bit. In this paper I review some of this
recent work on the `stochastic thermodynamics of computation'. After reviewing
the salient parts of information theory, computer science theory, and
stochastic thermodynamics, I summarize what has been learned about the entropic
costs of performing a broad range of computations, extending from bit erasure
to loop-free circuits to logically reversible circuits to information ratchets
to Turing machines. These results reveal new, challenging engineering problems
for how to design computers to have minimal thermodynamic costs. They also
allow us to start to combine computer science theory and stochastic
thermodynamics at a foundational level, thereby expanding both.
</p></div>
    </summary>
    <updated>2019-05-15T23:20:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05655</id>
    <link href="http://arxiv.org/abs/1905.05655" rel="alternate" type="text/html"/>
    <title>Online Computation with Untrusted Advice</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Angelopoulos:Spyros.html">Spyros Angelopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uuml=rr:Christoph.html">Christoph Dürr</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Shendan.html">Shendan Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamali:Shahin.html">Shahin Kamali</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Renault:Marc.html">Marc Renault</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05655">PDF</a><br/><b>Abstract: </b>The advice model of online computation captures the setting in which the
online algorithm is given some partial information concerning the request
sequence. This paradigm allows to establish tradeoffs between the amount of
this additional information and the performance of the online algorithm.
However, unlike real life in which advice is a recommendation that we can chose
to follow or to ignore based on trustworthiness, in the current advice model,
the online algorithm treats it as infallible. This means that if the advice is
corrupt or, worse, if it comes from a malicious source, the algorithm may
perform poorly. In this work, we study online computation in a setting in which
the advice is provided by an untrusted source. Our objective is to quantify the
impact of untrusted advice so as to design and analyze online algorithms that
are resilient and perform well even when the advice is generated in a
malicious, adversarial manner. To this end, we focus on well-studied online
problems such as ski rental, online bidding, bin packing, and list update. For
ski-rental and online bidding, we show how to obtain algorithms that are
Pareto-optimal with respect to the competitive ratios achieved; this improves
upon the framework of Purohit et al. [NeurIPS 2018] in which Pareto-optimality
is not necessarily guaranteed. For bin packing and list update, we give online
algorithms with worst-case tradeoffs in their competitiveness, depending on
whether the advice is trusted or not; this is motivated by work of Lykouris and
Vassilvitskii [ICML 2018] on the paging problem, but in which the
competitiveness depends on the reliability of the advice.
</p></div>
    </summary>
    <updated>2019-05-15T23:30:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05643</id>
    <link href="http://arxiv.org/abs/1905.05643" rel="alternate" type="text/html"/>
    <title>Sample Efficient Toeplitz Covariance Estimation</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eldar:Yonina_C=.html">Yonina C. Eldar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jerry.html">Jerry Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musco:Cameron.html">Cameron Musco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musco:Christopher.html">Christopher Musco</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05643">PDF</a><br/><b>Abstract: </b>We study the query complexity of estimating the covariance matrix $T$ of a
distribution $\mathcal{D}$ over $d$-dimensional vectors, under the assumption
that $T$ is Toeplitz. This assumption arises in many signal processing
problems, where the covariance between any two measurements only depends on the
time or distance between those measurements. We are interested in estimation
strategies that may choose to view only a subset of entries in each vector
sample $x^{(\ell)} \sim \mathcal{D}$, which often equates to reducing hardware
and communication requirements in applications ranging from wireless signal
processing to advanced imaging. Our goal is to minimize both 1) the number of
vector samples drawn from $\mathcal{D}$ and 2) the number of entries accessed
in each sample.
</p>
<p>We provide some of the first non-asymptotic bounds on these sample complexity
measures that exploit $T$'s Toeplitz structure, and by doing so, significantly
improve on results for generic covariance matrices. Our bounds follow from a
novel analysis of classical and widely used estimation algorithms (along with
some new variants), including methods based on selecting entries from each
vector sample according to a so-called sparse ruler. In many cases, we pair our
upper bounds with matching or nearly matching lower bounds.
</p>
<p>In addition to results that hold for any Toeplitz $T$, we further study the
important setting when $T$ is close to low-rank, which is often the case in
practice. We show that methods based on sparse rulers perform even better in
this setting, with sample complexity scaling sublinearly in $d$. Motivated by
this finding, we develop a new covariance estimation strategy that further
improves on all existing methods in the low-rank case: when $T$ is rank-$k$ or
nearly rank-$k$, it achieves sample complexity depending polynomially on $k$
and only logarithmically on $d$.
</p></div>
    </summary>
    <updated>2019-05-15T23:22:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05494</id>
    <link href="http://arxiv.org/abs/1905.05494" rel="alternate" type="text/html"/>
    <title>Practical Volume Estimation by a New Annealing Schedule for Cooling Convex Bodies</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalkis:Apostolos.html">Apostolos Chalkis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Emiris:Ioannis_Z=.html">Ioannis Z. Emiris</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fisikopoulos:Vissarion.html">Vissarion Fisikopoulos</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05494">PDF</a><br/><b>Abstract: </b>We study the problem of estimating the volume of convex polytopes, focusing
on H- and V-polytopes, as well as zonotopes. Although a lot of effort is
devoted to practical algorithms for H-polytopes there is no such method for the
latter two representations. We propose a new, practical algorithm for all
representations, which is faster than existing methods. It relies on
Hit-and-Run sampling, and combines a new simulated annealing method with the
Multiphase Monte Carlo (MMC) approach. Our method introduces the following key
features to make it adaptive: (a) It defines a sequence of convex bodies in MMC
by introducing a new annealing schedule, whose length is shorter than in
previous methods with high probability, and the need of computing an enclosing
and an inscribed ball is removed; (b) It exploits statistical properties in
rejection-sampling and proposes a better empirical convergence criterion for
specifying each step; (c) For zonotopes, it may use a sequence of convex bodies
for MMC different than balls, where the chosen body adapts to the input. We
offer an open-source, optimized C++ implementation, and analyze its performance
to show that it outperforms state-of-the-art software for H-polytopes by
Cousins-Vempala (2016) and Emiris-Fisikopoulos (2018), while it undertakes
volume computations that were intractable until now, as it is the first
polynomial-time, practical method for V-polytopes and zonotopes that scales to
high dimensions (currently 100). We further focus on zonotopes, and
characterize them by their order (number of generators over dimension), because
this largely determines sampling complexity. We analyze a related application,
where we evaluate methods of zonotope approximation in engineering.
</p></div>
    </summary>
    <updated>2019-05-15T23:34:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05376</id>
    <link href="http://arxiv.org/abs/1905.05376" rel="alternate" type="text/html"/>
    <title>Dimensionality Reduction for Tukey Regression</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Clarkson:Kenneth_L=.html">Kenneth L. Clarkson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Ruosong.html">Ruosong Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05376">PDF</a><br/><b>Abstract: </b>We give the first dimensionality reduction methods for the overconstrained
Tukey regression problem. The Tukey loss function $\|y\|_M = \sum_i M(y_i)$ has
$M(y_i) \approx |y_i|^p$ for residual errors $y_i$ smaller than a prescribed
threshold $\tau$, but $M(y_i)$ becomes constant for errors $|y_i| &gt; \tau$. Our
results depend on a new structural result, proven constructively, showing that
for any $d$-dimensional subspace $L \subset \mathbb{R}^n$, there is a fixed
bounded-size subset of coordinates containing, for every $y \in L$, all the
large coordinates, with respect to the Tukey loss function, of $y$. Our methods
reduce a given Tukey regression problem to a smaller weighted version, whose
solution is a provably good approximate solution to the original problem. Our
reductions are fast, simple and easy to implement, and we give empirical
results demonstrating their practicality, using existing heuristic solvers for
the small versions. We also give exponential-time algorithms giving provably
good solutions, and hardness results suggesting that a significant speedup in
the worst case is unlikely.
</p></div>
    </summary>
    <updated>2019-05-15T23:32:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05334</id>
    <link href="http://arxiv.org/abs/1905.05334" rel="alternate" type="text/html"/>
    <title>Generating Weighted MAX-2-SAT Instances of Tunable Difficulty with Frustrated Loops</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pei:Yan_Ru.html">Yan Ru Pei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manukian:Haik.html">Haik Manukian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Ventra:Massimiliano_Di.html">Massimiliano Di Ventra</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05334">PDF</a><br/><b>Abstract: </b>Many optimization problems can be cast into the maximum satisfiability
(MAX-SAT) form, and many solvers have been developed for tackling such
problems. To evaluate the performance of a MAX-SAT solver, it is convenient to
generate difficult MAX-SAT instances with solutions known in advance. Here, we
propose a method of generating weighted MAX-2-SAT instances inspired by the
frustrated-loop algorithm used by the quantum annealing community to generate
Ising spin-glass instances with nearest-neighbor coupling. Our algorithm is
extended to instances whose underlying coupling graph is general, though we
focus here on the case of bipartite coupling, with the associated energy being
the restricted Boltzmann machine (RBM) energy. It is shown that any MAX-2-SAT
problem can be reduced to the problem of minimizing an RBM energy over the
nodal values. The algorithm is designed such that the difficulty of the
generated instances can be tuned through a central parameter known as the
frustration index. Two versions of the algorithm are presented: the random- and
structured-loop algorithms. For the random-loop algorithm, we provide a
thorough theoretical and empirical analysis on its mathematical properties from
the perspective of frustration, and observe empirically, using simulated
annealing, a double phase transition behavior in the difficulty scaling
behavior driven by the frustration index. For the structured-loop algorithm, we
show that it offers an improvement in difficulty of the generated instances
over the random-loop algorithm, with the improvement factor scaling
super-exponentially with respect to the frustration index for instances at high
loop density. At the end of the paper, we provide a brief discussion of the
relevance of this work to the pre-training of RBMs.
</p></div>
    </summary>
    <updated>2019-05-15T23:21:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05329</id>
    <link href="http://arxiv.org/abs/1905.05329" rel="alternate" type="text/html"/>
    <title>Computing and Testing Small Vertex Connectivity in Near-Linear Time and Queries</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nanongkai:Danupon.html">Danupon Nanongkai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yingchareonthawornchai:Sorrachai.html">Sorrachai Yingchareonthawornchai</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05329">PDF</a><br/><b>Abstract: </b>We present a new, simple, algorithm for the local vertex connectivity problem
(LocalVC) introduced by Nanongkai~et~al. [STOC'19]. Roughly, given an
undirected unweighted graph $G$, a seed vertex $x$, a target volume $\nu$, and
a target separator size $k$, the goal of LocalVC is to remove $k$ vertices
`near' $x$ (in terms of $\nu$) to disconnect the graph in `local time', which
depends only on parameters $\nu$ and $k$. In this paper, we present a simple
randomized algorithm with running time $O(\nu k^2)$ and correctness probability
$2/3$.
</p>
<p>Plugging our new localVC algorithm in the generic framework of
Nanongkai~et~al. immediately lead to a randomized $\tilde O(m+nk^3)$-time
algorithm for the classic $k$-vertex connectivity problem on undirected graphs.
($\tilde O(T)$ hides $\text{polylog}(T)$.) This is the {\em first} near-linear
time algorithm for any $4\leq k \leq \text{polylog} n$. Previous fastest
algorithm for small $k$ takes $\tilde O(m+n^{4/3}k^{7/3})$ time
[Nanongkai~et~al., STOC'19].
</p>
<p>This work is inspired by the algorithm of Chechik~et~al. [SODA'17] for
computing the maximal $k$-edge connected subgraphs. Forster and Yang [arXiv'19]
has independently developed local algorithms similar to ours, and showed that
they lead to an $\tilde O(k^3/\epsilon)$ bound for testing $k$-edge and -vertex
connectivity, resolving two long-standing open problems in property testing
since the work of Goldreich and Ron [STOC'97] and Orenstein and Ron [Theor.
Comput. Sci.'11]. Inspired by this, we use local approximation algorithms to
obtain bounds that are near-linear in $k$, namely $\tilde O(k/\epsilon)$ and
$\tilde O(k/\epsilon^2)$ for the bounded and unbounded degree cases,
respectively. For testing $k$-edge connectivity for simple graphs, the bound
can be improved to $\tilde O(\min(k/\epsilon, 1/\epsilon^2))$.
</p></div>
    </summary>
    <updated>2019-05-15T23:31:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05291</id>
    <link href="http://arxiv.org/abs/1905.05291" rel="alternate" type="text/html"/>
    <title>5/4 approximation for Symmetric TSP</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Madhusudan Verma, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chauhan:Alok.html">Alok Chauhan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/V:Vijayakumar.html">Vijayakumar V</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05291">PDF</a><br/><b>Abstract: </b>Travelling Salesman Problem (TSP) is one of the unsolved problems in computer
science. TSP is NP Hard. Till now the best approximation ratio found for
symmetric TSP is three by two by Christofides Algorithm more than thirty years
ago. There are different approaches to solve this problem. These range from
methods based on neural networks, genetic algorithm, swarm optimization, ant
colony optimization etc. The bound is further reduced from three by two but for
graphic TSP. A factor of thirteen by nine was found for Graphic TSP. A newly
proposed heuristic called k RNN is considered here. It seems from experimental
results that five by four is the approximation ratio. A performance analysis is
done for this heuristic and it confirms experimental bound of five by four.
</p></div>
    </summary>
    <updated>2019-05-15T23:31:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05290</id>
    <link href="http://arxiv.org/abs/1905.05290" rel="alternate" type="text/html"/>
    <title>Revisiting Graph Width Measures for CNF-Encodings</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mengel:Stefan.html">Stefan Mengel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wallon:Romain.html">Romain Wallon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05290">PDF</a><br/><b>Abstract: </b>We consider bounded width CNF-formulas where the width is measured by popular
graph width measures on graphs associated to CNF-formulas. Such restricted
graph classes, in particular those of bounded treewidth, have been extensively
studied for their uses in the design of algorithms for various computational
problems on CNF-formulas. Here we consider the expressivity of these formulas
in the model of clausal encodings with auxiliary variables. We first show that
bounding the width for many of the measures from the literature leads to a
dramatic loss of expressivity, restricting the formulas to such of low
communication complexity. We then show that the width of optimal encodings with
respect to different measures is strongly linked: there are two classes of
width measures, one containing primal treewidth and the other incidence
cliquewidth, such that in each class the width of optimal encodings only
differs by constant factors. Moreover, between the two classes the width
differs at most by a factor logarithmic in the number of variables. Both these
results are in stark contrast to the setting without auxiliary variables where
all width measures we consider here differ by more than constant factors and in
many cases even by linear factors.
</p></div>
    </summary>
    <updated>2019-05-15T23:21:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.05254</id>
    <link href="http://arxiv.org/abs/1905.05254" rel="alternate" type="text/html"/>
    <title>Optimal Multithreaded Batch-Parallel 2-3 Trees</title>
    <feedworld_mtime>1557878400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lim:Wei_Quan.html">Wei Quan Lim</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.05254">PDF</a><br/><b>Abstract: </b>This paper presents a batch-parallel 2-3 tree T in the asynchronous PPM
(parallel pointer machine) model that supports searches, insertions and
deletions in sorted batches and has essentially optimal parallelism, even under
the QRMW (queued read-modify-write) memory model where concurrent memory
accesses to the same location are queued and serviced one by one.
</p>
<p>Specifically, if T has n items, then performing an item-sorted batch of b
operations on T takes only O( b * log(n/b+1) + b ) work and O( log b + log n )
span (in the worst case as b,n -&gt; inf). This is information-theoretically
work-optimal for b &lt;= n, and also span-optimal in the PPM model. The input
batch can be any balanced binary tree, and hence T can also be used to
implement sorted sets supporting optimal intersection, union and difference of
sets with sizes m &lt;= n in O( m * log(n/m+1) ) work and O( log m + log n ) span.
</p>
<p>To the author's knowledge, T is the first parallel sorted-set data structure
with these performance bounds that can be used in an asynchronous
multi-processor machine under a memory model with queued contention. This is
unlike PRAM data structures such as the PVW 2-3 tree (by Paul, Vishkin and
Wagener), which rely on lock-step synchronicity of the processors. In fact, T
is designed to have bounded contention and satisfy the claimed work and span
bounds regardless of the execution schedule.
</p>
<p>All data structures and algorithms in this paper fit into the dynamic
multithreading paradigm. Also, as a consequence of working in the asynchronous
PPM model, all their performance bounds are directly composable with those of
other data structures and algorithms in the same model.
</p></div>
    </summary>
    <updated>2019-05-15T23:32:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/071</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/071" rel="alternate" type="text/html"/>
    <title>TR19-071 |  Time-Space Lower Bounds for Two-Pass Learning | 

	Sumegha Garg, 

	Ran Raz, 

	Avishay Tal</title>
    <summary>A line of recent works showed that for a large class of learning problems, any learning algorithm  requires either super-linear memory size or a super-polynomial number of samples [Raz16,KRT17,Raz17,MM18,BOGY18,GRT18]. For example, any algorithm for learning parities of size $n$ requires either a memory of size $\Omega(n^{2})$ or an exponential number of samples [Raz16].

All these works modeled the learner as a one-pass branching program, allowing only one pass over the stream of samples. In this work, we prove the first memory-samples lower bounds (with a super-linear lower bound on the memory size and super-polynomial lower bound on the number of samples) when the learner is allowed two passes over the stream of samples. For example, we prove that any two-pass algorithm for learning parities of size $n$ requires either a memory of size $\Omega(n^{1.5})$ or at least $2^{\Omega(\sqrt{n})}$ samples.

More generally, a matrix $M: A \times X \rightarrow \{-1,1\}$ corresponds to the following learning problem: An unknown element $x \in X$ is chosen uniformly at random. A learner tries to learn $x$ from a stream of samples, $(a_1, b_1), (a_2, b_2) \ldots$, where for every $i$, $a_i \in A$ is chosen uniformly at random and $b_i = M(a_i,x)$.

Assume that $k,l, r$ are such that any submatrix of $M$ of at least $2^{-k} \cdot |A|$ rows and at least $2^{-l} \cdot |X|$ columns, has a bias of at most $2^{-r}$. We show that any two-pass learning algorithm for the learning problem corresponding to $M$ requires either a memory of size at least $\Omega\left(k \cdot  \min\{k,\sqrt{l}\} \right)$, or at least $2^{\Omega(\min\{k,\sqrt{l},r\})}$ samples.</summary>
    <updated>2019-05-14T16:35:18Z</updated>
    <published>2019-05-14T16:35:18Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-16T01:22:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15858</id>
    <link href="https://rjlipton.wordpress.com/2019/05/14/internet-dogs-and-the-abc-conjecture/" rel="alternate" type="text/html"/>
    <title>Internet, Dogs, and the ABC Conjecture</title>
    <summary>An inappropriate comment on the ABC conjecture Joseph Oesterlé and David Masser are famous for their independent discovery of the ABC conjecture. Today I want to point out an unfair comment about their discovery. Anonymity on the Internet was captured by a famous 1993 cartoon in the New Yorker magazine titled, “On the Internet, nobody […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>An inappropriate comment on the ABC conjecture</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<a href="https://rjlipton.wordpress.com/2019/05/14/internet-dogs-and-the-abc-conjecture/220px-david_masser/" rel="attachment wp-att-15867"><img alt="" class="alignright wp-image-15867" src="https://rjlipton.files.wordpress.com/2019/05/220px-david_masser.jpg?w=150" width="150"/></a><br/><a href="https://rjlipton.wordpress.com/2019/05/14/internet-dogs-and-the-abc-conjecture/220px-oesterle_joseph/" rel="attachment wp-att-15868"><img alt="" class="alignright  wp-image-15868" src="https://rjlipton.files.wordpress.com/2019/05/220px-oesterle_joseph.jpg?w=150" width="150"/></a><br/><table class="image alignright">
<tbody>
<tr>
<td>
</td>
</tr>
<tr>


</tr>
</tbody>
</table>
<p>
Joseph Oesterlé and David Masser are famous for their independent discovery of the ABC conjecture. </p>
<p>
Today I want to point out an unfair comment about their discovery.</p>
<p>
Anonymity on the Internet was captured by a famous 1993 <a href="https://en.wikipedia.org/wiki/On_the_Internet,_nobody_knows_you%27re_a_dog">cartoon</a> in the <i>New Yorker</i> magazine titled, “On the Internet, nobody knows you’re a dog.”  Amazing to think that was more than a quarter-century ago and remains true.  But people <i>can</i> tell if what you’ve written is something inappropriate.</p>
<p/><h2> The Comment </h2><p/>
<p/><p>
The comment is: </p>
<blockquote><p><b> </b> <em> <i>SAYS WHO??? I have some trouble with this item.</i> </em>
</p></blockquote>
<p/><p>
Masser is a Fellow of the Royal Society, who was elected in 2005. He is </p>
<blockquote><p><b> </b> <em> <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\dots}"/> also responsible, following an earlier insight of Joseph Oesterlé, for formulating the abc conjecture; this is a simple statement about integers which seems to hold the key to much of the future direction of number theory. </em>
</p></blockquote>
<p/><p>
See this <a href="https://royalsociety.org/people/david-masser-11903/">link</a> for his full citation and the comment. Click on the <i>show more bibliography</i> button there. The comment is apparently anonymous, although the author is probably known to some. I thank Joël Ouaknine for pointing out this strange comment.</p>
<p>
<b>Update:</b> Ken speculates that it’s a misplaced comment by an editor of the Royal Society website itself.  Perhaps they compose HTML from MS Word or Acrobat or other software that provides comment bubbles—but this one escaped the bubble and wasn’t noticed.  Editors of Wikipedia have automatic tools for flagging assertions that are unsupported or at least need citation.  </p>
<p>
What the comment undoubtedly shows is vigorous debate behind the walls of Britain’s august institution.  So let’s say a little more on what the comment is about.</p>
<p/><h2> The ABC Conjecture </h2><p/>
<p/><p>
The biggest mysteries about numbers often concern the interaction between addition and multiplication. For example: </p>
<ul>
<li>
The <a href="https://en.wikipedia.org/wiki/Twin_prime">twin</a> prime conjecture: There are an infinite number of primes <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> such that <img alt="{p+2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p+2}"/> is also prime. This is due to Alphonse de Polignac. <p/>
</li><li>
The <a href="https://en.wikipedia.org/wiki/Goldbach%27s_conjecture">Goldbach</a> conjecture: Every even number greater than four is the sum of two odd primes. This is due to Christian Goldbach. <p/>
</li><li>
The <a href="https://en.wikipedia.org/wiki/Brocard%27s_problem">Brocard</a> conjecture: There are only a finite number of solutions to <img alt="{n! = m^{2} + 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%21+%3D+m%5E%7B2%7D+%2B+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n! = m^{2} + 1}"/> in natural numbers. This is due to Henri Brocard.
</li></ul>
<p>
Suppose that <img alt="{A + B = C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%2B+B+%3D+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A + B = C}"/> where <img alt="{A,B,C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B,C}"/> are positive and co-prime natural numbers. Let <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> be the product of all the distinct prime divisors of <img alt="{ABC}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BABC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ABC}"/>. Then the ABC conjecture says that 	</p>
<p align="center"><img alt="\displaystyle  C \le O(D^{2}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+O%28D%5E%7B2%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le O(D^{2}). "/></p>
<p>Note, this inequality does indeed connect adding with multiplying. The usual conjecture is stronger, see <a href="https://en.wikipedia.org/wiki/Abc_conjecture">this</a> for details.	</p>
<p>
The ABC conjecture appears to be open, even though Shinichi Mochizuki has claimed a <a href="https://rjlipton.wordpress.com/2012/09/12/the-abc-conjecture-and-cryptography/">proof</a> for years. See <a href="https://www.quantamagazine.org/titans-of-mathematics-clash-over-epic-proof-of-abc-conjecture-20180920">this</a> for a discussion about the status of the conjecture. </p>
<blockquote><p><b> </b> <em> Despite multiple conferences dedicated to explicating Mochizuki’s proof, number theorists have struggled to come to grips with its underlying ideas. His series of papers, which total more than 500 pages, are written in an impenetrable style, and refer back to a further 500 pages or so of previous work by Mochizuki, creating what one mathematician, Brian Conrad of Stanford University, has called “a sense of infinite regress.” </em>
</p></blockquote>
<p>
</p><p/><h2> The Comment II </h2><p/>
<p/><p>
The comment on Masser’s work is wrong, strange, inappropriate. Oesterlé and Masser deserve more credit, not less, for their brilliant discovery of the ABC conjecture. There are now many—perhaps hundreds—of applications of the ABC conjecture. For example consider generalizations of Fermat’s Last Theorem. Suppose that 	</p>
<p align="center"><img alt="\displaystyle  x^{p} + y^{q} = z^{r} \text{  (*)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7Bp%7D+%2B+y%5E%7Bq%7D+%3D+z%5E%7Br%7D+%5Ctext%7B++%28%2A%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^{p} + y^{q} = z^{r} \text{  (*)}"/></p>
<p>where <img alt="{p,q,r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%2Cq%2Cr%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p,q,r}"/> are odd primes. And <img alt="r &gt; 6" class="latex" src="https://s0.wp.com/latex.php?latex=r+%3E+6&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r &gt; 6"/>. Provided <img alt="{x,y,z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y,z}"/> are positive and co-prime, it follows by the ABC conjecture that <img alt="{z^{r}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%5E%7Br%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z^{r}}"/> is bounded by <img alt="{O(z^{2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28z%5E%7B2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(z^{2})}"/>. This is impossible for <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> large enough since <img alt="{r \ge 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Cge+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r \ge 3}"/>. Therefore, (*) can only have a finite number of solutions. Pretty neat.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Do you know of any other inappropriate comments of this kind?</p>
<p/><p><br/>
[added remark by Ken, linked rather than embed dog cartoon]<br/>
[Added prime r must be 7 or larger. Thanks to comment by MadHatter.]</p></font></font></div>
    </content>
    <updated>2019-05-14T14:49:25Z</updated>
    <published>2019-05-14T14:49:25Z</published>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="ABC conjecture"/>
    <category term="Diophantine"/>
    <category term="equation"/>
    <category term="Fermat"/>
    <category term="strange"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-16T01:22:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/070</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/070" rel="alternate" type="text/html"/>
    <title>TR19-070 |  On Local Testability in the Non-Signaling Setting | 

	Alessandro Chiesa, 

	Peter Manohar, 

	Igor Shinkar</title>
    <summary>Non-signaling strategies are a generalization of quantum strategies that have been studied in physics for decades, and have recently found applications in theoretical computer science. These applications motivate the study of local-to-global phenomena for non-signaling functions.

We present general results about the local testability of linear codes in the non-signaling setting. Our contributions include formulating natural definitions that capture the condition that a non-signaling function "belongs" to a given code, and characterizing the sets of local constraints that imply membership in the code. We prove these results by relating the Fourier spectrum of non-signaling functions to Cayley hypergraphs induced by local constraints.

We apply the above results to show a separation between locally testable codes in the classical and non-signaling setting by proving that bivariate low-degree testing fails spectacularly in the non-signaling setting. Specifically, we show that there exist non-signaling functions that pass bivariate low-degree tests with probability 1, and yet are maximally far from low-degree.</summary>
    <updated>2019-05-14T05:26:11Z</updated>
    <published>2019-05-14T05:26:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-16T01:22:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=633</id>
    <link href="https://emanueleviola.wordpress.com/2019/05/13/and-this-is-me-with-my-buddies/" rel="alternate" type="text/html"/>
    <title>And this is me with my buddies</title>
    <summary>  Advertisements</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="RY3tic">
<div class="eGiHwc"><img alt="warriors2" class="alignnone size-full wp-image-634" src="https://emanueleviola.files.wordpress.com/2019/05/warriors2.jpg?w=640"/></div>
<div class="KYCEmd"/>
</div>
<div class="RY3tic">
<div class="eGiHwc"/>
<div class="KYCEmd"/>
</div>
<p> </p></div>
    </content>
    <updated>2019-05-13T15:24:14Z</updated>
    <published>2019-05-13T15:24:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Emanuele</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2019-05-16T01:22:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3507110503322010708</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3507110503322010708/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/ronald-grahams-other-large-number-well.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3507110503322010708" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3507110503322010708" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/ronald-grahams-other-large-number-well.html" rel="alternate" type="text/html"/>
    <title>Ronald Graham's other large number. Well---- it was large in 1964 anyway.</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Graham's number (see <a href="https://en.wikipedia.org/wiki/Graham%27s_number">here</a>) was at one time the largest number to appear in a math proof.<br/>
<br/>
a) GN was an upper bound on a problem in Ramsey theory. There are now better upper bounds, see <a href="https://www.sciencedirect.com/science/article/pii/S0195669814000936?via%3Dihub">here</a>. These better upper bounds are still large- Hales-Jewitt-Large, but that's already much smaller than the original GN.<br/>
<br/>
b) Other proofs now have numbers even larger than GN. For example Harvey Friedman's work on the finite versions of Kruskal's Tree Theorem. (There may be other cases- if you know some then let me know in the comments.)<br/>
<br/>
Since my dept recently moved buildings I found old papers that I had not looked at in years. One of them was<br/>
<br/>
<i>Old and New Problems and Results in Combinatorial Number Theory</i><br/>
<br/>
by Erdos and Graham<br/>
<br/>
(see <a href="http://www.math.ucsd.edu/~ronspubs/79_09_combinatorial_number_theory.pdf">here</a>)<br/>
<br/>
So I began reading it and came across a result of Graham from 1964 that used large numbers. No where near as large as GN, but I found it interesting that Graham was involved with large numbers way back then.<br/>
<br/>
Here is the problem:<br/>
<br/>
A <i>Lucas Sequence</i> is a sequence that obeys<br/>
<br/>
a(n) = a(n-1) + a(n-2).<br/>
<br/>
Clearly such a sequence is determined by a(0) and a(1).<br/>
<br/>
QUESTION: Does there exists a(0) and a(1)  that are rel prime such that the sequence has only composite numbers?<br/>
<br/>
By ingenuity and some computing power Graham found YES. For how the got the numbers see <a href="http://www.math.ucsd.edu/~ronspubs/64_06_fibonacci.pdf">here</a>. The numbers are of course in the paper, and how they got them is interesting, but I present them anyway. Hope I don't make a typo:<br/>
<br/>
a(0) = 1786772701928802632268715130455793<br/>
<br/>
a(1) = 1059683225053915111058164141686995<br/>
<br/>
The paper Old and New... says its open if there is a smaller pair of numbers, I do not know if it is still open. If you know, let us all  know in the comments!<br/>
<br/>
These numbers seem small today since we have modern computers that can store and manipulate them easily. Were the considered large numbers in 1964? They were never called Graham Numbers which is probably just as well since that honor lay ahead.<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-05-13T04:39:00Z</updated>
    <published>2019-05-13T04:39:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-15T18:02:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-7244168862511547770</id>
    <link href="http://processalgebra.blogspot.com/feeds/7244168862511547770/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=7244168862511547770" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7244168862511547770" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7244168862511547770" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2019/05/ice-tcs-theory-day-2019.html" rel="alternate" type="text/html"/>
    <title>ICE-TCS Theory Day 2019</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><i>This post also appears on the <a href="https://ice-tcs.blogspot.com/2019/05/ice-tcs-theory-day-2019.html">ICE-TCS blog</a>. </i><br/><br/>On Friday, 3 May, ICE-TCS hosted its <a href="http://icetcs.ru.is/theory-day2019.html">15th annual Theory Day</a>. The event consisted of two 45-minute presentations by <a href="https://dblp.uni-trier.de/pers/hd/b/Boppana:Ravi_B=">Ravi  Boppana</a> (Department of Mathematics, MIT) and <a href="https://dcc.fceia.unr.edu.ar/~erivas/">Exequiel Rivas</a>(Inria Paris - Rocquencourt, France),  and three ten-minute presentations by ICE-TCS researchers highlighting some of the recent research directions pursued by members of the centre.<br/><br/><a href="https://dblp.uni-trier.de/pers/hd/b/Boppana:Ravi_B=">Ravi  Boppana</a> kicked off the Theory Day with a wonderfully paced talk on his <a href="https://www.combinatorics.org/ojs/index.php/eljc/article/view/v24i3p40">work</a> with <a href="https://holzman.technion.ac.il/">Ron Holzman</a> on Tomaszewski’s problem on randomly signed sums. The problem is as follows. Let <span class="mjx-chtml MathJax_CHTML" id="MathJax-Element-1-Frame" style="font-size: 129%;" tabindex="0"><span class="mjx-math" id="MJXc-Node-1"><span class="mjx-mrow" id="MJXc-Node-2"><span class="mjx-msubsup" id="MJXc-Node-3"><span class="mjx-base"><span class="mjx-mi" id="MJXc-Node-4"><span class="mjx-char MJXc-TeX-math-I" style="padding-bottom: 0.28em;">v</span></span></span><span class="mjx-sub"><span class="mjx-mn" id="MJXc-Node-5"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em;">1</span></span></span></span></span></span></span>, <span class="mjx-chtml MathJax_CHTML" id="MathJax-Element-2-Frame" style="font-size: 129%;" tabindex="0"><span class="mjx-math" id="MJXc-Node-6"><span class="mjx-mrow" id="MJXc-Node-7"><span class="mjx-msubsup" id="MJXc-Node-8"><span class="mjx-base"><span class="mjx-mi" id="MJXc-Node-9"><span class="mjx-char MJXc-TeX-math-I" style="padding-bottom: 0.28em;">v</span></span></span><span class="mjx-sub"><span class="mjx-mn" id="MJXc-Node-10"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em;">2</span></span></span></span></span></span></span>, ..., <span class="mjx-chtml MathJax_CHTML" id="MathJax-Element-3-Frame" style="font-size: 129%;" tabindex="0"><span class="mjx-math" id="MJXc-Node-11"><span class="mjx-mrow" id="MJXc-Node-12"><span class="mjx-msubsup" id="MJXc-Node-13"><span class="mjx-base"><span class="mjx-mi" id="MJXc-Node-14"><span class="mjx-char MJXc-TeX-math-I" style="padding-bottom: 0.28em;">v</span></span></span><span class="mjx-sub"><span class="mjx-mi" id="MJXc-Node-15"><span class="mjx-char MJXc-TeX-math-I" style="padding-bottom: 0.28em;">n</span></span></span></span></span></span></span> be real numbers whose squares add up to 1.  Consider the <span class="mjx-chtml MathJax_CHTML" id="MathJax-Element-4-Frame" style="font-size: 129%;" tabindex="0"><span class="mjx-math" id="MJXc-Node-16"><span class="mjx-mrow" id="MJXc-Node-17"><span class="mjx-msubsup" id="MJXc-Node-18"><span class="mjx-base"><span class="mjx-mn" id="MJXc-Node-19"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em;">2</span></span></span><span class="mjx-sup" style="font-size: 70.7%; padding-left: 0px; vertical-align: 0.591em;"><span class="mjx-mi" id="MJXc-Node-20"><span class="mjx-char MJXc-TeX-math-I" style="padding-bottom: 0.28em;">n</span></span></span></span></span></span></span> signed sums of the form <span class="mjx-chtml MathJax_CHTML" id="MathJax-Element-5-Frame" style="font-size: 129%;" tabindex="0"><span class="mjx-math" id="MJXc-Node-21"><span class="mjx-mrow" id="MJXc-Node-22"><span class="mjx-mi" id="MJXc-Node-23"><span class="mjx-char MJXc-TeX-math-I" style="padding-bottom: 0.28em;">S</span></span><span class="mjx-mo MJXc-space3" id="MJXc-Node-24"><span class="mjx-char MJXc-TeX-main-R">=</span></span><span class="mjx-mo MJXc-space3" id="MJXc-Node-25"><span class="mjx-char MJXc-TeX-size1-R">∑</span></span><span class="mjx-mo MJXc-space1" id="MJXc-Node-26"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em;">±</span></span><span class="mjx-msubsup" id="MJXc-Node-27"><span class="mjx-base"><span class="mjx-mi" id="MJXc-Node-28"><span class="mjx-char MJXc-TeX-math-I" style="padding-bottom: 0.28em;">v</span></span></span><span class="mjx-sub"><span class="mjx-mi" id="MJXc-Node-29"><span class="mjx-char MJXc-TeX-math-I" style="padding-bottom: 0.28em;">i</span></span></span></span></span></span></span>.  Can there be more signed sums whose value is greater than 1 then those whose value  is at most 1? <a href="https://holzman.technion.ac.il/files/2012/09/combsigns.pdf">Holzman and Kleitman (1992) </a>proved that at least 3/8 of these sums satisfy <span class="mjx-chtml MathJax_CHTML" id="MathJax-Element-6-Frame" style="font-size: 129%;" tabindex="0"><span class="mjx-math" id="MJXc-Node-30"><span class="mjx-mrow" id="MJXc-Node-31"><span class="mjx-texatom" id="MJXc-Node-32"><span class="mjx-mrow" id="MJXc-Node-33"><span class="mjx-mo" id="MJXc-Node-34"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mi" id="MJXc-Node-35"><span class="mjx-char MJXc-TeX-math-I" style="padding-bottom: 0.28em;">S</span></span><span class="mjx-texatom" id="MJXc-Node-36"><span class="mjx-mrow" id="MJXc-Node-37"><span class="mjx-mo" id="MJXc-Node-38"><span class="mjx-char MJXc-TeX-main-R">|</span></span></span></span><span class="mjx-mo MJXc-space3" id="MJXc-Node-39"><span class="mjx-char MJXc-TeX-main-R">≤</span></span><span class="mjx-mn MJXc-space3" id="MJXc-Node-40"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.39em;">1</span></span></span></span></span>.  In his talk, Ravi showed us the main ideas Holzman and he used to improve the bound to  13/32.<br/><br/>Computational effects model the interaction of computer programs with their environment. In his talk, <a href="https://dcc.fceia.unr.edu.ar/~erivas/"> Exequiel Rivas</a>taught us how <a href="https://en.wikipedia.org/wiki/Monad_(category_theory)">monads</a>can be used to capture computational effects (a research programme that started with <a href="https://core.ac.uk/download/pdf/21173011.pdf">Moggi's award-winning work</a>), and then, discussed some attempts to incorporate merging operations in the monadic picture.<br/><br/>Two of the short talks were given by Henning A. Úlfarsson and Elli  Anastasiadi. Henning described the work of his group on  a tool, called  the CombSpecSearcher, that automates the methods used by  combinatorialists to prove some of their theorems, The tool is able to  prove results featured in dozens of research papers. Watch this space for updates on its  development and for its successes!<br/><br/>Elli Anastasiadi, a PhD student who is already playing an important role  for the centre, gave a clear seven-minute introduction to <a href="https://people.csail.mit.edu/virgi/ipec-survey.pdf">fine-grained complexity</a> and to the notion of <a href="https://en.wikipedia.org/wiki/Fine-grained_reduction">fine-grained reduction</a>.<br/><br/>The 2019 Theory Day was well attended, at least by the standards of a  TCS event in Iceland. If all goes well, we'll be back next year.</div>
    </content>
    <updated>2019-05-12T22:37:00Z</updated>
    <published>2019-05-12T22:37:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2019-05-13T09:16:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/069</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/069" rel="alternate" type="text/html"/>
    <title>TR19-069 |  Bounded-depth Frege complexity of Tseitin formulas for all graphs | 

	Artur Riazanov, 

	Dmitry Itsykson, 

	Nicola  Galesi, 

	Anastasia Sofronova</title>
    <summary>We prove that there is a constant $K$ such that \emph{Tseitin} formulas for an undirected graph $G$ requires proofs of 
size $2^{\mathrm{tw}(G)^{\Omega(1/d)}}$ in depth-$d$ Frege systems for $d&lt;\frac{K \log n}{\log \log n}$, where $\tw(G)$ is the treewidth of $G$. This extends  H{\aa}stad recent lower bound for the grid graph to any graph. Furthermore, we prove tightness of our bound up to a multiplicative constant in the top exponent. 
Namely, we show that if a Tseitin formula for a graph $G$ has size $s$, then for all large enough $d$, it has a depth-$d$ Frege proof of size $2^{\mathrm{tw}(G)^{\O(1/d)}} \mathrm{poly}(s)$. 
Through this result we settle the question posed by M. Alekhnovich and A. Razborov  of showing that the class of Tseitin formulas is quasi-automatizable for resolution.</summary>
    <updated>2019-05-12T09:21:17Z</updated>
    <published>2019-05-12T09:21:17Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-16T01:22:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15846</id>
    <link href="https://rjlipton.wordpress.com/2019/05/10/making-elections-safe/" rel="alternate" type="text/html"/>
    <title>Making Elections Safe</title>
    <summary>A new proof that MAJORITY is not in . [ Rich DeMillo ] Rich DeMillo is a strong leader, a famous researcher, and a long-time best friend. Proof: He was the first CTO at HP and was the Dean of Computing at Georgia Tech; He helped created mutation a powerful software testing method and did […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A new proof that MAJORITY is not in <img alt="{\mathsf{AC}^{0}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAC%7D%5E%7B0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{AC}^{0}}"/>.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/10/making-elections-safe/unknown-121/" rel="attachment wp-att-15848"><img alt="" class="alignright size-full wp-image-15848" src="https://rjlipton.files.wordpress.com/2019/05/unknown.jpeg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Rich DeMillo ]</font></td>
</tr>
</tbody>
</table>
<p>
Rich DeMillo is a strong leader, a famous researcher, and a long-time best friend. <i>Proof</i>: He was the first CTO at HP and was the Dean of Computing at Georgia Tech; He helped created <a href="https://en.wikipedia.org/wiki/Mutation_testing">mutation</a> a powerful software testing method and did seminal work in complexity theory. The last is clear.</p>
<p>
Today I want to talk about his recent work on voting systems. </p>
<p>
The 2020 election is over a year away, yet it is on our collective minds. People voice concerns everyday on social media, on TV, on cable, in print, everywhere. Their concerns are that our next national election will be compromised. Rich has turned his concern into activism: he is working hard to make elections trusted in general and the 2020 election in particular. </p>
<p>
Rich is scheduled to give a talk this coming Monday, May 13, at Georgia Tech. I wish I could be there, but cannot. I do plan to watch the video of his presentation—see <a href="https://www.cc.gatech.edu/calendar/day/2019/05/13/15151">here</a>. The talk is based on his recent <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3375755">paper</a> joint with Andrew Appel and Philip Stark titled, “Ballot-Marking devices (BMDs) cannot assure the will of the voters.” As an aside, their paper (ADS) has already generated measurable interest—it’s been downloaded over <img alt="{300}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B300%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{300}"/> times and viewed over <img alt="{5,000}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B5%2C000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{5,000}"/> times.</p>
<p>
</p><p/><h2> What Makes A Good Election? </h2><p/>
<p/><p>
There are several criterion that a “good” election should have. </p>
<ul>
<li>
<i>During the voting</i>: Only eligible voters can vote. <p/>
</li><li>
<i>During the tabulation</i>: Each voter gets exactly one vote. <p/>
</li><li>
<i>After the election</i>: Each vote remains secret.
</li></ul>
<p>
Rich says in his talk abstract: </p>
<blockquote><p><b> </b> <em> Many people believe that, in an Internet-enabled world, secure, safe voting should be easy to achieve. For example, using known cryptographically secure protocols (maybe even blockchains), a secure website might be developed to relieve voters of the burden of driving to a polling place on election day. </em>
</p></blockquote>
<p/><p>
This belief is wrong. Elections are hard—impossible?—to safeguard. A U.S. national election is the union of about ten thousand local elections. Each has different rules and protocols, which makes safeguarding the national election difficult. </p>
<ul>
<li>
<i>During the voting</i>: Who votes is subject to human decisions? People must prove they are eligible voters. Even computer identification is subject to bias. <p/>
</li><li>
<i>During the tabulation</i>: Counting is usually a combination of human and computer systems. The former make errors and as do the latter. Computers can have bugs or can be hacked by adversaries. <p/>
</li><li>
<i>After the election</i>: Since records are usually kept of the voting, they must be safeguarded to avoid disclosing how someone voted.
</li></ul>
<p>
The last point is central. There must be a record of the votes to allow audits after the election is over. We must be able to audit and check that the tabulation was correct. This is the central question that Rich and his co-authors discuss. We will turn to this issue in a moment. </p>
<p>
Before that I note that keeping a vote secret is impossible in an absolute sense. Suppose that you vote “yes” in some district. After the election suppose that the count in that district is made public, as it usually is. Say <img alt="{61\%}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B61%5C%25%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{61\%}"/> of the votes were yes. Then clearly information is leaked about how you voted.</p>
<p>
</p><p/><h2> How Are Votes Recorded? </h2><p/>
<p/><p>
There are two main ways to record votes. One method is to have voters hand-mark their ballots, in the old-fashioned way. It is simple, cheap, and not 21st century. Hand-marked ballots can be read by automatic scanners, at least in principle. A difficulty is voters are human and may mark their ballots incorrectly. They may miss a box, or mark two boxes, or make some other mistake. What if the voter is instructed to:</p>
<blockquote><p><b> </b> <em> Select two of the following six choices. </em>
</p></blockquote>
<p/><p>
There can be other difficulties: Some voters may have special needs and may require instructions to be in a large type font, for example.</p>
<p>
Another recoding method is to have voters use a device to print their paper ballots. These are cleverly called <a href="https://en.wikipedia.org/wiki/Ballot_marking_device">Ballot Marking Devices</a> (BMDs). The name sounds slightly strange to me; there is an alternative name, electronic ballot markers (EBMs).</p>
<p>
The authors, ADS, argue that BMDs are dangerous. Such devices can fail, they argue, and not protect the election. The BMDs rely on software, complex special purpose software, and thus are subject to bugs, errors, mistakes, and to active attacks by adversaries. </p>
<p>
A BMD device takes input from a voter and then prints out a paper ballot. Often the ballot will contain a machine-readable bar code. This is so scanners can more easily read the paper ballots, later. The problem, the danger, is that most voters cannot tell if a bar code is correct or not. An attacker need only have the BMD confirm that you voted say “yes” and print a ballot that says “yes”. Then the attacker has the BMD cheat you by printing a bar code that says “no”. This is a nasty attack, which is hard to stop. The ADS team discusses this and related problems with BDMs. </p>
<p>
<a href="https://rjlipton.wordpress.com/2019/05/10/making-elections-safe/bar/" rel="attachment wp-att-15852"><img alt="" class="aligncenter size-medium wp-image-15852" height="147" src="https://rjlipton.files.wordpress.com/2019/05/bar.png?w=300&amp;h=147" width="300"/></a></p>
<p>
</p><p/><h2> Mathematics and Voting </h2><p/>
<p/><p>
Can complexity theory help us design better elections? Unclear. Can election theory help us understand complexity theory? Perhaps.</p>
<p>
</p><p/><h3> Elections Inform Complexity Theory </h3><p/>
<p>
</p><blockquote><p><b>Theorem 1</b> <em> The <i>Election Hardness Axiom</i> implies that the MAJORITY function cannot be computed by a polynomial size constant depth Boolean circuit of NOT, AND, and OR gates. </em>
</p></blockquote>
<p/><p>
That is, the MAJORITY function is not in <img alt="{\mathsf{AC}^{0}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAC%7D%5E%7B0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{AC}^{0}}"/>. What is the Election Hardness Axiom? It is the empirical fact that there is no practical way to compute who won an election. The MAJORITY function is the tabulation of votes: The outcome of an election is the same as computing the MAJORITY function of the votes—“yes” is a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> and “no” is a <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>.</p>
<p>
Okay we are kidding. But not completely. Suppose that MAJORITY function were in <img alt="{\mathsf{AC}^{0}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAC%7D%5E%7B0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{AC}^{0}}"/>. Then a series of decisions of the form:</p>
<blockquote><p><b> </b> <em> The tabulators have looked at the following ballots <img alt="{B_{1},\dots,B_{m}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB_%7B1%7D%2C%5Cdots%2CB_%7Bm%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B_{1},\dots,B_{m}}"/> and we agree that there is a “yes” vote in ballot <img alt="{B_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB_%7Bi%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B_{i}}"/>. </em>
</p></blockquote>
<p>
</p><p/><h3> Complexity Informs Elections </h3><p/>
<p/><p>
Rich, in his talk abstract, states that it is unlikely that crypto theory could be used to create trusted elections. His reason is voters will not trust elections that rely on crypto results. I agree. But I wonder if ideas from theory could be useful. Here are two high-level thoughts.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> There is a vast literature on computing in the presence of faults. Usually “faults” are thought to occur at the nano level: the faults are due to hiccups in electronics. What if the faults came from errors in the counting of votes? What if the faults were at the macro level? That is at the level of human decisions? Perhaps we will revisit this in the future.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> There is a vast literature on computing as a “game”. An election is usually viewed as being run by some trusted party. This could be replaced by assuming that the election is a game. Imagine two parties D and R. As the tabulation is performed D and R can challenge each other. They interact as in game. Could this help make the election trusted? Perhaps we will revisit this too in the future.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Can we elections be trusted? Can we formalize the connection between election and <img alt="{\mathsf{AC}^{0}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAC%7D%5E%7B0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{AC}^{0}}"/>? Could this connect be useful? Can theory help with future elections?</p>
<p/></font></font></div>
    </content>
    <updated>2019-05-10T12:35:36Z</updated>
    <published>2019-05-10T12:35:36Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Proofs"/>
    <category term="AC0"/>
    <category term="ballots"/>
    <category term="election"/>
    <category term="MAJORITY"/>
    <category term="trust"/>
    <category term="voters"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-16T01:22:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17447</id>
    <link href="https://gilkalai.wordpress.com/2019/05/10/sansation-in-the-morning-news-yaroslav-shitov-counterexamples-to-hedetniemis-conjecture/" rel="alternate" type="text/html"/>
    <title>A sensation in the morning news –  Yaroslav Shitov: Counterexamples to Hedetniemi’s conjecture.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Two days ago Nati Linial sent me an email entitled “A sensation in the morning news”. The link was to a new arXived paper by Yaroslav Shitov: Counterexamples to Hedetniemi’s conjecture. Hedetniemi’s 1966 conjecture asserts that if and are two … <a href="https://gilkalai.wordpress.com/2019/05/10/sansation-in-the-morning-news-yaroslav-shitov-counterexamples-to-hedetniemis-conjecture/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Two days ago Nati Linial sent me an email entitled “A sensation in the morning news”. The link was to a new arXived paper by Yaroslav Shitov:<a href="https://arxiv.org/abs/1905.02167"> Counterexamples to Hedetniemi’s conjecture</a>.</p>
<p><a href="https://en.wikipedia.org/wiki/Hedetniemi%27s_conjecture">Hedetniemi’s 1966 conjecture</a> asserts that if <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> and <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> are two graphs, then the chromatic number of their tensor product <img alt="G \times H" class="latex" src="https://s0.wp.com/latex.php?latex=G+%5Ctimes+H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G \times H"/> equals the minimum of their individual chromatic numbers.  Here, the vertex set of <img alt="G \times H" class="latex" src="https://s0.wp.com/latex.php?latex=G+%5Ctimes+H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G \times H"/> is the Cartesian product of <img alt="V(G)" class="latex" src="https://s0.wp.com/latex.php?latex=V%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V(G)"/> and <img alt="V(H)" class="latex" src="https://s0.wp.com/latex.php?latex=V%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V(H)"/> and two vertices <img alt="(g_1,h_1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28g_1%2Ch_1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(g_1,h_1)"/> and <img alt="(g_2,h_2)" class="latex" src="https://s0.wp.com/latex.php?latex=%28g_2%2Ch_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(g_2,h_2)"/> are adjacent if <img alt="g_1" class="latex" src="https://s0.wp.com/latex.php?latex=g_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_1"/> is adjacent to <img alt="g_2" class="latex" src="https://s0.wp.com/latex.php?latex=g_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_2"/> and  <img alt="h_1" class="latex" src="https://s0.wp.com/latex.php?latex=h_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_1"/> is adjacent to <img alt="h_2" class="latex" src="https://s0.wp.com/latex.php?latex=h_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_2"/>. (mistake corrected.) Every coloring of <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> induces a coloring  of <img alt="G \times H" class="latex" src="https://s0.wp.com/latex.php?latex=G+%5Ctimes+H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G \times H"/>, and so is every coloring of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/>.  Therefore, <img alt="\chi (G \times H) \le \min (\chi (G), \chi (H))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi+%28G+%5Ctimes+H%29+%5Cle+%5Cmin+%28%5Cchi+%28G%29%2C+%5Cchi+%28H%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi (G \times H) \le \min (\chi (G), \chi (H))"/>. Hedetniemi conjectured that equality always hold and this is now refuted by  by Yaroslav Shitov.</p>
<p>The example and the entire proof are quite short (the entire paper is less than 3 pages; It is a bit densely-written).</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/js.png"><img alt="" class="alignnone size-full wp-image-17450" src="https://gilkalai.files.wordpress.com/2019/05/js.png?w=640"/></a></p>
<p><span style="color: #ff0000;"><strong> Yaroslav Shitov </strong></span></p>
<p>To tell you what the construction is, I need two important definitions.  The first  is the notion of the exponential graph <img alt="{\cal E}_c(H)" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+E%7D_c%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{\cal E}_c(H)"/>.</p>
<p>The exponential graph  <img alt="{\cal E}_c(H)" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+E%7D_c%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{\cal E}_c(H)"/> arose in the study of Hedetniemi’s conjecture in a 1985 paper by El-Zahar and Sauer. The vertices of <img alt="{\cal E}_c(H)" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+E%7D_c%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{\cal E}_c(H)"/> are all maps from <img alt="V(H)" class="latex" src="https://s0.wp.com/latex.php?latex=V%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V(H)"/> to <img alt="\{1,2,\dots,c\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C2%2C%5Cdots%2Cc%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{1,2,\dots,c\}"/>. Two maps <img alt="\phi, \psi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%2C+%5Cpsi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi, \psi"/> are adjacent if whenever <img alt="v,u" class="latex" src="https://s0.wp.com/latex.php?latex=v%2Cu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v,u"/> are adjacent vertices of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> then <img alt="\phi(u) \ne \psi (v)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28u%29+%5Cne+%5Cpsi+%28v%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(u) \ne \psi (v)"/>.  <a href="https://link.springer.com/article/10.1007/BF02579374">El-Zahar and Sauer showed</a> that importance of the case  that <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> is a graph and <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is an exponential graph of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> for Hedetniemi’s conjecture. (The entire conjecture reduces to this case.) It is thus crucial to study  coloring of exponential graphs which is the subject of the three claims of Section 1 of Shitov’s paper.</p>
<p>The second definition is another important notion of product of graphs: <a href="https://en.wikipedia.org/wiki/Strong_product_of_graphs">The strong product <i>G</i> ⊠ <i>H </i>of two graphs <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> and <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>.</a> The set of vertices is again the Cartesian product of the two sets of vertices. This time,  <img alt="(g_1,h_1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28g_1%2Ch_1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(g_1,h_1)"/> and <img alt="(g_2,h_2)" class="latex" src="https://s0.wp.com/latex.php?latex=%28g_2%2Ch_2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(g_2,h_2)"/> are adjacent in <i>G</i> ⊠ <i>H</i> if either</p>
<p>(a) <img alt="g_1" class="latex" src="https://s0.wp.com/latex.php?latex=g_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_1"/> is adjacent to <img alt="g_2" class="latex" src="https://s0.wp.com/latex.php?latex=g_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_2"/> and  <img alt="h_1" class="latex" src="https://s0.wp.com/latex.php?latex=h_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_1"/> is adjacent to <img alt="h_2" class="latex" src="https://s0.wp.com/latex.php?latex=h_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_2"/></p>
<p>OR</p>
<p>(b) <img alt="g_1" class="latex" src="https://s0.wp.com/latex.php?latex=g_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_1"/> is adjacent to <img alt="g_2" class="latex" src="https://s0.wp.com/latex.php?latex=g_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_2"/> and  <img alt="h_1 = h_2" class="latex" src="https://s0.wp.com/latex.php?latex=h_1+%3D+h_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_1 = h_2"/> or <img alt="g_1 =g_2" class="latex" src="https://s0.wp.com/latex.php?latex=g_1+%3Dg_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_1 =g_2"/> and  <img alt="h_1" class="latex" src="https://s0.wp.com/latex.php?latex=h_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_1"/> is adjacent to <img alt="h_2" class="latex" src="https://s0.wp.com/latex.php?latex=h_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_2"/></p>
<p>(The edges of condition (a) are the edges of the <em>tensor product</em> of the two graphs and the edges of condition (b) are the edges of the <em>Cartesian product</em> of the two graphs.)</p>
<p>For Shitov’s counterexample given in Section 2 of his paper, <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> is the strong product of a graph <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L"/> with girth at least 10 and fractional chromatic number at least 4.1 with a large clique of size <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/>. The second graph <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is the exponential graph <img alt="{\cal E}_c(H)" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+E%7D_c%28H%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{\cal E}_c(H)"/>. Put <img alt="c =\lceil 4.1q \rceil" class="latex" src="https://s0.wp.com/latex.php?latex=c+%3D%5Clceil+4.1q+%5Crceil&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c =\lceil 4.1q \rceil"/>.  Shitov shows that when <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/> is sufficiently large then  the chromatic number of both <img alt="H,G" class="latex" src="https://s0.wp.com/latex.php?latex=H%2CG&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H,G"/> is <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/>,  but the chromatic number of their tensor product is smaller than <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/>.</p>
<p><span style="color: #ff0000;"> (Have a look also at Yaroslav’s other <a href="https://arxiv.org/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Shitov&amp;terms-0-field=author&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first">arXived papers</a>! )</span></p>
<h3>Finite and infinite combinatorics</h3>
<p>Let me make one more remark. (See the <a href="https://en.wikipedia.org/wiki/Hedetniemi%27s_conjecture">Wikipedea article</a>.) The infinite version of Hedetniemi’s conjecture was known to be false.  Hajnal (1985) gave an example of two infinite graphs, each requiring an uncountable number of colors, such that their product can be colored with only countably many colors. Rinot (2013) proved that in the <a href="https://en.wikipedia.org/wiki/Constructible_universe" title="Constructible universe">constructible universe</a>, for every infinite cardinal <span class="mwe-math-element"><img alt="\kappa " class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54ddec2e922c5caea4e47d04feef86e782dc8e6d"/></span>, there exist a pair of graphs of chromatic number greater than <span class="mwe-math-element"><img alt="\kappa " class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54ddec2e922c5caea4e47d04feef86e782dc8e6d"/></span>, such that their product can still be colored with only countably many colors. (<a href="https://arxiv.org/pdf/1307.6841.pdf">Here is the paper</a>.) Is there a relation between the finite case and the infinite case? (Both theories are quite exciting but direct connections are rare. A rare statement where the same proof applies for the finite and infinite case is the inequality <img alt="2^n&gt;n" class="latex" src="https://s0.wp.com/latex.php?latex=2%5En%3En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^n&gt;n"/>.</p>
<h3>More information</h3>
<p>Here is a link to a survey article by Claude Tardif, (2008), <a class="external text" href="http://www.mast.queensu.ca/~ctardif/articles/gtn5406rp.pdf" rel="nofollow">“Hedetniemi’s conjecture, 40 years later”</a> .</p>
<p>A few more thing worth knowing:</p>
<p>1) The weak version of the conjecture that asserts that If <img alt="\chi (G)= \chi (H))=n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi+%28G%29%3D+%5Cchi+%28H%29%29%3Dn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi (G)= \chi (H))=n"/>, then <img alt="\chi (G \times H) \ge f(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi+%28G+%5Ctimes+H%29+%5Cge+f%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\chi (G \times H) \ge f(n)"/> where <img alt="f(n)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(n)"/> tends to infinity with <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> is still open.</p>
<p>2)  Xuding Zhu proved in 2011 that <a href="https://www.sciencedirect.com/science/article/pii/S0195669811000552">the fractional version of the conjecture is correct</a>,</p>
<p>3) The directed version of the conjecture was known to be false (Poljak and Rodl, 1981).</p>
<p>4) The conjecture is part of a rich and beautiful theory of graph homomorphisms (and the category of graphs) that I hope to come back to in another post.</p></div>
    </content>
    <updated>2019-05-10T12:32:56Z</updated>
    <published>2019-05-10T12:32:56Z</published>
    <category term="Combinatorics"/>
    <category term="Open problems"/>
    <category term="Updates"/>
    <category term="Hedetniemi's conjecture"/>
    <category term="Yaroslav Shitov"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-05-16T01:22:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/05/10/postdoc-in-drone-safety-at-department-of-electronic-systems-aalborg-university-apply-by-june-6-2019/</id>
    <link href="https://cstheory-jobs.org/2019/05/10/postdoc-in-drone-safety-at-department-of-electronic-systems-aalborg-university-apply-by-june-6-2019/" rel="alternate" type="text/html"/>
    <title>Postdoc in drone safety at Department of Electronic Systems, Aalborg University (apply by June 6, 2019)</title>
    <summary>At the Faculty of IT and Design, Department of Electronic Systems, a position as postdoc in the research project SafeEYE is open for appointment from July 1st, 2019, or soon hereafter, and until May 31, 2021. Website: https://www.stillinger.aau.dk/vis-stilling/?vacancy=1042671 Email: alc@es.aau.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>At the Faculty of IT and Design, Department of Electronic Systems, a position as postdoc in the research project SafeEYE is open for appointment from July 1st, 2019, or soon hereafter, and until May 31, 2021.</p>
<p>Website: <a href="https://www.stillinger.aau.dk/vis-stilling/?vacancy=1042671">https://www.stillinger.aau.dk/vis-stilling/?vacancy=1042671</a><br/>
Email: alc@es.aau.dk</p></div>
    </content>
    <updated>2019-05-10T07:42:15Z</updated>
    <published>2019-05-10T07:42:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-05-16T01:22:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/05/10/theory-and-practice-of-differential-privacy-part-of-ccs-2019/</id>
    <link href="https://cstheory-events.org/2019/05/10/theory-and-practice-of-differential-privacy-part-of-ccs-2019/" rel="alternate" type="text/html"/>
    <title>Theory and Practice of Differential Privacy (part of CCS 2019)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">November 11, 2019 London, UK https://tpdp.cse.buffalo.edu/2019/ Submission deadline: June 21, 2019 The fifth annual workshop on the Theory and Practice of Differential Privacy will be held as a part of CCS 2019 on November 11. We seek contributions from different research areas of computer science and statistics. Authors are invited to submit a short abstract … <a class="more-link" href="https://cstheory-events.org/2019/05/10/theory-and-practice-of-differential-privacy-part-of-ccs-2019/">Continue reading <span class="screen-reader-text">Theory and Practice of Differential Privacy (part of CCS 2019)</span></a></div>
    </summary>
    <updated>2019-05-10T03:02:58Z</updated>
    <published>2019-05-10T03:02:58Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-05-16T01:23:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/068</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/068" rel="alternate" type="text/html"/>
    <title>TR19-068 |  LARGE CLIQUE IS HARD ON AVERAGE FOR RESOLUTION | 

	Shuo Pang</title>
    <summary>We prove resolution lower bounds for $k$-Clique on the Erdos-Renyi random graph $G(n,n^{-{2\xi}\over{k-1}})$ (where $\xi&gt;1$ is constant). First we show for $k=n^{c_0}$, $c_0\in(0,1/3)$, an $\exp({\Omega(n^{(1-\epsilon)c_0})})$ average lower bound on resolution where $\epsilon$ is arbitrary constant. 

We then propose the model of $a$-irregular resolution. Extended from regular resolution, this model is interesting in that the power of general-over-regular resolution from all {\it known} exponential separations is below it. We prove an $n^{\Omega(k)}$ average lower bound of $k$-Clique for this model, for {\it any} $k&lt;n^{1/3-\Omega(1)}$.</summary>
    <updated>2019-05-09T16:39:36Z</updated>
    <published>2019-05-09T16:39:36Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-16T01:22:02Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-722419176401069744</id>
    <link href="https://blog.computationalcomplexity.org/feeds/722419176401069744/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/multiple-provers.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/722419176401069744" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/722419176401069744" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/multiple-provers.html" rel="alternate" type="text/html"/>
    <title>Multiple Provers</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Just over thirty years ago on May 5, 1989, I defended my PhD Thesis <a href="https://lance.fortnow.com/papers/files/thesis.pdf">Complexity-Theoretic Aspects of Interactive Proof Systems</a>. It's starts off with a parable for interactive proof systems.<br/>
<blockquote class="tr_bq">
Victor, a venture capitalist, had everything a man could desire: money, women and power.
But he felt something missing. He decided he lacked knowledge. So Victor packed up his
bags and headed to the Himalayas in search of ultimate truths.
The natives pointed Victor to a tall mountain and mentioned rumors of a great man full of
wisdom. Victor, who smartly brought some climbing equipment, tackled the mountain until
he reached a small cave near the summit. Victor found the great Pulu, grand guru of all that
is known. Victor inquired to some ultimate truths and Pulu responded,
<i>I will teach you but you must not trust my words</i>.
Victor agreed and found he learned much even though he had to verify all the sayings
of the great Pulu. Victor though lacked complete happiness and he asked if he could learn
knowledge beyond what he could learn in this manner. The grand guru replied,
<i>You may ask and I will answer</i>.
Victor pondered this idea for a minute and said, "Since you know all that is known, why can you not predict my questions?" A silence reigned over the mountain for a short while until the guru  finally spoke,
<i>You must use other implements, symbols of your past life</i>.
Victor thought for a while and reached into his backpack and brought out some spare
change he had unwittingly carried with him. Even the great Pulu can not predict the flip of
a coin. He started flipping the coins to ask the guru and wondered what can I learn now?</blockquote>
Without the coins, one gets the complexity class NP. My thesis didn't answer the last question, but by the end of the year, <a href="https://doi.org/10.1109/FSCS.1990.89519">Shamir</a> building on work of <a href="https://doi.org/10.1145/146585.146605">Lund, Fortnow, Karloff and Nisan</a> showed this class IP was equal to PSPACE, the problems we could solve in a polynomial amount of memory.<br/>
<br/>
Part of my thesis explored the class MIP where we had multiple Pulus (provers) on different mountain tops unable to communicate. The news was disappointing, we failed to get a PSPACE upper bound for MIP, only NEXP (nondeterministic exponential time) and our proof that two provers sufficed relied on a bad assumption on how errors get reduced when you run multiple protocols in parallel. Later Babai, Lund and myself showed <a href="http://doi.org/10.1007/BF01200056">MIP = NEXP</a> and Ran Raz <a href="https://doi.org/10.1137/S0097539795280895">showed</a> parallel repetition does reduce the error sufficiently.<br/>
<br/>
Back in the 80's we didn't even imagine the possibility that the Pulus had shared entangled quantum bits. Does the entanglement allow the provers to cheat or can the entanglement allow them to prove more things? Turns out to be much more, as a <a href="https://arxiv.org/abs/1904.05870">new result</a> by Anand Natarajan and John Wright shows that MIP*, MIP with classical communication, classical verifier and two provers with previously entangled quantum bits, can compute everything in NEEXP, nondeterministic double exponential time. This is only a lower bound for MIP*, possibly one can do even more.<br/>
<br/>
Neat to see my three-decade old thesis explored ideas that people are still thinking about today.</div>
    </content>
    <updated>2019-05-09T12:31:00Z</updated>
    <published>2019-05-09T12:31:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-15T18:02:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=353</id>
    <link href="https://tcsplus.wordpress.com/2019/05/08/tcs-talk-wednesday-may-15th-ewin-tang-university-of-washington/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 15th — Ewin Tang, University of Washington</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 15th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Ewin Tang from University of Washington will tell us about “Quantum-inspired classical linear algebra algorithms: why and how?” (abstract below). Please make sure you reserve a spot for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 15th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong><a href="https://ewintang.com/" rel="noopener" target="_blank">Ewin Tang</a></strong> from University of Washington will tell us about “<em>Quantum-inspired classical linear algebra algorithms: why and how?</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Over the past ten years, the field of quantum machine learning (QML) has produced many polylogarithmic-time procedures for linear algebra routines, assuming certain “state preparation” assumptions. Though such algorithms are formally incomparable with classical computing, a recent line of work uses an analogous classical model of computation as an effective point of comparison to reveal speedups (or lack thereof) gained by QML. The resulting “dequantized” algorithms assume sampling access to input to speed up runtimes to polylogarithmic in input size.</p>
<p>In this talk, we will discuss the motivation behind this model and its relation to existing randomized linear algebra literature. Then, we will delve into an example quantum-inspired algorithm: Gilyen, Lloyd, and Tang’s algorithm for low-rank matrix inversion. This dequantizes a variant of Harrow, Hassidim, and Lloyd’s matrix inversion algorithm, a seminal work in QML. Finally, we will consider the implications of this work on exponential speedups in QML. No background of quantum computing is assumed for this talk.</p></blockquote></div>
    </content>
    <updated>2019-05-08T22:26:55Z</updated>
    <published>2019-05-08T22:26:55Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-05-16T01:23:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/067</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/067" rel="alternate" type="text/html"/>
    <title>TR19-067 |  Sign rank vs Discrepancy | 

	Hamed Hatami, 

	Kaave Hosseini, 

	Shachar Lovett</title>
    <summary>Sign-rank and discrepancy are two central notions in communication complexity. The seminal work of  Babai, Frankl, and Simon from 1986  initiated an active line of research that investigates  the gap between these two notions.
In this article, we establish the strongest possible separation  by constructing a Boolean matrix whose sign-rank is only $3$, and yet its discrepancy is  $2^{-\Omega(n)}$. We note that every matrix of sign-rank $2$ has discrepancy $n^{-O(1)}$.
Our result in particular implies that there are Boolean functions with $O(1)$ unbounded error randomized communication complexity while having $\Omega(n)$ weakly unbounded error randomized communication complexity.</summary>
    <updated>2019-05-07T09:52:08Z</updated>
    <published>2019-05-07T09:52:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-16T01:22:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15840</id>
    <link href="https://rjlipton.wordpress.com/2019/05/06/the-network-coding-conjecture-is-powerful/" rel="alternate" type="text/html"/>
    <title>The Network Coding Conjecture Is Powerful</title>
    <summary>More hard Boolean functions Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen (AFKL) have a recent paper which we just discussed. Today Ken and I will update our discussion. Their paper assumes the network coding conjecture (NCC) and proves a lower bound on the Boolean complexity of integer multiplication. The main result of AFKL […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>More hard Boolean functions</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2019/05/akklshifted.jpg"><img alt="" class="alignright wp-image-15841" height="162" src="https://rjlipton.files.wordpress.com/2019/05/akklshifted.jpg?w=197&amp;h=162" width="197"/></a></p>
<p>
Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen (AFKL) have a recent <a href="https://arxiv.org/abs/1902.10935">paper</a> which we just <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">discussed</a>. </p>
<p>
Today Ken and I will update our discussion. </p>
<p>
Their paper assumes the network coding conjecture (NCC) and proves a lower bound on the Boolean complexity of integer multiplication. The main result of AFKL is:</p>
<blockquote><p><b>Theorem 1</b> <em> If the NCC is true, then every Boolean circuit that computes the <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/> function has size <img alt="{\Omega(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Omega(n \log n)}"/>. </em>
</p></blockquote>
<p/><p>
The <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/> function is: Given an <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-bit number <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and a number <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> so that <img alt="{1 \le k \le n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+k+%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \le k \le n}"/>, compute the <img alt="{2n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2n}"/>-bit product of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> by <img alt="{2^{k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{k}}"/>: 	</p>
<p align="center"><img alt="\displaystyle  x \times 2^{k}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%5Ctimes+2%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x \times 2^{k}. "/></p>
<p>This is a special case of the integer multiplication problem. In symbols it maps <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> to <img alt="{0^k x 0^{n-k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5Ek+x+0%5E%7Bn-k%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0^k x 0^{n-k}}"/>, as in our photo above. </p>
<p>
Our point, however, is not about integer multiplication. Nor even about NCC—no knowledge of it will be needed today, so read on even if you are not aware of NCC. No. Our point is that a whole lot of other Boolean functions would inherit the same <img alt="{\Omega(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(n \log n)}"/> circuit lower bound as <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/>. And several aspects of that seem troubling.<span id="more-15840"/></p>
<p>
</p><p/><h2> Some Worry </h2><p/>
<p/><p>
We are impressed by the AFKL paper but also worried. Proving a super-linear lower bound in the unrestricted Boolean complexity model has long been considered a difficult problem. Maybe a hopeless problem. Yes they are proving it not for a single-output function; they are proving it for a multiple-output function. Still I thought that it seems too good to be correct. Even worse, assuming NCC they also resolve other open problems in complexity theory. I am worried.</p>
<p>
What we suggest is to catalog and study the consequences of their results. If we find that their results lead to a contradiction, then there was something to be worried about. Or perhaps it would mean that NCC is false. If we find no contradiction, then everything we discover is also a consequence of NCC. Either way we learn more.</p>
<p>
</p><p/><h2> AFKL Functions </h2><p/>
<p/><p>
Let’s call a Boolean function an <i>AFKL function</i> provided it has Boolean circuit complexity <img alt="{\Omega(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(n \log n)}"/> if the NCC is true. Thanks to AFKL, we now know that integer multiplication is an AFKL function. I started to think about: What functions are in this class? Here are some examples: </p>
<ul>
<li>
Integer multiplication <p/>
</li><li>
Integer multiplication by a power of <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> <p/>
</li><li>
<img alt="{\mathsf{FLIP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}}"/> <p/>
</li><li>
Discrete convolution <p/>
</li><li>
Sparse convolution
</li></ul>
<p>
We describe the last three next. We show they have linear size-preserving reductions from the <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/> function.</p>
<p>
</p><p/><h2> The Flip Function </h2><p/>
<p/><p>
Define <img alt="{\mathsf{FLIP}_{n}: \{0,1\}^{n} \rightarrow \{0,1\}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%3A+%5C%7B0%2C1%5C%7D%5E%7Bn%7D+%5Crightarrow+%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}_{n}: \{0,1\}^{n} \rightarrow \{0,1\}^{n}}"/> by </p>
<p align="center"><img alt="\displaystyle  \mathsf{FLIP}_{n}(0^{k}1\alpha)= 1\alpha 0^{k}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BFLIP%7D_%7Bn%7D%280%5E%7Bk%7D1%5Calpha%29%3D+1%5Calpha+0%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{FLIP}_{n}(0^{k}1\alpha)= 1\alpha 0^{k}. "/></p>
<p>for <img alt="{k \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k \ge 1}"/>. For any input not of this form, let <img alt="{\mathsf{FLIP}_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}_{n}}"/> be <img alt="{0^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0^{n}}"/>.</p>
<blockquote><p><b>Theorem 2</b> <em> The Boolean function <img alt="{\mathsf{FLIP}_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}_{n}}"/> is an AFKL function. </em>
</p></blockquote>
<p/><p>
<em>Proof:</em>  Let <img alt="{x,k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Ck%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,k}"/> be the input to <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/> where <img alt="{|x| = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cx%7C+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|x| = n}"/> and <img alt="{k \leq n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k \leq n}"/> in binary. In linear size we can test <img alt="{k = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = 0}"/>, when there is nothing to do, so presume <img alt="{k \geq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k \geq 1}"/>. The first step is to create </p>
<p align="center"><img alt="\displaystyle  0^{n-k}10^{k-1}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0%5E%7Bn-k%7D10%5E%7Bk-1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  0^{n-k}10^{k-1}. "/></p>
<p>This is just binary-to-unary conversion and has linear-size circuits—as in multiplex decoding and as remarked by AFKL. This becomes the first <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits of an application of <img alt="{\mathsf{FLIP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}}"/> to the <img alt="{2n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2n}"/>-bit string </p>
<p align="center"><img alt="\displaystyle  0^{n-k}10^{k-1} x. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0%5E%7Bn-k%7D10%5E%7Bk-1%7D+x.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  0^{n-k}10^{k-1} x. "/></p>
<p>It yields </p>
<p align="center"><img alt="\displaystyle  10^{k-1} x 0^{n-k}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++10%5E%7Bk-1%7D+x+0%5E%7Bn-k%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  10^{k-1} x 0^{n-k}. "/></p>
<p>Changing the first bit to <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> then leaves the desired output of <img alt="{\mathsf{shift}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bshift%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{shift}}"/>. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
The point is that <img alt="{\mathsf{FLIP}_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}_{n}}"/> is a super-simple function. It just moves the initial block of <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>‘s in a string to the end. It is amazing that this function should have only non-linear, indeed <img alt="{\Omega(n\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(n\log n)}"/>-sized, circuits. </p>
<p>
This also means that Ken’s <a href="https://blog.computationalcomplexity.org/2007/07/concrete-open-problem.html">function</a>, which takes <img alt="{x \in \{0,1,2\}^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%5C%7B0%2C1%2C2%5C%7D%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x \in \{0,1,2\}^*}"/> and moves all the <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>s to the end of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, is hard even in the special cases where all the <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>‘s are at the front. What’s strange is that Ken proves his function equivalent to another special case where <img alt="{|x|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cx%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|x|}"/> is even and exactly half the characters are <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. This latter case is one in which <img alt="{\mathsf{FLIP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}}"/> is easy, but the two cases are separate. All this is touch-and-go enough to compound our “worry.”</p>
<p>
</p><p/><h2> The Sparse Convolution Function </h2><p/>
<p/><p>
The following is also an AFKL function. 	</p>
<p align="center"><img alt="\displaystyle  y_{l} = \bigvee_{i=1}^{n-l} w_{i} \wedge x_{i+l}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bl%7D+%3D+%5Cbigvee_%7Bi%3D1%7D%5E%7Bn-l%7D+w_%7Bi%7D+%5Cwedge+x_%7Bi%2Bl%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{l} = \bigvee_{i=1}^{n-l} w_{i} \wedge x_{i+l}, "/></p>
<p>for <img alt="{l=1,\dots,n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bl%3D1%2C%5Cdots%2Cn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{l=1,\dots,n}"/> where an empty OR is defined to be <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. This can even further be restricted to the case where exactly one of the <img alt="{w_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_{i}}"/> are <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> and the rest are <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. Call this the <i>sparse convolution function</i>.</p>
<blockquote><p><b>Theorem 3</b> <em> The sparse convolution is a monotone AFKL function. </em>
</p></blockquote>
<p/><p>
<em>Proof:</em>  We will give a sketch of why this is true. Define 	</p>
<p align="center"><img alt="\displaystyle  y_{l} = \bigvee_{i = 1}^{n-l} \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1} \wedge x_{i+l}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bl%7D+%3D+%5Cbigvee_%7Bi+%3D+1%7D%5E%7Bn-l%7D+%5Cbar%7Bx%7D_%7B1%7D+%5Cwedge+%5Ccdots+%5Cwedge+%5Cbar%7Bx%7D_%7Bi%7D+%5Cwedge+x_%7Bi%2B1%7D+%5Cwedge+x_%7Bi%2Bl%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{l} = \bigvee_{i = 1}^{n-l} \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1} \wedge x_{i+l}. "/></p>
<p>It is not hard to show that this yields the FLIP function. We can reduce computing it to a convolution of the <img alt="{x_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{i}}"/>‘s and <img alt="{\Gamma(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Gamma(i)}"/> where 	</p>
<p align="center"><img alt="\displaystyle  \Gamma(i) = \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CGamma%28i%29+%3D+%5Cbar%7Bx%7D_%7B1%7D+%5Cwedge+%5Ccdots+%5Cwedge+%5Cbar%7Bx%7D_%7Bi%7D+%5Cwedge+x_%7Bi%2B1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Gamma(i) = \bar{x}_{1} \wedge \cdots \wedge \bar{x}_{i} \wedge x_{i+1}. "/></p>
<p>The key is to note that exactly one <img alt="{\Gamma(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Gamma(i)}"/> will be non-zero, and so the convolution is sparse. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
The sparse convolution function raises an interesting question: Are the methods for sparse FFT useful here? The lower bound for AFKL functions suggests that they are not applicable. </p>
<p>
</p><p/><h2> Is the NCC-Boolean Connection New? </h2><p/>
<p/><p>
The subtitle of our <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">post</a> marveled that a core-theory advance on circuits for multiplication had come via the practical side of throughput in computer networks. AFKL deserve plaudits for linking two communities. We should mention that one theoretician we both know well, Mark Braverman, with his students Sumegha Garg and Ariel Schvartzman at Princeton, <a href="https://arxiv.org/pdf/1608.06545.pdf">proved</a> a fact about NCC that is relevant to this discussion:</p>
<blockquote><p><b>Theorem 4</b> <em><a name="BGS"/> Either NCC is false, or bit-operations save a whole <img alt="{(\log n)^{\Omega(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7B%5COmega%281%29%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{(\log n)^{\Omega(1)}}"/> factor in the network size. </em>
</p></blockquote>
<p/><p>
Even this paper, however, does not address lower bounds on Boolean circuits. The only prior link between NCC and Boolean complexity is a 2007 <a href="https://www.combinatorics.org/ojs/index.php/eljc/article/view/v14i1r44">paper</a> by Søren Riis, which is cited by AFKL, and has a 2011 <a href="http://emis.impa.br/EMIS/journals/EJC/Volume_18/PDF/v18i1p192.pdf">followup</a> by Demetres Christofides and Klas Markström. The paper by Riis has a new “guessing game” on graphs and a demonstration that a lower-bound conjecture of Leslie Valiant needs to be rescued by dividing by a <img alt="{\log\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log\log n}"/> factor. Theorem <a href="https://rjlipton.wordpress.com/feed/#BGS">4</a>, however, seems to say that no such <img alt="{\log\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log\log n}"/> shading can apply to NCC.</p>
<p>
When we ask Google “network coding conjecture Boolean circuit lower bounds” (without quotes), the first page shows AFKL, our posts, and this 2014 <a href="https://people.csail.mit.edu/rrw/ccc14-survey.pdf">survey</a> by Ryan Williams—which mentions neural networks but not NCC. On the next page of hits we see Riis and the followup paper but nothing else that seems directly relevant. Nor does appending `-multiplication’ help screen out AFKL and our posts.</p>
<p>
There is said to be empirical evidence for NCC. We wonder, however, whether that has reached the intensity of thought about circuit lower bounds. We say this because the implications from NCC make three giant steps:</p>
<ol>
<li>
Not only does it assert a super-linear circuit lower bound (okay, for a function with <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> output bits), but… <p/>
</li><li>
…it asserts <img alt="{\Omega(n\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(n\log n)}"/>… <p/>
</li><li>
…for functions that are easily in Turing machine linear time.
</li></ol>
<p>
So one side of our worry is whether NCC can actually shed light on so many fundamental issues from complexity theory, more than absorbing light. At the very least, AFKL have re-stimulated interest in all of these issues. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Is <img alt="{\mathsf{FLIP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFLIP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FLIP}}"/> hard? Is NCC true? What other Boolean functions are AFKL functions? What about other consequences of the NCC to complexity theory?</p>
<p/></font></font></div>
    </content>
    <updated>2019-05-07T03:05:05Z</updated>
    <published>2019-05-07T03:05:05Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="Results"/>
    <category term="AFKL functions"/>
    <category term="Casper Freksen"/>
    <category term="circuit complexity"/>
    <category term="conjecture"/>
    <category term="integer multiplication"/>
    <category term="Kasper Larsen"/>
    <category term="Lior Kamma"/>
    <category term="lower bounds"/>
    <category term="Mark Braverman"/>
    <category term="network coding"/>
    <category term="Peyman Afshani"/>
    <category term="reductions"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-16T01:22:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://kintali.wordpress.com/?p=1235</id>
    <link href="https://kintali.wordpress.com/2019/05/06/preventing-future-college-admissions-scandals-using-blockchain/" rel="alternate" type="text/html"/>
    <title>Preventing future college admissions scandals using Blockchain</title>
    <summary>The recent college admissions scandal resulted in the largest case of its kind to be prosecuted by the US Justice Department. A massive federal investigation code-named ‘Operation Varsity Blues’ uncovered this scandal and charged several high-profile people with bribery, racketeering, money laundering, conspiracy to commit mail and wire fraud. The internet commentary about this topic includes phrases […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="graf graf--p graf-after--h3" id="5bf6">The recent <a class="markup--anchor markup--p-anchor" href="https://en.wikipedia.org/wiki/2019_college_admissions_bribery_scandal" rel="nofollow noopener noopener noopener nofollow noopener" target="_blank">college admissions scandal</a> resulted in the largest case of its kind to be prosecuted by the US Justice Department. A massive federal investigation code-named ‘Operation Varsity Blues’ uncovered this scandal and charged several high-profile people with bribery, racketeering, money laundering, conspiracy to commit mail and wire fraud. The internet commentary about this topic includes phrases like “broken admissions system”, “rich can buy their way into college”, etc etc.</p>
<p class="graf graf--p graf-after--p" id="4b6a">There must be several other cases of fraud that go unnoticed on a daily basis. It’s in human nature to shortcut the rules, collude and cheat to achieve one’s own short-term goals, hoping to get away with one’s fraudulent actions. Let’s discuss how to use Blockchain technology and create a rigorous system to prevent some aspects of such scandals in future.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="b66e"><strong class="markup--strong markup--p-strong">Problems</strong>: Fake athletic certificates and phony athletic profile. Taking photos of students on a stationary rowing machine. Photoshopping students’ face on another athlete’s photo.</p>
<p class="graf graf--p graf-after--p" id="ad90"><strong class="markup--strong markup--p-strong">Solution</strong>: A genuine high-school athlete achieves his/her athletic credentials during a four year period. Getting a genuine athletic certificate involves achieving several intermediate goals. For example, if you achieved a black belt in karate in your 11th grade, you must have progressed through beginner level (with white, yellow and orange belts), intermediate level (with green, blue, purple, brown and red belts) and then reached the advanced level (with a black belt). <a class="markup--anchor markup--p-anchor" href="https://truecerts.co/" rel="noopener nofollow" target="_blank">TrueCerts</a> technology allows sports coaching centers to create certificates for each of these intermediate athletic achievements, sign them using their private keys and post only the SHA-256 hash of the certificate on a public blockchain, thus immutably time-stamping each achievement at the specific day/time the athlete achieved it. The athletic photos can be time-stamped similarly.</p>
<p class="graf graf--p graf-after--p" id="2c0c">This process achieves the seemingly impossible combination of <strong class="markup--strong markup--p-strong">security, privacy and transparency </strong>!! Security is achieved by the asymmetric key encryption. Privacy is achieved by the one-way nature of the cryptographic hash function <a class="markup--anchor markup--p-anchor" href="https://en.wikipedia.org/wiki/SHA-2" rel="noopener nofollow" target="_blank">SHA 256</a>. Transparency is achieved by the fact that anybody with the access to the sports certificate (upon student’s consent) can compute the SHA 256 hash of the certificate and verify that it is stored on a public blockchain validly signed by the private key of the issuer (sports coaching center). This eliminates the fraudulent behavior of creating a bunch of fake credentials and photos, all at once, in a brief period of time. Using a fairly decentralized public blockchain is very important here.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="c253"><strong class="markup--strong markup--p-strong">Problem</strong>: Bribing coaches to accept certain students in their sports teams or issue fake credentials.</p>
<p class="graf graf--p graf-after--p" id="6290"><strong class="markup--strong markup--p-strong">Solution</strong>: This problem can be solved by having several people (perhaps five coaches, some administrative assistants, one principal, one vice-principal, etc) in the organization collectively responsible (using multi-signature wallets and m-of-n signatures) to issue credentials. Each of the involved person is accountable for every issued certificate.</p>
<p class="graf graf--p graf-after--p" id="eb99">Bribing one coach is easy. Bribing ten people is hard. People often hesitate bribing multiple people. Collusion becomes increasingly hard when you increase the size of the group involved. When there are more people involved, there is a higher chance that at least one of them is honest (and brave) to overcome the pressure of the others and blow the whistle.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="0d42"><strong class="markup--strong markup--p-strong">Problem</strong>: Fake college entrance exam (SAT, ACT) test scores</p>
<p class="graf graf--p graf-after--p" id="fe6e"><strong class="markup--strong markup--p-strong">Solution</strong>: Current paper-based test score issuance and verification system is too time-consuming and error-prone. These credentials can be easily faked or tampered with. An ideal solution involves creating a digital certificate, validly signed (or multi-signed) by the issuer and time-stamped with a <a class="markup--anchor markup--p-anchor" href="https://en.wikipedia.org/wiki/SHA-2" rel="nofollow noopener" target="_blank">SHA 256</a> hash on a public blockchain. See <a class="markup--anchor markup--p-anchor" href="https://medium.com/@truecerts/dr-kintalis-motivation-behind-developing-truecerts-platform-aba93f4d290a" rel="noopener" target="_blank">my previous post</a> about preventing fraud in academic transcripts and making the entire system efficient.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="b957"><strong class="markup--strong markup--p-strong">Problem</strong>: Other students taking entrance tests on your behalf.</p>
<p class="graf graf--p graf-after--p" id="0e07"><strong class="markup--strong markup--p-strong">Solution</strong>: This is a problem of verifying the identity of the student taking the test. The current system of using paper-based credentials is broken. It’s easy to create fake driver’s license, passports etc. At <a class="markup--anchor markup--p-anchor" href="https://truecerts.co/" rel="nofollow noopener" target="_blank">TrueCerts</a>, we have created an automated multi-step identity verification that combines your standard KYC identity procedures, utility bills and more importantly biometrics. We define identity as a several data points achieved over a period of time, not just one piece of paper. It’s very hard to cheat all of these steps. If you have a look-alike twin then consider yourself lucky <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/></p>
<p> </p>
<p class="graf graf--p graf-after--p" id="a16a"><strong class="markup--strong markup--p-strong">Partially solvable problems</strong>: Bribing officials to change student’s answers in paper-based exam can be solved to certain extent by using a completely computer-based exam. Bribing proctors to tell the answers to a student (during the test) can be solved with computer-vision-based cheating detection software. Some photoshopped images can be detected using image analysis techniques.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="4d0f"><strong class="markup--strong markup--p-strong">Hard to solve problems</strong>: (1) Getting a fake medical certificate that your kid requires an isolated room to take the test and then bribing the proctor to tell him/her all the answers during the test. (2) Laundering bribes using a non-profit entity. Phew…. These things actually happened during this college admissions scandal. As the famous saying goes “a person is capable of as much atrocity as he/she has imagination”.</p>
<p> </p>
<p class="graf graf--p graf-after--p" id="5ad0">In summary, combining the existing technologies we can solve several of the above mentioned problems and simultaneously achieve <strong class="markup--strong markup--p-strong">security, privacy and transparency</strong>. The main goal here is to make the bad people’s job as difficult as possible and simultaneously making the good people’s job very efficient.</p>
<p class="graf graf--p graf-after--p" id="a4f5">If you want to know more about how we (at <a class="markup--anchor markup--p-anchor" href="https://truecerts.co/" rel="nofollow noopener" target="_blank">TrueCerts</a>) are using Blockchain and other technologies to prevent fraud and corruption in broad range of areas, <a class="markup--anchor markup--p-anchor" href="https://truecerts.co/" rel="nofollow noopener" target="_blank">contact us</a>.</p>
<p class="graf graf--p graf-after--p" id="e889">Stay tuned for my next post about a huge list of real-world fraud and corruption stories that can be prevented rigorously by using cutting-edge technologies.</p>
<p><img alt="TrueCertsTrustSimplified" class="alignnone size-full wp-image-1236" src="https://kintali.files.wordpress.com/2019/05/truecertstrustsimplified.png?w=660"/></p></div>
    </content>
    <updated>2019-05-07T01:29:32Z</updated>
    <published>2019-05-07T01:29:32Z</published>
    <category term="Education"/>
    <category term="Uncategorized"/>
    <category term="bitcoin"/>
    <category term="blockchain"/>
    <category term="college admissions scandal"/>
    <author>
      <name>kintali</name>
    </author>
    <source>
      <id>https://kintali.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/e1376dd220aa259d0efd0638d7619231?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://kintali.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://kintali.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://kintali.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://kintali.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computational Complexity, Polyhedral Combinatorics, Algorithms and Graph Theory</subtitle>
      <title>My Brain is Open</title>
      <updated>2019-05-16T01:22:15Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2431469456247088523</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2431469456247088523/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/thoughts-on-recent-jeopardy-streak.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2431469456247088523" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2431469456247088523" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/thoughts-on-recent-jeopardy-streak.html" rel="alternate" type="text/html"/>
    <title>Thoughts on the recent Jeopardy streak (SPOILERS)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">James Holzhauer  has won 22 consecutive games of Jeopardy and has made around 1.6 million dollars. Nice work if you can get it. Here are some thoughts no this<br/>
<br/>
1) Before James H the records for number of consecutive games was, and still is, Ken Jennings winning 74 in a row, and second place was 20. I was surprised that Ken was that much better than the competition.<br/>
<br/>
2) Before James H the record for amount of money in normal play (not extra from, say, tournament of champions or losing to a computer) was around 400,000. I was surprised that Ken was that much better than the competition.<br/>
<br/>
3) James is obliterating the records for most wins in a single game. He holds the top 12 records for this.  This is due to his betting A LOT on the daily doubles and the final jeop, as well as of course answering so many questions right.<br/>
<br/>
4) One reason players in Jeopardy don't have long streaks is fatigue. The actually play<br/>
5 games a day, two days of the week.  James H is getting a break since he has two weeks off now since they will soon have the Teachers Tournament. This could work either way--- he gets a break or he loses being in-the-zone.<br/>
<br/>
5) James strategy is:<br/>
<br/>
a) Begin with the harder (and more lucrative) questions.<br/>
<br/>
b) Bet A LOT on the daily doubles (which are more likely to be in the more lucrative questions) and almost always go into final jeop with more than twice your opponent (He failed to do this only once.)<br/>
<br/>
c) Bet A LOT on Final Jeop- though not enough so that if you lose you lose the game. I think he's gotten every Final Jeop question right.<br/>
<br/>
For more on his strategy see this article by Oliver Roeder in Nate Silvers Blog: <a href="https://fivethirtyeight.com/features/the-man-who-solved-jeopardy/">here</a><br/>
<br/>
6) I tend to think of this as being a high-risk, high-reward strategy and thus it is unlikely he will beat Ken Jennings, but every time he wins that thought seems sillier and sillier. While we are here, how likely is it that someone will beat Ken Jennings? In an article before all of this Ben Morrison in Nate Silvers Blog wrote that it was quite likely SOMEONE would break Ken J's record,  see <a href="https://fivethirtyeight.com/features/ken-jennings-has-nothing-on-joe-dimaggio/">here</a>.<br/>
<br/>
7) OKAY, how does James H compare to Ken J? According to Oliver Roeder in Nate Silvers Blog,<br/>
<a href="https://fivethirtyeight.com/features/the-battle-for-jeopardy-supremacy/">here</a>, they are similar in terms of percent of questions answered right, but James H bets so much more (bets better?) which is why he is getting so much money. I'll be curious to see a head-to-head contest at some point. But to the issue at hand, they don't give James H that good a chance to break Ken J's record.<br/>
<br/>
8) Jeop used to have a  5-game limit. Maybe that was a good idea- its not that interesting seeing the same person with the same strategy win 22 in a row. Also, the short-talk-with-Alex T-- James is running out of interesting things to say. I wonder what Alex did with Ken J after 50 games.<br/>
``So Ken, I hear you're good at Jeopardy''<br/>
<br/>
9) Misc: Ken J was the inspiration for IBM to do Watson.<br/>
<br/>
10) Will future players use James Strategy? Note that you have to be REALLY GOOD in the first place for it to help you. Maybe a modified version where you go for the lucrative questions and bet a lot on Daily Doubles (more than people have done in the past) when its an area you know really well (I'll take Ramsey Theory for $2000.)<br/>
<br/>
11) I used to DVR and watch Jeop but didn't mind if I was a few behind. Now I have to stay on top of it so articles like those pointed to above don't give me a spoiler.<br/>
<br/>
12) My prediction: He will beat Ken Jenning for money but not for number-of-games. I have no real confidence in these predictions.</div>
    </content>
    <updated>2019-05-07T00:41:00Z</updated>
    <published>2019-05-07T00:41:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-15T18:02:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17422</id>
    <link href="https://gilkalai.wordpress.com/2019/05/07/answer-to-tyi-37-arithmetic-progressions-in-3d-brownian-motion/" rel="alternate" type="text/html"/>
    <title>Answer to TYI 37: Arithmetic Progressions in 3D Brownian Motion</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Consider a Brownian motion in three dimensional space. We asked (TYI 37) What is the largest number of points on the path described by the motion which form an arithmetic progression? (Namely, , so that all are equal.) Here is … <a href="https://gilkalai.wordpress.com/2019/05/07/answer-to-tyi-37-arithmetic-progressions-in-3d-brownian-motion/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Consider a Brownian motion in three dimensional space. <a href="https://gilkalai.wordpress.com/2019/03/07/test-your-intuition-or-simply-guess-37-arithmetic-progressions-for-brownian-motion-in-space/">We asked (TYI 37)</a> What is the largest number of points on the path described by the motion which form an arithmetic progression? (Namely, <img alt="x_1,x_2, x_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2Cx_2%2C+x_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1,x_2, x_t"/>, so that all <img alt="x_{i+1}-x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bi%2B1%7D-x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_{i+1}-x_i"/> are equal.)</p>
<p>Here is what you voted for</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/poll-apb.png"><img alt="" class="alignnone size-full wp-image-17423" src="https://gilkalai.files.wordpress.com/2019/05/poll-apb.png?w=640"/></a></p>
<p><span style="color: #ff0000;"><strong>TYI37 poll: Final-results</strong></span></p>
<p>Analysis of the poll results:  Almost surely 2 is the winner with 30.14% of the 209 votes, and almost surely infinity (28.71%) comes close at second place. In the  third place is  almost surely 3 (14.83%),  and then comes positive probability for each integer (13.4%), almost surely 5 (5.26%),  almost surely 6 (2.87%), and  almost surely 4 (2.39%).</p>
<h2>Test your political intuition: which coalition is going to be formed?</h2>
<p>Almost surely 2 (briefly AS2) and almost surely infinity (ASI) can form a government  with no need for a larger coalition. But they represent two political extremes. Is AS3 politically closer to AS2 or to ASI? “k with probability p_k for every k&gt;2” (briefly, COM) represent a complicated political massage. Is it closer to AS2 or to ASI? (See the old posts on <a href="https://gilkalai.wordpress.com/2009/02/16/which-coalition/">which coalition</a> <a href="https://gilkalai.wordpress.com/2009/02/17/which-coalition-to-form-2/">will be formed</a>.)</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/05/poll-br.png"><img alt="" class="alignnone size-medium wp-image-17424" height="300" src="https://gilkalai.files.wordpress.com/2019/05/poll-br.png?w=131&amp;h=300" width="131"/></a> <a href="https://gilkalai.files.wordpress.com/2019/05/poll-brown.png"><img alt="" class="alignnone size-medium wp-image-17425" height="300" src="https://gilkalai.files.wordpress.com/2019/05/poll-brown.png?w=133&amp;h=300" width="133"/> </a><a href="https://gilkalai.files.wordpress.com/2019/05/poll189.png"><img alt="" class="alignnone size-medium wp-image-17427" height="300" src="https://gilkalai.files.wordpress.com/2019/05/poll189.png?w=127&amp;h=300" width="127"/></a></p>
<p><span style="color: #ff0000;"><strong>TYI37 poll: Partial results. It was exciting to see how the standing of the answers changed in the process of counting the votes.</strong></span></p>
<p>And the correct answer is: <span id="more-17422"/></p>
<h2><strong>5 (FIVE)</strong></h2>
<p>See the paper:</p>
<p class="title mathjax">Itai Benjamini and Gady Kozma: <a href="https://arxiv.org/abs/1810.10077">Arithmetic progressions in the trace of Brownian motion in space</a></p>
<p> </p></div>
    </content>
    <updated>2019-05-06T21:38:48Z</updated>
    <published>2019-05-06T21:38:48Z</published>
    <category term="Combinatorics"/>
    <category term="Open discussion"/>
    <category term="Probability"/>
    <category term="Brownian motion"/>
    <category term="Gady Kozma"/>
    <category term="Itai Benjamini"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-05-16T01:22:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4240</id>
    <link href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 3: Follow the Regularized Leader</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The multiplicative weights algorithm is simple to define and analyze, and it has several applications, but both its definition and its analysis seem to come out of nowhere. We mentioned that all the quantities arising in the algorithm and its … <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 The multiplicative weights algorithm is simple to define and analyze, and it has several applications, but both its definition and its analysis seem to come out of nowhere. We mentioned that all the quantities arising in the algorithm and its analysis have statistical physics interpretations, but even this observation brings up more questions than it answers. The Gibbs distribution, for example, does put more weight on lower-energy states, and so it makes sense in an optimization setting, but to get good approximations one wants to use lower temperatures, while the distributions used by the multiplicative weights algorithms have temperature <img alt="{1/\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/\epsilon}"/>, where <img alt="{2\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\epsilon}"/> is the final “amortized” regret bound, so that one uses, quite counterintuitively, higher temperatures for better approximations. </p>
<p>
Furthermore, it is not clear how we would generalize the ideas of multiplicative weights to the case in which the set of feasible solutions <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is anything other than the set of distributions.</p>
<p>
Today we discuss the <em>“Follow the Regularized Leader”</em> method, which provides a framework to design and analyze online algorithms in a versatile and well-motivated way. We will then see how we can “discover” the definition and analysis of multiplicative weights, and how to “discover” another online algorithm which can be seen as a generalization of projected gradient descent (that is, one can derive the projected gradient descent algorithm and its analysis from this other online algorithm).</p>
<p>
<span id="more-4240"/></p>
<p>
</p><p><b>1. Follow The Regularized Leader </b></p>
<p/><p>
We will first state some results in full generality, making no assumptions on the set <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> of feasible solutions or on the set of loss functions <img alt="{f_t : K \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t : K \rightarrow {\mathbb R}}"/> encountered by the algorithm at each step.</p>
<p>
Let us try to define an online optimization algorithm from scratch. The solution <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> proposed by the algorithm at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> can only depend on the previous cost functions <img alt="{f_1,\ldots,f_{t-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,\ldots,f_{t-1}}"/>; how should it depend on it? If the offline optimal solution <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is consistently better than all others at each time step, then we would like <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> to be that solution, so we want <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> to be a solution that would have worked well in the previous steps. The most extreme way of implementing this idea is the <em>Follow the Leader</em> algorithm (abbreviated FTL), in which we set the solution at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/></p>
<p/><p align="center"><img alt="\displaystyle  x_t := \arg\min_{x\in K} \sum_{k=1}^{t-1} f_k(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t := \arg\min_{x\in K} \sum_{k=1}^{t-1} f_k(x) "/></p>
<p> to be the best solution for the previous steps. (Note that the algorithm does not prescribe what solution to use at step <img alt="{t=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1}"/>.)</p>
<p>
It is possible for FTL to perform very badly. Consider for example the “experts” setting in which we analyzed multiplicative weights: the set of feasible solutions <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is the set <img alt="{\Delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Delta}"/> of probability distributions over <img alt="{\{1,\ldots,n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{1,\ldots,n\}}"/>, and the cost functions are linear <img alt="{f_t(x) = \sum_i \ell_t(i) x(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Csum_i+%5Cell_t%28i%29+x%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x) = \sum_i \ell_t(i) x(i)}"/> with coefficients <img alt="{0\leq \ell_t(i) \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5Cleq+%5Cell_t%28i%29+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0\leq \ell_t(i) \leq 1}"/>. Suppose that <img alt="{n=2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=2}"/> and that <img alt="{x_1 = (0.5,.0.5)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%280.5%2C.0.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1 = (0.5,.0.5)}"/>. Then a possible run of the algorithm could be: </p>
<ol>
<li> <img alt="{x_1 = (.5,.5)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%28.5%2C.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1 = (.5,.5)}"/>, <img alt="{\ell_1 = (0,.5)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1+%3D+%280%2C.5%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_1 = (0,.5)}"/>
</li><li> <img alt="{x_2 = (1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_2+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_2 = (1,0)}"/>, <img alt="{\ell_2 = (1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_2 = (1,0)}"/>
</li><li> <img alt="{x_3 = (0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_3+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_3 = (0,1)}"/>, <img alt="{\ell_3 = (0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_3+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_3 = (0,1)}"/>
</li><li> <img alt="{x_4 = (1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_4+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_4 = (1,0)}"/>, <img alt="{\ell_4 = (1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_4+%3D+%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_4 = (1,0)}"/>
</li><li> <img alt="{x_5 = (0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_5+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_5 = (0,1)}"/>, <img alt="{\ell_5 = (0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_5+%3D+%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_5 = (0,1)}"/>
</li></ol>
<p align="center"><img alt="\displaystyle \vdots" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cvdots&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \vdots"/></p>
<p>
In which, after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps, the algorithm suffers a loss of <img alt="{T- O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT-+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T- O(1)}"/> while the offline optimum is <img alt="{T/2 + O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2F2+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T/2 + O(1)}"/>. Thus, the regret is about <img alt="{T/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T/2}"/>, which compares very unfavorably to the <img alt="{O(\sqrt T)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+T%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\sqrt T)}"/> regret of the multiplicative weight algorithm. For general <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, a similar example shows that the regret of FTL can be as high as about <img alt="{T\cdot \left( 1- \frac 1n \right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%5Ccdot+%5Cleft%28+1-+%5Cfrac+1n+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T\cdot \left( 1- \frac 1n \right)}"/>.</p>
<p>
In the above bad example, the algorithm keeps “overfitting” to the past history: if an expert is a bit better than the others, the algorithm puts all its probability mass on that expert, and the algorithm keeps changing its mind at every step. Interestingly, this is the only failure mode of the algorithm.</p>
<blockquote><p><b>Theorem 1 (Analysis of FTL)</b> <em> For any sequence of cost functions <img alt="{f_1,\ldots,f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,\ldots,f_t}"/> and any number of time steps <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>, the FTL algorithm satisfies the regret bound </em></p><em>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \sum_{t=1}^T f_t(x_t ) - f_t(x_{t+1} ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t+%29+-+f_t%28x_%7Bt%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \sum_{t=1}^T f_t(x_t ) - f_t(x_{t+1} ) "/></p>
</em><p><em> </em></p></blockquote>
<p> So that if the functions <img alt="{f_t(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(\cdot)}"/> are Lipschitz with respect to a distance function on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>, then the only way for the regret to be large is for <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> to typically be far, in that distance, from <img alt="{x_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{t+1}}"/>.</p>
<p>
<em>Proof:</em>  Recalling the definition of regret, </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T := \sum_{t=1}^T f_t(x_t) - \min_{x\in K} \sum_{t=1}^T f_t(x) \ , " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3A%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T := \sum_{t=1}^T f_t(x_t) - \min_{x\in K} \sum_{t=1}^T f_t(x) \ , "/></p>
<p> the theorem is equivalent to <a name="ftl.analysis"/></p><a name="ftl.analysis">
<p align="center"><img alt="\displaystyle   \sum_{t=1}^T f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t(x) \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_%7Bt%2B1%7D%29+%5Cleq+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \sum_{t=1}^T f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t(x) \ \ \ \ \ (1)"/></p>
</a><p><a name="ftl.analysis"/> We will prove <a href="https://lucatrevisan.wordpress.com/feed/#ftl.analysis">(1)</a> by induction. The base case <img alt="{T=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T=1}"/> is just the definition of <img alt="{x_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_2}"/>. Assuming that $latex {<a href="https://lucatrevisan.wordpress.com/feed/#ftl.analysis">(1)</a>}&amp;fg=000000$ is true up to <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> we have </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^{T+1} f_t (x_{t+1}) = \left( \sum_{t=1}^{T} f_t (x_{t+1}) \right) + f_{T+1} (x_{T+2}) \leq \sum_{t=1}^{T+1} f_t (x_{T+2}) = \min_{x\in K} \sum_{t=1}^{T+1} f_t(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t+%28x_%7Bt%2B1%7D%29+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5E%7BT%7D+f_t+%28x_%7Bt%2B1%7D%29+%5Cright%29+%2B+f_%7BT%2B1%7D+%28x_%7BT%2B2%7D%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t+%28x_%7BT%2B2%7D%29+%3D+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D+f_t%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^{T+1} f_t (x_{t+1}) = \left( \sum_{t=1}^{T} f_t (x_{t+1}) \right) + f_{T+1} (x_{T+2}) \leq \sum_{t=1}^{T+1} f_t (x_{T+2}) = \min_{x\in K} \sum_{t=1}^{T+1} f_t(x) "/></p>
<p> where the middle step follows from the use of the inductive assumption, which gives </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^{T} f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t (x) \leq \sum_{t=1}^T f_t (x_{T+2}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5E%7BT%7D+f_t+%28x_%7Bt%2B1%7D%29+%5Cleq+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_%7BT%2B2%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^{T} f_t (x_{t+1}) \leq \min_{x\in K} \sum_{t=1}^T f_t (x) \leq \sum_{t=1}^T f_t (x_{T+2}) "/></p>
<p> <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
The above example and analysis suggest that we should modify FTL in such a way that the choices of the algorithm don’t change too much from step to step, and that the solution <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> should be a compromise between optimizing with respect to previous cost functions and not changing too much from step to step.</p>
<p>
In order to do this, we introduce a new function <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/>, called a <em>regularizer</em> (more on it later), and, at each step, we compute the solution</p>
<p/><p align="center"><img alt="\displaystyle  x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} f_k(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+R%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} f_k(x) "/></p>
<p> This algorithm is called <em>Follow the Regularized Leader</em> or FTRL. Typically, the function <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> is chosen to be strictly convex and to take values that are rather big in magnitude. Then <img alt="{x_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1}"/> will be the unique minimum of <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> and, at each subsequent step, <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> will be selected in a way to balance the pull toward the minimum of <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> and the pull toward the FTL solution <img alt="{\arg\min_{x\in K} \sum_k f_k(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5Csum_k+f_k%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\arg\min_{x\in K} \sum_k f_k(x)}"/>. In particular, if <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> is large in magnitude compared to each <img alt="{f_t(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(\cdot)}"/>, the solution will not change too much from step to step.</p>
<p>
We have the following analysis that makes no assumptions on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>, on the cost functions <img alt="{f_t(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(\cdot)}"/> and on the regularizer (not even that the regularizer is convex).</p>
<blockquote><p><b>Theorem 2 (Analysis of FTRL)</b> <em> For every sequence of cost functions and every regularizer function, the regret after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps of the FTRL algorithm is bounded as follows: for every <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, </em></p><em>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T(x) \leq \left( \sum_{t=1}^T f_t(x_{t}) - f_t (x_{t+1}) \right) + R(x) - R(x_1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_%7Bt%7D%29+-+f_t+%28x_%7Bt%2B1%7D%29+%5Cright%29+%2B+R%28x%29+-+R%28x_1%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T(x) \leq \left( \sum_{t=1}^T f_t(x_{t}) - f_t (x_{t+1}) \right) + R(x) - R(x_1)"/></p>
<p> where </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T (x) := \sum_{t=1}^T f_t (x_t) - f_t (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%28x%29+%3A%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_t%29+-+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T (x) := \sum_{t=1}^T f_t (x_t) - f_t (x) "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
<em>Proof:</em>  Let us run for <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps the FTRL algorithm with regularizer <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> and cost functions <img alt="{f_1,\ldots,f_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,\ldots,f_T}"/>, and call <img alt="{x_1,\ldots,x_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cldots%2Cx_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1,\ldots,x_T}"/> the solutions computed by the FTL algorithm. </p>
<p>
Now consider the following mental experiment: we run the FTL algorithm for <img alt="{T+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T+1}"/> steps, with the sequence of cost functions <img alt="{R,f_1,\ldots,f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%2Cf_1%2C%5Cldots%2Cf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R,f_1,\ldots,f_t}"/>, and we use <img alt="{x_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1}"/> as a first solution. Then we see that the solutions computed by the FTL algorithm will be precisely <img alt="{x_1,x_1,x_2,\ldots,x_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%2Cx_1%2Cx_2%2C%5Cldots%2Cx_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1,x_1,x_2,\ldots,x_T}"/>. The regret bound for FTL implies that, for every <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>,</p>
<p/><p align="center"><img alt="\displaystyle  R(x_1) - R(x) + \sum_{t=1}^T f_t(x_t) - f_t(x) \leq R(x_1) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x_1%29+-+R%28x%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_t%28x%29+%5Cleq+R%28x_1%29+-+R%28x_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  R(x_1) - R(x) + \sum_{t=1}^T f_t(x_t) - f_t(x) \leq R(x_1) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1}) "/></p>
<p> <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
Having established these results, the general recipe to solve an online optimization problem will be to find a regularizer function <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> such that the minimum of <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> “pulls away from” solutions that would make the FTL algorithm overfit, and such that there is a good balance between how big <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> gets over <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> (because we pay <img alt="{R(x^*) - R(x_1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%5E%2A%29+-+R%28x_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(x^*) - R(x_1)}"/> in the regret, where <img alt="{x^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^*}"/> is the offline optimum) and how stable is the minimum of <img alt="{R(x) + \sum_{k=1}^{t-1} f_k(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+f_k%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(x) + \sum_{k=1}^{t-1} f_k(x)}"/> as <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> varies.</p>
<p>
</p><p><b>2. Negative-Entropy Regularization </b></p>
<p/><p>
Let us consider again the “experts” setting, that is, the online optimization setup in which <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is the set of probability distributions over <img alt="{\{ 1,\ldots, n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2C+n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 1,\ldots, n\}}"/> and the cost functions are linear <img alt="{f_t (x) = \sum_i \ell_t (i) x(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+%5Cell_t+%28i%29+x%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t (x) = \sum_i \ell_t (i) x(i)}"/> with bounded coefficients.</p>
<p>
The example we showed above showed that FTL will tend to put all the probability mass on one expert. We would like to choose a regularizer that fights this tendency by penalizing “concentrated” distributions and favoring “spread-out” distributions. This observation might trigger the thought that the <em>entropy</em> of a distribution is a good measure of how concentrated or spread out it is, although the entropy is actually higher for spread-out distribution and smaller for concentrated ones. So we will use as a regularizer <em>minus the entropy</em>, multiplied by an appropriate scaling factor: </p>
<p align="center"><img alt="\displaystyle  R(x) := c \cdot \sum_{i=1}^n x_i \ln x_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3A%3D+c+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  R(x) := c \cdot \sum_{i=1}^n x_i \ln x_i "/></p>
<p> (Entropy is usually defined using logarithms in base 2, but using natural logarithms will make it cleaner to take derivatives, and it only affects the constant factor <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/>.) With this choice of regularizer, we have</p>
<p/><p align="center"><img alt="\displaystyle  x_t = \arg\min_{x\in \Delta} \ \ \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \sum_{i=1}^n x_i \ln x_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+%5CDelta%7D+%5C+%5C+%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+x_k+%5Crangle+%5Cright+%29+%2B+c+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t = \arg\min_{x\in \Delta} \ \ \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \sum_{i=1}^n x_i \ln x_i "/></p>
<p> To compute the minimum of the above function we will use the method of Lagrange multipliers. Specialized to our setting, the method of Lagrange multiplier states that if we want to solve the constrained minimization problem </p>
<p align="center"><img alt="\displaystyle  \min_{x : \ a^Tx = b } \ \ f(x ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx+%3A+%5C+a%5ETx+%3D+b+%7D+%5C+%5C+f%28x+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min_{x : \ a^Tx = b } \ \ f(x ) "/></p>
<p> we introduce a new parameter <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> and define the function </p>
<p align="center"><img alt="\displaystyle  f_\lambda (x) := f(x) + \lambda \cdot (a^T x - b ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_%5Clambda+%28x%29+%3A%3D+f%28x%29+%2B+%5Clambda+%5Ccdot+%28a%5ET+x+-+b+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_\lambda (x) := f(x) + \lambda \cdot (a^T x - b ) "/></p>
<p> Then it is possible to prove that if <img alt="{x^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^*}"/> is a feasible minimizer of <img alt="{f(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(\cdot)}"/>, then there is at least a value of <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> such that <img alt="{\nabla f_\lambda (x^*) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f_%5Clambda+%28x%5E%2A%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\nabla f_\lambda (x^*) = 0}"/>, that is, such that <img alt="{x^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^*}"/> is a stable point of <img alt="{f_\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_\lambda}"/>. So one can proceed by finding all <img alt="{x,\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2C%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,\lambda}"/> such that <img alt="{\nabla f_\lambda (x) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f_%5Clambda+%28x%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\nabla f_\lambda (x) = 0}"/> and then filtering out the values of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> such that <img alt="{a^T x \neq b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%5ET+x+%5Cneq+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a^T x \neq b}"/>, and finally looking at which of the remaining <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> minimizes <img alt="{f(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(\cdot)}"/>.</p>
<p>
Ignoring for a moment the non-negativity constraints, the constraint <img alt="{x\in \Delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in \Delta}"/> reduces to <img alt="{\sum_i x_i = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+x_i+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_i x_i = 1}"/>, so we have to consider the function </p>
<p align="center"><img alt="\displaystyle  \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \left( \sum_{i=1}^n x_i \ln x_i \right) + \lambda \cdot \left( \langle x, {\bf 1} \rangle - 1\right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+x_k+%5Crangle+%5Cright+%29+%2B+c+%5Ccdot+%5Cleft%28+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+%5Cright%29+%2B+%5Clambda+%5Ccdot+%5Cleft%28+%5Clangle+x%2C+%7B%5Cbf+1%7D+%5Crangle+-+1%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left( \sum_{k=1}^{t-1} \langle \ell_k , x_k \rangle \right ) + c \cdot \left( \sum_{i=1}^n x_i \ln x_i \right) + \lambda \cdot \left( \langle x, {\bf 1} \rangle - 1\right) "/></p>
<p> The partial derivative of the above expression with respect to <img alt="{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_i}"/> is </p>
<p align="center"><img alt="\displaystyle  \left( \sum_{k=1}^{t-1} \ell_k (i) \right ) + c \cdot \left( 1 + \ln x_i \right) + \lambda " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29+%5Cright+%29+%2B+c+%5Ccdot+%5Cleft%28+1+%2B+%5Cln+x_i+%5Cright%29+%2B+%5Clambda+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left( \sum_{k=1}^{t-1} \ell_k (i) \right ) + c \cdot \left( 1 + \ln x_i \right) + \lambda "/></p>
<p> If we want the gradient to be zero then we want all the above expressions to be zero, which translates to </p>
<p align="center"><img alt="\displaystyle  x_i = {\rm exp} \left( -1 - \frac \lambda c - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_i+%3D+%7B%5Crm+exp%7D+%5Cleft%28+-1+-+%5Cfrac+%5Clambda+c+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_i = {\rm exp} \left( -1 - \frac \lambda c - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) "/></p>
<p> There is only one value of <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> that makes the above solution a probability distribution, and the corresponding solution is </p>
<p align="center"><img alt="\displaystyle  x_i = \frac {{\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) } {\sum_j {\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(j) \right) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_i+%3D+%5Cfrac+%7B%7B%5Crm+exp%7D+%5Cleft%28+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+%5Cright%29+%7D+%7B%5Csum_j+%7B%5Crm+exp%7D+%5Cleft%28+-+%5Cfrac+1c+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28j%29+%5Cright%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_i = \frac {{\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(i) \right) } {\sum_j {\rm exp} \left( - \frac 1c \sum_{k=1}^{t-1} \ell_k(j) \right) } "/></p>
<p> Notice that this is exactly the solution computed by the multiplicative weights algorithm, if we choose <img alt="{c = 1/\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c = 1/\epsilon}"/>. So we have “rediscovered” the multiplicative weights algorithm and we have also “explained” what it does: at every step it balances the goals of finding a solution that is good for the past and that has large entropy.</p>
<p>
Now it remains to bound, at each time step, </p>
<p/><p align="center"><img alt="\displaystyle  f_t (x_t) - f_t (x_{t+1}) = \langle \ell_t , x_t - x_{t+1} \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x_t%29+-+f_t+%28x_%7Bt%2B1%7D%29+%3D+%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t (x_t) - f_t (x_{t+1}) = \langle \ell_t , x_t - x_{t+1} \rangle "/></p>
<p> For this, it is convenient to return to the notation that we used in describing the multiplicative weights algorithm, that is, it is convenient to work with the weights defined as </p>
<p align="center"><img alt="\displaystyle  w_1(i) = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w_1%28i%29+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  w_1(i) = 1"/></p>
<p align="center"><img alt="\displaystyle  w_{t+1} (i) = w_t (i) \cdot e^{ \ell_t (i) / c}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w_%7Bt%2B1%7D+%28i%29+%3D+w_t+%28i%29+%5Ccdot+e%5E%7B+%5Cell_t+%28i%29+%2F+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  w_{t+1} (i) = w_t (i) \cdot e^{ \ell_t (i) / c}"/></p>
<p> so that, at each time step </p>
<p align="center"><img alt="\displaystyle  x_t(i) = \frac {w_t(i)}{\sum_j w_t (j) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7Bw_t%28i%29%7D%7B%5Csum_j+w_t+%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t(i) = \frac {w_t(i)}{\sum_j w_t (j) } "/></p>
<p> We are assuming <img alt="{0 \leq \ell_t (i) \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%5Cleq+%5Cell_t+%28i%29+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 \leq \ell_t (i) \leq 1}"/>, so the weights are non-increasing with time. Then </p>
<p align="center"><img alt="\displaystyle  x_{t+1} (i) = \frac {w_{t+1} (i) }{\sum_j w_{t+1} (j) } = \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t+1} (j) } \geq \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t} (j) } \geq x_t(i) \cdot e^{-1/c} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%28i%29+%3D+%5Cfrac+%7Bw_%7Bt%2B1%7D+%28i%29+%7D%7B%5Csum_j+w_%7Bt%2B1%7D+%28j%29+%7D+%3D+%5Cfrac+%7Bw_%7Bt%7D+%28i%29+e%5E%7B-%5Cell_t+%28i%29+%2Fc+%7D%7D%7B%5Csum_j+w_%7Bt%2B1%7D+%28j%29+%7D+%5Cgeq+%5Cfrac+%7Bw_%7Bt%7D+%28i%29+e%5E%7B-%5Cell_t+%28i%29+%2Fc+%7D%7D%7B%5Csum_j+w_%7Bt%7D+%28j%29+%7D+%5Cgeq+x_t%28i%29+%5Ccdot+e%5E%7B-1%2Fc%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} (i) = \frac {w_{t+1} (i) }{\sum_j w_{t+1} (j) } = \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t+1} (j) } \geq \frac {w_{t} (i) e^{-\ell_t (i) /c }}{\sum_j w_{t} (j) } \geq x_t(i) \cdot e^{-1/c} "/></p>
<p> For every <img alt="{c \geq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c \geq 1}"/> we have <img alt="{e^{-1/c} \geq 1 - 1/c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-1%2Fc%7D+%5Cgeq+1+-+1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{-1/c} \geq 1 - 1/c}"/>, so </p>
<p align="center"><img alt="\displaystyle  x_{t}(i) - x_{t+1}(i) \leq \frac 1c x_t (i) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%7D%28i%29+-+x_%7Bt%2B1%7D%28i%29+%5Cleq+%5Cfrac+1c+x_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t}(i) - x_{t+1}(i) \leq \frac 1c x_t (i) "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \langle \ell_t , x_t - x_{t+1} \rangle \leq \sum_i \ell_t(i) \cdot \frac 1c x_t(i) \leq \frac 1c " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%5Cleq+%5Csum_i+%5Cell_t%28i%29+%5Ccdot+%5Cfrac+1c+x_t%28i%29+%5Cleq+%5Cfrac+1c+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \langle \ell_t , x_t - x_{t+1} \rangle \leq \sum_i \ell_t(i) \cdot \frac 1c x_t(i) \leq \frac 1c "/></p>
<p> Putting it all together, we have </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \frac Tc + c \ln n " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cfrac+Tc+%2B+c+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \frac Tc + c \ln n "/></p>
<p> Choosing <img alt="{c = \sqrt{\frac T {\ln n}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Csqrt%7B%5Cfrac+T+%7B%5Cln+n%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c = \sqrt{\frac T {\ln n}}}"/>, we have </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} "/></p>
<p> Thus, we have reconstructed the analysis of the multiplicative weights algorithm.</p>
<p>
Interestingly, the analysis that we derived today is not exactly identical to the one from the post on multiplicative weights. There, we derived the bound</p>
<p/><p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t^2 (i) x_t (i) \ + \frac {\ln n}{\epsilon } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+%5Cell_t%5E2+%28i%29+x_t+%28i%29+%5C+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t^2 (i) x_t (i) \ + \frac {\ln n}{\epsilon } "/></p>
<p> while here, setting <img alt="{\epsilon = 1/c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+1%2Fc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon = 1/c}"/>, we derived </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t (i) x_t(i) + \frac {\ln n}{\epsilon } - \frac 1 \epsilon H(x^*) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+%5Cell_t+%28i%29+x_t%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon+%7D+-+%5Cfrac+1+%5Cepsilon+H%28x%5E%2A%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n \ell_t (i) x_t(i) + \frac {\ln n}{\epsilon } - \frac 1 \epsilon H(x^*) "/></p>
<p> where <img alt="{x^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^*}"/> is the offline optimum and <img alt="{H(x) = \sum_i x_i \ln \frac 1 {x_i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%28x%29+%3D+%5Csum_i+x_i+%5Cln+%5Cfrac+1+%7Bx_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H(x) = \sum_i x_i \ln \frac 1 {x_i}}"/> is the entropy function (computed using natural logarithms). </p>
<p>
</p><p><b>3. L2 Regularization </b></p>
<p/><p>
Now that we have a general method, let us apply it to a new context: suppose that, as before, our cost functions are linear, but let <img alt="{K = {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK+%3D+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K = {\mathbb R}^n}"/>. With linear cost functions and no bound on the size of solutions, it will not be possible to talk about regret with respect to the offline optimum, because the offline optimum will always be <img alt="{-\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-\infty}"/>, but it will be possible to talk about regret with respect to a particular offline solution <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, which will already lead to interesting consequences.</p>
<p>
What regularizer should we use? In reasoning about regularizers, it can be helpful to think about what would go wrong if we use FTL, and then considering what regularizer would successfully “pull away” from the bad solutions found by FTL. In this context of linear loss functions and unbounded solutions, FTL will pick an infinitely big solution at each step, or, to be more precise, the “max” in the definition of FTL is undefined. To fight this tendency of FTL to go off to infinity, it makes sense for the regularizer to be a measure of how big a solution is. Since we are going to have to compute derivatives, it is good to use a measure of “bigness” with a nice gradient, and <img alt="{||x ||^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7Cx+%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{||x ||^2}"/> is a natural choice. So, for a scale parameter <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> to be optimized later, our regularizer will be </p>
<p align="center"><img alt="\displaystyle  R(x) := c || x||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3A%3D+c+%7C%7C+x%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  R(x) := c || x||^2 "/></p>
<p> This tells us that </p>
<p align="center"><img alt="\displaystyle  x_ 1 = {\bf 0} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_+1+%3D+%7B%5Cbf+0%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_ 1 = {\bf 0} "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  x_{t+1} = \arg\min_{x \in {\mathbb R}^n} \ \ c ||x||^2 + \sum_{k=1}^t \langle \ell_k , x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx+%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+c+%7C%7Cx%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5Et+%5Clangle+%5Cell_k+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} = \arg\min_{x \in {\mathbb R}^n} \ \ c ||x||^2 + \sum_{k=1}^t \langle \ell_k , x \rangle "/></p>
<p> The function that we are minimizing in the above expression is convex, so we just have to compute the gradient and set it to zero </p>
<p align="center"><img alt="\displaystyle  2c x + \sum_{k=1}^t \ell_k = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2c+x+%2B+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_k+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  2c x + \sum_{k=1}^t \ell_k = 0 "/></p>
<p align="center"><img alt="\displaystyle  x = - \frac 1 {2c} \sum_{k=1}^t \ell_k " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+-+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x = - \frac 1 {2c} \sum_{k=1}^t \ell_k "/></p>
<p> Which can be also expressed as </p>
<p align="center"><img alt="\displaystyle  x_1 = {\bf 0}; \ \ \ x_{t+1} = x_t - \frac 1 {2c} \ell_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%7B%5Cbf+0%7D%3B+%5C+%5C+%5C+x_%7Bt%2B1%7D+%3D+x_t+-+%5Cfrac+1+%7B2c%7D+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_1 = {\bf 0}; \ \ \ x_{t+1} = x_t - \frac 1 {2c} \ell_t "/></p>
<p> This makes perfect sense because, in the “experts” interpretation, we want to penalize the experts that performed badly in the past. Here we have no constraints on our allocations, so we simply decrease (additively this time, not multiplicatively) the allocation to the experts that caused a higher loss.</p>
<p>
To compute the regret bound, we have </p>
<p align="center"><img alt="\displaystyle  f_t(x_t) - f_{t} (x_{t+1}) = \langle \ell_t, x_t - x_{t+1} \rangle = \left\langle \ell_t , \frac 1 {2c} \ell_t \right\rangle = \frac 1 {2c} || \ell_t||^2 || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D%29+%3D+%5Clangle+%5Cell_t%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%3D+%5Cleft%5Clangle+%5Cell_t+%2C+%5Cfrac+1+%7B2c%7D+%5Cell_t+%5Cright%5Crangle+%3D+%5Cfrac+1+%7B2c%7D+%7C%7C+%5Cell_t%7C%7C%5E2+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t(x_t) - f_{t} (x_{t+1}) = \langle \ell_t, x_t - x_{t+1} \rangle = \left\langle \ell_t , \frac 1 {2c} \ell_t \right\rangle = \frac 1 {2c} || \ell_t||^2 || "/></p>
<p> and so the regret with respect to a solution <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T(x) \leq R(x) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1} ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+R%28x%29+-+R%28x_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+-+f_%7Bt%7D+%28x_%7Bt%2B1%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T(x) \leq R(x) - R(x_1) + \sum_{t=1}^T f_t(x_t) - f_{t} (x_{t+1} ) "/></p>
<p align="center"><img alt="\displaystyle  = c || x||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+c+%7C%7C+x%7C%7C%5E2+%2B+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bt%3D1%7D%5ET+%7C%7C+%5Cell_t%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = c || x||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t||^2 "/></p>
<p> If we know a bound </p>
<p align="center"><img alt="\displaystyle  \forall t: \ \ || \ell_t || \leq L " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+t%3A+%5C+%5C+%7C%7C+%5Cell_t+%7C%7C+%5Cleq+L+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall t: \ \ || \ell_t || \leq L "/></p>
<p align="center"><img alt="\displaystyle  || x|| \leq D " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x%7C%7C+%5Cleq+D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || x|| \leq D "/></p>
<p> then we can optimize <img alt="{c = \sqrt{\frac T {2D^2 L^2}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Csqrt%7B%5Cfrac+T+%7B2D%5E2+L%5E2%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c = \sqrt{\frac T {2D^2 L^2}}}"/> and we have </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T(x) \leq D L \sqrt{ 2 T} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D+L+%5Csqrt%7B+2+T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T(x) \leq D L \sqrt{ 2 T} "/></p>
<p>
</p><p><b>  3.1. Dealing with Constraints </b></p>
<p/><p>
Consider now the case in which the loss functions are linear and <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is an arbitrary convex set. Using the same regularizer <img alt="{R(x) = c || x||^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%3D+c+%7C%7C+x%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(x) = c || x||^2}"/> we have the algorithm </p>
<p align="center"><img alt="\displaystyle  x_1 = \arg\min_{x\in K} c ||x ||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+c+%7C%7Cx+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_1 = \arg\min_{x\in K} c ||x ||^2 "/></p>
<p align="center"><img alt="\displaystyle  x_{t+1} = \arg\min_{x\in K} \ \ c ||x ||^2 + \sum_{k=1}^{t} \langle \ell_t , x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+c+%7C%7Cx+%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt%7D+%5Clangle+%5Cell_t+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} = \arg\min_{x\in K} \ \ c ||x ||^2 + \sum_{k=1}^{t} \langle \ell_t , x \rangle "/></p>
<p> How can we solve the above constrained optimization problem? A very helpful observation is that we can first solve the unconstrained optimization and then project on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>, that is we can proceed as follows: </p>
<p align="center"><img alt="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} \ \ c ||y ||^2 + \sum_{k=1}^{t} \langle \ell_t , y \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+c+%7C%7Cy+%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt%7D+%5Clangle+%5Cell_t+%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} \ \ c ||y ||^2 + \sum_{k=1}^{t} \langle \ell_t , y \rangle "/></p>
<p align="center"><img alt="\displaystyle  x'_{t+1} = \Pi_K (y_{t+1} ) = \arg\min_{x\in K} || x - y_{t+1} || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%27_%7Bt%2B1%7D+%3D+%5CPi_K+%28y_%7Bt%2B1%7D+%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x'_{t+1} = \Pi_K (y_{t+1} ) = \arg\min_{x\in K} || x - y_{t+1} || "/></p>
<p> and we claim that we always have <img alt="{x'_{t+1} = x_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27_%7Bt%2B1%7D+%3D+x_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x'_{t+1} = x_{t+1}}"/>. The fact that we can reduce a regularized constrained optimization problem to an unconstrained problem and a projection is part of a broader theory that we will describe in a later post. For now, we will limit to prove the equivalence in this specific setting. First of all, we already have an expression for <img alt="{y_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_{t+1}}"/>, namely </p>
<p align="center"><img alt="\displaystyle  y_{t+1} = - \frac 1{2c} \sum_{k=1}^t \ell_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+-+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{t+1} = - \frac 1{2c} \sum_{k=1}^t \ell_t "/></p>
<p> Now the definition of <img alt="{x'_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x'_{t+1}}"/> is </p>
<p align="center"><img alt="\displaystyle  x'_{t+1} = \arg\min_{x\in K} || x - y_{t+1} || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%27_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x'_{t+1} = \arg\min_{x\in K} || x - y_{t+1} || "/></p>
<p align="center"><img alt="\displaystyle  = \arg\min_{x\in K} || x - y_{t+1} ||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \arg\min_{x\in K} || x - y_{t+1} ||^2 "/></p>
<p align="center"><img alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle + || y_{t+1} ||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft+%5Clangle+x+%2C+y_%7Bt%2B1%7D+%5Cright%5Crangle+%2B+%7C%7C+y_%7Bt%2B1%7D+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle + || y_{t+1} ||^2 "/></p>
<p align="center"><img alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft+%5Clangle+x+%2C+y_%7Bt%2B1%7D+%5Cright%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left \langle x , y_{t+1} \right\rangle "/></p>
<p align="center"><img alt="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left\langle x , \frac 1{2c} \sum_{k=1}^t \ell_t \right\rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7Cx%7C%7C%5E2+-+2+%5Cleft%5Clangle+x+%2C+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5Et+%5Cell_t+%5Cright%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \arg\min_{x\in K} ||x||^2 - 2 \left\langle x , \frac 1{2c} \sum_{k=1}^t \ell_t \right\rangle "/></p>
<p align="center"><img alt="\displaystyle  = \arg\min_{x\in K} c ||x||^2 - \sum_{k=1}^t \left\langle x , \ell_t \right\rangle = x_{t+1} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+c+%7C%7Cx%7C%7C%5E2+-+%5Csum_%7Bk%3D1%7D%5Et+%5Cleft%5Clangle+x+%2C+%5Cell_t+%5Cright%5Crangle+%3D+x_%7Bt%2B1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \arg\min_{x\in K} c ||x||^2 - \sum_{k=1}^t \left\langle x , \ell_t \right\rangle = x_{t+1} "/></p>
<p>
In order to bound the regret, we have to compute </p>
<p align="center"><img alt="\displaystyle  f_t(x_t) - f_t(x_{t+1} ) = \langle \ell_t , x_t - x_{t+1} \rangle \leq || \ell_t || \cdot ||x_t - x_{t+1} || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x_t%29+-+f_t%28x_%7Bt%2B1%7D+%29+%3D+%5Clangle+%5Cell_t+%2C+x_t+-+x_%7Bt%2B1%7D+%5Crangle+%5Cleq+%7C%7C+%5Cell_t+%7C%7C+%5Ccdot+%7C%7Cx_t+-+x_%7Bt%2B1%7D+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t(x_t) - f_t(x_{t+1} ) = \langle \ell_t , x_t - x_{t+1} \rangle \leq || \ell_t || \cdot ||x_t - x_{t+1} || "/></p>
<p> and since L2 projections cannot increase L2 distances, we have </p>
<p align="center"><img alt="\displaystyle  || x_t - x_{t+1} || \leq || y_t - y_{t+1} || = \frac 1 {2c} || \ell_t || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+x_t+-+x_%7Bt%2B1%7D+%7C%7C+%5Cleq+%7C%7C+y_t+-+y_%7Bt%2B1%7D+%7C%7C+%3D+%5Cfrac+1+%7B2c%7D+%7C%7C+%5Cell_t+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || x_t - x_{t+1} || \leq || y_t - y_{t+1} || = \frac 1 {2c} || \ell_t || "/></p>
<p>
So the regret bound is </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq c ||x^*||^2 - c|| x_1||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t ||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+c+%7C%7Cx%5E%2A%7C%7C%5E2+-+c%7C%7C+x_1%7C%7C%5E2+%2B+%5Cfrac+1+%7B2c%7D+%5Csum_%7Bt%3D1%7D%5ET+%7C%7C+%5Cell_t+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq c ||x^*||^2 - c|| x_1||^2 + \frac 1 {2c} \sum_{t=1}^T || \ell_t ||^2 "/></p>
<p> If <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is an upper bound to <img alt="{\max_{x\in K} || x||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bx%5Cin+K%7D+%7C%7C+x%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\max_{x\in K} || x||}"/>, and <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> is an upper bound to the norm <img alt="{|| \ell_t ||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+%5Cell_t+%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|| \ell_t ||}"/> of all the loss vectors, then</p>
<p/><p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq c D^2 + \frac 1 {2c} T L^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+c+D%5E2+%2B+%5Cfrac+1+%7B2c%7D+T+L%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq c D^2 + \frac 1 {2c} T L^2 "/></p>
<p> which can be optimized to </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq DL \sqrt {2T} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+DL+%5Csqrt+%7B2T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq DL \sqrt {2T} "/></p>
<p>
</p><p><b>  3.2. Deriving the Analysis of Gradient Descent </b></p>
<p/><p>
Suppose that <img alt="{g: K \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g: K \rightarrow {\mathbb R}}"/> is a convex function whose gradient <img alt="{\nabla g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\nabla g}"/> is well defined at all points in <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>, and that we are interested in minimizing <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>. Then a way to reduce this problem to online optimization would be to use the function <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> as loss function at each step. Then the offline optimum would be the minimizer of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>, and achieving small regret means that <img alt="{\frac 1T \sum_t g(x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1T+%5Csum_t+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac 1T \sum_t g(x_t)}"/> is close to the minimum of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>, and so the best <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> is an approximate minimizer.</p>
<p>
Unfortunately, this is not a very helpful idea, because if we ran an FTRL algorithm against an adversary that keeps proposing <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> as a cost function at each step then we would have </p>
<p align="center"><img alt="\displaystyle  x_{t+1} = \arg\min_{x\in K} R(x) + t \cdot g(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+R%28x%29+%2B+t+%5Ccdot+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} = \arg\min_{x\in K} R(x) + t \cdot g(x) "/></p>
<p> which, for large <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, is essentially the same problem as minimizing <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>, so we have basically reduced the problem of minimizing <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> to itself.</p>
<p>
Indeed, the power of the FTRL algorithm is that the algorithm does well even though it does not know the cost function, and if we keep using the same cost function at each step we are not making a good use of its power. Now, suppose that we use cost functions <img alt="{f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t}"/> such that </p>
<ul>
<li> <img alt="{f_t(x_t) = g(x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%3D+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x_t) = g(x_t)}"/>
</li><li> <img alt="{\forall x\in K \ \ f_t(x) \leq g(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cforall+x%5Cin+K+%5C+%5C+f_t%28x%29+%5Cleq+g%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\forall x\in K \ \ f_t(x) \leq g(x)}"/>
</li></ul>
<p> Then, after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps, we have </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T g(x_t) = \sum_{t=1}^T f_t(x_t) = {\rm Regret}_T + \min{x\in K} \sum_{t=1}^T f_t(x) \leq {\rm Regret}_T + \min_{x\in K} \sum_{t=1}^T g (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%3D+%7B%5Crm+Regret%7D_T+%2B+%5Cmin%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cleq+%7B%5Crm+Regret%7D_T+%2B+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+g+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^T g(x_t) = \sum_{t=1}^T f_t(x_t) = {\rm Regret}_T + \min{x\in K} \sum_{t=1}^T f_t(x) \leq {\rm Regret}_T + \min_{x\in K} \sum_{t=1}^T g (x) "/></p>
<p> meaning </p>
<p align="center"><img alt="\displaystyle  \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%5Cleq+%5Cfrac+%7B%7B%5Crm+Regret%7D_T%7DT+%2B+%5Cmin_%7Bx%5Cin+K%7D+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) "/></p>
<p> and so one of the <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> is an approximate minimizer. Indeed, using convexity, we also have </p>
<p align="center"><img alt="\displaystyle  g \left( \frac 1T \sum_{t=1}^T x_t \right) \leq \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cleft%28+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+x_t+%5Cright%29+%5Cleq+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+g%28x_t%29+%5Cleq+%5Cfrac+%7B%7B%5Crm+Regret%7D_T%7DT+%2B+%5Cmin_%7Bx%5Cin+K%7D+g%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g \left( \frac 1T \sum_{t=1}^T x_t \right) \leq \frac 1T \sum_{t=1}^T g(x_t) \leq \frac {{\rm Regret}_T}T + \min_{x\in K} g(x) "/></p>
<p> and so the average of the <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> is also an approximate minimizer. From the point of view of exploiting FTRL do to minimize <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>, cost functions <img alt="{f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t}"/> as above work just as well as presenting <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> as a cost functions at each step.</p>
<p>
How do we find cost functions that satisfy the above two properties and for which the FTRL algorithm is easy to implement? The idea is to let <img alt="{f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t}"/> be the linear approximation of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> at <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/>: </p>
<p align="center"><img alt="\displaystyle  f_t (x) := g(x_t) + \langle \nabla g (x_t), x - x_t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+g%28x_t%29+%2B+%5Clangle+%5Cnabla+g+%28x_t%29%2C+x+-+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t (x) := g(x_t) + \langle \nabla g (x_t), x - x_t \rangle "/></p>
<p> The <img alt="{f_t(x_t) = g(x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%3D+g%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x_t) = g(x_t)}"/> condition is immediate, and </p>
<p align="center"><img alt="\displaystyle  g(x) \geq f_t (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%5Cgeq+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(x) \geq f_t (x) "/></p>
<p> is a consequence of the convexity of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>.</p>
<p>
The cost functions that we have defined are affine functions, that is, each of them equals a constant plus a linear function </p>
<p align="center"><img alt="\displaystyle  f_t(x) = \left( g(x_t) - \langle \nabla g(x_t) , x_t\rangle \right) + \langle \nabla g(x_t) , x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t%28x%29+%3D+%5Cleft%28+g%28x_t%29+-+%5Clangle+%5Cnabla+g%28x_t%29+%2C+x_t%5Crangle+%5Cright%29+%2B+%5Clangle+%5Cnabla+g%28x_t%29+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t(x) = \left( g(x_t) - \langle \nabla g(x_t) , x_t\rangle \right) + \langle \nabla g(x_t) , x \rangle "/></p>
<p>
Adding a constant term to a cost function does not change the iteration of FTRL, and does not change the regret (because the same term is added both to the solution found by the algorithm and to the offline optimum), so the algorithm is just initialized with</p>
<p/><p align="center"><img alt="\displaystyle  y_1 = {\bf 0} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_1+%3D+%7B%5Cbf+0%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_1 = {\bf 0} "/></p>
<p align="center"><img alt="\displaystyle  x_1 = \Pi_K({\bf 0}) = \arg\min_{x\in K} || x|| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5CPi_K%28%7B%5Cbf+0%7D%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_1 = \Pi_K({\bf 0}) = \arg\min_{x\in K} || x|| "/></p>
<p> and then continues with the update rules </p>
<p align="center"><img alt="\displaystyle  y_{t+1} =y_t -\frac 1 {2c} \nabla g (x_t) \mbox{ for } t \geq 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3Dy_t+-%5Cfrac+1+%7B2c%7D+%5Cnabla+g+%28x_t%29+%5Cmbox%7B+for+%7D+t+%5Cgeq+1&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{t+1} =y_t -\frac 1 {2c} \nabla g (x_t) \mbox{ for } t \geq 1"/></p>
<p align="center"><img alt="\displaystyle  x_{t+1} = \Pi_K(y_{t+1}) = \arg\min_{x\in K} || x - y_{t+1} || \mbox{ for } t \geq 1 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5CPi_K%28y_%7Bt%2B1%7D%29+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_%7Bt%2B1%7D+%7C%7C+%5Cmbox%7B+for+%7D+t+%5Cgeq+1+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} = \Pi_K(y_{t+1}) = \arg\min_{x\in K} || x - y_{t+1} || \mbox{ for } t \geq 1 "/></p>
<p> which is just projected gradient descent.</p>
<p>
If we have known upper bounds </p>
<p align="center"><img alt="\displaystyle  \forall x \in K \ \ || \nabla g(x) || \leq L " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cin+K+%5C+%5C+%7C%7C+%5Cnabla+g%28x%29+%7C%7C+%5Cleq+L+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall x \in K \ \ || \nabla g(x) || \leq L "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \forall x \in K \ \ || x || \leq D " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cin+K+%5C+%5C+%7C%7C+x+%7C%7C+%5Cleq+D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall x \in K \ \ || x || \leq D "/></p>
<p> then we have </p>
<p align="center"><img alt="\displaystyle  g \left( \frac 1 T \sum_{t=1}^T x_t \right ) \leq DL \cdot \sqrt{\frac 2 T} + \min_{x\in K} \sum_{t=1}^T g (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g+%5Cleft%28+%5Cfrac+1+T+%5Csum_%7Bt%3D1%7D%5ET+x_t+%5Cright+%29+%5Cleq+DL+%5Ccdot+%5Csqrt%7B%5Cfrac+2+T%7D+%2B+%5Cmin_%7Bx%5Cin+K%7D+%5Csum_%7Bt%3D1%7D%5ET+g+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g \left( \frac 1 T \sum_{t=1}^T x_t \right ) \leq DL \cdot \sqrt{\frac 2 T} + \min_{x\in K} \sum_{t=1}^T g (x) "/></p>
<p> which means that to achieve additive error <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> it is enough to proceed for <img alt="{2D^2L^2 / \epsilon^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2D%5E2L%5E2+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2D^2L^2 / \epsilon^2}"/> steps. </p></div>
    </content>
    <updated>2019-05-06T14:05:54Z</updated>
    <published>2019-05-06T14:05:54Z</published>
    <category term="theory"/>
    <category term="follow the leader"/>
    <category term="follow the regularized leader"/>
    <category term="gradient descent"/>
    <category term="online optimization"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-16T01:20:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=628</id>
    <link href="https://emanueleviola.wordpress.com/2019/05/05/e-ink-on-the-move/" rel="alternate" type="text/html"/>
    <title>E-ink on the move</title>
    <summary>Today I was overjoyed to notice that the MBTA is installing e-ink signs. I didn’t know about this when I wrote in the previous post that the market for e-ink monitors will be huge. I was actually about to report more on my experience, and by another standard coincidence today a reader asks: Some time […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: justify;">Today I was overjoyed to notice that the <a href="https://www.mbta.com/projects/solar-powered-e-ink-signs">MBTA is installing e-ink signs</a>. I didn’t know about this when <a href="https://emanueleviola.wordpress.com/2019/03/07/a-dream-come-true-sort-of-e-ink-monitors/">I wrote in the previous post</a> that the market for e-ink monitors will be huge.</p>
<p style="text-align: justify;">I was actually about to report more on my experience, and by another standard coincidence today a reader asks:</p>
<blockquote><p>Some time have passed, is your evaluation the same? Did you come across any unexpected difficulties?</p></blockquote>
<p style="text-align: justify;">Well, I wrote a <a href="http://www.ccs.neu.edu/home/viola/papers/tm.pdf">paper</a> entirely in e-ink. But I regret to admit that towards the end of the semester I got really busy with the usual end-of-Spring matters at the university, and I switched back to my back-lit 30-inch Dell monitor.  I had to interact with a number of computer systems where I could not easily change font size (the story of my life), and where color tended to matter, and I felt that the new monitor was slowing me down.  I haven’t switched back to the e-ink monitor yet, partly because I am still recovering from the burst.</p>
<p style="text-align: justify;">However I look forward to using the e-ink monitor more during this summer, especially outdoors.  Here the fact that it’s usb powered will be essential.  In the MBTA project they use solar power which I think is really cool and makes me think of bringing my monitor to the secluded off-the-grid cabin in Maine I don’t have.</p></div>
    </content>
    <updated>2019-05-06T00:40:40Z</updated>
    <published>2019-05-06T00:40:40Z</published>
    <category term="Uncategorized"/>
    <category term="health"/>
    <category term="tech"/>
    <author>
      <name>Emanuele</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2019-05-16T01:22:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://gradientscience.org/adv/</id>
    <link href="http://gradientscience.org/adv/" rel="alternate" type="text/html"/>
    <title>Adversarial Examples Are Not Bugs, They Are Features</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/1905.02175" style="float: left;">
<i class="fas fa-file-pdf"/>
    Read the paper
</a>
<a class="bbutton" href="http://git.io/adv-datasets" style="float: right;">
<i class="fab fa-github"/>
   Download the datasets
</a></p>

<p>Over the past few years, adversarial examples – or inputs that have been slightly perturbed by an adversary to cause unintended behavior in machine learning systems – have received significant attention in the machine learning community (for more background, read our introduction to adversarial examples <a href="https://gradientscience.org/intro_adversarial">here</a>). There has been much work on training models that are not vulnerable to adversarial examples (in previous posts, we discussed methods for training robust models: <a href="https://gradientscience.org/robust_opt_pt1/">part 1</a>, <a href="https://gradientscience.org/robust_opt_pt2/">part 2</a>, but all this research does not really confront the fundamental question: <i>why</i> do these adversarial examples arise in the first place?</p>

<p>So far, the prevailing view has been that adversarial examples stem from “quirks” of the models that will eventually disappear once we make enough progress towards better training algorithms and larger scale data collection. Common views include adversarial examples being either a consequence of the input space being high-dimensional (e.g. <a href="https://arxiv.org/abs/1801.02774">here</a>) or attributed to finite-sample phenomena (e.g. <a href="https://arxiv.org/abs/1608.07690">here</a> or <a href="https://arxiv.org/abs/1804.11285">here</a>).</p>

<p>Today we will discuss our <a href="https://gradientscience.org/adv.pdf">recent work</a> that provides a new perspective on the reasons for adversarial examples arise. However, before we dive into the details, let us first tell you a short story:</p>

<h2 id="a-planet-called-erm">A Planet called <i>Erm</i></h2>

<p>Our tale begins on <i><span class="sc">Erm</span></i>, a distant planet inhabited by members of an ancient alien race known as <i>Nets</i>. The Nets are a strange species; each individual’s place in the social hierarchy is determined by their ability to classify bizarre 32-by-32 pixel images (which are meaningless to the Nets) into ten completely arbitrary categories. These images are drawn from a top-secret dataset, <span class="sc">See-Far</span>—outside of looking at those curious pixelated images, the Nets live their lives totally blind.</p>

<p>As Nets grow older and wiser they begin to discover more and more tell-tale <i>patterns</i> in <span class="sc">See-Far</span>. Each new pattern that an alien discovers helps them classify the dataset even more accurately. Due to the immense societal value of increased classification accuracy, the aliens have names for the most predictive image patterns:</p>

<p><img alt="A Toogit, highly indicative of a '1' image." src="https://gradientscience.org/images/featsnotbugs/toogit.png" style="width: 30%;"/></p>
<div class="footnote">
        A TOOGIT, highly indicative of a "1" image.
        Nets are extremely sensitive to TOOGITs.
</div>

<p>The most powerful aliens were remarkably adept at spotting patterns, and thus were extremely sensitive to their presence in <span class="sc">See-Far</span> images.</p>

<p>Somehow (perhaps looking for <span class="sc">See-Far</span> classification tips), some of the aliens obtain access to a <i>human-written</i> machine learning paper. One figure in
particular caught the aliens’ eye:</p>

<p>
<img alt="An quote-unquote adversarial example?" src="https://gradientscience.org/images/featsnotbugs/bagaboop.png"/>
</p><div class="footnote">
An "adversarial example"?
</div>
<p/>

<p>The figure was relatively simple, they thought: on the left was a “2”, in the middle there was a GAB pattern, which was known to indicate “4”—unsurprisingly, adding a GAB to the image on the left resulted in a new image, <i>which looked (to the Nets) exactly like an image corresponding to the “4” category</i>.</p>

<p>The Nets could not understand why, according to the paper, the original and final images, being completely different, should be identically classified. Confused, the Nets flipped on through the manuscript, wondering what other useful patterns humans were oblivious to$\ldots$</p>

<h2 id="what-we-can-learn-from-erm">What we can learn from <i>Erm</i></h2>

<p>As the names may suggest, this story is not merely one of aliens and their curious social constructs: the way Nets develop is reminiscent of how we train machine learning models. In particular, we maximize accuracy without incorporating much prior context about classified classes, the physical world, or other human-related concepts. The result in the story is that the aliens are able to realize that what humans think of as meaningless adversarial perturbation are actually patterns crucial to <span class="sc">See-Far</span> classification. The tale of the Nets should thus make us wonder:</p>

<p><i>Are adversarial perturbations really unnatural and meaningless?</i></p>

<h3 id="a-simple-experiment">A simple experiment</h3>
<p>To investigate this issue, let us first perform a simple experiment:</p>

<ul>
  <li>We start with an image from the training set of a standard dataset (e.g. CIFAR10):</li>
</ul>

<p><img alt="An image from the training set" src="https://gradientscience.org/images/featsnotbugs/train.png" style="width: 50%;"/></p>

<ul>
  <li>We synthesize a targeted adversarial example (on a standard pre-trained model) from each (x, y) towards the “next” class y+1 (or 0, if y is the last class):</li>
</ul>

<p><img alt="An adversarial perturabtion to the next class" src="https://gradientscience.org/images/featsnotbugs/adv.png"/></p>

<ul>
  <li>We then construct a new training set, by labeling these adversarial examples with their corresponding target class:</li>
</ul>

<p><img alt="A new training set based on the perturbation" src="https://gradientscience.org/images/featsnotbugs/newtrain.png" style="width: 35%;"/></p>

<p>Now, the resulting training set is imperceptibly perturbed from the original, but the labels have been changed—as such, it looks <i>completely</i> mislabeled to a human. In fact, the mislabelings are even consistent with a “permuted” hypothesis (i.e. every dog is labeled as a cat, every cat as a bird, etc.).</p>

<p>We train a new classifier (not necessarily with the same architecture as the first) on the “mislabeled dataset.” How will this classifier perform on the  <em>original (unaltered) test set</em> (i.e. the standard CIFAR-10 test set)?</p>

<p>Remarkably, we find that the resulting classifier actually has moderate accuracy (e.g. 44% for CIFAR)! This is despite the fact that training inputs are associated with their “true” labels <em>solely through imperceptible perturbations</em>, and are associated with a different (now incorrect) label matching through <i>all</i> visible features.</p>

<p>What’s going on here?</p>

<h2 id="our-conceptual-model-for-adversarial-examples">Our Conceptual Model for Adversarial Examples</h2>

<p>The experiment we just described establishes adversarial perturbations of standard models as patterns predictive of the target class in a <i>well-generalizing</i> sense. That is, adversarial perturbations in the training set alone allowed moderately accurate predictions on the test set. In this light, one might wonder: perhaps these patterns are <i>not</i> fundamentally different from what humans use to classify images (e.g. ears, whiskers, snouts)! This is precisely our hypothesis—there exist a variety of features of the input that are predictive of the label, and only some of these are perceptible to humans.</p>

<p>More precisely, we posit that predictive features of the data can be split into “robust” and “non-robust” features. Robust features correspond to patterns that are predictive of the true label <em>even when adversarially perturbed</em> (e.g. the presence of “fur” in an image) under some pre-defined (and crucially human-defined) perturbation set, e.g. the $\ell_2$ ball. Conversely, non-robust features correspond to patterns that while predictive, can be “flipped” by an adversary within a pre-defined perturbation set to be indicate a wrong class (see <a href="https://gradientscience.org/adv.pdf">our paper</a> for a formal definition).</p>

<p>Since we always only consider perturbation sets that do not affect human classification performance,  we expect humans to rely solely on robust features. However, when the goal is maximizing (standard) test-set accuracy, non-robust features can be just as useful as robust ones–in fact, the two types of features are completely interchangeable. This is illustrated in the following figure:</p>

<p><img alt="" src="https://gradientscience.org/images/featsnotbugs/model.png"/></p>

<p>From this perspective, our experiment describes something quite simple. In the original training set, both the robust and non-robust features of the input are predictive of the label. When we make a small adversarial perturbation, we cannot significantly affect the robust features (essentially by definition), but  we can still flip <i>non-robust features</i>. For instance, every dog image now retains the robust features of a dog (and thus appears to us to be a dog), but has non-robust features of a cat. After the training set is relabelled, we make it so that the robust features actually point in the <i>wrong direction</i> (i.e. the pictures with robust “dog” features are labeled as cats) and hence only the non-robust features actually provide correct guidance for generalization.</p>

<p>In summary, both robust and non-robust features are predictive on the training set, but <i>only non-robust features will yield generalization to the original test set</i>:</p>

<p><img alt="" src="https://gradientscience.org/images/featsnotbugs/diagram.png"/></p>

<p>Thus, the fact that models trained on this dataset actually generalize to the standard test set indicates that (a) non-robust features exist and are sufficient for good generalization, and (b) deep neural networks indeed rely on these non-robust features, even in the presence of predictive robust features.</p>

<h2 id="do-robust-models-learn-robust-features">Do Robust Models Learn Robust Features?</h2>

<p>Our experiments establish that adversarial perturbations are not meaningless artifacts but actually correspond directly to perturbing features that are crucial to generalization. At the same time, our blog posts about adversarial examples (<a href="https://gradientscience.org/robust1">here</a> and <a href="https://gradientscience.org/robust2">here</a>) showed that by using robust optimization, we can get models that are more robust to adversarial examples.</p>

<p>So a natural question to ask is: can we verify that robust models actually rely on <i>robust</i> features? To test this, we establish a methodology for restricting (in a best-effort sense) inputs to only the features a model is sensitive to (for deep neural networks, features correspond to the penultimate layer activations). Using this method, we create a new training set that is restricted to only contain the features that an already-trained robust model utilizes:</p>

<p><img alt="" src="https://gradientscience.org/images/featsnotbugs/frog.png"/></p>

<p>We then train a model on the resulting dataset  <em>without</em> adversarial training and find that the resulting model has non-trivial accuracy <em>and</em> robustness! This is in stark contrast to training on the standard training set which leads to models that are accurate yet completely brittle.</p>

<p><img alt="" src="https://gradientscience.org/images/featsnotbugs/CIFAR_res.png"/></p>
<div class="footnote">
Standard and robust accuracy, tested on CIFAR-10 test set ($\mathcal{D}$). Training: <br/>
<strong>Left</strong>: training normally on CIFAR-10 ($\mathcal{D}$) <br/>
<strong>Middle</strong>: training adversarially on CIFAR-10 ($\mathcal{D}$) <br/>
<strong>Right</strong>: training normally on constructed dataset ($\widehat{\mathcal{D}}_R$)
</div>

<p>Our results thus indicate that robustness (and in turn non-robustness) can in fact arise as a property of the dataset itself. In particular, when we remove non-robust features from the original training set, we can get robust models just via standard (non-adversarial) training. This is further evidence that adversarial examples arise as a result of non-robust features and are not necessarily tied to the standard training framework.</p>

<h2 id="transferability">Transferability</h2>

<p>An immediate consequence of this change in perspective is that the <i>transferability</i> of adversarial examples (the thus-far mysterious phenomenon that perturbations for one model are often adversarial for others) no longer requires a separate explanation. Specifically, now that we view adversarial vulnerability as a direct product of the <i>features derived from the dataset</i> (as opposed to quirks in the training of individual models), we would expect similarly expressive models to be able to find and use these features to improve their classification accuracy too.</p>

<p>To further explore this idea, we study how the tendency of different architectures to learning similar non-robust features relates to the transferability of adversarial examples between them:</p>

<p><img alt="" src="https://gradientscience.org/images/featsnotbugs/transfer.png"/>
In the above, we generate the dataset that we described in our very first experiment (a training set of adversarial examples labeled with the target class), using a ResNet-50 to construct the adversarial examples. We can think of the resulting dataset as having all of the ResNet-50’s non-robust features “flipped” to the target class. We then train the five architectures shown above on this dataset, and record their generalization performance on the true test set: this corresponds to how well the architecture is able to generalize using only the non-robust features from a ResNet-50.</p>

<p>When we analyze the results we see that, as our new view of adversarial examples suggests, models’ ability to pick up the non-robust features introduced by the ResNet-50 correlates extremely well with the adversarial transferability from ResNet-50 to standard models of each architecture.</p>

<h2 id="implications">Implications</h2>

<p>Our discussion and experiments establish adversarial examples as a purely human-centric phenomenon. From the perspective of classification performance there is no reason for a model to prefer robust over non-robust features. After all, the notion of robustness is human-specified. Hence, if we want our models to rely mostly on robust features we need to explicitly account for that by incorporating priors into architecture or training process. From that perspective, adversarial training (and more broadly robust optimization) can be thought of as a tool to incorporate desired invariances into the learned model. For example, robust training can be viewed as attempting to undermine the predictiveness of non-robust features by constantly “flipping” them, and thus steering the trained model away from relying on them.</p>

<p>At the same time, the reliance of standard models on non-robust (and hence incomprehensible to humans) features needs to be accounted for when designing interpretability methods. In particular, any “explanation” of a standardly trained model’s prediction should either highlight such non-robust features (and hence not be fully human-meaningful) or hide them (and hence not be fully faithful to the model’s decision process). Therefore, if we want interpretability methods that are both human-meaningful and faithful, resorting only to post-training processing is fundamentally insufficient: one needs to intervene <i>at</i> training time.</p>

<h2 id="more-in-the-paper">More in the Paper</h2>

<p>In <a href="https://gradientscience.org/adv.pdf">our paper</a>, we also describe a precise framework for discussing robust and non-robust features, further experiments corroborating our hypothesis, and a theoretical model under which we can study the dynamics of robust training in the presence of non-robust features.</p></div>
    </summary>
    <updated>2019-05-06T00:00:00Z</updated>
    <published>2019-05-06T00:00:00Z</published>
    <source>
      <id>http://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="http://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="http://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2019-05-15T23:36:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1116</id>
    <link href="https://ptreview.sublinear.info/?p=1116" rel="alternate" type="text/html"/>
    <title>News for April 2019</title>
    <summary>After a quite slow month of March, things sped up quite significantly in April: six different papers, ranging from graph testing to function testing to quantum distribution testing! Update (05/04): We missed one. Seven! Junta correlation is testable, by Anindya De, Elchanan Mossel, and Joe Neeman (arXiv). Junta testing really has seen a lot of […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>After a quite slow month of March, things sped up quite significantly in April: six different papers, ranging from graph testing to function testing to quantum distribution testing!</p>



<p><strong>Update (05/04): </strong>We missed one. Seven!</p>



<p><strong>Junta correlation is testable</strong>, by Anindya De, Elchanan Mossel, and Joe Neeman (<a href="https://arxiv.org/abs/1904.04216">arXiv</a>).  Junta testing really has seen <a href="https://ptreview.sublinear.info/?s=junta">a lot of attention and progress</a> over the pass couple years! In this new work, the focus is on the <em>tolerant</em> version of (Boolean) junta testing, where the goal is to decide whether a given function \(f\colon \{-1,1\}^n\to \{-1,1\}\) is close to some \(k\)-junta, or far from any such simple function. The current paper improves on previous work of Blais, Canonne, Eden, Levi, and Ron, which showed how to distinguish between \(\varepsilon\)-close to \(k\)-junta and \(16\varepsilon\)-far from \(k\)-junta in \(O_{k,\varepsilon}(1)\) queries, in two ways: (i) the gap-factor of 16 can now be made arbitrarily small, yielding the first <em>fully</em> tolerant non-trivial junta tester; and (ii) the new algorithm also identifies the underlying “core junta” (up to permutation of the coordinates). Besides the other results contained in the paper (such as results for a relaxation of the tolerant question akin to that of Blais et al.), a key aspect of this paper is to go beyond the use of (set) influence as a proxy for distance to junta-ness which underlied all previous work, thus potentially opening a new avenue towards get fully tolerant, \(\mathrm{poly}(k,1/\varepsilon)\)-query junta testing algorithms.</p>



<p><strong>Testing Tensor Products</strong>, by Irit Dinur and Konstantin Golubev (<a href="https://arxiv.org/abs/1904.12747">arXiv</a>). In this paper, the authors consider the following testing question: given query access to a Boolean function \(f\colon [n]^d \to \mathbb{F}_2\), test whether \(f\) is of the form \(f(x) = f_1(x_1)+\dots f_d(x_d)\) (equivalently, this is the task of testing whether a \(d\)-dimensional tensor with \(\pm 1\)  is of rank \(1\)). They provide two different proximity-oblivious testers (POT) for this task: a \(4\)-query one, reminiscent and building upon the BLR linearity test; as well as a \((d+1)\)-query POT <em>(which they dub “Shapka Test,” one can assume for their love of warm and comfortable hats)</em>, whose analysis is significantly simpler.</p>



<p>Changing direction, the next paper we saw is concerned with quantum testing of (regular, good ol’ classical) probability distributions:</p>



<p><strong>Quantum Algorithms for Classical Probability Distributions</strong>, by Aleksandrs Belovs (<a href="https://arxiv.org/abs/1904.02192">arXiv</a>). This work on quantum distribution testing considers 4 of the existing models of access to samples from an unknown probability distribution, analyzes their relationship, and argues their merits. Further, the autho establishes that for the simple hypothesis testing question of distinguishing between two (known, fixed) probability distributions \(p,q\), in all four models the optimal sample complexity is proportional to the inverse of the Hellinger distance between  \(p\) and \(q\)—in contrast to the classical setting, where it is known to be proportional to the <em>squared</em> inverse of this distance.</p>



<p>Turning to graphs, the next work considers clustering of bounded-degree graphs, a topic we <a href="https://ptreview.sublinear.info/?p=1075">recently discussed as well</a>:</p>



<p><strong>Robust Clustering Oracle and Local Reconstructor of Cluster Structure of Graphs</strong>, by Pan Peng (<a href="https://arxiv.org/abs/1904.09710">arXiv</a>). Consider the following noisy model: an unknown bounded-degree graph (of maximum degree \(d\)) \(G\) which is \((k, \phi_{\rm in},\phi_{\rm out})\)-clusterable (i.e., clusterable in at most \(k\) clusters of inner and outer conductances bounded by \(\phi_{\rm in},\phi_{\rm out}\)) is adversarially modified in at most \($\varepsilon d n\) edges between clusters. This noise model, introduced in this work, leads to the natural questions of “recovering the underlying graph \(G\)”: this is what the author tackles, by designing sublinear-time <em>local clustering oracles</em> and <em>local reconstruction algorithms</em>  (local filters) in this setting. Further, in view of the noise model reminiscent of property testing in the bounded-degree graph setting, connections to testing clusterability are discussed; the implications of the results for testing clusterability are discussed in Section 1.4.</p>



<p><strong>A Faster Local Algorithm for Detecting Bounded-Size Cuts with Applications to Higher-Connectivity Problems</strong>, by Sebastian Forster and Liu Yang (<a href="https://arxiv.org/abs/1904.08382">arXiv</a>). The authors focus on the problem of finding an edge (or vertex) cut in a given graph from a <em>local</em> viewpoint, i.e., in the setting of local computation algorithms, and provide a slew of results in detecting bounded-size cuts. This in turn has direct implications for testing \(k\)-edge and \(k\)-vertex connectivity, directed and undirected, in both the bounded-degree and general (unbounded degree) models, improving on or subsuming the current state-of-the-art across the board  <em>(see Section 1.2.4 of the paper, and the very helpful Tables 3 and 4, for a summary of the improvements)</em>.</p>



<p><strong>Random walks and forbidden minors II: A \(\mathrm{poly}(d\varepsilon^−1)\)-query tester for minor-closed properties of bounded degree graphs</strong>, by Akash Kumar, C. Seshadhri, and Andrew Stolman (<a href="https://eccc.weizmann.ac.il/report/2019/046/">ECCC</a>). Following their <a href="https://ptreview.sublinear.info/?p=999">breakthrough from last May</a>, the authors are at it again! Leveraging a random-walk approach, they resolve the following open question in testing bounded-degree graph: <em>is there a \(\mathrm{poly}(1/\varepsilon)\)-query tester for planarity (and, more generality, for minor-closed graph properties)? </em>The previous state-of-the-art, due to Levi and Ron, had a quasipolynomial dependence on \(1/\varepsilon\).<br/>Without spoiling too much: the answer to the open question is <em>yes—</em> and further the authors settle it by showing an even more general result on testing \(H\)-freeness (for any constant-size graph \(H\)).</p>



<p><strong>Update (05/04): </strong></p>



<p><strong>Testing Unateness Nearly Optimally</strong>, by Xi Chen and Erik Waingarten (<a href="https://arxiv.org/abs/1904.05309">arXiv</a>). Recall that a function \(f\colon \{0,1\}^n\to \{0,1\}\) is said to be unate if there exists some \(s\in\{0,1\}^n\) such that \(f(\cdot \oplus s)\) is monotone; i.e., if \(f\) is either non-increasing or non-decreasing in each coordinate. Testing unateness has seen a surge of interest over the past year or so; this work essentially settles the question, up to polylogarithmic factors in \(n\) (and the exact dependence on \(\varepsilon\)). Namely, the authors present and analyze an \(\tilde{O}(n^{2/3}/\varepsilon^2)\)-query adaptive tester for unateness, which nearly matches the \(\tilde{\Omega}(n^{2/3})\)-query lower bound for adaptive testers previously established by Chen, Waingarten, and Xie.  </p></div>
    </content>
    <updated>2019-05-04T06:22:27Z</updated>
    <published>2019-05-04T06:22:27Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2019-05-15T23:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4184</id>
    <link href="https://www.scottaaronson.com/blog/?p=4184" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4184#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4184" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">On the scientific accuracy of “Avengers: Endgame”</title>
    <summary xml:lang="en-US">[BY REQUEST: SPOILERS FOLLOW] Today Ben Lindbergh, a writer for The Ringer, put out an article about the scientific plausibility (!) of the time-travel sequences in the new “Avengers” movie. The article relied on two interviewees: (1) David Deutsch, who confirmed that he has no idea what the “Deutsch proposition” mentioned by Tony Stark refers […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>[BY REQUEST: SPOILERS FOLLOW]</p>



<p>Today Ben Lindbergh, a writer for <em>The Ringer</em>, put out <a href="https://www.theringer.com/movies/2019/5/3/18527776/marvel-avengers-endgame-time-travel-david-deutsch-proposition-scott-aaronson">an article</a> about the scientific plausibility (!) of the time-travel sequences in the new “Avengers” movie.  The article relied on two interviewees:</p>



<p>(1) David Deutsch, who confirmed that he has no idea what the “Deutsch proposition” mentioned by Tony Stark refers to but declined to comment further, and</p>



<p>(2) some quantum computing dude from UT Austin who had no similar scruples about spouting off on the movie.</p>



<p>To be clear, the UT Austin dude hadn’t even <em>seen</em> the movie, or any of the previous “Avengers” movies for that matter!  He just watched the clips dealing with time travel.  Yet Lindbergh still saw fit to introduce him as “a real-life [Tony] Stark without the vast fortune and fancy suit.”  Hey, I’ll take it.</p>



<p>Anyway, if you’ve seen the movie, and/or you know Deutsch’s causal consistency proposal for quantum closed timelike curves, and you can do better than I did at trying to reconcile the two, feel free to take a stab in the comments.</p></div>
    </content>
    <updated>2019-05-03T19:01:35Z</updated>
    <published>2019-05-03T19:01:35Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Embarrassing Myself"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-05-15T03:23:26Z</updated>
    </source>
  </entry>
</feed>
