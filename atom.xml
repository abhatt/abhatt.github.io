<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-07-27T18:21:55Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17349</id>
    <link href="https://rjlipton.wordpress.com/2020/07/27/a-brilliant-book-on-combinatorics/" rel="alternate" type="text/html"/>
    <title>A Brilliant Book on Combinatorics</title>
    <summary>And Razborov’s brilliant proof method Stasys Jukna is the author of the book Extremal Combinatorics With Applications in Computer Science. Today we talk about Jukna’s book on extremal combinatorics. The structure of his book is great. The material is useful and well presented. Rather than add more general comments about his book, we thought we […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>And Razborov’s brilliant proof method</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2020/07/jukna1.png"><img alt="" class="alignright size-full wp-image-17351" src="https://rjlipton.files.wordpress.com/2020/07/jukna1.png?w=600"/></a></p>
<p>
Stasys Jukna is the author of the <a href="https://www.google.com/books/edition/Extremal_Combinatorics/NV3Y8vjWo8kC?hl=en&amp;gbpv=1">book</a> <em>Extremal Combinatorics With Applications in Computer Science</em>. </p>
<p>
Today we talk about Jukna’s book on extremal combinatorics.</p>
<p>
The structure of his book is great. The material is useful and well presented. Rather than add more general comments about his book, we thought we might highlight one tiny part—the part on monotone circuit lower bounds. Here goes. All below is based directly on his discussion. Any errors or misguided comments are ours.</p>
<p>
</p><p/><h2> Monotone Boolean Functions </h2><p/>
<p/><p>
Fix an input size <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> and consider some property of subsets <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> of <img alt="{[n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[n]}"/>. Let <img alt="{f(S)=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28S%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(S)=1}"/> exactly when <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> has the property. We can think of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> as a Boolean function. You believe that this property is hard to compute—how do you go about proving that? </p>
<p>
In general we have no tools, but if the property is monotone, then there are some powerful methods. Recall <em>monotone</em> means that if <img alt="{f(S)=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28S%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(S)=1}"/> then any set <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> so that <img alt="{S \subset T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Csubset+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S \subset T}"/> still has the property. For example, <img alt="{f(S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(S)}"/> could be that <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> includes at least half of the elements of <img alt="{[n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[n]}"/>. It cannot be that <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> has an even number of elements. Another example is when <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> is given in <em>disjunctive normal form</em> (DNF), </p>
<p align="center"><img alt="\displaystyle  f \equiv T_1 \vee T_2 \vee \cdots T_m, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f+%5Cequiv+T_1+%5Cvee+T_2+%5Cvee+%5Ccdots+T_m%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f \equiv T_1 \vee T_2 \vee \cdots T_m, "/></p>
<p>where each <b>term</b> <img alt="{T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_k}"/> is a conjunction of variables. Each <img alt="{T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_k}"/> can be regarded as a subset of <img alt="{[n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[n]}"/>. Then <img alt="{f(S) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28S%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(S) = 1}"/> if and only if <img alt="{S \supseteq T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Csupseteq+T_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S \supseteq T_k}"/> for some <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>. Every monotone function also has a <em>conjunctive normal form</em> (CNF) </p>
<p align="center"><img alt="\displaystyle  f \equiv C_f = C_1 \wedge C_2 \wedge \cdots \wedge C_\ell, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f+%5Cequiv+C_f+%3D+C_1+%5Cwedge+C_2+%5Cwedge+%5Ccdots+%5Cwedge+C_%5Cell%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f \equiv C_f = C_1 \wedge C_2 \wedge \cdots \wedge C_\ell, "/></p>
<p>where each <b>clause</b> <img alt="{C_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_k}"/> is a disjunction of variables. Then <img alt="{f(S) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28S%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(S) = 1}"/> if and only if <img alt="{S \cap C_k \neq \emptyset}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Ccap+C_k+%5Cneq+%5Cemptyset%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S \cap C_k \neq \emptyset}"/> for <em>all</em> <img alt="{k.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k.}"/> The problem is that the numbers <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> of terms and <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell}"/> of clauses involved may be huge. The clauses may have different sizes. Given a CNF <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> of maximum clause size <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>, we write <img alt="{C^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C^s}"/> for the conjunction of clauses of size exactly <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> and <img alt="{C^{&lt;s}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%5E%7B%3Cs%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C^{&lt;s}}"/> for the rest. We similarly write <img alt="{D^r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D^r}"/> and <img alt="{D^{&lt;r}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%5E%7B%3Cr%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D^{&lt;r}}"/> for DNFs <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/>.</p>
<p>
The lower bound methods are on the size of a monotone circuit for <img alt="{f(S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(S)}"/>. That is the circuit can only use gates <img alt="{AND}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAND%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{AND}"/> and <img alt="{OR}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BOR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{OR}"/>, but no other types of gates, especially not <img alt="{NOT}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BNOT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{NOT}"/> gates. Of course, if <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> has no small monotone circuits, then it has no small DNF or CNF formulas either. </p>
<p>
The neat fact on which the lower-bound technique builds is that if <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> <b>does</b> have small monotone circuits, then we can “wrap” it between a CNF and a DNF in various customizable ways:</p>
<blockquote><p><b>Theorem 1 (informal)</b> <em><a name="informal"/> For every <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f}"/> with small monotone circuits and <img alt="{r,s &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%2Cs+%3E+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r,s &gt; 0}"/> we can find a CNF <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/> of maximum clause size <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{s}"/> and a DNF <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D}"/> of maximum term size <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r}"/> such that </em></p><em>
<p align="center"><img alt="\displaystyle  C \leq f \leq D \qquad\text{and also}\qquad D^{&lt;r} \leq C^{&lt;s}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cleq+f+%5Cleq+D+%5Cqquad%5Ctext%7Band+also%7D%5Cqquad+D%5E%7B%3Cr%7D+%5Cleq+C%5E%7B%3Cs%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  C \leq f \leq D \qquad\text{and also}\qquad D^{&lt;r} \leq C^{&lt;s}. "/></p>
</em><p><em>Moreover, <img alt="{C^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%5Es%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C^s}"/> and <img alt="{D^r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%5Er%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D^r}"/> are small. </em>
</p></blockquote>
<p/><p>
We have said “wrap” not “sandwich” because although <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is the “upper slice,” the part of <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> with smaller terms—but there could be many of them—wraps around to be under the corresponding part of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. This fact will enable us to throw away the smaller clauses and terms. How small is “small”? We will say later. We are trying to solve problems of exposition by keeping a high-level view at the start. </p>
<p>
</p><p/><h2> Exposition Problems </h2><p/>
<p/><p>
Tim Gowers has written an <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.587.8986&amp;rep=rep1&amp;type=pdf">article</a> about the lower method for monotone functions. The method is due to Alexander Razborov in his seminal 1985 <a href="http://people.cs.uchicago.edu/~razborov/files/clique.pdf">paper</a> and extended by Noga Alon and Ravi Boppana in their <a href="https://core.ac.uk/download/pdf/191378189.pdf">paper</a> right afterward, and by Benjamin Rossman in his 2009 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.5526&amp;rep=rep1&amp;type=pdf">paper</a>, to name a few. </p>
<p>
Gowers says right away that the original papers on this method are clear and well written. But he believes that there is need for more exposition. The method is so important that it must be made easy for all to understand. He says his article is an attempt to solve an <i>open exposition problem</i>. The notion of an exposition problem is due to Timothy Chow who <a href="https://arxiv.org/pdf/0712.1320.pdf">wrote</a>:</p>
<blockquote><p><b> </b> <em> All mathematicians are familiar with the concept of an open research problem. I propose the less familiar concept of an open exposition problem. </em>
</p></blockquote>
<p/><p>
Chow raised this issue with respect to the forcing method in set theory due to Paul Cohen. A modest suggestion: Read Chow on forcing, a great exposition; read Gowers on the monotone lower bound method, another great one. Both are much better than anything we can do. But we will put our own spin on the lower bound method. And hope to add to the quest to solve the exposition problem. </p>
<p/><h2> The Method—High Level </h2><p/>
<p/><p>
Suppose that <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> is a monotone boolean circuit that has <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> inputs and computes <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/> at the last gate. The method is called the <i>approximation method</i> because the idea is that it builds two other boolean functions <img alt="{\mathsf{lower}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Blower%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{lower}}"/> and <img alt="{\mathsf{upper}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bupper%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{upper}}"/>: for all <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> in <img alt="{\{0,1\}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1\}^{n}}"/>: </p>
<p align="center"><img alt="\displaystyle  \mathsf{lower}(x) \le f(x) \le \mathsf{upper}(x). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7Blower%7D%28x%29+%5Cle+f%28x%29+%5Cle+%5Cmathsf%7Bupper%7D%28x%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{lower}(x) \le f(x) \le \mathsf{upper}(x). "/></p>
<p>This follows a tradition in math that we often replace a complex function, <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/>, with simpler upper and lower bounds. Standard stuff. </p>
<p>
Usually the point is that the approximators are not only easier to understand but also simpler in some objective sense. For example, Christophe Chesneau and Yogesh Bagul give a nice short <a href="https://hal.archives-ouvertes.fr/hal-01934571/document">compendium</a> of approximating formulas involving trigonometric functions by formulas without them, including that for all <img alt="{0&lt;x&lt;1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%3Cx%3C1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0&lt;x&lt;1}"/>, </p>
<p align="center"><img alt="\displaystyle  \exp(-bx^{2}) &lt; \sin(x)/x &lt; \exp(-x^{2}/6), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cexp%28-bx%5E%7B2%7D%29+%3C+%5Csin%28x%29%2Fx+%3C+%5Cexp%28-x%5E%7B2%7D%2F6%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \exp(-bx^{2}) &lt; \sin(x)/x &lt; \exp(-x^{2}/6), "/></p>
<p>with <img alt="{b \approx 0.172604}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb+%5Capprox+0.172604%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b \approx 0.172604}"/>. If you have to reason about the behavior of <img alt="{\sin(x)/x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csin%28x%29%2Fx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sin(x)/x}"/>, it is nice to have these upper and lower bounds. Note that the upper bound kind-of wraps around because it is the same kind of function as the lower bound.</p>
<p>
What gives the monotone method a special twist is that <img alt="{\mathsf{lower}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Blower%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{lower}}"/> and <img alt="{\mathsf{upper}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bupper%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{upper}}"/> are not necessarily simple in the sense of being small.  Rather, they <em>make simple errors</em>—ones that can be corrected with small effort. The correction process yields <img alt="{C^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C^s}"/> and <img alt="{D^r.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%5Er.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D^r.}"/>  Isolating what is small, however, requires us to trade an “AND” of two inequalities for an “OR” of two economical ones. We know that at least one of the latter inequalities must be true. We arrange that either one gives us the kind of lower bound we seek. </p>
<p>
</p><p/><h2> Some More Detail </h2><p/>
<p/><p>
Here is how the trade happens. From Theorem <a href="https://rjlipton.wordpress.com/feed/#informal">1</a> we have: </p>
<p align="center"><img alt="\displaystyle  C^s \wedge C^{&lt;s} \leq f \leq D^{&lt;r} \vee D^r, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C%5Es+%5Cwedge+C%5E%7B%3Cs%7D+%5Cleq+f+%5Cleq+D%5E%7B%3Cr%7D+%5Cvee+D%5Er%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C^s \wedge C^{&lt;s} \leq f \leq D^{&lt;r} \vee D^r, "/></p>
<p>where: <img alt="{C^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C^s}"/> and <img alt="{D^r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D^r}"/> are small, and while <img alt="{C^{&lt;s}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%5E%7B%3Cs%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C^{&lt;s}}"/> and <img alt="{D^{&lt;r}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%5E%7B%3Cr%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D^{&lt;r}}"/> might be big, we have <img alt="{D^{&lt;r} \leq C^{&lt;s}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%5E%7B%3Cr%7D+%5Cleq+C%5E%7B%3Cs%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D^{&lt;r} \leq C^{&lt;s}}"/>. The trick is to ask:</p>
<blockquote><p><b> </b> <em> Is <img alt="{C^{&lt;s}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%5E%7B%3Cs%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C^{&lt;s}}"/> empty—that is, is it the trivial <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{1}"/> function? </em>
</p></blockquote>
<p>
</p><ul>
<li>
If <em>yes</em>, then it goes away on the left-hand side. We get: <p/>
<p align="center"><img alt="\displaystyle  C^s \leq f. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C%5Es+%5Cleq+f.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C^s \leq f. "/></p>
<p>Since <img alt="{C^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C^s}"/> is small, this is something we want. We got a small lower bound on <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> that holds for <b>all</b> arguments <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. </p>
</li><li>
If <em>no</em>, then it has a nontrivial clause corresponding to a set <img alt="{E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E}"/> of size at most <img alt="{s-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s-1}"/>. This is where the wraparound comes in. We have: <p/>
<p align="center"><img alt="\displaystyle  D^{&lt;r} \leq C^{&lt;s} \leq E, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%5E%7B%3Cr%7D+%5Cleq+C%5E%7B%3Cs%7D+%5Cleq+E%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D^{&lt;r} \leq C^{&lt;s} \leq E, "/></p>
<p>since we chose at least one clause. Substituting on the right-hand side thus gives us: </p>
<p align="center"><img alt="\displaystyle  f \leq E \vee D^r. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f+%5Cleq+E+%5Cvee+D%5Er.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f \leq E \vee D^r. "/></p>
<p>Now <img alt="{E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E}"/> is small, since it is just one clause, and <img alt="{D^r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D^r}"/> is small. We got a small upper bound rather than lower bound, but the fact that it has a restricted form and holds for <b>all</b> cases we can input to <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> will give us a lower bound on <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/>.
</p></li></ul>
<p>
Finally we are ready to state the theorem, which quantifies “small.” To follow Jukna, we now need to replace “<img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>” by “<img alt="{r+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r+1}"/>” and “<img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>” by “<img alt="{s+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s+1}"/>.” But the essence is the same.</p>
<blockquote><p><b>Theorem 2</b> <em><a name="tsimple"/> If <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f}"/> has a monotone Boolean circuit of size <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t}"/>, then for any <img alt="{r,s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%2Cs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r,s}"/> such that <img alt="{1 \leq r,s \leq n-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+r%2Cs+%5Cleq+n-1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{1 \leq r,s \leq n-1}"/>, we can build a conjunction <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/> of at most <img alt="{t \cdot r^{s+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt+%5Ccdot+r%5E%7Bs%2B1%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t \cdot r^{s+1}}"/> clauses of size exactly <img alt="{s+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%2B1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{s+1}"/>, a disjunction <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D}"/> of at most <img alt="{t \cdot s^{r+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt+%5Ccdot+s%5E%7Br%2B1%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t \cdot s^{r+1}}"/> terms of size exactly <img alt="{r+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%2B1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r+1}"/>, and a set <img alt="{E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{E}"/> of size at most <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{s}"/> such that either <img alt="{C \leq f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%5Cleq+f%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C \leq f}"/> or <img alt="{f \leq D \cup E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf+%5Cleq+D+%5Ccup+E%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f \leq D \cup E}"/>. </em>
</p></blockquote>
<p/><p>
Rather than re-prove this, we will continue the discussion with a concrete example. An exposition trick is: give examples before the general case and then abstract. Our example will involve graphs <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>—so the variables have the form <img alt="{x_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{i,j}}"/>, where <img alt="{x_{i,j} = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%2Cj%7D+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{i,j} = 1}"/> means there is an edge between vertex <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> and vertex <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/>, <img alt="{x_{i,j} = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%2Cj%7D+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{i,j} = 0}"/> otherwise. Putting <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> as the number of vertices, the number of possible edges is <img alt="{n = \binom{m}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+%5Cbinom%7Bm%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = \binom{m}{2}}"/>. We think of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> as a set of edges, so <img alt="{G \subseteq [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%5Csubseteq+%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G \subseteq [n]}"/>.</p>
<p>
</p><p/><h2> Checking for Triangles </h2><p/>
<p/><p>
Let <img alt="{f(G)=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28G%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(G)=1}"/> hold precisely when <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> has a triangle. This is clearly a monotone property. Our goal is to use the lower and upper bounds to prove that the monotone complexity of <img alt="{f(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(G)}"/> is almost of order <img alt="{m^{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%5E%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m^{3}}"/>. A side note is that the general complexity is much less via <img alt="{m \times m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%5Ctimes+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m \times m}"/> matrix products. </p>
<p>
The first beauty of using the method is that <em>you</em> get to choose the parameters <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> and <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> with a goal <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> in mind. The <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> and <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> must be in <img alt="{[1,n-1]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5B1%2Cn-1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[1,n-1]}"/>. The value of <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> will be a lower bound on the size of any monotone boolean circuit for <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/>. The parameters <img alt="{r,s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%2Cs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r,s}"/> are bounds on the clause and term size of the DNF and the CNF. You can select them any way you wish. But of course choose them wisely.</p>
<p>
In this case we know that <img alt="{r=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r=1}"/> is a right choice. We will say what <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> is later but we will have <img alt="{s=(\log n)^{O(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%3D%28%5Clog+n%29%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s=(\log n)^{O(1)}}"/>. Once you pick them, the CNF <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and DNF <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> (and small set <img alt="{E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E}"/>, a set of <img alt="{O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\log n)}"/> edges in this case) are chosen for you. You have no control over the sets <img alt="{T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_k}"/> that make up the terms of <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> and the sets <img alt="{C_\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_\ell}"/> that correspond to the clauses of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. Well you do know something about them. Here is what you do know about how many sets there are and how big the sets are:</p>
<ol>
<li>
For <img alt="{k=1,\dots,t \cdot s^{r+1} = ts^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%3D1%2C%5Cdots%2Ct+%5Ccdot+s%5E%7Br%2B1%7D+%3D+ts%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k=1,\dots,t \cdot s^{r+1} = ts^2}"/>, each <img alt="{T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_k}"/> is of size <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. <p/>
</li><li>
For <img alt="{\ell=1,\dots, t \cdot r^{s+1} = t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%3D1%2C%5Cdots%2C+t+%5Ccdot+r%5E%7Bs%2B1%7D+%3D+t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell=1,\dots, t \cdot r^{s+1} = t}"/>, each <img alt="{C_\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_\ell}"/> is of size <img alt="{s+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s+1}"/>.
</li></ol>
<p>The goal in either case is to force <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> to be large. We’ve numbered the right-hand case first.</p>
<ol>
<li>
Case <img alt="{f \leq D \cup E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf+%5Cleq+D+%5Ccup+E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f \leq D \cup E}"/>. Here we want to consider graphs <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> that <b>do</b> have a triangle—and nothing else. Because <img alt="{E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E}"/> includes at most <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> edges, hence touches at most <img alt="{2s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2s}"/> vertices, and <img alt="{2s \ll m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2s+%5Cll+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2s \ll m}"/>, we can focus on triangles among the <img alt="{m' = m - 2s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%27+%3D+m+-+2s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m' = m - 2s}"/> untouched vertices. There are <img alt="{T = \binom{m'}{3} = \Theta(m^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3D+%5Cbinom%7Bm%27%7D%7B3%7D+%3D+%5CTheta%28m%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T = \binom{m'}{3} = \Theta(m^3)}"/> such triangles, hence <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> graphs <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> to consider.<p/>
<p>
Since these graphs <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> have no edges in <img alt="{E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E}"/> but make <img alt="{f(G) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28G%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(G) = 1}"/>, there must be some <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> such that <img alt="{T_k(G) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%28G%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_k(G) = 1}"/>. Since <img alt="{T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_k}"/> has size <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>, this means <img alt="{T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_k}"/> has two edges of the triangle. Now the point is:</p>
<blockquote><p><b> </b> <em> For each <img alt="{T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{T_k}"/>, there is at most <b>one</b> triangle that <img alt="{T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{T_k}"/> can be two edges of. </em>
</p></blockquote>
<p/><p>
Hence there must be at least as many terms as possible triangles. This means: </p>
<p align="center"><img alt="\displaystyle  ts^2 \geq \binom{m'}{3}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++ts%5E2+%5Cgeq+%5Cbinom%7Bm%27%7D%7B3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  ts^2 \geq \binom{m'}{3}. "/></p>
<p>Because <img alt="{s = (\log n)^{O(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs+%3D+%28%5Clog+n%29%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s = (\log n)^{O(1)}}"/>, we finally get <img alt="{t = \tilde{\Omega}(m^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+%5Ctilde%7B%5COmega%7D%28m%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t = \tilde{\Omega}(m^3)}"/>, where the tilde means to ignore factors of <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log n}"/>.</p>
<p/></li><li>
Case <img alt="{C \leq f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%5Cleq+f%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C \leq f}"/>. Here we want to consider graphs <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> such that <img alt="{f(G) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28G%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(G) = 0}"/> but <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is chock full of as many edges as one can have without creating a triangle. Such <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> include complete bipartite graphs. There are <img alt="{2^{m-1} - 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bm-1%7D+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{m-1} - 1}"/> such graph inputs, as can be realized from how any binary string <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> except <img alt="{0^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5Em%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0^m}"/> and <img alt="{1^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%5Em%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1^m}"/> encodes such a graph—and only its bit-complement <img alt="{w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'}"/> encodes the same labeled graph.<p/>
<p>
In order to keep <img alt="{C \leq f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%5Cleq+f%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C \leq f}"/> we need <img alt="{C(G) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%28G%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C(G) = 0}"/> for all such <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>, so we need (at least) one clause <img alt="{C_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_k}"/> to <em>fail</em> on <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. This means that all vertices touched by the edges in <img alt="{C_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_k}"/> must be in the same partition. The more vertices touched, the fewer strings <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> have all <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>s (or all <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>s) in the corresponding positions, which means the fewer graphs <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> “covered” by that clause. We want to know how many clauses we need to cover all these graphs, hence we try to minimize the number of vertices touched by each clause. That number is at least <img alt="{s' = \lceil \sqrt{2s}\rceil}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%27+%3D+%5Clceil+%5Csqrt%7B2s%7D%5Crceil%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s' = \lceil \sqrt{2s}\rceil}"/>. The number of graphs we cover is at most <img alt="{2^{m - s'} - 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bm+-+s%27%7D+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{m - s'} - 1}"/> (the <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/> excludes the empty graph). Thus the number <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> of clauses we need satisfies </p>
<p align="center"><img alt="\displaystyle  t \geq \frac{2^{m-1} - 1}{2^{m - s'} - 1} \geq 2^{s' - 1}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++t+%5Cgeq+%5Cfrac%7B2%5E%7Bm-1%7D+-+1%7D%7B2%5E%7Bm+-+s%27%7D+-+1%7D+%5Cgeq+2%5E%7Bs%27+-+1%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  t \geq \frac{2^{m-1} - 1}{2^{m - s'} - 1} \geq 2^{s' - 1}. "/></p>
<p>By taking <img alt="{s' &gt; 4.5\log^2 m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%27+%3E+4.5%5Clog%5E2+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s' &gt; 4.5\log^2 m}"/> we can make <img alt="{t \geq m^3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt+%5Cgeq+m%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t \geq m^3}"/> in this case. We can actually get bigger functions with bigger <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>, but this balances against case 1 where <img alt="{t = \tilde{\Omega}(m^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+%5Ctilde%7B%5COmega%7D%28m%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t = \tilde{\Omega}(m^3)}"/> was the best we could do, so that is our lower bound.
</p></li></ol>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Does this help in understanding the approximation method? Can you work out the concretely optimum choice of <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> in the triangle example?</p>
<p>
Would you prefer not changing <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> and <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> in the statement of Theorem <a href="https://rjlipton.wordpress.com/feed/#tsimple">2</a>? Then we would have worded the triangle example with “<img alt="{r = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = 2}"/>” rather than “<img alt="{r = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = 1}"/>.” The former is a little more suggestive of the idea of having two edges of a triangle. Doing so, however, could make notation in the proof of Theorem <a href="https://rjlipton.wordpress.com/feed/#tsimple">2</a> somewhat messier. Another possibility was keeping Jukna’s usage throughout, so that the earlier version <a href="https://rjlipton.wordpress.com/feed/#informal">1</a> of the theorem would say <img alt="{D^{\leq r} \leq C^{\leq s}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%5E%7B%5Cleq+r%7D+%5Cleq+C%5E%7B%5Cleq+s%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D^{\leq r} \leq C^{\leq s}}"/> with <img alt="{C^{s+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%5E%7Bs%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C^{s+1}}"/> and <img alt="{D^{r+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%5E%7Br%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D^{r+1}}"/> being small. We try to solve “exposition problems” in every post but feel a dilemma here. Comments might help us on a followup post. </p>
<p/></font></font></div>
    </content>
    <updated>2020-07-27T06:39:13Z</updated>
    <published>2020-07-27T06:39:13Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Proofs"/>
    <category term="Alexander Razborov"/>
    <category term="approximation"/>
    <category term="Boolean functions"/>
    <category term="circuits"/>
    <category term="complexity"/>
    <category term="exposition problems"/>
    <category term="lower bounds"/>
    <category term="monotone"/>
    <category term="Stasys Jukna"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-07-27T18:20:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3389282706697250678</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3389282706697250678/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/do-computers-make-us-more-safe-or-less.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3389282706697250678" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3389282706697250678" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/do-computers-make-us-more-safe-or-less.html" rel="alternate" type="text/html"/>
    <title>Do computers make us more safe or less safe?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Norbert Weiner wrote a paper <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/moral.pdf">Some Moral and Technical Consequences of Automation</a> in 1960. It warns of the dangers of computers in two ways:<br/>
<br/>
1) If a chess program is only trained against expert chess players then it might get confused if its opponent makes a bad move. This is not dangerous. But imagine a nuclear missle system that assumes the opponent is rational. If the opponent is not rational then it might launch and have an accidental nuclear war. So <i>there must be a human component </i>so that this won't happen.<br/>
<br/>
I offer a story and a counter narrative. In the 5th season, 23rd episode of the TV show Castle,<br/>
title <i>The Human Factor </i>a character had the following story to tell:<br/>
<i><br/>
The drone on its own was going to bomb a car. But the human noticed that there were red roses on the car, so it was a wedding couple, not a terrorist. If a human had not been involved the drone may have killed an innocent just married couple!</i><br/>
<br/>
This scene bothered me. It could EASILY be the other way around: the human wants to bomb and the drone (which has better vision) notices the roses. Or there may be many other ways that a computer could be BETTER than a human. I am not saying that a completely automated system is better, I am saying that its not obvious which way to go.  Both in some combination? What combination? Who has the final say? And in the drone scenario there may not be time for a human to consider the options.<br/>
<br/>
2) The Sorcerer's apprentice scenario. In The Sorcerer's Apprentice segment of the (original) movie Fantasia, Mickey mouse tells a broom to get him a glass of water. The broom keeps bringing him water and Mickey almost drowns. Computers may take orders to literally and not stop. I wonder if  automated stock-trading and automated auctions may have this problem. Is there a case known where this really did cause a problem?<div><br/></div><div>So what do you think?</div><div><br/></div><div>NOW- do computers (or, more generally technology) make us more safe or less safe?</div><div><br/></div><div>FUTURE- same question.</div></div>
    </content>
    <updated>2020-07-27T03:18:00Z</updated>
    <published>2020-07-27T03:18:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-07-27T14:26:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2020/07/27/discrete-fragility/</id>
    <link href="http://benjamin-recht.github.io/2020/07/27/discrete-fragility/" rel="alternate" type="text/html"/>
    <title>Digital Witnesses</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Doyle derived his LQG counterexample in the time before the ubiquity of numerical computing. This meant that numerical examples did not carry the rhetorical weight of algebraic closed form instances. The need for clean, persuasive formulae also meant that controllers were idealized in continuous time. Continuous-time optimal control often produced policies that couldn’t be implemented because of the limits of physical reality: no system can act instantaneously with arbitrary power. These issues of infeasibility were <a href="https://ieeexplore.ieee.org/document/1099822/">certainly noted in the literature</a> during the hey day of optimal control, but continuous time models still often made it difficult to pinpoint these issues.</p>

<p>Discrete-time models don’t share many of these issues. In discrete time, we explicitly encode the sequential, computational nature of decision and control. Discrete-time formulae are unfortunately less elegant than their continuous-time counterparts, but, as I hope to show here, they are often more revealing. Indeed, constructing examples where discrete-time optimal control leads to fragile solutions seems to be surprisingly easy.</p>

<p>Here, I’ll highlight a few examples where relatively innocuous problem formulations lead to very fragile control policies. The examples are weirdly simple and almost comical to a point. But anyone who has played with discrete-time optimal control may have stumbled into similar control policies and had to step back and think about why.</p>

<p>Let’s revisit the discrete-time LQR problem:</p>



<p>We again assume $x_t$ is observed perfectly without noise. While such perfect state information is not realistic, even ideal state feedback ends up being fragile in discrete time. $w_t$ is assumed to be stochastic, but I don’t think much changes if we move to a more adversarial setting. Here, we need the decision variable $u_t$ to be <em>causal</em>. It must be a function of only the values $x_s$ and $u_s$ with $s\leq t$. For stochastic disturbances, the optimal $u$ can always be found by dynamic programming.</p>

<p>Consider the following innocuous dynamics:</p>



<p>This system is a simple, two-state shift register. I’ll write the state out with indexed components $x=[x^{(1)},x^{(2)}]^\top$. New states enter through the control $B$ into the second state. The first state, $x^{(1)}$ is simply whatever was in the second register at the previous time step. The open loop dynamics of this system are as stable as you could imagine. Both eigenvalues of $A$ are zero.</p>

<p>Let’s say our control objective aims to try to keep the two states equal to each other. We can model this with the quadratic cost:</p>



<p>I assume $R=0$ here for simplicity, as the formulae are particularly nice for this case. But, as I will discuss in a moment, the situation is not improved simply by having $R$ be positive. For the disturbance, assume that $w_t$ is zero mean, has bounded second moment, $\Sigma_t = \mathbb{E}[w_t w_t^\top]$, and is uncorrelated with $x_t$ and $u_t$.</p>

<p>The cost is asking to minimize</p>



<p>When $w_t=0$, $x_t^{(1)}+x_t^{(2)} = x_{t-1}^{(2)}+u_{t-1}$, so it seems like our best bet is to just set $u_{t}=x_t^{(2)}$. This turns out to be the optimal action, and you can prove this directly using standard dynamic programming computations. What this means is that the closed loop dynamics of the system are</p>



<p>This closed-loop system is <em>marginally stable</em>, meaning that while signals don’t blow up, some states will persist forever and not converge to $0$. Indeed, the state-transition matrix here has eigenvalues $0$ and $1$. The $1$ corresponds the state where the two components are equal, and such a state can persist forever.</p>

<p>If we learned an incorrect model of the dynamics, how would that influence the closed loop behavior? The simplest scenario is that we identified $B$ from some preliminary experiments. We can immediately see that if the true $B_\star=\alpha B $, then the closed loop dynamics are</p>



<p>This system is unstable for any $\alpha&gt;1$. That is, the system is arbitrarily sensitive to misidentification of the dynamics. Note that this lack of robustness has nothing to do with the noise sequence. The structure of the cost is what drives the system to fragility.</p>

<p>If $R&gt;0$, you will get a slightly different policy. Again, using elementary dynamic programming shows that the optimal control is $u_t=\beta_t(R) x_t^{(2)}$ for some $\beta_t(R) \in (1/2,1)$. The closed loop system will be a bit more stable, but this comes at the price of reduced performance. And, at best, the gain margin of this system approaches $2$ as $R$ goes to infinity. You can also check that if you add $\epsilon$ times the identity to $Q$, you again get a control policy proportional to $x_t^{(2)}$.</p>

<p>This behavior can occur in even simpler systems. Consider the one-state linear system</p>



<p>The open loop system is again as stable as it gets. Now let’s aim to minimize $\Vert x-u \Vert$. It doesn’t matter what norm you choose here or whether you treat the noise as stochastic or worst case with respect to $w$, the optimal control is going to be $u_t = x_t/b$. Once again, the closed loop system has a pole at $1$ and is arbitrary fragile to misspecification of $b$.</p>

<p>I could continue to construct nasty examples, but I hope these examples are sufficiently illustrative. They are certainly contrived and pathological, and it’s not at all clear that they reflect any optimal control problem you might have been hoping to solve. However, both examples involve systems that are robust and stable in open loop. It’s only when we close the feedback loop that we end up in a dangerous situation. That simple optimal control problems give some profoundly fragile solutions should be a clear warning: <em>You can’t just optimize and hope to be robust.</em> You have to consider uncertainty as a first class citizen when designing feedback systems.</p>

<p>In some sense, the core contribution of robust control is in raising awareness of fundamental tradeoffs in the design of feedback systems. Optimal control promises that you can roughly identify a system, model uncertainty as noise, solve an optimization problem,  and then ship your policy. Hopefully, the examples in the last two posts have shown why this particular approach is fraught with danger.</p>

<p>If failure of a feedback system has any consequences, then a more holistic robust approach is <em>necessary</em>. We have to work with experts at different levels of the engineering pipeline, worry about unmodeled behaviors, and understand hard limits and practical tradeoffs. That is, engineering has to be more concerned with <em>design</em> than with <em>optimization.</em></p>

<p>There are all sorts of questions that a robust, systems level engineering effort might ask. Where should you put that extra sensor? Which parts of the system are likely to create issues? Is it possible to avoid performance disruptions when updating a single component in a legacy system? These questions are important in all aspects of system engineering, and developing accessible tools for addressing them in machine learning systems remains a daunting but essential challenge.</p>

<p>I am emphatically not saying that the design of feedback systems is hopeless. It’s easy to walk away with the impression “Ben’s examples are pathologies and unlike what I see in practice” or the pessimistic feeling of “shoot, all of this ML stuff is hopeless, I’m going to go work on something tractable like vaccine development.” I’m not saying that engineering robust machine learning systems is hopeless. I’m just saying that our community has to work better to incorporate multiple levels of uncertainty in its thinking. What are the fundamental tradeoffs between performance and robustness in machine learning? What do we even want to be robust to? In the next post I want to describe some of these robustness tradeoffs without using the language of optimization, probing if that provides some possible paths forward.</p></div>
    </summary>
    <updated>2020-07-27T00:00:00Z</updated>
    <published>2020-07-27T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2020-07-27T13:21:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12653</id>
    <link href="http://arxiv.org/abs/2007.12653" rel="alternate" type="text/html"/>
    <title>Beating Greedy For Approximating Reserve Prices in Multi-Unit VCG Auctions</title>
    <feedworld_mtime>1595808000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Derakhshan:Mahsa.html">Mahsa Derakhshan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pennock:David_M=.html">David M. Pennock</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Slivkins:Aleksandrs.html">Aleksandrs Slivkins</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12653">PDF</a><br/><b>Abstract: </b>We study the problem of finding personalized reserve prices for unit-demand
buyers in multi-unit eager VCG auctions with correlated buyers. The input to
this problem is a dataset of submitted bids of $n$ buyers in a set of auctions.
The goal is to find a vector of reserve prices, one for each buyer, that
maximizes the total revenue across all auctions.
</p>
<p>Roughgarden and Wang (2016) showed that this problem is APX-hard but admits a
greedy $\frac{1}{2}$-approximation algorithm. Later, Derakhshan, Golrezai, and
Paes Leme (2019) gave an LP-based algorithm achieving a $0.68$-approximation
for the (important) special case of the problem with a single-item, thereby
beating greedy. We show in this paper that the algorithm of Derakhshan et al.
in fact does not beat greedy for the general multi-item problem. This raises
the question of whether or not the general problem admits a
better-than-$\frac{1}{2}$ approximation.
</p>
<p>In this paper, we answer this question in the affirmative and provide a
polynomial-time algorithm with a significantly better approximation-factor of
$0.63$. Our solution is based on a novel linear programming formulation, for
which we propose two different rounding schemes. We prove that the best of
these two and the no-reserve case (all-zero vector) is a $0.63$-approximation.
</p></div>
    </summary>
    <updated>2020-07-27T02:04:14Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12652</id>
    <link href="http://arxiv.org/abs/2007.12652" rel="alternate" type="text/html"/>
    <title>MurTree: Optimal Classification Trees via Dynamic Programming and Search</title>
    <feedworld_mtime>1595808000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Emir Demirović, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lukina:Anna.html">Anna Lukina</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hebrard:Emmanuel.html">Emmanuel Hebrard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chan:Jeffrey.html">Jeffrey Chan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bailey:James.html">James Bailey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leckie:Christopher.html">Christopher Leckie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramamohanarao:Kotagiri.html">Kotagiri Ramamohanarao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stuckey:Peter_J=.html">Peter J. Stuckey</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12652">PDF</a><br/><b>Abstract: </b>Decision tree learning is a widely used approach in machine learning,
favoured in applications that require concise and interpretable models.
Heuristic methods are traditionally used to quickly produce models with
reasonably high accuracy. A commonly criticised point, however, is that the
resulting trees may not necessarily be the best representation of the data in
terms of accuracy, size, and other considerations such as fairness. In recent
years, this motivated the development of optimal classification tree algorithms
that globally optimise the decision tree in contrast to heuristic methods that
perform a sequence of locally optimal decisions. We follow this line of work
and provide a novel algorithm for learning optimal classification trees based
on dynamic programming and search. Our algorithm supports constraints on the
depth of the tree and number of nodes and we argue it can be extended with
other requirements. The success of our approach is attributed to a series of
specialised techniques that exploit properties unique to classification trees.
Whereas algorithms for optimal classification trees have traditionally been
plagued by high runtimes and limited scalability, we show in a detailed
experimental study that our approach uses only a fraction of the time required
by the state-of-the-art and can handle datasets with tens of thousands of
instances, providing several orders of magnitude improvements and notably
contributing towards the practical realisation of optimal decision trees.
</p></div>
    </summary>
    <updated>2020-07-27T01:23:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12651</id>
    <link href="http://arxiv.org/abs/2007.12651" rel="alternate" type="text/html"/>
    <title>Tromino Tilings with Pegs via Flow Networks</title>
    <feedworld_mtime>1595808000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akagi:Javier_T=.html">Javier T. Akagi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Canale:Eduardo_A=.html">Eduardo A. Canale</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Villagra:Marcos.html">Marcos Villagra</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12651">PDF</a><br/><b>Abstract: </b>A tromino tiling problem is a packing puzzle where we are given a region of
connected lattice squares and we want to decide whether there exists a tiling
of the region using trominoes with the shape of an L. In this work we study a
slight variation of the tromino tiling problem where some positions of the
region have pegs and each tromino comes with a hole that can only be placed on
top of the pegs. We present a characterization of this tiling problem with pegs
using flow networks and show that (i) there exists a linear-time parsimonious
reduction to the maximum-flow problem, and (ii) counting the number of such
tilings can be done in linear-time. The proofs of both results contain
algorithms that can then be used to decide the tiling of a region with pegs in
$O(n)$ time.
</p></div>
    </summary>
    <updated>2020-07-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12502</id>
    <link href="http://arxiv.org/abs/2007.12502" rel="alternate" type="text/html"/>
    <title>Using a geometric lens to find k disjoint shortest paths</title>
    <feedworld_mtime>1595808000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bentert:Matthias.html">Matthias Bentert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nichterlein:Andr=eacute=.html">André Nichterlein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Renken:Malte.html">Malte Renken</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zschoche:Philipp.html">Philipp Zschoche</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12502">PDF</a><br/><b>Abstract: </b>Given an undirected $n$-vertex graph and $k$ pairs of terminal vertices
$(s_1,t_1), \ldots, (s_k,t_k)$, the $k$-Disjoint Shortest Paths
($k$-DSP)-problem asks whether there are $k$ pairwise vertex-disjoint paths
$P_1,\ldots, P_k$ such that $P_i$ is a shortest $s_i$-$t_i$-path for each $i
\in [k]$. Recently, Lochet [arXiv 2019] provided an algorithm that solves
$k$-DSP in $n^{O(k^{4^k})}$ time, answering a 20-year old question about the
computational complexity of $k$-DSP for constant $k$. On the one hand, we
present an improved $O(k n^{12k \cdot k! + k + 1})$-time algorithm based on a
novel geometric view on this problem. For the special case $k=2$, we show that
the running time can be further reduced to $O(n^2m)$ by small modifications of
the algorithm and a further refined analysis. On the other hand, we show that
$k$-DSP is W[1]-hard with respect to $k$, showing that the dependency of the
degree of the polynomial running time on the parameter $k$ is presumably
unavoidable.
</p></div>
    </summary>
    <updated>2020-07-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12388</id>
    <link href="http://arxiv.org/abs/2007.12388" rel="alternate" type="text/html"/>
    <title>Improved approximation schemes for early work scheduling on identical parallel machines with common due date</title>
    <feedworld_mtime>1595808000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Weidong.html">Weidong Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12388">PDF</a><br/><b>Abstract: </b>We study the early work scheduling problem on identical parallel machines in
order to maximize the total early work, i.e., the parts of non-preemptive jobs
executed before a common due date. By preprocessing and constructing an
auxiliary instance which has several good properties, we propose an efficient
polynomial time approximation scheme with running time $O(n)$, which improves
the result in [Gy\"{o}rgyi, P., Kis, T. (2020). A common approximation
framework for early work, late work, and resource leveling problems. {\it
European Journal of Operational Research}, 286(1), 129-137], and a fully
polynomial time approximation scheme with running time $O(n)$ when the number
of machines is a fixed number, which improves the result in [Chen, X., Liang,
Y., Sterna, M., Wang, W., B{\l}a\.{z}ewicz, J. (2020b). Fully polynomial time
approximation scheme to maximize early work on parallel machines with common
due date. {\it European Journal of Operational Research}, 284(1), 67-74], where
$n$ is the number of jobs, and the hidden constant depends on the desired
accuracy.
</p></div>
    </summary>
    <updated>2020-07-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12372</id>
    <link href="http://arxiv.org/abs/2007.12372" rel="alternate" type="text/html"/>
    <title>On the Parameterized Complexity of Synthesizing Boolean Petri Nets With Restricted Dependency (Technical Report)</title>
    <feedworld_mtime>1595808000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tredup:Ronny.html">Ronny Tredup</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Erofeev:Evgeny.html">Evgeny Erofeev</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12372">PDF</a><br/><b>Abstract: </b>The problem of $\tau$-synthesis consists in deciding whether a given directed
labeled graph $A$ is isomorphic to the reachability graph of a Boolean Petri
net $N$ of type $\tau$. In case of a positive decision, $N$ should be
constructed. For many Boolean types of nets, the problem is NP-complete. This
paper deals with a special variant of $\tau$-synthesis that imposes
restrictions for the target net $N$: we investigate \emph{dependency
$d$-restricted $\tau$-synthesis (DR$\tau$S)} where each place of $N$ can
influence and be influenced by at most $d$ transitions. For a type $\tau$, if
$\tau$-synthesis is NP-complete then DR$\tau$S is also NP-complete. In this
paper, we show that DR$\tau$S parameterized by $d$ is in XP. Furthermore, we
prove that it is $W[2]$-hard, for many Boolean types that allow unconditional
interactions $set$ and $reset$.
</p></div>
    </summary>
    <updated>2020-07-27T01:21:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12350</id>
    <link href="http://arxiv.org/abs/2007.12350" rel="alternate" type="text/html"/>
    <title>Improving the dilation of a metric graph by adding edges</title>
    <feedworld_mtime>1595808000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gudmundsson:Joachim.html">Joachim Gudmundsson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wong:Sampson.html">Sampson Wong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12350">PDF</a><br/><b>Abstract: </b>Most of the literature on spanners focuses on building the graph from
scratch. This paper instead focuses on adding edges to improve an existing
graph. A major open problem in this field is: given a graph embedded in a
metric space, and a budget of k edges, which k edges do we add to produce a
minimum-dilation graph? The special case where k=1 has been studied in the
past, but no major breakthroughs have been made for k &gt; 1. We provide the first
positive result, an O(k)-approximation algorithm that runs in O(n^3 \log n)
time.
</p></div>
    </summary>
    <updated>2020-07-27T02:07:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12330</id>
    <link href="http://arxiv.org/abs/2007.12330" rel="alternate" type="text/html"/>
    <title>Largest triangles in a polygon</title>
    <feedworld_mtime>1595808000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Seungjun.html">Seungjun Lee</a>, Taekang Eom, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahn:Hee=Kap.html">Hee-Kap Ahn</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12330">PDF</a><br/><b>Abstract: </b>We study the problem of finding maximum-area triangles that can be inscribed
in a polygon in the plane. We consider eight versions of the problem: we use
either convex polygons or simple polygons as the container; we require the
triangles to have either one corner with a fixed angle or all three corners
with fixed angles; we either allow reorienting the triangle or require its
orientation to be fixed. We present exact algorithms for all versions of the
problem. In the case with reorientations for convex polygons with $n$ vertices,
we also present $(1-\varepsilon)$-approximation algorithms.
</p></div>
    </summary>
    <updated>2020-07-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12323</id>
    <link href="http://arxiv.org/abs/2007.12323" rel="alternate" type="text/html"/>
    <title>Tight Distributed Sketching Lower Bound for Connectivity</title>
    <feedworld_mtime>1595808000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Huacheng.html">Huacheng Yu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12323">PDF</a><br/><b>Abstract: </b>In this paper, we study the distributed sketching complexity of connectivity.
In distributed graph sketching, an $n$-node graph $G$ is distributed to $n$
players such that each player sees the neighborhood of one vertex. The players
then simultaneously send one message to the referee, who must compute some
function of $G$ with high probability. For connectivity, the referee must
output whether $G$ is connected. The goal is to minimize the message lengths.
Such sketching schemes are equivalent to one-round protocols in the broadcast
congested clique model.
</p>
<p>We prove that the expected average message length must be at least
$\Omega(\log^3 n)$ bits, if the error probability is at most $1/4$. It matches
the upper bound obtained by the AGM sketch [AGM12], which even allows the
referee to output a spanning forest of $G$ with probability
$1-1/\mathrm{poly}\, n$. Our lower bound strengthens the previous
$\Omega(\log^3 n)$ lower bound for spanning forest computation [NY19]. Hence,
it implies that connectivity, a decision problem, is as hard as its "search"
version in this model.
</p></div>
    </summary>
    <updated>2020-07-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12307</id>
    <link href="http://arxiv.org/abs/2007.12307" rel="alternate" type="text/html"/>
    <title>Computing nearest neighbour interchange distances between ranked phylogenetic trees</title>
    <feedworld_mtime>1595808000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Lena Collienne, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gavryushkin:Alex.html">Alex Gavryushkin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12307">PDF</a><br/><b>Abstract: </b>Many popular algorithms for searching the space of leaf-labelled trees are
based on tree rearrangement operations. Under any such operation, the problem
is reduced to searching a graph where vertices are trees and (undirected) edges
are given by pairs of trees connected by one rearrangement operation (sometimes
called a move). Most popular are the classical nearest neighbour interchange,
subtree prune and regraft, and tree bisection and reconnection moves. The
problem of computing distances, however, is NP-hard in each of these graphs,
making tree inference and comparison algorithms challenging to design in
practice.
</p>
<p>Although ranked phylogenetic trees are one of the central objects of interest
in applications such as cancer research, immunology, and epidemiology, the
computational complexity of the shortest path problem for these trees remained
unsolved for decades. In this paper, we settle this problem for the ranked
nearest neighbour interchange operation by establishing that the complexity
depends on the weight difference between the two types of tree rearrangements
(rank moves and edge moves), and varies from quadratic, which is the lowest
possible complexity for this problem, to NP-hard, which is the highest. In
particular, our result provides the first example of a phylogenetic tree
rearrangement operation for which shortest paths, and hence the distance, can
be computed efficiently. Specifically, our algorithm scales to trees with
thousands of leaves (and likely hundreds of thousands if implemented
efficiently).
</p>
<p>We also connect the problem of computing distances in our graph of ranked
trees with the well-known version of this problem on unranked trees by
introducing a parameter for the weight difference between move types. We
propose to study a family of shortest path problems indexed by this parameter
with computational complexity varying from quadratic to NP-hard.
</p></div>
    </summary>
    <updated>2020-07-27T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-07-26-private-set-intersection-2/</id>
    <link href="https://decentralizedthoughts.github.io/2020-07-26-private-set-intersection-2/" rel="alternate" type="text/html"/>
    <title>Private Set Intersection #2</title>
    <summary>In the first post on Private Set Intersection, I presented the problem of Private Set Intersection, its applications and the simple protocol of [KMRS14], that allows Alice and Bob to learn the intersection of their sets with the aid of an untrusted third party Steve who is assumed to not...</summary>
    <updated>2020-07-26T23:00:00Z</updated>
    <published>2020-07-26T23:00:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-07-27T17:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12120</id>
    <link href="http://arxiv.org/abs/2007.12120" rel="alternate" type="text/html"/>
    <title>The Asymmetric Travelling Salesman Problem in Sparse Digraphs</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Łukasz Kowalik, Konrad Majewski <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12120">PDF</a><br/><b>Abstract: </b>Asymmetric Travelling Salesman Problem (ATSP) and its special case Directed
Hamiltonicity are among the most fundamental problems in computer science. The
dynamic programming algorithm running in time $O^*(2^n)$ developed almost 60
years ago by Bellman, Held and Karp, is still the state of the art for both of
these problems.
</p>
<p>In this work we focus on sparse digraphs. First, we recall known approaches
for Undirected Hamiltonicity and TSP in sparse graphs and we analyse their
consequences for Directed Hamiltonicity and ATSP in sparse digraphs, either by
adapting the algorithm, or by using reductions. In this way, we get a number of
running time upper bounds for a few classes of sparse digraphs, including
$O^*(2^{n/3})$ for digraphs with both out- and indegree bounded by 2, and
$O^*(3^{n/2})$ for digraphs with outdegree bounded by 3.
</p>
<p>Our main results are focused on digraphs of bounded {\em average} outdegree
$d$. The baseline for ATSP here is a simple enumeration of cycle covers which
can be done in time bounded by $O^*(\mu(d)^n)$ for a function
$\mu(d)\le(\lceil{d}\rceil!)^{n/{\lceil{d}\rceil}}$. One can also observe that
Directed Hamiltonicity can be solved in randomized time $O^*((2-2^{-d})^n)$ and
polynomial space, by adapting a recent result of Bj\"{o}rklund [ISAAC 2018]
stated originally for Undirected Hamiltonicity in sparse bipartite graphs.
</p>
<p>We present two new deterministic algorithms: the first running in time
$O(2^{0.441(d-1)n})$ and polynomial space, and the second in exponential space
with running time of $O^*(\tau(d)^{n/2})$ for a function $\tau(d)\le d$.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12117</id>
    <link href="http://arxiv.org/abs/2007.12117" rel="alternate" type="text/html"/>
    <title>Silhouette Vectorization by Affine Scale-space</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Yuchen.html">Yuchen He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kang:Sung_Ha.html">Sung Ha Kang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morel:Jean=Michel.html">Jean-Michel Morel</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12117">PDF</a><br/><b>Abstract: </b>Silhouettes or 2D planar shapes are extremely important in human
communication, which involves many logos, graphics symbols and fonts in vector
form. Many more shapes can be extracted from image by binarization or
segmentation, thus in raster form that requires a vectorization. There is a
need for disposing of a mathematically well defined and justified shape
vectorization process, which in addition provides a minimal set of control
points with geometric meaning. In this paper we propose a silhouette
vectorization method which extracts the outline of a 2D shape from a raster
binary image, and converts it to a combination of cubic B\'{e}zier polygons and
perfect circles. Starting from the boundary curvature extrema computed at
sub-pixel level, we identify a set of control points based on the affine
scale-space induced by the outline. These control points capture similarity
invariant geometric features of the given silhouette and give precise locations
of the shape's corners.of the given silhouette. Then, piecewise B\'{e}zier
cubics are computed by least-square fitting combined with an adaptive splitting
to guarantee a predefined accuracy. When there are no curvature extrema
identified, either the outline is recognized as a circle using the
isoperimetric inequality, or a pair of the most distant outline points are
chosen to initiate the fitting. Given their construction, most of our control
points are geometrically stable under affine transformations. By comparing with
other feature detectors, we show that our method can be used as a reliable
feature point detector for silhouettes. Compared to state-of-the-art image
vectorization software, our algorithm demonstrates superior reduction on the
number of control points, while maintaining high accuracy.
</p></div>
    </summary>
    <updated>2020-07-26T23:28:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12102</id>
    <link href="http://arxiv.org/abs/2007.12102" rel="alternate" type="text/html"/>
    <title>Faster algorithms for sampling connected induced subgraphs</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bressan:Marco.html">Marco Bressan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12102">PDF</a><br/><b>Abstract: </b>We consider the problem of sampling connected induced $k$-node subgraphs
($k$-graphlets) uniformly at random from a given graph $G=(V,E)$. The best
algorithms known achieve $\epsilon$-uniformity and are based on random walks or
color coding. The random walk approach has a worst-case running time of
$\Delta^{\!\Theta(k)}\log\frac{n}{\epsilon}$ where $n=|V|$ and $\Delta$ is the
maximum degree of $G$, while color coding requires a preprocessing that takes
time and space $2^{\Theta(k)}O(m \log\frac{1}{\epsilon})$ where $m=|E|$. Our
main result is an algorithm for $\epsilon$-uniform sampling with the following
guarantees. The preprocessing runs in time $k^{O(k)}O\big(\frac{1}{\epsilon}\,
n \log n \big)$ and space $O(n)$. After, the algorithm yields independent
$\epsilon$-uniform $k$-graphlets in
$k^{{O}(k)}{O}\big((\frac{1}{\epsilon})^{10}\log \frac{1}{\epsilon} \big)$
expected time per sample. The preprocessing phase computes in one pass an
approximate ordering of $G$ that makes rejection sampling efficient in the
sampling phase, and the $\epsilon$-uniformity is based on estimating cuts and
coupling arguments. In fact, the algorithm derives from a much simpler
algorithm which has ${O}(m \log \Delta)$ preprocessing time and returns
perfectly uniform $k$-graphlets from $G$ in $k^{{O}(k)}{O}(\log \Delta)$
expected time per sample. In addition, we give an almost-tight bound for the
random walk technique. More precisely, we show that the most commonly used
random walk has mixing time $k^{{O}(k)} {O}(t(G)(\frac{\Delta}{\delta})^{k-1}\!
\log n)$ where $t(G)$ is the mixing time of $G$ and $\delta$ is its minimum
degree. This improves on recent results and is tight up to a factor $k^{{O}(k)}
\delta \log n$.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12077</id>
    <link href="http://arxiv.org/abs/2007.12077" rel="alternate" type="text/html"/>
    <title>Detecting and Enumerating Small Induced Subgraphs in $c$-Closed Graphs</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koana:Tomohiro.html">Tomohiro Koana</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nichterlein:Andr=eacute=.html">André Nichterlein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12077">PDF</a><br/><b>Abstract: </b>Fox et al. [SIAM J. Comp. 2020] introduced a new parameter, called
$c$-closure, for a parameterized study of clique enumeration problems. A graph
$G$ is $c$-closed if every pair of vertices with at least $c$ common neighbors
is adjacent. The $c$-closure of $G$ is the smallest $c$ such that $G$ is
$c$-closed. We systematically explore the impact of $c$-closure on the
computational complexity of detecting and enumerating small induced subgraphs.
More precisely, for each graph $H$ on three or four vertices, we investigate
parameterized polynomial-time algorithms for detecting $H$ and for enumerating
all occurrences of $H$ in a given $c$-closed graph.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.12048</id>
    <link href="http://arxiv.org/abs/2007.12048" rel="alternate" type="text/html"/>
    <title>Lower Bounds and Hardness Magnification for Sublinear-Time Shrinking Cellular Automata</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Modanese:Augusto.html">Augusto Modanese</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.12048">PDF</a><br/><b>Abstract: </b>The minimum circuit size problem (MCSP) is a string compression problem with
a parameter $s$ in which, given the truth table of a Boolean function over
inputs of length $n$, one must answer whether it can be computed by a Boolean
circuit of size at most $s(n) \ge n$. Recently, McKay, Murray, and Williams
(STOC, 2019) proved a hardness magnification result for MCSP involving
(one-pass) streaming algorithms: For any reasonable $s$, if there is no
$\mathsf{poly}(s(n))$-space streaming algorithm with $\mathsf{poly}(s(n))$
update time for $\mathsf{MCSP}[s]$, then $\mathsf{P} \neq \mathsf{NP}$. We
prove an equivalent result for the (provably) strictly less capable model of
shrinking cellular automata (SCAs), which are cellular automata whose cells can
spontaneously delete themselves. We show every language accepted by an SCA can
also be accepted by a streaming algorithm of similar complexity, and we
identify two different aspects in which SCAs are more restricted than streaming
algorithms. We also show there is a language which cannot be accepted by any
SCA in $o(n / \log n)$ time, even though it admits an $O(\log n)$-space
streaming algorithm with $O(\log n)$ update time, where $n$ is the input
length.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11997</id>
    <link href="http://arxiv.org/abs/2007.11997" rel="alternate" type="text/html"/>
    <title>Total Domination in Unit Disk Graphs</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jena:Sangram_K=.html">Sangram K. Jena</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Das:Gautam_K=.html">Gautam K. Das</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11997">PDF</a><br/><b>Abstract: </b>Let $G=(V,E)$ be an undirected graph. We call $D_t \subseteq V$ as a total
dominating set (TDS) of $G$ if each vertex $v \in V$ has a dominator in $D$
other than itself. Here we consider the TDS problem in unit disk graphs, where
the objective is to find a minimum cardinality total dominating set for an
input graph. We prove that the TDS problem is NP-hard in unit disk graphs.
Next, we propose an 8-factor approximation algorithm for the problem. The
running time of the proposed approximation algorithm is $O(n \log k)$, where
$n$ is the number of vertices of the input graph and $k$ is output size. We
also show that TDS problem admits a PTAS in unit disk graphs.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11964</id>
    <link href="http://arxiv.org/abs/2007.11964" rel="alternate" type="text/html"/>
    <title>Sign-curing local Hamiltonians: termwise versus global stoquasticity and the use of Clifford transformations</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ioannou:Marios.html">Marios Ioannou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Piddock:Stephen.html">Stephen Piddock</a>, Milad Marvian, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klassen:Joel.html">Joel Klassen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Terhal:Barbara_M=.html">Barbara M. Terhal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11964">PDF</a><br/><b>Abstract: </b>We elucidate the distinction between global and termwise stoquasticity for
local Hamiltonians and prove several complexity results. We prove coNP-hardness
of deciding global stoquasticity in a fixed basis and $\Sigma_2^p$-hardness of
deciding global stoquasticity under single-qubit transformations. We expand the
class of sign-curing transformations by showing how Clifford transformations
can sign-cure a class of disordered 1D XYZ Hamiltonians.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11868</id>
    <link href="http://arxiv.org/abs/2007.11868" rel="alternate" type="text/html"/>
    <title>Two-way Greedy: Algorithms for Imperfect Rationality</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferraioli:Diodato.html">Diodato Ferraioli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Penna:Paolo.html">Paolo Penna</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Ventre:Carmine.html">Carmine Ventre</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11868">PDF</a><br/><b>Abstract: </b>The realization that selfish interests need to be accounted for in the design
of algorithms has produced many contributions in computer science under the
umbrella of algorithmic mechanism design. Novel algorithmic properties and
paradigms have been identified and studied. Our work stems from the observation
that selfishness is different from rationality; agents will attempt to
strategize whenever they perceive it to be convenient according to their
imperfect rationality. Recent work has focused on a particular notion of
imperfect rationality, namely absence of contingent reasoning skills, and
defined obvious strategyproofness (OSP) as a way to deal with the selfishness
of these agents. Essentially, this definition states that to care for the
incentives of these agents, we need not only pay attention about the
relationship between input and output, but also about the way the algorithm is
run. However, it is not clear what algorithmic approaches must be used for OSP.
In this paper, we show that, for binary allocation problems, OSP is fully
captured by a combination of two well-known algorithmic techniques: forward and
reverse greedy. We call two-way greedy this algorithmic design paradigm. Our
main technical contribution establishes the connection between OSP and two-way
greedy. We build upon the recently introduced cycle monotonicity technique for
OSP. By means of novel structural properties of cycles and queries of OSP
mechanisms, we fully characterize these mechanisms in terms of extremal
implementations. These are protocols that ask each agent to consistently
separate one extreme of their domain at the current history from the rest.
Through the connection with the greedy paradigm, we are able to import a host
of approximation bounds to OSP and strengthen the strategic properties of this
family of algorithms. Finally, we begin exploring the power of two-way greedy
for set systems.
</p></div>
    </summary>
    <updated>2020-07-26T23:25:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11863</id>
    <link href="http://arxiv.org/abs/2007.11863" rel="alternate" type="text/html"/>
    <title>Plane augmentation of plane graphs to meet parity constraints</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>J. C. Catana, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garc=iacute=a:A=.html">A. García</a>, J. Tejel, J. Urrutia <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11863">PDF</a><br/><b>Abstract: </b>A plane topological graph $G=(V,E)$ is a graph drawn in the plane whose
vertices are points in the plane and whose edges are simple curves that do not
intersect, except at their endpoints. Given a plane topological graph $G=(V,E)$
and a set $C_G$ of parity constraints, in which every vertex has assigned a
parity constraint on its degree, either even or odd, we say that $G$ is
\emph{topologically augmentable} to meet $C_G$ if there exits a plane
topological graph $H$ on the same set of vertices, such that $G$ and $H$ are
edge-disjoint and their union is a plane topological graph that meets all
parity constraints.
</p>
<p>In this paper, we prove that the problem of deciding if a plane topological
graph is topologically augmentable to meet parity constraints is
$\mathcal{NP}$-complete, even if the set of vertices that must change their
parities is $V$ or the set of vertices with odd degree. In particular, deciding
if a plane topological graph can be augmented to a Eulerian plane topological
graph is $\mathcal{NP}$-complete. Analogous complexity results are obtained,
when the augmentation must be done by a plane topological perfect matching
between the vertices not meeting their parities.
</p>
<p>We extend these hardness results to planar graphs, when the augmented graph
must be planar, and to plane geometric graphs (plane topological graphs whose
edges are straight-line segments). In addition, when it is required that the
augmentation is made by a plane geometric perfect matching between the vertices
not meeting their parities, we also prove that this augmentation problem is
$\mathcal{NP}$-complete for plane geometric trees and paths.
</p>
<p>For the particular family of maximal outerplane graphs, we characterize
maximal outerplane graphs that are topological augmentable to satisfy a set of
parity constraints.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11773</id>
    <link href="http://arxiv.org/abs/2007.11773" rel="alternate" type="text/html"/>
    <title>FPT Approximation for Constrained Metric $k$-Median/Means</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Dishant.html">Dishant Goyal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaiswal:Ragesh.html">Ragesh Jaiswal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Amit.html">Amit Kumar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11773">PDF</a><br/><b>Abstract: </b>The Metric $k$-median problem over a metric space $(\mathcal{X}, d)$ is
defined as follows: given a set $L \subseteq \mathcal{X}$ of facility locations
and a set $C \subseteq \mathcal{X}$ of clients, open a set $F \subseteq L$ of
$k$ facilities such that the total service cost, defined as $\Phi(F, C) \equiv
\sum_{x \in C} \min_{f \in F} d(x, f)$, is minimised. The metric $k$-means
problem is defined similarly using squared distances. In many applications
there are additional constraints that any solution needs to satisfy. This gives
rise to different constrained versions of the problem such as $r$-gather,
fault-tolerant, outlier $k$-means/$k$-median problem. Surprisingly, for many of
these constrained problems, no constant-approximation algorithm is known. We
give FPT algorithms with constant approximation guarantee for a range of
constrained $k$-median/means problems. For some of the constrained problems,
ours is the first constant factor approximation algorithm whereas for others,
we improve or match the approximation guarantee of previous works. We work
within the unified framework of Ding and Xu that allows us to simultaneously
obtain algorithms for a range of constrained problems. In particular, we obtain
a $(3+\varepsilon)$-approximation and $(9+\varepsilon)$-approximation for the
constrained versions of the $k$-median and $k$-means problem respectively in
FPT time. In many practical settings of the $k$-median/means problem, one is
allowed to open a facility at any client location, i.e., $C \subseteq L$. For
this special case, our algorithm gives a $(2+\varepsilon)$-approximation and
$(4+\varepsilon)$-approximation for the constrained versions of $k$-median and
$k$-means problem respectively in FPT time. Since our algorithm is based on
simple sampling technique, it can also be converted to a constant-pass
log-space streaming algorithm.
</p></div>
    </summary>
    <updated>2020-07-26T23:22:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11651</id>
    <link href="http://arxiv.org/abs/2007.11651" rel="alternate" type="text/html"/>
    <title>R*-Grove: Balanced Spatial Partitioning for Large-scale Datasets</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vu:Tin.html">Tin Vu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eldawy:Ahmed.html">Ahmed Eldawy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11651">PDF</a><br/><b>Abstract: </b>The rapid growth of big spatial data urged the research community to develop
several big spatial data systems. Regardless of their architecture, one of the
fundamental requirements of all these systems is to spatially partition the
data efficiently across machines. The core challenges of big spatial
partitioning are building high spatial quality partitions while simultaneously
taking advantages of distributed processing models by providing load balanced
partitions. Previous works on big spatial partitioning are to reuse existing
index search trees as-is, e.g., the R-tree family, STR, Kd-tree, and Quad-tree,
by building a temporary tree for a sample of the input and use its leaf nodes
as partition boundaries. However, we show in this paper that none of those
techniques has addressed the mentioned challenges completely. This paper
proposes a novel partitioning method, termed R*-Grove, which can partition very
large spatial datasets into high quality partitions with excellent load balance
and block utilization. This appealing property allows R*-Grove to outperform
existing techniques in spatial query processing. R*-Grove can be easily
integrated into any big data platforms such as Apache Spark or Apache Hadoop.
Our experiments show that R*-Grove outperforms the existing partitioning
techniques for big spatial data systems. With all the proposed work publicly
available as open source, we envision that R*-Grove will be adopted by the
community to better serve big spatial data research.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11643</id>
    <link href="http://arxiv.org/abs/2007.11643" rel="alternate" type="text/html"/>
    <title>FPT-space Graph Kernelizations</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kammer:Frank.html">Frank Kammer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sajenko:Andrej.html">Andrej Sajenko</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11643">PDF</a><br/><b>Abstract: </b>Let $n$ be the size of a parametrized problem and $k$ the parameter. We
present polynomial-time kernelizations for Cluster Editing/Deletion, Path
Contractions and Feedback Vertex Set that run with $O(\mathrm{poly}(k) \log n)$
bits and compute a kernel of size polynomial in $k$. By first executing the new
kernelizations and subsequently the best known polynomial-time kernelizations
for the problem under consideration, we obtain the best known kernels in
polynomial time with $O(\mathrm{poly}(k) \log n)$ bits.
</p>
<p>Our kernelization for Feedback Vertex Set computes in a first step an
approximated solution, which can be used to build a simple algorithm for
undirected $s$-$t$-connectivity (USTCON) that runs in polynomial time and with
$O(\mathrm{poly}(k) \log n)$ bits.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11636</id>
    <link href="http://arxiv.org/abs/2007.11636" rel="alternate" type="text/html"/>
    <title>Light Euclidean Spanners with Steiner Points</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Hung.html">Hung Le</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomon:Shay.html">Shay Solomon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11636">PDF</a><br/><b>Abstract: </b>The FOCS'19 paper of Le and Solomon, culminating a long line of research on
Euclidean spanners, proves that the lightness (normalized weight) of the greedy
$(1+\epsilon)$-spanner in $\mathbb{R}^d$ is $\tilde{O}(\epsilon^{-d})$ for any
$d = O(1)$ and any $\epsilon = \Omega(n^{-\frac{1}{d-1}})$ (where $\tilde{O}$
hides polylogarithmic factors of $\frac{1}{\epsilon}$), and also shows the
existence of point sets in $\mathbb{R}^d$ for which any $(1+\epsilon)$-spanner
must have lightness $\Omega(\epsilon^{-d})$. Given this tight bound on the
lightness, a natural arising question is whether a better lightness bound can
be achieved using Steiner points.
</p>
<p>Our first result is a construction of Steiner spanners in $\mathbb{R}^2$ with
lightness $O(\epsilon^{-1} \log \Delta)$, where $\Delta$ is the spread of the
point set. In the regime of $\Delta \ll 2^{1/\epsilon}$, this provides an
improvement over the lightness bound of Le and Solomon [FOCS 2019]; this regime
of parameters is of practical interest, as point sets arising in real-life
applications (e.g., for various random distributions) have polynomially bounded
spread, while in spanner applications $\epsilon$ often controls the precision,
and it sometimes needs to be much smaller than $O(1/\log n)$. Moreover, for
spread polynomially bounded in $1/\epsilon$, this upper bound provides a
quadratic improvement over the non-Steiner bound of Le and Solomon [FOCS 2019],
We then demonstrate that such a light spanner can be constructed in
$O_{\epsilon}(n)$ time for polynomially bounded spread, where $O_{\epsilon}$
hides a factor of $\mathrm{poly}(\frac{1}{\epsilon})$. Finally, we extend the
construction to higher dimensions, proving a lightness upper bound of
$\tilde{O}(\epsilon^{-(d+1)/2} + \epsilon^{-2}\log \Delta)$ for any $3\leq d =
O(1)$ and any $\epsilon = \Omega(n^{-\frac{1}{d-1}})$.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.11582</id>
    <link href="http://arxiv.org/abs/2007.11582" rel="alternate" type="text/html"/>
    <title>The importance of the spectral gap in estimating ground-state energies</title>
    <feedworld_mtime>1595721600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deshpande:Abhinav.html">Abhinav Deshpande</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gorshkov:Alexey_V=.html">Alexey V. Gorshkov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fefferman:Bill.html">Bill Fefferman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.11582">PDF</a><br/><b>Abstract: </b>The field of quantum Hamiltonian complexity lies at the intersection of
quantum many-body physics and computational complexity theory, with deep
implications to both fields. The main object of study is the LocalHamiltonian
problem, which is concerned with estimating the ground-state energy of a local
Hamiltonian and is complete for the class QMA, a quantum generalization of the
class NP. A major challenge in the field is to understand the complexity of the
LocalHamiltonian problem in more physically natural parameter regimes. One
crucial parameter in understanding the ground space of any Hamiltonian in
many-body physics is the spectral gap, which is the difference between the
smallest two eigenvalues. Despite its importance in quantum many-body physics,
the role played by the spectral gap in the complexity of the LocalHamiltonian
is less well-understood. In this work, we make progress on this question by
considering the precise regime, in which one estimates the ground-state energy
to within inverse exponential precision. Computing ground-state energies
precisely is a task that is important for quantum chemistry and quantum
many-body physics.
</p>
<p>In the setting of inverse-exponential precision, there is a surprising result
that the complexity of LocalHamiltonian is magnified from QMA to PSPACE, the
class of problems solvable in polynomial space. We clarify the reason behind
this boost in complexity. Specifically, we show that the full complexity of the
high precision case only comes about when the spectral gap is exponentially
small. As a consequence of the proof techniques developed to show our results,
we uncover important implications for the representability and circuit
complexity of ground states of local Hamiltonians, the theory of uniqueness of
quantum witnesses, and techniques for the amplification of quantum witnesses in
the presence of postselection.
</p></div>
    </summary>
    <updated>2020-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/111</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/111" rel="alternate" type="text/html"/>
    <title>TR20-111 |  Lifting: As Easy As 1,2,3 | 

	Ian Mertz, 

	Toniann Pitassi</title>
    <summary>Query-to-communication lifting theorems translate lower bounds on query complexity to lower bounds for the corresponding communication model. In this paper, we give a simplified proof of deterministic lifting (in both the tree-like and dag-like settings). Whereas previous proofs used sophisticated Fourier analytic techniques, our proof uses elementary counting together with the sunflower lemma.</summary>
    <updated>2020-07-25T05:05:34Z</updated>
    <published>2020-07-25T05:05:34Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-27T18:20:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8081773524220393104</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8081773524220393104/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/virtual-complexity.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8081773524220393104" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8081773524220393104" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/virtual-complexity.html" rel="alternate" type="text/html"/>
    <title>Virtual Complexity</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The <a href="https://computationalcomplexity.org/Archive/2020/fullsite/">Complexity Complexity Conference</a>, the conference that shares its name and URL with this blog, originally scheduled for Saarbrücken will be held virtually next week. Registration is free for non-authors. <a href="https://www.youtube.com/channel/UCXgNLnzWOP4bM2-xfHDHUrw">Talks</a> are already posted. Looking forward to seeing you at the business meeting and the social.<br/>
<div>
<br/></div>
<div>
<div>
Award winners have already been announced: The Best Student Paper Award goes to Rahul Ilango for <a href="https://drops.dagstuhl.de/opus/volltexte/2020/12583/">Connecting Perebor Conjectures: Towards a Search to Decision Reduction for Minimizing Formulas</a> and the Best Paper Award goes to Daniel Dadush and Samarth Tiwari for <a href="https://drops.dagstuhl.de/opus/volltexte/2020/12586/">On the Complexity of Branching Proofs</a>.</div>
</div>
<div>
<br/></div>
<div>
Virtual conferences give an opportunity for far more people to attend since you don't have the expense and time needed to go to Germany. On the other hand it's hard to dedicate time for a conference when you aren't there. I missed STOC which would have been walking distance from where I live but I did attend parts of the <a href="http://ec20.sigecom.org/">Economics and Computation</a> conference which was supposed to be in Budapest. EC made great use of <a href="http://gather.town/">gather.town</a> where you can wander around virtual rooms bumping into and talking to people. I caught up with a few people there. Complexity plans to use gather for its social meeting next week. Looking forward to the virtual beer.</div></div>
    </content>
    <updated>2020-07-24T16:21:00Z</updated>
    <published>2020-07-24T16:21:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-07-27T14:26:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=19859</id>
    <link href="https://gilkalai.wordpress.com/2020/07/24/noam-lifshitz-a-new-hypercontractivity-inequality-the-proof/" rel="alternate" type="text/html"/>
    <title>Noam Lifshitz: A new hypercontractivity inequality — The proof!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is a guest post kindly contributed by Noam Lifshitz. Here is a pdf version.  This post is a continuation of the post  To cheer you up in difficult times 3: A guest post by Noam Lifshitz on the new … <a href="https://gilkalai.wordpress.com/2020/07/24/noam-lifshitz-a-new-hypercontractivity-inequality-the-proof/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>This is a guest post kindly contributed by Noam Lifshitz</em>. <em>Here is a <a href="https://gilkalai.files.wordpress.com/2020/07/proof-of-hypercontractivity.pdf">pdf version</a>.  This post is a continuation of the post  </em><a href="https://gilkalai.wordpress.com/2020/05/08/to-cheer-you-up-in-difficult-times-3-a-guest-post-by-noam-lifshitz-on-the-new-hypercontractivity-inequality-of-peter-keevash-noam-lifshitz-eoin-long-and-dor-minzer/" rel="bookmark">To cheer you up in difficult times 3: A guest post by Noam Lifshitz on the new hypercontractivity inequality of Peter Keevash, Noam Lifshitz, Eoin Long and Dor Minzer</a>, <em>and it gives the proof of the new hypercontractive inequality. We plan a third post where various applications will be mentioned.</em></p>
<p>Before we get to the post I want to mention that there are a lot of activities on the web. I may devote a special post to links and discussion (and contributing links in the comment section is very welcome.) but meanwhile a few links:  1) <a href="https://simons.berkeley.edu/events/boolean">Advances in Boolean Function Analysis Lecture Series</a> (thanks to Avishay Tal and Prasad Raghavendra for letting me know); 2) <a href="https://math216.wordpress.com/agittoc-2020/">Online course in Foundations of Algebraic Geometry</a> Given by Ravi Vakil from Stanford. You can take the course at varying levels of involvement. (Thanks to Tami Ziegler for telling me) A very very interesting way of online teaching. 3) <a href="https://researchseminars.org/">A site with online mathematical lectures.</a></p>
<h2><img alt="Bonami" class="alignnone size-full wp-image-19984" height="480" src="https://gilkalai.files.wordpress.com/2020/07/bonami.jpg?w=640&amp;h=480" width="640"/></h2>
<p><span style="color: #ff0000;">Aline Bonami with Szilard Revesz and me (2006). Aline Bonami first proved the 2-point hypercontractive inequality which is very useful in the analysis of Boolean functions. (Leonard Gross proved it independently a few years later and William Beckner found important applications to harmonic analysis.)</span></p>
<h2>Proof of the new hypercontractivity inequality</h2>
<p>Our aim is to prove the hypercontractivity theorem for global functions. The proof here is taken from a joint paper with David Ellis and Guy Kindler that’ll soon be out on the Arxiv.</p>
<h3><strong>Theorem 1:</strong></h3>
<p><img alt="\displaystyle \|\mathrm{T}_{1/100}f\|_{4}^{4}\le\sum_{S\subseteq\left[n\right]}\mathbb{E}_{x\in\left\{ 1,\ldots,m\right\} ^{S}}\|L_{S}[f]_{S\rightarrow x}\|_{2}^{4}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B1%2F100%7Df%5C%7C_%7B4%7D%5E%7B4%7D%5Cle%5Csum_%7BS%5Csubseteq%5Cleft%5Bn%5Cright%5D%7D%5Cmathbb%7BE%7D_%7Bx%5Cin%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7BS%7D%7D%5C%7CL_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D%5C%7C_%7B2%7D%5E%7B4%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|\mathrm{T}_{1/100}f\|_{4}^{4}\le\sum_{S\subseteq\left[n\right]}\mathbb{E}_{x\in\left\{ 1,\ldots,m\right\} ^{S}}\|L_{S}[f]_{S\rightarrow x}\|_{2}^{4}, "/></p>
<p>Here we use the notations given in the <a href="https://gilkalai.wordpress.com/2020/05/08/to-cheer-you-up-in-difficult-times-3-a-guest-post-by-noam-lifshitz-on-the-new-hypercontractivity-inequality-of-peter-keevash-noam-lifshitz-eoin-long-and-dor-minzer/">last blog post</a>. Let us first get a feel for our hypercontractivity theorem by proving the <img alt="{n=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=1}"/> case. Here the RHS is <img alt="{\|f\|_{2}^{4}+\|f-\mathbb{E}\left[f\right]\|_{4}^{4}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Cf%5C%7C_%7B2%7D%5E%7B4%7D%2B%5C%7Cf-%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5C%7C_%7B4%7D%5E%7B4%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|f\|_{2}^{4}+\|f-\mathbb{E}\left[f\right]\|_{4}^{4}.}"/></p>
<h3>1. Proof of the <img alt="{n=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=1}"/> case</h3>
<p>We will prove the following slightly stronger version of Theorem 1 for the <img alt="{n=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=1}"/> case.</p>
<p><strong>Proposition 2:</strong></p>
<p>Let <img alt="{f\colon\left\{ 1,\ldots,m\right\} \rightarrow\mathbb{C}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5Crightarrow%5Cmathbb%7BC%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left\{ 1,\ldots,m\right\} \rightarrow\mathbb{C}.}"/> Let <img alt="{\rho\le\frac{1}{10}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%5Cle%5Cfrac%7B1%7D%7B10%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho\le\frac{1}{10}.}"/> Then<br/>
<img alt="\displaystyle \|\mathrm{T}_{\rho}f\|_{4}^{4}\le\|f\|_{2}^{4}+\|f-\mathbb{E}\left[f\right]\|_{4}^{4}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Crho%7Df%5C%7C_%7B4%7D%5E%7B4%7D%5Cle%5C%7Cf%5C%7C_%7B2%7D%5E%7B4%7D%2B%5C%7Cf-%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5C%7C_%7B4%7D%5E%7B4%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|\mathrm{T}_{\rho}f\|_{4}^{4}\le\|f\|_{2}^{4}+\|f-\mathbb{E}\left[f\right]\|_{4}^{4}. "/></p>
<p style="text-align: left;"><strong>Proof</strong>: Let us write <img alt="{L\left[f\right]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5Cleft%5Bf%5Cright%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L\left[f\right]}"/> for <img alt="{L_{1}\left[f\right]=f-\mathbb{E}\left[f\right].}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7B1%7D%5Cleft%5Bf%5Cright%5D%3Df-%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{1}\left[f\right]=f-\mathbb{E}\left[f\right].}"/> Rearranging, we have<br/>
<img alt="\displaystyle f=\mathbb{E}\left[f\right]+L\left[f\right]. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f%3D%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%2BL%5Cleft%5Bf%5Cright%5D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle f=\mathbb{E}\left[f\right]+L\left[f\right]. "/><br/>
The noise operator in the <img alt="{n=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=1}"/> case is by definition equal to <img alt="{\rho Id+\left(1-\rho\right)\mathbb{E},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho+Id%2B%5Cleft%281-%5Crho%5Cright%29%5Cmathbb%7BE%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho Id+\left(1-\rho\right)\mathbb{E},}"/> where <img alt="{\mathbb{E}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{E}}"/> is the expectation over <img alt="{\text{\ensuremath{\left\{ 1,\ldots,m\right\} }}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctext%7B%5Censuremath%7B%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\text{\ensuremath{\left\{ 1,\ldots,m\right\} }}}"/> operator, and <img alt="{Id}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BId%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Id}"/> is the identity operator. Hence,<br/>
<img alt="\displaystyle \mathrm{T}_{\rho}f=\mathbb{E}\left[f\right]+\rho L[f]. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BT%7D_%7B%5Crho%7Df%3D%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%2B%5Crho+L%5Bf%5D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \mathrm{T}_{\rho}f=\mathbb{E}\left[f\right]+\rho L[f]. "/></p>
<p>Now when expanding the 4-norm of the function <img alt="{\|\mathrm{T}_{1/100}f\|_{4}^{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7C%5Cmathrm%7BT%7D_%7B1%2F100%7Df%5C%7C_%7B4%7D%5E%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|\mathrm{T}_{1/100}f\|_{4}^{4}}"/>, we obtain</p>
<p style="text-align: center;"><img alt="\displaystyle \|\mathrm{T}_{\rho}f\|_{4}^{4}&#xA0; \le\left|\mathbb{E}\left[f\right]\right|^{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Crho%7Df%5C%7C_%7B4%7D%5E%7B4%7D%C2%A0+%5Cle%5Cleft%7C%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%7C%5E%7B4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \|\mathrm{T}_{\rho}f\|_{4}^{4}&#xA0; \le\left|\mathbb{E}\left[f\right]\right|^{4}"/><br/>
<img alt="\displaystyle +6\rho^{2}\left|\mathbb{E}\left[f\right]\right|^{2}\|Lf\|_{2}^{2}+" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%2B6%5Crho%5E%7B2%7D%5Cleft%7C%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%7C%5E%7B2%7D%5C%7CLf%5C%7C_%7B2%7D%5E%7B2%7D%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle +6\rho^{2}\left|\mathbb{E}\left[f\right]\right|^{2}\|Lf\|_{2}^{2}+"/><br/>
<img alt="\displaystyle&#xA0; +4\rho^{3}\left|\mathbb{E}\left[f\right]\right|\|L\left[f\right]\|_{3}^{3}+" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%C2%A0+%2B4%5Crho%5E%7B3%7D%5Cleft%7C%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%7C%5C%7CL%5Cleft%5Bf%5Cright%5D%5C%7C_%7B3%7D%5E%7B3%7D%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle&#xA0; +4\rho^{3}\left|\mathbb{E}\left[f\right]\right|\|L\left[f\right]\|_{3}^{3}+"/><br/>
<img alt="\displaystyle + \rho^{4}\|L\left[f\right]\|_{4}^{4}," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%2B+%5Crho%5E%7B4%7D%5C%7CL%5Cleft%5Bf%5Cright%5D%5C%7C_%7B4%7D%5E%7B4%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle + \rho^{4}\|L\left[f\right]\|_{4}^{4},"/></p>
<p>where we used the fact that the expectation of <img alt="{L\left[f\right]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5Cleft%5Bf%5Cright%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L\left[f\right]}"/> is 0. When looking at the right hand side of the global hypercontractivity theorem, we see most of the above terms except for the one involving the third norm of the Laplacian. Indeed we have</p>
<p style="text-align: center;"><img alt="\displaystyle RHS =\|f\|_{2}^{4}+\|L\left[f\right]\|_{4}^{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+RHS+%3D%5C%7Cf%5C%7C_%7B2%7D%5E%7B4%7D%2B%5C%7CL%5Cleft%5Bf%5Cright%5D%5C%7C_%7B4%7D%5E%7B4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle RHS =\|f\|_{2}^{4}+\|L\left[f\right]\|_{4}^{4}"/><br/>
<img alt="\displaystyle =\left|\mathbb{E}\left[f\right]\right|^{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D%5Cleft%7C%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%7C%5E%7B4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle =\left|\mathbb{E}\left[f\right]\right|^{4}"/><br/>
<img alt="\displaystyle +2\|L\left[f\right]\|_{2}^{2}\left|\mathbb{E}\left[f\right]\right|^{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%2B2%5C%7CL%5Cleft%5Bf%5Cright%5D%5C%7C_%7B2%7D%5E%7B2%7D%5Cleft%7C%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%7C%5E%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle +2\|L\left[f\right]\|_{2}^{2}\left|\mathbb{E}\left[f\right]\right|^{2}"/><br/>
<img alt="\displaystyle +\|L\left[f\right]\|_{2}^{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%2B%5C%7CL%5Cleft%5Bf%5Cright%5D%5C%7C_%7B2%7D%5E%7B4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle +\|L\left[f\right]\|_{2}^{4}"/><br/>
<img alt="\displaystyle +\|L\text{\ensuremath{\left[f\right]\|}}_{4}^{4}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%2B%5C%7CL%5Ctext%7B%5Censuremath%7B%5Cleft%5Bf%5Cright%5D%5C%7C%7D%7D_%7B4%7D%5E%7B4%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle +\|L\text{\ensuremath{\left[f\right]\|}}_{4}^{4}."/></p>
<p>Hence we see that the only term in the left hand side that doesn’t appear with a greater coefficient in the left hand side is the term <img alt="{\left|\mathbb{E}\left[f\right]\right|\|L\left[f\right]\|_{3}^{3},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%7C%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%7C%5C%7CL%5Cleft%5Bf%5Cright%5D%5C%7C_%7B3%7D%5E%7B3%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left|\mathbb{E}\left[f\right]\right|\|L\left[f\right]\|_{3}^{3},}"/> and by AM-GM we have</p>
<p style="text-align: center;"><img alt="\displaystyle \left|\mathbb{E}\left[f\right]\right|\|L\left[f\right]\|_{3}^{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cleft%7C%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%7C%5C%7CL%5Cleft%5Bf%5Cright%5D%5C%7C_%7B3%7D%5E%7B3%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \left|\mathbb{E}\left[f\right]\right|\|L\left[f\right]\|_{3}^{3}"/> <img alt="\displaystyle =\mathbb{E}\left[\left|\mathbb{E}\left[f\right]\right|\left|L\left[f\right]\right|^{3}\right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D%5Cmathbb%7BE%7D%5Cleft%5B%5Cleft%7C%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%7C%5Cleft%7CL%5Cleft%5Bf%5Cright%5D%5Cright%7C%5E%7B3%7D%5Cright%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle =\mathbb{E}\left[\left|\mathbb{E}\left[f\right]\right|\left|L\left[f\right]\right|^{3}\right]"/><br/>
<img alt="\displaystyle \le\mathbb{E}\left[\frac{\left|\mathbb{E}\left[f\right]\right|^{2}\left|L\left[f\right]\right|^{2}+\left|L\text{\ensuremath{\left[f\right]}}\right|^{4}}{2}\right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cle%5Cmathbb%7BE%7D%5Cleft%5B%5Cfrac%7B%5Cleft%7C%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%7C%5E%7B2%7D%5Cleft%7CL%5Cleft%5Bf%5Cright%5D%5Cright%7C%5E%7B2%7D%2B%5Cleft%7CL%5Ctext%7B%5Censuremath%7B%5Cleft%5Bf%5Cright%5D%7D%7D%5Cright%7C%5E%7B4%7D%7D%7B2%7D%5Cright%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \le\mathbb{E}\left[\frac{\left|\mathbb{E}\left[f\right]\right|^{2}\left|L\left[f\right]\right|^{2}+\left|L\text{\ensuremath{\left[f\right]}}\right|^{4}}{2}\right]"/><br/>
<img alt="\displaystyle =\frac{1}{2}\left|\mathbb{E}\left[f\right]\right|^{2}\|L\left[f\right]\|_{2}^{2}+\frac{1}{2}\|L\left[f\right]\|_{4}^{4}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D%5Cfrac%7B1%7D%7B2%7D%5Cleft%7C%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%7C%5E%7B2%7D%5C%7CL%5Cleft%5Bf%5Cright%5D%5C%7C_%7B2%7D%5E%7B2%7D%2B%5Cfrac%7B1%7D%7B2%7D%5C%7CL%5Cleft%5Bf%5Cright%5D%5C%7C_%7B4%7D%5E%7B4%7D%2C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle =\frac{1}{2}\left|\mathbb{E}\left[f\right]\right|^{2}\|L\left[f\right]\|_{2}^{2}+\frac{1}{2}\|L\left[f\right]\|_{4}^{4}, "/></p>
<p>which allows us to upper bound the only term appearing in the left hand side but not in the right hand side by corresponding terms that do appear in the right hand side. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<h3>2. Tensorisation lemma</h3>
<p>Next we are going to prove a theorem that doesn’t seem to fit to our setting, but we’re going to fit it in by force. Let <img alt="{X,Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%2CY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X,Y}"/> be finite sets. Let us write <img alt="{\mathcal{F}\left(X\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5Cleft%28X%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}\left(X\right)}"/> for the linear space of complex valued functions on <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>. The space <img alt="{\mathcal{F}\left(X\times Y\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5Cleft%28X%5Ctimes+Y%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}\left(X\times Y\right)}"/> can be identified with the space <img alt="{\mathcal{F}\left(X\right)\otimes\mathcal{F}\left(Y\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5Cleft%28X%5Cright%29%5Cotimes%5Cmathcal%7BF%7D%5Cleft%28Y%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}\left(X\right)\otimes\mathcal{F}\left(Y\right),}"/> where a pair of function <img alt="{f\otimes g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Cotimes+g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\otimes g}"/> is identified with the function<br/>
<img alt="\displaystyle \left(x,y\right)\mapsto f\left(x\right)g\left(y\right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cleft%28x%2Cy%5Cright%29%5Cmapsto+f%5Cleft%28x%5Cright%29g%5Cleft%28y%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \left(x,y\right)\mapsto f\left(x\right)g\left(y\right) "/><br/>
in <img alt="{\mathcal{F}\left(X\times Y\right).}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5Cleft%28X%5Ctimes+Y%5Cright%29.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}\left(X\times Y\right).}"/></p>
<p>Given two operators <img alt="{A_{1}\colon\mathcal{F}\left(X_{1}\right)\rightarrow\mathcal{F}\left(Y_{1}\right),A_{2}\colon\mathcal{F}\left(X_{2}\right)\rightarrow\mathcal{F}\left(Y_{2}\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B1%7D%5Ccolon%5Cmathcal%7BF%7D%5Cleft%28X_%7B1%7D%5Cright%29%5Crightarrow%5Cmathcal%7BF%7D%5Cleft%28Y_%7B1%7D%5Cright%29%2CA_%7B2%7D%5Ccolon%5Cmathcal%7BF%7D%5Cleft%28X_%7B2%7D%5Cright%29%5Crightarrow%5Cmathcal%7BF%7D%5Cleft%28Y_%7B2%7D%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{1}\colon\mathcal{F}\left(X_{1}\right)\rightarrow\mathcal{F}\left(Y_{1}\right),A_{2}\colon\mathcal{F}\left(X_{2}\right)\rightarrow\mathcal{F}\left(Y_{2}\right)}"/>, the operator <img alt="{A_{1}\otimes A_{2}\colon\mathcal{F}\left(X_{1}\times X_{2}\right)\rightarrow\mathcal{F}\left(Y_{1}\times Y_{2}\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B1%7D%5Cotimes+A_%7B2%7D%5Ccolon%5Cmathcal%7BF%7D%5Cleft%28X_%7B1%7D%5Ctimes+X_%7B2%7D%5Cright%29%5Crightarrow%5Cmathcal%7BF%7D%5Cleft%28Y_%7B1%7D%5Ctimes+Y_%7B2%7D%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{1}\otimes A_{2}\colon\mathcal{F}\left(X_{1}\times X_{2}\right)\rightarrow\mathcal{F}\left(Y_{1}\times Y_{2}\right)}"/> is the unique operator sending <img alt="{f\otimes g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Cotimes+g%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\otimes g}"/> to <img alt="{A_{1}f\otimes A_{2}g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B1%7Df%5Cotimes+A_%7B2%7Dg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{1}f\otimes A_{2}g}"/>. We write <img alt="{A^{\otimes n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7B%5Cotimes+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^{\otimes n}}"/> for <img alt="{A\otimes\cdots\otimes A.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5Cotimes%5Ccdots%5Cotimes+A.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A\otimes\cdots\otimes A.}"/> The operator <img alt="{A_{1}\otimes A_{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B1%7D%5Cotimes+A_%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{1}\otimes A_{2}}"/> can also be defined more explictly in terms of its values on functions. The operator <img alt="{A_{1}\otimes A_{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B1%7D%5Cotimes+A_%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{1}\otimes A_{2}}"/> can be understood more explicitly by noting that it is the composition of the operators <img alt="{A_{1}\otimes I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B1%7D%5Cotimes+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{1}\otimes I}"/> and <img alt="{I\otimes A_{2}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BI%5Cotimes+A_%7B2%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{I\otimes A_{2}.}"/> Now the operator <img alt="{A\otimes I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5Cotimes+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A\otimes I}"/> is given by <img alt="{A\otimes If\left(x,y\right)=Af_{y}\left(x\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5Cotimes+If%5Cleft%28x%2Cy%5Cright%29%3DAf_%7By%7D%5Cleft%28x%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A\otimes If\left(x,y\right)=Af_{y}\left(x\right),}"/> where <img alt="{f_{y}\left(x\right)=f\left(x,y\right).}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7By%7D%5Cleft%28x%5Cright%29%3Df%5Cleft%28x%2Cy%5Cright%29.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{y}\left(x\right)=f\left(x,y\right).}"/></p>
<p><strong>Lemma 3:</strong> Let <img alt="{X,Y,Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%2CY%2CZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X,Y,Z}"/> be measure spaces with finite underlying sets. Let <img alt="{A\colon\mathcal{F}\left(X\right)\rightarrow\mathcal{F}\left(Y\right),B\colon\mathcal{F}\left(X\right)\rightarrow\mathcal{F}\left(Z\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5Ccolon%5Cmathcal%7BF%7D%5Cleft%28X%5Cright%29%5Crightarrow%5Cmathcal%7BF%7D%5Cleft%28Y%5Cright%29%2CB%5Ccolon%5Cmathcal%7BF%7D%5Cleft%28X%5Cright%29%5Crightarrow%5Cmathcal%7BF%7D%5Cleft%28Z%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A\colon\mathcal{F}\left(X\right)\rightarrow\mathcal{F}\left(Y\right),B\colon\mathcal{F}\left(X\right)\rightarrow\mathcal{F}\left(Z\right)}"/> be operators satisfying</p>
<p style="text-align: center;"><img alt="\displaystyle \|Af\|_{4}\le\|Bf\|_{4} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7CAf%5C%7C_%7B4%7D%5Cle%5C%7CBf%5C%7C_%7B4%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|Af\|_{4}\le\|Bf\|_{4} "/></p>
<p>for all functions <img alt="{f\in\mathcal{F}\left(X\right).}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin%5Cmathcal%7BF%7D%5Cleft%28X%5Cright%29.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\in\mathcal{F}\left(X\right).}"/><br/>
Then</p>
<p style="text-align: center;"><img alt="\displaystyle \|A^{\otimes n}f\|_{4}\le\|B^{\otimes n}f\|_{4} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7CA%5E%7B%5Cotimes+n%7Df%5C%7C_%7B4%7D%5Cle%5C%7CB%5E%7B%5Cotimes+n%7Df%5C%7C_%7B4%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|A^{\otimes n}f\|_{4}\le\|B^{\otimes n}f\|_{4} "/></p>
<p>for all <img alt="{f\in\mathcal{F}\left(X^{n}\right).}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin%5Cmathcal%7BF%7D%5Cleft%28X%5E%7Bn%7D%5Cright%29.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\in\mathcal{F}\left(X^{n}\right).}"/></p>
<p>Here the spaces <img alt="{X^{n},Y^{n},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%5E%7Bn%7D%2CY%5E%7Bn%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X^{n},Y^{n},}"/> and <img alt="{Z^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z^{n}}"/> are equipped with the product measure, where the measure of an atom is the product of the measures of its coordiates.</p>
<p><strong>Proof:</strong> For each <img alt="{y\in X,}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5Cin+X%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y\in X,}"/> let <img alt="{g_{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_{y}}"/> be given by <img alt="{g_{y}:=A^{\otimes\left(n-1\right)}f\left(\cdot,y\right).}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_%7By%7D%3A%3DA%5E%7B%5Cotimes%5Cleft%28n-1%5Cright%29%7Df%5Cleft%28%5Ccdot%2Cy%5Cright%29.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_{y}:=A^{\otimes\left(n-1\right)}f\left(\cdot,y\right).}"/> As mentioned <img alt="{A^{\otimes n}f\left(x,y\right)=Ag_{y}\left(x\right).}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7B%5Cotimes+n%7Df%5Cleft%28x%2Cy%5Cright%29%3DAg_%7By%7D%5Cleft%28x%5Cright%29.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^{\otimes n}f\left(x,y\right)=Ag_{y}\left(x\right).}"/> Hence by hypothesis, we have</p>
<p style="text-align: center;"><img alt="\displaystyle \mathbb{E}\left[\left|\mathrm{A}^{\otimes n}f\right|^{4}\right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathbb%7BE%7D%5Cleft%5B%5Cleft%7C%5Cmathrm%7BA%7D%5E%7B%5Cotimes+n%7Df%5Cright%7C%5E%7B4%7D%5Cright%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \mathbb{E}\left[\left|\mathrm{A}^{\otimes n}f\right|^{4}\right]"/> <img alt="\displaystyle =\mathbb{E}_{y}\mathbb{E}_{x}\left|Ag_{y}\left(x\right)\right|^{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D%5Cmathbb%7BE%7D_%7By%7D%5Cmathbb%7BE%7D_%7Bx%7D%5Cleft%7CAg_%7By%7D%5Cleft%28x%5Cright%29%5Cright%7C%5E%7B4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle =\mathbb{E}_{y}\mathbb{E}_{x}\left|Ag_{y}\left(x\right)\right|^{4}"/><br/>
<img alt="\displaystyle&#xA0; \le\mathbb{E}_{y}\mathbb{E}_{x}\left|Bg_{y}\left(x\right)\right|^{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%C2%A0+%5Cle%5Cmathbb%7BE%7D_%7By%7D%5Cmathbb%7BE%7D_%7Bx%7D%5Cleft%7CBg_%7By%7D%5Cleft%28x%5Cright%29%5Cright%7C%5E%7B4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle&#xA0; \le\mathbb{E}_{y}\mathbb{E}_{x}\left|Bg_{y}\left(x\right)\right|^{4}"/><br/>
<img alt="\displaystyle =\|A^{\otimes n-1}\otimes B\|_{4}^{4}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D%5C%7CA%5E%7B%5Cotimes+n-1%7D%5Cotimes+B%5C%7C_%7B4%7D%5E%7B4%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle =\|A^{\otimes n-1}\otimes B\|_{4}^{4}."/> We may now repeat the same process on each of the other coordinates to replace the <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>s by <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>s one by one. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<h3>3. The main idea: Fourifying the 2-norms.</h3>
<p>The strategy of our proof is to take the theorem</p>
<p style="text-align: center;"><img alt="\displaystyle \|\mathrm{T}_{\rho}f\|_{4}\le\|f\|_{2}^{4}+\|f-\mathbb{E}\left[f\right]\|_{4}^{4}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Crho%7Df%5C%7C_%7B4%7D%5Cle%5C%7Cf%5C%7C_%7B2%7D%5E%7B4%7D%2B%5C%7Cf-%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5C%7C_%7B4%7D%5E%7B4%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|\mathrm{T}_{\rho}f\|_{4}\le\|f\|_{2}^{4}+\|f-\mathbb{E}\left[f\right]\|_{4}^{4}, "/></p>
<p>which we established in the <img alt="{n=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=1}"/> case for <img alt="{\rho\le\frac{1}{10}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%5Cle%5Cfrac%7B1%7D%7B10%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho\le\frac{1}{10}}"/>, and to turn it into an essentially equivalent statement about 4-norms. We will then get a tensorised statement for general <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, which we will be able to convert back into our hypercontractivity theorem for global functions. Our idea is to encode our function <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> as a function <img alt="{\mathrm{En}\left(f\right)\colon\left\{ -1,1\right\} ^{n\left(p-1\right)}\rightarrow\mathbb{R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BEn%7D%5Cleft%28f%5Cright%29%5Ccolon%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%5E%7Bn%5Cleft%28p-1%5Cright%29%7D%5Crightarrow%5Cmathbb%7BR%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{En}\left(f\right)\colon\left\{ -1,1\right\} ^{n\left(p-1\right)}\rightarrow\mathbb{R}}"/> satisfying</p>
<p style="text-align: center;"><img alt="\displaystyle \mathrm{En}\circ T_{\rho}=\mathrm{T}_{\rho}\circ\mathrm{En} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BEn%7D%5Ccirc+T_%7B%5Crho%7D%3D%5Cmathrm%7BT%7D_%7B%5Crho%7D%5Ccirc%5Cmathrm%7BEn%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \mathrm{En}\circ T_{\rho}=\mathrm{T}_{\rho}\circ\mathrm{En} "/></p>
<p>and</p>
<p style="text-align: center;"><img alt="\displaystyle \|\mathrm{En}f\|_{2}=\|f\|_{2}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BEn%7Df%5C%7C_%7B2%7D%3D%5C%7Cf%5C%7C_%7B2%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|\mathrm{En}f\|_{2}=\|f\|_{2}. "/></p>
<p>The benefit of working with <img alt="{\mathrm{En}f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BEn%7Df%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{En}f}"/> rather than <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> is that in <img alt="{\left\{ 0,1\right\} ^{n\left(p-1\right)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7B+0%2C1%5Cright%5C%7D+%5E%7Bn%5Cleft%28p-1%5Cright%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left\{ 0,1\right\} ^{n\left(p-1\right)}}"/> one may move between 4-norms and 2-norms by appealing to the hypercontractivity theorem there, which gives</p>
<p style="text-align: center;"><img alt="\displaystyle \|\mathrm{T}_{\frac{1}{\sqrt{3}}}\circ\mathrm{En}f\|_{4}\le\|\mathrm{E}nf\|_{2}\le\|\mathrm{En}f\|_{4} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%7D%5Ccirc%5Cmathrm%7BEn%7Df%5C%7C_%7B4%7D%5Cle%5C%7C%5Cmathrm%7BE%7Dnf%5C%7C_%7B2%7D%5Cle%5C%7C%5Cmathrm%7BEn%7Df%5C%7C_%7B4%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|\mathrm{T}_{\frac{1}{\sqrt{3}}}\circ\mathrm{En}f\|_{4}\le\|\mathrm{E}nf\|_{2}\le\|\mathrm{En}f\|_{4} "/></p>
<p>at the cost of some noise.</p>
<p>To define <img alt="{\mathrm{En}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BEn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{En}}"/> we use Fourier analysis of Abelian groups. Let us briefly recall it. For simplicity let us assume that <img alt="{f\colon\left(\mathbb{Z}/p\mathbb{Z}\right)^{n}\rightarrow\mathbb{C},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%28%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5Cright%29%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BC%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left(\mathbb{Z}/p\mathbb{Z}\right)^{n}\rightarrow\mathbb{C},}"/> where <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> is a prime. Let <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega}"/> be a <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/>th root of unity. For any <img alt="{\gamma\in\left(\mathbb{Z}/p\mathbb{Z}\right)^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma%5Cin%5Cleft%28%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5Cright%29%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\gamma\in\left(\mathbb{Z}/p\mathbb{Z}\right)^{n}}"/> we have a character <img alt="{\chi_{\gamma}\colon\left(\mathbb{Z}/p\mathbb{Z}\right)^{n}\rightarrow\mathbb{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_%7B%5Cgamma%7D%5Ccolon%5Cleft%28%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5Cright%29%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\chi_{\gamma}\colon\left(\mathbb{Z}/p\mathbb{Z}\right)^{n}\rightarrow\mathbb{C}}"/> given by <img alt="{\chi_{\gamma}\left(x\right)=\omega^{\left\langle \gamma,x\right\rangle }.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_%7B%5Cgamma%7D%5Cleft%28x%5Cright%29%3D%5Comega%5E%7B%5Cleft%5Clangle+%5Cgamma%2Cx%5Cright%5Crangle+%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\chi_{\gamma}\left(x\right)=\omega^{\left\langle \gamma,x\right\rangle }.}"/> The <img alt="{\chi_{\gamma}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_%7B%5Cgamma%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\chi_{\gamma}}"/> are an orthonormal basis of <img alt="{\left(\mathbb{Z}/p\right)^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%28%5Cmathbb%7BZ%7D%2Fp%5Cright%29%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left(\mathbb{Z}/p\right)^{n}}"/> and we write <img alt="{f=\sum\hat{f}\left(\gamma\right)\chi_{\gamma}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%3D%5Csum%5Chat%7Bf%7D%5Cleft%28%5Cgamma%5Cright%29%5Cchi_%7B%5Cgamma%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f=\sum\hat{f}\left(\gamma\right)\chi_{\gamma}}"/>, where <img alt="{\hat{f}\left(\gamma\right)=\left\langle f,\chi_{\gamma}\right\rangle .}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat%7Bf%7D%5Cleft%28%5Cgamma%5Cright%29%3D%5Cleft%5Clangle+f%2C%5Cchi_%7B%5Cgamma%7D%5Cright%5Crangle+.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\hat{f}\left(\gamma\right)=\left\langle f,\chi_{\gamma}\right\rangle .}"/> Note that <img alt="{\chi_{0}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_%7B0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\chi_{0}}"/> is the constant function, and so we have</p>
<p style="text-align: center;"><img alt="\displaystyle \hat{f}\left(0\right)=\left\langle f,\chi_{0}\right\rangle =\mathbb{E}\left[f\right], " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Chat%7Bf%7D%5Cleft%280%5Cright%29%3D%5Cleft%5Clangle+f%2C%5Cchi_%7B0%7D%5Cright%5Crangle+%3D%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \hat{f}\left(0\right)=\left\langle f,\chi_{0}\right\rangle =\mathbb{E}\left[f\right], "/></p>
<p>which gives</p>
<p style="text-align: center;"><img alt="\displaystyle f=\mathbb{E}\left[f\right]+\sum\hat{f}\left(i\right)\chi_{i}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f%3D%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%2B%5Csum%5Chat%7Bf%7D%5Cleft%28i%5Cright%29%5Cchi_%7Bi%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle f=\mathbb{E}\left[f\right]+\sum\hat{f}\left(i\right)\chi_{i}. "/></p>
<p>Our mission will first be to convert the <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>-norm of a function <img alt="{f\colon\mathbb{Z}/p\rightarrow\mathbb{R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cmathbb%7BZ%7D%2Fp%5Crightarrow%5Cmathbb%7BR%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\mathbb{Z}/p\rightarrow\mathbb{R}}"/> to the <img alt="{4-}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4-%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{4-}"/>norm of a different function.</p>
<p>We define an encoding operator <img alt="{\mathrm{En}\colon\mathbb{Z}/p\mathbb{Z}\rightarrow\left\{ -1,1\right\} ^{p-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BEn%7D%5Ccolon%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5Crightarrow%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%5E%7Bp-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{En}\colon\mathbb{Z}/p\mathbb{Z}\rightarrow\left\{ -1,1\right\} ^{p-1}}"/> by setting</p>
<p style="text-align: center;"><img alt="\displaystyle f\mapsto\mathbb{E}\left[f\right]+\sum_{i\in\left\{ 1,\ldots,p-1\right\} }\hat{f}\left(i\right)x_{i}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f%5Cmapsto%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%2B%5Csum_%7Bi%5Cin%5Cleft%5C%7B+1%2C%5Cldots%2Cp-1%5Cright%5C%7D+%7D%5Chat%7Bf%7D%5Cleft%28i%5Cright%29x_%7Bi%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle f\mapsto\mathbb{E}\left[f\right]+\sum_{i\in\left\{ 1,\ldots,p-1\right\} }\hat{f}\left(i\right)x_{i}. "/></p>
<p>We have</p>
<p style="text-align: center;"><img alt="\displaystyle \|f\|_{2}^{2}=\|\mathrm{En}f\|_{2}^{2}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7Cf%5C%7C_%7B2%7D%5E%7B2%7D%3D%5C%7C%5Cmathrm%7BEn%7Df%5C%7C_%7B2%7D%5E%7B2%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|f\|_{2}^{2}=\|\mathrm{En}f\|_{2}^{2}, "/></p>
<p>as the <img alt="{\chi_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\chi_{i}}"/> are orthonormal and so are the <img alt="{x_{i}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{i}.}"/> Moreover, <img alt="{\mathrm{T}_{\rho}\circ\mathrm{En}=\mathrm{En}\circ T_{\rho}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BT%7D_%7B%5Crho%7D%5Ccirc%5Cmathrm%7BEn%7D%3D%5Cmathrm%7BEn%7D%5Ccirc+T_%7B%5Crho%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{T}_{\rho}\circ\mathrm{En}=\mathrm{En}\circ T_{\rho}}"/> by the Fourier formula for <img alt="{\mathrm{T}_{\rho}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BT%7D_%7B%5Crho%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{T}_{\rho}.}"/> Since <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>-norms are always smaller than 4-norms on probability spaces, we’ve got the following corollary of Proposition 2.</p>
<p><strong>Lemma 4.</strong> For all <img alt="{\rho\le\frac{1}{10}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%5Cle%5Cfrac%7B1%7D%7B10%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho\le\frac{1}{10}}"/> and all <img alt="{f\colon\mathbb{Z}/p\mathbb{Z}\rightarrow\mathbb{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5Crightarrow%5Cmathbb%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\mathbb{Z}/p\mathbb{Z}\rightarrow\mathbb{C}}"/> we have</p>
<p style="text-align: center;"><img alt="\displaystyle \|\mathrm{T}_{\rho}f\|_{4}^{4}\le\|\mathrm{En}\left(f\right)\|_{4}^{4}+\|f-\mathbb{E}\left[f\right]\|_{4}^{4}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Crho%7Df%5C%7C_%7B4%7D%5E%7B4%7D%5Cle%5C%7C%5Cmathrm%7BEn%7D%5Cleft%28f%5Cright%29%5C%7C_%7B4%7D%5E%7B4%7D%2B%5C%7Cf-%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5C%7C_%7B4%7D%5E%7B4%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|\mathrm{T}_{\rho}f\|_{4}^{4}\le\|\mathrm{En}\left(f\right)\|_{4}^{4}+\|f-\mathbb{E}\left[f\right]\|_{4}^{4}. "/></p>
<p>We now reach the final little trick. We define a measure space <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/> whose underlying set is <img alt="{\mathbb{Z}/p\mathbb{Z}\sqcup\left\{ 0,1\right\} ^{\left\{ p-1\right\} },}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5Csqcup%5Cleft%5C%7B+0%2C1%5Cright%5C%7D+%5E%7B%5Cleft%5C%7B+p-1%5Cright%5C%7D+%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}/p\mathbb{Z}\sqcup\left\{ 0,1\right\} ^{\left\{ p-1\right\} },}"/> and where the measure is given by <img alt="{\mu\left(i\right)=\frac{1}{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%5Cleft%28i%5Cright%29%3D%5Cfrac%7B1%7D%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu\left(i\right)=\frac{1}{p}}"/> for <img alt="{i\in\mathbb{Z}/p\mathbb{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%5Cin%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i\in\mathbb{Z}/p\mathbb{Z}}"/> and <img alt="{\mu\left(x\right)=\frac{1}{2}^{p-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%5Cleft%28x%5Cright%29%3D%5Cfrac%7B1%7D%7B2%7D%5E%7Bp-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu\left(x\right)=\frac{1}{2}^{p-1}}"/> for <img alt="{x\in\left\{ -1,1\right\} ^{p-1}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%5E%7Bp-1%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in\left\{ -1,1\right\} ^{p-1}.}"/> We let <img alt="{B\colon\mathcal{F}\left(X\right)\rightarrow\mathcal{F}\left(Y\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%5Ccolon%5Cmathcal%7BF%7D%5Cleft%28X%5Cright%29%5Crightarrow%5Cmathcal%7BF%7D%5Cleft%28Y%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B\colon\mathcal{F}\left(X\right)\rightarrow\mathcal{F}\left(Y\right)}"/> be given by <img alt="{Bf=\mathrm{En}f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BBf%3D%5Cmathrm%7BEn%7Df%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Bf=\mathrm{En}f}"/> on <img alt="{\left\{ -1,1\right\} ^{p-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%5E%7Bp-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left\{ -1,1\right\} ^{p-1}}"/> and letting it be <img alt="{f-\mathbb{E}\left[f\right]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf-%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f-\mathbb{E}\left[f\right]}"/> on <img alt="{\mathbb{Z}/p\mathbb{Z}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}/p\mathbb{Z}.}"/> This way Lemma 4 takes the form <img alt="{\|\mathrm{T}_{\rho}f\|_{4}\le\|Bf\|_{4}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7C%5Cmathrm%7BT%7D_%7B%5Crho%7Df%5C%7C_%7B4%7D%5Cle%5C%7CBf%5C%7C_%7B4%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|\mathrm{T}_{\rho}f\|_{4}\le\|Bf\|_{4}.}"/></p>
<h3>4. Tensorised operators</h3>
<p>The operator <img alt="{\mathrm{T}_{\rho}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BT%7D_%7B%5Crho%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{T}_{\rho}}"/> on <img alt="{\mathbb{Z}/p\mathbb{Z}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}/p\mathbb{Z}^{n}}"/> satisfies <img alt="{\mathrm{T}_{\rho}=\mathrm{T}_{\rho}^{\otimes n},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BT%7D_%7B%5Crho%7D%3D%5Cmathrm%7BT%7D_%7B%5Crho%7D%5E%7B%5Cotimes+n%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{T}_{\rho}=\mathrm{T}_{\rho}^{\otimes n},}"/> where the latter <img alt="{\mathrm{T}_{\rho}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BT%7D_%7B%5Crho%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{T}_{\rho}}"/> refers to the noise operator on <img alt="{\mathbb{Z}/p.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%2Fp.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}/p.}"/> The characters <img alt="{\chi_{\gamma}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_%7B%5Cgamma%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\chi_{\gamma}}"/> satisfy <img alt="{\chi_{\gamma}=\bigotimes\chi_{\gamma_{i}},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_%7B%5Cgamma%7D%3D%5Cbigotimes%5Cchi_%7B%5Cgamma_%7Bi%7D%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\chi_{\gamma}=\bigotimes\chi_{\gamma_{i}},}"/> and so we have the Fourier formula</p>
<p style="text-align: center;"><img alt="\displaystyle \mathrm{T}_{\rho}f&#xA0; =\sum_{\gamma}\rho^{\#\left\{ i:\gamma_{i}\ne0\right\} }\hat{f}\left(\gamma\right)\chi_{\gamma}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cmathrm%7BT%7D_%7B%5Crho%7Df%C2%A0+%3D%5Csum_%7B%5Cgamma%7D%5Crho%5E%7B%5C%23%5Cleft%5C%7B+i%3A%5Cgamma_%7Bi%7D%5Cne0%5Cright%5C%7D+%7D%5Chat%7Bf%7D%5Cleft%28%5Cgamma%5Cright%29%5Cchi_%7B%5Cgamma%7D.+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \mathrm{T}_{\rho}f&#xA0; =\sum_{\gamma}\rho^{\#\left\{ i:\gamma_{i}\ne0\right\} }\hat{f}\left(\gamma\right)\chi_{\gamma}. "/></p>
<p>We also have</p>
<p style="text-align: center;"><img alt="\displaystyle L_{S}\left[f\right]=\bigotimes_{i\in S}\left(f\mapsto f-\mathbb{E}\left[f\right]\right)\otimes\bigotimes_{i\notin S}Id, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L_%7BS%7D%5Cleft%5Bf%5Cright%5D%3D%5Cbigotimes_%7Bi%5Cin+S%7D%5Cleft%28f%5Cmapsto+f-%5Cmathbb%7BE%7D%5Cleft%5Bf%5Cright%5D%5Cright%29%5Cotimes%5Cbigotimes_%7Bi%5Cnotin+S%7DId%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle L_{S}\left[f\right]=\bigotimes_{i\in S}\left(f\mapsto f-\mathbb{E}\left[f\right]\right)\otimes\bigotimes_{i\notin S}Id, "/></p>
<p>and so</p>
<p style="text-align: center;"><img alt="\displaystyle L_{S}\left[f\right]=\sum_{\gamma:\gamma_{i}\ne0\text{ for all }i\in S}\hat{f}\left(\gamma\right)\chi_{\gamma}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L_%7BS%7D%5Cleft%5Bf%5Cright%5D%3D%5Csum_%7B%5Cgamma%3A%5Cgamma_%7Bi%7D%5Cne0%5Ctext%7B+for+all+%7Di%5Cin+S%7D%5Chat%7Bf%7D%5Cleft%28%5Cgamma%5Cright%29%5Cchi_%7B%5Cgamma%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle L_{S}\left[f\right]=\sum_{\gamma:\gamma_{i}\ne0\text{ for all }i\in S}\hat{f}\left(\gamma\right)\chi_{\gamma}. "/></p>
<p>This will allow us to conclude that</p>
<p style="text-align: center;"><img alt="\displaystyle L_{S}\left[\mathrm{T}_{\rho}f\right]_{S\rightarrow x}=\rho^{\left|S\right|}T_{\rho}L_{S}[f]_{S\rightarrow x}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+L_%7BS%7D%5Cleft%5B%5Cmathrm%7BT%7D_%7B%5Crho%7Df%5Cright%5D_%7BS%5Crightarrow+x%7D%3D%5Crho%5E%7B%5Cleft%7CS%5Cright%7C%7DT_%7B%5Crho%7DL_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle L_{S}\left[\mathrm{T}_{\rho}f\right]_{S\rightarrow x}=\rho^{\left|S\right|}T_{\rho}L_{S}[f]_{S\rightarrow x}. "/></p>
<p>We will also encounter the operator <img alt="{\mathrm{En}^{\otimes n},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BEn%7D%5E%7B%5Cotimes+n%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{En}^{\otimes n},}"/> which by abusing notation we also call <img alt="{\mathrm{En}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BEn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{En}}"/> encodes</p>
<p style="text-align: center;"><img alt="\displaystyle f=\sum_{\gamma}\hat{f}\left(\gamma\right)\chi_{\gamma} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f%3D%5Csum_%7B%5Cgamma%7D%5Chat%7Bf%7D%5Cleft%28%5Cgamma%5Cright%29%5Cchi_%7B%5Cgamma%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle f=\sum_{\gamma}\hat{f}\left(\gamma\right)\chi_{\gamma} "/></p>
<p>as the function <img alt="{\sum_{\gamma}\widehat{f}\left(\gamma\right)\prod_{i=1}^{n}x_{pi+\gamma_{i}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7B%5Cgamma%7D%5Cwidehat%7Bf%7D%5Cleft%28%5Cgamma%5Cright%29%5Cprod_%7Bi%3D1%7D%5E%7Bn%7Dx_%7Bpi%2B%5Cgamma_%7Bi%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{\gamma}\widehat{f}\left(\gamma\right)\prod_{i=1}^{n}x_{pi+\gamma_{i}}}"/> on <img alt="{\left\{ -1,1\right\} ^{n\left(p-1\right)}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%5E%7Bn%5Cleft%28p-1%5Cright%29%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left\{ -1,1\right\} ^{n\left(p-1\right)}.}"/><br/>
Now finally we can get to the understanding of the operator <img alt="{B^{\otimes n}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%5E%7B%5Cotimes+n%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B^{\otimes n}.}"/> The space <img alt="{Y^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y^{n}}"/> is the disjoint union of <img alt="{2^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{n}}"/> spaces of the form</p>
<p style="text-align: center;"><img alt="\displaystyle \left(\mathbb{Z}/p\mathbb{Z}\right)^{S}\times\left(\left\{ -1,1\right\} ^{p-1}\right)^{\left[n\right]\setminus S}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cleft%28%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5Cright%29%5E%7BS%7D%5Ctimes%5Cleft%28%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%5E%7Bp-1%7D%5Cright%29%5E%7B%5Cleft%5Bn%5Cright%5D%5Csetminus+S%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \left(\mathbb{Z}/p\mathbb{Z}\right)^{S}\times\left(\left\{ -1,1\right\} ^{p-1}\right)^{\left[n\right]\setminus S}. "/></p>
<p>By definition of the tensor product, for <img alt="{x,y\in\left(\mathbb{Z}/p\mathbb{Z}\right)^{S}\times\left(\left\{ -1,1\right\} ^{p-1}\right)^{\left[n\right]\setminus S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%5Cin%5Cleft%28%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5Cright%29%5E%7BS%7D%5Ctimes%5Cleft%28%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%5E%7Bp-1%7D%5Cright%29%5E%7B%5Cleft%5Bn%5Cright%5D%5Csetminus+S%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y\in\left(\mathbb{Z}/p\mathbb{Z}\right)^{S}\times\left(\left\{ -1,1\right\} ^{p-1}\right)^{\left[n\right]\setminus S}}"/> is the function</p>
<p style="text-align: center;"><img alt="\displaystyle B^{n}f\left(x,y\right)=\mathrm{En}\left(L_{S}[f]_{S\rightarrow x}\right)\left(y\right). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+B%5E%7Bn%7Df%5Cleft%28x%2Cy%5Cright%29%3D%5Cmathrm%7BEn%7D%5Cleft%28L_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D%5Cright%29%5Cleft%28y%5Cright%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle B^{n}f\left(x,y\right)=\mathrm{En}\left(L_{S}[f]_{S\rightarrow x}\right)\left(y\right). "/></p>
<h3>5. Finishing the proof</h3>
<p><strong>Proof:</strong> Lemmas 3 and 4 yield:</p>
<p style="text-align: center;"><img alt="\displaystyle \|\mathrm{T}_{\rho}f\|_{4}^{4}&#xA0; \le\|B^{\otimes n}f\|_{4}^{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Crho%7Df%5C%7C_%7B4%7D%5E%7B4%7D%C2%A0+%5Cle%5C%7CB%5E%7B%5Cotimes+n%7Df%5C%7C_%7B4%7D%5E%7B4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \|\mathrm{T}_{\rho}f\|_{4}^{4}&#xA0; \le\|B^{\otimes n}f\|_{4}^{4}"/> <img alt="\displaystyle =\sum_{S\subseteq\left[n\right]}\mathbb{E}_{x\sim\mathbb{Z}/p\mathbb{Z}^{S}}\|\mathrm{En}\left(L_{S}[f]_{S\rightarrow x}\right)\|_{4}^{4}," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D%5Csum_%7BS%5Csubseteq%5Cleft%5Bn%5Cright%5D%7D%5Cmathbb%7BE%7D_%7Bx%5Csim%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5E%7BS%7D%7D%5C%7C%5Cmathrm%7BEn%7D%5Cleft%28L_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D%5Cright%29%5C%7C_%7B4%7D%5E%7B4%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle =\sum_{S\subseteq\left[n\right]}\mathbb{E}_{x\sim\mathbb{Z}/p\mathbb{Z}^{S}}\|\mathrm{En}\left(L_{S}[f]_{S\rightarrow x}\right)\|_{4}^{4},"/></p>
<p>for any <img alt="{\rho\le\frac{1}{10}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%5Cle%5Cfrac%7B1%7D%7B10%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho\le\frac{1}{10}.}"/> We now have</p>
<p style="text-align: center;"><img alt="\displaystyle \|\mathrm{T}_{\frac{\rho}{\sqrt{3}}}f\|_{4}^{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Cfrac%7B%5Crho%7D%7B%5Csqrt%7B3%7D%7D%7Df%5C%7C_%7B4%7D%5E%7B4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \|\mathrm{T}_{\frac{\rho}{\sqrt{3}}}f\|_{4}^{4}"/> <img alt="\displaystyle =\sum_{S\subseteq\left[n\right]}(\frac{1}{\sqrt{3}})^{\left|S\right|}\mathbb{E}_{x\sim\mathbb{Z}/p\mathbb{Z}^{S}}\|\mathrm{T}_{\frac{1}{\sqrt{3}}}\left(\mathrm{En}\left(L_{S}[f]_{S\rightarrow x}\right)\right)\|_{4}^{4}," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D%5Csum_%7BS%5Csubseteq%5Cleft%5Bn%5Cright%5D%7D%28%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%29%5E%7B%5Cleft%7CS%5Cright%7C%7D%5Cmathbb%7BE%7D_%7Bx%5Csim%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5E%7BS%7D%7D%5C%7C%5Cmathrm%7BT%7D_%7B%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%7D%5Cleft%28%5Cmathrm%7BEn%7D%5Cleft%28L_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D%5Cright%29%5Cright%29%5C%7C_%7B4%7D%5E%7B4%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle =\sum_{S\subseteq\left[n\right]}(\frac{1}{\sqrt{3}})^{\left|S\right|}\mathbb{E}_{x\sim\mathbb{Z}/p\mathbb{Z}^{S}}\|\mathrm{T}_{\frac{1}{\sqrt{3}}}\left(\mathrm{En}\left(L_{S}[f]_{S\rightarrow x}\right)\right)\|_{4}^{4},"/><br/>
<img alt="\displaystyle&#xA0; \le\sum_{S\subseteq\left[n\right]}\mathbb{E}_{x\sim\mathbb{Z}/p\mathbb{Z}^{S}}\|\mathrm{En}\left(L_{S}[f]_{S\rightarrow x}\right)\|_{2}^{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%C2%A0+%5Cle%5Csum_%7BS%5Csubseteq%5Cleft%5Bn%5Cright%5D%7D%5Cmathbb%7BE%7D_%7Bx%5Csim%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5E%7BS%7D%7D%5C%7C%5Cmathrm%7BEn%7D%5Cleft%28L_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D%5Cright%29%5C%7C_%7B2%7D%5E%7B4%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle&#xA0; \le\sum_{S\subseteq\left[n\right]}\mathbb{E}_{x\sim\mathbb{Z}/p\mathbb{Z}^{S}}\|\mathrm{En}\left(L_{S}[f]_{S\rightarrow x}\right)\|_{2}^{4}"/><br/>
<img alt="\displaystyle =\sum_{S\subseteq\left[n\right]}\mathbb{E}_{x\sim\mathbb{Z}/p\mathbb{Z}^{S}}\|L_{S}[f]_{S\rightarrow x}\|_{2}^{4}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%3D%5Csum_%7BS%5Csubseteq%5Cleft%5Bn%5Cright%5D%7D%5Cmathbb%7BE%7D_%7Bx%5Csim%5Cmathbb%7BZ%7D%2Fp%5Cmathbb%7BZ%7D%5E%7BS%7D%7D%5C%7CL_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D%5C%7C_%7B2%7D%5E%7B4%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle =\sum_{S\subseteq\left[n\right]}\mathbb{E}_{x\sim\mathbb{Z}/p\mathbb{Z}^{S}}\|L_{S}[f]_{S\rightarrow x}\|_{2}^{4}."/></p>
<p>The first equality follows from the formula <img alt="{L_{S}\left[\mathrm{T}_{\rho}f\right]_{S\rightarrow x}=\rho^{\left|S\right|}\mathrm{T}_{\rho}L_{S}\left[f\right]_{S\rightarrow x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7BS%7D%5Cleft%5B%5Cmathrm%7BT%7D_%7B%5Crho%7Df%5Cright%5D_%7BS%5Crightarrow+x%7D%3D%5Crho%5E%7B%5Cleft%7CS%5Cright%7C%7D%5Cmathrm%7BT%7D_%7B%5Crho%7DL_%7BS%7D%5Cleft%5Bf%5Cright%5D_%7BS%5Crightarrow+x%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{S}\left[\mathrm{T}_{\rho}f\right]_{S\rightarrow x}=\rho^{\left|S\right|}\mathrm{T}_{\rho}L_{S}\left[f\right]_{S\rightarrow x}}"/> and the fact that <img alt="{\mathrm{T_{\rho}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BT_%7B%5Crho%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{T_{\rho}}}"/> commutes with the encoding. The inequality used hypercontractivity on the discrete cube. The last equality follows from the fact that the <img alt="{\mathrm{En}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BEn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{En}}"/> operator preserves 2-norms. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p> </p>
<p> </p>
<p> </p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2020-07-24T15:21:49Z</updated>
    <published>2020-07-24T15:21:49Z</published>
    <category term="Analysis"/>
    <category term="Combinatorics"/>
    <category term="Guest blogger"/>
    <category term="Noam Lifshitz"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-07-27T18:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7779</id>
    <link href="https://windowsontheory.org/2020/07/24/ryan-odonnels-tcs-toolkit-and-other-resources/" rel="alternate" type="text/html"/>
    <title>Ryan O’Donnell’s “TCS Toolkit” and other resources</title>
    <summary>When I was in grad school a common advice for beginning grad students was to leaf through the (paper) STOC or FOCS proceedings to see papers that you are interested in. This is still a decent advice (and requires less physical strength these days 🙂 ) but papers are not always the best source for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>When I was in grad school a common advice for beginning grad students was to leaf through the (paper) STOC or FOCS proceedings to see papers that you are interested in. This is still a decent advice (and requires less physical strength these days <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/> ) but papers are not always the best source for people starting out. More often than we’d like to, the author of the 10th paper on a topic writes it to the audience of people that read (in fact probably wrote) the previous 9 papers.</p>



<p>Talks often do a better job of giving an overview of the field, and one great resource is the <a href="https://simons.berkeley.edu/videos">videos</a> from the Simons Institute. If you want to get more in-depth information about a particular topic, it’s hard to beat the extended surveys in <a href="https://www.nowpublishers.com/TCS">Foundations and Trends in TCS</a>, as well as the related areas such as <a href="https://www.nowpublishers.com/MAL">Machine Learning</a> and <a href="https://www.nowpublishers.com/CIT">Information Theory</a>. </p>



<p>But if you are not yet sure what topic you’re interested in, or perhaps not even sure if you want to go to grad school, but you just know that you are interested in theory, there is now a new great resource. As I learned from <a href="https://twitter.com/BooleanAnalysis/status/1286658578049359873">Twitter</a>, Ryan O’Donnell has just finished his <a href="https://www.diderot.one/course/28/">TCS Toolkit course</a>. All 99(!) lectures are on <a href="https://www.youtube.com/watch?v=prI35GmCon4&amp;list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX">YouTube</a>.</p>



<p>The topics are the following (these links are to the handwritten notes, for the lecture videos see <a href="https://www.youtube.com/watch?v=prI35GmCon4&amp;list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX">YouTube channel</a>):</p>



<p><a href="https://www.diderot.one/course/28/chapters/1824/">1.   Course Overview, and How to TCS</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1881/">2.   Basic Asymptotics</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1889/">3.   Factorials and Binomial Coefficients</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1923/">4.   Central Limit Theorem</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1956/">5.   Chernoff Bounds</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1973/">6.   Computational Models</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1981/">7.   Fast Multiplication with the DFT</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/1990/">8.   Analysis of Boolean Functions</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2003/">9.   Quantum Computation</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2037/">10.   Fields and Polynomials</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2038/">11.   Error-Correcting Codes</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2064/">12.   Derandomization</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2065/">13.   Spectral Graph Theory I</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2094/">14.   Spectral Graph Theory II</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2099/">15.   Spectral Graph Theory III</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2101/">15.1.   Cheeger’s Inequality (Spectral Graph Theory bonus)</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2112/">16.   Expander Graphs</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2149/">17.   Linear Programming I</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2162/">18.   Linear Programming II</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2173/">19.   The Ellipsoid Algorithm</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2190/">20.   CSPs and Approximation</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2197/">21.   LP Hierarchies and Proof Systems</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2209/">22.   Treewidth</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2227/">23.   Communication Complexity</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2250/">24.   Information Theory</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2251/">25.   Cryptography</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2290/">26.   Hardness Assumptions</a></p>



<p><a href="https://www.diderot.one/course/28/chapters/2291/">27.   The PCP Theorem</a></p>



<p>p.s. For giving a high level taste of theory to beginning undergraduates, a great resource is Aaronson’s <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing since Democritus</a> or <a href="https://www.math.ias.edu/avi/book">Wigderson’s Math and Computation</a> if they’re more math inclined. </p></div>
    </content>
    <updated>2020-07-24T14:34:02Z</updated>
    <published>2020-07-24T14:34:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-07-27T18:21:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4916</id>
    <link href="https://www.scottaaronson.com/blog/?p=4916" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4916#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4916" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Busy Beaver Frontier</title>
    <summary xml:lang="en-US">Update (July 27): I now have a substantially revised and expanded version, which incorporates (among other things) the extensive feedback that I got from this blog post. There are new philosophical remarks, some lovely new open problems, and an even-faster-growing (!) integer sequence. Check it out! A life that was all covid, cancellations, and Trump, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Update (July 27):</span></strong> I now have a <a href="https://www.scottaaronson.com/papers/bb.pdf">substantially revised and expanded version</a>, which incorporates (among other things) the extensive feedback that I got from this blog post.  There are new philosophical remarks, some lovely new open problems, and an <em>even-faster-growing</em> (!) integer sequence.  Check it out!</p>



<p/><hr/><p/>



<p>A life that was all covid, cancellations, and Trump, all desperate rearguard defense of the beleaguered ideals of the Enlightenment, would hardly be worth living.  So it was an exquisite delight, these past two weeks, to forget current events and write an <a href="https://www.scottaaronson.com/papers/bb.pdf">18-page survey article</a> about the <a href="https://en.wikipedia.org/wiki/Busy_beaver">Busy Beaver function</a>: the staggeringly quickly-growing function that probably encodes a huge portion of all interesting mathematical truth in its first hundred values, if only we could know those values or exploit them if we did.</p>



<p>Without further ado, here’s the title, abstract, and link:</p>



<blockquote class="wp-block-quote"><p><a href="https://www.scottaaronson.com/papers/bb.pdf"><strong>The Busy Beaver Frontier</strong></a><br/>by Scott Aaronson</p><p>The Busy Beaver function, with its incomprehensibly rapid growth, has captivated generations of computer scientists, mathematicians, and hobbyists.  In this survey, I offer a personal view of the BB function 58 years after its introduction, emphasizing lesser-known insights, recent progress, and especially favorite open problems.  Examples of such problems include: when does the BB function first exceed the Ackermann function?  Is the value of BB(20) independent of set theory?  Can we prove that BB(n+1)&gt;2<sup>BB(n)</sup> for large enough n?  Given BB(n), how many advice bits are needed to compute BB(n+1)?  Do all Busy Beavers halt on all inputs, not just the 0 input?  Is it decidable whether BB(n) is even or odd?</p></blockquote>



<p>The article is slated to appear soon in <em>SIGACT News</em>.  I’m grateful to Bill Gasarch for suggesting it—even with everything else going on, this was a commission I felt I couldn’t turn down!</p>



<p>Besides Bill, I’m grateful to the various Busy Beaver experts who answered my inquiries, to Marijn Heule and Andy Drucker for suggesting some of the open problems, to Marijn for creating a figure, and to Lily, my 7-year-old daughter, for raising the question about the first value of n at which the Busy Beaver function exceeds the Ackermann function.  (Yes, Lily’s covid homeschooling has included multiple lessons on very large positive integers.)</p>



<p>There are still a few days until I have to deliver the final version. So if you spot anything wrong or in need of improvement, don’t hesitate to leave a comment or send an email.  Thanks in advance!</p>



<p>Of course Busy Beaver has been an obsession that I’ve returned to many times in my life: for example, in that <a href="https://www.scottaaronson.com/writings/bignumbers.html">Who Can Name the Bigger Number?</a> essay that I wrote way back when I was 18, in <em><a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing Since Democritus</a></em>, in my <a href="https://www.scottaaronson.com/blog/?p=3445">public lecture at Festivaletteratura</a>, and in my <a href="https://www.scottaaronson.com/blog/?p=2725">2016 paper with Adam Yedidia</a> that showed that the values of all Busy Beaver numbers beyond the 7910<sup>th</sup> are independent of the axioms of set theory (Stefan O’Rear has since shown that independence starts at the 748<sup>th</sup> value or sooner).  This survey, however, represents the first time I’ve tried to take stock of BusyBeaverology <em>as a research topic</em>—collecting in one place all the lesser-known theorems and empirical observations and open problems that I found the most striking, in the hope of inspiring not just contemplation or wonderment but actual progress.</p>



<p>Within the last few months, the world of <em>deep mathematics that you can actually explain to a child</em> lost two of its greatest giants: <a href="https://en.wikipedia.org/wiki/John_Horton_Conway">John Conway</a> (who died of covid, and who I <a href="https://www.scottaaronson.com/blog/?p=4732">eulogized here</a>) and <a href="https://en.wikipedia.org/wiki/Ronald_Graham">Ron Graham</a>.  One thing I found poignant, and that I didn’t know before I started writing, is that Conway and Graham <em>both</em> play significant roles in the story of the Busy Beaver function.  Conway, because most of the best known candidates for Busy Beaver Turing machines turn out, when you analyze them, to be testing variants of the notorious <a href="https://en.wikipedia.org/wiki/Collatz_conjecture">Collatz Conjecture</a>—and Conway is the one who proved, in 1972, that the set of “Collatz-like questions” is Turing-undecidable.  And Graham because of <a href="https://en.wikipedia.org/wiki/Graham%27s_number">Graham’s number</a> from <a href="https://en.wikipedia.org/wiki/Ramsey_theory">Ramsey theory</a>—a candidate for the biggest number that’s ever played a role in mathematical research—and because of the <a href="https://googology.wikia.org/wiki/User_blog:Wythagoras/The_nineteenth_Busy_Beaver_number_is_greater_than_Graham%27s_Number!">discovery</a>, four years ago, that the 18<sup>th</sup> Busy Beaver number exceeds Graham’s number.</p>



<p>(“Just how big is Graham’s number?  So big that the <em>17<sup>th</sup> Busy Beaver number</em> is not yet known to exceed it!”)</p>



<p>Anyway, I tried to make the survey pretty accessible, while still providing enough technical content to sink one’s two overgrown front teeth into (don’t worry, there are no such puns in the piece itself).  I hope you like reading it at least 1/BB(10) as much as I liked writing it.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (July 24):</span></strong> Longtime commenter Joshua Zelinsky gently reminded me that one of the main questions discussed in the survey—namely, whether we can prove BB(n+1)&gt;2<sup>BB(n)</sup> for all large enough n—was <a href="https://www.scottaaronson.com/blog/?p=1385#comment-73298">first brought to my attention</a> by him, Joshua, in a 2013 Ask-Me-Anything session on this blog!  I apologize to Joshua for the major oversight, which has now been corrected.  On the positive side, we just got a powerful demonstration <em>both</em> of the intellectual benefits of blogging, and of the benefits of sharing paper drafts on one’s blog before sending them to the editor!</p></div>
    </content>
    <updated>2020-07-23T06:42:48Z</updated>
    <published>2020-07-23T06:42:48Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-07-27T09:36:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/07/23/tenure-track-or-tenured-faculty-positions-at-center-on-frontiers-of-computing-studies-peking-university-apply-by-september-30-2020/</id>
    <link href="https://cstheory-jobs.org/2020/07/23/tenure-track-or-tenured-faculty-positions-at-center-on-frontiers-of-computing-studies-peking-university-apply-by-september-30-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track or Tenured Faculty Positions at Center on Frontiers of Computing Studies, Peking University (apply by September 30, 2020)</title>
    <summary>The Center on Frontiers of Computing Studies (CFCS), Peking University (PKU), China, is a university new initiative co-founded by Professors John Hopcroft and Wen Gao. We are seeking applicants from all areas of Computer Science, spanning theoretical foundations, systems, software, and applications, with special interests in artificial intelligence and machine learning. Website: https://cfcs.pku.edu.cn/english/people/joinus/236979.htm Email: cfcs_recruiting@pku.edu.cn</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Center on Frontiers of Computing Studies (CFCS), Peking University (PKU), China, is a university new initiative co-founded by Professors John Hopcroft and Wen Gao.</p>
<p>We are seeking applicants from all areas of Computer Science, spanning theoretical foundations, systems, software, and applications, with special interests in artificial intelligence and machine learning.</p>
<p>Website: <a href="https://cfcs.pku.edu.cn/english/people/joinus/236979.htm">https://cfcs.pku.edu.cn/english/people/joinus/236979.htm</a><br/>
Email: cfcs_recruiting@pku.edu.cn</p></div>
    </content>
    <updated>2020-07-23T02:00:39Z</updated>
    <published>2020-07-23T02:00:39Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-07-27T18:20:50Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/07/22/three-cccg-videos</id>
    <link href="https://11011110.github.io/blog/2020/07/22/three-cccg-videos.html" rel="alternate" type="text/html"/>
    <title>Three CCCG videos</title>
    <summary>Three new ten-minute research talk videos by me are now up on YouTube as part of the collection of videos to be presented in the 2020 Canadian Conference on Computational Geometry. The pdf conference program has links to all the videos. The conference includes an opportunity to interact with the speakers online, on the August 5–7 dates of the actual conference, with free registration at the CCCG web site.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Three new ten-minute research talk videos by me are now up on YouTube as part of the collection of videos to be presented in the 2020 Canadian Conference on Computational Geometry. The <a href="http://vga.usask.ca/cccg2020/program.pdf">pdf conference program</a> has links to all the videos. The conference includes an opportunity to interact with the speakers online, on the August 5–7 dates of the actual conference, with free registration at <a href="http://vga.usask.ca/cccg2020/">the CCCG web site</a>.</p>

<p>My three videos are on <a href="https://11011110.github.io/blog/2020/07/16/comparing-multi-sport.html">dynamic products of ranks, discussed in my previous post</a>, polyhedra that are difficult to unfold, and mathematics inspired by <a href="https://en.wikipedia.org/wiki/Lusona">the sona drawings of southwest Africa</a>. For your convenience here they are more directly:</p>

<div style="text-align: center;">

<p> </p>

<p> </p>

<p> </p>
</div>

<p>(Apologies for the weird aspect ratio. I should probably aim for a more standard 16x9 format in future.)</p>

<p>I also have a fourth CCCG paper, on unfolding orthogonal polyhedra, with a video produced by Joe O’Rourke. I posted about its preprint version, “<a href="https://11011110.github.io/blog/2019/07/29/zipless-polycube.html">Some polycubes have no edge-unzipping</a>”, a year ago. Here’s Joe’s talk:</p>

<div style="text-align: center;">
<p> </p>

</div></div>
    </content>
    <updated>2020-07-22T14:48:00Z</updated>
    <published>2020-07-22T14:48:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-07-23T07:39:15Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/average-case-dp/</id>
    <link href="https://differentialprivacy.org/average-case-dp/" rel="alternate" type="text/html"/>
    <title>The Pitfalls of Average-Case Differential Privacy</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Differential privacy protects against extremely strong adversaries—even ones who know the entire dataset except for one bit of information about one individual.  Since its inception, people have considered ways to relax the definition to assume a more realistic adversary.  A natural way to do so is to incorporate some distributional assumptions. That is, rather than considering a worst-case dataset, assume the dataset is drawn from some distribution and provide some form of “average-case” or “Bayesian” privacy guarantee with respect to this distribution. This is especially tempting as it is common for statistical analysis to work under distributional assumptions.</p>

<p>In this post and in a planned follow-up post, we will discuss some pitfalls of average-case or Bayesian versions of differential privacy.  To avoid keeping you in suspense:</p>

<ul>
  <li>The average-case assumptions in relaxations of differential privacy are qualitatively different to and much more brittle than the typical assumptions made about how the data is generated.</li>
  <li>Average-case relaxations do not satisfy the strong composition properties that have made differential privacy so successful.</li>
  <li>It is safer to use distributional assumptions in the accuracy analysis instead of the privacy analysis. That is, we can provide average-case utility and worst-case privacy. Recent work has shown that this model can capture most of the advantages of distributional assumptions.</li>
</ul>

<p>We will show some illustrative examples for each of these points, but we will be purposefully vague as to exactly which alternative definition we are considering, as these issues arise in a wide variety of definitions.  Our hope is not to shut down discussion of these relaxations, or to single out specific definitions as flawed.  There are specific concrete applications where average-case differential privacy might be useful, and our goal is to highlight some issues that must be carefully considered in each application.</p>

<h3 id="assumptions-about-nature-vs-assumptions-about-the-adversary">Assumptions about nature vs. assumptions about the adversary?</h3>

<p>In any reasonable definition of privacy, we have to think about whom we are hiding sensitive information from.  This person—“the adversary”—could be a stranger, a close friend, a relative, a corporation we do business with, or the government, and who they are affects what information they have access to and what defenses are appropriate.   How the adversary can access the private system defines the <a href="https://differentialprivacy.org/\trustmodels">trust model</a>. Distributional assumptions correspond to the adversary’s side information.  Our key point is:</p>

<blockquote>
  <p>Assumptions incorporated into the definition of privacy are assumptions about the adversary and these are qualitatively different from assumptions about “nature,” which is the process that generates the data.</p>
</blockquote>

<p>For example, suppose an employer learns that two of its employees have expensive medical conditions. On its own, this information does not identify those employees and this privacy intuition could be formalized via distributional assumptions. But these distributional assumptions will break if the employer later receives some side information. For example, the other healthy employees may voluntarily disclose their medical status or the employer may find out that, before you were hired, that number was only one. (Incidentally, this is an example of a failure of composition, which we will discuss in another post.)</p>

<p>This example illustrates how assumptions about the adversary that might seem reasonable in a vacuum can be invalidated by context. Plus, assumptions about the adversary can be invalidated by <em>future</em> side information, and you can’t retract a privacy leak once it happens the way you can a medical study. So assumptions about the adversary are much less future-proof than assumptions about nature.</p>

<h3 id="all-models-are-wrong-but-some-are-useful">All models are wrong, but some are useful</h3>

<p>One justification for incorporating distributional assumptions into the privacy definition is that the person using the data is often making these assumptions anyway—for example, that the data is i.i.d. Gaussian, or that two variables have some underlying linear relationship to be discovered.  So, if the assumption were false, wouldn’t we already be in trouble?  Not really.</p>

<blockquote>
  <p>It’s important to remember the old saw “all models are wrong, but some are useful.”  Some models have proven themselves useful for statistical purposes, but that does not mean they are useful as a basis for privacy.</p>
</blockquote>

<p>For example, our methods may be robust to the relatively friendly ways that nature deviates from the model, but we can’t trust adversaries to be as friendly.</p>

<p>For a toy example, suppose we model our data as coming from a normal distribution \( N(\mu,\sigma^2) \), but actually the data is collected at two different testing centers, one of which rounds its measurements to the nearest integer and the other of which provides two decimal places of precision.  This rounding makes the model wrong, but won’t significantly affect our estimate of the mean.  However, just looking at the estimate of the mean might reveal that someone in the dataset went to the second testing center, potentially compromising that person’s privacy.</p>

<p>A more natural setting where this issue arises is in dealing with <em>outliers</em> or other extreme examples, which we will discuss in the next section.</p>

<h3 id="privacy-for-outliers">Privacy for outliers</h3>

<p>The usual worst-case definition of differential privacy provides privacy for everyone, including outliers.  Although there are lots of ways to achieve differential privacy, in order to compare definitions, it will help to restrict attention to the basic approach based on calibrating noise to sensitivity:</p>

<p>Suppose we have a private dataset \( x \in \mathcal{X}^n \) containing the data of \( n \) individuals, and some real-valued query \( q : \mathcal{X}^n \to \mathbb{R} \).  The standard way to release an estimate of \( q(x) \) is to compute
\[
M(x) = q(x) + Z \cdot \sup_{\textrm{neighboring}~x’,x”} |q(x’) - q(x”)|
\]
where 
\(
\sup_{\textrm{neighboring}~x’,x”} |q(x’) - q(x”)|
\)
is called the “worst-case sensitivity” of \( q \) and \( Z \) is some noise, commonly drawn from a Laplace or Gaussian distribution.</p>

<p>Unfortunately, the worst-case sensitivity may be large or even infinite for basic statistics of interest, such as the mean \( q(x) = \frac{1}{n} \sum_{i} x_i \) of unbounded real values.  There are a variety of differentially private algorithms for addressing this problem,<sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1">1</a></sup> but that is not what this post is about.  It’s tempting to, instead, try to scale the noise to some notion of “average-case sensitivity,” with the goal of satisfying some average-case version of differential privacy.  For example, suppose the data is drawn from some normal distribution \( N(\mu,\sigma^2) \) and the neighboring datasets \( x', x'' \) are each \( n \) i.i.d. samples from this distribution, but differing on exactly one random sample.  Then the worst-case sensitivity of the mean is infinite:
\[
\sup_{\textrm{neighboring}~x’,x”} |q(x’) - q(x”)| = \infty,
\]
but the average-sensitivity is proportional to \(1/n\):
\[
\mathbb{E}_{\textrm{neighboring}~x’, x”}(|q(x’) - q(x”)|) \approx \frac{\sigma}{n}.
\]
Thus, under an average-case privacy guarantee, we can estimate the mean with very little noise.</p>

<p>But what happens to privacy if this assumption fails, perhaps because of outliers?  Imagine computing the average wealth of a subset of one hundred Amazon employees who test positive for COVID-19, and discovering that it’s over one billion dollars.  Maybe Jeff Bezos isn’t feeling well?<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2">2</a></sup></p>

<p>Yes, this example is a little contrived, since you probably shouldn’t have computed the empirical mean of such skewed data anyway. But, if this fact leaks out, you can’t just go back in time and truncate the data or compute the median instead. Privacy tends to be high-stakes both because of the potential consequences of a breach and the inability to retract or correct a privacy violation after it is discovered.</p>

<p>In the next section we’ll see a slightly more complex example where average-case privacy / average-case sensitivity fails to protect privacy even when the distributional assumptions hold.</p>

<h3 id="example-pairwise-correlations-and-linear-regression">Example: pairwise correlations and linear regression</h3>

<p>Suppose our dataset \( X \) is a matrix \( \{-1,+1\}^{n \times (d+1)} \) where each row \( X_i \) corresponds to one person’s data and each column corresponds to one feature.  For simplicity, let’s suppose our distributional assumption is that the dataset is completely uniform—each bit is sampled independently and uniformly from \( \{-1,+1\} \).  We’ll think of the first \( d \) columns as “features” and the last column as a “secret label.”</p>

<p>First, consider the set of pairwise correlations between each feature and the secret label:
\[
q_j(X) = \sum_{i = 1}^{n} X_{i,j} X_{i,d+1}
\]
for \( j = 1,\dots,d \).  Note that \( q_j(X) \) has mean 0 and variance \( n \) under our distributional model of the data.</p>

<p>Now, suppose we have a weight vector \( w \in \mathbb{R}^{d} \) and want to estimate the weighted average of correlations
\[
q(X) = \sum_{j = 1}^{d} w_j q_j(X) = \sum_{i=1}^{n} \sum_{j=1}^{d} w_{j} X_{i,j} X_{i,d+1}
\]
This statistic may look a little odd, but it’s pretty close to computing the average squared error of the linear predictor \( \hat X_{i,d+1} = \sum_{j=1}^{d} w_j X_{i,j} \) given by the weight vector \( w \), which is a natural thing to estimate.</p>

<p>The worst-case sensitivity of \(q\) is proportional to \( \|w\|_1 \).<br/>
However, it’s not too hard to show that, under our distributional model, the average-case sensitivity is much lower; it is proportional to \( \| w \|_2 \).  Thus, using average-case privacy may allow us to add significantly less noise.</p>

<p>What could go wrong here?  Well, we’ve implicitly assumed that the weights \( w \) are independent of the data \( X \).  That is, the person specifying the weights has no knowledge of the data itself, only its distribution.  Suppose the weights are specified by an adversary who has learned the \( d \) features of the first individual (although there is nothing special about considering the first individual), who sets the weights to \(w = (X_{1,1},\dots,X_{1,d}) \).  Another calculation shows that, in this case, <em>even when our model of the data is exactly correct</em>, the query \( q(X) \) has mean \( d \cdot X_{1,d+1} \) and standard deviation approximately \( \sqrt{nd} \).  Thus, if \( d \gg n \) we can confidently determine the secret label \( X_{1,d+1} \)  of the first individual from the value \( q(X) \).  Moreover, adding noise of standard deviation \( \ll d \) will not significantly affect the adversary’s ability to learn the secret label.  But, earlier, we argued that average-case sensitivity is proportional to \( \|w \|_2 = \sqrt{d} \), so this form of average-case privacy fails to protect a user’s data in this scenario!  Note that adding noise proportional to \( \|w\|_1 = d \) would satisfy (worst-case) differential privacy and would thwart this adversary.</p>

<blockquote>
  <p>What went wrong is that the data satisfied our assumptions, but the adversary’s beliefs about the data did not!</p>
</blockquote>

<p>The set of reasonable distributions to consider for the adversary’s beliefs may look very different from the set of reasonable distributions to consider for your analysis of the data.  You may think that it’s not reasonable for the attacker to choose this weight vector \( w \) containing a lot of prior information about an individual, but assuming that the attacker cannot obtain or specify such a vector is very different from assuming that the data is uniform, and requires its own justification.</p>

<p>Before wrapping up, let’s just make a couple more observations about this example:</p>

<ul>
  <li>This attack is pretty robust. The assumption that the data is uniform with independent features can be relaxed significantly.  It’s also not necessary for the adversary to exactly know all the features of the first user, all we need is for the weights to have correlation \( \gg \sqrt{nd} \) with the features.  For example, if the dataset is genomic data, having the data of a relative might suffice.</li>
  <li>This problem isn’t specific to high-dimensional data with \( d \gg n \).  If we allow more general types of “queries”, then a similar attack is possible when there are only \( d \approx \log n \) features.</li>
  <li>To make this example as crisp as possible, we allowed an adversarial data analyst to specify the weight vector \( w \).  You might think examples like this can’t arise if the algorithm designer specifies all of the queries internally, but ensuring that requires great care (as we’ll see in our upcoming post about composition).</li>
</ul>

<h3 id="conclusion">Conclusion</h3>
<p>As we have discussed, the main issue that arises in average-case or Bayesian versions of differential privacy is that we must make strong assumptions about the adversary. A simple distributional assumption about the data, which may be entirely reasonable for statistical analysis, entails assuming a naïve adversary with essentially no side information, which is not reasonable from a privacy perspective.</p>

<p>In a future post, we will discuss <em>composition</em>, which is a key robustness property and really the secret to differential privacy’s success.  As we’ll see, average-case versions of differential privacy do not enjoy strong composition properties the way worst-case differential privacy does, which makes them much harder to deploy.</p>

<p>Incorporating assumptions about the adversary into the privacy guarantee requires great care; and it is safest to make fewer assumptions, which quickly pushes us towards the worst-case definition of differential privacy. Nevertheless, assumptions about the adversary are often made implicitly and it is worth studying how to make these explicit.</p>

<p>So, is there are role for distributional assumptions in differential privacy? Yes! Although we’ve discussed the pitfalls of making the <em>privacy guarantee</em> contingent on distributional assumptions, none of these pitfalls apply to making the <em>utility guarantee</em> contingent on distributional assumptions, as is normally done in statistical analysis.  In recent years, this combination—worst-case privacy, average-case utility—has been fruitful, and seems to allow many of the benefits that average-case privacy definitions seek to capture.  For example, recent work has shown that worst-case differential privacy permits accurate mean and covariance estimation of unbounded data under natural modeling assumptions <a href="https://arxiv.org/abs/1711.03908" title="Vishesh Karwa, Salil Vadhan. Finite Sample Differentially Private Confidence Intervals. ITCS 2018."><strong>[KV18]</strong></a>, <a href="https://arxiv.org/abs/1805.00216" title="Gautam Kamath, Jerry Li, Vikrant Singhal, Jonathan Ullman. Privately Learning High-Dimensional Distributions. COLT 2019."><strong>[KLSU19]</strong></a>, <a href="https://arxiv.org/abs/1906.02830" title="Mark Bun, Thomas Steinke. Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation. NeurIPS 2019."><strong>[BS19]</strong></a>, <a href="https://arxiv.org/abs/2001.02285" title="Wenxin Du, Canyon Foot, Monica Moniot, Andrew Bray, Adam Groce. Differentially Private Confidence Intervals. 2020."><strong>[DFMBG20]</strong></a>, <a href="https://arxiv.org/abs/2002.09464" title="Gautam Kamath, Vikrant Singhal, Jonathan Ullman.  Private Mean Estimation of Heavy-Tailed Distributions. COLT 2020."><strong>[KSU20]</strong></a>, <a href="https://arxiv.org/abs/2006.06618" title="Sourav Biswas, Yihe Dong, Gautam Kamath, Jonathan Ullman. CoinPress: Practical Private Mean and Covariance Estimation. 2020."><strong>[BDKU20]</strong></a>, but this remains an active area of research.</p>

<hr/>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For example, there are approaches based on various paradigms like Smooth Sensitivity <a href="http://www.cse.psu.edu/~ads22/pubs/NRS07/NRS07-full-draft-v1.pdf" title="Kobbi Nissim, Sofya Raskhodnikova, Adam Smith. Smooth Sensitivity and Sampling in Private Data Analysis. STOC 2007."><strong>[NRS07]</strong></a> <a href="https://arxiv.org/abs/1906.02830" title="Mark Bun, Thomas Steinke. Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation. NeurIPS 2019."><strong>[BS19]</strong></a>, Propose-Test-Release <a href="http://www.stat.cmu.edu/~jinglei/dl09.pdf" title="Cynthia Dwork, Jing Lei. Differential Privacy and Robust Statistics. STOC 2009."><strong>[DL09]</strong></a>, or Truncation/Winsorization <a href="http://www.cse.psu.edu/~ads22/pubs/2011/stoc194-smith.pdf" title="Adam Smith. Privacy-preserving Statistical Estimation with Optimal Convergence Rates. STOC 2011."><strong>[S11]</strong></a> <a href="https://arxiv.org/abs/1711.03908" title="Vishesh Karwa, Salil Vadhan. Finite Sample Differentially Private Confidence Intervals. ITCS 2018."><strong>[KV18]</strong></a> <a href="https://arxiv.org/abs/1805.00216" title="Gautam Kamath, Jerry Li, Vikrant Singhal, Jonathan Ullman. Privately Learning High-Dimensional Distributions. COLT 2019."><strong>[KLSU19]</strong></a> <a href="https://arxiv.org/abs/2002.09464" title="Gautam Kamath, Vikrant Singhal, Jonathan Ullman.  Private Mean Estimation of Heavy-Tailed Distributions. COLT 2020."><strong>[KSU20]</strong></a> to name a few. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>If you are confident that Jeff Bezos or other extremely high-wealth individuals are not in the sample, then you could <em>truncate</em> each sample and compute the mean of the truncated samples.  This would give worst-case privacy, and, if you are correct in your assumption, would not affect the mean. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">↩</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2020-07-22T14:00:00Z</updated>
    <published>2020-07-22T14:00:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-07-26T23:37:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/110</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/110" rel="alternate" type="text/html"/>
    <title>TR20-110 |  Capacity Lower Bounds via Productization | 

	Leonid Gurvits, 

	Jonathan Leake</title>
    <summary>The purpose of this note is to state and prove a lower bound on the capacity of a real stable polynomial $p(x)$ which is based only on its value and gradient at $x=1$. This result implies a sharp improvement to a similar inequality proved by Linial-Samorodnitsky-Wigderson in 2000. Such inequalities have played an important role in the recent work on operator scaling and its generalizations and applications.</summary>
    <updated>2020-07-22T04:32:00Z</updated>
    <published>2020-07-22T04:32:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-27T18:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4912</id>
    <link href="https://www.scottaaronson.com/blog/?p=4912" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4912#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4912" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Is this blog obsolete?</title>
    <summary xml:lang="en-US">Longtime friend-of-the-blog Michael Nielsen, formerly a quantum computing theorist and now an independent thinker about science and AI and learning, posted the following exchange with the astounding GPT-3 text generator / chatbot: Q: Is it possible to use quantum computers to solve NP-complete problems in polynomial time?A: It is not possible to use quantum computers […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Longtime friend-of-the-blog <a href="http://michaelnielsen.org/">Michael Nielsen</a>, formerly a quantum computing theorist and now an independent thinker about science and AI and learning, posted the following <a href="https://twitter.com/michael_nielsen/status/1284988349418098688">exchange</a> with the astounding <a href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/">GPT-3</a> text generator / chatbot:</p>



<blockquote class="wp-block-quote"><p>Q: Is it possible to use quantum computers to solve NP-complete problems in polynomial time?<br/>A: It is not possible to use quantum computers to solve NP-complete problems in polynomial time.</p><p>Q: Why not?<br/>A: Because NP-complete problems are not in BQP.</p><p>Q: You’ve just restated your earlier assertion!  What I’d like is to understand the underlying reason why not!<br/>A: I’m sorry, but I don’t have time to explain the underlying reason why not.</p><p>Q: No offence, but this seems really bogus.  I don’t believe you understand the reason why!<br/>A: I’m sorry, but I don’t have time to explain the underlying reason why not.</p></blockquote>



<p>(And then there are a few more rounds of the AI not having time to “explain the underlying reason why not.”)</p>



<p>This is, like, 80% of the way to replacing <em>Shtetl-Optimized</em>!</p>



<p>For much more discussion of GPT-3 and its implications, and samples of its output, see for example the <a href="https://www.reddit.com/r/slatestarcodex/">SSC subreddit</a>.  At the moment, as far as I can tell, the closest a person off the street can easily come to experimenting with GPT-3 themselves is using a website called <a href="https://play.aidungeon.io/">AI Dungeon</a>.</p>



<p>And yes, as many have already remarked, this is clearly the <a href="https://en.wikipedia.org/wiki/Altair_8800">MITS Altair</a> of text-generating AI, an amusing toy that’s also the start of something that will change the world.</p></div>
    </content>
    <updated>2020-07-21T00:16:18Z</updated>
    <published>2020-07-21T00:16:18Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-07-27T09:36:09Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/stoc2020/</id>
    <link href="https://differentialprivacy.org/stoc2020/" rel="alternate" type="text/html"/>
    <title>Conference Digest - STOC 2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="http://acm-stoc.org/stoc2020/">STOC 2020</a> was recently held online, as one of the first major theory conferences during the COVID-19 era.
It featured four papers on differential privacy, which we list and link below.
Each one is accompanied by a video from the conference, as well as a longer video if available.
Please let us know if we missed any papers on differential privacy, either in the comments below or by email.</p>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1911.08339">The Power of Factorization Mechanisms in Local and Central Differential Privacy</a> (<a href="https://www.youtube.com/watch?v=hSenRTxhZhM">video</a>)<br/>
<a href="https://dblp.uni-trier.de/pers/hd/e/Edmonds:Alexander">Alexander Edmonds</a>, <a href="http://www.cs.toronto.edu/~anikolov/">Aleksandar Nikolov</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.04763">Private Stochastic Convex Optimization: Optimal Rates in Linear Time</a> (<a href="https://www.youtube.com/watch?v=Tlc-z-MFAmM">video</a>)<br/>
<a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="https://tomerkoren.github.io/">Tomer Koren</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.04014">Interaction is necessary for distributed learning with privacy or communication constraints</a> (<a href="https://www.youtube.com/watch?v=AWgzaFOU_HM">video</a>)<br/>
<a href="https://yuvaldagan.wordpress.com/">Yuval Dagan</a>, <a href="http://vtaly.net/">Vitaly Feldman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1906.05271">Does Learning Require Memorization? A Short Tale about a Long Tail</a> (<a href="https://www.youtube.com/watch?v=sV59uoWJRnk">video</a>, <a href="https://www.youtube.com/watch?v=Fp7cgHRl8Yc">longer video</a>)<br/>
<a href="http://vtaly.net/">Vitaly Feldman</a></p>
  </li>
</ul></div>
    </summary>
    <updated>2020-07-20T14:00:00Z</updated>
    <published>2020-07-20T14:00:00Z</published>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-07-26T23:37:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/transfer-learning/</id>
    <link href="https://gradientscience.org/transfer-learning/" rel="alternate" type="text/html"/>
    <title>Transfer Learning with Adversarially Robust Models</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/2007.08489" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/Microsoft/robust-models-transfer" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Models and Code
</a>
<br/></p>

<p><i>In our <a href="https://arxiv.org/abs/2007.08489">latest paper</a>, in collaboration with <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>, we explore adversarial
robustness as an avenue for training computer vision models with more transferrable
features. We find that robust models outperform their standard counterparts on
a variety of transfer learning tasks.</i></p>

<h2 id="what-is-transfer-learning">What is transfer learning?</h2>

<p>Transfer learning is a paradigm where one leverages information
from a “source” task to better solve another “target” task. Particularly when there is little training data or compute available for solving the target
task, transfer learning provides a simple and efficient way to obtain performant
machine learning models.</p>

<p>Transfer learning has already proven its utility in many ML contexts. In natural language processing, for example, one can leverage language models pre-trained on large
text corpora to beat state-of-the-art performance on
tasks like query answering, entity recognition or part-of-speech classification.</p>

<p>In our work we focus on computer vision; in this context, a standard—and
remarkably successful—transfer learning pipeline is “ImageNet pre-training.”
This pipeline starts with a deep neural network trained on the <a href="http://image-net.org">ImageNet-1K</a>
dataset, and then refines this pre-trained model for a target task. The target task can range
from classification of smaller datasets (e.g., <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR</a>) to more complex
tasks like object detection (e.g., <a href="http://host.robots.ox.ac.uk/pascal/VOC/">VOC</a>).</p>

<p>Although there are many ways in which one can refine a pre-trained model, we
will restrict our attention to the two most popular methods:</p>

<ul>
  <li><strong>Fixed-feature</strong>: In fixed-feature transfer learning, we replace the final
(linear) layer of the neural network with a new layer that has the correct
number of outputs for the target task. Then, keeping the rest of the layers
<em>fixed</em>, we train the newly replaced layer on the target task.</li>
  <li><strong>Full-network</strong>: In full-network transfer learning, we also replace the last
layer but do not freeze any layers afterwards. Instead, we use the pre-trained
network
as a sort of “initialization,” and continue training <em>all</em> the layers on the
target task.</li>
</ul>

<p>When at least a moderate amount of data is available, full-network transfer
learning typically outperforms the fixed-feature strategy.</p>

<h2 id="how-can-we-improve-transfer-learning">How can we improve transfer learning?</h2>

<p>Although we don’t have a comprehensive understanding of what makes transfer
learning algorithms tick, there has been a long line of work focused on identifying 
factors that improve (or worsen) performance (examples include
<a href="https://arxiv.org/abs/1406.5774">[1]</a>,
<a href="https://arxiv.org/abs/1608.08614">[2]</a>,
<a href="https://arxiv.org/abs/1805.08974">[3]</a>,
<a href="https://arxiv.org/abs/1804.08328">[4]</a>,
<a href="https://arxiv.org/abs/1411.1792">[5]</a>).</p>

<p>By design, the pre-trained ImageNet model itself plays a major role here:
indeed, a recent study by <a href="https://arxiv.org/abs/1805.08974">Kornblith, Shlens, and Le</a> finds that
pre-trained models which achieve a higher ImageNet accuracy also perform better when
transferred to downstream classification tasks, with a tight linear
correspondence between ImageNet accuracy and the accuracy on the target task:</p>

<p><img alt="A scatter plot of ImageNet accuracy versus downstream     transfer accuracy showing the linear relation." class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/ksl.png"/></p>
<div class="footnote">
Reproduced from <a href="https://arxiv.org/abs/1805.08974">[KSL19]</a>. 
Each dot is a pre-trained model whose $x$ coordinate is given by its 
ImageNet accuracy and $y$ coordinate is given by its downstream 
accuracy on the target task (after the corresponding refinement on that task).
</div>

<p>But is improving ImageNet accuracy of the pre-trained model the <em>only</em> way to improve transfer learning performance?</p>

<p>After all, we want to obtain models that have learned broadly applicable features from the
source dataset. ImageNet accuracy likely correlates with the quality of
features that a model has learned, but may not fully describe the downstream
utility of these features.
Ultimately, the nature of learned features stems from the <em>priors</em> placed on
them during training. For example, there have been studies of the (sometimes
implicit) priors imposed by architectural components (e.g., <a href="https://dmitryulyanov.github.io/deep_image_prior">convolutional layers</a>),
<a href="https://www.tandfonline.com/doi/abs/10.1198/10618600152418584">data</a>
<a href="https://arxiv.org/abs/1911.09071">augmentation</a>, 
<a href="https://arxiv.org/abs/1811.00401">loss functions</a> and even
<a href="https://stats385.github.io/assets/lectures/Stanford_Donoho_class_Nov_19.pdf">gradient descent</a> on neural network training.</p>

<p>In <a href="https://arxiv.org/abs/2007.08489">our paper</a>, we study another prior: <em>adversarial robustness</em>.
Adversarial robustness—a rather frequent subject on this blog—refers to
model’s invariance to small (often imperceptible) perturbations of natural
inputs, called <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.</p>

<p>Standard neural networks (i.e., trained with the goal of maximizing
accuracy) are extremely vulnerable to such adversarial examples. For example,
with just a tiny perturbation to the pig image below, a pre-trained ImageNet
classifier will predict it as an “airliner” with 99% confidence:</p>

<p><img alt="An adversarial example: a pig on the left which is imperceptibly perturbed to be classified as an airliner on the right." src="https://gradientscience.org/images/piggie.png"/></p>
<div class="footnote">
A "pigs-can-fly" adversarial example: The "pig" image on the left is correctly classified by a standard ML model, but its imperceptibly perturbed counterpart on the right is classified as an "airliner" with 99% confidence.
</div>

<p>Adversarial robustness is thus typically induced at training time by replacing
the standard loss minimization objective with a <em>robust optimization</em> objective
(see our <a href="https://gradientscience.org/robust_opt_pt1">post on robust optimization</a> for more background):</p>



<p>The above objective trains models to be robust to image perturbations that are
small in (pixel-wise) $\ell_2$ (Euclidean) normIn reality, an $\ell_2$ ball doesn't perfectly capture the
set of imperceptible perturbations we want models to be robust to—but robustness with respect to this fairly rudimentary notion of perturbations turns out to be already non-trivial and very helpful.. 
The parameter $\varepsilon$ is a hyperparameter
governing the intended degree of invariance of the resulting models to the
corresponding perturbations. Setting 
$\varepsilon = 0$ corresponds to standard training, and increasing $\varepsilon$
asks the model to be robust to increasingly large perturbations.
In short, the objective asks the model to minimize risk on not only the 
training datapoints but also the entire radius-$\varepsilon$
neighbourhood around them.</p>

<p><em>[A quick plug: Our <a href="https://github.com/MadryLab/robustness"><code class="language-plaintext highlighter-rouge">robustness</code> Python library</a>, used for the code release of this paper, enables one to easily train and manipulate both standard and adversarially robust models.]</em></p>

<p>Although adversarial robustness has been initially studied solely through the lens of machine learning security, a line
of recent work (including some that’s been <a href="https://gradientscience.org/adv">previously</a> 
<a href="https://gradientscience.org/robust_apps">covered</a> on this blog) has begun to study
adversarially robust models in their own right, framing adversarial robustness
as a prior that forces models to learn features that are locally stable.
These works have found that on the one hand, adversarially robust models tend
to attain lower accuracy than their standardly-trained
counterparts.</p>

<p>On the other hand, recent work suggests that the feature
representations of robust models carry several advantages over those of
standard models, such as <a href="https://arxiv.org/abs/1805.12152">better-behaved</a>
<a href="https://arxiv.org/abs/1905.09797">gradients</a>, <a href="https://arxiv.org/abs/1910.08640">representation
invertibility</a>, and more <a href="https://arxiv.org/abs/2005.10190">specialized
features</a>.
We’ve actually discussed some of these observations in earlier posts on this
blog—see, e.g., our posts about 
<a href="https://gradientscience.org/robust_reps">representation learning</a> and 
<a href="https://gradientscience.org/robust_apps">image synthesis</a>.</p>

<p>These desirable properties
might suggest that robust neural networks are learning better feature
representations than standard networks, which could improve transfer
performance.</p>

<h3 id="adversarial-robustness-and-transfer-learning">Adversarial robustness and transfer learning</h3>

<p>So in summary, we have standard models with high accuracy on the source task but
little (or no) robustness; and we have adversarially robust models, which are
worse in terms of ImageNet accuracy, but have the “nice”
representational properties identified and discussed by prior works. Which
models are better for transfer learning?</p>

<p>To answer this question, we trained and examined a large collection
of standard and robust ImageNet models, while grid searching over a wide range of
hyperparameters and architectures to find the best model of each type. (All
models are available for download via our <a href="https://github.com/microsoft/robust-models-transfer">code/model
release</a> and more
details on our training procedure can be found there and in <a href="https://arxiv.org/abs/2007.08489">our
paper</a>). We then performed transfer
learning (using both fixed-feature and full-network refinement) from each
trained model to 12 downstream classification tasks.</p>

<p>It turns out that 
adversarially robust source models fairly consistently outperform their standard counterparts in
terms of downstream accuracy. In the table below, we compare the accuracies of
the best standard model (searching over hyperparameters and
architecture) and the best robust model (searching over the
previous factors as well as robustness level $\varepsilon$):</p>

<p><img alt="Table showing that robust models     perform better than their standard counterparts." class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/results-table.svg" style="width: 100%;"/></p>
<div class="footnote">
    The main result: Adversarially robust models outperform their standard counterparts when transferred to downstream classification tasks.
</div>

<p>This difference in performance tends to be particularly striking in the context of fixed-feature transfer learning. The following graph shows, for each architecture and
downstream classification task, the best standard model compared to the best
robust model in that setting. As we can see, adversarially robust models
improve on the performance of their standard counterparts, and the gap tends to
<em>increase</em> as networks increase in width:</p>

<p><img alt="A bar chart showing that robust models improve on     standard ones even without taking the maximum over architectures." class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/LogisticRegression.svg"/></p>
<div class="footnote">
    Adversarially robust models tend to improve over standard networks for
    individual architectures too. (An analogous graph for full-network
    transfer learning is given in Figure 3 of <a href="https://arxiv.org/abs/2007.08489">our paper</a>.)
</div>

<p>Adversarial robustness improved downstream transfer
performance even when the target task was not a classification one. For example, the
following table compares standard and robust pre-training for use in downstream
object detection and instance segmentation:</p>

<p><img class="bigimg" src="https://gradientscience.org/../assets/robust-transfer-learning/obj-det-results.svg" style="width: 80%;"/></p>
<div class="footnote">
</div>

<h3 id="robustness-versus-accuracy">Robustness versus accuracy</h3>

<p>So it seems like robust models, despite being less accurate on the source task, are actually
better for transfer learning purposes. Indeed, the linear relation between
 ImageNet accuracy and transfer performance observed in prior work (see our discussion above) doesn’t seem
 to hold when the robustness parameter is varied. Compare the graphs below to the ones at the very start of this post:</p>

<p><img class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/wide_resnet50_4_LogisticRegression.svg"/></p>
<div class="footnote">
    Source-task (ImageNet) versus target (fixed-feature) accuracy for models with the same
    architecture while varying the robustness levels. Each dot is a
    WideResNet-50x4 model with $x$ coordinate given by source-task accuracy and
    $y$ coordinate given by fixed-feature transfer learning accuracy.
    Contrast the trends here with the "fixed-feature" trend in the first
    figure of this post—the linear trend depicted there largely disappears as less
    accurate but more robust models perform better in terms of transfer.
</div>

<p>How do we reconcile our observations with these trends observed by prior work?</p>

<p>We hypothesize that robustness and accuracy have <em>disentangled</em> effects on
transfer performance. That is, for a fixed level of robustness, higher
accuracy on the source task helps transfer, and for a fixed level of
accuracy, increased robustness helps transfer. Indeed, as shown below, for a
fixed level of robustness, the accuracy-transfer relation tends to hold
strongly:</p>

<p><img class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/fixed-robustness.svg"/></p>
<div class="footnote">
Even though robust models appear to break the linear
accuracy-transfer trend, this trend is actually preserved for a fixed value of
robustness. Each dot in the graph is a different architecture, trained for the same level of robustness ($\varepsilon = 3.0$). The $x$ coordinate is source task (ImageNet) accuracy, and the $y$ coordinate is the downstream accuracy on each target dataset.
</div>

<p>In addition to reconciling our results with those of prior work, these findings suggest that ongoing work on developing more accurate robust models
may have the added benefit of further improving transfer learning performance.</p>

<h3 id="other-empirical-mysteries-and-future-work">Other empirical mysteries and future work</h3>

<p>This post discussed how adversarially robust models might constitute a promising
avenue for improving transfer learning, and already often outperform standard
models in terms of downstream accuracy. In <a href="https://arxiv.org/abs/2007.08489">our paper</a>, 
we study this phenomenon more closely: for example, we examine the effects of
model width, and we compare adversarial robustness to other notions of
robustness. We also uncover a few somewhat mysterious properties: for example,
resizing images seems to have a non-trivial effect on the relationship between
robustness and downstream accuracy.</p>

<p>Finally, while our work provides evidence that adversarially
robust computer vision models transfer better, understanding precisely <em>why</em> this is the case remains open. More broadly, the results we
observe indicate that we still do not yet fully understand (even empirically)
the ingredients that make transfer learning successful. We hope that our work
prompts an inquiry into the underpinnings of modern transfer learning.</p></div>
    </summary>
    <updated>2020-07-20T00:00:00Z</updated>
    <published>2020-07-20T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2020-07-26T23:34:36Z</updated>
    </source>
  </entry>
</feed>
