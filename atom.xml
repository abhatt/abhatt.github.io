<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-08-20T23:39:22Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8678359761350967382</id>
    <link href="http://blog.computationalcomplexity.org/feeds/8678359761350967382/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/trusting-scientists.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/8678359761350967382" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/8678359761350967382" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/trusting-scientists.html" rel="alternate" type="text/html"/>
    <title>Trusting Scientists</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> A tweet that made me think.</p>
<blockquote class="twitter-tweet"><p dir="ltr" lang="en">If you think you don't trust scientists, you're mistaken. You trust scientists in a million different ways every time you step on a plane, or for that matter turn on your tap or open a can of beans. The fact that you're unaware of this doesn't mean it's not so.</p>— Paul Graham (@paulg) <a href="https://twitter.com/paulg/status/1419765657080578052?ref_src=twsrc%5Etfw">July 26, 2021</a></blockquote><p>The point here is subtle. We don't get on a plane because we "trust scientists", rather we do so because of the strong safety record of commercial aviation. I knew some physicists who won't get on a commuter plane because they worry about the science. Never stopped me.</p><p>It is science that we trust to tell us why planes fly, or the water is our tap is (mostly) safe and healthy. I'm not a big fan of beans but not because of the science. Of course I trust science that created the vaccines.</p><p>It's not just science, but solid engineering and lots and lots of testing. </p><p>Science isn't always right or consistent. When I was a kid not that long ago, we had nine planets in this solar system, dinosaurs were killed off by climate change and homosexuality was a mental illness. Science is fluid, updating as we learn with new data, models and experimentation. Science is at its best when it doesn't trust itself.</p><p>Sometimes people say trust in science to reinforce their beliefs. I've seen smart people say "Trust in the science" about whether vaccinated people should wear masks with completely different conclusions.</p><p>I'm a scientist, should you trust me? Let me quote another Paul G. </p><blockquote><p>“There’s a slightly humorous stereotype about computational complexity that says what we often end up doing is taking a problem that is solved a lot of the time in practice and proving that it’s actually very difficult,” said Goldberg.</p></blockquote><p>The quote comes from a recent <a href="https://www.quantamagazine.org/computer-scientists-discover-limits-of-major-research-algorithm-20210817/">Quanta Magazine article</a> about Paul's recent work with John Fearnley, Alexandros Hollender and Rahul Savani on the hardness of gradient descent. Even many NP-complete problems these days can often be solved in practice.</p><p>Let's end with the quote attributed to statistician George Box, "All models are wrong, but some are useful". Science gives us ways to understand the world and we need to both trust in the science but know the limitations of what it has to say.</p></div>
    </content>
    <updated>2021-08-20T14:51:00Z</updated>
    <published>2021-08-20T14:51:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-20T22:13:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.08842</id>
    <link href="http://arxiv.org/abs/2108.08842" rel="alternate" type="text/html"/>
    <title>Communication-Efficient Federated Learning via Robust Distributed Mean Estimation</title>
    <feedworld_mtime>1629417600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vargaftik:Shay.html">Shay Vargaftik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Basat:Ran_Ben.html">Ran Ben Basat</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Portnoy:Amit.html">Amit Portnoy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mendelson:Gal.html">Gal Mendelson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Ben=Itzhak:Yaniv.html">Yaniv Ben-Itzhak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitzenmacher:Michael.html">Michael Mitzenmacher</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.08842">PDF</a><br/><b>Abstract: </b>Federated learning commonly relies on algorithms such as distributed
(mini-batch) SGD, where multiple clients compute their gradients and send them
to a central coordinator for averaging and updating the model. To optimize the
transmission time and the scalability of the training process, clients often
use lossy compression to reduce the message sizes. DRIVE is a recent state of
the art algorithm that compresses gradients using one bit per coordinate (with
some lower-order overhead). In this technical report, we generalize DRIVE to
support any bandwidth constraint as well as extend it to support heterogeneous
client resources and make it robust to packet loss.
</p></div>
    </summary>
    <updated>2021-08-20T22:49:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.08831</id>
    <link href="http://arxiv.org/abs/2108.08831" rel="alternate" type="text/html"/>
    <title>U-match factorization: sparse homological algebra, lazy cycle representatives, and dualities in persistent(co)homology</title>
    <feedworld_mtime>1629417600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hang:Haibin.html">Haibin Hang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giusti:Chad.html">Chad Giusti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Ziegelmeier:Lori.html">Lori Ziegelmeier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henselman=Petrusek:Gregory.html">Gregory Henselman-Petrusek</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.08831">PDF</a><br/><b>Abstract: </b>Persistent homology is a leading tool in topological data analysis (TDA).
Many problems in TDA can be solved via homological -- and indeed, linear --
algebra. However, matrices in this domain are typically large, with rows and
columns numbered in billions. Low-rank approximation of such arrays typically
destroys essential information; thus, new mathematical and computational
paradigms are needed for very large, sparse matrices.
</p>
<p>We present the U-match matrix factorization scheme to address this challenge.
U-match has two desirable features. First, it admits a compressed storage
format that reduces the number of nonzero entries held in computer memory by
one or more orders of magnitude over other common factorizations. Second, it
permits direct solution of diverse problems in linear and homological algebra,
without decompressing matrices stored in memory. These problems include look-up
and retrieval of rows and columns; evaluation of birth/death times, and
extraction of generators in persistent (co)homology; and, calculation of bases
for boundary and cycle subspaces of filtered chain complexes. Such bases are
key to unlocking a range of other topological techniques for use in TDA, and
U-match factorization is designed to make such calculations broadly accessible
to practitioners.
</p>
<p>As an application, we show that individual cycle representatives in
persistent homology can be retrieved at time and memory costs orders of
magnitude below current state of the art, via global duality. Moreover, the
algebraic machinery needed to achieve this computation already exists in many
modern solvers.
</p></div>
    </summary>
    <updated>2021-08-20T22:56:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-08-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.08825</id>
    <link href="http://arxiv.org/abs/2108.08825" rel="alternate" type="text/html"/>
    <title>Maintaining an EDCS in General Graphs: Simpler, Density-Sensitive and with Worst-Case Time Bounds</title>
    <feedworld_mtime>1629417600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Fabrizio Grandoni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schwiegelshohn:Chris.html">Chris Schwiegelshohn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomon:Shay.html">Shay Solomon</a>, Amitai Uzrad <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.08825">PDF</a><br/><b>Abstract: </b>In their breakthrough ICALP'15 paper, Bernstein and Stein presented an
algorithm for maintaining a $(3/2+\epsilon)$-approximate maximum matching in
fully dynamic {\em bipartite} graphs with a {\em worst-case} update time of
$O_\epsilon(m^{1/4})$; we use the $O_\epsilon$ notation to suppress the
$\epsilon$-dependence. Their main technical contribution was in presenting a
new type of bounded-degree subgraph, which they named an {\em edge degree
constrained subgraph (EDCS)}, which contains a large matching -- of size that
is smaller than the maximum matching size of the entire graph by at most a
factor of $3/2+\epsilon$. They demonstrate that the EDCS can be maintained with
a worst-case update time of $O_\epsilon(m^{1/4})$, and their main result
follows as a direct corollary. In their followup SODA'16 paper, Bernstein and
Stein generalized their result for general graphs, achieving the same update
time of $O_\epsilon(m^{1/4})$, albeit with an amortized rather than worst-case
bound. To date, the best {\em deterministic} worst-case update time bound for
{\em any} better-than-2 approximate matching is $O(\sqrt{m})$ [Neiman and
Solomon, STOC'13], [Gupta and Peng, FOCS'13]; allowing randomization (against
an oblivious adversary) one can achieve a much better (still polynomial) update
time for approximation slightly below 2 [Behnezhad, Lacki and Mirrokni,
SODA'20].
</p>
<p>In this work we\footnote{\em quasi nanos, gigantium humeris insidentes}
simplify the approach of Bernstein and Stein for bipartite graphs, which allows
us to generalize it for general graphs while maintaining the same bound of
$O_\epsilon(m^{1/4})$ on the {\em worst-case} update time. Moreover, our
approach is {\em density-sensitive}: If the {\em arboricity} of the dynamic
graph is bounded by $\alpha$ at all times, then the worst-case update time of
the algorithm is $O_\epsilon(\sqrt{\alpha})$.
</p></div>
    </summary>
    <updated>2021-08-20T22:43:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.08780</id>
    <link href="http://arxiv.org/abs/2108.08780" rel="alternate" type="text/html"/>
    <title>Optimally Efficient Sequential Calibration of Binary Classifiers to Minimize Classification Error</title>
    <feedworld_mtime>1629417600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Kaan Gokcesu, Hakan Gokcesu <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.08780">PDF</a><br/><b>Abstract: </b>In this work, we aim to calibrate the score outputs of an estimator for the
binary classification problem by finding an 'optimal' mapping to class
probabilities, where the 'optimal' mapping is in the sense that minimizes the
classification error (or equivalently, maximizes the accuracy). We show that
for the given target variables and the score outputs of an estimator, an
'optimal' soft mapping, which monotonically maps the score values to
probabilities, is a hard mapping that maps the score values to $0$ and $1$. We
show that for class weighted (where the accuracy for one class is more
important) and sample weighted (where the samples' accurate classifications are
not equally important) errors, or even general linear losses; this hard mapping
characteristic is preserved. We propose a sequential recursive merger approach,
which produces an 'optimal' hard mapping (for the observed samples so far)
sequentially with each incoming new sample. Our approach has a logarithmic in
sample size time complexity, which is optimally efficient.
</p></div>
    </summary>
    <updated>2021-08-20T22:37:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-08-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.08767</id>
    <link href="http://arxiv.org/abs/2108.08767" rel="alternate" type="text/html"/>
    <title>Threshold Phenomena in Learning Halfspaces with Massart Noise</title>
    <feedworld_mtime>1629417600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diakonikolas:Ilias.html">Ilias Diakonikolas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel_M=.html">Daniel M. Kane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kontonis:Vasilis.html">Vasilis Kontonis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tzamos:Christos.html">Christos Tzamos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zarifis:Nikos.html">Nikos Zarifis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.08767">PDF</a><br/><b>Abstract: </b>We study the problem of PAC learning halfspaces on $\mathbb{R}^d$ with
Massart noise under Gaussian marginals. In the Massart noise model, an
adversary is allowed to flip the label of each point $\mathbf{x}$ with
probability $\eta(\mathbf{x}) \leq \eta$, for some parameter $\eta \in
[0,1/2]$.
</p>
<p>The goal of the learner is to output a hypothesis with missclassification
error $\mathrm{opt} + \epsilon$, where $\mathrm{opt}$ is the error of the
target halfspace. Prior work studied this problem assuming that the target
halfspace is homogeneous and that the parameter $\eta$ is strictly smaller than
$1/2$. We explore how the complexity of the problem changes when either of
these assumptions is removed, establishing the following threshold phenomena:
</p>
<p>For $\eta = 1/2$, we prove a lower bound of $d^{\Omega (\log(1/\epsilon))}$
on the complexity of any Statistical Query (SQ) algorithm for the problem,
which holds even for homogeneous halfspaces. On the positive side, we give a
new learning algorithm for arbitrary halfspaces in this regime with sample
complexity and running time $O_\epsilon(1) \, d^{O(\log(1/\epsilon))}$.
</p>
<p>For $\eta &lt;1/2$, we establish a lower bound of $d^{\Omega(\log(1/\gamma))}$
on the SQ complexity of the problem, where $\gamma = \max\{\epsilon,
\min\{\mathbf{Pr}[f(\mathbf{x}) = 1], \mathbf{Pr}[f(\mathbf{x}) = -1]\} \}$ and
$f$ is the target halfspace. In particular, this implies an SQ lower bound of
$d^{\Omega (\log(1/\epsilon) )}$ for learning arbitrary Massart halfspaces
(even for small constant $\eta$). We complement this lower bound with a new
learning algorithm for this regime with sample complexity and runtime
$d^{O_{\eta}(\log(1/\gamma))} \mathrm{poly}(1/\epsilon)$.
</p>
<p>Taken together, our results qualitatively characterize the complexity of
learning halfspaces in the Massart model.
</p></div>
    </summary>
    <updated>2021-08-20T22:43:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.08734</id>
    <link href="http://arxiv.org/abs/2108.08734" rel="alternate" type="text/html"/>
    <title>odeN: Simultaneous Approximation of Multiple Motif Counts in Large Temporal Networks</title>
    <feedworld_mtime>1629417600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sarpe:Ilie.html">Ilie Sarpe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vandin:Fabio.html">Fabio Vandin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.08734">PDF</a><br/><b>Abstract: </b>Counting the number of occurrences of small connected subgraphs, called
temporal motifs, has become a fundamental primitive for the analysis of
temporal networks, whose edges are annotated with the time of the event they
represent. One of the main complications in studying temporal motifs is the
large number of motifs that can be built even with a limited number of vertices
or edges. As a consequence, since in many applications motifs are employed for
exploratory analyses, the user needs to iteratively select and analyze several
motifs that represent different aspects of the network, resulting in an
inefficient, time-consuming process. This problem is exacerbated in large
networks, where the analysis of even a single motif is computationally
demanding. As a solution, in this work we propose and study the problem of
simultaneously counting the number of occurrences of multiple temporal motifs,
all corresponding to the same (static) topology (e.g., a triangle). Given that
for large temporal networks computing the exact counts is unfeasible, we
propose odeN, a sampling-based algorithm that provides an accurate
approximation of all the counts of the motifs. We provide analytical bounds on
the number of samples required by odeN to compute rigorous, probabilistic,
relative approximations. Our extensive experimental evaluation shows that odeN
enables the approximation of the counts of motifs in temporal networks in a
fraction of the time needed by state-of-the-art methods, and that it also
reports more accurate approximations than such methods.
</p></div>
    </summary>
    <updated>2021-08-20T22:47:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.08613</id>
    <link href="http://arxiv.org/abs/2108.08613" rel="alternate" type="text/html"/>
    <title>A Conditional Lower Bound for Episode Matching</title>
    <feedworld_mtime>1629417600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bille:Philip.html">Philip Bille</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=oslash=rtz:Inge_Li.html">Inge Li Gørtz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mozes:Shay.html">Shay Mozes</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Steiner:Teresa_Anna.html">Teresa Anna Steiner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weimann:Oren.html">Oren Weimann</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.08613">PDF</a><br/><b>Abstract: </b>Given two strings $S$ and $P$, the Episode Matching problem is to compute the
length of the shortest substring of $S$ that contains $P$ as a subsequence. The
best known upper bound for this problem is $\tilde O(nm)$ by Das et al. (1997),
where $n,m$ are the lengths of $S$ and $P$, respectively. Although the problem
is well studied and has many applications in data mining, this bound has never
been improved. In this paper we show why this is the case by proving that an
$O((nm)^{1-\epsilon})$ algorithm (even for binary strings) would refute the
popular Strong Exponential Time Hypothesis (SETH). The proof is based on a
simple reduction from Orthogonal Vectors.
</p></div>
    </summary>
    <updated>2021-08-20T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.08558</id>
    <link href="http://arxiv.org/abs/2108.08558" rel="alternate" type="text/html"/>
    <title>Improved Linear-Time Algorithm for Computing the $4$-Edge-Connected Components of a Graph</title>
    <feedworld_mtime>1629417600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Georgiadis:Loukas.html">Loukas Georgiadis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Italiano:Giuseppe_F=.html">Giuseppe F. Italiano</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kosinas:Evangelos.html">Evangelos Kosinas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.08558">PDF</a><br/><b>Abstract: </b>We present an improved algorithm for computing the $4$-edge-connected
components of an undirected graph in linear time. The new algorithm uses only
elementary data structures, and it is simple to describe and to implement in
the pointer machine model of computation.
</p></div>
    </summary>
    <updated>2021-08-20T22:39:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.08444</id>
    <link href="http://arxiv.org/abs/2108.08444" rel="alternate" type="text/html"/>
    <title>A 1+O(1/N) approximation algorithm for TTP(2)</title>
    <feedworld_mtime>1629417600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Imahori:Shinji.html">Shinji Imahori</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.08444">PDF</a><br/><b>Abstract: </b>The traveling tournament problem is a well-known benchmark problem of the
sports scheduling. We propose an approximation algorithm for the traveling
tournament problem with the constraints such that both the number of
consecutive home games and that of consecutive away games are at most two
(called TTP(2)). The approximation ratio of the proposed algorithm is 1 + 24/n
for n teams, which is the first 1 + O(1/n) approximation algorithm for TTP(2).
</p></div>
    </summary>
    <updated>2021-08-20T22:48:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2108.08406</id>
    <link href="http://arxiv.org/abs/2108.08406" rel="alternate" type="text/html"/>
    <title>Estimating distinguishability measures on quantum computers</title>
    <feedworld_mtime>1629417600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Rochisha Agarwal, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rethinasamy:Soorya.html">Soorya Rethinasamy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharma:Kunal.html">Kunal Sharma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wilde:Mark_M=.html">Mark M. Wilde</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2108.08406">PDF</a><br/><b>Abstract: </b>The performance of a quantum information processing protocol is ultimately
judged by distinguishability measures that quantify how distinguishable the
actual result of the protocol is from the ideal case. The most prominent
distinguishability measures are those based on the fidelity and trace distance,
due to their physical interpretations. In this paper, we propose and review
several algorithms for estimating distinguishability measures based on trace
distance and fidelity, and we evaluate their performance using simulators of
quantum computers. The algorithms can be used for distinguishing quantum
states, channels, and strategies (the last also known in the literature as
"quantum combs"). The fidelity-based algorithms offer novel physical
interpretations of these distinguishability measures in terms of the maximum
probability with which a single prover (or competing provers) can convince a
verifier to accept the outcome of an associated computation. We simulate these
algorithms by using a variational approach with parameterized quantum circuits
and find that they converge well for the examples that we consider.
</p></div>
    </summary>
    <updated>2021-08-20T22:48:14Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-08-20T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/08/19/research-staff-member-at-ibm-research-apply-by-august-19-2021/</id>
    <link href="https://cstheory-jobs.org/2021/08/19/research-staff-member-at-ibm-research-apply-by-august-19-2021/" rel="alternate" type="text/html"/>
    <title>Research Staff Member at IBM Research (apply by August 19, 2021)</title>
    <summary>We are seeking to hire ASAP a Research Scientist for the Mathematics of AI research group, with a demonstrated publication record in related areas of mathematics and theoretical computer science. We are specifically interested in expertise in formal logic as well as learning theory, generalizability, information theory, graph algorithms, reinforcement learning, and optimization. Website: https://krb-sjobs.brassring.com/TGnewUI/Search/home/HomeWithPreLoad?PageType=JobDetails&amp;partnerid=26059&amp;siteid=5016&amp;Areq=435593BR#jobDetails=458585_5016 […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are seeking to hire ASAP a Research Scientist for the Mathematics of AI research group, with a demonstrated publication record in related areas of mathematics and theoretical computer science. We are specifically interested in expertise in formal logic as well as learning theory, generalizability, information theory, graph algorithms, reinforcement learning, and optimization.</p>
<p>Website: <a href="https://krb-sjobs.brassring.com/TGnewUI/Search/home/HomeWithPreLoad?PageType=JobDetails&amp;partnerid=26059&amp;siteid=5016&amp;Areq=435593BR#jobDetails=458585_5016">https://krb-sjobs.brassring.com/TGnewUI/Search/home/HomeWithPreLoad?PageType=JobDetails&amp;partnerid=26059&amp;siteid=5016&amp;Areq=435593BR#jobDetails=458585_5016</a><br/>
Email: lhoresh@us.ibm.com</p></div>
    </content>
    <updated>2021-08-19T17:19:34Z</updated>
    <published>2021-08-19T17:19:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-08-20T23:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://bit-player.org/?p=2359</id>
    <link href="http://bit-player.org/2021/riding-the-covid-coaster" rel="alternate" type="text/html"/>
    <link href="http://bit-player.org/2021/riding-the-covid-coaster#comments" rel="replies" type="text/html"/>
    <link href="http://bit-player.org/2021/riding-the-covid-coaster/feed/atom" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Riding the Covid coaster</title>
    <summary type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml">Figure 1 Peaks and troughs, lumps and slumps, wave after wave of surge and retreat: I have been following the ups and downs of this curve, day by day, for a year and a half. The graph records the number … <a href="http://bit-player.org/2021/riding-the-covid-coaster">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Figure 1<img alt="US daily Covid cases from New York Times data, showing raw counts and a seven-day rolling average" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/daily_case_count_graph.svg" width="660"/></p>
<p>Peaks and troughs, lumps and slumps, wave after wave of surge and retreat: I have been following the ups and downs of this curve, day by day, for a year and a half. The graph records the number of newly reported cases of Covid-19 in the United States for each day from 21 January 2020 through 20 July 2021. That’s 547 days, and also exactly 18 months. The faint, slender vertical bars in the background give the raw daily numbers; the bold blue line is a seven-day trailing average. (In other words, the case count for each day is averaged with the counts for the six preceding days.)</p>
<p>I struggle to understand the large-scale undulations of that graph. If you had asked me a few years ago what a major epidemic might look like, I would have mumbled something about exponential growth and decay, and I might have sketched a curve like this one:</p>
<p>Figure 2<img alt="Sketch of simple exponential growth and decay" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/sketch-of-exponential-growth-and-decay-crop.jpg" width="400"/></p>
<p class="undent">My imaginary epidemic is so much simpler than the real thing! The number of daily infections goes up, and then it comes down again. It doesn’t bounce around like a nervous stock market. It doesn’t have seasonal booms and busts. </p>
<p>The graph tracing the actual incidence of the disease makes at least a dozen reversals of direction, along with various small-scale twitches and glitches. The big mountain in the middle has foothills on both sides, as well as some high alpine valleys between craggy peaks. I’m puzzled by all this structural embellishment. Is it mere noise—a product of random fluctuations—or is there some driving mechanism we ought to know about, some switch or dial that’s turning the infection process on and off every few months?</p>
<p>I have a few ideas about possible explanations, but I’m not so keen on any of them that I would try to persuade you they’re correct. However, I <em>do</em> hope to persuade you there’s something here that needs explaining.</p>
<p>Before going further, I want to acknowledge my sources. The data files I’m working with are curated by <em>The New York Times</em>, based on information collected from state and local health departments. Compiling the data is a big job; the <em>Times</em> lists more than 150 workers on the project. They need to reconcile the differing and continually shifting policies of the reporting agencies, and then figure out what to do when the incoming numbers look fishy. (Back in June, Florida had a day with \(-40,000\) new cases.) The entire data archive, now about \(2.3\) gigabytes, is freely available on <a href="https://github.com/nytimes/covid-19-data">GitHub</a>. Figure 1 in this article is modeled on a graph <a href="https://www.nytimes.com/interactive/2021/us/covid-cases.html">updated daily</a> in the <em>Times</em>.</p>
<p>I must also make a few disclaimers. In noodling around with this data set I am not trying to forecast the course of the epidemic, or even to retrocast it—to develop a model accurate enough to reproduce details of timing and magnitude observed over the past year and a half. I’m certainly not offering medical or public-health advice. I’m just a puzzled person looking for simple mechanisms that might explain the overall shape of the incidence curve, and in particular the roller coaster pattern of recurring hills and valleys.</p>
<hr/>
<p>So far, four main waves of infection have washed over the U.S., with a fifth wave now beginning to look like a tsunami. Although the waves differ greatly in height, they seem to be coming at us with some regularity. Eyeballing Figure 1, I get the impression that the period from peak to peak is pretty consistent, at roughly four months.</p>
<p>Periodic oscillations in epidemic diseases have been noticed many times before. The classical example is measles in Great Britain, for which there are weekly records going back to the early 18th century. In 1917 John Brownlee studied the measles data with a form of Fourier analysis called the periodogram. He found that the strongest peak in the frequency spectrum came at a period of 97 weeks, reasonably consistent with the widespread observation that the disease reappears every second year. But Brownlee’s periodograms bristle with many lesser peaks, indicating that the measles rhythm is not a simple, uniform drumbeat. Of particular note is the work of M. S. Bartlett in the 1950s, which includes an early instance of computer modeling in epidemiology, using the Manchester University Computer.Later work, using different methods, suggested that the dynamics of measles epidemics may actually be chaotic, with no long-term order.</p>
<p>The mechanism behind the oscillatory pattern in measles is easy to understand. The disease strikes children in the early school years, and the virus is so contagious that it can run through an entire community in a few weeks. Afterwards, another outbreak can’t take hold until a new cohort of children has reached the appropriate age. No such age dependence exists in Covid-19, and the much shorter period of recurrence suggests that some other mechanism must be driving the oscillations. Nevertheless, it seems worthwhile to try applying Fourier methods to the data in Figure 1.</p>
<p>The Fourier transform de­composes any curve representing a function of time into a sum of simple sine and cosine waves of various frequencies. In textbook examples, the algorithm works like magic. Take a wiggly curve like this one:</p>
<p>Figure 3<img alt="Graph of cos(x) + cos(3x) over range from 0 to 4?" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/cosx+cos3xcurve.svg" width="500"/></p>
<p class="undent">Feed it into the Fourier transformer, turn the crank, and out comes a graph that looks like this, showing the coefficients of various frequency components:</p>
<p>Figure 4<img alt="DCT coefficiencts of the compound cosine curve" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/DCT_coefficiencts_of_coscurve.svg" width=""/></p>
<p class="undent">Technical details: The classical Fourier transform yields complex coefficients, with real and imaginary parts. I am using a variant called the discrete cosine transform, which produces real coefficients. The input curve is generated by the function \(\cos(x) + \cos(3x)\) over the interval from \(0\) to \(4\pi\).Only two coefficients are substantially different from zero, corresponding to waves that make two or six full cycles over the span of the input curve. Those two coefficients capture all the information needed to reconstruct the input. If you draw the curves specified by the two coefficients, then add them up point by point, you get back a replica of the original.</p>
<p>It would be illuminating to have such a succinct encoding of the Covid curve—a couple of numbers that explain its comings and goings. Alas, that’s not so easy. When I poured the Covid data into the Fourier machine, this is what came out:</p>
<p>Figure 5<img alt="DCT coefficients for the Covid curve" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/dct_coef_graph_for_cases_7avg.svg" width=""/></p>
<p>More than a dozen coefficients have significant magnitude; some are positive and some are negative; no obvious pattern leaps off the page. This spectrum, like the simpler one in Figure 4, holds all the information needed to reconstruct its input. I confirmed that fact with a quick computational experiment. But looking at the jumble of coefficients doesn’t help me to understand the structure of the Covid curve. The Fourier-transformed version is even more baffling than the original.</p>
<p>One lesson to be drawn from this exercise is that the Fourier transform is indeed magic: If you want to make it work, you need to master the dark arts. I am no wizard in this department; as a matter of fact, most of my encounters with Fourier analysis have ended in tears and trauma. No doubt someone with higher skills could tease more insight from the numbers than I can. But I doubt that any degree of Fourier finesse will lead to some clear and concise description of the Covid curve. Even with \(200\) years of measles records, Brownlee wasn’t able to isolate a clear signal; with just a year and a half of Covid data, success is unlikely.</p>
<p>Yet my foray into the Fourier realm was not a complete waste of time. Applying the inverse Fourier transform to the first \(13\) coefficients (for wavenumbers \(0\) through \(6\)) yields this set of curves:</p>
<p>Figure 6<img alt="Curves created by inverse Fourier transform from the first 13 coefficients" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/fourier_components_from_dct_cases_7avg.svg" width="400"/></p>
<p class="undent">It looks a mess, but the sum of these \(13\) sinusoidal waves yields quite a handsome, smoothed version of the Covid curve. In Figure 7 below, the pink area in the background shows the <em>Times</em> data, smoothed with the seven-day rolling average. The blue curve, much smoother still, is the waveform reconstructed from the \(13\) Fourier coefficients.</p>
<p>Figure 7<img alt="Reconstructed covid curve from 14 Fourier coefficients" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/reconstructed_covid_curve_from_14_coefs_helv.svg" width="640"/></p>
<p>The reconstruction traces the outlines of all the large-scale features of the Covid curve, with serious errors only at the end points (which are always problematic in Fourier analysis). The Fourier curve also fails to reproduce the spiky triple peak atop the big surge from last winter, but I’m not sure that’s a defect.</p>
<hr/>
<p>Let’s take a closer look at that triple peak. The graph below is an expanded view of the two-month interval from 20 November 2020 through 20 January 2021. The light-colored bars in the background are raw data on new cases for each day; the dark blue line is the seven-day rolling average computed by the <em>Times</em>. </p>
<p>Figure 8<img alt="Expanded view of the Covid graph showing Nov 20 to Jan 20, raw data and rolling average" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/nov_jan_details_graph.svg" width="640"/></p>
<p>The peaks and valleys in this view are just as high and low as those in Figure 1; they look less dramatic only because the horizontal axis has been stretched ninefold. My focus is not on the peaks but on the troughs between them. (After all, there wouldn’t be three peaks if there weren’t two troughs to separate them.) Three data points marked by pink bars have case counts far lower than the surrounding days. Note the dates of those events. November 26 was Thanksgiving Day in the U.S. in 2020; December 25 is Christmas Day, and January 1 is New Years Day. It looks like the virus went on holiday, but of course it was actually the medical workers and public health officials who took a day off, so that many cases did not get recorded on those days.</p>
<p>There may be more to this story. Although the holidays show up on the chart as low points in the progress of the epidemic, they were very likely occasions of higher-than-normal contagion, because of family gatherings, religious services, revelry, and so on. (I commented on the <a href="http://bit-player.org/2020/we-gather-together-2">Thanksgiving risk</a> last fall.) Those “extra” infections would not show up in the statistics until several days later, along with the cases that went undiagnosed or unreported on the holidays themselves. Thus each dip appears deeper because it is followed by a surge.</p>
<p>All in all, it seems likely that the troughs creating the triple peak are a reporting anomaly, rather than a reflection of genuine changes in the viral transmission rate. Thus a curve that smooths them away may give a better account of what’s really going on in the population.</p>
<hr/>
<p>There’s another transformation—quite different from Fourier analysis—that might tell us something about the data. The time derivative of the Covid curve gives the rate of change in the infection rate—positive when the epidemic is surging, negative when it’s retreating. Because we’re working with a series of discrete values, computing the derivative is trivially easy: It’s just the series of differences between successive values.</p>
<p>Figure 9<img alt="First derivatives of three versions of the Covid curve--the raw data, the curve smoothed with a seven-day rolling average, and the curve reconstructed from 13 Fourier coefficients." border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/07/first_derivatives.svg" width=""/></p>
<p>The derivative of the raw data <em>(blue)</em> looks like a seismograph recording from a jumpy day along the San Andreas. The three big holiday anomalies—where case counts change by \(100,000\) per day—produce dramatic excursions. The smaller jagged waves that extend over most of the \(18\)-month interval are probably connected with the seven-day cycle of data collection, which typically show case counts increasing through the work week and then falling off on the weekend.</p>
<p>The seven-day trailing average is designed to suppress that weekly cycle, and it also smooths over some larger fluctuations. The resulting curve <em>(red)</em> is not only less jittery but also has much lower amplitude. (I have expanded the vertical scale by a factor of two for clarity.)</p>
<p>Finally, the reconstituted curve built by summing \(13\) Fourier components yields a derivative curve <em>(green)</em> whose oscillations are mere ripples, even when stretched ver­tically by a factor of four.</p>
<p>The points where the derivative curves cross the zero line—going from positive to negative or vice versa—correspond to peaks or troughs in the underlying case-count curve. Each zero crossing marks a moment when the epidemic’s trend reversed direction, when a growing daily case load began to decline, or a falling one turned around and started gaining again. The blue raw-data curve has \(255\) zero crossings, and the red averaged curve has \(122\). Even the lesser figure implies that the infection trend is reversing course every four or five days, which is not plausible; most of those sign changes must result from noise in the data. </p>
<p>The silky smooth green curve has nine zero crossings, most of which seem to signal real changes in the course of the epidemic. I would like to understand what’s causing those events.</p>
<hr/>
<p>You catch a virus. (Sorry about that.) Some days later you infect a few others, who after a similar delay pass the gift on to still more people. This is the mechanism of exponential (or geometric) growth. With each link in the chain of transmission the number of new cases is multiplied by a factor of \(R\), which is the natural growth ratio of the epidemic—the average number of cases spawned by each infected individual. Starting with a single case at time \(t = 0\), the number of new infections at any later time \(t\) is \(R^t\). If \(R\) is greater than \(1\), even very slightly, the number of cases increases without limit; if \(R\) is less than \(1\), the epidemic fades away.</p>
<p>The average delay between when you become infected and when you infect others is known as the serial passage time, which I am going to abbreviate T<sub>SP</sub> and take as the basic unit for measuring the duration of events in the epidemic. For Covid-19, one T<sub>SP</sub> is probably about five days.</p>
<p>Exponential growth is famously unmanageable. If \(R = 2\), the case count doubles with every iteration: \(1, 2, 4, 8, 16, 32\dots\). It increases roughly a thousandfold after \(10\) T<sub>SP</sub>, and a millionfold after \(20\) T<sub>SP</sub>. The rate of increase becomes so steep that I can’t even graph it except on a logarithmic scale, where an exponential trajectory becomes a straight line.</p>
<p>Figure 10<img alt="Exponential growth for R = 0.8, 1.0, 1.25, 2.0, and 3.0, plotted on a log scale." border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/exp_log_plot.svg" width="500"/></p>
<p class="indent">What is the value of \(R\) for the SARS-CoV-2 virus? No one knows for sure. The number is difficult to measure, and it varies with time and place. Another number, \(R_0\), is often regarded as an intrinsic property of the virus itself, an indicator of how easily it passes from person to person. The Centers for Disease Control and Prevention (CDC) <a href="https://www.cdc.gov/coronavirus/2019-ncov/hcp/planning-scenarios.html">suggests</a> that \(R_0\) for SARS-CoV-2 probably lies between \(2.0\) and \(4.0\), with a best guess of \(2.5\). That would make it catchier than influenza but less so than measles. However, the CDC has also published <a href="https://wwwnc.cdc.gov/eid/article/25/1/17-1901_article">a report</a> arguing that \(R_0\) is “easily misrepresented, misinterpreted, and misapplied.” I’ve certainly been confused by much of what I’ve read on the subject.</p>
<p>Whatever numerical value we assign to \(R\), if it’s greater than \(1\), it cannot possibly describe the complete course of an epidemic. As \(t\) increases, \(R^t\) will grow at an accelerating pace, and before you know it the predicted number of cases will exceed the global human population. For \(R = 2\), this absurdity arrives after about \(33\) T<sub>SP</sub>, which is less than six months.</p>
<p>What we need is a mathematical model with a built-in limit to growth. As it happens, the best-known model in epidemiology features just such a mechanism. Introduced almost 100 years ago by W. O. Kermack and A. G. McKendrick of the Royal College of Physicians in Edinburgh, Recent descriptions of the SIR model usually say \((\mathcal{R})\) stands for <em>removed</em>, acknowledging that recovery is not the only way an infection can end. But I don’t want to be grim today. Also note that I’m using a calligraphic font for \(\mathcal{S},\mathcal{I}\), and \(\mathcal{R}\) to avoid confusion between the growth rate \(R\) and the recovered group \(\mathcal{R}\).it is now called the SIR model because it partitions the human population into three subsets called <em>susceptible</em> \((\mathcal{S})\), <em>infective</em> \((\mathcal{I})\), and <em>recovered</em> \((\mathcal{R})\). Initially (before a pathogen enters the population), everyone is of type \(\mathcal{S}\). Susceptibles who contract the virus become infectives—capable of transmitting the disease to other susceptibles. Then, after each infective’s illness has run its course, that person joins the recovered class. Having acquired immunity through infection, the recovereds will never be susceptible again.</p>
<p>A SIR epidemic can’t keep growing indefinitely for the same reason that a forest fire can’t keep burning after all the trees are reduced to ashes. At the beginning of an epidemic, when the entire population is susceptible, the case count can grow exponentially. But growth slows later, when each infective has a harder time finding susceptibles to infect. Kermack and McKendrick made the interesting discovery that the epidemic dies out before it has reached the entire population. That is, the last infective recovers before the last susceptible is infected, leaving a residual \(\mathcal{S}\) population that has never experienced the disease.</p>
<p>The SIR model itself has gone viral in the past few years. There are tutorials everywhere on the web, as well as scholarly articles and books. (I recommend <a href="https://www.cambridge.org/core/books/epidemic-modelling/6F7376322E00A98D6801B97D9429A0CF#"><em>Epidemic Modelling: An Introduction</em></a>, by Daryl J. Daley and Joseph Gani. Or try <a href="https://people.maths.ox.ac.uk/maini/PKM%20publications/384.pdf"><em>Mathematical Modelling of Zombies</em></a> if you’re feeling brave.) Most accounts of the SIR model, including the original by Kermack and McKendrick, are presented in terms of differential equations. I’m instead going to give a version with discrete time steps—\(\Delta t\) rather than \(dt\)—because I find it easier to explain and because it translates line for line into computer code. In the equations that follow, \(\mathcal{S}\), \(\mathcal{I}\), and \(\mathcal{R}\) are real numbers in the range \([0, 1]\), representing proportions of some fixed-size population.</p>
<p>\[\begin{align}<br/>
	\Delta\mathcal{I} &amp; = \beta \mathcal{I}\mathcal{S}\\[0.8ex]<br/>
	\Delta\mathcal{R} &amp; = \gamma \mathcal{I}\\[1.5ex]<br/>
	\mathcal{S}_{t+\Delta t} &amp; = \mathcal{S}_{t} - \Delta\mathcal{I}\\[0.8ex]<br/>
	\mathcal{I}_{t+\Delta t} &amp; = \mathcal{I}_{t} + \Delta\mathcal{I} - \Delta\mathcal{R}\\[0.8ex]<br/>
	\mathcal{R}_{t+\Delta t} &amp; = \mathcal{R}_{t} + \Delta\mathcal{R}\\[0.8ex]<br/>
\end{align}\]</p>
<p class="undent">The first equation, with \(\Delta\mathcal{I}\) on the left hand side, describes the actual contagion process—the recruitment of new infectives from the susceptible population. The number of new cases is proportional to the product of \(\mathcal{I}\) and \(\mathcal{S}\), since the only way to propagate the disease is to bring together someone who already has it with someone who can catch it. The constant of proportionality, \(\beta\), is a basic parameter of the model. It measures how often (per T<sub>SP</sub>) an infective person encounters others closely enough to communicate the virus.</p>
<p>The second equation, for \(\Delta\mathcal{R}\), similarly describes recovery. For epidemiological pur­poses, you don’t have to be feeling tiptop again to be done with the disease; recovery is defined as the moment when you are no long capable of infecting other people. The model takes a simple approach to this idea, withdrawing a fixed fraction of the infectives in every time step. The fraction is given by the parameter \(\gamma\).</p>
<p>After the first two equations calculate the number of people who are changing their status in a given time step, the last three equations update the population segments accordingly. The susceptibles lose \(\Delta\mathcal{I}\) members; the infectives gain \(\Delta\mathcal{I}\) and lose \(\Delta\mathcal{R}\); the recovereds gain \(\Delta\mathcal{R}\). The total popu­la­tion \(\mathcal{S} + \mathcal{I} + \mathcal{R}\) remains constant throughout.</p>
<p>InIn the recent literature, the ratio \(\beta / \gamma\) is commonly presented not just as analogous to \(R_0\) but as a definition of \(R_0\). I resist this practice because \(R_0\) has too many definitions already. In adopting the symbol \(\rho\) I am following the precedent of David G. Kendall in a 1956 paper. this version of the SIR model, the ratio \(\rho = \beta / \gamma\) determines a natural growth rate, closely analogous to \(R_0\). Higher \(\beta\) means faster recruitment of infectives; lower \(\gamma\) means they remain infective longer. Either of those adjustments increases the growth rate \(\rho\), although the rate also depends on \(\mathcal{S}\) and \(\mathcal{I}\). </p>
<p>Here’s what happens when you put the model in motion. For this run I set \(\beta = 0.6\) and  \(\gamma = 0.2\), which implies that \(\rho =  3.0\). Another number that needs to be specified is the initial proportion of infectives; I chose \(10^{-6}\), or in other words one in a million.  The model ran for \(100\) T<sub>SP</sub>, with a time step of \(\Delta t = 0.1\) T<sub>SP</sub>; thus there were \(1{,}000\) iterations overall.</p>
<p>Figure 11<img alt="SIR model: a graph of susceptible, infective, and recovered populations as a function of time" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/SIR_model.svg" width="640"/></p>
<p>Let me call your attention to a few features of this graph. At the outset, nothing seems to happen for weeks and weeks, and then all of a sudden a huge blue wave rises up out of the calm waters. Starting from one case in a population of a million, it takes \(18\) T<sub>SP</sub> to reach one case in a thousand, but just \(12\) more T<sub>SP</sub> to reach one in \(10\). </p>
<p>Note that the population of infectives reaches a peak near where the susceptible and recovered curves cross—that is, where \(\mathcal{S} = \mathcal{R}\). This relationship holds true over a wide range of parameter values. That’s not surprising, because the whole epidemic process acts as a mechanism for converting susceptibles into recovereds, via a brief transit through the infective stage. But, as Kermack and McKendrick predicted, the conversion doesn’t quite go to completion. At the end, about \(6\) percent of the population remains in the susceptible category, and there are no infectives left to convert them. This is the condition called herd immunity, where the population of susceptibles is so diluted that most infectives recover before they can find someone to infect. It’s the end of the epidemic, though it comes only after \(90+\) percent of the people have gotten sick. (That’s not what I would call a victory over the virus.)</p>
<p>The \(\mathcal{I}\) class in the SIR model can be taken as a proxy for the tally of new cases tracked in the <em>Times</em> data. The two variables are not quite the same—infectives remain in the class \(\mathcal{I}\) until they recover, whereas new cases are counted only on the day they are reported—but they are very similar and roughly proportional to one another. And that brings me to the main point I want to make about the SIR model: In Figure 11 the blue curve for infectives looks nothing like the corresponding new-case tally in Figure 1. In the SIR model, the number of infectives starts near zero, rises steeply to a peak, and thereafter tapers gradually back to zero, never to rise again. It’s a one-hump camel. The roller coaster Covid curve is utterly different. </p>
<p>The detailed geometry of the \(\mathcal{I}\) curve depends on the values assigned to the parameters \(\beta\) and \(\gamma\). Changing those variables can make the curve longer or shorter, taller or flatter. But no choice of parameters will give the curve multiple ups and downs. There are no oscillatory solutions to these equations.</p>
<p>The SIR model strikes me as so plausible that it—or some variant of it—really <em>must</em> be a correct description of the natural course of an epidemic. But that doesn’t mean it can explain what’s going on right now with Covid-19. A key element of the model is saturation: the spread of the disease stalls when there are too few susceptibles left to catch it. That can’t be what caused the steep downturn in Covid incidence that began in January of this year, or the earlier slumps that began in April and July of 2020. We were nowhere near saturation during any of those events, and we still aren’t now. (For the moment I’m ignoring the effects of vaccination. I’ll take up that subject below.)</p>
<p>In Figure 11 there comes a dramatic triple point where each of the three categories constitutes about a third of the total population. If we projected that situation onto the U.S., we would have (in very round numbers) \(100\) million active infections, another \(100\) million people who have recovered from an earlier bout with the virus, and a third \(100\) million who have so far escaped (but most of whom will catch it in the coming weeks). That’s orders of magnitude beyond anything seen so far. The cumulative case count, which combines the \(\mathcal{I}\) and \(\mathcal{R}\) categories, is approaching \(37\) million, or \(11\) percent of the U.S. population. Figure 12<img alt="Us cases avg full population" border="0" class="alignright" height="" src="http://bit-player.org/wp-content/uploads/2021/08/us_cases_avg_full_population.svg" width="330"/>Even if the true case count is double the official tally, we are still far short of the model’s crucial triple point. Judged from the per­spective of the SIR model, we are still in the early stages of the epidemic, where case counts are too low to see in the graph. (If you rescale Figure 1 so that the <em>y</em> axis spans the full U.S. population of 330 million, you get the flatline graph at right.)</p>
<hr/>
<p>If we are still on the early part of the curve, in the regime of rampant exponential growth, it’s easy to understand the surging accelerations we’ve seen in the worst moments of the epidemic. The hard part is explaining the repeated slowdowns in viral transmission that punctuate the Covid curve. In the SIR model, the turnaround comes when the virus begins to run out of victims, but that’s a one-time phenomenon, and we haven’t gotten there yet. What can account for the deep valleys in the <em>Times</em> Covid curve?</p>
<p>Among the ideas that immediately come to mind, one strong contender is feedback. We all have access to real-time information on the status of the epidemic. It comes from governmental agencies, from the news media, from idiots on Facebook, and from the personal communications of family, friends, and neighbors. Most of us, I think, respond appropriately to those messages, modulating our anti-contagion precautions according to the perceived severity of the threat. When it’s scary outside, we hunker down and mask up. When the risk abates, it’s party time again! I can easily imagine a scenario where such on-again, off-again measures would trigger oscillations in the incidence of the disease.</p>
<p>If this hypothesis turns out to be true, it is cause for both hope and frustration. Hope because the interludes of viral retreat suggest that our tools for fighting the epidemic must be reasonably effective. Frustration because the rebounds indicate we’re not deploying those tools as well as we could. Look again at the Covid curve of Figure 1, specifically at the steep downturn following the winter peak. In early February, the new case rate was dropping by \(30{,}000\) per week. Over the first three weeks of that month, the rate was cut in half. Whatever we were doing then, it was working brilliantly. If we had just continued on the same trajectory, the case count would have hit zero in early March. Instead, the downward slope flattened, and then turned upward again. </p>
<p>We had another chance in June. All through April and May, new cases had been falling steadily, from \(65{,}000\) to \(17{,}000\), a pace of about \(-800\) cases a day. If we’d been able to sustain that rate for just three more weeks, we’d have crossed the finish line in late June. But again the trend reversed course, and by now we’re back up well above \(100{,}000\) cases a day.</p>
<p>Are these pointless ups and downs truly caused by feedback effects? I don’t know. I am particularly unsure about the “if only” part of the story—the idea that if only we’d kept the clamps on for just a few more weeks, the virus would have been eradicated, or nearly so. But it’s an idea to keep in mind.</p>
<p>Perhaps we could learn more by creating a feedback loop in the SIR model, and looking for oscillatory dynamics. Negative feedback is anything that acts to slow the infection rate when that rate is high, and to boost it when it’s low. Such a contrarian mechanism could be added to the model in several ways. Perhaps the simplest is a lockdown threshold: Whenever the number of infectives rises above some fixed limit, everyone goes into isolation; when the \(\mathcal{I}\) level falls below the threshold again, all cautions and restrictions are lifted. It’s an all-or-nothing rule, which makes it simple to implement. We need a constant to represent the threshold level, and a new factor (which I am naming \(\varphi\), for fear) in the equation for \(\Delta \mathcal{I}\):</p>
<p>\[\Delta\mathcal{I} = \beta \varphi \mathcal{I} \mathcal{S}\]</p>
<p class="undent">The \(\varphi\) factor is \(1\) whenever \(\mathcal{I}\) is below the threshold, and \(0\) when it rises above. The effect is to shut down all new infections as soon as the threshold is reached, and start them up again when the rate falls.</p>
<p>Does this scheme produce oscillations in the \(\mathcal{I}\) curve? Strictly speaking, the answer is yes, but you’d never guess it by looking at the graph.</p>
<p>Figure 13<img alt="SIR model with feedback but no visible oscillations" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/SIR_model_with_feedback.svg" width=""/></p>
<p class="undent">The feedback loop serves as a control system, like a thermostat that switches the furnace off and on to maintain a set temperature. In this case, the feedback loop holds the infective population steady at the threshold level, which is set at \(0.05\). On close examination, it turns out that \(\mathcal{I}\) is oscillating around the threshold level, but with such a short period and tiny amplitude that the waves are invisible. The value bounces back and forth between \(0.049\) and \(0.051\).</p>
<p>To get macroscopic oscillations, we need more than feedback. The SIR output shown below comes from a model that combines feedback with a delay between measuring the state of the epidemic and acting on that information. Introducing such a delay is not the only way to make the model swing, but it’s certainly a plausible one. As a matter of fact, a model <em>without</em> any delay, in which a society responds instantly to every tiny twitch in the case count, seems wholly unrealistic.</p>
<p>Figure 14<img alt="SIR model with oscillations caused by feedback combined with a sensory delay" border="0" class="alignright" height="" src="http://bit-player.org/wp-content/uploads/2021/08/SIR_model_with_oscillations.svg" width="640"/></p>
<p>The model of Figure 14 adopts the same parameters, \(\beta = 0.6\) and \(\gamma = 0.2\), as the version of Figure 13, as well as the same lockdown threshold \((0.05)\). It differs only in the timing of events. If the infective count climbs above the threshold at time \(t\), control measures do not take effect until \(t + 3\); in the meantime, infections continue to spread through the population. The delay and overshoot on the way up are matched by a delay and undershoot at the other end of the cycle, when lockdown continues for three T<sub>SP</sub> after the threshold is crossed on the way down.</p>
<p>Given these specific parameters and settings, the model produces four cycles of diminishing amplitude and increasing wavelength. (No further cycles are possible because \(\mathcal{I}\) remains below the threshold.) Admittedly, those four spiky sawtooth peaks don’t look much like the humps in the Covid curve. If we’re going to seriously consider the feedback hypothesis, we’ll need stronger evidence than this. But the model is very crude; it could be refined and improved.</p>
<p>The fact is, I really want to believe that feedback could be a major component in the oscillatory dynamics of Covid-19. It would be comforting to know that our measures to combat the epidemic have had a powerful effect, and that we therefore have some degree of control over our fate. But I’m having a hard time keeping the faith. For one thing, I would note that our countermeasures have not always been on target. In the epidemic’s first wave, when the characteristics of the virus were largely unknown, the use of facemasks was discouraged (except by medical personnel), and there was a lot of emphasis on hand washing, gloves, and sanitizing surfaces. Not to mention drinking bleach. Those measures were probably not very effective in stopping the virus, but the wave receded anyway.</p>
<p>Another source of doubt is that wavelike fluctuations are not unique to Covid-19. Figure 15<img alt="1918 spanish flu waves  Wikipedia" border="0" class="alignright" height="" src="http://bit-player.org/wp-content/uploads/2021/08/1918_spanish_flu_waves-Wikipedia.gif" width="400"/>On the contrary, they seem to be a common characteristic of epi­demics across many times and places. The \(1918\textrm{–}1919\) influ­enza epidemic had at least three waves. Figure 15, which <a href="https://commons.wikimedia.org/wiki/File:1918_spanish_flu_waves.gif">Wikipedia attrib­utes</a> to the CDC, shows influ­enza deaths per \(1{,}000\) people in the United Kingdom. Those humps look awfully familiar. They are similar enough to the Covid waves that it seems natural to look for a common cause. But if both patterns are a product of feedback effects, we have to suppose that public health measures undertaken a century ago, in the middle of a world war, worked about as well as those today. (I’d like to think there’s been some progress.)</p>
<hr/>
<p>One detail of the SIR model troubles me. As formulated by Kermack and McKendrick, the model treats infection and recovery as symmetrical, mirror-image processes, both of them described by exponential functions. The exponential rule for infections makes biological sense. You can only get the virus via transmission from someone who already has it, so the number of new infections is proportional to the number of existing infections. But recovery is different; it’s not contagious. Although the duration of the illness may vary to come extent, there’s no reason to suppose it would depend on the number of other people who are sick at the same time.</p>
<p>In the model, a fixed fraction of the infectives, \(\gamma \mathcal{I}\), recover at every time step. Figure 16<img alt="Exponential decline bargraph" border="0" class="alignright" height="" src="http://bit-player.org/wp-content/uploads/2021/08/exponential_decline_bargraph.svg" width=""/>For \(\gamma = 0.2\), this rule generates the ex­ponential distribution seen in Figure 16. Imagine that some large group of people have all been infected at the same time, \(t = 0\). At \(t = 1\), a fifth of the infectives recover, leaving \(80\) per­cent of the cohort still infective. At \(t = 2\), a fifth of the remaining \(80\) per­cent are removed from the infective class, leaving \(64\) percent. And so it goes. Even after \(10\) T<sub>SP</sub>, more than \(10\) percent of the original group remain infectious.</p>
<p>The long tail of this distribution corresponds to illnesses that persist for many weeks. Such cases exist, but they are rare. <a href="https://www.cdc.gov/coronavirus/2019-ncov/hcp/duration-isolation.html">According to the CDC</a>, most Covid patients have no detectable “replication-competent virus” \(10\) days after the onset of symptoms. Even in the most severe cases, with immunocompromised patients, \(20\) days of infectivity seems to be the outer limit. I don’t know who was the first to notice that the exponential distribution of recovery times is too broad, but I know it wasn’t me. In 2001 Alun L. Lloyd wrote on this theme in the context of measles epidemics (<a href="https://pubmed.ncbi.nlm.nih.gov/11589638/"><em>Theoretical Population Biology</em> Vol. 60, No. 1, pp. 59–71</a>).These observations suggest a different strategy for modeling recovery. Rather than assuming that a fixed fraction of patients recover at every time step, we might get a better approximation to the truth by assuming that all patients recover (or at least become noninfective) after a fixed duration of illness.</p>
<p>Modifying the model for a fixed period of infectivity is not difficult. We can keep track of the infectives with a data structure called a queue. Each new batch of newly recruited infectives goes into the tail of the queue, then advances one place with each time step. After \(m\) steps (where \(m\) is the duration of the illness), the batch reaches the head of the queue and joins the company of the recovered. Here is what happens when \(m = 3\) T<sub>SP</sub>:</p>
<p>Figure 17<img alt="SIR model outcome when infection lasts 3 TSP." border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/sirq_0p000001_0p6_3p0_0p1_100.svg" width="640"/></p>
<p>I chose \(3\) T<sub>SP</sub> for this example because it is close to the median duration in the expon­ential distribution in Figure 11, and therefore ought to resemble the earlier result. And so it does, approximately. As in Figure 11, the peak in the infectives curve lies near where the susceptible and recovered curves cross. But the peak never grows quite as tall; and, for obvious reasons, it decays much faster. As a result, the epidemic ends with many more susceptibles untouched by the disease—more than 25 percent.</p>
<p>A disease duration of \(3\) T<sub>SP</sub>, or about \(15\) days, is still well over the CDC estimates of the typical length. Shortening the queue to \(2\) T<sub>SP</sub>, or about \(10\) days, transforms the outcome even more dramatically. Now the susceptible and recovered curves never cross, and almost \(70\) percent of the susceptible population remains uninfected when the epidemic peters out.</p>
<p>Figure 18<img alt="SIR model outcome when infection lasts 2 TSP" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/sirq_0p000001_0p6_2p0_0p1_100.svg" width="640"/></p>
<p>Figure 18 comes a little closer to describing the current Covid situation in the U.S. than the other models considered above. It’s not that the curves’ shape resembles that of the data, but the overall magnitude or intensity of the epidemic is closer to observed levels. Of the models presented so far, this is the first that reaches a natural limit without burning through most of the population. Maybe we’re on to something.</p>
<p>On the other hand, there are a couple of reasons for caution. First, with these parameters, the initial growth of the epidemic is extremely slow; it takes \(40\) or \(50\) T<sub>SP</sub> before infections have a noticeable effect on the population. That’s well over six months. Second, we’re still dealing with a one-hump camel. Even though most of the population is untouched, the epidemic has run its course, and there will not be a second wave. Something important is still missing.</p>
<p>Before leaving this topic behind, I want to point out that the finite time span of a viral infection gives us a special point of leverage for controlling the spread of the disease. The viruses that proliferate in your body must find a new host within a week or two, or else they face extinction. Therefore, if we could completely isolate every individual in the country for just two or three weeks, the epidemic would be over. Admittedly, putting each and every one of us into solitary confinement is not feasible (or morally acceptable), but we could strive to come as close as possible, strongly discouraging all forms of person-to-person contact. Testing, tracing, and quarantines would deal with straggler cases. My point is that a very strict but brief lockdown could be both more effective and less disruptive than a loose one that goes on for months. Where other strategies aim to flatten the curve, this one attempts to break the chain.</p>
<hr/>
<p>When Covid emerged late in 2019, it was soon labeled a <em>pandemic</em>, signifying that it’s bigger than a mere epidemic, that it’s everywhere. But it’s not everywhere at once. Flareups have skipped around from region to region and country to country. Perhaps we should view the pandemic not as a single global event but as an ensemble of more localized outbreaks.</p>
<p>Suppose small clusters of infections erupt at random times, then run their course and subside. By chance, several geographically isolated clusters might be active over the same range of dates and add up to a big bump in the national case tally. Random fluctuations could also produce interludes of widespread calm, which would cause a dip in the national curve.</p>
<p>We can test this notion with a simple computational experiment, modeling a popu­lation divided into \(N\) clusters or communities. For each cluster a SIR model generates a curve giving the proportion of infectives as a function of time. The initiation time for each of these mini-epidemics is chosen randomly and independently. Summing the \(N\) curves gives the total case count for the country as a whole, again as a function of time.</p>
<p>Before scrolling down to look at the graphs generated by this process, you might make a guess about how the experiment will turn out. In particular, how will the shape of the national curve change as the number of local clusters increases?</p>
<p>If there’s just one cluster, then the national curve is obviously identical to the trajectory of the disease in that one place. With two clusters, there’s a good chance they will not overlap much, and so the national curve will probably have two humps, with a deep valley between them. With \(N = 3\) or \(4\), overlap becomes more of an issue, but the sum curve still seems likely to have \(N\) humps, perhaps with shallower depressions separating them. Before I saw the results, I made the following guess about the behavior of the sum as \(N\) continues increasing: The sum curve will always have approximately \(N\) peaks, I thought, but the height difference between peaks and troughs should get steadily smaller. Thus at large \(N\) the sum curve would have many tiny ripples, small enough that the overall curve would appear to be one broad, flat-topped hummock.</p>
<p>So much for my intuition. Here are two examples of sum curves generated by clusters of \(N\) mini-epidemics, one curve for \(N = 6\) and one for \(N = 50\). The histories for individual clusters are traced by fine red lines; the sums are blue. All the curves have been scaled so that the highest peak of the sum curve touches \(1.0\).</p>
<p>Figure 19<img alt="MultiSVG curves 6 and 50 with 4 peaks" border="0" class="alignleft" height="" src="http://bit-player.org/wp-content/uploads/2021/08/multiSVG_curves_6_and_50_with_4_peaks.svg" width="900"/></p>
<p class="indent">My Technical details: Cluster initiation time is chosen uniformly at random between \(0\) and \(80\). \(\beta\) is a random normal variable with mean \(0.6\) and standard deviation \(0.1\), allowing some variation in the intensity and duration of the individual sub-epidemics. The initial infective level is \(0.001\).guess about the “broad, flat-topped hummock” with many shallow ripples was altogether wrong. The number of peaks does <em>not</em> increase in proportion to \(N\). As a matter of fact, both of the sum curves in Figure 19 have four distinct peaks (possibly five in the example at right), even though the number of component curves contributing to the sum is only six in one case and is \(50\) in the other.</p>
<p>I have to confess that the two examples in Figure 19 were not chosen at random. I picked them because they looked good, and because they illustrated a point I wanted to make—namely that the number of peaks in the sum curve remains nearly constant, regardless of the value of \(N\). Figure 20 assembles a more representative sample, selected without deliberate bias but again showing that the number of peaks is not sensitive to \(N\), although the valleys separating those peaks get shallower as \(N\) grows.</p>
<p>Figure 20<img alt="MultiSIR tableau A" border="0" class="alignleft" height="" src="http://bit-player.org/wp-content/uploads/2021/08/multiSIR_tableau_A.svg" width="900"/></p>
<p class="indent">The Figure 21<img alt="Peak numbers bar graph" border="0" class="marginalia alignleft" height="" src="http://bit-player.org/wp-content/uploads/2021/08/peak_numbers_bar_graph.svg" width=""/>takeaway message from these simulations seems to be that almost any collection of randomly timed mini-epidemics will combine to form a macro-epidemic with just a few waves. The number of peaks is not always four, but it’s seldom very far from that number. The bar graphs in Figure 21 offer some quantitative evidence on this point. They record the distribution of the number of peaks in the sum curve for values of \(N\) between \(4\) and \(100\). Each set of bars represents \(1{,}000\) repetitions of the process. In all cases the peak falls at \(N = 3, 4,\) or \(5\).</p>
<p>The question is: Why \(4 \pm 1\)? Why do we keep seeing those particular numbers? And if \(N\), the number of components being summed, has little influence on this property of the sum curve, then what <em>does</em> govern it? I puzzled over these questions for some time before a helpful analogy occurred to me. </p>
<p>Suppose you have a bunch of sine waves, all at the same frequency \(f\) but with randomly assigned phases; that is, the waves all have the same shape, but they are shifted left or right along the \(x\) axis by random amounts. What would the sum of those waves look like? The answer is: another sine wave of frequency \(f\). This is a little fact that’s been known for ages (at least since Euler) and is <a href="https://math.stackexchange.com/questions/535600/sum-of-sinusoids-with-same-frequency-sinusoid-proof/1239123">not hard to prove</a>, but it still comes as a bit of a shock every time I run into it. I believe the same kind of argument can explain the behavior of a sum of SIR curves, even though those curves are not sinusoidal. The component SIR curves have a period of \(20\) to \(30\) T<sub>SP</sub>. In a model run that spans \(100\) T<sub>SP</sub>, these curves can be considered to have a frequency of between three and five cycles per epidemic period. Their sum should be a wave with the same frequency—something like the Covid curve, with its four (or four and a half) prominent humps. In support of this thesis, when I let the model run to \(200\) T<sub>SP</sub>, I get a sum curve with seven or eight peaks.</p>
<p>I am intrigued by the idea that an epidemic might arrive in cyclic waves not because of anything special about viral or human behavior but because of a mathematical process akin to wave interference. It’s such a cute idea, dressing up an obscure bit of counterintuitive mathematics and bringing it to bear on a matter of great importance to all of us. And yet, alas, a closer look at the Covid data suggests that nature doesn’t share my fondness for summing waves with random phases.</p>
<p>Figure 22, again based on data extracted from the <em>Times</em> archive, plots \(49\) curves, representing the time course of case counts in the Lower \(48\) states and the District of Columbia. I have separated them by region, and in each group I’ve labeled the trace with the highest peak. We already know that these curves yield a sum with four tall peaks; that’s where this whole investigation began. But the \(49\) curves do not support the notion that those peaks might be produced by summing randomly timed mini-epidemics. The oscillations in the \(49\) curves are <em>not</em> randomly timed; there are strong correlations between them. And many of the curves have multiple humps, which isn’t possible if each mini-epidemic is supposed to act like a SIR model that runs to completion. </p>
<p>Figure 22<img alt="Covid curves for 48 U.S. states and the District of Columbia, in six regional groups." border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/region_curves.svg" width="640"/></p>
<p>Although these curves spoil a hypothesis I had found alluring, they also reveal some interesting facts about the Covid epidemic. I knew that the first wave was concentrated in New York City and surrounding areas, but I had not realized how much the second wave, in the summer of 2020, was confined to the country’s southern flank, from Florida all the way to California. The summer wave this year is also most intense in Florida and along the Gulf Coast. Coincidence? When I showed the graphs to a friend, she responded: “Air conditioning.”</p>
<hr/>
<p>Searching for the key to Covid, I’ve tried out three slightly whimsical notions: the possibility of a periodic signal, like the sunspot cycle, bringing us waves of infection on a regular schedule; feedback loops producing yo-yo dynamics in the case count; and randomly timed mini-epidemics that add up to a predictable, slow variation in the infection rate. In retrospect they still seem like ideas worth looking into, but none of them does a convincing job of explaining the data. </p>
<p>In my mind the big questions remain unanswered. In November of 2020 the daily tally of new Covid cases was above \(100{,}000\) and rising at a fearful rate. Three months later the infection rate was falling just as steeply. What changed between those dates? What action or circumstance or accident of fate blunted the momentum of the onrushing epidemic and forced it into retreat? And now, just a few months after the case count bottomed out, we are again above \(100{,}000\) cases per day and still climbing. What has changed again to bring the epidemic roaring back?</p>
<p>There are a couple of obvious answers to these questions. As a matter of fact, those answers are sitting in the back of the room, frantically waving their hands, begging me to call on them. First is the vaccination campaign, which has now reached half the U.S. population. The incredibly swift development, manufacture, and distribution of those vaccines is a wonder. In the coming months and years they are what will save us, if anything can. But it’s not so clear that vaccination is what stopped the big wave last winter. The sharp downturn in infection rates began in the first week of January, when vaccination was just getting under way in the U.S. On January 9 (the date when the decline began) only about \(2\) percent of the population had received even one dose. The vaccination effort reached a peak in April, when more than three million doses a day were being administered. By then, however, the dropoff in case numbers had stopped and reversed. If you want to argue that the vaccine ended the big winter surge, it’s hard to align causation with chronology.</p>
<p>On the other hand, the level of vaccination that has now been achieved should exert a powerful damping effect on any future waves. Removing half the people from the susceptible list may not be enough to reach herd immunity and eliminate the virus from the population, but it ought to be enough to turn a growing epidemic into a wilting one.</p>
<p>Figure 23<img alt="SIR model with vaccination of 50 percent of the population and R_0 left unchanged at 3.0." border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/sirv_graph_0p6_0p2_0p5_0.svg" width=""/></p>
<p>The SIR model of Figure 23 has the same parameters as the model of Figure 3 \((\beta = 0.6, \gamma = 0.2,\) implying \(\rho = 3.0)\), but \(50\) percent of the people are vaccinated at the start of the simulation. With this diluted population of susceptibles, essentially nothing happens for almost a year. The epidemic is dormant, if not quite defunct.</p>
<p>That’s the world we should be living in right now, according to the SIR model. Instead, today’s new case count is \(141{,}365\); almost \(81{,}556\) people are hospitalized with Covid infections; and 704 people have died. What gives? How can this be happening?</p>
<p>At this point I must acknowledge the other hand waving in the back of the room: the Delta variant, otherwise known as B.1.617.2. Half a dozen mutations in the viral spike protein, which binds to a human cell-surface receptor, have apparently made this new strain at least twice as contagious as the original one.</p>
<p>Figure 24<img alt="Sirv graph 0p6 0p1 0p5 0" border="0" class="centered" height="" src="http://bit-player.org/wp-content/uploads/2021/08/sirv_graph_0p6_0p1_0p5_0.svg" width=""/></p>
<p class="undent">In Figure 24 contagiousness is doubled by increasing \(\rho\) from \(3.0\) to \(6.0\). That boost brings the epidemic back to life, although there is still quite a long delay before the virus becomes widespread in the unvaccinated half of the population.</p>
<p>The situation is may well be worse than the model suggests. All the models I have reported on here pretend that the human population is homogeneous, or thoroughly mixed. If an infected person is about to spread the virus, everyone in the country has the same probability of being the recipient. This assumption greatly simplifies the con­struction of the model, but of course it’s far from the truth. In daily life you most often cross paths with people like yourself—people from your own neighborhood, your own age group, your own workplace or school. Those frequent contacts are also people who share your vaccination status. If you are unvaccinated, you are not only more vulnerable to the virus but also more likely to meet people who carry it. This somewhat subtle birds-of-a-feather effect is what allows us to have “<a href="https://www.nytimes.com/2021/07/16/health/covid-delta-cdc-walensky.html">a pandemic of the unvaccinated</a>.”</p>
<p>Recent reports have brought still more unsettling and unwelcome news, with evidence that even fully vaccinated people may sometimes spread the virus. I’m waiting for confirmation of that before I panic. (But I’m waiting with my face mask on.)</p>
<p>Having demonstrated that I understand nothing about the history of the epidemic in the U.S.—why it went up and down and up and down and up and down and up and down—I can hardly expect to understand the present upward trend. About the future I have no clue at all. Will this new wave tower over all the previous ones, or is it Covid’s last gasp? I can believe anything.</p>
<p>But let us not despair. This is not the zombie apocalypse. The survival of humanity is not in question. It’s been a difficult ordeal for the past \(18\) months, and it’s not over yet, but we can get through this. Perhaps, at some point in the not-too-distant future, we’ll even understand what’s going on.</p>
<h3>Data and Source Code</h3>
<p><em>The New York Times</em> data archive for Covid-19 cases and deaths in the United States is available in <a href="https://github.com/nytimes/covid-19-data">this GitHub repository</a>. The version I used in preparing this article, cloned on 21 July 2021, is identified as “commit c3ab8c1beba1f4728d284c7b1e58d7074254aff8″. You should be able to access the identical set of files through <a href="https://github.com/nytimes/covid-19-data/tree/c3ab8c1beba1f4728d284c7b1e58d7074254aff8">this link</a>.</p>
<p>Source code for the SIR models and for generating the illustrations in this article is also <a href="https://github.com/bit-player/Covidcurve">available on GitHub</a>. The code is written in the <a href="https://julialang.org/">Julia programming language</a> and organized in <a href="https://github.com/fonsp/Pluto.jl">Pluto</a> notebooks.</p>
<h3>Further Reading</h3>
<p class="biblio">Bartlett, M. S. 1956. Deterministic and stochastic models for recurrent epidemics. In <em>Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 4: Contributions to Biology and Problems of Health</em>, pp. 81–109. Berkeley: University of California Press.</p>
<p class="biblio">Bartlett, M. S. 1957. Measles periodicity and community size. <em>Journal of the Royal Statistical Society, Series A (General)</em> 120(1):48–70.</p>
<p class="biblio">Brownlee, John. 1917. An investigation into the periodicity of measles epidemics in London from 1703 to the present day by the method 0f the periodogram. <em>Philosophical Transactions of the Royal Society  of London, Series B</em>, 208:225–250.</p>
<p class="biblio">Daley, D. J., and  J. Gani. 1999.  <em>Epidemic Modelling: An Introduction</em>.  Cam­bridge: Cambridge University Press.</p>
<p class="biblio">Kendall, David G. 1956. Deterministic and stochastic epidemics in closed populations. In <em>Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 4: Contributions to Biology and Problems of Health</em>, pp. 149–165. Berkeley: University of California Press.</p>
<p class="biblio">Kermack, W. O., and A. G. McKendrick. 1927. A contribution to the mathematical theory of epidemics. <em>Proceedings of the Royal Society of London, Series A</em> 115:700–721.</p>
<p class="biblio">Lloyd, Alun L. 2001. Realistic distributions of infectious periods in epidemic models: changing patterns of persistence and dynamics. <em>Theoretical Population Biology</em> 60:59–71.</p>
<p class="biblio">Smith?, Robert (editor). 2014. <em>Mathematical Modelling of Zombies</em>. Ottawa: University of Ottawa Press.</p></div>
    </content>
    <updated>2021-08-18T22:00:22Z</updated>
    <published>2021-08-18T22:00:22Z</published>
    <category scheme="http://bit-player.org" term="biology"/>
    <category scheme="http://bit-player.org" term="computing"/>
    <category scheme="http://bit-player.org" term="mathematics"/>
    <category scheme="http://bit-player.org" term="modern life"/>
    <category scheme="http://bit-player.org" term="statistics"/>
    <author>
      <name>Brian Hayes</name>
      <uri>http://bit-player.org</uri>
    </author>
    <source>
      <id>http://bit-player.org/feed/atom</id>
      <link href="http://bit-player.org" rel="alternate" type="text/html"/>
      <link href="http://bit-player.org/feed/atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">An amateur's outlook on computation and mathematics</subtitle>
      <title xml:lang="en-US">bit-player</title>
      <updated>2021-08-20T16:57:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/120</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/120" rel="alternate" type="text/html"/>
    <title>TR21-120 |  How to Find Water in the Ocean: A Survey on Quantified Derandomization | 

	Roei Tell</title>
    <summary>The focus of this survey is the question of *quantified derandomization*, which was introduced by Goldreich and Wigderson (2014): Does derandomization of probabilistic algorithms become easier if we only want to derandomize algorithms that err with extremely small probability? How small does this probability need to be in order for the problem's complexity to be affected?

This question opens the door to studying natural relaxed versions of the derandomization problem, and allows us to construct algorithms that are more efficient than in the general case as well as to make gradual progress towards solving the general case. In the survey I describe the body of knowledge accumulated since the question's introduction, focusing on the following directions and results:

1. *Hardness vs ``quantified'' randomness:* Assuming sufficiently strong circuit lower bounds, we can derandomize probabilistic algorithms that err extremely rarely while incurring essentially no time overhead.

2. For general probabilistic polynomial-time algorithms, *improving on the brute-force algorithm for quantified derandomization implies breakthrough circuit lower bounds*, and this statement holds for any given probability of error.

3. Unconditional *algorithms for quantified derandomization of low-depth circuits and formulas*, as well as *near-matching reductions* of the general derandomization problem to quantified derandomization for such models.

4. *Arithmetic quantified derandomization*, and in particular constructions of hitting-set generators for polynomials that vanish extremely rarely. 

5. *Limitations of certain black-box techniques* in quantified derandomization, as well as a tight connection between black-box quantified derandomization and the classic notion of *pseudoentropy*.

Most of the results in the survey are from known works, but several results are either new or are strengthenings of known results. The survey also offers a host of concrete challenges and open questions surrounding quantified derandomization.</summary>
    <updated>2021-08-18T20:15:46Z</updated>
    <published>2021-08-18T20:15:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-20T23:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21911</id>
    <link href="https://gilkalai.wordpress.com/2021/08/18/to-cheer-you-up-in-difficult-times-29-free-will-predictability-and-quantum-computers/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 29: Free will, predictability and quantum computers</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I wrote a paper, in Hebrew, entitled “Free will, predictability and quantum computers.” (Click for the pdf file). As you probably know, the free will problem is the apparent contradiction between the fact that the laws of nature are deterministic … <a href="https://gilkalai.wordpress.com/2021/08/18/to-cheer-you-up-in-difficult-times-29-free-will-predictability-and-quantum-computers/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I wrote a paper, in Hebrew, entitled “<a href="https://gilkalai.files.wordpress.com/2021/08/freewill.pdf">Free will, predictability and quantum computers</a>.” (Click for the pdf file). As you probably know, the free will problem is the apparent contradiction between the fact that the laws of nature are deterministic on the one hand, and the notion that people are making free choices that effect their future on the other. Here on my blog, I mentioned the free will problem twice: once, briefly, in connection with a <a href="https://gilkalai.wordpress.com/2008/09/03/the-prisonner-dilemma-sympathy-and-yaaris-challenge/">lecture by Menachem Yaari</a> (2008), and once in <a href="https://gilkalai.wordpress.com/2017/10/30/test-your-intuition-33-the-great-free-will-poll/">TYI 33</a> (2017) conducting the “great free will poll” (see picture below for the outcomes). As for the paper, I hesitated whether to write it in Hebrew or in English; At the end I chose Hebrew and I plan to prepare also an English version sometime in the fall.</p>
<p>Here is the abstract of my new paper.</p>
<blockquote>
<p style="text-align: right;"><span style="color: #0000ff;"><strong>תקציר:</strong> המאמר עוסק בקשר בין השאלות הנוגעות להיתכנות מחשבים קוונטיים, הפרדיקטביליות של מערכות קוונטיות מורכבות בטבע, והסתירה הקיימת לכאורה בין חוקי הטבע לבין רצון חופשי. נדון במקביל במחשב הקוונטי “סיקמור” בעל 12 יחידות חישוב (קיוביטים), ובאליס, שעל רצונה החופשי ננסה לתהות. התאוריה של המחבר העוסקת באי האפשרות של חישוב קוונטי, מצביעה באופן ישיר על אי האפשרות לנבא במדויק את מחשב הסיקמור, כמו גם את מוחה של אליס.  בניתוח מורכב יותר נראה, שאי האפשרות של חישוב קוונטי תומכת בגישה לפיה חוקי הטבע אינם שוללים בחירה חופשית. בבסיס הטיעון הזה עומדת עמימות בדרך שבה העתיד נקבע מהעבר ואשר איננה נעוצה באופי המתמטי של חוקי הפיסיקה (שהם לגמרי דטרמיניסטים), אלא בתיאור הפיסיקלי של העצמים שאנו דנים בהם. אנו דנים גם בהפרדה בין טענות לגבי העתיד שטמונות במארג הסיבתי הקיים בין העבר והעתיד, לבין טענות הנוגעות לעתיד ואשר אינן נמצאות במארג זה</span></p>
<p><em><span style="color: #0000ff;"><strong>Abstract:</strong> We study the connection between the possibility of quantum computers, the predictability of complex quantum systems in nature, and the free will problem. We consider in parallel two examples: the Sycamore quantum computer with 12 qubits (computing elements) and Alice, whose decisions and free will we try to question. The author’s theory that quantum computation is impossible (in principle) directly indicates that the future of both Alice’s brain and the Sycamore quantum computer cannot be predicted. A more involved analysis shows that failure of quantum computation supports the view that the laws of nature do not contradict free will. At the center of the argument is ambiguity in the way the future is determined by the past, not in terms of the mathematical laws of physics (which are fully deterministic) but in terms of the physical description of the objects we discuss. In addition, we discuss the separation between claims about the future that belong to the causal fabric of the past and the future, and claims that don’t belong to this fabric.</span></em></p>
</blockquote>
<p>Two examples that we consider in parallel throughout the paper are “Alice”, whose free will we try to explore, and Google’s “Sycamore” quantum computer with 12 qubits. So, even if Google’s Sycamore does not lead to “quantum supremacy” (and I suspect it does not!) it could still be used in the pursuit of human free will <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/> . Aram Harrow’s hypothetical quantum computer from our 2012 quantum debate (<a href="https://rjlipton.wpcomstaging.com/2012/03/05/the-quantum-super-pac/">post 4</a>) also plays a role in my paper.</p>
<p>My interest in the problem was provoked by Avishai Margalit in the mid 90s. Later I was engaged in a long email debate about it and related philosophical questions with <a href="https://gilkalai.wordpress.com/2012/11/25/happy-birthday-ron-aharoni/">Ron Aharoni</a> following his 2009 (Hebrew) book on philosophy.  Over the years since then, I followed with interest writings on the problem also by Scott Aaronson, Sabine Hossenfelder, Tzahi Gilboa, and others, and had brief discussions about free will with Bernard Chazelle and few other friends. Writing the paper gave me an opportunity to discuss it again with philosophers friends and colleagues. An opportunity that I am greatly enjoying.</p>
<p>One question that I initially discussed but later left out is the following:</p>
<blockquote>
<p><span style="color: #ff0000;"><em>From the point of view of people in the mid 20th century, did quantum mechanics offer an opportunity for understanding the apparent contradiction between free will and the deterministic laws of nature? (Schrödinger himself wrote a paper where he was skeptical about this.)</em></span></p>
</blockquote>
<p>My <em>a priori</em> intuition about the free will problem is in analogy with Zeno’s famous motions paradoxes. Zeno’s paradoxes gave an opportunity to reexamine the mathematics and physics of motion but not to shake the common sense understanding of motion. (In fact, the ancient Greeks knew enough about the mathematics and physics of motion to conclude that motion is possible and that Achilles will overtake the tortoise, and they could compute precisely when.)  Similarly, the question of free will is an opportunity to explore determinism and related issues, but probably not to challenge our basic understanding of human choice. </p>
<p>Here are a few relevant links. Papers by <a href="https://idp.nature.com/authorize/casa?redirect_uri=https://www.nature.com/articles/138013a0.pdf%3Forigin%3Dppub&amp;casa_token=QYfC6JDMEysAAAAA:3L1hIHNrjrkwAfM7R3gDlMOiNf6qt2G0gfeIbv-uEktIikDVOz-0m38p2QEyQHAEMw4hKN9rxXH_mS4F8w">Schrödinger</a> (1936), <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwifwZvCkbDyAhUK_rsIHZ-WCNIQFnoECAYQAQ&amp;url=http%3A%2F%2Ftotallyrandom.info%2Fwp-content%2Fuploads%2F2018%2F05%2FBohrcausandcomp.pdf&amp;usg=AOvVaw2GWtvQ0cMc-G8srN2dE57z">Bohr</a> (1937), and an essay by Einstein on free will (see picture below). <a href="https://www.jstor.org/stable/23349956?seq=1#metadata_info_tab_contents">Ron Aharoni’s paper (Hebrew)</a> on Newcomb’s paradox published in “Iyun” from 1984 (Ron kindly agreed to explain his view on the FW problem in some future post.); <a href="https://www.scottaaronson.com/blog/?p=1438">A post</a> by Scott Aaronson about <a href="http://arxiv.org/abs/1306.0159">his paper</a> on the matter; Posts by Sabine Hossenfelder (<a href="http://backreaction.blogspot.com/2016/01/free-will-is-dead-lets-bury-it.html">2016</a>), (<a href="http://backreaction.blogspot.com/2014/01/10-misconceptions-about-free-will.html">2014</a>), (<a href="http://backreaction.blogspot.com/2020/10/you-dont-have-free-will-but-dont-worry.html">2020</a>), (<a href="http://backreaction.blogspot.com/2013/07/you-probably-have-no-free-will-but-dont.html">2013</a>), (<a href="http://backreaction.blogspot.com/2019/05/how-to-live-without-free-will.html">2019</a>), (<a href="http://backreaction.blogspot.com/2011/09/predetermined-lunch-and-moral.html">2011</a>), (<a href="http://backreaction.blogspot.com/2012/02/free-will-function.html">2012</a>) (and <a href="https://arxiv.org/abs/1202.0720">her paper</a> on the matter); <a href="https://www.preposterousuniverse.com/blog/2011/07/13/free-will-is-as-real-as-baseball/">A post</a> by Sean Carrol (2011); <a href="https://www.tau.ac.il/~igilboa/pdf/Gilboa_Free_Will.pdf">Itzhak Gilboa’s take</a>; A <a href="https://arxiv.org/abs/2104.11591">paper</a> by Neven, Read and Rees with a proposed engineering of conscious quantum animat that possesses agency and feelings.</p>
<p>This post can be seen as my response to <a href="https://gilkalai.wordpress.com/2017/10/30/test-your-intuition-33-the-great-free-will-poll/">TTY33</a> for which <a href="https://gilkalai.wordpress.com/2017/10/30/test-your-intuition-33-the-great-free-will-poll/#comment-36833">Ori was eagerly waiting</a>. The paper could be seen as a FW booster for Hebrew-speaking audience, and, as I said, I hope to produce a similar FW booster for English-speaking audience during the next fall.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/08/fw-pic.png"><img alt="FW-pic" class="alignnone size-full wp-image-21924" height="796" src="https://gilkalai.files.wordpress.com/2021/08/fw-pic.png" width="1144"/></a></p>
<p><span style="color: #ff0000;">(Clockwise) The cover of Jennan Ismael’s 2016 book, Alice, the Sycamore computer, and a cartoon about free will.</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2021/08/fw-pic5.png"><img alt="FW-pic5" class="alignnone size-full wp-image-21993" height="868" src="https://gilkalai.files.wordpress.com/2021/08/fw-pic5.png" width="1279"/></a></p>
<p><span style="color: #ff0000;">Three figures from the paper</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2021/08/fw-pic2.png"><img alt="FW-pic2" class="alignnone size-full wp-image-21934" height="1053" src="https://gilkalai.files.wordpress.com/2021/08/fw-pic2.png" width="1321"/></a></p>
<p><span style="color: #ff0000;">Einstein’s Essay on free will (left) and the outcomes of our 2017 great free will poll.</span></p>


<p/></div>
    </content>
    <updated>2021-08-18T19:07:55Z</updated>
    <published>2021-08-18T19:07:55Z</published>
    <category term="Philosophy"/>
    <category term="Quantum"/>
    <category term="Free will"/>
    <category term="predictability"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-08-20T23:37:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/08/18/doomed-apartments</id>
    <link href="https://11011110.github.io/blog/2021/08/18/doomed-apartments.html" rel="alternate" type="text/html"/>
    <title>Doomed apartments</title>
    <summary>The University of California, Irvine, maintains a sprawling faculty housing complex, University Hills, in which I live, in order to make homes affordable in what would otherwise be a very expensive part of the country. Most of it is single-family homes, owned by faculty on long-term land leases, but it also has several clusters of rental units. The oldest of these, and the nearest to my office, is unimaginatively named Las Lomas (“the hills” in Spanish, a frequent language for California place names).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The University of California, Irvine, maintains a sprawling faculty housing complex, <a href="https://en.wikipedia.org/wiki/University_Hills,_Irvine">University Hills</a>, in which I live, in order to make homes affordable in what would otherwise be a very expensive part of the country. Most of it is single-family homes, owned by faculty on long-term land leases, but it also has several clusters of rental units. The oldest of these, and the nearest to my office, is unimaginatively named Las Lomas (“the hills” in Spanish, a frequent language for California place names).</p>

<p style="text-align: center;"><img alt="Las Lomas Apartments, UC Irvine" src="https://www.ics.uci.edu/~eppstein/pix/laslomas/C-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p>I haven’t spent a lot of time within Las Lomas, but I walk past it every time I go to work, as it’s the closest part of University Hills to my office. It’s mainly been used for short-term visitors: when Martin Nöllenburg was a postdoc, he lived there, and when Vijay Vazirani joined our department he stayed there until his house with Milena Mihail (who moved here a little later) became ready.</p>

<p style="text-align: center;"><img alt="Las Lomas Apartments, UC Irvine" src="https://www.ics.uci.edu/~eppstein/pix/laslomas/G-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p>Already in December 2019, the university announced that it would demolish Las Lomas to make way for higher-density for-purchase faculty homes, even less imaginatively named “<a href="https://icha.uci.edu/area12/">Area 12</a>”. The last apartment residents were kicked out last Spring, and it’s been closed off to vehicles (but still open to pedestrians, with the grounds still maintained) since then. The most recent timeline I’ve seen has demolition scheduled to begin in November, with move-ins to the new homes ready in time for the 2023–2024 academic year. I’m not sure what it’s likely to end up looking like; the project site used to have two site plans (<a href="https://icha.uci.edu/wp-content/uploads/2020/04/ICHA-PA12-CONCEPTUAL-SITE-PLAN-EXHIBIT_2020-04-27-with-Ortho-Image-at-150dpi-1.pdf">1</a>, <a href="https://icha.uci.edu/wp-content/uploads/2020/04/PA12-illustrative-L1.1_Illustrative-Site-Plan-2.pdf">2</a>) and <a href="https://web.archive.org/web/20200922150048/https://icha.uci.edu/area12/">elevations</a> but they’re no longer linked and maybe no longer current.</p>

<p>Because of its imminent demise, I made a project this summer of visiting Las Lomas and taking some photos. There are no residents, and some of the amenities have been slowly falling apart for lack of maintenance, but it still has a quiet residential feeling.</p>

<p style="text-align: center;"><img alt="Las Lomas Apartments, UC Irvine" src="https://www.ics.uci.edu/~eppstein/pix/laslomas/K-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p><a href="https://www.ics.uci.edu/~eppstein/pix/laslomas/">The rest of the photo album</a>.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106779815577019752">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-08-18T16:46:00Z</updated>
    <published>2021-08-18T16:46:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-08-18T23:52:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://kamathematics.wordpress.com/?p=328</id>
    <link href="https://kamathematics.wordpress.com/2021/08/18/how-to-ask-for-a-letter-of-recommendation/" rel="alternate" type="text/html"/>
    <title>How to Ask for a Letter of Recommendation</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The end of the year is approaching at an alarming rate, which means that many students will require letters of recommendations for graduate school or jobs. These letters are written by more senior academics and researchers at the busiest time of the year, when they’re handling other responsibilities like research, teaching, grant writing, and reviewing, … <a class="more-link" href="https://kamathematics.wordpress.com/2021/08/18/how-to-ask-for-a-letter-of-recommendation/">Continue reading<span class="screen-reader-text"> "How to Ask for a Letter of Recommendation"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The end of the year is approaching at an alarming rate, which means that many students will require letters of recommendations for graduate school or jobs. These letters are written by more senior academics and researchers at the busiest time of the year, when they’re handling other responsibilities like research, teaching, grant writing, and reviewing, and may be writing a dozen letters on top of it all. The purpose of this post is to guide applicants on how to ask for a letter of recommendation, simultaneously strengthening the applicant’s package, and making the letter writing process as painless as possible for their writers. Feel free to share this with anyone applying for grad school or jobs. If someone is asking you for a letter, you could share this post with them, so they know what you need.</p>



<p>Note that this document is written by a computer scientist and intended for applications to computer science things, and may or may not carry over to other fields.</p>



<p><strong>Whom to ask</strong></p>



<p>Ask people who know you and your work well. Ideally people who you’ve worked on research with. Their words will carry the most influence for whatever position you’re applying to. On the other hand, if you don’t have a letter from someone you did a major research project with (especially when applying for grad school), this is a red flag. Aim to have at least one such letter. While grad school applications typically solicit three letters of recommendation, very few people have all focused on research (I personally had two, from co-advisors on one project).</p>



<p>Some have suggested having someone “at arm’s length” is beneficial for faculty applications (and anything that comes later, including awards and fellowships), in order to show that your work is known more broadly in the community. </p>



<p>Letters from people who did nothing more than teach you a course are typically valued much less than research-based letters. Nonetheless, most grad school applicants will have at least one or two such letters, since it is rare to have the opportunity to do research projects with three different advisors. If you must, try to request letters from people who you left an impression on, or otherwise had meaningful interactions with. Try to avoid letters from people who can do nothing more than repeat information on your CV/transcript: these letters are often very short and uninformative, and as with any such letter, are unlikely to help your case (and may even hurt it).</p>



<p><strong>When to ask</strong></p>



<p>Please ask <strong>well </strong>in advance. As a rough rule of thumb, by late October is reasonable for “standard” deadlines which are in December, which is common for CS grad school, postdoc, and faculty applications. You might have to ask earlier if there are earlier deadlines (which is the case for some postdoc applications), but give at least a 6 week lead time. Try to mention the earliest deadline in your email request.</p>



<p>Asking early also benefits the applicant. You should have a solid picture about these key details of your application well in advance.</p>



<p>I may still agree to write a letter for you even after the end of October, but chances decrease the longer you wait. On the other hand, I am more likely to agree to late requests if I know you very well or we have worked together very closely. However, I would still be annoyed, and you probably want to avoid annoying your allies in this process.</p>



<p><strong>What to provide</strong></p>



<p>You should give your letter writers as much information as possible. They might not use all of it, but it is good to have on hand when they are inevitably writing letters at 4 AM the day <s>before</s> after the deadline. I’m writing this as a catch-all list, so some points might not be relevant for the position you’re applying to. Say, when you’re applying for faculty positions, they don’t need to know your undergraduate grades. The goal is to give your writers the gist of who you’re trying to market yourself as, so they can support your story as best they can.</p>



<p>This doesn’t all need to be provided when you first ask, but please share it as early as possible. Make it as easy to find as you can. For example, a single email with everything as an attachment, and a clear title to the email such as “X’s application materials”. You could also share a link to a Dropbox or Google Drive folder with all the materials. This has the benefit that you can include preliminary versions of your materials, and update them as you refine them.</p>



<p>Mandatory:</p>



<ul><li>A shared spreadsheet with a list of all the places you’re applying to, the deadlines, any special instructions for letter submission, and a space to mark when the letter is submitted.</li><li>Your research statement/statement of purpose</li><li>Your CV</li></ul>



<p>Optional but recommended:</p>



<ul><li>Anything else that the application asks you for, e.g. your cover letter, teaching statement,  diversity statement, transcript.</li><li>Who else is writing letters for you, and what your relationship to them is.</li><li>A brief summary of the work you’ve done or the contributions you’ve made with the recommender, to refresh their memory.</li></ul>



<p>The last one is helpful since I like to know what “role” I’m playing. Am I your lead letter writer who people will look to first? Or am I the third letter, mostly confirming what other people already know? There may also be some unique perspective I can give, based on who your other reviewers are.</p>



<p><strong>Afterwards?</strong></p>



<p>To the best of my knowledge, it is not customary in Computer Science to send gifts to your letter writers. You should just thank them genuinely, and pass on the favour when you are in a similar position of power.</p>



<p><strong>What if…</strong></p>



<p>It has been brought to my attention that, unfortunately, many letter writers ask the applicant for either a draft or full letter, which they simply sign. I could write a whole post on this topic, but in short, I consider this unacceptable behaviour from the perspective of the letter writer, and I urge them to rethink this practice. </p>



<p>But what should the applicant do when put in this position? It helps if you can see what typical academic recommendation letters look like, since they have a certain tone, but access to such letters is generally out of the reach of most applicants. My recommendation is to be positive (now is not the time for modesty), but don’t lie (these things can follow you around for longer than you might think). Try to make the first paragraph a brief summary that conveys the overall sentiment of the letter. In subsequent paragraphs, you should describe the research you worked on with the letter writer (don’t assume the reader is highly familiar with the topic, but be succinct in terms of background), as well as your specific contributions to the project. Include anecdotes, if appropriate. This may be awkward, but you have been put into an awkward position.</p>



<p><strong>Acknowledgments</strong></p>



<p>Thanks to Anand Sarwate, Thomas Steinke, and Jon Ullman for helpful comments.</p></div>
    </content>
    <updated>2021-08-18T16:35:21Z</updated>
    <published>2021-08-18T16:35:21Z</published>
    <category term="Academic"/>
    <author>
      <name>Gautam</name>
    </author>
    <source>
      <id>https://kamathematics.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://kamathematics.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://kamathematics.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://kamathematics.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://kamathematics.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Kamathematics</title>
      <updated>2021-08-20T23:39:06Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/08/17/hyperbolic-geometry-squishes</id>
    <link href="https://11011110.github.io/blog/2021/08/17/hyperbolic-geometry-squishes.html" rel="alternate" type="text/html"/>
    <title>Hyperbolic geometry squishes graphs</title>
    <summary>An important theme in information visualization is focus+context: allowing viewers to look at part of a visualization in detail, while still keeping a big picture of the whole. One common method simulates a magnifying glass by having a small zoomed-in window that expands the point your mouse is pointing to. But if this window is placed over the mouse it can block parts of the visualization at an intermediate level of focus, near enough that they’re covered by the zoom window but not so near that they’re visible in it. If it’s off to the side, your eye bounces back and forth between the zoom and the big picture. Instead, another old method for focus+context shows everything in a single view by bringing in hyperbolic geometry. Draw the big visualization in the hyperbolic plane, and then view it using a Poincaré disk model, centered on your point of interest. This smoothly interpolates all scales of interest, from your focus to the whole visualization, into a single view, without any distortion of angles and with controlled distortion of other shape properties.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>An important theme in information visualization is focus+context: allowing viewers to look at part of a visualization in detail, while still keeping a big picture of the whole. One common method simulates a magnifying glass by having a small zoomed-in window that expands the point your mouse is pointing to. But if this window is placed over the mouse it can block parts of the visualization at an intermediate level of focus, near enough that they’re covered by the zoom window but not so near that they’re visible in it. If it’s off to the side, your eye bounces back and forth between the zoom and the big picture. Instead, another old method for focus+context shows everything in a single view by bringing in hyperbolic geometry. Draw the big visualization in the hyperbolic plane, and then view it using a <a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model">Poincaré disk model</a>, centered on your point of interest. This smoothly interpolates all scales of interest, from your focus to the whole visualization, into a single view, without any distortion of angles and with controlled distortion of other shape properties.</p>

<p>Hyperbolic focus+context seems like it could be a good fit for graph drawing. And it can work pretty well for trees, which always have nice drawings in the hyperbolic plane. Here’s <a href="https://commons.wikimedia.org/wiki/File:Hyperbolic_tree_Space_in_general.jpg">an example</a> of what it ends up looking like, produced by information visualization specialist <a href="http://www.johnold.org/">L. John Old</a>:</p>

<p style="text-align: center;"><img alt="Hyperbolic tree visualization, by L. John Old, from https://commons.wikimedia.org/wiki/File:Hyperbolic_tree_Space_in_general.jpg" src="https://11011110.github.io/blog/assets/2021/old-hypertree.jpg"/></p>

<p>But my newest preprint, “Limitations on Realistic Hyperbolic Graph Drawing” (<a href="https://arxiv.org/abs/2108.07441">arXiv:2108.07441</a>) argues that for other kinds of graphs, this idea of drawing a graph hyperbolically and then viewing it through a Euclidean model of hyperbolic geometry has some significant limitations, in some ways more severe than the limitations imposed by making Euclidean drawings (and then using some other method for focus+context).</p>

<p>To understand why, it’s important to know that hyperbolic geometry lacks the scalability of Euclidean geometry. What I mean is that, in Euclidean geometry, you can expand any shape or drawing by a constant scale factor and get another drawing that looks the same. The shape of the plane does not depend on the units that you use to measure length: light-years, miles, millimeters, angstroms, etc., all just differ from each other by an arbitrary choice of scale factor. On the other hand, the hyperbolic plane has a single natural unit of length. At smaller scales, it looks very much like the Euclidean plane. At larger scales, the differences between hyperbolic and Euclidean geometry kick in. When we use the Poincaré disk model to view the hyperbolic plane, we see that unit distance. The Euclidean radius of the whole disk model is proportional to the Euclidean size that we would see when viewing an object of unit hyperbolic size at the center of our focus. We can’t zoom in the Poincaré model any deeper than that.</p>

<p>This means that, to use hyperbolic drawings that work with the Poincaré disk model in focus+context applications, we need graph drawings in the hyperbolic plane where the sizes of the parts of the graph we want to zoom in on are not much smaller than the unit of length. But this is just not possible, in many cases. The geometry of the hyperbolic plane forces the features of graphs drawn in it to be squeezed tightly together, much more tightly than unit distance. In particular:</p>

<ul>
  <li>
    <p>Maximal planar graphs can never be drawn planarly with straight edges and with their vertices farther than distance \(O(1/\sqrt{n})\) from the nearest edge, and for some graphs this goes down to \(O(1/n)\).</p>
  </li>
  <li>
    <p>When a maximal planar graph is drawn with straight edges its vertices all constant distance apart from each other (which is always possible), some of its angles will be exponentially sharp.</p>
  </li>
  <li>
    <p>Even very simple graphs like \(K_{1,1,n}\) (below) and the \(n\times n\) grid graphs have both kinds of bad behavior: vertices very close to edges, and (if vertices are well separated) very sharp angles.</p>

    <p style="text-align: center;"><img alt="The complete tripartite graph K_{1,1,9}" src="https://11011110.github.io/blog/assets/2021/k119.svg" width="80%"/></p>
  </li>
  <li>
    <p>Nonplanar drawings can be forced to have angles of \(O(1/n^2)\), compared to the Euclidean plane where less-sharp angles proportional to \(1/n\) are always possible.</p>
  </li>
  <li>
    <p>When drawing nonplanar graphs, with nonzero thickness for the edges and nonzero radius for the vertices, you may be forced to use very thin edges with thickness \(O(1/n)\). Otherwise some edges may be completely covered and invisible.</p>
  </li>
</ul>

<p>Of course, there still exist plenty of graphs other than the trees and the bad examples of my paper that have nice hyperbolic drawings. It might be interesting to try to characterize which ones they are and to find good drawings when they exist.</p>

<p style="text-align: center;"><img alt="Order-5 square tiling of the hyperbolic plane, viewed in the Poincar&#xE9; disk model" src="https://11011110.github.io/blog/assets/2021/45tess.svg" width="60%"/></p>

<p>I’ll be presenting this at <a href="https://algo.inf.uni-tuebingen.de/gd2021/">this year’s Graph Drawing symposium</a>, in mid-September, for which registration is currently open. They’re trying a hybrid conference with both limited in-person participation (in Tübingen, Germany) and online participation; I’ll be on the online side, but I’m curious to see how this format works.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106775723303189658">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-08-17T23:14:00Z</updated>
    <published>2021-08-17T23:14:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-08-18T23:52:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8195</id>
    <link href="https://windowsontheory.org/2021/08/16/random-approx-conference/" rel="alternate" type="text/html"/>
    <title>RANDOM/APPROX conference</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[Guest post by Mary Wooters; the conference already started but there are still great activities tomorrow and Wednesday. On an unrelated note, please make sure to watch the new Theory Shorts episode by the Simons Institute of Computing on lower bounds in computational complexity, featuring Madhu Sudan, Paul Beame, Faith Ellen, Jelani Nelson, and Manuel … <a class="more-link" href="https://windowsontheory.org/2021/08/16/random-approx-conference/">Continue reading <span class="screen-reader-text">RANDOM/APPROX conference</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Mary Wooters;  the conference already started but there are still great activities tomorrow and Wednesday.  On an unrelated note, please make sure to watch the new <a href="https://www.youtube.com/watch?v=-DWmBhMgWrI">Theory Shorts episode </a>by the Simons Institute of Computing on lower bounds in computational complexity, featuring Madhu Sudan, Paul Beame, Faith Ellen, Jelani Nelson, and Manuel Sabin.–Boaz]</em></p>



<p><strong>Call for Participation: APPROX/RANDOM 2021</strong></p>



<p>The 25th International Workshop on Randomization and Computation (RANDOM 2021) and the 24th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2021) will be starting on Monday August 16!  The conferences will be held as parallel virtual conferences, August 16-18, 2021. RANDOM 2021 focuses on applications of randomness to computational and combinatorial problems while APPROX 2021 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems.</p>



<p>To learn more about the conferences and program, visit:</p>



<p>APPROX: <a href="https://approxconference.wordpress.com/approx-2021/" rel="noreferrer noopener" target="_blank">https://approxconference.wordpress.com/approx-2021/</a><br/>RANDOM: <a href="https://randomconference.com/random-2021-home/" rel="noreferrer noopener" target="_blank">https://randomconference.com/random-2021-home/</a></p>



<p>In addition to an exciting program of live talks and discussion (to complement pre-recorded talks), the conference will feature two invited talks, by Jelani Nelson (UC Berkeley) and Vera Traub (ETH Zurich), as well as a social event with trivia and a cartoon caption contest!</p>



<p>Registration is only $10 for general audience members.  You can register here: <a href="https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience" rel="noreferrer noopener" target="_blank">https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience</a></p>



<p/>



<p/></div>
    </content>
    <updated>2021-08-16T22:20:17Z</updated>
    <published>2021-08-16T22:20:17Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-08-20T23:38:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4373523315220884395</id>
    <link href="http://blog.computationalcomplexity.org/feeds/4373523315220884395/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/what-are-most-important-46-papers-in.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4373523315220884395" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4373523315220884395" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/what-are-most-important-46-papers-in.html" rel="alternate" type="text/html"/>
    <title>What are the most important 46 papers in Computer Science? Harry Lewis has a book about them!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> (Disclosure:  Harry Lewis was my PhD advisor. For a blog post on  disclosures and bias see my post on that topic <a href="https://blog.computationalcomplexity.org/2010/07/conflicts-of-interest.html">here</a>.)</p><p>Harry Lewis has a book out: <a href="https://www.amazon.com/Ideas-That-Created-Future-Computer/dp/0262045303/ref=sr_1_1?dchild=1&amp;qid=1626491251&amp;refinements=p_27%3AHarry+Lewis&amp;s=books&amp;sr=1-1">Ideas that Created the Future: Classic Papers in Computer Science</a></p><p>He picked out the 46 (why 46? Why not 46?) classic papers in computer science and, for each one, has a short article saying why its important, and then has the paper itself, though perhaps shortened (leave out the boring parts) or in some cases he has an excerpt of a book (e.g.,<i> The Mythical Man Month</i> which is why I blogged about that book recently <a href="https://blog.computationalcomplexity.org/search?q=brooks">here</a>).</p><p>Harry Lewis has blogged about his book <a href="http://harry-lewis.blogspot.com/2021/07/book-reviews-and-talk-i-gave-in-hong.html">here</a> where he points to my review which is in SIGACT News. </p><p>OR you an use my link to my review <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/greatideas.pdf">here</a>. </p><p>The list of 46 papers had some constraints, so if you wonder <i>why isn't X ther</i>e it might have hit one of those constraints.</p><p>1) No paper past 1980 (he had to stop somewhere).</p><p>2) He preferred short readable papers to long or unreadable ones (don't we all!). Before thinking `Gee why isn't paper X in the book' go read paper X. </p><p>3) Some papers cost to much to get permission to reprint. My review points to one such paper that I found 5 links to on the web. </p><p>4) We don't need X papers on topic Y.</p><p>Of more interest is some papers that you had not heard of but we can now see are important.</p><p>For more thought, read my review!</p><p>For even more information, buy the book!</p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-08-16T01:10:00Z</updated>
    <published>2021-08-16T01:10:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-20T22:13:02Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/08/15/linkage</id>
    <link href="https://11011110.github.io/blog/2021/08/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Trying to watch Olympics replays on Roku / NBC Sports is an exercise in frustration (\(\mathbb{M}\)):</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p>Trying to watch Olympics replays on Roku / NBC Sports is an exercise in frustration (<a href="https://mathstodon.xyz/@11011110/106684830455526311">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>Up to 14 unskippable ads in a row</li>
      <li>Trying to fast-foward over breaks in sports action gets into a broken mode showing the same ads over and over while the underlying fast-foward goes on with disabled controls</li>
      <li>Rewinding to content you missed while uncontrollably fast-fowarding gets back into ad overload mode.</li>
    </ul>

    <p>Who designed this unusable app and why do they think this will bring me back for more?</p>
  </li>
  <li>
    <p>Rankings in all-play-all competitions (like group play stages of many Olympic games) typically use total numbers of points for wins. It’s simple, so audiences understand it. But really you might want to do something more complicated, like finding a ranking having the minimum number of upsets.  The good news is that <a href="https://cse.buffalo.edu/faculty/atri/papers/algos/fas-soda.pdf">point score approximates the minimum-upset ranking</a> (<a href="https://mathstodon.xyz/@11011110/106691070346474540">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>Topology is witchcraft (<a href="https://lainchan.gay/objects/0f77d3e4-aa38-428e-aa4a-250574c66dda">\(\mathbb{M}\)</a>): link goes to animated gif of surprising disentanglements, made possible because topologically things were never tangled to begin with.</p>
  </li>
  <li>
    <p><a href="https://www.bloomberg.com/news/articles/2021-08-03/facebook-disables-accounts-tied-to-nyu-research-project">Facebook shuts down the personal accounts of university researchers studying how Facebook violates its users’ privacy in targeting political ads</a> (<a href="https://mathstodon.xyz/@11011110/106700052721454124">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=28066214">via</a>), citing as an excuse to do so…its agreements with the FTC over its violations of user privacy. Mozilla weighs in on <a href="https://blog.mozilla.org/en/mozilla/news/why-facebooks-claims-about-the-ad-observer-are-wrong/">why Facebook’s side of the story doesn’t hold water</a>.</p>
  </li>
  <li>
    <p><a href="https://muboard.net/">Muboard</a> (<a href="https://mathstodon.xyz/@11011110/106713423407783688">\(\mathbb{M}\)</a>, <a href="https://lobste.rs/s/mqfkap/mathematics_chalkboard_with_mathjax">via</a>). There are any number of demo sites where you can type LaTeX math and get mathjax to format it for you in your web browser window, but this one looks like a good choice for sharing your screen when conferencing, office hours, etc: markdown for the non-math formatting, set up to look like a blackboard, with no unnecessary distractions, free to copy or modify.</p>
  </li>
  <li>
    <p>The truncated octahedron can tile space, forming the <a href="https://en.wikipedia.org/wiki/Bitruncated_cubic_honeycomb">bitruncated cubic honeycomb</a> (<a href="https://mathstodon.xyz/@11011110/106717788120113051">\(\mathbb{M}\)</a>). One of my neighborhood playgrounds has a climbing structure in this pattern, so kids can learn some geometry as they play. It wasn’t easy to find views without distracting background houses, but here’s one straight up from below its center.</p>

    <p style="text-align: center;"><img alt="Bitruncated cubic honeycomb play structure" src="https://www.ics.uci.edu/~eppstein/pix/bchps/1-m.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>

    <p>I used this set to test out Lightroom instead of Photoshop for processing photos, with settings on auto. <a href="https://www.ics.uci.edu/~eppstein/pix/bchps/">A few more shots</a>. See discussion for another tessellation-based play structure and its grown-up architectural relative, the <a href="https://en.wikipedia.org/wiki/Nakagin_Capsule_Tower">Nakagin Capsule Tower</a>.</p>
  </li>
  <li>
    <p><a href="https://www.freemangallerysantafe.com/art/guardian-by-susan-latham">Guardian</a>, by sculptor <a href="https://susanlatham.net/">Susan Latham</a> (<a href="https://mathstodon.xyz/@11011110/106724752509935406">\(\mathbb{M}\)</a>). Its strange crescent moon or fortune cookie shapes are what you get from two overlapping circles intersecting in a <a href="https://en.wikipedia.org/wiki/Vesica_piscis">vesica piscis</a>, by folding  the center arcs to make the two outer arcs meet. See also <a href="https://www.youtube.com/watch?v=H6UeMmWx6i0">a short animation of this folding process</a> and <a href="http://archive.bridgesmathart.org/2018/bridges2018-535.html">mathematical analysis by Klara Mundilova and Tony Wills</a>.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.06751">‘Tortured phrases’ give away fabricated research papers</a> (<a href="https://mathstodon.xyz/@11011110/106727203820504644">\(\mathbb{M}\)</a>, <a href="https://boingboing.net/2021/08/09/tortured-phrases-used-to-fool-plagiarism-detectors-now-infest-scientific-papers.html">via</a>, <a href="https://www.nature.com/articles/d41586-021-02134-0">via2</a>, <a href="https://retractionwatch.com/2021/07/12/elsevier-says-integrity-and-rigor-of-peer-review-for-400-papers-fell-beneath-the-high-standards-expected/">see also</a>). The gist of the story appears to be: paper mills are using automated synonym replacement to hide plagiarism, producing odd wording (“irregular timberland” for “random forest”, in the math/CS sense of forest), and bypassing normal editorial processes in hijacked established journals, particularly through topical special issues.</p>
  </li>
  <li>
    <p><a href="http://jdh.hamkins.org/the-lattice-of-sets-of-natural-numbers-is-rich/">Joel David Hamkins tries to visualize the power set of the naturals</a> (<a href="https://mathstodon.xyz/@11011110/106741533744552581">\(\mathbb{M}\)</a>), showing also that it contains all countable partial orders.</p>
  </li>
  <li>
    <p>Given recent news breathlessly hyping the “discovery” that the people of Sippar, in the First Babylonian Dynasty, knew about and used the Pythagorean theorem, based on the existence of rectangles in a land survey, I think maybe it’s relevant to point to Eleanor Robson’s review on the Pythagorean theorem in First Dynasty Sippar, citing work on the topic back to 1916. It’s “<a href="https://www.jstor.org/stable/1359891">Three Old Babylonian Methods for Dealing with Pythagorean Triangles</a>”, <em>J. Cuneiform Studies</em>, 1997 (<a href="https://mathstodon.xyz/@11011110/106747623214954539">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p>The WADS (Algorithms and Data Structures Symposium) and CCCG (Canadian Conference on Computational Geometry) conferences were online this week (<a href="https://mathstodon.xyz/@11011110/106750493101878056">\(\mathbb{M}\)</a>). The talks were mostly live on zoom rather than prerecorded, and although recordings exist temporarily, they’re only available to participants. The proceedings are online and public, though. The <a href="https://doi.org/10.1007/978-3-030-83508-8">WADS proceedings</a> through Springer LNCS is paywalled but the <a href="https://projects.cs.dal.ca/cccg2021/wordpress/wp-content/uploads/2021/08/CCCG2021.pdf">CCCG proceedings</a> is free.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Steinitz%27s_theorem">Steinitz’s theorem</a> (<a href="https://mathstodon.xyz/@11011110/106755883632277176">\(\mathbb{M}\)</a>), showing that the graphs of convex polyhedra can be described in a purely combinatorial way using only planarity and connectivity, and (through its proofs) describing how to turn a graph into a polyhedron. Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p>Etienne Cliquet makes kinetic artworks by cutting and folding sheets of paper, floating them on water, and <a href="https://thekidshouldseethis.com/post/flottille-etienne-cliquet-video">filming them as the water causes them to unfold</a> (<a href="https://mathstodon.xyz/@11011110/106762367791482461">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/192329/Floating-Origami">via</a>).</p>
  </li>
</ul></div>
    </content>
    <updated>2021-08-15T17:39:00Z</updated>
    <published>2021-08-15T17:39:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-08-18T23:52:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2762</id>
    <link href="https://theorydish.blog/2021/08/13/random-2021-starts-on-monday/" rel="alternate" type="text/html"/>
    <title>RANDOM 2021 Starts on Monday</title>
    <summary>Call for Participation: APPROX/RANDOM 2021 The 25th International Workshop on Randomization and Computation (RANDOM 2021) and the 24th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2021) will be starting on Monday August 16!  The conferences will be held as parallel virtual conferences, August 16-18, 2021. RANDOM 2021 focuses on applications of randomness to computational and combinatorial problems while APPROX 2021 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems. To learn more about the conferences and program, visit: APPROX: https://approxconference.wordpress.com/approx-2021/RANDOM: https://randomconference.com/random-2021-home/ In addition to an exciting program of live talks and discussion (to complement pre-recorded talks), the conference will feature two invited talks, by Jelani Nelson (UC Berkeley) and Vera Traub (ETH Zurich), as well as a social event with trivia and a cartoon caption contest! Registration is only $10 for general audience members.  You can register here: https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience Hope to see you at the conference!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Call for Participation: APPROX/RANDOM 2021</p>



<p>The 25th International Workshop on Randomization and Computation (RANDOM 2021) and the 24th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX 2021) will be starting on Monday August 16!  The conferences will be held as parallel virtual conferences, August 16-18, 2021. RANDOM 2021 focuses on applications of randomness to computational and combinatorial problems while APPROX 2021 focuses on algorithmic and complexity theoretic issues relevant to the development of efficient approximate solutions to computationally difficult problems.</p>



<p>To learn more about the conferences and program, visit:</p>



<p>APPROX: <a href="https://approxconference.wordpress.com/approx-2021/" rel="noreferrer noopener" target="_blank">https://approxconference.wordpress.com/approx-2021/</a><br/>RANDOM: <a href="https://randomconference.com/random-2021-home/" rel="noreferrer noopener" target="_blank">https://randomconference.com/random-2021-home/</a></p>



<p>In addition to an exciting program of live talks and discussion (to complement pre-recorded talks), the conference will feature two invited talks, by Jelani Nelson (UC Berkeley) and Vera Traub (ETH Zurich), as well as a social event with trivia and a cartoon caption contest!</p>



<p>Registration is only $10 for general audience members.  You can register here: <a href="https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience" rel="noreferrer noopener" target="_blank">https://www.eventbrite.com/e/approx-2021-and-random-2021-tickets-162840998811?discount=audience</a></p>



<p>Hope to see you at the conference!</p></div>
    </content>
    <updated>2021-08-13T23:18:23Z</updated>
    <published>2021-08-13T23:18:23Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-08-20T23:38:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5730</id>
    <link href="https://www.scottaaronson.com/blog/?p=5730" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5730#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5730" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Stephen Wiesner (1942-2021)</title>
    <summary xml:lang="en-US">These have not been an auspicious few weeks for Jewish-American-born theoretical physicists named Steve who made epochal contributions to human knowledge in the late 1960s, and who I had the privilege to get to know a bit when they were old. This morning, my friend and colleague Or Sattath brought me the terrible news that […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p/>



<p/>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" src="https://www.scottaaronson.com/wiesner-sm.jpg"/>Photo credit: Lev Vaidman</figure></div>



<p>These have <a href="https://www.scottaaronson.com/blog/?p=5566">not</a> been an auspicious few weeks for Jewish-American-born theoretical physicists named Steve who made epochal contributions to human knowledge in the late 1960s, and who I had the privilege to get to know a bit when they were old.</p>



<p>This morning, my friend and colleague <a href="https://orsattath.wordpress.com/about/">Or Sattath</a> brought me the terrible news that <a href="https://en.wikipedia.org/wiki/Stephen_Wiesner">Stephen Wiesner</a> has passed away in Israel.  [Because people have asked: I’ve now also heard directly from Wiesner’s daughter Sarah.]</p>



<p>Decades ago, Wiesner left academia, embraced Orthodox Judaism, moved from the US to Israel, and took up work there as a construction laborer—believing (or so he told me) that manual labor was good for the soul.  In the late 1960s, however, Wiesner was still a graduate student in physics at Columbia University, when he wrote <a href="http://users.cms.caltech.edu/~vidick/teaching/120_qcrypto/wiesner.pdf">Conjugate Coding</a>: arguably the foundational document of the entire field of quantum information science.  <s>Famously, this paper was so far ahead of its time that it was rejected over and over from journals, taking nearly 15 years to get published.</s>  (Fascinatingly, Gilles Brassard tells me that this isn’t true: it was rejected <em>once</em>, from <em>IEEE Transactions on Information Theory</em>, and then Wiesner simply shelved it.)  When it finally appeared, in 1983, it was in <em><a href="https://dl.acm.org/newsletter/sigact">SIGACT News</a></em>—a venue that I know and love, where I’ve published too, but that’s more like the house newsletter for theoretical computer scientists than an academic journal.</p>



<p>But it didn’t matter.  By the early 1980s, Wiesner’s ideas had been successfully communicated to <a href="https://en.wikipedia.org/wiki/Charles_H._Bennett_(physicist)">Charlie Bennett</a> and <a href="https://en.wikipedia.org/wiki/Gilles_Brassard">Gilles Brassard</a>, who refashioned them into the first scheme for <a href="https://en.wikipedia.org/wiki/Quantum_key_distribution">quantum key distribution</a>—what we now call <a href="https://en.wikipedia.org/wiki/BB84">BB84</a>.  Even as Bennett and Brassard received scientific acclaim for the invention of quantum cryptography—including, a few years ago, the <a href="https://en.wikipedia.org/wiki/Wolf_Prize">Wolf Prize</a> (often considered second only to the Nobel Prize), at a ceremony in the Knesset in Jerusalem that I attended—the two B’s were always careful to acknowledge their massive intellectual debt to Steve Wiesner.</p>



<hr class="wp-block-separator"/>



<p>Let me explain what Wiesner does in the Conjugate Coding paper.  As far as I know, this is the first paper ever to propose that quantum information—what Wiesner called “polarized light” or “spin-1/2 particles” but we now simply call <a href="https://en.wikipedia.org/wiki/Qubit">qubits</a>—works differently than classical bits, in ways that could <em>actually be useful</em> for achieving cryptographic tasks that are impossible in a classical world.  What could enable these cryptographic applications, wrote Wiesner, is the fact that there’s no physical means for an attacker or eavesdropper to <em>copy</em> an unknown qubit, to produce a second qubit in the same quantum state.  This observation—now called the <a href="https://en.wikipedia.org/wiki/No-cloning_theorem">No-Cloning Theorem</a>—would only be named and published in 1982, but Wiesner treats it in his late-1960s manuscript as just obvious background.</p>



<p>Wiesner went further than these general ideas, though, to propose an explicit scheme for <a href="https://en.wikipedia.org/wiki/Quantum_money">quantum money</a> that would be physically impossible to counterfeit—a scheme that’s still of enormous interest half a century later (I teach it every year in my <a href="https://www.scottaaronson.com/qclec.pdf">undergraduate course</a>).  In what we now call the Wiesner money scheme, a central bank prints “quantum bills,” each of which contains a classical serial number as well as a long string of qubits.  Each qubit is prepared in one of four possible quantum states:</p>



<ul><li>|0⟩,</li><li>|1⟩,</li><li>|+⟩ = (|0⟩+|1⟩)/√2, or</li><li>|-⟩ = (|0⟩-|1⟩)/√2.</li></ul>



<p>The bank, in a central database, stores the serial number of every bill in circulation, as well as the preparation instructions for each of the bill’s qubits.  If you want to <em>verify</em> a bill as genuine—this, as Wiesner knew, is the big drawback—you have to bring it back to the bank.  The bank, using its secret knowledge of how each qubit was prepared, measures each qubit in the appropriate basis—the {<span style="font-size: revert; color: initial;">|0⟩,|1⟩</span>} basis for <span style="font-size: revert; color: initial;">|0⟩ or |1⟩</span> qubits, the {<span style="font-size: revert; color: initial;">|+⟩,|-⟩</span>} basis for <span style="font-size: revert; color: initial;">|+⟩ or |-⟩</span> qubits—and checks that it gets the expected outcomes.  If even one qubit yields the wrong outcome, the bill is rejected as counterfeit.</p>



<p>Now consider the situation of a counterfeiter, who holds a quantum bill but lacks access to the bank’s secret database.  When the counterfeiter tries to copy the bill, they won’t know the right basis in which to measure each qubit—and if they make the wrong choice, then it’s not only that they fail to make a copy; it’s that the measurement destroys even the <em>original</em> copy!  For example, measuring a <span style="font-size: revert; color: initial;">|+⟩ or |-⟩</span> qubit in the {<span style="font-size: revert; color: initial;">|0⟩,|1⟩</span>} basis will randomly collapse the qubit to either <span style="font-size: revert; color: initial;">|0⟩ or |1⟩</span>—so that, when the bank later measures the same qubit in the correct {<span style="font-size: revert; color: initial;">|+⟩,|-⟩</span>} basis, it will see the wrong outcome, and realize that the bill has been compromised, with 1/2 probability (with the probability increasing to nearly 1 as we repeat across hundreds or thousands of qubits).</p>



<p>Admittedly, the handwavy argument above, which Wiesner offered, is far from a security proof by cryptographers’ standards.  In 2011, I <a href="https://cstheory.stackexchange.com/questions/11363/rigorous-security-proof-for-wiesners-quantum-money">pointed that out on StackExchange</a>.  My post, I’m happy to say, spurred Molina, Vidick, and Watrous to write a <a href="https://arxiv.org/abs/1202.4010">beautiful 2012 paper</a>, where they rigorously proved for the first time that in Wiesner’s money scheme, no counterfeiter consistent with the laws of quantum mechanics can turn a single n-qubit bill into two bills that both pass the bank’s verification with success probability greater than (3/4)<sup>n</sup> (and this is tight).  But the intuition was already clear enough to Wiesner in the 1960s.</p>



<p>In 2003—when I was already a PhD student in quantum information, but incredibly, had never heard of Stephen Wiesner or his role in founding my field—I rediscovered the idea of quantum states |ψ⟩ that you could store, measure, and feed into a quantum computer, but that would be <em>usefully uncopyable</em>.  (My main interest was in whether you could create “unpiratable quantum software programs.”)  Only in 2006, at the University of Waterloo, did Michele Mosca and his students make the connection for me to quantum money, Stephen Wiesner, and his Conjugate Coding paper, which I then read with amazement—along with a <a href="https://static.aminer.org/pdf/PDF/000/120/546/quantum_cryptography_or_unforgeable_subway_tokens.pdf">comparably amazing followup work</a> by Bennett, Brassard, Breidbart, and Wiesner.</p>



<p>But it was clear that there was still a great deal to do.  Besides unpiratable software, Wiesner and his collaborators had lacked the tools in the early 1980s seriously to tackle the problem of secure quantum money that <em>anybody</em> could verify, not only the bank that had created the money.  I realized that, if such a thing was possible at all, then just like unpiratable software, it would require cryptographic hardness assumptions, a restriction to polynomial-time counterfeiters, and (hence) ideas from quantum computational complexity.  The No-Cloning Theorem couldn’t do the job on its own.</p>



<p>That realization led to my 2009 paper <a href="https://arxiv.org/abs/1110.5353">Quantum Copy-Protection and Quantum Money</a>, and from there, to the “modern renaissance” of Wiesner’s old idea of quantum money, with well over a hundred papers (e.g., <a href="https://arxiv.org/abs/1203.4740">my 2012 paper with Paul Christiano</a>, Farhi et al.’s <a href="https://arxiv.org/abs/0912.3823">quantum state restoration paper</a>, their <a href="https://arxiv.org/abs/1004.5127">quantum money from knots paper</a>, Mark Zhandry’s 2017 <a href="https://arxiv.org/abs/1711.02276">quantum lightning paper</a>, Dmitry Gavinsky’s <a href="https://arxiv.org/abs/1109.0372">improvement of Wiesner’s scheme</a> wherein the money is verified by classical communication with the bank, Broduch et al.’s <a href="https://arxiv.org/abs/1404.1507">adaptive attack</a> on Wiesner’s original scheme, my <a href="https://arxiv.org/abs/1711.01053">shadow tomography paper</a> proving the necessity for the bank to keep a giant database in information-theoretic quantum money schemes like Wiesner’s, Daniel Kane’s <a href="https://arxiv.org/abs/1809.05925">strange scheme based on modular forms</a>…).  The purpose of many of these papers was either to break the quantum money schemes proposed in previous papers, or to patch the schemes that were previously broken.</p>



<p>After all this back-and-forth, spanning more than a decade, I’d say that Wiesner’s old idea of quantum money is now in good enough theoretical shape that the main obstacle to its practical realization is merely the “engineering difficulty”—namely, how to get the qubits in a bill, sitting in your pocket or whatever, to maintain their quantum coherence for more than a few nanoseconds!  (Or possibly a few hours, if you’re willing to schlep a cryogenic freezer everywhere you go.)  It’s precisely because quantum key distribution doesn’t have this storage problem—because there the qubits are simply sent across a channel and then immediately measured on arrival—that QKD is actually practical today, although the market for it has proven to be extremely limited so far.</p>



<p>In the meantime, while the world waits for the quantum error-correction that could keep qubits alive indefinitely, there’s Bitcoin.  The latter perversely illustrates just how immense the demand for quantum money might someday be: the staggering lengths to which people will go, diverting the electricity to power whole nations into mining rigs, to get around our current inability to realize Wiesner’s elegant quantum-mechanical solution to the same problem.  When I first learned about Bitcoin, shortly after its invention, it was in the context of: “here’s something I’d better bring up in my lectures on quantum money, in order to explain how much better WiesnerCoin could eventually be, when it’s the year 2200 or whatever and we all have quantum computers wired up by a quantum Internet!”  It never occurred to me that I should forget about the year 2200, liquidate my life savings, and immediately buy up all the Bitcoin I could.  [Added: I’ve since learned that Wiesner’s daughter Sarah is a professional in the Bitcoin space.]</p>



<hr class="wp-block-separator"/>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" src="https://www.scottaaronson.com/wiesnaar-sm.jpg"/>Photo credit: Or Sattath</figure></div>



<p>In his decades as a construction laborer, Wiesner had (as far as I know) no Internet presence; many of my colleagues didn’t even realize he was still alive.  Even then, though, Wiesner never turned his back <em>so</em> far on his previous life, his academic life, that the quantum information faculty at Hebrew University in Jerusalem couldn’t entice him to participate in some seminars there.  Those seminars are where I had the privilege to meet and talk to him several times over the last decade.  He was thoughtful and kind, listening with interest as I told him how I and others were trying to take quantum money into the modern era by making it publicly verifiable.</p>



<p>I also vividly remember a conversation in 2013 where Steve shared his fears about the American physics establishment and military-industrial complex, and passionately urged me to</p>



<ol><li>quit academia and get a “real job,” and</li><li>flee the US immediately and move my family to Israel, because of a wave of fascism and antisemitism that was about to sweep the US, just like with Germany in the 1930s.</li></ol>



<p>I politely nodded along, pointing out that my Israeli wife and I had considered living in Israel but the job opportunities were better in US, silently wondering when Steve had gone <em>completely</em> off his rocker.  Today, Steve’s urgent warning about an impending fascist takeover of the US seems … uh, <em>slightly</em> less crazy than in 2013?  Maybe, just like with quantum money, Wiesner was simply too far ahead of his time to sound sane.</p>



<p>Wiesner also talked to me about his father, <a href="https://en.wikipedia.org/wiki/Jerome_Wiesner">Jerome Wiesner</a>, who was a legendary president of MIT—still spoken about in reverent tones when I taught there—as well as the chief science advisor to John F. Kennedy.  One of JFK’s most famous decisions was to override the elder Wiesner’s fervent opposition to sending humans to the moon (Wiesner thought it a waste of money, as robots could do the same science for vastly cheaper).</p>



<p>While I don’t know all the details (I hope someone someday researches it and writes a book), Steve Wiesner made it clear to me that he did not get along with his famous father <em>at all</em>—in fact they became estranged.  Steve told me that his embrace of Orthodox Judaism was, at least in part, a reaction against everything his father had stood for, including militant scientific atheism.  I suppose that in the 1960s, millions of young Americans defied their parents via sex, drugs, and acoustic guitar; only a tiny number did so by donning <a href="https://en.wikipedia.org/wiki/Tzitzit">tzitzit</a> and moving to Israel to pray and toil with their hands.  The two groups of rebels did, however, share a tendency to grow long beards.</p>



<p>Wiesner’s unique, remarkable, <em>uncloneable</em> life trajectory raises the question: who are the young Stephen Wiesners of our time?  Will we be faster to recognize their foresight than Wiesner’s contemporaries were to recognize his?</p>



<hr class="wp-block-separator"/>



<p>Feel free to share any other memories of Stephen Wiesner or his influence in the comments.</p>



<hr class="wp-block-separator"/>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Aug. 14):</span></strong> See also <a href="https://orsattath.wordpress.com/2021/08/14/stephen-wiesner/">Or Sattath’s memorial post</a>, which (among other things) points out something that my narrative missed: namely, besides quantum money, Wiesner <em>also</em> invented <a href="https://en.wikipedia.org/wiki/Superdense_coding">superdense coding</a> in 1970, although he and Bennett only published the idea 22 years later (!).</p>



<p>And I have more photos!  Here’s <a href="https://www.scottaaronson.com/wiesner2.jpg">Wiesner with an invention of his</a> and <a href="https://www.scottaaronson.com/wiesner3.jpg">another photo</a> (thanks to his daughter Sarah).  Here’s <a href="https://www.scottaaronson.com/wiesner1970.jpg">another photo from 1970</a> and <a href="https://www.scottaaronson.com/wiesnernotes1970.jpg">Charlie Bennett’s handwritten notes</a> (!) after first meeting Wiesner in 1970 (thanks to Charlie Bennett).</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Another Update:</span></strong> Stephen’s daughter Sarah gave me the following fascinating information to share.</p>



<blockquote class="wp-block-quote"><p>In the 70’s he lived in California where he worked in various Silicon Valley startups while also working weekends as part of a produce (fruits and vegetables) distribution co-op. During this time he became devoted to the ideas of solar energy, clean energy and space migration and exploration. He also became interested in Judaism. He truly wanted to help and make our world more peaceful and safe with his focus being on clean energy and branching out into space. He also believed that instead of fighting over the temple mount in Jerusalem, the Third Temple should be built in outer-space or in a structure above the original spot, an idea he tried to promote to prevent wars over land.</p></blockquote></div>
    </content>
    <updated>2021-08-13T20:54:29Z</updated>
    <published>2021-08-13T20:54:29Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-08-20T03:09:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/119</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/119" rel="alternate" type="text/html"/>
    <title>TR21-119 |  Visible Rank and Codes with Locality | 

	Omar Alrabiah, 

	Venkatesan Guruswami</title>
    <summary>We propose a framework to study the effect of local recovery requirements of codeword symbols on the dimension of linear codes, based on a combinatorial proxy that we call "visible rank." The locality constraints of a linear code are stipulated by a matrix $H$ of $\star$'s and $0$'s (which we call a "stencil"), whose rows correspond to the local parity checks (with the $\star$'s indicating the support of the check). The visible rank of $H$ is the largest $r$ for which there is a $r \times r$ submatrix in $H$ with a unique generalized diagonal of $\star$'s. The visible rank yields a field-independent combinatorial lower bound on the rank of $H$ and thus the co-dimension of the code. 

We point out connections of the visible rank to other notions in the literature such as unique restricted graph matchings, matroids, spanoids, and min-rank. In particular, we prove a rank-nullity type theorem relating visible rank to the rank of an associated construct called symmetric spanoid, which was introduced by Dvir, Gopi, Gu, and Wigderson [DGWW]. Using this connection and a construction of appropriate stencils, we answer a question posed in [DGGW] and demonstrate that symmetric spanoid rank cannot improve the currently best known $\widetilde{O}(n^{(q-2)/(q-1)})$ upper bound on the dimension of $q$-query locally correctable codes (LCCs) of length $n$. This also pins down the efficacy of visible rank as a proxy for the dimension of LCCs. 

We also study the $t$-Disjoint Repair Group Property ($t$-DRGP) of codes where each codeword symbol must belong to $t$ disjoint check equations. It is known that  linear codes with $2$-DRGP must have co-dimension $\Omega(\sqrt{n})$ (which is matched by a simple product code construction). We show that there are stencils corresponding to $2$-DRGP with visible rank as small as $O(\log n)$. However, we show the second tensor of any $2$-DRGP stencil has visible rank $\Omega(n)$, thus recovering the $\Omega(\sqrt{n})$ lower bound for $2$-DRGP. For $q$-LCC, however, the $k$'th tensor power for $k\le n^{o(1)}$ is unable to improve the $\widetilde{O}(n^{(q-2)/(q-1)})$ upper bound on the dimension of $q$-LCCs by a polynomial factor. Inspired by this and as a notion of intrinsic interest, we define the notion of visible capacity of a stencil as the limiting visible rank of high tensor powers, analogous to Shannon capacity, and pose the question whether there can be large gaps between visible capacity and algebraic rank.</summary>
    <updated>2021-08-13T14:04:34Z</updated>
    <published>2021-08-13T14:04:34Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-20T23:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/118</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/118" rel="alternate" type="text/html"/>
    <title>TR21-118 |  Efficient multivariate low-degree tests via interactive oracle proofs of proximity for polynomial codes | 

	Sarah Bordage, 

	Daniel Augot, 

	Jade Nardi</title>
    <summary>We consider the proximity testing problem for error-correcting codes which consist in evaluations of multivariate polynomials either of bounded individual degree or bounded total degree. Namely, given an
oracle function $f : L^m \rightarrow \mathbb F_q$, where $L\subset \mathbb F_q$, a verifier     distinguishes whether $f$ is the evaluation of a low-degree polynomial or is far (in relative Hamming distance) from being one, by making only a few queries to $f$.  This topic has been studied in the context of locally testable codes, interactive proofs, probabilistically checkable proofs, and interactive oracle proofs.
We present the first interactive oracle proofs of proximity (IOPP) for tensor products of Reed-Solomon codes (evaluation of polynomials with bounds on individual degrees) and for Reed-Muller codes (evaluation of polynomials with a bound on the total
degree).
        
Such low-degree polynomials play a central role in constructions of probabilistic proof systems and succinct non-interactive arguments of knowledge with zero-knowledge. For these applications, highly-efficient multivariate low-degree tests are
desired, but prior probabilistic proofs of proximity required super-linear proving time. In contrast, for multivariate codes of length $N$, our constructions admit a prover running in time linear in $N$ and a verifier which is logarithmic in $N$.
        
      For fixed constant  number of variables $m$, the efficiency parameters of our IOPPs for multivariate codes compare well, all things equal,  with those of the IOPP for Reed-Solomon codes of [Ben-Sasson et al., ICALP 2018] from which they are directly inspired.</summary>
    <updated>2021-08-13T13:52:54Z</updated>
    <published>2021-08-13T13:52:54Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-20T23:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/117</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/117" rel="alternate" type="text/html"/>
    <title>TR21-117 |  Simplicity Meets Near-Optimal Rate: Non-malleable Codes and Non-malleable Two-source Extractors via Rate Boosters | 

	Divesh Aggarwal, 

	Bhavana Kanukurthi, 

	SaiLakshmiBhavana Obbattu, 

	Maciej Obremski, 

	Sruthi Sekar</title>
    <summary>At ITCS 2010, Dziembowski, Pietrzak, and Wichs introduced Non-malleable Codes (NMCs). Non-malleability is one of the strongest and most challenging notions of security considered in cryptography and protects against tampering attacks. In the context of coding schemes, non-malleability requires that it be infeasible to tamper the codeword of a message into the codeword of a related message. A natural and well-studied model of tampering is the $2$-split-state model where the codeword consists of two independently tamperable states. As with standard error-correcting codes, it is of great importance to build codes with high rates. Cheraghchi and Guruswami (ITCS 2014) showed that one cannot obtain NMCs in the $2$-split state model with a rate better than $1/2$.  Since its inception, this area has witnessed huge strides leading to the construction of a constant-rate NMC in the $2$-split state model due to Aggarwal and Obremski (FOCS 2020). However, the rate of this construction -- roughly $1/1,500,000$ -- is nowhere close to the best achievable rate of $1/2$! In this work, we dramatically improve this status quo by building a rate booster that converts any augmented non-malleable code into an augmented non-malleable code with a rate of $1/3$. Using similar, but simpler techniques we also obtain rate boosters that convert any unbalanced (with sources of unequal length) non-malleable $2$-source extractor into an unbalanced non-malleable $2$-source extractor with rate $1/2$.
  
The beauty of our construction lies in its simplicity. In particular, if we apply our rate booster to the non-malleable code construction by Aggarwal, Dodis, and Lovett (STOC 2014), then all we need is one instance of the inner-product extractor, one instance of a seeded extractor, and an affine-evasive function for the construction to work.

Further, as an application of our $1/3$-rate augmented NMC (which we also prove to be leakage resilient), we give an extremely simple computational binding and statistical hiding non-malleable commitment scheme using only standard assumptions, and with a communication cost of $41$ times the length of the message to be committed, which is optimal up to a constant factor.</summary>
    <updated>2021-08-13T13:49:56Z</updated>
    <published>2021-08-13T13:49:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-20T23:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/08/13/postdoc-at-unsw-sydney-apply-by-september-13-2021/</id>
    <link href="https://cstheory-jobs.org/2021/08/13/postdoc-at-unsw-sydney-apply-by-september-13-2021/" rel="alternate" type="text/html"/>
    <title>postdoc at UNSW Sydney (apply by September 13, 2021)</title>
    <summary>A 2-year postdoc position is available at UNSW Sydney. The position is part of a new research project on “Improved algorithms via random sampling” (Serge Gaspers, Fedor Fomin, Daniel Lokshtanov) funded by the Australian Research Council. It revolves around the design and analysis of parameterized, moderately exponential, randomized, and approximation algorithms for graph problems. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A 2-year postdoc position is available at UNSW Sydney.</p>
<p>The position is part of a new research project on “Improved algorithms via random sampling” (Serge Gaspers, Fedor Fomin, Daniel Lokshtanov) funded by the Australian Research Council. It revolves around the design and analysis of parameterized, moderately exponential, randomized, and approximation algorithms for graph problems.</p>
<p>Website: <a href="https://www.cse.unsw.edu.au/~sergeg/contact.html#postdocs">https://www.cse.unsw.edu.au/~sergeg/contact.html#postdocs</a><br/>
Email: serge.gaspers@unsw.edu.au</p></div>
    </content>
    <updated>2021-08-13T05:13:27Z</updated>
    <published>2021-08-13T05:13:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-08-20T23:37:46Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2021/08/13/relative-risk/</id>
    <link href="http://benjamin-recht.github.io/2021/08/13/relative-risk/" rel="alternate" type="text/html"/>
    <title>Relative risk is more informative than effectiveness.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The past few weeks have sent a tremendous amount of disappointing news about the Delta variant and the possible waning effectiveness of vaccines. <a href="https://www.washingtonpost.com/health/2021/07/30/provincetown-covid-outbreak-vaccinated/">An outbreak among partygoers in Provincetown found a high number of infected individuals who had already been fully vaccinated.</a> <a href="https://nymag.com/intelligencer/2021/08/breakthrough-covid-19-cases-may-be-a-bigger-problem.html">News stories and anecdotes about “breakthrough infections” abound.</a> <a href="https://apnews.com/article/health-coronavirus-pandemic-79959d313428d98ab8aa905bbe287ba0">And the CDC has confusingly made an about face on its recommendations about face coverings for the vaccinated.</a> This has fed a growing anxiety that the vaccines have not delivered what was promised.</p>

<p>The Pfizer and Moderna vaccine, <a href="https://www.pfizer.com/news/press-release/press-release-detail/pfizer-and-biontech-conclude-phase-3-study-covid-19-vaccine">heralded as “95% effective”</a>, led many to hope that they could eradicate this coronavirus. However, and unfortunately, from the beginning, there has been a lack of clarity among not only the media but among health professionals themselves around what “effectiveness” means. Indeed, <a href="https://www.npr.org/sections/goatsandsoda/2021/08/11/1026190062/covid-delta-variant-transmission-cdc-chickenpox">misunderstandings of vaccine effectiveness persist in media reporting on the pandemic</a>.</p>

<p>Pfizer ran a randomized control trial where they took a diverse pool of sixty thousand people and randomly gave half of the people the vaccine and half a placebo injection with no effect at all. Effectiveness measures the percent reduction of infection in the vaccinated group when compared against the control group. A vaccine effectiveness of 95% means that Pfizer observed 95% fewer infections amongst the vaccinated than the unvaccinated.</p>

<p>An identical way of stating the result of the trial is that 20 times as many people who received the placebo developed symptomatic COVID-19 as those that received the vaccine. In the language of risk, we would say that the vaccinated group had 20 times less risk of symptomatic COVID-19 infection than the control group.</p>

<p>While “95% effective” is numerically the same as “20-fold risk reduction,” it leads to more confusion. It is far too common for “95% effective” to be incorrectly interpreted as a 5% chance of getting infected at all, which is not the case. If the message had been that vaccines reduce risk by a factor of 20, then we would be equipped to both better understand the power of these vaccines and better plan for moving into an open, but increasingly vaccinated world.</p>

<p>20-fold risk reduction allows one to consider their everyday experiences and make judgement calls. Hanging out with friends outside was already very low risk. If everyone is vaccinated, the risk after vaccination becomes effectively zero. On the other hand, going to a packed, sweaty dance club and sharing drinks with strangers was very high risk before vaccination, so vaccination makes it 20 times less risky but still risky.</p>

<p>The Delta variant itself is more infectious than previous strains and can partially evade immunity. This further reduces vaccine effectiveness. But, like with the clubbing example, behavioral changes also change effectiveness. The virus is not the same today as it was in the Fall of 2020, but we are also not the same people.</p>

<p>Clinical studies are done in the best of circumstances, and most medical professionals expect the effectiveness to decrease in the general population. Scientists do their best to ensure that the population of individuals in a study are representative of all of the people in the world. They perform complex matching and outreach to ensure characteristic diversity in the pool of subjects. But one of the hardest factors to control for is the psychological and behavioral changes in the broader population over time.</p>

<p>In particular, people are taking more risks now than they did in 2020, and the risk of infection has increased for all, vaccinated or not. Just imagine you get vaccinated and then increase your risk by a factor of four. For example, you stop staying at home on Zoom all day, and go back to the office, bars, and restaurants. You start meeting with all sorts of people you don’t know very well. Then your risk of infection is now a fifth, not a twentieth of what it was. And further imagine that a new variant comes along that is twice as infectious as the old one. Now your risk reduction is down to 2.5, which would amount to a total “effectiveness”—combining the vaccine, your behavior, and the variant—of only 60%. But 60% is still better than most seasonal flu shots.</p>

<p>Perhaps this is the hardest part about where we are in the pandemic. Everyone wants to return to normal and never think about coronaviruses again. But a <a href="https://www.newyorker.com/science/annals-of-medicine/coexisting-with-the-coronavirus">preponderance of evidence indicates that SARS-CoV-2 will become endemic and will circulate like other common viruses</a>. It is likely that all of us will become immune to COVID-19 in one way or another, though, because of the vaccines, the disease will cause much less death and suffering than it did before. But even with such dramatic reductions in mortality, any COVID-19-associated deaths in the United States feel like too many for a society that has been terrorized and torn apart by the pandemic.</p>

<p>And given that children do not yet have a vaccine available, parents worry that their children remain at risk. But we accept much more deadly risks in our lives. It is uncomfortable to accept that a 3 year old has a similar risk of severe COVID-19 as a vaccinated 40 year old. Doing a cost benefit analysis about your child is emotionally impossible. I understand how any risk, no matter how small, can feel intolerable. Unfortunately, that same 3 year old is <a href="https://www.nytimes.com/2021/04/22/opinion/covid-vaccine-kids.html">more vulnerable to death or serious injury by driving, swimming, or even eating</a>. The risks entailed with COVID-19 do not seem to be much different than those of just growing up. Since we want nothing more than for our kids to be safe, we delude ourselves by never conceptualizing the risks of their ordinary activities. We live our lives as though these risks are zero. Though it seems impossible to imagine this now, in time, we’ll accept the remaining risk from COVID-19 as well.</p>

<p><em>Many thanks to Sarah Dean, Jordan Ellenberg, Eric Jonas, Lauren Kroiz, Deb Raji, Lawrence Recht, Chris Re, and Isaac Sparks for reading drafts of this post and offering insightful comments and suggestions.</em></p></div>
    </summary>
    <updated>2021-08-13T00:00:00Z</updated>
    <published>2021-08-13T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2021-08-20T22:58:43Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4087249606769943482</id>
    <link href="http://blog.computationalcomplexity.org/feeds/4087249606769943482/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/recognizing-faces.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4087249606769943482" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4087249606769943482" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/recognizing-faces.html" rel="alternate" type="text/html"/>
    <title>Recognizing Faces</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I sometimes have trouble recognizing faces, matching faces to people I've interacted with in the past. It's not a disease like <a href="https://en.wikipedia.org/wiki/Prosopagnosia">prosopagnosia</a>, I can certainly tell the difference between faces and have no trouble with people I work with directly. But if I haven't seen someone in a while, I may not recognize them or confuse them for someone else. It's especially bad out of context, say running into a professor in my campus on the streets of Frankfurt. It's gotten worse with age but I've had challenges my whole life.</p><p>I have my coping mechanisms. I start a conversation to get enough clues to figure out who I'm talking to. I'll google an image before I'm supposed to meet someone I haven't seen in a while. Sometimes I'll just say "Remind me how to pronounce your name again". Sometimes I'll just say something embarrassing thinking the person I'm talking to is someone else.</p><p>Name tags are useful, if it isn't obvious you are looking at them. Zoom has been great--everyone's name is just there. I worry that 18 months of zoom meetings means I've lost much of my coping ability, much the way I can no longer navigate by maps the way I used to.</p><p>We have technological solutions but mostly unable to make use of them. Through the magic of machine learning, computers have gotten extremely good at recognizing faces. Nevertheless Google Googles actively prevented their one killer app, telling you who you were looking at, for privacy reasons. Perhaps they could limit it to people in your contacts with pictures you uploaded. It would only recognize people you already know.</p><p>I know I'm not alone, and I'm writing this post so others won't feel alone. And next time you see me and I look confused, remind me of your name.</p></div>
    </content>
    <updated>2021-08-12T14:16:00Z</updated>
    <published>2021-08-12T14:16:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-20T22:13:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8184</id>
    <link href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/" rel="alternate" type="text/html"/>
    <title>Replica Method for the Machine Learning Theorist: Part 2 of 2</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Blake Bordelon, Haozhe Shan, Abdul Canatar, Boaz Barak, Cengiz Pehlevan See part 1 of this series, and pdf version of both parts. See also all seminar posts. In the previous post we described the outline of the replica method, and outlined the analysis per this figure: Specifically, we reduced the task of evaluating the expectation … <a class="more-link" href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/">Continue reading <span class="screen-reader-text">Replica Method for the Machine Learning Theorist: Part 2 of 2</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h4>Blake Bordelon, Haozhe Shan, Abdul Canatar, Boaz Barak, Cengiz Pehlevan</h4>



<p>See <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-1-of-2/">part 1</a> of this series, and <a href="https://boazbarak.org/Papers/replica.pdf">pdf version of both parts</a>. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a>.</p>



<p>In the previous post we described the outline of the replica method, and outlined the analysis per this figure:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/eVOI8EA.png"/></figure>



<p>Specifically, we reduced the task of evaluating the expectation of a (potentially modified) log partition function to evaluating expectation over <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> replicas which in turn amount to</p>



<p><img alt="\left&lt; \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) \right) \right&gt;_{\mathcal D} = \int\exp(-nN \mathcal{F}(Q))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D+%3D+%5Cint%5Cexp%28-nN+%5Cmathcal%7BF%7D%28Q%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix of _overlaps_ (inner products between pairs of replicas), and <img alt="\mathcal{F}(Q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D%28Q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is some nice analytical function depending on <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the log probability obtaining this overlap <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Then since this is an integral of exponentials, it turns out to be dominated by the maximizer <img alt="Q^\star" class="latex" src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, arriving at</p>



<p><img alt="\lim_{N \rightarrow \infty}\left&lt; Z^n \right&gt; = \exp(-n N \mathcal F(Q^\star ))." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN+%5Crightarrow+%5Cinfty%7D%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cexp%28-n+N+%5Cmathcal+F%28Q%5E%5Cstar+%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>reducing our desired quantity <img alt="&lt;-\log Z&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%3C-%5Clog+Z%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="N \mathcal F(Q^\star)" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Cmathcal+F%28Q%5E%5Cstar%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and so if we’re lucky, all we need to do is run Mathemtica or Sympy to find the maximizer of <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> over the space of all?? (not really: see below) <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrices.</p>



<h2>IV. Constraints on <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>: Replica Symmetry and Replica Symmetry Breaking</h2>



<p>Unfortunately, the description of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> above is a gross simplification. <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> cannot be any matrix — it needs to satisfy particular constraints. In particular, it cannot be a matrix that appears with probability tending to zero with <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the overlap matrix of a tuple of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> replicas from the Gibbs distribution.<br/>Hence, we need to understand the space of potential matrices <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that could arise from the probability distribution, and <img alt="Q^\star" class="latex" src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the global minimum under these constraints.</p>



<p>The most important constraint on <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the <b>replica symmetry</b> (RS), or the lack thereof (<b>replica symmetry breaking</b>, or <b>RSB</b>). Recall that <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> encodes the overlap between <img alt="\{x^{(a)}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bx%5E%7B%28a%29%7D%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where each element is a Gibbs random variable. On a high level, the structure of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> describes the geometry of the Gibbs distribution. An in depth description of the relationship between the two is beyond the scope of this post (check out <a href="http://michel.talagrand.net/challenge/volume1.pdf">Mean Field Models for Spin Glasses</a> by Michel Talagrand). We will give some intuitions that apply in the zero-temperature limit.</p>



<h3>A. What is the symmetry ansatz and when is it a good idea?</h3>



<p>The <b>replica symmetric ansatz</b> studies the following special form of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix</p>



<p><img alt="Q_{ab} = (q-q_0) \delta_{ab} + q_0" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%28q-q_0%29+%5Cdelta_%7Bab%7D+%2B+q_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\delta_{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the Kroneker delta. In other words, this ansatz corresponds to the guess that if we pick <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> random replicas <img alt="x^{(1)},\ldots, x^{(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2C+x%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then they will satisfy that <img alt="\| x^{(a)} \|^2 \approx q" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7C+x%5E%7B%28a%29%7D+%5C%7C%5E2+%5Capprox+q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for all <img alt="a=1,\ldots n" class="latex" src="https://s0.wp.com/latex.php?latex=a%3D1%2C%5Cldots+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and <img alt="x^{(a)} \cdot x^{(b)} \approx q_0" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%28a%29%7D+%5Ccdot+x%5E%7B%28b%29%7D+%5Capprox+q_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="a \neq b" class="latex" src="https://s0.wp.com/latex.php?latex=a+%5Cneq+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>This ansatz is especially natural for problems with unique minimizers for a fixed problem instance <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>In such a problem we might imagine that the <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> replicas are all random vectors that are have the same correlation with the true minimizer <img alt="x^{(0)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%280%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and since they are random and in high dimension, this correlation explains their correlation with one another (see example below).</p>



<p>&gt;<b>What have we done?</b> It is worthwhile to pause and take stock of what we have done here. We have reduced computing <img alt="\langle \log Z \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> into finding an expression for <img alt="\langle Z^n \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and then reduced this to computing <img alt="\mathbb{E} \exp(- n N \mathcal{F} (Q))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+%5Cexp%28-+n+N+%5Cmathcal%7BF%7D+%28Q%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> whre the expectation is taken over the induced distribution of the <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> overlap matrix. Now for every fixed <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we reduce the task to optimizing over just two parameters <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="q_0" class="latex" src="https://s0.wp.com/latex.php?latex=q_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Once we find the matrix <img alt="Q^\star" class="latex" src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that optimizes this bound, we obtain the desired quantity by taking <img alt="\lim_{n \rightarrow 0} \tfrac{1}{n}\log \exp(-n N \mathcal{F}(Q^\star) = N \mathcal{F}(Q^\star)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Crightarrow+0%7D+%5Ctfrac%7B1%7D%7Bn%7D%5Clog+%5Cexp%28-n+N+%5Cmathcal%7BF%7D%28Q%5E%5Cstar%29+%3D+N+%5Cmathcal%7BF%7D%28Q%5E%5Cstar%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h3>B. An illustration:</h3>



<p>Annealed langevin dynamics on a convex and non-convex objective below illustrate how the geometry of the learning problem influences the structure of the overlap matrix <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h4>A convex problem</h4>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"/>
</div></figure>



<h4>A non-convex problem</h4>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"/>
</div></figure>



<p>We see that even in low-dimensional problems, the structure of the loss landscape influences the resulting <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrices. Since the replica method works only in high dimensions, these animations cannot be taken too seriously as a justification of the symmetry ansatz, but below we discuss in what kinds of models we could expect the symmetry ansatz to be a good idea.</p>



<h3>C. Replica Symmetry in High Dimensions</h3>



<p>We will now discuss a simple model where the replica symmetry ansatz is especially natural. For a fixed problem instance <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, suppose that the <img alt="x_a" class="latex" src="https://s0.wp.com/latex.php?latex=x_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> vectors are distributed in a point cloud about some mean vector <img alt="\mu \in\mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p><img alt="x_a = \mu + \epsilon_a" class="latex" src="https://s0.wp.com/latex.php?latex=x_a+%3D+%5Cmu+%2B+%5Cepsilon_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\epsilon_a" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are zero-mean noise independently sampled across different replicas with covariance <img alt="\left&lt; \epsilon_{a,i} \epsilon_{b,j} \right&gt; = \frac{\sigma^2}{N} \delta_{ab}\delta_{ij}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cepsilon_%7Ba%2Ci%7D+%5Cepsilon_%7Bb%2Cj%7D+%5Cright%3E+%3D+%5Cfrac%7B%5Csigma%5E2%7D%7BN%7D+%5Cdelta_%7Bab%7D%5Cdelta_%7Bij%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This is equivalent to stipulating a Gibbs measure with energy <img alt="\beta H(x) = - \log p_{\epsilon}(x-\mu)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+H%28x%29+%3D+-+%5Clog+p_%7B%5Cepsilon%7D%28x-%5Cmu%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="p_\epsilon(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cepsilon%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the distribution of each noise variable. In this case, the <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix has elements</p>



<p><img alt="Q_{ab} = |\mu|^2 + \mu^\top \epsilon_a + \mu^\top \epsilon_b + \epsilon_a^\top \epsilon_b" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%7C%5Cmu%7C%5E2+%2B+%5Cmu%5E%5Ctop+%5Cepsilon_a+%2B+%5Cmu%5E%5Ctop+%5Cepsilon_b+%2B+%5Cepsilon_a%5E%5Ctop+%5Cepsilon_b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>By the central limit theorem, these sums of independently sampled random variables are approximately Gaussian (remember <img alt="N \to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>), so we can estimate how <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> behaves in the large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit</p>



<p><img alt="\left&lt; Q_{ab} \right&gt; = |\mu|^2 + \sigma^2 \delta_{ab} \ , \ \text{Var} \ Q_{ab} = O(1/N)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Q_%7Bab%7D+%5Cright%3E+%3D+%7C%5Cmu%7C%5E2+%2B+%5Csigma%5E2+%5Cdelta_%7Bab%7D+%5C+%2C+%5C+%5Ctext%7BVar%7D+%5C+Q_%7Bab%7D+%3D+O%281%2FN%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This implies that in the thermodynamic <img alt="N\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit, the <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix concentrates around a replica symmetric structure. Note that the emergence of this RS structure relied on the fact that high dimensional random vectors are approximately orthogonal. For many supervised learning problems such as least squares fitting, this toy model is actually relevant by specifically taking <img alt="\epsilon \sim \mathcal N(0,\sigma^2/N)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%5Csim+%5Cmathcal+N%280%2C%5Csigma%5E2%2FN%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h2>V. Example Simple Problem: Learning Curve Phase Transition in Least Squares Fitting</h2>



<p>To show these tools in action we will first study the simplest possible example with that has an interesting outcome. We will study the generalization performance of ridge regression on Gaussian distributed random features. In particular we will study a thermodynamic limit where the number of samples <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the number of features <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are both tending to infinity <img alt="P,N \to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=P%2CN+%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> but with finite ratio <img alt="\alpha = P/N" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+P%2FN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We will observe a phase transition the point <img alt="\alpha = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where the learning problem transitions from over-parameterized (<img alt="P &lt; N" class="latex" src="https://s0.wp.com/latex.php?latex=P+%3C+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) to under-parameterized (<img alt="P&gt;N" class="latex" src="https://s0.wp.com/latex.php?latex=P%3EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>). In the presence of noise this leads to an overfitting peak which can be eliminated through explicit regularization.</p>



<h3>A. Some References</h3>



<p><a href="https://iopscience.iop.org/article/10.1088/0305-4470/22/12/016">Hertz, Krogh, and Thorbergsson</a> first studied this problem and noted the phase transition at <img alt="\alpha = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. <a href="https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.031034">Advani and Ganguli</a> examine this model as a special case of M-estimation. Analysis of this model can also be obtained as a special case of kernel regression, for which general learning curves were obtained by <a href="https://arxiv.org/abs/2006.13198">Canatar, Bordelon, and Pehlevan</a> with the replica method. Similar overfitting peaks were recently observed in nonlinear two layer neural networks by <a href="https://www.pnas.org/content/116/32/15849">Belkin, Hsu, Ma, and Mandal</a> and modeled with the replica method by <a href="http://proceedings.mlr.press/v119/d-ascoli20a.html">d’Ascoli, Refinetti, Biroli, and, Krzakala </a> allowing them to <a href="https://arxiv.org/abs/2006.03509">clarify the two possible types of overfitting peaks</a> in random feature models. This problem can also be studied with tools from random matrix theory as in the work of <a href="https://arxiv.org/abs/1908.05355">Mei and Montanari</a> and several others.</p>



<h3>B. Problem Setup</h3>



<p>Our problem instance is a dataset <img alt="\mathcal D = \{ (x^\mu, y^\mu) \}_{\mu=1}^P" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D+%3D+%5C%7B+%28x%5E%5Cmu%2C+y%5E%5Cmu%29+%5C%7D_%7B%5Cmu%3D1%7D%5EP&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with <img alt="x^\mu \in \mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> drawn i.i.d. from a Gaussian distribution <img alt="x^\mu_k \sim \mathcal N(0,1/N )" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%5Cmu_k+%5Csim+%5Cmathcal+N%280%2C1%2FN+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The target values <img alt="y^\mu" class="latex" src="https://s0.wp.com/latex.php?latex=y%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are generated by a noisy linear teacher</p>



<p><img alt="y^\mu = \sum_{k=1}^N w_k^* x_k^\mu + \sigma \epsilon^\mu" class="latex" src="https://s0.wp.com/latex.php?latex=y%5E%5Cmu+%3D+%5Csum_%7Bk%3D1%7D%5EN+w_k%5E%2A+x_k%5E%5Cmu+%2B+%5Csigma+%5Cepsilon%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="||w^*||^2 = N" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%7Cw%5E%2A%7C%7C%5E2+%3D+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and noise is Gaussian distributed <img alt="\epsilon^\mu \sim \mathcal N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon%5E%5Cmu+%5Csim+%5Cmathcal+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We will compute, not the generalization error for a particular problem instance <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, but the <i>average</i> performance over random datasets! The energy function we study is the ridge regression loss function</p>



<p><img alt="H(w,\mathcal D) = \frac{1}{\lambda} \sum_{\mu=1}^P \left( \sum_{k=1}^N w_k x_k^\mu - y^\mu \right)^2 + \sum_k w_k^2 " class="latex" src="https://s0.wp.com/latex.php?latex=H%28w%2C%5Cmathcal+D%29+%3D+%5Cfrac%7B1%7D%7B%5Clambda%7D+%5Csum_%7B%5Cmu%3D1%7D%5EP+%5Cleft%28+%5Csum_%7Bk%3D1%7D%5EN+w_k+x_k%5E%5Cmu+-+y%5E%5Cmu+%5Cright%29%5E2+%2B+%5Csum_k+w_k%5E2+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>The ridge parameter <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> controls the trade-off between training accuracy and regularization of the weight vectors. When <img alt="\lambda \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the training data are fit perfectly while the <img alt="\lambda \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit gives <img alt="w = 0" class="latex" src="https://s0.wp.com/latex.php?latex=w+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the minimizer of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The <img alt="\beta \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit of the Gibbs distribution corresponds to studying the performance of the ridge regression solution which minimizes <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The generalization error is an average over possible test points drawn from the same distribution<br/><img alt="E_g = \left&lt; \left( \sum_k (w_k-w^*_k) x_k \right)^2 \right&gt;_x = \frac{1}{N} \sum_{k=1}^N (w_k-w_k^*)^2" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%3D+%5Cleft%3C+%5Cleft%28+%5Csum_k+%28w_k-w%5E%2A_k%29+x_k+%5Cright%29%5E2+%5Cright%3E_x+%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_%7Bk%3D1%7D%5EN+%28w_k-w_k%5E%2A%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>C. Partition Function</h3>



<p>We introduce a partition function for the Gibbs distribution on <img alt="H(w,\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28w%2C%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><img alt="Z(\mathcal D) = \int dw \exp\left( - \frac{\beta}{2 \lambda} \sum_{\mu=1}^P ( w^\top x^\mu - y^\mu )^2 + \frac{\beta }{2}||w||^2 \right) ." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%29+%3D+%5Cint+dw+%5Cexp%5Cleft%28+-+%5Cfrac%7B%5Cbeta%7D%7B2+%5Clambda%7D+%5Csum_%7B%5Cmu%3D1%7D%5EP+%28+w%5E%5Ctop+x%5E%5Cmu+-+y%5E%5Cmu+%29%5E2+%2B+%5Cfrac%7B%5Cbeta+%7D%7B2%7D%7C%7Cw%7C%7C%5E2+%5Cright%29+.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>D. Replicated Partition Function</h3>



<p>We can rewrite the integral through a simple change of variables <img alt="\Delta = w-w^*" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta+%3D+w-w%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> since <img alt="y^\mu = w^{* \top} x^\mu + \sigma \epsilon^\mu" class="latex" src="https://s0.wp.com/latex.php?latex=y%5E%5Cmu+%3D+w%5E%7B%2A+%5Ctop%7D+x%5E%5Cmu+%2B+%5Csigma+%5Cepsilon%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. <img alt="\Delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> represents the discrepancy between the learned weights <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the target weights <img alt="w^*" class="latex" src="https://s0.wp.com/latex.php?latex=w%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We will now replicate and average over the training data <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, ie compute <img alt="\left&lt; Z^n \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p><img alt="\left&lt; Z(\mathcal{D}, J)^n \right&gt;_{\mathcal D} = \int \prod_{a=1}^n d\Delta_a \left&lt; \exp\left( - \frac{\beta}{2\lambda} \sum_{\mu=1}^P \sum_{a=1}^n ( \Delta_a \cdot x^\mu - \sigma \epsilon^\mu )^2 - \frac{\beta}{2} \sum_{a=1}^n ||\Delta_a + w^*||^2 \right) \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%28%5Cmathcal%7BD%7D%2C+J%29%5En+%5Cright%3E_%7B%5Cmathcal+D%7D+%3D+%5Cint+%5Cprod_%7Ba%3D1%7D%5En+d%5CDelta_a+%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Cfrac%7B%5Cbeta%7D%7B2%5Clambda%7D+%5Csum_%7B%5Cmu%3D1%7D%5EP+%5Csum_%7Ba%3D1%7D%5En+%28+%5CDelta_a+%5Ccdot+x%5E%5Cmu+-+%5Csigma+%5Cepsilon%5E%5Cmu+%29%5E2+-+%5Cfrac%7B%5Cbeta%7D%7B2%7D+%5Csum_%7Ba%3D1%7D%5En+%7C%7C%5CDelta_a+%2B+w%5E%2A%7C%7C%5E2+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><strong>Warning:</strong> Notice that by writing these integrals, we are implicitly assuming that <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is an integer. Eventually, we need to take <img alt="n \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit to obtain the generalization error <img alt="E_g" class="latex" src="https://s0.wp.com/latex.php?latex=E_g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> from <img alt="\left&lt; \log Z \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. After computation of <img alt="\left&lt; Z^n \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at integer <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we will get an analytic expression of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which we will allow us to non-rigorously take <img alt="n \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>The randomness from the dataset <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is present in the first term only appears through mean-zero Gaussian variables <img alt="\gamma_a^\mu = \Delta_a \cdot x^\mu - \epsilon^\mu \sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_a%5E%5Cmu+%3D+%5CDelta_a+%5Ccdot+x%5E%5Cmu+-+%5Cepsilon%5E%5Cmu+%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which have covariance structure</p>



<p><img alt="\left&lt; \gamma_a^\mu \gamma_b^\nu \right&gt; = \delta_{\mu \nu} \left[ \frac{1}{N} \Delta_a \cdot \Delta_b + \sigma^2 \right] = \delta_{\mu \nu} \left[ Q_{ab} + \sigma^2 \right] \implies \left&lt; \gamma \gamma^\top \right&gt; = Q + \sigma^2 1 1^\top \in \mathbb{R}^{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cgamma_a%5E%5Cmu+%5Cgamma_b%5E%5Cnu+%5Cright%3E+%3D+%5Cdelta_%7B%5Cmu+%5Cnu%7D+%5Cleft%5B+%5Cfrac%7B1%7D%7BN%7D+%5CDelta_a+%5Ccdot+%5CDelta_b+%2B+%5Csigma%5E2+%5Cright%5D+%3D+%5Cdelta_%7B%5Cmu+%5Cnu%7D+%5Cleft%5B+Q_%7Bab%7D+%2B+%5Csigma%5E2+%5Cright%5D+%5Cimplies+%5Cleft%3C+%5Cgamma+%5Cgamma%5E%5Ctop+%5Cright%3E+%3D+Q+%2B+%5Csigma%5E2+1+1%5E%5Ctop+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>where <img alt="1 \in \mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=1+%5Cin+%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the vector of all ones and we introduced overlap order parameters <img alt="Q_{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> defined as</p>



<p><img alt="Q_{ab} = \frac{1}{N} \Delta_a \cdot \Delta_b \ ." class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%5Cfrac%7B1%7D%7BN%7D+%5CDelta_a+%5Ccdot+%5CDelta_b+%5C+.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>The average over the randomness in the dataset <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is therefore converted into a routine Gaussian integral. Exploiting the independence over each data point, we break the average into a product of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> averages.</p>



<p><img alt="\left&lt; \exp\left( -\frac{\beta}{2\lambda} \sum_{a,\mu} \left( \gamma_{a}^\mu\right)^2 \right) \right&gt;_{\{\gamma_a^\mu\}} = \left&lt; \exp\left( -\frac{\beta}{2\lambda} \sum_{a} \gamma_{a}^2 \right) \right&gt;_{\{\gamma_a\}}^P" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cexp%5Cleft%28+-%5Cfrac%7B%5Cbeta%7D%7B2%5Clambda%7D+%5Csum_%7Ba%2C%5Cmu%7D+%5Cleft%28+%5Cgamma_%7Ba%7D%5E%5Cmu%5Cright%29%5E2+%5Cright%29+%5Cright%3E_%7B%5C%7B%5Cgamma_a%5E%5Cmu%5C%7D%7D+%3D+%5Cleft%3C+%5Cexp%5Cleft%28+-%5Cfrac%7B%5Cbeta%7D%7B2%5Clambda%7D+%5Csum_%7Ba%7D+%5Cgamma_%7Ba%7D%5E2+%5Cright%29+%5Cright%3E_%7B%5C%7B%5Cgamma_a%5C%7D%7D%5EP&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Each average is a multivariate Gaussian integral of the form<br/><img alt="\int \frac{d\gamma_1 d\gamma_2 ... d\gamma_n}{\sqrt{\left( 2\pi \right)^n \det(Q+\sigma^2 I)}} \exp\left( -\frac{1}{2} \sum_{ab} \gamma_a \gamma_b \left( Q + \sigma^2 11^\top \right)^{-1}_{ab} - \frac{\beta}{2 \lambda} \sum_{a} \gamma_a^2 \right) = \det\left(I + \frac{\beta}{\lambda} Q + \frac{\beta}{\lambda} \sigma^2 11^\top \right)^{-1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+%5Cfrac%7Bd%5Cgamma_1+d%5Cgamma_2+...+d%5Cgamma_n%7D%7B%5Csqrt%7B%5Cleft%28+2%5Cpi+%5Cright%29%5En+%5Cdet%28Q%2B%5Csigma%5E2+I%29%7D%7D+%5Cexp%5Cleft%28+-%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bab%7D+%5Cgamma_a+%5Cgamma_b+%5Cleft%28+Q+%2B+%5Csigma%5E2+11%5E%5Ctop+%5Cright%29%5E%7B-1%7D_%7Bab%7D+-+%5Cfrac%7B%5Cbeta%7D%7B2+%5Clambda%7D+%5Csum_%7Ba%7D+%5Cgamma_a%5E2+%5Cright%29+%3D+%5Cdet%5Cleft%28I+%2B+%5Cfrac%7B%5Cbeta%7D%7B%5Clambda%7D+Q+%2B+%5Cfrac%7B%5Cbeta%7D%7B%5Clambda%7D+%5Csigma%5E2+11%5E%5Ctop+%5Cright%29%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This integral can be derived by routine integration of Gaussian functions, which we derive in the Appendix.</p>



<h3>E. Enforcing Order Parameter Definition</h3>



<p>To enforce the definition of the order parameters, we insert delta-functions into the expression for <img alt="\left&lt; Z^n \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which we write as Fourier integrals over dual order parameters <img alt="\hat{Q}_{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7BQ%7D_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><img alt="\delta( N Q_{ab} - \Delta_a \cdot \Delta_b) = \frac{1}{2\pi} \int d\hat{Q}_{ab} \exp\left(i \hat{Q}_{ab} ( N Q_{ab} - \Delta_a \cdot \Delta_b)\right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%28+N+Q_%7Bab%7D+-+%5CDelta_a+%5Ccdot+%5CDelta_b%29+%3D+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint+d%5Chat%7BQ%7D_%7Bab%7D+%5Cexp%5Cleft%28i+%5Chat%7BQ%7D_%7Bab%7D+%28+N+Q_%7Bab%7D+-+%5CDelta_a+%5Ccdot+%5CDelta_b%29%5Cright%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This trick is routine and is derived in the Appendix of this post.</p>



<p>After integration over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\Delta_a" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we are left with an expression of the form</p>



<p><img alt="\left&lt; Z^n \right&gt; = \int \prod_{ab} dQ_{ab} \prod_{ab} d\hat{Q}_{ab} \exp\left( - P G_E(Q) + i N \sum_{ab} Q_{ab} \hat{Q}_{ab} - N G_S(\hat{Q}) \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint+%5Cprod_%7Bab%7D+dQ_%7Bab%7D+%5Cprod_%7Bab%7D+d%5Chat%7BQ%7D_%7Bab%7D+%5Cexp%5Cleft%28+-+P+G_E%28Q%29+%2B+i+N+%5Csum_%7Bab%7D+Q_%7Bab%7D+%5Chat%7BQ%7D_%7Bab%7D+-+N+G_S%28%5Chat%7BQ%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="G_E" class="latex" src="https://s0.wp.com/latex.php?latex=G_E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a function which arises from the average over <img alt="\gamma_a^\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_a%5E%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="G_S" class="latex" src="https://s0.wp.com/latex.php?latex=G_S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is calculated through integration over the <img alt="\Delta_a" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> variables.</p>



<p><strong>Warning:</strong> The functions <img alt="G_E" class="latex" src="https://s0.wp.com/latex.php?latex=G_E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="G_S" class="latex" src="https://s0.wp.com/latex.php?latex=G_S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> have complicated formulas and we omit them here to focus on the conceptual steps in the replica method. Interested readers can find explicit expressions for these functions in the references above.</p>



<h3>F. Replica Symmetry</h3>



<p>To make progress on the integral above, we will make the replica symmetry assumption, leveraging the fact that the ridge regression loss is convex and has unique minimizer for <img alt="\lambda &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Based on our simulations and arguments above, we will assume that the <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\hat{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrices satisfy <i>replica symmetry</i><br/><img alt="{Q}_{ab} = q \delta_{ab} + q_0 \ , \ \hat{Q}_{ab} = \hat{q} \delta_{ab} + \hat{q}_0 " class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D_%7Bab%7D+%3D+q+%5Cdelta_%7Bab%7D+%2B+q_0+%5C+%2C+%5C+%5Chat%7BQ%7D_%7Bab%7D+%3D+%5Chat%7Bq%7D+%5Cdelta_%7Bab%7D+%2B+%5Chat%7Bq%7D_0+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>G. Saddle Point Equations and Final Result</h3>



<p>After the replica symmetry ansatz, the replicated partition function has the form</p>



<p><img alt="\left&lt; Z^n \right&gt; = \int dq d\hat{q} dq_0 d\hat{q}_0 \exp( - n N \mathcal F(q,\hat{q},q_0,\hat{q}_0))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint+dq+d%5Chat%7Bq%7D+dq_0+d%5Chat%7Bq%7D_0+%5Cexp%28+-+n+N+%5Cmathcal+F%28q%2C%5Chat%7Bq%7D%2Cq_0%2C%5Chat%7Bq%7D_0%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>In the <img alt="N \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit, this integral is dominated by the order parameters <img alt="q, \hat{q},q_0,\hat{q}_0" class="latex" src="https://s0.wp.com/latex.php?latex=q%2C+%5Chat%7Bq%7D%2Cq_0%2C%5Chat%7Bq%7D_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which satisfy the saddle point equations</p>



<p><img alt="\frac{\partial \mathcal F}{\partial q} = 0 \ , \ \frac{\partial \mathcal F}{\partial \hat{q}} = 0 \ , \ \frac{\partial \mathcal F}{\partial q_0} = 0 \ , \ \frac{\partial \mathcal F}{\partial \hat{q}_0} = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+q%7D+%3D+0+%5C+%2C+%5C+%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+%5Chat%7Bq%7D%7D+%3D+0+%5C+%2C+%5C+%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+q_0%7D+%3D+0+%5C+%2C+%5C+%5Cfrac%7B%5Cpartial+%5Cmathcal+F%7D%7B%5Cpartial+%5Chat%7Bq%7D_0%7D+%3D+0+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><strong>Warning</strong>:Notice that <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is small (we are working in <img alt="n\to0" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Cto0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit to study <img alt="\log Z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) but <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is large (we are studying the “thermodynamic” <img alt="N\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit). The order of taking these limits matters. It is important that we take <img alt="N \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> first before taking <img alt="n \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> so that, at finite value of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the integral for <img alt="\left&lt; Z^n \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is dominated by the saddle point of <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>We can solve the saddle point equations symbolically with Mathematica (see <a href="https://www.dropbox.com/s/m0pje9mr0gp0x1n/saddle_point_demo.nb?dl=1">this notebook</a>) in the <img alt="\beta \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit. We notice that <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> must scale like <img alt="O(1/\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%2F%5Cbeta%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\hat{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> must scale like <img alt="O(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Cbeta%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. After factoring out the dependence on the temperature, we can compute the saddle point conditions through partial differentiation.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Ps8WGh6.png"/></figure>



<p>This symbolically gives us the order parameters at the saddle point. For example, the overlap parameter <img alt="q = \frac{1}{2}[1-\lambda-\alpha + \sqrt{(1-\lambda-\alpha)^2 + 4\lambda} ]" class="latex" src="https://s0.wp.com/latex.php?latex=q+%3D+%5Cfrac%7B1%7D%7B2%7D%5B1-%5Clambda-%5Calpha+%2B+%5Csqrt%7B%281-%5Clambda-%5Calpha%29%5E2+%2B+4%5Clambda%7D+%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. After solving the saddle point equations, the generalization error can be written entirely in terms of the first order parameter <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at the saddle point. For replica <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the generalization error is merely <img alt="||\Delta_a||^2 = ||w_a - w^*||^2 = Q_{aa} = q+q_0" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%7C%5CDelta_a%7C%7C%5E2+%3D+%7C%7Cw_a+-+w%5E%2A%7C%7C%5E2+%3D+Q_%7Baa%7D+%3D+q%2Bq_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Thus</p>



<p><img alt="E_g = q+q_0 = \frac{(q+\lambda)^2 + \sigma^2 \alpha}{(q+\lambda + \alpha)^2 - \alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%3D+q%2Bq_0+%3D+%5Cfrac%7B%28q%2B%5Clambda%29%5E2+%2B+%5Csigma%5E2+%5Calpha%7D%7B%28q%2B%5Clambda+%2B+%5Calpha%29%5E2+-+%5Calpha%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Where <img alt="q + \lambda = \frac{1}{2} \left[ 1 + \lambda - \alpha + \sqrt{(1+\lambda-\alpha)^2 + 4\lambda\alpha} \right]" class="latex" src="https://s0.wp.com/latex.php?latex=q+%2B+%5Clambda+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cleft%5B+1+%2B+%5Clambda+-+%5Calpha+%2B+%5Csqrt%7B%281%2B%5Clambda-%5Calpha%29%5E2+%2B+4%5Clambda%5Calpha%7D+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at the saddle point.</p>



<h3>H. Noise Free Estimation</h3>



<p>When <img alt="\sigma^2, \lambda \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2%2C+%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> the generalization error decreases linearly with <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>: <img alt="E_g = 1-\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%3D+1-%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\alpha &lt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3C+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="E_g=0" class="latex" src="https://s0.wp.com/latex.php?latex=E_g%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\alpha &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3E+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This indicates the target weights are perfectly estimated when the number of samples equals the number of features <img alt="P \to N" class="latex" src="https://s0.wp.com/latex.php?latex=P+%5Cto+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. A finite ridge parameter <img alt="\lambda &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> increases the generalization error when noise is zero <img alt="\sigma^2=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Asymptotically, the generalization error scales like <img alt="E_g \sim \frac{\lambda^2}{\alpha^2}" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Cfrac%7B%5Clambda%5E2%7D%7B%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for large <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h3>I. Phase transition and overfitting peaks</h3>



<p>In the presence of noise <img alt="\sigma^2 &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the story is different. In this case, the generalization error exhibits a peak at <img alt="\alpha \approx 1+\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+1%2B%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> before falling at a rate <img alt="E_g \sim \sigma^2/\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Csigma%5E2%2F%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at large <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In this regime, accurate estimation requires reducing the variance of the estimator by increasing the number of samples.</p>



<p>In small <img alt="\lambda \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit, the order parameter behaves like <img alt="q+\lambda \sim 1-\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=q%2B%5Clambda+%5Csim+1-%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\alpha&lt;1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3C1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="q+\lambda \sim \lambda" class="latex" src="https://s0.wp.com/latex.php?latex=q%2B%5Clambda+%5Csim+%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\alpha &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3E+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/nmDihyt.png"/></figure>



<p>The free energy <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> exhibits a discontinuous first derivative as <img alt="\alpha \to 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, a phenomenon known as <a href="https://en.wikipedia.org/wiki/Phase_transition#Ehrenfest_classification">first-order phase transition</a>. Let <img alt="\mathcal F^*" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be the value of the free energy at the saddle point <img alt="(Q^\star, \hat{q}^*, q_0^*, \hat{q}_0^*)" class="latex" src="https://s0.wp.com/latex.php?latex=%28Q%5E%5Cstar%2C+%5Chat%7Bq%7D%5E%2A%2C+q_0%5E%2A%2C+%5Chat%7Bq%7D_0%5E%2A%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Then we find</p>



<p><img alt="\frac{\partial \mathcal F^*}{\partial \alpha} \sim \frac{\sigma^2}{2\lambda} \Theta(\alpha - 1) + \mathcal{O}_\lambda(1) \ , \ (\lambda \to 0, \alpha \to 1 )" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+%5Cmathcal+F%5E%2A%7D%7B%5Cpartial+%5Calpha%7D+%5Csim+%5Cfrac%7B%5Csigma%5E2%7D%7B2%5Clambda%7D+%5CTheta%28%5Calpha+-+1%29+%2B+%5Cmathcal%7BO%7D_%5Clambda%281%29+%5C+%2C+%5C+%28%5Clambda+%5Cto+0%2C+%5Calpha+%5Cto+1+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>which indicates a discontinous first derivative in the <img alt="\lambda \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit as <img alt="\alpha \to 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We plot this free energy <img alt="\mathcal F^*(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F%5E%2A%28%5Calpha%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for varying values of <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, showing that as <img alt="\lambda\to 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%5Cto+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> a discontinuity in the free energy occurs at <img alt="\alpha \to 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The non-zero ridge parameter <img alt="\lambda &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> prevents the strict phase transition at <img alt="\alpha = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/f4t9Tun.png"/></figure>



<h3>J. Putting it all together</h3>



<p>Using the analysis of the saddle point, we are now prepared to construct a full picture of the possibilities. A figure below from <a href="https://arxiv.org/abs/2006.13198">this paper</a> provies all of the major insights. We plot experimental values of generalization error <img alt="E_g" class="latex" src="https://s0.wp.com/latex.php?latex=E_g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in a <img alt="N=800" class="latex" src="https://s0.wp.com/latex.php?latex=N%3D800&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> dimensional problem to provide a comparison with the replica prediction.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Vy7GwUs.png"/></figure>



<p>( a ) When <img alt="\lambda = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the generalization error either falls like <img alt="1-\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=1-%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> if noise is zero, or it exhibits a divergence at <img alt="\alpha \to 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Cto+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> if noise is non-zero.</p>



<p>( b ) When noise is zero, increasing the explicit ridge <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> increases the generalization error. At large <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="E_g \sim \frac{\lambda^2}{\alpha^2}" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Cfrac%7B%5Clambda%5E2%7D%7B%5Calpha%5E2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>( c ) When there is noise, explicit regularization can prevent the overfitting peak and give optimal generalization. At large <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="E_g \sim \frac{\sigma^2}{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=E_g+%5Csim+%5Cfrac%7B%5Csigma%5E2%7D%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>( d ) In the <img alt="(\lambda,\sigma^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> plane, there are multiple possibilities for the learning curve <img alt="E_g(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=E_g%28%5Calpha%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Monotonic learning curves <img alt="E_g'(\alpha) &lt;0" class="latex" src="https://s0.wp.com/latex.php?latex=E_g%27%28%5Calpha%29+%3C0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are guaranteed provided <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is sufficiently large compared to <img alt="\sigma^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. If regularization is too small, then two-critical points can exist in the learning curve, ie two values <img alt="\alpha^*" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="E_g'(\alpha^*) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=E_g%27%28%5Calpha%5E%2A%29+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (sample wise double descent). For very large noise, a single local maximum exists in the learning curve <img alt="E_g(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=E_g%28%5Calpha%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which is followed by monotonic decreasing error.</p>



<h2>VI. Example Problem 2: Spiked Matrix Recovery</h2>



<p>_Detailed calculations can be found in this excellent <a href="https://meisong541.github.io/jekyll/update/2019/08/04/Replica_method_1.html">introduction of the problem by Song Mei</a>._</p>



<p>Suppose we have a <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>-by-<img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> rank-1 matrix, <img alt="\lambda \mathbf u \mathbf u ^T" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cmathbf+u+%5Cmathbf+u+%5ET&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where <img alt="\mathbf u" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a norm-1 column vector constituting the signal that we would like to recover. The input <img alt="\mathbf A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> we receive is corrupted by symmetric Gaussian i.i.d. noise, i.e.,</p>



<p><img alt="\mathbf A = \lambda \mathbf{uu}^T + \mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A+%3D+%5Clambda+%5Cmathbf%7Buu%7D%5ET+%2B+%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="W_{ij}=W_{ji}\sim \mathcal N(0, 1/N), W_{ii}\sim \mathcal N (0, 2/N)" class="latex" src="https://s0.wp.com/latex.php?latex=W_%7Bij%7D%3DW_%7Bji%7D%5Csim+%5Cmathcal+N%280%2C+1%2FN%29%2C+W_%7Bii%7D%5Csim+%5Cmathcal+N+%280%2C+2%2FN%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (<img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is drawn from a Gaussian Orthogonal Ensemble). At large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, eigenvalues of <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are distributed uniformly on a unit disk in the complex plane. Thus, the best estimate (which we call <img alt="\mathbf v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) of<br/><img alt="\mathbf u" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> from <img alt="\mathbf A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the eigenvector associated with the largest eigenvalue. In other words</p>



<p><img alt="\mathbf v = \arg \max_{\mathbf x\in \mathbb S^{N-1}} \mathbf{x}^T \mathbf{A} \mathbf {x}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v+%3D+%5Carg+%5Cmax_%7B%5Cmathbf+x%5Cin+%5Cmathbb+S%5E%7BN-1%7D%7D+%5Cmathbf%7Bx%7D%5ET+%5Cmathbf%7BA%7D+%5Cmathbf+%7Bx%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>The <i>observable</i> of interest is how well the estimate, <img alt="\mathbf v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, matches <img alt="\mathbf u" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, as measured by <img alt="(\mathbf v \cdot \mathbf u)^2" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+v+%5Ccdot+%5Cmathbf+u%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We would like to know its average over different <img alt="\mathbf{W}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>In the problem setup, <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a constant controlling the signal-to-noise ratio. Intuitively, the larger <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is, the better the estimate should be (when averaged over <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>). This is indeed true. Remarkably, for large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="\mathbf v \cdot \mathbf u" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v+%5Ccdot+%5Cmathbf+u&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is almost surely <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\lambda &lt;1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3C1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. For <img alt="\lambda \geq 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, it grows quickly as <img alt="1-\lambda^{-2}" class="latex" src="https://s0.wp.com/latex.php?latex=1-%5Clambda%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This discontinuity at <img alt="\lambda=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a <b>phase transition</b>. This dependence on <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be derived using the replica method.<br/><img src="https://i.imgur.com/ysQ4W3Y.png"/></p>



<p>In the simulations above, we see two trend with increasing <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. First, the average curve approaches the theory, which is good. In addition, trial-to-trial variability (as reflected by the error bars) shrinks. This reflects the fact that our observable is indeed self-averaging.</p>



<p>Here, we give a brief overview of how the steps of a replica calculation can be set up and carried out.</p>



<h3>Step 1</h3>



<p>Here, <img alt="\mathbf{W}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the problem parameter (<img alt="\mathcal P" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) that we average over. The minimized function is</p>



<p><img alt="H(\mathbf x, \mathbf W) = -\mathbf x^T (\lambda \mathbf u \mathbf u^T + \mathbf W) \mathbf{x} = -\lambda (\mathbf x \cdot \mathbf{u })^2 - \mathbf{x}^T \mathbf{W} \mathbf{x}." class="latex" src="https://s0.wp.com/latex.php?latex=H%28%5Cmathbf+x%2C+%5Cmathbf+W%29+%3D+-%5Cmathbf+x%5ET+%28%5Clambda+%5Cmathbf+u+%5Cmathbf+u%5ET+%2B+%5Cmathbf+W%29+%5Cmathbf%7Bx%7D+%3D+-%5Clambda+%28%5Cmathbf+x+%5Ccdot+%5Cmathbf%7Bu+%7D%29%5E2+-+%5Cmathbf%7Bx%7D%5ET+%5Cmathbf%7BW%7D+%5Cmathbf%7Bx%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This energy function already contains a “source term” for our observable of interest. Thus, the vanilla partition function will be used as the augmented partition function. In addition, this function does not scale with <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. To introduce the appropriate <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> scaling, we add an <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> factor to <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, yielding the partition function</p>



<p><img alt="Z(\mathbf W, \lambda) = \int_{\mathbb S^{N-1}}d \mathbf x \exp \left( -\beta N H(\mathbf x, \mathbf W) \right)." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathbf+W%2C+%5Clambda%29+%3D+%5Cint_%7B%5Cmathbb+S%5E%7BN-1%7D%7Dd+%5Cmathbf+x+%5Cexp+%5Cleft%28+-%5Cbeta+N+H%28%5Cmathbf+x%2C+%5Cmathbf+W%29+%5Cright%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>It follows that (again using angular brackets to denote average over <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>)</p>



<p><img alt="\langle (\mathbf x \cdot \mathbf u)^2 \rangle = \frac{1}{\beta N}\frac{d}{d\lambda }\langle \log Z(\mathbf W, \lambda) \rangle \Big|_{\lambda=0}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%28%5Cmathbf+x+%5Ccdot+%5Cmathbf+u%29%5E2+%5Crangle+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Cfrac%7Bd%7D%7Bd%5Clambda+%7D%5Clangle+%5Clog+Z%28%5Cmathbf+W%2C+%5Clambda%29+%5Crangle+%5CBig%7C_%7B%5Clambda%3D0%7D.+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>Since we are ultimately interested in this observable for the best estimate, <img alt="\mathbf v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, at the large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, limit, we seek to compute</p>



<p><img alt="\lim_{N\rightarrow \infty}\mathbb E [(\mathbf v \cdot \mathbf u)^2] =\lim_{N\rightarrow \infty} \lim_{\beta\rightarrow \infty} \frac{1}{\beta N}\frac{d}{d\lambda }\langle \log Z(\mathbf W, \lambda) \rangle ." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN%5Crightarrow+%5Cinfty%7D%5Cmathbb+E+%5B%28%5Cmathbf+v+%5Ccdot+%5Cmathbf+u%29%5E2%5D+%3D%5Clim_%7BN%5Crightarrow+%5Cinfty%7D+%5Clim_%7B%5Cbeta%5Crightarrow+%5Cinfty%7D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Cfrac%7Bd%7D%7Bd%5Clambda+%7D%5Clangle+%5Clog+Z%28%5Cmathbf+W%2C+%5Clambda%29+%5Crangle+.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Why don’t we evaluate the derivative only at <img alt="\lambda=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>? Because <img alt="\lambda (\mathbf x \cdot \mathbf u)^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%28%5Cmathbf+x+%5Ccdot+%5Cmathbf+u%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not a source term that we introduced. Another way to think about it is that this result needs to be a function of <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, so of course we don’t just evaluate it at one value.</p>



<h3>Step 2</h3>



<p>Per the replica trick, we need to compute</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/7f2JdBt.png"/></figure>



<p><strong>Warning:</strong> Hereafter, our use of “<img alt="\langle Z^n\rangle =" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>” is loose. When performing integrals, we will ignore the constants generated and only focus on getting the exponent right. This is because we will eventually take <img alt="\log" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of <img alt="\langle Z^n \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and take the derivative w.r.t. <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. A constant in <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in front of the integral expression for <img alt="\langle Z^n\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> does not affect this integral. This is often the case in replica calculations.</p>



<p>where <img alt="D\mathbf x" class="latex" src="https://s0.wp.com/latex.php?latex=D%5Cmathbf+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a uniform measure on <img alt="\mathbb S^{N-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+S%5E%7BN-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="D\mathbf{W}" class="latex" src="https://s0.wp.com/latex.php?latex=D%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the probability measure for <img alt="\mathbf{W}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, as described above.</p>



<p>We will not carry out the calculation in detail in this note as the details are problem-specific. But the overall workflow is rather typical of replica calculations:</p>



<p>1. Integrate over <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This can be done by writing <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the sum of a Gaussian i.i.d. matrix with its own transpose. The integral is then over the i.i.d. matrix and thus a standard Gaussian integral. After this step, we obtain an expression that no longer contains <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>,a major simplification.<br/>2. Introduce the order parameter <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. After the last integral, the exponent only depends on <img alt="\mathbf x^{(a)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> through <img alt="\mathbf u \cdot \mathbf x^{(a)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+u+%5Ccdot+%5Cmathbf+x%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\mathbf{x^{(a)}} \cdot \mathbf{x^{(b)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%5E%7B%28a%29%7D%7D+%5Ccdot+%5Cmathbf%7Bx%5E%7B%28b%29%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. These dot products can be described by a matrix <img alt="\mathbf Q \in \mathbb R^{N+1 \times N+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q+%5Cin+%5Cmathbb+R%5E%7BN%2B1+%5Ctimes+N%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where we define <img alt="Q_{0,a}=Q_{a,0}=\mathbf{u} \cdot \mathbf{x}^{(a)}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7B0%2Ca%7D%3DQ_%7Ba%2C0%7D%3D%5Cmathbf%7Bu%7D+%5Ccdot+%5Cmathbf%7Bx%7D%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="Q_{a\geq 1, b\geq1}=\mathbf x^{(a)} \cdot \mathbf x^{(b)}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Ba%5Cgeq+1%2C+b%5Cgeq1%7D%3D%5Cmathbf+x%5E%7B%28a%29%7D+%5Ccdot+%5Cmathbf+x%5E%7B%28b%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>3. Replace the integral over <img alt="\mathbf x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with one over <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. A major inconvenience of the integral over <img alt="\mathbf x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is that it is not over the entire real space but over a hypersphere. However, we can demand <img alt="\mathbf x^{(a)} \in \mathbb S^{N-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+x%5E%7B%28a%29%7D+%5Cin+%5Cmathbb+S%5E%7BN-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> by requiring <img alt="Q_{aa}=1" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Baa%7D%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Now, we rewrite the exponent in terms of <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and integrate over <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> instead, but we add many Dirac delta functions to enforce the definition of <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We get an expression in the form</p>



<p><img alt="\langle Z^n\rangle = \int d\mathbf Q \exp (f(Q)) \prod_{i=1}^N \delta (\mathbf u \cdot \mathbf x^{(i)} - Q_{0i}) \prod _{1\leq i \leq j \leq N} \delta (\mathbf x^{(j)} u \cdot \mathbf x^{(i)} - Q_{ji})." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D+%5Cint+d%5Cmathbf+Q+%5Cexp+%28f%28Q%29%29+%5Cprod_%7Bi%3D1%7D%5EN+%5Cdelta+%28%5Cmathbf+u+%5Ccdot+%5Cmathbf+x%5E%7B%28i%29%7D+-+Q_%7B0i%7D%29+%5Cprod+_%7B1%5Cleq+i+%5Cleq+j+%5Cleq+N%7D+%5Cdelta+%28%5Cmathbf+x%5E%7B%28j%29%7D+u+%5Ccdot+%5Cmathbf+x%5E%7B%28i%29%7D+-+Q_%7Bji%7D%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>4. After some involved simplifications, we have</p>



<p><img alt="\langle Z^n\rangle = \int d \mathbf Q \exp(N g(\mathbf Q) + C)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D+%5Cint+d+%5Cmathbf+Q+%5Cexp%28N+g%28%5Cmathbf+Q%29+%2B+C%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>where <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> does not depend on <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="g(\mathbf Q)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cmathbf+Q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is <img alt="O(1)" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. By the saddle point method,</p>



<p><img alt="\langle Z^n\rangle = \max_\mathbf{Q} \exp(N g(\mathbf Q))," class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+Z%5En%5Crangle+%3D+%5Cmax_%5Cmathbf%7BQ%7D+%5Cexp%28N+g%28%5Cmathbf+Q%29%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\mathbf{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> needs to satisfy the various constraints we proposed (e.g., its diagonal is all <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and it is symmetric).</p>



<h3>Step 3</h3>



<p>The optimization over <img alt="\mathbf Q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not trivial. Hence, we make some guesses about the structure of <img alt="\mathbf Q^\star" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which maximizes the exponent. This is where the *replica symmetry (RS) ansatz* comes in. Since the indices of <img alt="\mathbf{x}^{(a)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are arbitrary, one guess is that for all <img alt="a,b\geq 1" class="latex" src="https://s0.wp.com/latex.php?latex=a%2Cb%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> <img alt="Q_{a\neq b}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Ba%5Cneq+b%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> has the same value ,<img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In addition, for all <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="Q_{0,a}=\mu" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7B0%2Ca%7D%3D%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This is the RS ansatz — it assumes an equivalency between replicas. Rigorously showing whether this is indeed the case is challenging, but we can proceed with this assumption and see if the results are correct.</p>



<p>The maximization of <img alt="g(\mathbf{Q})" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cmathbf%7BQ%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is now over two scalars, <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Writing the maximum as <img alt="\mu^*" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="Q^\star" class="latex" src="https://s0.wp.com/latex.php?latex=Q%5E%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and using the replica identity</p>



<p><img alt="\langle \log Z \rangle = \lim_{n\rightarrow 0} \log \langle Z^n \rangle /n= \lim_{ n \rightarrow 0} Ng(\mu^*, Q^\star)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle+%3D+%5Clim_%7Bn%5Crightarrow+0%7D+%5Clog+%5Clangle+Z%5En+%5Crangle+%2Fn%3D+%5Clim_%7B+n+%5Crightarrow+0%7D+Ng%28%5Cmu%5E%2A%2C+Q%5E%5Cstar%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Setting the derivative of <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> w.r.t. them to zero yields two solutions.</p>



<p><strong>Bad Math Warning:</strong> Maximizing <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> w.r.t. <img alt="\mu,q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%2Cq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> requires checking that the solutions are indeed global minima, a laborious effort that has done for some models. We will assume them to be global minima.</p>



<p>For each solution, we can compute <img alt="\lim_{N\rightarrow \infty} \lim_{\beta\rightarrow \infty} \frac{1}{\beta N}\langle \log Z \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN%5Crightarrow+%5Cinfty%7D+%5Clim_%7B%5Cbeta%5Crightarrow+%5Cinfty%7D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Clangle+%5Clog+Z+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which will become what we are looking for after being differentiated w.r.t. <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We obtain an expression of <img alt="\langle \log Z \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The two solutions yield <img alt="\langle \log Z \rangle= \lambda + 1/\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle%3D+%5Clambda+%2B+1%2F%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\langle \log Z \rangle= 2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Clog+Z+%5Crangle%3D+2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, respectively. Differentiating each w.r.t. <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to get <img alt="\langle (v \cdot u)^2 \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%28v+%5Ccdot+u%29%5E2+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we have <img alt="1 - \lambda^{-2}" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+%5Clambda%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/ciDxSHD.png"/></figure>



<p>Which one is correct? We decide by checking whether the solutions (black line and blue line above) are sensible (“physical”). It can be verified that <img alt="\lim_{N\rightarrow \infty} \lim_{\beta\rightarrow \infty} \frac{1}{\beta N}\langle \log Z \rangle=\langle \lambda_\text{max} \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN%5Crightarrow+%5Cinfty%7D+%5Clim_%7B%5Cbeta%5Crightarrow+%5Cinfty%7D+%5Cfrac%7B1%7D%7B%5Cbeta+N%7D%5Clangle+%5Clog+Z+%5Crangle%3D%5Clangle+%5Clambda_%5Ctext%7Bmax%7D+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which is the largest eigenvalue of <img alt="\mathbf A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Clearly, it should be non-decreasing as a function of <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Thus, for <img alt="\lambda \leq 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cleq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we choose the <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> solution, and for <img alt="\lambda \geq 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cgeq+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> the <img alt="\lambda + 1 / \lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%2B+1+%2F+%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> solution. Thus, <img alt="\langle (v \cdot u)^2 \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%28v+%5Ccdot+u%29%5E2+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="1-\lambda^{-2}" class="latex" src="https://s0.wp.com/latex.php?latex=1-%5Clambda%5E%7B-2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in the two regimes, respectively.</p>



<h2>Appendix: Gaussian Integrals and Delta Function Representation</h2>



<p>We frequently encounter Gaussian integrals when using the replica method and it is often convenient to rely on basic integration results which we provide in this Appendix.</p>



<h4>Single Variable</h4>



<p>The simplest Gaussian integral is the following one dimensional integral</p>



<p><img alt="I(a) = \int_{-\infty}^\infty \exp\left( -\frac{a}{2} x^2 \right) dx" class="latex" src="https://s0.wp.com/latex.php?latex=I%28a%29+%3D+%5Cint_%7B-%5Cinfty%7D%5E%5Cinfty+%5Cexp%5Cleft%28+-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We can calculate the square of this quantity by changing to polar coordinates <img alt="x=r\cos\phi, y = r \sin \phi" class="latex" src="https://s0.wp.com/latex.php?latex=x%3Dr%5Ccos%5Cphi%2C+y+%3D+r+%5Csin+%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/><img alt="I(a)^2 = \int_{\mathbb{R}^2} \exp\left(-\frac{a}{2} \left( x^2 + y^2 \right) \right) dx dy = \int_{0}^{2\pi} d\phi \int_{0}^{\infty} r \exp\left( - \frac{a}{2} r^2 \right) dr = 2 \pi a^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=I%28a%29%5E2+%3D+%5Cint_%7B%5Cmathbb%7BR%7D%5E2%7D+%5Cexp%5Cleft%28-%5Cfrac%7Ba%7D%7B2%7D+%5Cleft%28+x%5E2+%2B+y%5E2+%5Cright%29+%5Cright%29+dx+dy+%3D+%5Cint_%7B0%7D%5E%7B2%5Cpi%7D+d%5Cphi+%5Cint_%7B0%7D%5E%7B%5Cinfty%7D+r+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+r%5E2+%5Cright%29+dr+%3D+2+%5Cpi+a%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We thus conclude that <img alt="I(a) = \sqrt{2\pi / a}" class="latex" src="https://s0.wp.com/latex.php?latex=I%28a%29+%3D+%5Csqrt%7B2%5Cpi+%2F+a%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Thus we find that the function <img alt="\sqrt{\frac{a}{2\pi}} e^{-\frac{a}{2} x^2 }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+e%5E%7B-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a normalized function over <img alt="(-\infty, \infty)" class="latex" src="https://s0.wp.com/latex.php?latex=%28-%5Cinfty%2C+%5Cinfty%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This integral can also be calculated with Mathematica or Sympy. Below is the result in Sympy.</p>



<p><code>from sympy import *<br/>from sympy.abc import a, b, x, y<br/>x = Symbol('x')<br/>integrate( exp( -a/2 <i> x</i>*2 ) , (x, -oo,oo))<br/></code></p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/k8OcjPa.png"/></figure>



<p>This agrees with our result since we were implicitly assuming <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> real and positive (<img alt="\arg a = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Carg+a+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>).</p>



<p>We can generalize this result to accomodate slightly more involved integrals which contain both quadratic and linear terms in the exponent. This exercise reduces to the previous case through simple completion of the square</p>



<p><img alt="I(a,b)=\int_{-\infty}^{\infty} \exp\left(-\frac{a}{2} x^2 \pm bx \right) dx = \exp\left( \frac{b^2}{2a} \right) \int_{-\infty}^{\infty} \exp\left( - \frac{a}{2} \left[ x \mp \frac{b}{a} \right]^2 \right) dx = \exp\left( \frac{b^2}{2a} \right) \sqrt{2\pi/a}" class="latex" src="https://s0.wp.com/latex.php?latex=I%28a%2Cb%29%3D%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+%5Cexp%5Cleft%28-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+bx+%5Cright%29+dx+%3D+%5Cexp%5Cleft%28+%5Cfrac%7Bb%5E2%7D%7B2a%7D+%5Cright%29+%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+%5Cleft%5B+x+%5Cmp+%5Cfrac%7Bb%7D%7Ba%7D+%5Cright%5D%5E2+%5Cright%29+dx+%3D+%5Cexp%5Cleft%28+%5Cfrac%7Bb%5E2%7D%7B2a%7D+%5Cright%29+%5Csqrt%7B2%5Cpi%2Fa%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We can turn this equality around to find an expression</p>



<p><img alt="\exp\left( \frac{b^2}{2a} \right) = \sqrt{\frac{a}{2\pi}} \int \exp\left( -\frac{a}{2} x^2 \pm b x \right) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+%5Cfrac%7Bb%5E2%7D%7B2a%7D+%5Cright%29+%3D+%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+%5Cint+%5Cexp%5Cleft%28+-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+b+x+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Viewed in this way, this formula allows one to transform a term quadratic in <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in the exponential function into an integral involving a term linear in <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This is known as the <a href="https://en.wikipedia.org/wiki/Hubbard%E2%80%93Stratonovich_transformation">Hubbard-Stratanovich</a> transformation. Taking <img alt="b" class="latex" src="https://s0.wp.com/latex.php?latex=b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be imaginary (<img alt="b=ik" class="latex" src="https://s0.wp.com/latex.php?latex=b%3Dik&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for real <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>), we find an alternative expression of a Gaussian function</p>



<p><img alt="\exp\left( - \frac{k^2}{2a} \right) = \sqrt{\frac{a}{2\pi}} \int \exp\left( -\frac{a}{2} x^2 \pm i k x \right) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+-+%5Cfrac%7Bk%5E2%7D%7B2a%7D+%5Cright%29+%3D+%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+%5Cint+%5Cexp%5Cleft%28+-%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+i+k+x+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h4>Delta Function Integral Representation</h4>



<p>A delta function <img alt="\delta(z)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be considered as a limit of a normalized mean-zero Gaussian function with variance taken to zero</p>



<p><img alt="\delta(z) = \lim_{a \to 0} \sqrt{\frac{1}{2\pi a}} \exp\left( - \frac{1}{2 a} z^2 \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%28z%29+%3D+%5Clim_%7Ba+%5Cto+0%7D+%5Csqrt%7B%5Cfrac%7B1%7D%7B2%5Cpi+a%7D%7D+%5Cexp%5Cleft%28+-+%5Cfrac%7B1%7D%7B2+a%7D+z%5E2+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We can now use the <a href="https://en.wikipedia.org/wiki/Hubbard%E2%80%93Stratonovich_transformation">Hubbard-Stratanovich</a> trick to rewrite the Gaussian function</p>



<p><img alt="\exp\left( - \frac{1}{2 a} z^2 \right) = \sqrt{\frac{a}{2\pi}} \int \exp\left( - \frac{a}{2} x^2 \pm i z x \right) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+-+%5Cfrac%7B1%7D%7B2+a%7D+z%5E2+%5Cright%29+%3D+%5Csqrt%7B%5Cfrac%7Ba%7D%7B2%5Cpi%7D%7D+%5Cint+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+i+z+x+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Thus we can relate the delta function to an integral</p>



<p><img alt="\delta(z) = \lim_{a \to 0} \frac{1}{2\pi} \int \exp\left( - \frac{a}{2} x^2 \pm i x z \right) dx = \frac{1}{2\pi} \int \exp\left( \pm i x z \right) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%28z%29+%3D+%5Clim_%7Ba+%5Cto+0%7D+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint+%5Cexp%5Cleft%28+-+%5Cfrac%7Ba%7D%7B2%7D+x%5E2+%5Cpm+i+x+z+%5Cright%29+dx+%3D+%5Cfrac%7B1%7D%7B2%5Cpi%7D+%5Cint+%5Cexp%5Cleft%28+%5Cpm+i+x+z+%5Cright%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This trick is routinely utilized to represent delta functions with integrals over exponential functions during a replica calculation. In particular, this identity is often used to enforce defintions of the order parameters <img alt="Q_{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in the problem. For example, in the least squares problem where <img alt="Q_{ab} = \frac{1}{N}\Delta_a \cdot \Delta_b" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D+%5Cfrac%7B1%7D%7BN%7D%5CDelta_a+%5Ccdot+%5CDelta_b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> we used</p>



<p><img alt="\delta(NQ_{ab} - \Delta_{a} \cdot \Delta_b) = \frac{1}{2\pi } \int d\hat{Q}_{ab} \exp\left( i N Q_{ab} \hat{Q}_{ab} - i \hat{Q}_{ab} \Delta_{a} \cdot \Delta_b \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%28NQ_%7Bab%7D+-+%5CDelta_%7Ba%7D+%5Ccdot+%5CDelta_b%29+%3D+%5Cfrac%7B1%7D%7B2%5Cpi+%7D+%5Cint+d%5Chat%7BQ%7D_%7Bab%7D+%5Cexp%5Cleft%28+i+N+Q_%7Bab%7D+%5Chat%7BQ%7D_%7Bab%7D+-+i+%5Chat%7BQ%7D_%7Bab%7D+%5CDelta_%7Ba%7D+%5Ccdot+%5CDelta_b+%5Cright%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h4>Multivariate Gaussian integrals</h4>



<p>We commonly encounter integrals of the following form<br/><img alt="I(M) = \int_{\mathbb{R}^n} \exp\left( - \frac{1}{2} \sum_{ab} M_{ab} x_a x_b \right) dx_1 dx_2 ... dx_n" class="latex" src="https://s0.wp.com/latex.php?latex=I%28M%29+%3D+%5Cint_%7B%5Cmathbb%7BR%7D%5En%7D+%5Cexp%5Cleft%28+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bab%7D+M_%7Bab%7D+x_a+x_b+%5Cright%29+dx_1+dx_2+...+dx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where matrix <img alt="M_{ab}" class="latex" src="https://s0.wp.com/latex.php?latex=M_%7Bab%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is symmetric and positive definite. An example is the data average in the least squares problem studied in this blog post where <img alt="M = (Q + \sigma^2 11^\top)^{-1} + \beta I" class="latex" src="https://s0.wp.com/latex.php?latex=M+%3D+%28Q+%2B+%5Csigma%5E2+11%5E%5Ctop%29%5E%7B-1%7D+%2B+%5Cbeta+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We can reduce this to a collection of one dimensional problems by computing the eigendecomposition of <img alt="M = \sum_{\rho} \lambda_\rho u_\rho u_\rho^\top" class="latex" src="https://s0.wp.com/latex.php?latex=M+%3D+%5Csum_%7B%5Crho%7D+%5Clambda_%5Crho+u_%5Crho+u_%5Crho%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. From this decomposition, we introduce variables</p>



<p><img alt="z_\rho = \sum_{a=1}^n u_{\rho,a} x_a" class="latex" src="https://s0.wp.com/latex.php?latex=z_%5Crho+%3D+%5Csum_%7Ba%3D1%7D%5En+u_%7B%5Crho%2Ca%7D+x_a&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>The transformation from <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is orthogonal so the determinant of the Jacobian has absolute value one. After changing variables, we therefore obtain the following decoupled integrals</p>



<p><img alt="I(M) = \int_{\mathbb{R}^n} \exp\left( -\frac{1}{2} \sum_{\rho} \lambda_\rho z_\rho^2 \right) dz_1...dz_n = \prod_{\rho=1}^n \int \exp\left( -\frac{\lambda_\rho}{2 } z_\rho^2 \right) dz_\rho = \prod_{\rho=1}^n \sqrt{\frac{2\pi}{\lambda_\rho}}" class="latex" src="https://s0.wp.com/latex.php?latex=I%28M%29+%3D+%5Cint_%7B%5Cmathbb%7BR%7D%5En%7D+%5Cexp%5Cleft%28+-%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7B%5Crho%7D+%5Clambda_%5Crho+z_%5Crho%5E2+%5Cright%29+dz_1...dz_n+%3D+%5Cprod_%7B%5Crho%3D1%7D%5En+%5Cint+%5Cexp%5Cleft%28+-%5Cfrac%7B%5Clambda_%5Crho%7D%7B2+%7D+z_%5Crho%5E2+%5Cright%29+dz_%5Crho+%3D+%5Cprod_%7B%5Crho%3D1%7D%5En+%5Csqrt%7B%5Cfrac%7B2%5Cpi%7D%7B%5Clambda_%5Crho%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Using the fact that the determinant is the product of eigenvalues <img alt="\det M = \prod_{\rho} \lambda_\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdet+M+%3D+%5Cprod_%7B%5Crho%7D+%5Clambda_%5Crho&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we have the following expression for the multivariate Gaussian integral</p>



<p><img alt="I(M) = \left(2 \pi \right)^{n/2} \det\left( M \right)^{-1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=I%28M%29+%3D+%5Cleft%282+%5Cpi+%5Cright%29%5E%7Bn%2F2%7D+%5Cdet%5Cleft%28+M+%5Cright%29%5E%7B-1%2F2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p></div>
    </content>
    <updated>2021-08-11T16:06:56Z</updated>
    <published>2021-08-11T16:06:56Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-08-20T23:38:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8163</id>
    <link href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-1-of-2/" rel="alternate" type="text/html"/>
    <title>Replica Method for the Machine Learning Theorist: Part 1 of 2</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Blake Bordelon, Haozhe Shan, Abdul Canatar, Boaz Barak, Cengiz Pehlevan [Boaz’s note: Blake and Haozhe were students in the ML theory seminar this spring; in that seminar we touched on the replica method in the lecture on inference and statistical physics but here Blake and Haozhe (with a little help from the rest of us) … <a class="more-link" href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-1-of-2/">Continue reading <span class="screen-reader-text">Replica Method for the Machine Learning Theorist: Part 1 of 2</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h4>Blake Bordelon, Haozhe Shan, Abdul Canatar, Boaz Barak, Cengiz Pehlevan</h4>



<p><em>[Boaz’s note: Blake and Haozhe were students in <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">the ML theory seminar</a> this spring; in that seminar we touched on the replica method in the <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">lecture on inference and statistical physics</a> but here Blake and Haozhe (with a little help from the rest of us) give a great overview of the method and its relations to ML. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a>.]</em></p>



<p>See also: <a href="https://boazbarak.org/Papers/replica.pdf">PDF version of both parts</a> and <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/">part 2 of this post</a>.</p>



<h2>I. Analysis of Optimization Problems with Statistical Physics</h2>



<p>In computer science and machine learning, we are often interested in solving optimization problems of the form</p>



<p><img alt="\min_{x \in \mathcal{S}} H(x,\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin_%7Bx+%5Cin+%5Cmathcal%7BS%7D%7D+H%28x%2C%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is an objective function which depends on our decision variables <img alt="x \in \mathcal S" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal+S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as well as on a set of problem-specific parameters <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Frequently, we encounter problems relevant to machine learning, where <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a random variable. The replica method is a useful tool to analyze <em>large problems</em> and their <em>typical</em> behavior over the distribution of <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Here are a few examples of problems that fit this form:</p>



<ol><li>In supervised learning, <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> may be a training loss, <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> a set of neural network weights and <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> the data points and their labels</li><li>We may want to find the most efficient way to visit all nodes on a graph. In this case <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> describes nodes and edges of the graph, <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a representation of the set of chosen edges, and <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be the cost of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> if <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> encodes a valid path and <img alt="\infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (or a very large number ) if it doesn’t encode a valid path.</li><li>Satisfiability: <img alt="x \in \{0,1\}^N" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B0%2C1%5C%7D%5EN&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a collection booleans which must satisfy a collection of constraints. In this case the logical constraints (clauses) are the parameters <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. <img alt="H(x)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be the number of constraints violated by <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li><li>Recovery of structure in noisy data: <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is our guess of the structure and <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are instances of the observed noisy data. For example PCA attempts to identify the directions of maximal variation in the data. With the replica method, we could ask how the accuracy of the estimated top eigenvector degrades with noise.</li></ol>



<h2>II. The Goal of the Replica Method</h2>



<p>The <strong>replica method</strong> is a way to calculate the value of some statistic (<strong>observable</strong> in physics-speak) <img alt="O(x)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a “typical” minimizer of <img alt="H(x,\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=H%28x%2C%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a “typical” value for the parameters (which are also known in physics-speak as the <strong>disorder</strong>).</p>



<p>In Example 1 (supervised learning), the observable may be the generalization error of a chosen algorithm (e.g. a linear classifier) on a given dataset. For Example 2 (path), this could be the cost of the best path. For Example 3 (satisfiability), the observable might be whether or not a solution exists at all for the problem. In Example 4 (noisy data), the observable might be the quality of decoded data (distance from ground truth under some measure).</p>



<p>An observable like generalization error obviously depends on <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the problem instance. However, can we say something more general about this <em>type of problem</em>? In particular, if <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> obeys some probability distribution, is it possible to characterize the the typical observable over different problem instances <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>?</p>



<p>For instance, in Example 1, we can draw all of our training data from a distribution. For each random sample of data points <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we find the set of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which minimize <img alt="H(x, \mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28x%2C+%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and compute a generalization error. We then repeat this procedure many times and average the results. Sometimes, there are multiple <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that minimize <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for a given sample of <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>; this requires averaging the observable over all global minima for each <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> first, before averaging over different <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>To give away a “spolier”, towards the end of this note, we will see how to use the replica method to give accurate predictions of performance for noisy least square fitting and spiked matrix recovery.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/BKjGG3O.png"/></figure>



<p><em>Generalization gap in least squares ridge regression, figure taken from <a href="https://arxiv.org/abs/2006.13198">Canatar, Bordelon, and Pehlevan</a></em></p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Jrth8N3.png"/></figure>



<p><em>Performance (agreement with planted signal) as function of signal strength for spiked matrix recovery, as the dimension grows, the experiment has stronger agreement with theory. See also <a href="https://meisong541.github.io/jekyll/update/2019/08/04/Replica_method_1.html">Song Mei’s exposition</a></em></p>



<h3>A. What do we actually do?</h3>



<p>Now that we are motivated, let’s see what quantities the replica method attempts to obtain. In general, given some observable <img alt="O(x,\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28x%2C%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the average of <img alt="O" class="latex" src="https://s0.wp.com/latex.php?latex=O&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> over a minimizer <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> chosen at random from the set <img alt="\text{arg} \min H(x,\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Barg%7D+%5Cmin+H%28x%2C%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and take the average of this quantity over the choice of the disorder <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>In other words, we want to compute the following quantity:</p>



<p><img alt="\text{Desired quantity } = \mathbb{E}_{\mathcal{D}} \mathbb{E}_{x \in \text{arg}\min H(x,\mathcal{D})} \left[ O(x, \mathcal{D}) \right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Cmathbb%7BE%7D_%7B%5Cmathcal%7BD%7D%7D+%5Cmathbb%7BE%7D_%7Bx+%5Cin+%5Ctext%7Barg%7D%5Cmin+H%28x%2C%5Cmathcal%7BD%7D%29%7D+%5Cleft%5B+O%28x%2C+%5Cmathcal%7BD%7D%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>The above equation has two types of expectation- over the disorder <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and over the minimizers <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>The physics convention is to</p>



<ul><li>use <img alt="\left&lt; f \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+f+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for the expectation of a function <img alt="f(\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> over the disorder <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></li><li>use <img alt="\int g(x) \mu(x) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+g%28x%29+%5Cmu%28x%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for the expectation of a function <img alt="g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> over <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> chosen according to some measure <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li></ul>



<p>Using this notation, we can write the above as</p>



<p><img alt="\text{Desired quantity } = \left&lt; \int p^*(x;\mathcal D) O(x,\mathcal D) dx \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Cleft%3C+%5Cint+p%5E%2A%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\left&lt; \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> denotes an average over the probability measure for problem parameters <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and <img alt="p^*(x;\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p%5E%2A%28x%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a uniform distribution over the set of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that minimize <img alt="H(x,\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=H%28x%2C%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with zero probability mass placed on sub-optimal points.</p>



<p>The ultimate goal of the replica method is to express</p>



<p><img alt="\text{Desired quantity } = \text{solution of optimization on constant number of variables}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Ctext%7Bsolution+of+optimization+on+constant+number+of+variables%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>but it will take some time to get there.</p>



<h3>B. The concept of “self-averaging” and concentration</h3>



<p>Above, we glossed over an important distinction between the “typical” value of <img alt="f(\mathcal D) = \int p^*(x; \mathcal D) O(x,\mathcal D) dx" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal+D%29+%3D+%5Cint+p%5E%2A%28x%3B+%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the *average* value of <img alt="f(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This is OK only when we have <em>concentration</em> in the sense that with high probability over the choice of <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="f(\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is close to its expected value. We define this as the property that with probability at least <img alt="1-\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1-%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the quantity <img alt="f(\mathcal{D})" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cmathcal%7BD%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is within a <img alt="1\pm \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1%5Cpm+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> multiplicative factor of its expectation, whwere <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is some quantity that goes to zero as the system size grows. A quantity <img alt="f(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that concentrates in this sense is called <strong>self averaging</strong>.</p>



<p>For example, suppose that <img alt="X= \sum_{i=1}^n X_i" class="latex" src="https://s0.wp.com/latex.php?latex=X%3D+%5Csum_%7Bi%3D1%7D%5En+X_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where each <img alt="X_i" class="latex" src="https://s0.wp.com/latex.php?latex=X_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> equals <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with probabilty <img alt="1/2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with probability <img alt="1/2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> independently. Standard Chernoff bounds show that with high probability <img alt="X \in [n/2 \pm O(\sqrt{n})]" class="latex" src="https://s0.wp.com/latex.php?latex=X+%5Cin+%5Bn%2F2+%5Cpm+O%28%5Csqrt%7Bn%7D%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> or <img alt="\tfrac{X}{\mathbb{E} X} \in \left(1 + O(\tfrac{1}{\sqrt n})\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctfrac%7BX%7D%7B%5Cmathbb%7BE%7D+X%7D+%5Cin+%5Cleft%281+%2B+O%28%5Ctfrac%7B1%7D%7B%5Csqrt+n%7D%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Hence <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a self averaging quantity.</p>



<p>In contrast the random variable <img alt="Y=2^X" class="latex" src="https://s0.wp.com/latex.php?latex=Y%3D2%5EX&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not self averaging. Since <img alt="Y = \prod_{i=1}^n 2^{X_i}" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cprod_%7Bi%3D1%7D%5En+2%5E%7BX_i%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and these random variables are independent, we know that <img alt="\mathbb{E} Y = \prod_{i=1}^n \mathbb{E} 2^{X_i} = \left(\tfrac{1}{2} 2^1 + \tfrac{1}{2} 2^0 \right)^n = (3/2)^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+Y+%3D+%5Cprod_%7Bi%3D1%7D%5En+%5Cmathbb%7BE%7D+2%5E%7BX_i%7D+%3D+%5Cleft%28%5Ctfrac%7B1%7D%7B2%7D+2%5E1+%2B+%5Ctfrac%7B1%7D%7B2%7D+2%5E0+%5Cright%29%5En+%3D+%283%2F2%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. However, with high probability a typical value of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> will be of the form <img alt="2^{n/2 \pm O(\sqrt{n})} = \sqrt{2}^{n \pm O(\sqrt n)}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bn%2F2+%5Cpm+O%28%5Csqrt%7Bn%7D%29%7D+%3D+%5Csqrt%7B2%7D%5E%7Bn+%5Cpm+O%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Since <img alt="\sqrt{2} &lt; 3/2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B2%7D+%3C+3%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> we see that the typical value of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is exponentially smaller than the expected value of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>The example above is part of a more general pattern. Often even if a variable <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not self averaging, the variable <img alt="X = \log Y" class="latex" src="https://s0.wp.com/latex.php?latex=X+%3D+%5Clog+Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> will be self-averaging. Hence if we are interested in the typical value of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the quantity <img alt="\exp\left( \mathbb{E} [\log Y] \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+%5Cmathbb%7BE%7D+%5B%5Clog+Y%5D+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is more representative than the quantity <img alt="\mathbb{E}[Y]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BY%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h3>C. When is using the replica method a good idea?</h3>



<p>Suppose that we want to compute a quantity of the form above. When is it a good idea to use the replica method to do so?<br/>Generally, we would want it to satisfy the following conditions:</p>



<ol><li>The learning problem is high dimensional with a large budget of data. The replica method describes a <em>thermodynamic limit</em> where the system size and data budget are taken to infinity with some fixed ratio between the two quantities. Such a limit is obviously never achieved in reality, but in practice sufficiently large learning problems can be accurately modeled by the method.</li><li>The loss or the constraints are convenient functions of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Typically <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> will be a low degree polynomial or a sum of local functions (each depending on small number of variables) in <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li><li>Averages over the disorder in <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are tractable analytically. That is, we can compute marginals of the distribution over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li><li>The statistic that we are interested in is self-averaging.</li></ol>



<p>If the above conditions aren’t met, it is unlikely that this problem will gain much analytical insight from the replica method.</p>



<h2>III. The Main Conceptual Steps Behind Replica Calculations</h2>



<p>We now describe the conceptual steps that are involved in calculating a quantity using the replica method.<br/>They are also outlined in this figure:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/eVOI8EA.png"/></figure>



<h3>Step 1:”Softening” Constraints with the Gibbs Measure</h3>



<p>The uniform measure on minimizers <img alt="p^*(x;\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p%5E%2A%28x%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is often difficult to work with. To aid progress, we can think of it as a special case of what is known as the <a href="https://en.wikipedia.org/wiki/Gibbs_measure">Gibbs measure</a>, defined as<br/><img alt="p_\beta(x;\mathcal D)dx= \frac{1}{Z(\mathcal D)}\exp \left( -\beta H(x,\mathcal{D})\right)dx" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28x%3B%5Cmathcal+D%29dx%3D+%5Cfrac%7B1%7D%7BZ%28%5Cmathcal+D%29%7D%5Cexp+%5Cleft%28+-%5Cbeta+H%28x%2C%5Cmathcal%7BD%7D%29%5Cright%29dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="Z(\mathcal D) = \int \exp\left(-\beta {H}(x, \mathcal D) \right)dx" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%29+%3D+%5Cint+%5Cexp%5Cleft%28-%5Cbeta+%7BH%7D%28x%2C+%5Cmathcal+D%29+%5Cright%29dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the normalization factor, or <strong>partition function</strong>. <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is called the <strong>inverse temperature</strong>, a name from thermodynamics. It is easy to see that when <img alt="\beta\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (i.e., when the temperature tends to the absolute zero), the Gibbs measure converges to a uniform distribution on the minimizers of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>: <img alt="p_{\beta} \to p^*" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7B%5Cbeta%7D+%5Cto+p%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Hence we can write</p>



<p><img alt="\text{Desired quantity } = \left&lt; \int p^*(x;\mathcal D) O(x,\mathcal D) dx \right&gt;{\mathcal D} = \left&lt; \lim_{\beta \rightarrow \infty} \int p_\beta(x;\mathcal D) O(x,\mathcal D) dx \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Cleft%3C+%5Cint+p%5E%2A%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E%7B%5Cmathcal+D%7D+%3D+%5Cleft%3C+%5Clim_%7B%5Cbeta+%5Crightarrow+%5Cinfty%7D+%5Cint+p_%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Physicists often exchange the order of limits and expectations at will, which generally makes sense in this setting, and so assume</p>



<p><img alt="\text{Desired quantity } = \lim_{\beta \rightarrow \infty} \left&lt; \int p_\beta(x;\mathcal D) O(x,\mathcal D) dx \right&gt;_{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Clim_%7B%5Cbeta+%5Crightarrow+%5Cinfty%7D+%5Cleft%3C+%5Cint+p_%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx+%5Cright%3E_%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Thus general approach taken in the replica method is to derive an expression for the average observable for any <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and then take the <img alt="\beta \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit. The quantity <img alt="\int p_\beta(x;\mathcal D) O(x,\mathcal D) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+p_%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is also known as the <em>thermal average</em> of <img alt="O" class="latex" src="https://s0.wp.com/latex.php?latex=O&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, since it is taken with respect to the Gibbs distribution at some positive temperature.</p>



<p>To compute the thermal average of <img alt="O" class="latex" src="https://s0.wp.com/latex.php?latex=O&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we define the following <em>augmented partition function</em>:</p>



<p><img alt="Z(\mathcal D, J) = \int_{S} \exp\left( -\beta H(x,\mathcal D) + J O(x; \mathcal D) \right) dx." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%2C+J%29+%3D+%5Cint_%7BS%7D+%5Cexp%5Cleft%28+-%5Cbeta+H%28x%2C%5Cmathcal+D%29+%2B+J+O%28x%3B+%5Cmathcal+D%29+%5Cright%29+dx.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>One can then check that</p>



<p><img alt="\frac{d}{dJ} \log Z(\mathcal{D}, J)\Big|_{J=0}= \frac{1}{Z} \int O(x,\mathcal D) \exp(- \beta H(x,\mathcal D) ) dx = \int p\beta(x;\mathcal D) O(x,\mathcal D) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7Bd%7D%7BdJ%7D+%5Clog+Z%28%5Cmathcal%7BD%7D%2C+J%29%5CBig%7C_%7BJ%3D0%7D%3D+%5Cfrac%7B1%7D%7BZ%7D+%5Cint+O%28x%2C%5Cmathcal+D%29+%5Cexp%28-+%5Cbeta+H%28x%2C%5Cmathcal+D%29+%29+dx+%3D+%5Cint+p%5Cbeta%28x%3B%5Cmathcal+D%29+O%28x%2C%5Cmathcal+D%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Hence our desired quantity can be obtained as</p>



<p><img alt="\text{Desired quantity } = \lim_{\beta \to \infty} \frac{\partial}{\partial J} \left&lt; \log Z(\mathcal D, J) \right&gt;_{\mathcal D}(0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Clim_%7B%5Cbeta+%5Cto+%5Cinfty%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+J%7D+%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+J%29+%5Cright%3E_%7B%5Cmathcal+D%7D%280%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>or (assuming we can again exchange limits at will):</p>



<p><img alt="\text{Desired quantity } = \lim_{\epsilon \to 0} \tfrac{1}{\epsilon}\left[ \lim_{\beta \to \infty} \left&lt; \log Z(\mathcal D, \epsilon) \right&gt;{\mathcal D} - \lim_{\beta \to \infty} \left&lt; \log Z(\mathcal D, 0) \right&gt;_{\mathcal D}\right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BDesired+quantity+%7D+%3D+%5Clim_%7B%5Cepsilon+%5Cto+0%7D+%5Ctfrac%7B1%7D%7B%5Cepsilon%7D%5Cleft%5B+%5Clim_%7B%5Cbeta+%5Cto+%5Cinfty%7D+%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+%5Cepsilon%29+%5Cright%3E%7B%5Cmathcal+D%7D+-+%5Clim_%7B%5Cbeta+%5Cto+%5Cinfty%7D+%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+0%29+%5Cright%3E_%7B%5Cmathcal+D%7D%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We see that ultimately computing the desired quantity reduces to computing quantities of the form</p>



<p><img alt="\left&lt; \log Z'(\mathcal{D}) \right&gt;_{\mathcal D}\;\; (\star)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z%27%28%5Cmathcal%7BD%7D%29+%5Cright%3E_%7B%5Cmathcal+D%7D%5C%3B%5C%3B+%28%5Cstar%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>for the original or modified partition function <img alt="Z'" class="latex" src="https://s0.wp.com/latex.php?latex=Z%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Hence our focus from now on will be on computing (<img alt="\star" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cstar&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>).<br/>Averaging over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is known as “configurational average” or <strong>quenched average</strong>. All together, we obtain the observable, first thermal averaged to get <img alt="O^*(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=O%5E%2A%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (averaged over <img alt="p^*(x;\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p%5E%2A%28x%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) and then quenched averaged over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<blockquote class="wp-block-quote"><p><img alt="&#x26A0;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a0.png" style="height: 1em;"/> <em>What Concentrates?</em>: It is not just an algebraic convenience to average <img alt="\log Z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> instead of averaging <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> itself. When the system size <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is large, <img alt="\frac{1}{N} \log Z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D+%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> concentrates around its average. Thus, the typical behavior of the system can be understood by studying the quenched average <img alt="\frac{1}{N} \left&lt; \log Z \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D+%5Cleft%3C+%5Clog+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> The partition function <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> itself often does not concentrate and in general the values <img alt="\frac{1}{N} \log \left&lt; Z \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D+%5Clog+%5Cleft%3C+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (known as the “annealed average”) and <img alt="\frac{1}{N}\left&lt; \log Z \right&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D%5Cleft%3C+%5Clog+Z+%5Cright%3E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (known as the “quenched average”) could differ subtantially. For more information, please consult <a href="https://web.stanford.edu/~montanar/RESEARCH/book.html">Mezard and Montanari’s</a> excellent book, Chapter 5.</p></blockquote>



<h3>Step 2: The Replica Trick</h3>



<p>Hereafter, we use <img alt="\langle \cdot \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Ccdot+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to denote the average and drop the dependence of <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. To compute <img alt="\left&lt; \log Z(\mathcal D, J) \right&gt;{\mathcal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z%28%5Cmathcal+D%2C+J%29+%5Cright%3E%7B%5Cmathcal+D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we use an identity <img alt="\left&lt; \log Z \right&gt; = \lim_{n \to 0} \frac{1}{n} \log \left&lt; Z^n \right&gt;." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Clog+Z+%5Cright%3E+%3D+%5Clim_%7Bn+%5Cto+0%7D+%5Cfrac%7B1%7D%7Bn%7D+%5Clog+%5Cleft%3C+Z%5En+%5Cright%3E.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>For the limit to make sense, <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> should be any real number. However, the expression for <img alt="Z^n" class="latex" src="https://s0.wp.com/latex.php?latex=Z%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is only easily computable for natural numbers. <img alt="&#x26A0;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a0.png" style="height: 1em;"/> This step is non-rigorous: we will obtain an expression for <img alt="\log \langle Z^n \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+%5Clangle+Z%5En+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for natural number <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and then take the <img alt="n \rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit after the fact.</p>



<p>Recall that under the Gibbs distribution <img alt="p_\beta(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the probability density on state <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is equal to <img alt="\exp(-\beta H(x,\mathcal D) )/Z(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+H%28x%2C%5Cmathcal+D%29+%29%2FZ%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Denote by <img alt="p_\beta(\mathcal D)^n" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> the probability distribution over a tuple <img alt="\vec{x} = (x^{(1)},\ldots,x^{(n)})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%3D+%28x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> independent samples (also known as <strong>replicas</strong>) chosen from <img alt="p_\beta(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Since the partition function <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is an integral (or sum in the discrete case) of the form <img alt="\int \exp(-\beta H(x;\mathcal D)) dx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+%5Cexp%28-%5Cbeta+H%28x%3B%5Cmathcal+D%29%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we can write <img alt="Z(\mathcal D)^n" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathcal+D%29%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the integral of <img alt="\prod_{a=1}^n \exp(-\beta H(x^{(a)};\mathcal D))= \exp\left( - \beta \sum_{a=1}^n H(x^{(a)}, \mathcal D)\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cprod_%7Ba%3D1%7D%5En+%5Cexp%28-%5Cbeta+H%28x%5E%7B%28a%29%7D%3B%5Cmathcal+D%29%29%3D+%5Cexp%5Cleft%28+-+%5Cbeta+%5Csum_%7Ba%3D1%7D%5En+H%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="x^{(1)},\ldots,x^{(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are independent variables.</p>



<p>Now since each <img alt="x^{(a)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%28a%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is weighed with a factor of <img alt="\exp(-\beta H(x^{(a)});\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%5Cbeta+H%28x%5E%7B%28a%29%7D%29%3B%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, this expression can be shown as equal to taking expectation of some exponential function <img alt="\exp( \sum_{a=1}^n G(x^{(a)}; \mathcal D))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%3B+%5Cmathcal+D%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> over a tuple <img alt="(x^{(1)},\ldots,x^{(n)})" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> independent samples of <strong>replicas</strong> all coming from the same Gibbs distribution <img alt="p_\beta(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> corresponding to the same instance <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>(The discussion on <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is just for intuition – we will not care about the particular form of this <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, since soon average it over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.)</p>



<p>Hence</p>



<p><img alt="\left&lt; Z^n \right&gt; = \left&lt; \int_{\vec{x} \sim p_*(\mathcal D)^n} \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) dx \right) \right&gt;_{\mathcal D}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cleft%3C+%5Cint_%7B%5Cvec%7Bx%7D+%5Csim+p_%2A%28%5Cmathcal+D%29%5En%7D+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+dx+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>Step 3: The Order Parameters</h3>



<p>The above expression is an expectation of an integral, and so we can switch the order of summation, and write it also as</p>



<p><img alt="\left&lt; Z^n \right&gt; = \int_{\vec{x} \sim p_*(\mathcal D)^n} \left&lt; \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) \right) \right&gt;_{\mathcal D} dx." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint_%7B%5Cvec%7Bx%7D+%5Csim+p_%2A%28%5Cmathcal+D%29%5En%7D+%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D+dx.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>It turns out that for natural energy functions (for example when <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is quadratic in <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> such as when it corresponds to Mean Squared Error loss), for any tuple of <img alt="x^{(1)},\ldots,x^{(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the expectation over <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of <img alt="\exp\left( - \beta \sum_{a=1}^n H(x^{(a)}, \mathcal D) \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%5Cleft%28+-+%5Cbeta+%5Csum_%7Ba%3D1%7D%5En+H%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> only depends on the angles between the <img alt="x^(a)" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%28a%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>‘s.<br/>That is, rather than depending on all of these <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>-dimensional vectors, it only depends on the <img alt="n^2" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> coefficients <img alt="Q_{ab} =\frac{1}{N} x^{(a)}\cdot x^{(b)}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bab%7D+%3D%5Cfrac%7B1%7D%7BN%7D+x%5E%7B%28a%29%7D%5Ccdot+x%5E%7B%28b%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The <img alt="n\times n" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix Q is known as the <strong>overlap matrix</strong> or <strong>order parameters</strong> and one can often find a nice analytical function <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> whose values are bounded (independently of <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) such that</p>



<p><img alt="\left&lt; \exp\left( - \sum_{a=1}^n G(x^{(a)}, \mathcal D) \right) \right&gt;_{\mathcal D} = \exp(-nN \mathcal{F}(Q))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+%5Cexp%5Cleft%28+-+%5Csum_%7Ba%3D1%7D%5En+G%28x%5E%7B%28a%29%7D%2C+%5Cmathcal+D%29+%5Cright%29+%5Cright%3E_%7B%5Cmathcal+D%7D+%3D+%5Cexp%28-nN+%5Cmathcal%7BF%7D%28Q%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Hence we can replace the integral over <img alt="x^{(1)},\ldots,x^{(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%7B%281%29%7D%2C%5Cldots%2Cx%5E%7B%28n%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with an integral over <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and write</p>



<p><img alt="\left&lt; Z^n \right&gt; = \int dQ \exp\left(- n N \mathcal F(Q) \right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cint+dQ+%5Cexp%5Cleft%28-+n+N+%5Cmathcal+F%28Q%29+%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where the measure <img alt="dQ" class="latex" src="https://s0.wp.com/latex.php?latex=dQ&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the one induced by the overlap distribution of a tuple <img alt="\vec{x} \sim p_\beta(\mathcal D)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%5Csim+p_%5Cbeta%28%5Cmathcal+D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> taken for a random choice of the parameters <img alt="\mathcal D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Since <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> only ranges over a small (<img alt="n^2" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> dimensional set), at the large <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> limit, the integral is dominated by the maximum of its integrand (“method of steepest descent” / “saddle point method”). Let <img alt="Q^*" class="latex" src="https://s0.wp.com/latex.php?latex=Q%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be the global minimum of <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (within some space of matrices). We have</p>



<p><img alt="\lim_{N \rightarrow \infty}\left&lt; Z^n \right&gt; = \exp(-n N \mathcal F(Q^* ))." class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7BN+%5Crightarrow+%5Cinfty%7D%5Cleft%3C+Z%5En+%5Cright%3E+%3D+%5Cexp%28-n+N+%5Cmathcal+F%28Q%5E%2A+%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Once we arrive at this expression, the configurational average of <img alt="-\log Z" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Clog+Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is simply <img alt="N \mathcal F(Q^*)" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Cmathcal+F%28Q%5E%2A%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. These steps constitute the replica method. The ability to compute the configurational average by creating an appropriate <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is one of the factors determining whether the replica method can be used. For example, in the supervised learning example, <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is almost always assumed to be quadratic in <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>; cross-entropy loss, for instance, is generally not amendable.</p>



<blockquote class="wp-block-quote"><p><img alt="&#x26A0;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26a0.png" style="height: 1em;"/> <em>Bad Math Warning</em>: there are three limits, <img alt="\beta \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="N \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and <img alt="n \rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In replica calculations, we assume that we take take these limits in whichever order that is arithmetically convenient.</p></blockquote>



<p><strong>Coming up:</strong> In <a href="https://windowsontheory.org/2021/08/11/replica-method-for-the-machine-learning-theorist-part-2-of-2/">part two of this blog post</a>, we will explain the replica symmetric assumption (or “Ansatz” in Physics-speak) on the order parameters <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and then demonstrate how to use the replica method for two simple examples: <em>least squares regression</em> and <em>spiked matrix model</em>.</p></div>
    </content>
    <updated>2021-08-11T15:00:11Z</updated>
    <published>2021-08-11T15:00:11Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-08-20T23:38:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-7046389272941836220</id>
    <link href="http://processalgebra.blogspot.com/feeds/7046389272941836220/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=7046389272941836220" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7046389272941836220" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7046389272941836220" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/08/the-first-movie.html" rel="alternate" type="text/html"/>
    <title>The First: A Movie</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I had the great pleasure to watch <a href="https://pantar.com/" target="_blank">Ali Hossaini</a> and <a href="http://www.lucavigano.com/" target="_blank">Luca Viganò</a>'s short movie "<a href="https://www.nationalgallery.org.uk/national-gallery-x/the-ai-gallery" target="_blank">The First</a>" that has been released by National Gallery X. "The First" is a coproduction of <a href="https://www.tas.ac.uk/" target="_blank">UKRI TAS Hub</a>, <a href="https://rusi.org/" target="_blank">RUSI </a>and <a href="https://www.nationalgallery.org.uk/national-gallery-x" target="_blank">National Gallery X</a>. It was produced as a keynote for <a href="https://www.tas.ac.uk/bigeventscpt/trusting-machines/" target="_blank">Trusting Machines?</a>, a conference on how to develop trustworthy AI. </p><p>For the little that it may be worth, I strongly recommend the movie. Do watch also the <a href="https://vimeo.com/577412746" target="_blank">conversation</a> between National Gallery X co-director Ali Hossaini and Luca Viganò, possibly before enjoying a second viewing of the movie. You can also read the paper "<a href="https://arxiv.org/pdf/1807.06078.pdf" target="_blank">Gnirut: The Trouble With Being Born Human In An Autonomous World</a>" mentioned in that conversation.  </p><p>I fully subscribe to Luca Viganò's vision of using artistic tools to explain computer science concepts to the public, whose members will have to make use of the technological artifacts based on those concepts. Indeed, we live in a world in which technologists will increasingly have to be great humanists. IMHO, we are lucky to have people like Luca Viganò, who is also a playwright, paving the way in connecting "<a href="https://en.wikipedia.org/wiki/The_Two_Cultures" target="_blank">The Two Cultures</a>". (In case any of you is interested, I recommend Luca Viganò's  <a href="https://us02web.zoom.us/rec/play/3uNdz1F0g1JCAir0C10Y3_jVs7k6fJgrIwr5RomnzBJGTjqBMOYv6SJgD2jINwMTuBvx-CSsHclof5Vn.29XNpJA7FKbfL5dG?autoplay=true&amp;startTime=1614787555000" target="_blank">GSSI+ICE-TCS webinar</a>.)</p><p> </p></div>
    </content>
    <updated>2021-08-11T09:38:00Z</updated>
    <published>2021-08-11T09:38:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-08-15T21:11:11Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/08/10/relandscaping</id>
    <link href="https://11011110.github.io/blog/2021/08/10/relandscaping.html" rel="alternate" type="text/html"/>
    <title>Relandscaping</title>
    <summary>We recently redid our front yard, after spending too long with a boring flat weedy mostly-unused and water-thirsty lawn and after our neighbor took out the only part of it that we actually cared for, a liquidambar tree between our yards. This week’s WADS/CCCG conference, with its four-hour time difference from Pacific, gave me an excuse to catch it in the early morning light:</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We recently redid our front yard, after spending too long with a boring flat weedy mostly-unused and water-thirsty lawn and after our neighbor took out the only part of it that we actually cared for, a liquidambar tree between our yards. This week’s WADS/CCCG conference, with its four-hour time difference from Pacific, gave me an excuse to catch it in the early morning light:</p>

<p style="text-align: center;"><img alt="My house with its old front lawn" src="https://www.ics.uci.edu/~eppstein/pix/frontyard/frontyard-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p>I don’t have a photo of the old front, so for comparison here’s a similar angle snagged from Google Street View.</p>

<p style="text-align: center;"><img alt="My house with its old front lawn" src="https://www.ics.uci.edu/~eppstein/pix/frontyard/lawn.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>

<p>More for my own later benefit than because I imagine anyone else cares:</p>

<p>The frontmost tree is a <a href="https://en.wikipedia.org/wiki/Parkinsonia_florida">palo verde</a>; a neighbor across the street has a much bigger one, a little messy but quite pretty, especially when covered with its yellow flowers. The tree in the back corner with the blue bistro table under it is some kind of mesquite, I think maybe a black mesquite; the flagstone path makes a loop around it. The two tallest bushes, forming a quadrilateral with the two trees, are <a href="https://en.wikipedia.org/wiki/Feijoa_sellowiana">pineapple guavas / feijoas</a>.</p>

<p>Behind the palo verde and against the house are some <a href="https://en.wikipedia.org/wiki/Kangaroo_paw">kangaroo paws</a>. The closest medium-sized bush on the bottom right (and elsewhere) is <a href="https://en.wikipedia.org/wiki/Salvia_yangii">Russian sage</a>. There are also some <a href="https://en.wikipedia.org/wiki/Buddleja_davidii">butterfly bushes</a>, and several other low bushes and flowers whose names I already lost track of or didn’t get. I think the tall bunchgrass may be a <a href="https://en.wikipedia.org/wiki/Lomandra">lomandra</a>, and the low blue ones <a href="https://en.wikipedia.org/wiki/Festuca_glauca">blue fescue</a>.</p>

<p>Lining the path to the door is <a href="https://en.wikipedia.org/wiki/Curio_repens">senecio / blue chalksticks</a>, and there’s more of it around the base of the mesquite. The groundcover in front is <a href="https://en.wikipedia.org/wiki/Dymondia">dymondia / silver carpet</a> (well, and some crabgrass and spurge, but let’s not count that), with some <a href="https://11011110.github.io/blog/2021/08/10/Myoporum parvifolium">myoporum parvifolium</a> behind it (the low spreading groundcover with white flowers); there’s more myoporum barely visible near the porch bench.</p>

<p>The neighbor ended up replacing the liquidambar (at the far left of the Street View shot) with a <a href="https://11011110.github.io/blog/2021/08/10/Podocarpus henkelii">podocarpus henkelii</a> and some bottlebrushes.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106735698135397706">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-08-10T21:24:00Z</updated>
    <published>2021-08-10T21:24:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-08-18T23:52:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19041</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/08/10/p-vs-np-proof-claims/" rel="alternate" type="text/html"/>
    <title>P vs NP Proof Claims</title>
    <summary>Ken Ribet once was sent a freebie book that he looked at and decided he didn’t want, so took it to a second hand bookstore on his lunch break, sold it, and bought lunch with the proceeds. On the way back to the math department he realized he’d turned theorems into coffee. IAS page Robbert […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
Ken Ribet once was sent a freebie book that he looked at and decided he didn’t want, so took it to a second hand bookstore on his lunch break, sold it, and bought lunch with the proceeds. On the way back to the math department he realized he’d turned theorems into coffee.<br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p/><p>
</p><p/>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/08/10/p-vs-np-proof-claims/dijkgraafias/" rel="attachment wp-att-19043"><img alt="" class="alignright size-full wp-image-19043" height="144" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/08/DijkgraafIAS.jpg?resize=120%2C144&amp;ssl=1" width="120"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">IAS <a href="https://www.ias.edu/scholars/dijkgraaf">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Robbert Dijkgraaf is a mathematical physicist and the current Director of the Institute for Advanced Study in Princeton. He is also a down-to-earth communicator of mathematics and a <a href="https://www.ias.edu/news/dijkgraaf-blackhole-mural">spacy</a> surrealist artist. He wrote a guest <a href="https://www.quantamagazine.org/the-subtle-art-of-the-mathematical-conjecture-20190507/">column</a> for <em>Quanta</em> two years ago titled, “The Subtle Art of the Mathematical Conjecture.”</p>
<p>
Today I talk again about what I feel is a meta-error in all the claims to resolve our favorite conjecture, <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Or <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if you are so inclined.</p>
<p>
I have an issue with them that goes beyond well-noted <a href="https://www.scottaaronson.com/blog/?p=458">advice</a> by Scott Aaronson on how to tell claimed proofs of <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are wrong. It is not about whether they are incorrect. They all are incorrect. And I believe that they will continue to be so into the future.</p>
<p>
My problem with these claims is: they are always all or nothing. </p>
<p>
The claims never improve what we know about <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> versus <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. They always resolve the entire question. There is no partial result, no improvement of what we know. They always solve the conjecture and are ready to get the $1,000,000 dollars. Of course the prize money is not in any jeopardy. </p>
<p>
</p><p/><h2> Climbing a Mountain </h2><p/>
<p/><p>
This is where Dijkgraaf’s article comes in. He talks first about mountain climbing (as we have <a href="https://rjlipton.wpcomstaging.com/2010/02/02/climbing-mountains-and-proving-theorems/">also</a> <a href="https://rjlipton.wpcomstaging.com/2012/12/08/mounting-or-solving-open-problems/">done</a>) rather than art:</p>
<blockquote><p><b> </b> <em> Mountain climbing is a beloved metaphor for mathematical research. … [T]he role of these highest peaks is played by the great conjectures—sharply formulated statements that are most likely true but for which no conclusive proof has yet been found.</em></p><em>
</em><p><em>
The highest summits are not conquered in a single effort. Climbing expeditions carefully lay out base camps and fixed ropes, then slowly work their way to the peak. Similarly, in mathematics one often needs to erect elaborate structures to attack a major problem. A direct assault is seen as foolish and naive. These auxiliary mathematical constructions can sometimes take centuries to build and in the end often prove to be more valuable than the conquered theorem itself. The scaffold then becomes a permanent addition to the architecture of mathematics. </em>
</p></blockquote>
<p/><p>
What strikes me and Ken about the <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> versus <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> claims we see so often is that they try to reach the summit in one bound. There is no scaffolding, no rope, no new handhold. The reader learns little.</p>
<p>
The term that most needs expounding is <em>base camp</em>. A base camp is not at the bottom. The main <a href="https://en.wikipedia.org/wiki/Everest_base_camps">base camps</a> for Mount Everest are halfway up to the summit. Getting to a base camp takes substantial work by itself. <em>Building</em> a base camp certainly does. But getting to one is essential. This is what is missing for <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> versus <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
</p><p/><h2> P&lt;NP Base Camps </h2><p/>
<p/><p>
Let’s look at <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Suppose you claim that you can show that CLIQUE requires super polynomial time. This is what you need to do to prove <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This is way beyond anything we can imagine proving. </p>
<p>
Suppose rather you claimed that CLIQUE requires <img alt="{m^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> deterministic time where <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the number of edges. This would be the best result ever on the difficulty of CLIQUE. It would easily be the best paper in complexity theory in decades. Would win prizes of all kinds. </p>
<p>
It is even worse. If one could prove that CLIQUE is not in deterministic time 	</p>
<p align="center"><img alt="\displaystyle  m\log\log\log m " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m%5Clog%5Clog%5Clog+m+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>that would also be a major result. Forget about proving a lower bound of 	</p>
<p align="center"><img alt="\displaystyle  m^{1000} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++m%5E%7B1000%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>and more that is needed to solve <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Just the above would be major.</p>
<p>
If we skirt around some technical issues with time on Turing machines, we can pitch our camp right at going beyond linear time. Or certainly linear size circuits. Nobody knows a language in <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that does not have linear size circuits. Proving one would bring enough renown for anyone.</p>
<p>
</p><p/><h2> P=NP Base Camps </h2><p/>
<p/><p>
Let’s look at <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Most regard this as the far side of the mountain but please bear with us to the end. Suppose you claim that CLIQUE is in polynomial time. </p>
<p>
The usual paper of this type claims it is doable by some straightforward polynomial time algorithm. The method might use linear programming in some standard manner. This might lead to a <em>practical</em> algorithm, but none of those has ever been observed in the wild. Even worse, any proof that CLIQUE can be resolved in time 	</p>
<p align="center"><img alt="\displaystyle  n^{C} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n%5E%7BC%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>would be also the best result ever on this problem. This applies even if <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an unknown constant, or is equal to some astronomically large value. Think <a href="https://en.wikipedia.org/wiki/Graham%27s_number">Graham’s number</a> or the <a href="https://en.wikipedia.org/wiki/Skewes%27s_number">Skewes</a> number: 	</p>
<p align="center"><img alt="\displaystyle 10^{10^{10^{502}}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+10%5E%7B10%5E%7B10%5E%7B502%7D%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Nor do we need <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be constant. Say it is <img alt="{O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or some polynomial in <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This would be a <em>quasi-polynomial</em> time algorithm. Here a really big campsite was built by László Babai, who <a href="https://www.quantamagazine.org/graph-isomorphism-vanquished-again-20170114/">proved</a> that Graph Isomorphism is in quasi-polynomial time. His <a href="http://people.cs.uchicago.edu/~laci/17groups/version2.1.pdf">paper</a> has a wealth of ideas that might be extended.</p>
<p>
But what really might be attractive about this side is that you can make progress without “shaking the Earth.” These results would be in the direction of <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> but not opposed by as many factors:</p>
<ul>
<li>
<em>Refute the Strong Exponential Time Hypothesis</em> (SETH). That is, find an algorithm in time <img alt="{2^{O(cn)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7BO%28cn%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with <img alt="{c &lt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for CNF-SAT. We have written about SETH <a href="https://rjlipton.wpcomstaging.com/2015/06/01/puzzling-evidence/">here</a>. <p/>
</li><li>
<em>Refute the Unique Games Conjecture</em> (UGC). Unlike SETH, this technically does not imply <img alt="{\mathsf{P &lt; NP}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> But refuting it does reduce the reach of <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-hardness. A large special case of UGC was, however, <a href="https://ieeexplore.ieee.org/document/8555140">proved</a> three years ago.
</li></ul>
<p>
Or prove that they cannot both be true simultaneously. My old <a href="https://rjlipton.wpcomstaging.com/2010/05/05/unique-games-a-three-act-play/">post</a> on UGC covered a sense in which there is no “SETH for UGC.” </p>
<p>
</p><p/><h2> But … </h2><p/>
<p/><p>
The trouble with our insight is that in the past, sometimes a full conjecture has been solved. That is, partial progress did not happen first—the mountain was scaled in one go. Or at least a lot of it, from a relatively low base camp. For <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> there is an essence of “polynomial” versus “exponential” that is sharply defined in other ways, for instance Mikhail Gromov’s theorem about growth rates in groups, which we wrote about <a href="https://rjlipton.wpcomstaging.com/2013/06/20/three-theorems-about-growth/">here</a>.</p>
<p>
Ken and I have differed on how long and steep and sudden the final ascent was for the <a href="https://en.wikipedia.org/wiki/Four_color_theorem">Four Color Theorem</a> and <a href="https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem">Fermat’s Last Theorem</a> (FLT). The proof of the former by Kenneth Appel and Wolfgang Haken was a watershed for its use of computers, but drew on ideas that had gone through the non-computer proofs-and-refutations process. Andrew Wiles’s announcement of FLT was a shock even with a three-day buildup of his lectures at a conference in 1993 having hinted it to several attendees. But he drew on partial progress that had been ramped up by Ribet and others since the mid-1980s.</p>
<p>
Maybe if Évariste Galois had beaten Niels Abel to showing the unsolvability of the quintic, his invention of group theory for the proof used today would have been a single bound. But Abel got a big lift from Paolo Ruffini’s 500 pages of work in 1799. (Évariste is the same name as <a href="https://appellationmountain.net/baby-name-of-the-day-everest/">Everest</a>, go figure.)</p>
<p>
The proof of the Boolean Sensitivity Conjecture two years ago by Hao Huang was short and sudden. But along lines remarked also in Dijkgraaf’s article, perhaps it was “more of a foothill.” Or maybe a base camp for harder problems, such as improving the upper bound from quartic to cubic or quadratic, as we discussed <a href="https://rjlipton.wpcomstaging.com/2019/07/12/tools-and-sensitivity/">here</a> and <a href="https://rjlipton.wpcomstaging.com/2019/07/25/discrepancy-games-and-sensitivity/">here</a>.</p>
<p>
This leads Ken into a historical daydream, taking over from here.</p>
<p>
</p><p/><h2> A Fermat Fantasy </h2><p/>
<p/><p>
Pierre de Fermat famously <a href="https://www.maths-et-tiques.fr/index.php/detentes/la-conjecture-de-fermat">wrote</a> the following in French in the margin of his copy of the famous book on arithmetic by Diophantus:</p>
<blockquote><p><b> </b> <em> Un cube n’est jamais la somme de deux cubes, une puissance quatrième n’est jamais la somme de deux puissances quatriémes et plus généralement aucune puissance supérieure à 2 n’est la somme de deux puissances analogues. J’ai trouvé une merveilleuse démonstration de cette proposition, mais la marge est trop étroite pour la contenir. </em>
</p></blockquote>
<p/><p>
I (Ken) think he could just as easily have written the following—and in place of the margin being too narrow, he could have given a more reasonable excuse, one I know all too well:</p>
<blockquote><p><b> </b> <em> Un cube n’est jamais la somme de moins que trois cubes, une puissance quatrième n’est jamais la somme de moins que quatre puissances quatriémes, et plus généralement aucune puissance n’est la somme de un moindre nombre de puissances analogues. J’ai trouvé une merveilleuse démonstration de cette proposition, que je rédigerai après avoir traité onze nouveaux cas de triche aux échecs en ligne. </em>
</p></blockquote>
<p/><p>
The stronger statement here is that no cube can be a nontrivial sum of fewer than three cubes (such as <img alt="{6^3 = 3^3 + 4^3 + 5^3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B6%5E3+%3D+3%5E3+%2B+4%5E3+%2B+5%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>), no fourth power a sum of fewer than four other like powers, and so on. This also was a hallowed conjecture that stood for centuries, <a href="https://en.wikipedia.org/wiki/Euler's_sum_of_powers_conjecture">named for</a> the giant Leonhard Euler no less. Well, if Pierre had just changed a few of his words, then <em>this</em> is what we would have known as FLT. Call it EFLT. It could have been just as worthy. That there were reservations known to Euler, even as he lent his name to it, might have made it all the more Olympian. </p>
<p>
Then what we actually know as FLT would have been a hard-work base camp for EFLT. Would we have seen the vast number of unsuccessful FLT proofs directed at EFLT instead? By people claiming to climb this peak in one bound—without first trying to prove that a sum of <b>just two</b> <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-th powers cannot be a higher <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-th power, for all <img alt="{n \geq 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? Well, there would have been a problem with that, one we <a href="https://rjlipton.wpcomstaging.com/2015/09/03/open-problems-that-might-be-easy/">discussed</a> in connection with solutions that might be easy after all:</p>
<p/><p align="center"><img alt="\displaystyle  \begin{array}{rcl}  144^5 &amp;=&amp; 27^5 + 84^5 + 110^5 + 133^5.\\ 20615673^4 &amp;=&amp; 2682440^4 + 15365639^4 + 18796760^4. \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++144%5E5+%26%3D%26+27%5E5+%2B+84%5E5+%2B+110%5E5+%2B+133%5E5.%5C%5C+20615673%5E4+%26%3D%26+2682440%5E4+%2B+15365639%5E4+%2B+18796760%5E4.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Do you have your own issues with these claimed proofs? Or, do you see other cases of people having suddenly scaled a mountain in one stride?</p>
<p/></font></font></div>
    </content>
    <updated>2021-08-10T19:50:43Z</updated>
    <published>2021-08-10T19:50:43Z</published>
    <category term="All Posts"/>
    <category term="graph isomorphism"/>
    <category term="History"/>
    <category term="P=NP"/>
    <category term="Proofs"/>
    <category term="base camp"/>
    <category term="claimed proofs"/>
    <category term="conjectures"/>
    <category term="Euler"/>
    <category term="Fermat"/>
    <category term="mountain"/>
    <category term="Robbert Dijkgraaf"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-08-20T23:37:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5706</id>
    <link href="https://www.scottaaronson.com/blog/?p=5706" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5706#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5706" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Yet more mistakes in papers</title>
    <summary xml:lang="en-US">Amazing Update (Aug. 19): My former PhD student Daniel Grier tells me that he, Sergey Bravyi, and David Gosset have an arXiv preprint, from February, where they give a corrected proof of my and Andris Ambainis’s claim that any k-query quantum algorithm can be simulated by an O (N1-1/2k)-query classical randomized algorithm (albeit, not of […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Amazing Update (Aug. 19):</span></strong> My former PhD student Daniel Grier tells me that he, Sergey Bravyi, and David Gosset have an <a href="https://arxiv.org/pdf/2102.06963.pdf">arXiv preprint</a>, from February, where they give a corrected proof of my and Andris Ambainis’s claim that any k-query quantum algorithm can be simulated by an O (N<sup>1-1/2k</sup>)-query classical randomized algorithm (albeit, not of our stronger statement, about a randomized algorithm to estimate any bounded low-degree real polynomial).  The reason I hadn’t known about this is that they don’t mention it in the abstract of their paper (!!).  But it’s right there in Theorem 5.</p>



<hr class="wp-block-separator"/>



<p>In my <a href="https://www.scottaaronson.com/blog/?p=5675">last post</a>, I came down pretty hard on the blankfaces: people who relish their power to persist in easily-correctable errors, to the detriment of those subject to their authority.  The sad truth, though, is that <em>I</em> don’t obviously do better than your average blankface in my ability to resist falsehoods on early encounter with them.  As one of many examples that readers of this blog might know, I didn’t think covid seemed like a big deal in early February 2020—although by mid-to-late February 2020, I’d repented of my doofosity.  If I have <em>any</em> tool with which to unblank my face, then it’s only my extreme self-consciousness when confronted with evidence of my own stupidities—the way I’ve trained myself over decades in science to see error-correction as a or even <em>the</em> fundamental virtue.</p>



<p>Which brings me to today’s post.  Continuing what’s become a <em>Shtetl-Optimized</em> tradition—see <a href="https://www.scottaaronson.com/blog/?p=2072">here from 2014</a>, <a href="https://www.scottaaronson.com/blog/?p=2854">here from 2016</a>, <a href="https://www.scottaaronson.com/blog/?p=3256">here from 2017</a>—I’m going to fess up to two serious mistakes in research papers on which I was a coauthor.</p>



<hr class="wp-block-separator"/>



<p>In 2015, Andris Ambainis and I had a STOC paper entitled <a href="https://arxiv.org/abs/1411.5729">Forrelation: A Problem that Optimally Separates Quantum from Classical Computing</a>.  We gave two main results there:</p>



<ol><li>A Ω((√N)/log(N)) lower bound on the randomized query complexity of my “Forrelation” problem, which was known to be solvable with only a single quantum query.</li><li>A proposed way to take any k-query quantum algorithm that queries an N-bit string, and simulate it using only O(N<sup>1-1/2k</sup>) classical randomized queries.</li></ol>



<p>Later, <a href="https://arxiv.org/abs/2008.07003">Bansal and Sinha</a> and independently <a href="https://arxiv.org/abs/2008.10223">Sherstov, Storozhenko, and Wu</a> showed that a k-query generalization of Forrelation, which I’d also defined, requires ~Ω(N<sup>1-1/2k</sup>) classical randomized queries, in line with my and Andris’s conjecture that k-fold Forrelation <em>optimally</em> separates quantum and classical query complexities.</p>



<p>A couple months ago, alas, my former grad school officemate <a href="https://www.cse.cuhk.edu.hk/~andrejb/">Andrej Bogdanov</a>, along with Tsun Ming Cheung and Krishnamoorthy Dinesh, emailed me and Andris to say that they’d discovered an error in result 2 of our paper (result 1, along with the Bansal-Sinha and Sherstov-Storozhenko-Wu extensions of it, remained fine).  So, adding our own names, we’ve now posted a <a href="https://eccc.weizmann.ac.il/report/2021/115/">preprint on ECCC</a> that explains the error, while also showing how to recover our result for the special case k=1: that is, any 1-query quantum algorithm really can be simulated using only O(√N) classical randomized queries.</p>



<p>Read the preprint if you really want to know the details of the error, but to summarize it in my words: Andris and I used a trick that we called “variable-splitting” to handle variables that have way more influence than average on the algorithm’s acceptance probability.  Alas, variable-splitting fails to take care of a situation where there are a bunch of variables that are non-influential individually, but that on some unusual input string, can “conspire” in such a way that their signs all line up and their contribution overwhelms those from the other variables.  A single mistaken inequality fooled us into thinking such cases were handled, but an explicit counterexample makes the issue obvious.</p>



<p>I <em>still</em> conjecture that my original guess was right: that is, I conjecture that any problem solvable with k quantum queries is solvable with O(N<sup>1-1/2k</sup>) classical randomized queries, so that k-fold Forrelation is the extremal example, and so that no problem has constant quantum query complexity but linear randomized query complexity.  More strongly, I reiterate the conjecture that any bounded degree-d real polynomial, p:{0,1}<sup>N</sup>→[0,1], can be approximated by querying only O(N<sup>1-1/d</sup>) input bits drawn from some suitable distribution.  But proving these conjectures, if they’re true, will require a new algorithmic idea.</p>



<hr class="wp-block-separator"/>



<p>Now for the second <em>mea culpa</em>.  Earlier this year, my student Sabee Grewal and I posted a short preprint on the arXiv entitled <a href="https://arxiv.org/abs/2102.10458">Efficient Learning of Non-Interacting Fermion Distributions</a>.  In it, we claimed to give a classical algorithm for reconstructing any “free fermionic state” |ψ⟩—that is, a state of n identical fermionic particles, like electrons, each occupying one of m&gt;n possible modes, that can be produced using only “fermionic beamsplitters” and no interaction terms—and for doing so in polynomial time and using a polynomial number of samples (i.e., measurements of where all the fermions are, given a copy of |ψ⟩).  Alas, after trying to reply to confused comments from readers and reviewers (albeit, none of them <em>exactly</em> putting their finger on the problem), Sabee and I were able to figure out that we’d done no such thing.</p>



<p>Let me explain the error, since it’s actually really interesting.  In our underlying problem, we’re trying to find a collection of unit vectors, call them |v<sub>1</sub>⟩,…,|v<sub>m</sub>⟩, in C<sup>n</sup>.  Here, again, n is the number of fermions and m&gt;n is the number of modes.  By measuring the “2-mode correlations” (i.e., the probability of finding a fermion in both mode i and mode j), we can figure out the approximate value of |⟨v<sub>i</sub>|v<sub>j</sub>⟩|—i.e., the absolute value of the inner product—for any i≠j.  From that information, we want to recover |v<sub>1</sub>⟩,…,|v<sub>m</sub>⟩ themselves—or rather, their relative configuration in n-dimensional space, isometries being irrelevant.</p>



<p>It seemed to me and Sabee that, if we knew ⟨v<sub>i</sub>|v<sub>j</sub>⟩ for all i≠j, then we’d get linear equations that iteratively constrained each |v<sub>j</sub>⟩ in terms of ⟨v<sub>i</sub>|v<sub>j</sub>⟩ for j&lt;i, so all we’d need to do is solve those linear systems, and then (crucially, and this was the main work we did) show that the solution would be <em>robust</em> with respect to small errors in our estimates of each ⟨v<sub>i</sub>|v<sub>j</sub>⟩.  It seemed further to us that, while it was true that the measurements only revealed |⟨v<sub>i</sub>|v<sub>j</sub>⟩| rather than ⟨v<sub>i</sub>|v<sub>j</sub>⟩ itself, the “phase information” in ⟨v<sub>i</sub>|v<sub>j</sub>⟩ was manifestly irrelevant, as it in any case depended on the irrelevant global phases of |v<sub>i</sub>⟩ and |v<sub>j</sub>⟩ themselves.</p>



<p>Alas, it turns out that the phase information <em>does</em> matter.  As an example, suppose I told you only the following about three unit vectors |u⟩,|v⟩,|w⟩ in R<sup>3</sup>:</p>



<p>|⟨u|v⟩| = |⟨u|w⟩| = |⟨v|w⟩| = 1/2.</p>



<p>Have I thereby determined these vectors up to isometry?  Nope!  In one class of solution, all three vectors belong to the same plane, like so:</p>



<p>|u⟩=(1,0,0),<br/>|v⟩=(1/2,(√3)/2,0),<br/>|w⟩=(-1/2,(√3)/2,0).</p>



<p>In a completely different class of solution, the three vectors <em>don’t</em> belong to the same plane, and instead look like three edges of a tetrahedron meeting at a vertex:</p>



<p>|u⟩=(1,0,0),<br/>|v⟩=(1/2,(√3)/2,0),<br/>|w⟩=(1/2,1/(2√3),√(2/3)).</p>



<p>These solutions correspond to different sign choices for |⟨u|v⟩|, |⟨u|w⟩|, and |⟨v|w⟩|—choices that <em>collectively</em> matter, even though each of them is individually irrelevant.</p>



<p>It follows that, even in the special case where the vectors are all real, the 2-mode correlations are <em>not </em>enough information to determine the vectors’ relative positions.  (Well, it takes some more work to convert this to a counterexample that could actually arise in the fermion problem, but that work can be done.)  And alas, the situation gets even gnarlier when, as for us, the vectors can be complex.</p>



<p>Any possible algorithm for our problem will have to solve a system of <em>non</em>linear equations (albeit, a massively overconstrained system that’s guaranteed to have a solution), and it will have to use <em>3-mode</em> correlations (i.e., statistics of <em>triples</em> of fermions), and quite possibly 4-mode correlations and above.</p>



<p>But now comes the good news!  Googling revealed that, for reasons having nothing to do with fermions or quantum physics, problems <em>extremely</em> close to ours had already been studied in classical machine learning.  The key term here is <a href="https://en.wikipedia.org/wiki/Determinantal_point_process">“Determinantal Point Processes”</a> (DPPs).  A DPP is a model where you specify an m×m matrix A (typically symmetric or Hermitian), and then the probabilities of various events are given by the determinants of various principal minors of A.  Which is <em>precisely</em> what happens with fermions!  In terms of the vectors |v<sub>1</sub>⟩,…,|v<sub>m</sub>⟩ that I was talking about before, to make this connection we simply let A be the m×m <em>covariance matrix</em>, whose (i,j) entry equals ⟨v<sub>i</sub>|v<sub>j</sub>⟩.</p>



<p>I first learned of this remarkable correspondence between fermions and DPPs a decade ago, from a talk on DPPs that <a href="https://www.cs.washington.edu/people/faculty/taskar">Ben Taskar</a> gave at MIT.  Immediately after the talk, I made a mental note that Taskar was a rising star in theoretical machine learning, and that his work would probably be relevant to me in the future.  While researching this summer, I was devastated to learn that Taskar died of heart failure in 2013, in his mid-30s and only a couple of years after I’d heard him speak.</p>



<p>The most relevant paper for me and Sabee was called <a href="https://www.alexkulesza.com/pubs/spmap_laa14.pdf">An Efficient Algorithm for the Symmetric Principal Minor Assignment Problem</a>, by Rising, Kulesza, and Taskar.  Using a combinatorial algorithm based on minimum spanning trees and chordless cycles, this paper <em>nearly</em> solves our problem, except for two minor details:</p>



<ol><li>It doesn’t do an error analysis, and</li><li>It considers complex <em>symmetric</em> matrices, whereas our matrix A is <a href="https://en.wikipedia.org/wiki/Hermitian_matrix">Hermitian</a> (i.e., it equals its <em>conjugate</em> transpose, not its transpose).</li></ol>



<p>So I decided to email <a href="https://www.alexkulesza.com/">Alex Kulezsa</a>, one of Taskar’s surviving collaborators who’s now a research scientist at Google NYC, to ask his thoughts about the Hermitian case.  Alex kindly replied that they’d been meaning to study that case—a reviewer had even asked about it!—but they’d ran into difficulties and didn’t know what it was good for.  I asked Alex whether he’d like to join forces with me and Sabee in tackling the Hermitian case, which (I told him) was enormously relevant in quantum physics.  To my surprise and delight, Alex agreed.</p>



<p>So we’ve been working on the problem together, making progress, and I’m optimistic that we’ll have <em>some</em> nice result.  By using the 3-mode correlations, at least “generically” we can recover the entries of the matrix A <em>up to complex conjugation</em>, but further ideas will be needed to resolve the complex conjugation ambiguity, to whatever extent it actually matters.</p>



<p>In short: on the negative side, there’s much more to the problem of learning a fermionic state than we’d realized.  But on the positive side, there’s much more to the problem than we’d realized!  As with the simulation of k-query quantum algorithms, my coauthors and I would welcome any ideas.  And I apologize to anyone who was misled by our premature (and hereby retracted) claims.</p>



<hr class="wp-block-separator"/>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Aug. 11):</span></strong> Here’s a third bonus retraction, which I thank my colleague <a href="https://www.markwilde.com/">Mark Wilde</a> for bringing to my attention.  Way back in 2005, in my <a href="https://arxiv.org/abs/quant-ph/0502072">NP-complete Problems and Physical Reality</a> survey article, I “left it as an exercise for the reader” to prove that BQP<sub>CTC</sub>, or quantum polynomial time augmented with Deutschian closed timelike curves, is contained in a complexity class called SQG (Short Quantum Games).  While it turns out to be <em>true</em> that BQP<sub>CTC</sub> ⊆ SQG—as follows from <a href="https://arxiv.org/abs/0808.2669">my and Watrous’s 2008 result</a> that BQP<sub>CTC</sub> = PSPACE, combined with <a href="https://arxiv.org/abs/1011.2787">Gutoski and Wu’s 2010 result</a> that SQG = PSPACE—it’s not something for which I could possibly have had a correct proof back in 2005.  I.e., it was a harder exercise than I’d intended!</p></div>
    </content>
    <updated>2021-08-10T19:02:42Z</updated>
    <published>2021-08-10T19:02:42Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Embarrassing Myself"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-08-20T03:09:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/116</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/116" rel="alternate" type="text/html"/>
    <title>TR21-116 |  Quantum Meets the Minimum Circuit Size Problem | 

	Nai-Hui Chia, 

	Chi-Ning  Chou, 

	Jiayu Zhang, 

	Ruizhe Zhang</title>
    <summary>In this work, we initiate the study of the Minimum Circuit Size Problem (MCSP) in the quantum setting. MCSP is a problem to compute the circuit complexity of Boolean functions. It is a fascinating problem in complexity theory---its hardness is mysterious, and a better understanding of its hardness can have surprising implications to many fields in computer science.

We first define and investigate the basic complexity-theoretic properties of minimum quantum circuit size problems for three natural objects: Boolean functions, unitaries, and quantum states. We show that these problems are not trivially in NP but in QCMA (or have QCMA protocols). Next, we explore the relations between the three quantum MCSPs and their variants. We discover that some reductions that are not known for classical MCSP exist for quantum MCSPs for unitaries and states, e.g., search-to-decision reduction and self-reduction. Finally, we systematically generalize results known for classical MCSP to the quantum setting (including quantum cryptography, quantum learning theory, quantum circuit lower bounds, and quantum fine-grained complexity) and also find new connections to tomography and quantum gravity. Due to the fundamental differences between classical and quantum circuits, most of our results require extra care and reveal properties and phenomena unique to the quantum setting. Our findings could be of interest for future studies, and we post several open problems for further exploration along this direction.</summary>
    <updated>2021-08-10T18:30:50Z</updated>
    <published>2021-08-10T18:30:50Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-08-20T23:37:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-480455417739937086</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/480455417739937086/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=480455417739937086" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/480455417739937086" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/480455417739937086" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2021/08/queues-with-small-advice.html" rel="alternate" type="text/html"/>
    <title>Queues with Small Advice</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have had papers rejected, with comments of the form that the results seem too easy, and are at the level of a homework assignment.  Generally, I think these reviewers miss the point.  The fact that the results seem easy may be because the point isn't the derivation but the conception and framing of the problem.  I actually think that generally it's an interesting subclass of good papers that can be and are turned into homework assignments.</p><p>A new-ish paper of mine, Queues with Small Advice, was recently accepted to the very new SIAM Conference on Applied and Computational Discrete Algorithms (<a href="https://www.siam.org/conferences/cm/conference/acda21">ACDA21</a>), which took place July 19-21.  This conference focuses on algorithms with a close tie to applications.  Some people unfamiliar with theory conferences might think that algorithms work would naturally be tied to applications, but I've generally found that algorithmic work tied to applications is more negatively reviewed in theory conferences.  Indeed, that type of work is much more likely to receive comments of the form that the results seem too easy, and are at the level of a homework assignment.  So perhaps this new conference will fill an important role and hole in the current slate of theory conferences. </p><p>In any case, I actually do think this paper is in some ways easy (in that the analysis is readily approachable with standard tools), and parts of it would, I believe, make a great homework assignment.  The goal was to show the potential power of using even very simple advice, such as from machine-learning algorithms, in queueing systems.  This seems to me to be a very understudied topic, and fits into the recently growing theme of <a href="https://arxiv.org/abs/2006.09123">Algorithms with Predictions</a>.  (The paper was rejected previously from a conference, where the most negative review said "Very accessible and well written paper, which certainly provides motivation to consider problems of this type." but also said "The mathematical analysis in this paper is fairly standard, and in that sense not novel... the paper is interesting, but not advancing sufficiently the state of the art.")  </p><p>The paper focuses on the case of 1 bit of advice -- essentially, is the job "short" or "long".  I think this type is advice is a good approach to look at for queueing -- it corresponds naturally to putting a job at the front of the queue, or the back.  And it may be easier for machine-learning algorithms to generate accurately.  Simple is often good in practice.  </p><p>Rather than describe the paper further, I'll go ahead and turn it directly into a collection of homework problems.  Feel free to use them or variations you come up with;  hopefully, the students won't find the paper for answers. I personally would be thrilled if one outcome of this paper was that prediction-based problems of this form made their way into problem sets.  (Although, serious question:  who still teaches queueing theory any more?)  </p><p><b>Section 1:  One-Bit Advice (Single Queue)</b></p><p>a)  Consider the standard M/M/1 queue, with Poisson arrivals at rate λ, and exponentially distributed service times of mean 1;  the expected time a job spends in the queue in equilibrium is 1/(1-λ).  Now suppose each job comes with one bit advice;  if the job has service time greater than T, the bit is 1, and if it is smaller than T, the bit is 0.  A "big" job goes to the end of the queue, a "small" job goes to the front.  (Assume the queue is non-preemptive.)  Find the expected time for a job in this queue in equilibrium, as a function of T and λ.</p><p>b)  What is the optimal value for T (as a function of λ)? </p><p>c)  Repeat parts a and b, but this time with a preemptive queue.  Does preemption help or hurt performance?</p><p>Harder variation:  The above questions, but with an M/G/1 queue (that is, for a general, given service distribution);  derive a formula for the expected time in the system, where the formula may involve terms based on the service distribution.</p><p>Easier variation:  Write a simulation, experimentally determine the best threshold, and the improvements from one bit of advice.  Different service time distributions can be tried.  </p><p><b>Section 2:  One-Bit Advice with Predictions (Single Queue)</b></p><p>Where would possibly get a bit of advice in real life?  Perhaps from a machine learning predictor.  But in that case, the advice might turn out to be wrong.  What if our bit of advice is just right most of the time?</p><p>a)  Consider the (non-preemptive) M/M/1 queue variation from Section 1 part a above, but now the advice is correct with some probability p.  Find the expected time for a job in this queue in equilibrium, as a function of p, T, and λ.</p><p>b)  Repeat part a with a preemptive queue.</p><p>Harder variations:  The above questions, but have the probability the advice is correct depend on the size of the job.  A particularly fun example is when the "predicted service time" for a job with true time x is exponentially distributed with mean x, and the prediction bit is 1 if the predicted time is larger than T, and 0 otherwise.  Also, one can again consider general service times.  </p><p>Easier variation:  Again, write a simulation and derive experimental results/insights.  </p><p><b>Section 3:  One-Bit Advice with Prediction (Power of 2 Choices)</b>  <i>[harder, grad student level;  needs to know fluid limit models;  I'd stick with sections 1 and 2!]</i></p><p>a)  Derive fluid limit equations for a collection of N queues, where there are two types of jobs:  "large" jobs arrive as a Poisson stream of rate λ₁N and have exponentially distributed service times with mean μ₁ and "small" jobs arrive as a Poisson stream of rate λ₂N and have exponentially distributed service times of mean μ₂.  Each job comes with a bit of advice determining whether it is large or small, but large jobs are mislabelled with probability p₁ and small jobs are mislabelled with probability p₂.  An incoming job selects a queue using "the power of two choices" -- it is up to you to describe how a job determines what is the better of the two choices (there are multiple possibilities) and how jobs are processed within a queue (non-preemptive is suggested).   </p><p>[Hint:  the queue state can be represented by the number of jobs that are labelled short that are waiting, the number of jobs that are labelled long that are waiting, and the type of the job currently being serviced.]  </p><p>b)  Compare fluid limit results to simulations for 1000 queues to see if your equations seem accurate.  </p><p><br/></p></div>
    </content>
    <updated>2021-08-09T20:01:00Z</updated>
    <published>2021-08-09T20:01:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2021-08-19T15:46:05Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/one-shot-top-k/</id>
    <link href="https://differentialprivacy.org/one-shot-top-k/" rel="alternate" type="text/html"/>
    <title>One-shot DP Top-k mechanisms</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the last <a href="https://differentialprivacy.org/exponential-mechanism-bounded-range/"><em>blog post</em></a>, we showed that the exponential mechanism enjoys improved composition bounds over general pure DP mechanisms due to a property called <strong>bounded range</strong>.  For this post, we will present another useful, and somewhat surprising, property of the exponential mechanism in its application of top-\(k\) selection.</p>

<h2 id="differentially-private-top-k-selection">Differentially Private Top-\(k\) Selection</h2>

<p>We will focus on datasets that are a vector of counts \(h = (h_1, \cdots, h_d) \in \mathbb{N}^d\), which consist of counts \(h_i\) for elements from a universe \(\mathcal{U}\) where \(|\mathcal{U}| = d\).  Let’s assume that a user’s data can modify each count by at most 1, yet can change all \(d\) counts, i.e. the \(\ell_\infty\)-sensitivity is 1 and the \(\ell_0\)-sensitivity is \(d\).  The task here is to return the top-\(k\) elements from the input counts in a differentially private way.</p>

<p>For top-\(1\), this is simply returning the element with the max count, and this is precisely the problem that the exponential mechanism is set up to solve.  Let’s write out the exponential mechanism \(M^{(1)}: \mathbb{N}^d \to [d]\) for this instance:
\[
\mathbb{P}[M^{(1)}(h) = i] = \frac{e^{ \varepsilon h_i }}{\sum_{j \in [d] } e^{ \varepsilon h_j } }, \qquad \forall i \in [d].
\]
For those wondering why this formula omits the factor of \(1/2\) in the exponent, we are using the <a href="https://dongjs.github.io/2020/02/10/ExpMech.html">stronger result</a> of the exponential mechanism which replaces global sensitivity with the range of the loss function \(\ell(i,h) = - h_i\), which is \(1\) in this case.  Recall from the last blog post that the exponential mechanism is \(\varepsilon^2/8\)-CDP.</p>

<p>Hence, to generalize this to top-\(k\) selection, we can simply iteratively apply this exponential mechanism by removing the <em>discovered</em> element from each previous round.  That is, we write \(M^{(k)}: \mathbb{N}^d \to [d]^k\) as the following for any outcome \( (i_1, i_2, \cdots, i_k) \in [d]^k\),</p>

<p>\[
\mathbb{P}[M^{(k)}(h) = (i_1, i_2, \cdots, i_k)] \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
<a name="eq:peelingEM"/>
\[\qquad = \frac{e^{ \varepsilon h_{i_1} }}{\sum_{j \in [d] } e^{ \varepsilon h_j } } \cdot  \frac{ e^{\varepsilon h_{i_2} } }{\sum_{j\in [d]\setminus \{ i_1\}} e^{\varepsilon h_j } } \cdot \cdots \cdot \frac{ e^{ \varepsilon h_{i_2} } }{\sum_{j\in [d]\setminus \{ i_1, \cdots, i_{k-1}\}} e^{ \varepsilon h_j } }. 
\tag{1}
\]</p>

<p>We can then apply composition to conclude that \(M^{(k)}\) is \(k \varepsilon^2/8\)-CDP.</p>

<h2 id="gumbel-noise-and-the-exponential-mechanism">Gumbel Noise and the Exponential Mechanism</h2>

<p>As we discussed in our last post, we can implement the exponential mechanism by adding <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel</a> noise to each count and reporting the noisy max element.  A Gumbel random variable \(X \sim \text{Gumbel}(\beta) \), parameterized by scale parameter \(\beta&gt;0\), has the following density function
<a name="eq:GumbelDensity"/>
\[
p(x;\beta) = \frac{1}{\beta} \exp\left( - x/\beta - e^{-x/\beta} \right), \qquad \forall x \in \mathbb{R}.
\tag{2}
\]</p>

<p>Hence, we can write the exponential mechanism in the following way
\[
M^{(1)}(h) = \arg\max \{ h_i + X_i : i \in [d] \}, \qquad \{X_i \} \stackrel{i.i.d.}{\sim} \text{Gumbel}(1/\varepsilon).
\]</p>

<p>We can then extend this to top-\(k\) by repeatedly adding independent Gumbel noise to each count and removing the discovered element for the next round.  However, something that would significantly improve run time would be to add Gumbel noise to each count <em>once</em> and then take the elements with the top-\(k\) noisy counts.  We could then add only \(d\) many noise terms, rather than \(O(d^k)\) noise terms if we were to iteratively run \(k\) different exponential mechanisms.  The question is, does this one-shot top-\(k\) Gumbel noise mechanism ensure the same level of privacy?</p>

<p>Let’s denote the one-shot Gumbel mechanism as \(\tilde{M}^{(k)}\).  At first glance, it does not seem like the one-shot Gumbel mechanism \(\tilde{M}^{(k)}\) should be just as private as the iterative exponential mechanism \(M^{(k)}\), but it turns out they are exactly the same mechanism!  The following result is due to <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a>.</p>

<blockquote>
  <p><strong>Theorem 1</strong>
For any input vector of counts \(h \in \mathbb{N}^d\), the one-shot Gumbel mechanism \(\tilde{M}^{(k)}(h)\) and iteratively applying the exponential mechanism \(M^{(k)}(h)\) are equal in distribution.</p>
</blockquote>

<p><em>Proof.</em> 
Recall the distribution of the iterative exponential mechanism \(M^{(k)}(h)\) from <a href="https://differentialprivacy.org/feed.xml#eq:peelingEM">(1)</a>.
Now we consider the one-shot Gumbel mechanism \(\tilde{M}^{(k)}(h)\) where we use the density of \(X \sim \) Gumbel\( (1/\varepsilon)\) from <a href="https://differentialprivacy.org/feed.xml#eq:GumbelDensity">(2)</a>.<br/>
\[
\mathbb{P}[\tilde{M}^{(k)}(h) = (i_1, \cdots, i_k)] \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
\[
\qquad = \int_{-\infty}^\infty p(u_1 - h_{i_1}) \int_{-\infty}^{u_1}  p(u_2 - h_{i_2}) \cdots \int_{-\infty}^{u_{k-1}} p(u_k - h_k) 
\]
<a name="eq:integral"/>
\[
\qquad \qquad \cdot \prod_{j \in [d] \setminus \{i_1, \cdots, i_k \} } \mathbb{P}[ X &lt; u_k - h_j]du_k \cdots du_2 du_1.
\tag{3}
\]
Note that we have 
\[
\mathbb{P}[X &lt; y] = \exp\left( - \exp\left( -\varepsilon y \right) \right).
\]
Let’s focus on the inner integral over \(u_k\) in <a href="https://differentialprivacy.org/feed.xml#eq:integral">(3)</a>.<br/>
\[
\int_{-\infty}^{u_{k-1}} p(u_k - h_{i_k} )\prod_{j \in [d] \setminus \{i_1, \cdots, i_k \} } \mathbb{P}[X &lt; u_k - h_j ]du_k \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
\[
\quad = \int_{-\infty}^{u_{k-1}} \varepsilon \cdot  \exp\left( - \varepsilon (u_k - h_{i_k}) - e^{ -\varepsilon (u_k - h_{i_k}) } \right) \cdot  \exp\left( -e^{-\varepsilon u_k}  \sum_{j \in [d] \setminus \{i_1, \cdots, i_k \} } e^{\varepsilon h_j}  \right) du_k 
\]
\[
\quad  = \varepsilon e^{\varepsilon h_{i_k}}  \int_{-\infty}^{u_{k-1}} \exp\left( -\varepsilon u_k - e^{-\varepsilon u_k} \left( e^{\varepsilon h_{i_k}} + \sum_{j \in [d] \setminus \{i_1, \cdots i_k \} } e^{\varepsilon h_j} \right) \right) du_k \qquad
\]
<a name="eq:lastLine"/>
\[
\qquad =  \varepsilon e^{\varepsilon h_{i_k}}  \int_{-\infty}^{u_{k-1}} \exp\left( -\varepsilon u_k - e^{-\varepsilon u_k} \left(\sum_{j \in [d] \setminus \{i_1, \cdots i_{k-1} \} } e^{\varepsilon h_j} \right) \right) du_k. \qquad \qquad
\tag{4}
\]
We now integrate with a \(v\)-substitution,
\[
v =e^{-\varepsilon u_{k}} \sum_{j \in [d] \setminus \{i_1, \cdots i_{k-1} \} } e^{\varepsilon h_j}<br/>
\]
\[
dv = - \varepsilon \sum_{j \in [d] \setminus \{i_1, \cdots i_{k-1} \} } e^{\varepsilon h_j}  \cdot e^{-\varepsilon u_{k}} du_{k}.
\]</p>

<p>Continuing with <a href="https://differentialprivacy.org/feed.xml#eq:lastLine">(4)</a>, we get
\[
\int_{-\infty}^{u_{k-1}} p(u_k - h_{i_k} )\prod_{j \in [d] \setminus \{i_1, \cdots, i_k \} } \mathbb{P}[X &lt; u_k - h_j ]du_k \qquad \qquad \qquad \qquad \qquad
\]
\[
\qquad = \frac{e^{\varepsilon h_{i_k} }}{\sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j}} \cdot \exp\left( - e^{-\varepsilon u_{k-1}} \cdot \sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j} \right)
\]
\[
\qquad = \frac{e^{\varepsilon h_{i_k} }}{\sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j}}  \cdot \prod_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \} } \mathbb{P}[X &lt; u_{k-1} - h_j ] .
\] 
Note how this line has the last term in the expression for \(M^{(k)}(h)\) in <a href="https://differentialprivacy.org/feed.xml#eq:peelingEM">(1)</a>, which is independent of \(u_{k-1}\) and can hence be pulled out of the larger integral in <a href="https://differentialprivacy.org/feed.xml#eq:integral">(3)</a>.  By induction, we have
\[
\mathbb{P}[\tilde{M}^{(k)}(h) = (i_1, \cdots, i_k)] \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
\]
\[
\qquad  = \frac{e^{\varepsilon h_{i_1} }}{\sum_{j \in [d]} e^{\varepsilon h_j}}  \cdot \frac{e^{\varepsilon h_{i_2} }}{\sum_{j \in [d] \setminus \{ i_1\}} e^{\varepsilon h_j}}  \cdot \cdots \cdot \frac{e^{\varepsilon h_k }}{\sum_{j \in [d] \setminus \{i_1, \cdots, i_{k-1} \}} e^{\varepsilon h_j}} 
\]
\[
\qquad = \mathbb{P}[M^{(k)}(h) =(i_1, \cdots, i_k)].
\] ∎</p>

<p>So that’s great!  We can now run the one-shot Gumbel mechanism for top-\(k\) and still get the improved composition bounds of the exponential mechanism.  In addition to this achieving better runtime, this analysis can help with proving top-\(k\) DP algorithms over a large domain universe despite giving access to only the true top-\(\bar{k}\) items and their counts where \(\bar{k} &gt; k \), see <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a> for more details.</p>

<h2 id="report-noisy-max-for-dp-top-k">Report Noisy Max for DP Top-\(k\)</h2>

<p>We now turn to comparing this algorithm to some natural alternatives.  As we discussed in the last post, there is a family of mechanisms, report noisy max (RNM) mechanisms, that ensure differential privacy for the selection problem, and hence the top-\(1\) problem.  We showed that the exponential mechanism is equivalent to RNM with Gumbel noise, there is also RNM with Laplace and with Exponential noise, the last being the recently discovered <em>permute-and-flip</em> mechanism <a href="https://arxiv.org/abs/2010.12603" title="Ryan McKenna, Daniel Sheldon. Permute-and-Flip: A new mechanism for differentially private selection . NeurIPS 2020."><strong>[MS20]</strong></a> <a href="https://arxiv.org/abs/2105.07260" title="Zeyu Ding, Daniel Kifer, Sayed M. Saghaian N. E., Thomas Steinke, Yuxin Wang, Yingtai Xiao, Danfeng Zhang. The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with Exponential Noise. 2021."><strong>[DKSSWXZ21]</strong></a>.</p>

<p>To then use RNM mechanisms for top-\(k\), we can again iteratively apply them and use composition to get the overall privacy guarantee.  However, it turns out that you can also use the Laplace noise version of RNM in one-shot <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a>.</p>

<p>We can compare the relative noise that is added to each count in both the Laplace and Gumbel versions.  Since <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a> gives their privacy guarantee in terms of approximate \((\varepsilon,\delta )\)-DP, we will now make the comparison there.  We first look at the standard deviation for Laplace \( \sigma_{\text{Lap}}\) (using Theorem 2.2 in <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a>).
\[
\sigma_{\text{Lap}} =  \frac{8 \sqrt{2k \ln(d/\delta)}}{\varepsilon}.
\]
Note that the one-shot Laplace mechanism returns counts as well as the indices of the top-\(k\), both of which use Laplace noise with standard deviation \(\sigma_{\text{Lap}}\), so we will also include Laplace noise to the discovered elements in the Gumbel version. That is, we add Gumbel noise with scale \(\sqrt{k}/\varepsilon’\) for the discovery portion and Laplace noise with scale \(2\sqrt{k}/\varepsilon’\) for obtaining their counts, resulting in standard deviation noise \(\sigma_{\text{Gumb}}’\) and \(\sigma_{\text{Lap}}’\), respectively.<br/>
\[
\sigma_{\text{Gumb}}’  = \frac{\pi \sqrt{k} }{\sqrt{6}\varepsilon’}, \qquad 
\sigma_{\text{Lap}}’ = \frac{2\sqrt{2k}}{\varepsilon’}.
\]
Recall that adding this scale of Gumbel noise and Laplace noise will ensure \(\tfrac{\varepsilon’^2}{8} \)-CDP each, so combining will ensure \(\tfrac{\varepsilon’^2}{4}\)-CDP.  We could also use Gaussian noise to return the counts since we are using CDP, but we will analyze it with Laplace noise for comparison.  To ensure \((\varepsilon,\delta)\)-DP, we use the CDP to DP conversion from Lemma 3.5 in <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a> and solve for \(\varepsilon’\).  Hence, we get for any \(\delta&gt;0\)
\[
\varepsilon’^2/4 = \left( \sqrt{\ln(1/\delta) + \varepsilon} - \sqrt{\ln(1/\delta)} \right)^2 
\]
\[ \implies \varepsilon’ = 2 \sqrt{\ln(1/\delta)} \left( \sqrt{1 + \tfrac{\varepsilon}{\ln(1/\delta)}} - 1 \right).
\]</p>

<p>Let’s consider a typical privacy setting where \(\varepsilon &lt;  \ln(1/\delta)\), and use the inequality \(\sqrt{1+x} \geq 1 + x/4\) for \(0&lt;x&lt;1\).  Here is a short proof of this inequality:
\[
(1 + x/4)^2 = 1 + x/2  + x^2/16  \leq 1 + x/2 + x/2 = 1 + x.<br/>
\]
Note that the privacy guarantee for one-shot Laplace noise only holds when \(\varepsilon &lt; 0.2\) and \(\delta &lt; 0.05\) as stated in Theorem 2.2 in <a href="https://arxiv.org/abs/2105.08233" title="Gang Qiao, Weijie J. Su, Li Zhang. Oneshot Differentially Private Top-k Selection. ICML 2021."><strong>[QSZ21]</strong></a>.  In this case, we have 
\[
\varepsilon’ \geq 2 \sqrt{\ln(1/\delta)} \left( 1 + 1/4 \cdot \tfrac{\varepsilon}{\ln(1/\delta)} - 1 \right) = 1/2 \cdot \tfrac{\varepsilon}{\sqrt{\ln(1/\delta)}}.
\]
Plugging \(\varepsilon’\) into the standard deviation of Gumbel and Laplace, we get
\[
\sigma_{\text{Gumb}}’  \leq \frac{2\pi\sqrt{k \ln(1/\delta)}}{\sqrt{6}\cdot \varepsilon}, \qquad \sigma_{\text{Lap}}’ \leq \frac{4\sqrt{2k\ln(1/\delta)}}{\varepsilon}.
\]</p>

<p>Putting this together, we can show that we add significantly less noise for the discovery and releasing noisy count phases, 
\[
\sigma_{\text{Gumb}}’\leq \sigma_{\text{Lap}}/4 ,\qquad  \sigma_{\text{Lap}}’ \leq \sigma_{\text{Lap}}/2.
\]
Note that these bounds can be improved further with similar analysis.</p>

<p>Although it has not been studied yet whether the permute-and-flip mechanism \(M_{\text{PF}} \) can also ensure DP in one shot by using Exponential noise, we briefly discuss whether it can be bounded range for a similar parameter as the Exponential Mechanism, and hence achieve similar composition bounds.  Consider running permute-and-flip on two items \(\{1,2 \}\) with a monotonic quality score \(q: \mathcal{X} \times \{1,2 \} \to \mathbb{R} \) whose sensitivity is 1.  Let \(x, x’ \in \mathcal{X} \) be neighbors where 
\[
q(x,1) = q(x,2) = 0
\]
\[
q(x’,1) = 0 , \quad q(x’,2) = 1.
\]
Hence, permute-and-flip will return outcome \(1\) or \(2\) with half probability each on dataset \(x\), while with dataset \(x’\) outcome \(1\) occurs with probability \(1/2 \cdot  e^{-\varepsilon}\) and outcome \(2\) occurs with probability \( 1/2 + 1/2 \cdot (1 - e^{-\varepsilon})\).  We can then compute the bounded range parameter \(\alpha\) as
\[
\frac{\mathbb{P}[M_{\text{PF}}(x’) = 2 ] }{\mathbb{P}[M_{\text{PF}}(x) = 2 ]}\leq e^{\alpha}\frac{\mathbb{P}[M_{\text{PF}}(x’) = 1 ]}{\mathbb{P}[M_{\text{PF}}(x) = 1 ]} \implies  \alpha \geq \varepsilon + \ln(2 - e^{-\varepsilon} ).
\]
Note that with \(\varepsilon \gg 1\), we get \(\alpha \) close to \(\varepsilon\), which would be the same bounded range parameter as the exponential mechanism.  However, with \(\varepsilon&lt; 1\), we get \(\alpha\) close to \(2\varepsilon\).  This example provides a lower bound on the BR parameter for permute-and-flip.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We have looked at the top-\(k\) selection problem subject to differential privacy and although there are many different mechanisms to use, the exponential mechanism stands out for several reasons:</p>
<ol>
  <li>The exponential mechanism is \(\varepsilon\)-DP and \( \varepsilon^2/8\)-CDP and hence gets improved composition.</li>
  <li>Iteratively applying the exponential mechanism for top-\(k\) can be implemented by adding Gumbel noise to each count and returning the elements with the top-\(k\) noisy counts in one-shot.</li>
  <li>The one-shot Gumbel mechanism returns a ranked list of \(k\) elements, rather than a set of \(k\) elements.</li>
</ol></div>
    </summary>
    <updated>2021-08-09T17:00:00Z</updated>
    <published>2021-08-09T17:00:00Z</published>
    <author>
      <name>Ryan Rogers</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-08-20T22:59:18Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1648249705477846335</id>
    <link href="http://blog.computationalcomplexity.org/feeds/1648249705477846335/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/combing-two-posts-blankface-scott-aa.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/1648249705477846335" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/1648249705477846335" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/08/combing-two-posts-blankface-scott-aa.html" rel="alternate" type="text/html"/>
    <title>Combing two posts: Blankface (Scott Aa) and Is Science Slowing Down? (Scott Al)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(I also posted this to the Less Wrong Website. At least I tried to- I don't quite know if or when it will appear there as its my first post there.) </p><p>Some papers result from taking two papers and combining them. Perhaps nobody else had read both of them so you can say something new! Or (looking over this post) it may guide people to two really good papers, or in this case two really good posts. </p><p>This blog will draw from two excellent blog posts.</p><p>Scott Aaronson  blogged on  his website Aug 2, 2021 about <a href="https://www.scottaaronson.com/blog/?p=5675#comments">blankfaces</a>, people who let stupid or undefined rules dictate what you can do  without apology (see his post for a better explanation). One example that struck me I quote</p><p><i>No, I never applied for that grant. I spend two hours struggling to log in to a web portal designed by the world's top blankfaces until I finally gave up in despair. </i></p><p><i><br/></i></p><p>Scott Alexander blogged  on LessWrong on Nov 26, 2018 about <a href="https://www.lesswrong.com/posts/v7c47vjta3mavY3QC/is-science-slowing-down">Is science slowing down?</a> which answers with an emphatic <i>yes.</i> His point is science-per-researcher is much less than it used to be, and he has graphs and stats to prove it (see his post for the evidence and some speculation as to why this is) One of the reasons he gave struck me which I quote</p><p><i>Certain features of the modern academic system like undepaid PhD's, interminably long postdocs, endless grant writing drudgery, and clueless funders have lowered productivity. The 1930's academic system was ineed 25x more effective at getting researchers to actually do good research.</i></p><p>(A commenter reminded me that Scott Alexander himself dismisses this reason. I do not.) </p><p>(I note that he gives other reasons as well, most notably for our field that the low hanging fruit is gone. Our lack of progress on P vs NP is likely that its a hard problem, rather than the reason above. Of course, if its solved tomorrow by an outsider without funding, I will happily be proven wrong.) </p><p>Scott Alexander hits upon two types of blankfaces (without using the term).</p><p><i>Grant writing drudgery</i>: the rules for how to submit get more and more detailed an onerous. This is  what Scott Aaronson was alluding to. There are other ways its drudgery as well. </p><p><i>Clueless Funders</i>: the people deciding who gets funded might not know the area (actually in my experience the grant I've reviews have been quite good and the problem is more not enough money to award all that are deserving.) </p><p>SO I pose the following non-rhetorically as always</p><p>1) How big a factor is the slowing down of science that blankfaces get in the way?</p><p>2) What can we do about it?</p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><i><br/></i></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-08-09T01:50:00Z</updated>
    <published>2021-08-09T01:50:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-08-20T22:13:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/</id>
    <link href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/" rel="alternate" type="text/html"/>
    <title>School on Modern Directions in Discrete Optimization</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">September 13-17, 2021 Online https://www.him.uni-bonn.de/programs/future-programs/future-trimester-programs/discrete-optimization/discrete-optimization-school/ Aims and Scope: The school provides an introduction to some of the main topics of the trimester program on discrete optimization. The lectures will address the interface between tropical geometry and discrete optimization; recent developments in continuous optimization with applications to combinatorial problems; topics in approximation algorithms; and fixed parameter … <a class="more-link" href="https://cstheory-events.org/2021/08/08/school-on-modern-directions-in-discrete-optimization/">Continue reading <span class="screen-reader-text">School on Modern Directions in Discrete Optimization</span></a></div>
    </summary>
    <updated>2021-08-08T14:26:42Z</updated>
    <published>2021-08-08T14:26:42Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-08-20T23:38:35Z</updated>
    </source>
  </entry>
</feed>
