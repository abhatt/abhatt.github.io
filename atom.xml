<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-04-09T06:22:00Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.04003</id>
    <link href="http://arxiv.org/abs/2004.04003" rel="alternate" type="text/html"/>
    <title>Earned Benefit Maximization in Social Networks Under Budget Constraint</title>
    <feedworld_mtime>1586390400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Banerjee:Suman.html">Suman Banerjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jenamani:Mamata.html">Mamata Jenamani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pratihar:Dilip_Kumar.html">Dilip Kumar Pratihar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.04003">PDF</a><br/><b>Abstract: </b>Given a social network with nonuniform selection cost of the users, the
problem of \textit{Budgeted Influence Maximization} (BIM in short) asks for
selecting a subset of the nodes within an allocated budget for initial
activation, such that due to the cascading effect, influence in the network is
maximized. In this paper, we study this problem with a variation, where a set
of nodes are designated as target nodes, each of them is assigned with a
benefit value, that can be earned by influencing them, and our goal is to
maximize the earned benefit by initially activating a set of nodes within the
budget. We call this problem as the \textsc{Earned Benefit Maximization
Problem}. First, we show that this problem is NP\mbox{-}Hard and the benefit
function is \textit{monotone}, \textit{sub\mbox{-}modular} under the
\textit{Independent Cascade Model} of diffusion. We propose an incremental
greedy strategy for this problem and show, with minor modification it gives
$(1-\frac{1}{\sqrt{e}})$\mbox{-}factor approximation guarantee on the earned
benefit. Next, by exploiting the sub\mbox{-}modularity property of the benefit
function, we improve the efficiency of the proposed greedy algorithm. Then, we
propose a hop\mbox{-}based heuristic method, which works based on the
computation of the `expected earned benefit' of the effective neighbors
corresponding to the target nodes. Finally, we perform a series of extensive
experiments with four real\mbox{-}life, publicly available social network
datasets. From the experiments, we observe that the seed sets selected by the
proposed algorithms can achieve more benefit compared to many existing methods.
Particularly, the hop\mbox{-}based approach is found to be more efficient than
the other ones for solving this problem.
</p></div>
    </summary>
    <updated>2020-04-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03853</id>
    <link href="http://arxiv.org/abs/2004.03853" rel="alternate" type="text/html"/>
    <title>Shape-Constrained Regression using Sum of Squares Polynomials</title>
    <feedworld_mtime>1586390400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Curmei:Mihaela.html">Mihaela Curmei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hall:Georgina.html">Georgina Hall</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03853">PDF</a><br/><b>Abstract: </b>We consider the problem of fitting a polynomial to a set of data points, each
data point consisting of a feature vector and a response variable. In contrast
to standard least-squares polynomial regression, we require that the polynomial
regressor satisfy shape constraints, such as monotonicity with respect to a
variable, Lipschitz-continuity, or convexity over a region. Constraints of this
type appear quite frequently in a number of areas including economics,
operations research, and pricing. We show how to use semidefinite programming
to obtain polynomial regressors that have these properties. We further show
that, under some assumptions on the generation of the data points, the
regressors obtained are consistent estimators of the underlying
shape-constrained function that maps the feature vectors to the responses. We
apply our methodology to the US KLEMS dataset to estimate production of a
sector as a function of capital, energy, labor, materials, and services. We
observe that it outperforms the more traditional approach (which consists in
modelling the production curves as Cobb-Douglas functions) on 50 out of the 65
industries listed in the KLEMS database.
</p></div>
    </summary>
    <updated>2020-04-09T01:20:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-04-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03816</id>
    <link href="http://arxiv.org/abs/2004.03816" rel="alternate" type="text/html"/>
    <title>Graph Matching with Partially-Correct Seeds</title>
    <feedworld_mtime>1586390400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Liren.html">Liren Yu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Jiaming.html">Jiaming Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Xiaojun.html">Xiaojun Lin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03816">PDF</a><br/><b>Abstract: </b>The graph matching problem aims to find the latent vertex correspondence
between two edge-correlated graphs and has many practical applications. In this
work, we study a version of the seeded graph matching problem, which assumes
that a set of seeds, i.e., pre-mapped vertex-pairs, is given in advance.
Specifically, consider two correlated graphs whose edges are sampled
independently with probability $s$ from a parent \ER graph $\mathcal{G}(n,p)$.
Furthermore, a mapping between the vertices of the two graphs is provided as
seeds, of which an unknown $\beta$ fraction is correct. This problem was first
studied in \cite{lubars2018correcting} where an algorithm is proposed and shown
to perfectly recover the correct vertex mapping with high probability if
$\beta\geq\max\left\{\frac{8}{3}p,\frac{16\log{n}}{nps^2}\right\}$. We improve
their condition to $\beta\geq\max\left\{30\sqrt{\frac{\log
n}{n(1-p)^2s^2}},\frac{45\log{n}}{np(1-p)^2s^2}\right)$. However, when
$p=O\left( \sqrt{{\log n}/{ns^2}}\right)$, our improved condition still
requires that $\beta$ must increase inversely proportional to $np$. In order to
improve the matching performance for sparse graphs, we propose a new algorithm
that uses "witnesses" in the 2-hop neighborhood, instead of only 1-hop
neighborhood as in \cite{lubars2018correcting}. We show that when
$np^2\leq\frac{1}{135\log n}$, our new algorithm can achieve perfect recovery
with high probability if $\beta\geq\max\left\{900\sqrt{\frac{np^3(1-s)\log
n}{s}},600\sqrt{\frac{\log n}{ns^4}}, \frac{1200\log n}{n^2p^2s^4}\right\}$ and
$nps^2\geq 128\log n$. Numerical experiments on both synthetic and real graphs
corroborate our theoretical findings and show that our 2-hop algorithm
significantly outperforms the 1-hop algorithm when the graphs are relatively
sparse.
</p></div>
    </summary>
    <updated>2020-04-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03693</id>
    <link href="http://arxiv.org/abs/2004.03693" rel="alternate" type="text/html"/>
    <title>Cutting cycles of rods in space is FPT</title>
    <feedworld_mtime>1586390400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Mitchell.html">Mitchell Jones</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03693">PDF</a><br/><b>Abstract: </b>In this short note, we show that cutting cycles of rods is fixed-parameter
tractable by reducing the problem to computing a feedback vertex set in a mixed
graph.
</p></div>
    </summary>
    <updated>2020-04-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2003.07746</id>
    <link href="http://arxiv.org/abs/2003.07746" rel="alternate" type="text/html"/>
    <title>NP-completeness Results for Graph Burning on Geometric Graphs</title>
    <feedworld_mtime>1586390400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Arya_Tanmay.html">Arya Tanmay Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lokhande:Swapnil.html">Swapnil Lokhande</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mondal:Kaushik.html">Kaushik Mondal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2003.07746">PDF</a><br/><b>Abstract: </b>Graph burning runs on discrete time steps. The aim of the graph burning
problem is to burn all the vertices in a given graph in the least amount of
time steps. The least number of required time steps is known to be the burning
number of the graph. The spread of social influence, an alarm, or a social
contagion can be modeled using graph burning. The less the burning number, the
quick the spread.
</p>
<p>Computationally, graph burning is hard. It has already been proved that
burning of path forests, spider graphs, and trees with maximum degree three are
NP-Complete. In this work we study graph burning on geometric graphs and show
NP-completeness results on several sub classes. More precisely, we show burning
problem to be NP-complete on interval graph, permutation graph and disk graph.
</p></div>
    </summary>
    <updated>2020-04-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/044</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/044" rel="alternate" type="text/html"/>
    <title>TR20-044 |  Cryptography from Information Loss | 

	Marshall Ball, 

	Elette Boyle, 

	Akshay Degwekar, 

	Apoorvaa Deshpande, 

	Alon Rosen, 

	Vinod Vaikuntanathan, 

	Prashant Vasudevan</title>
    <summary>Reductions between problems, the mainstay of theoretical computer science, efficiently map an instance of one problem to an instance of another in such a way that solving the latter allows solving the former. The subject of this work is ``lossy'' reductions, where the reduction loses some information about the input instance. We show that such reductions, when they exist, have interesting and powerful consequences for lifting hardness into ``useful'' hardness, namely cryptography.

Our first, conceptual, contribution is a definition of lossy reductions in the language of mutual information. Roughly speaking, our definition says that a reduction $C$ is $t$-lossy if, for any distribution $X$ over its inputs, the mutual information $I(X;C(X)) \leq t$. Our treatment generalizes a variety of seemingly related but distinct notions such as worst-case to average-case reductions, randomized encodings (Ishai and Kushilevitz, FOCS 2000), homomorphic computations (Gentry, STOC 2009), and instance compression (Harnik and Naor, FOCS 2006).

We then proceed to show several consequences of lossy reductions:

1. We say that a language $L$ has an $f$-reduction to a language $L'$ for a Boolean function $f$ if there is a (randomized) polynomial-time algorithm $C$ that takes an $m$-tuple of strings $X = (x_1,\ldots,x_m)$, with each $x_i\in\{0,1\}^n$, and outputs a string $z$ such that with high probability, L'(z) = f(L(x_1),L(x_2),...,L(x_m))
        
2. Suppose a language $L$ has an $f$-reduction $C$ to $L'$ that is $t$-lossy. Our first result is that one-way functions exist if $L$ is worst-case hard and one of the following conditions holds:
   - $f$ is the OR function, $t \leq m/100$, and $L'$ is the same as $L$
   - $f$ is the Majority function, and $t \leq m/100$
   - $f$ is the OR function, $t \leq O(m\log{n})$, and the reduction has no error

This improves on the implications that follow from combining (Drucker, FOCS 2012) with (Ostrovsky and Wigderson, ISTCS 1993) that result in auxiliary-input one-way functions.

3. Our second result is about the stronger notion of $t$-compressing $f$-reductions -- reductions that only output $t$ bits. We show that if there is an average-case hard language $L$ that has a $t$-compressing Majority reduction to some language for $t=m/100$, then there exist collision-resistant hash functions.

This improves on the result of (Harnik and Naor, STOC 2006), whose starting point is a cryptographic primitive (namely, one-way functions) rather than average-case hardness, and whose assumption is a compressing OR-reduction of SAT (which is now known to be false unless the polynomial hierarchy collapses).

Along the way, we define a non-standard one-sided notion of average-case hardness, which is the notion of hardness used in the second result above, that may be of independent interest.</summary>
    <updated>2020-04-08T07:58:37Z</updated>
    <published>2020-04-08T07:58:37Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-04-09T06:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4719</id>
    <link href="https://www.scottaaronson.com/blog/?p=4719" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4719#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4719" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">When events make craziness sane</title>
    <summary xml:lang="en-US">This post is simply to say the following (and thereby to make it common knowledge that I said it, and that I no longer give a shit who thinks less of me for saying it): If the pandemic has radicalized you, I won’t think that makes you crazy. It’s radicalized me, noticeably shifted my worldview. […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post is simply to say the following (and thereby to make it common knowledge that I said it, and that I no longer give a shit who thinks less of me for saying it):</p>



<p>If the pandemic has radicalized you, I won’t think that makes you crazy.  It’s radicalized me, noticeably shifted my worldview.  And in some sense, I no more apologize for that, than I apologize for my worldview presumably differing from what it would’ve been in some parallel universe with no WWII.</p>



<p>If you suspect that all those earnest, well-intentioned plans and slogans about “flattening the curve” are wonderful and essential, but still, <em>“flattening” is only a desperate gambit to buy some time and nothing more</em>; still, flattening or no flattening, the fundamentals of the situation are that either</p>



<p>(1) a vaccine or cure gets discovered and deployed, or else</p>



<p>(2) we continue in quasi-lockdown mode for the rest of our lives, or else</p>



<p>(3) the virus spreads to the point where it definitely kills some people you know,</p>



<p>—if you suspect this, then at least in my book you’re not crazy.  I suspect the same.</p>



<p>If you still don’t understand, no matter how patiently it’s explained to you, why ~18 months is the <em>absolute bare minimum</em> needed to get a vaccine out; if all the talk of Phase 1, 2, and 3 trials and the need to learn more about rare side effects and so forth seems hard to square with the desperate world war that this is; if you wonder whether the Allied commanders and Allied medical authorities in WWII, transported to the present, would <em>agree</em> that 18 months is the bare minimum, or whether they’d already be distributing vaccines a month ago that probably work well enough and do bounded damage if they don’t—I hereby confess that I don’t understand it either.</p>



<p>If you wonder how the US will possibly hold an election in November that the world won’t rightly consider a sham—given that the only safe way will be universal vote-by-mail, but Trump and his five Vichy justices will never allow it—know that I wonder this too.</p>



<p>If you think that all those psychiatrists now doing tele-psychiatry should tell their patients, “listen, I’ve been noticing an unhealthy absence of panic attacks, obsessions about the government trying to kill your family, or compulsive disinfecting of doorknobs, so I think we’d better up your dose of pro-anxiety medication”—I’m with you.</p>



<p>If you see any US state that wants to avoid &gt;2% deaths, being pushed to the brink of openly defying the FDA, smuggling in medical supplies to escape federal confiscation, using illegal tests and illegal masks and illegal ventilators and illegal everything else, and you also see military commanders getting fired for going outside the chain of command to protect their soldiers’ lives, and you wonder whether this is the start of some broader dissolution of the Union—well, <em>I</em> don’t intend to repeat the mistake of underestimating this crisis.</p>



<p>If you think that the feds who <em>literally confiscate medical supplies before they can reach the hospitals</em>, might as well just shoot the patients as they’re wheeled into the ICU and say “we’re sorry, but this action was obligatory under directive 48c(7)”—I won’t judge you for feeling that way. </p>



<p>If you feel like, while there are still pockets of brilliance and kindness and inspiration and even heroism all over US territory, still, as a federal entity the United States <em>effectively no longer exists or functions</em>, at least not if you treat “try to stop the mass death of the population” as a nonnegotiable component of the “life, liberty, and happiness” foundation for the nation’s existence—if you think this, I won’t call you crazy.  I feel more like a citizen of nowhere every day.</p>



<p>If you’d jump, should the opportunity arise (as it won’t), to appoint Bill Gates as temporary sovereign for as long as this crisis lasts, and thereafter hold a new Constitutional Convention to design a stronger democracy, attempting the first-ever Version 2.0 (as opposed to 1.3, 1.4, etc.) of the American founders’ vision, this time with <em>even more</em> safeguards against destruction by know-nothings and demagogues—if you’re in for that, I don’t think you’re crazy.  I’m wondering where to sign up.</p>



<p>Finally, if you’re one of the people who constantly emails me wrong P=NP proofs or local hidden-variable explanations of quantum mechanics … sorry, I still think you’re crazy.  That stuff hasn’t been affected.</p>



<p>Happy Passover and Easter!</p></div>
    </content>
    <updated>2020-04-08T02:58:26Z</updated>
    <published>2020-04-08T02:58:26Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Embarrassing Myself"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-04-08T06:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03493</id>
    <link href="http://arxiv.org/abs/2004.03493" rel="alternate" type="text/html"/>
    <title>Exact Single-Source SimRank Computation on Large Graphs</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Hanzhi.html">Hanzhi Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei:Zhewei.html">Zhewei Wei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuan:Ye.html">Ye Yuan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Du:Xiaoyong.html">Xiaoyong Du</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wen:Ji=Rong.html">Ji-Rong Wen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03493">PDF</a><br/><b>Abstract: </b>SimRank is a popular measurement for evaluating the node-to-node similarities
based on the graph topology. In recent years, single-source and top-$k$ SimRank
queries have received increasing attention due to their applications in web
mining, social network analysis, and spam detection. However, a fundamental
obstacle in studying SimRank has been the lack of ground truths. The only exact
algorithm, Power Method, is computationally infeasible on graphs with more than
$10^6$ nodes. Consequently, no existing work has evaluated the actual
trade-offs between query time and accuracy on large real-world graphs. In this
paper, we present ExactSim, the first algorithm that computes the exact
single-source and top-$k$ SimRank results on large graphs. With high
probability, this algorithm produces ground truths with a rigorous theoretical
guarantee. We conduct extensive experiments on real-world datasets to
demonstrate the efficiency of ExactSim. The results show that ExactSim provides
the ground truth for any single-source SimRank query with a precision up to 7
decimal places within a reasonable query time.
</p></div>
    </summary>
    <updated>2020-04-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03486</id>
    <link href="http://arxiv.org/abs/2004.03486" rel="alternate" type="text/html"/>
    <title>Probing a Set of Trajectories to Maximize Captured Information</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fekete:S=aacute=ndor_P=.html">Sándor P. Fekete</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hill:Alexander.html">Alexander Hill</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krupke:Dominik.html">Dominik Krupke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mayer:Tyler.html">Tyler Mayer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitchell:Joseph_S=_B=.html">Joseph S. B. Mitchell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parekh:Ojas.html">Ojas Parekh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Phillips:Cynthia_A=.html">Cynthia A. Phillips</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03486">PDF</a><br/><b>Abstract: </b>We study a trajectory analysis problem we call the Trajectory Capture Problem
(TCP), in which, for a given input set ${\cal T}$ of trajectories in the plane,
and an integer $k\geq 2$, we seek to compute a set of $k$ points (``portals'')
to maximize the total weight of all subtrajectories of ${\cal T}$ between pairs
of portals. This problem naturally arises in trajectory analysis and
summarization.
</p>
<p>We show that the TCP is NP-hard (even in very special cases) and give some
first approximation results. Our main focus is on attacking the TCP with
practical algorithm-engineering approaches, including integer linear
programming (to solve instances to provable optimality) and local search
methods. We study the integrality gap arising from such approaches. We analyze
our methods on different classes of data, including benchmark instances that we
generate. Our goal is to understand the best performing heuristics, based on
both solution time and solution quality. We demonstrate that we are able to
compute provably optimal solutions for real-world instances.
</p></div>
    </summary>
    <updated>2020-04-08T23:32:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03477</id>
    <link href="http://arxiv.org/abs/2004.03477" rel="alternate" type="text/html"/>
    <title>An Algorithm for Context-Free Path Queries over Graph Databases</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Medeiros:Ciro_M=.html">Ciro M. Medeiros</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musicante:Martin_A=.html">Martin A. Musicante</a>, Umberto S. Costa <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03477">PDF</a><br/><b>Abstract: </b>RDF (Resource Description Framework) is a standard language to represent
graph databases. Query languages for RDF databases usually include primitives
to support path queries, linking pairs of vertices of the graph that are
connected by a path of labels belonging to a given language. Languages such as
SPARQL include support for paths defined by regular languages (by means of
Regular Expressions). A context-free path query is a path query whose language
can be defined by a context-free grammar. Context-free path queries can be used
to implement queries such as the "same generation queries", that are not
expressible by Regular Expressions. In this paper, we present a novel algorithm
for context-free path query processing. We prove the correctness of our
approach and show its run-time and memory complexity. We show the viability of
our approach by means of a prototype implemented in Go. We run our prototype
using the same cases of study as proposed in recent works, comparing our
results with another, recently published algorithm. The experiments include
both synthetic and real RDF databases. Our algorithm can be seen as a step
forward, towards the implementation of more expressive query languages.
</p></div>
    </summary>
    <updated>2020-04-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03387</id>
    <link href="http://arxiv.org/abs/2004.03387" rel="alternate" type="text/html"/>
    <title>Exact separation of forbidden-set cuts associated with redundant parity checks of binary linear codes</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Puchert:Christian.html">Christian Puchert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tillmann:Andreas_M=.html">Andreas M. Tillmann</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03387">PDF</a><br/><b>Abstract: </b>In recent years, several integer programming (IP) approaches were developed
for maximum-likelihood decoding and minimum distance computation for binary
linear codes. Two aspects in particular have been demonstrated to improve the
performance of IP solvers as well as adaptive linear programming decoders: the
dynamic generation of forbidden-set (FS) inequalities, a family of valid
cutting planes, and the utilization of so-called redundant parity-checks
(RPCs). However, to date, it had remained unclear how to solve the exact RPC
separation problem (i.e., to determine whether or not there exists any violated
FS inequality w.r.t. any known or unknown parity-check). In this note, we prove
NP-hardness of this problem. Moreover, we formulate an IP model that combines
the search for most violated FS cuts with the generation of RPCs, and report on
computational experiments. Empirically, for various instances of the minimum
distance problem, it turns out that while utilizing the exact separation IP
does not appear to provide a computational advantage, it can apparently be
avoided altogether by combining heuristics to generate RPC-based cuts.
</p></div>
    </summary>
    <updated>2020-04-08T23:20:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03206</id>
    <link href="http://arxiv.org/abs/2004.03206" rel="alternate" type="text/html"/>
    <title>Zipping Segment Trees</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barth:Lukas.html">Lukas Barth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wagner:Dorothea.html">Dorothea Wagner</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03206">PDF</a><br/><b>Abstract: </b>Stabbing queries in sets of intervals are usually answered using segment
trees. A dynamic variant of segment trees has been presented by van Kreveld and
Overmars, which uses red-black trees to do rebalancing operations. This paper
presents zipping segment trees - dynamic segment trees based on zip trees,
which were recently introduced by Tarjan et al. To facilitate zipping segment
trees, we show how to uphold certain segment tree properties during the
operations of a zip tree. We present an in-depth experimental evaluation and
comparison of dynamic segment trees based on red-black trees, weight-balanced
trees and several variants of the novel zipping segment trees. Our results
indicate that zipping segment trees perform better than rotation-based
alternatives.
</p></div>
    </summary>
    <updated>2020-04-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03166</id>
    <link href="http://arxiv.org/abs/2004.03166" rel="alternate" type="text/html"/>
    <title>The Optimality of Profile Maximum Likelihood in Estimating Sorted Discrete Distributions</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Han:Yanjun.html">Yanjun Han</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shiragur:Kirankumar.html">Kirankumar Shiragur</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03166">PDF</a><br/><b>Abstract: </b>A striking result of [Acharya et al. 2017] showed that to estimate symmetric
properties of discrete distributions, plugging in the distribution that
maximizes the likelihood of observed multiset of frequencies, also known as the
profile maximum likelihood (PML) distribution, is competitive compared with any
estimators regardless of the symmetric property. Specifically, given $n$
observations from the discrete distribution, if some estimator incurs an error
$\varepsilon$ with probability at most $\delta$, then plugging in the PML
distribution incurs an error $2\varepsilon$ with probability at most
$\delta\exp(3\sqrt{n})$. In this paper, we strengthen the above result and show
that using a careful chaining argument, the error probability can be reduced to
$\delta^{1-c}\exp(c'n^{1/3+c})$ for arbitrarily small constants $c&gt;0$ and some
constant $c'&gt;0$. In particular, we show that the PML distribution is an optimal
estimator of the sorted true distribution: it is $\varepsilon$-close in sorted
$\ell_1$ distance to the true distribution with support size $k$ for any
$n=\Omega(k/(\varepsilon^2\log k))$ and $\varepsilon\gg n^{-1/3}$, which are
the information-theoretically optimal sample complexity and the largest error
regime where the classical empirical distribution is sub-optimal, respectively.
</p>
<p>In order to strengthen the analysis of the PML, a key ingredient is to employ
novel "continuity" properties of the PML distributions and construct a chain of
suitable quantized PMLs, or "coverings". We also construct a novel
approximation-based estimator for the sorted distribution with a near-optimal
concentration property without any sample splitting, where as a byproduct we
obtain tight trade-offs between the polynomial approximation error and the
maximum magnitude of coefficients in the Poisson approximation of $1$-Lipschitz
functions.
</p></div>
    </summary>
    <updated>2020-04-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03114</id>
    <link href="http://arxiv.org/abs/2004.03114" rel="alternate" type="text/html"/>
    <title>Approximating Min-Mean-Cycle for low-diameter graphs in near-optimal time and memory</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Altschuler:Jason_M=.html">Jason M. Altschuler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parrilo:Pablo_A=.html">Pablo A. Parrilo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03114">PDF</a><br/><b>Abstract: </b>We revisit Min-Mean-Cycle, the classical problem of finding a cycle in a
weighted directed graph with minimum mean weight. Despite an extensive
algorithmic literature, previous work falls short of a near-linear runtime in
the number of edges $m$--in fact, there is a natural barrier which precludes
such a runtime for solving Min-Mean-Cycle exactly. Here, we give a much faster
approximation algorithm that, for graphs with polylogarithmic diameter, has
near-linear runtime. In particular, this is the first algorithm whose runtime
for the complete graph scales in the number of vertices $n$ as
$\tilde{O}(n^2)$. Moreover--unconditionally on the diameter--the algorithm uses
only $O(n)$ memory beyond reading the input, making it "memory-optimal". The
algorithm is also simple to implement and has remarkable practical performance.
</p>
<p>Our approach is based on solving a linear programming (LP) relaxation using
entropic regularization, which effectively reduces the LP to a Matrix Balancing
problem--a la the popular reduction of Optimal Transport to Matrix Scaling. We
then round the fractional LP solution using a variant of the classical
Cycle-Cancelling algorithm that is sped up to near-linear runtime at the
expense of being approximate, and implemented in a memory-optimal manner.
</p>
<p>We also provide an alternative algorithm with slightly faster theoretical
runtime, albeit worse memory usage and practicality. This algorithm uses the
same rounding procedure, but solves the LP relaxation by leveraging recent
developments in area-convexity regularization. Its runtime scales inversely in
the approximation accuracy, which we show is optimal--barring a major
breakthrough in algorithmic graph theory, namely faster Shortest Paths
algorithms.
</p></div>
    </summary>
    <updated>2020-04-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03050</id>
    <link href="http://arxiv.org/abs/2004.03050" rel="alternate" type="text/html"/>
    <title>The Impact of Message Passing in Agent-Based Submodular Maximization</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grimsman:David.html">David Grimsman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kirchner:Matthew_R=.html">Matthew R. Kirchner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hespanha:Jo=atilde=o_P=.html">João P. Hespanha</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marden:Jason_R=.html">Jason R. Marden</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03050">PDF</a><br/><b>Abstract: </b>Submodular maximization problems are a relevant model set for many real-world
applications. Since these problems are generally NP-Hard, many methods have
been developed to approximate the optimal solution in polynomial time. One such
approach uses an agent-based greedy algorithm, where the goal is for each agent
to choose an action from its action set such that the union of all actions
chosen is as high-valued as possible. Recent work has shown how the performance
of the greedy algorithm degrades as the amount of information shared among the
agents decreases, whereas this work addresses the scenario where agents are
capable of sharing more information than allowed in the greedy algorithm.
Specifically, we show how performance guarantees increase as agents are capable
of passing messages, which can augment the allowable decision set for each
agent. Under these circumstances, we show a near-optimal method for message
passing, and how much such an algorithm could increase performance for any
given problem instance.
</p></div>
    </summary>
    <updated>2020-04-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.03033</id>
    <link href="http://arxiv.org/abs/2004.03033" rel="alternate" type="text/html"/>
    <title>SOPanG 2: online searching over a pan-genome without false positives</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Aleksander Cisłak, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grabowski:Szymon.html">Szymon Grabowski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.03033">PDF</a><br/><b>Abstract: </b>The pan-genome can be stored as elastic-degenerate (ED) string, a recently
introduced compact representation of multiple overlapping sequences. However, a
search over the ED string does not indicate which individuals (if any) match
the entire query. We augment the ED string with sources (individuals' indexes)
and propose an extension of the SOPanG (Shift-Or for Pan-Genome) tool to report
only true positive matches, omitting those not occurring in any of the
haplotypes. The additional stage for checking the matches yields a penalty of
less than 3.5% relative speed in practice, which means that SOPanG 2 is able to
report pattern matches in a pan-genome, mapping them onto individuals, at the
single-thread throughput of above 430 MB/s on real data.
</p>
<p>Availability and implementation: SOPanG 2 can be downloaded here:
github.com/MrAlexSee/sopang.
</p>
<p>Contact: sgrabow@kis.p.lodz.pl
</p></div>
    </summary>
    <updated>2020-04-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.02881</id>
    <link href="http://arxiv.org/abs/2004.02881" rel="alternate" type="text/html"/>
    <title>Parametrization of Neural Networks with Connected Abelian Lie Groups as Data Manifold</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Melodia:Luciano.html">Luciano Melodia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lenz:Richard.html">Richard Lenz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.02881">PDF</a><br/><b>Abstract: </b>Neural nets have been used in an elusive number of scientific disciplines.
Nevertheless, their parameterization is largely unexplored. Dense nets are the
coordinate transformations of a manifold from which the data is sampled. After
processing through a layer, the representation of the original manifold may
change. This is crucial for the preservation of its topological structure and
should therefore be parameterized correctly. We discuss a method to determine
the smallest topology preserving layer considering the data domain as abelian
connected Lie group and observe that it is decomposable into $\mathbb{R}^p
\times \mathbb {T}^q$. Persistent homology allows us to count its $k$-th
homology groups. Using K\"unneth's theorem, we count the $k$-th Betti numbers.
Since we know the embedding dimension of $\mathbb{R}^p$ and $\mathcal{S}^1$, we
parameterize the bottleneck layer with the smallest possible matrix group,
which can represent a manifold with those homology groups. Resnets guarantee
smaller embeddings due to the dimension of their state space representation.
</p></div>
    </summary>
    <updated>2020-04-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1605.06362</id>
    <link href="http://arxiv.org/abs/1605.06362" rel="alternate" type="text/html"/>
    <title>Reconstruction of convex bodies from moments</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kousholt:Astrid.html">Astrid Kousholt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schulte:Julia.html">Julia Schulte</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1605.06362">PDF</a><br/><b>Abstract: </b>We investigate how much information about a convex body can be retrieved from
a finite number of its geometric moments. We give a sufficient condition for a
convex body to be uniquely determined by a finite number of its geometric
moments, and we show that among all convex bodies, those which are uniquely
determined by a finite number of moments form a dense set. Further, we derive a
stability result for convex bodies based on geometric moments. It turns out
that the stability result is improved considerably by using another set of
moments, namely Legendre moments. We present a reconstruction algorithm that
approximates a convex body using a finite number of its Legendre moments. The
consistency of the algorithm is established using the stability result for
Legendre moments. When only noisy measurements of Legendre moments are
available, the consistency of the algorithm is established under certain
assumptions on the variance of the noise variables.
</p></div>
    </summary>
    <updated>2020-04-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1511.00820</id>
    <link href="http://arxiv.org/abs/1511.00820" rel="alternate" type="text/html"/>
    <title>Local digital algorithms applied to Boolean models</title>
    <feedworld_mtime>1586304000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/H=ouml=rrmann:Julia.html">Julia Hörrmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svane:Anne_Marie.html">Anne Marie Svane</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1511.00820">PDF</a><br/><b>Abstract: </b>We investigate the estimation of specific intrinsic volumes of stationary
Boolean models by local digital algorithms; that is, by weighted sums of $n
\times\ldots \times n$ configuration counts. We show that asymptotically
unbiased estimators for the specific surface area or integrated mean curvature
do not exist if the dimension is at least two or three, respectively. For
3-dimensional stationary, isotropic Boolean models, we derive asymptotically
unbiased estimators for the specific surface area and integrated mean
curvature. For a Boolean model with balls as grains we even obtain an
asymptotically unbiased estimator for the specific Euler characteristic.
</p></div>
    </summary>
    <updated>2020-04-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3470</id>
    <link href="https://agtb.wordpress.com/2020/04/07/call-for-papers-the-2nd-workshop-on-behavioral-economics-and-computation/" rel="alternate" type="text/html"/>
    <title>Call for Papers: The 2nd Workshop on Behavioral Economics and Computation</title>
    <summary>We solicit research contributions and participants for the 2nd Workshop on Behavioral Economics and Computation, to be held in conjunction with the Twenty-First ACM Conference on Economics and Computation (ACM EC '20).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div><span><a href="https://sites.google.com/view/behavioralec2020/">https://sites.google.com/view/behavioralec2020/</a><br/>
July 17, 2020, Budapest, Hungary<br/>
At the 21st ACM Conference on Economics and Computation (ACM EC ’20)</span></div>
<div><span>**In the event the in-person conference does not happen due to the COVID-19 pandemic, we will hold the workshop virtually.</span></div>
<div/>
<div><strong><span style="color: #ff0000;">SUBMISSIONS DUE May 18, 2020, 11:59pm PDT.</span></strong></div>
<div/>
<div/>
<div><strong>Call for Papers: the 2nd Workshop on Behavioral Economics and Computation</strong></div>
<div/>
<div><span>We solicit research contributions and participants for the 2nd Workshop on Behavioral Economics and Computation, to be held in conjunction with the Twenty-First ACM Conference on Economics and Computation (ACM EC ’20). </span></div>
<div/>
<div><span>Based on the successful workshop last year, we aim to bring together again researchers and practitioners from diverse subareas of EC, who are interested in the intersection of human economic behavior and computation, to share new results and to discuss future directions for behavioral research related to economics and computation. It will be a full-day workshop, and will feature invited speakers, contributed paper presentations and a panel discussion. </span></div>
<div/>
<div><span>The gap between rationality-based analysis that assumes utility-maximizing agents and actual human behavior in the real world has been well recognized in economics, psychology and other social sciences. In recent years, there has been growing interest in conducting behavioral research across many of the sub-areas related to economics and computation to address this gap. In one direction, some of these studies leverage insights on human decision making from the behavioral economics and psychology literature to study economic and computational systems with human users. In the other direction, computational tools are used to study and gain insights on human behavior and a data-driven approach is used to learn behavior models from user-generated data.</span></div>
<div><span><br/>
The 2nd Behavioral EC workshop aims to provide a venue for researchers and practitioners from diverse fields, including but not limited to computer science, economics, psychology and sociology, to exchange ideas related to behavioral research in economics and computation. In addition to sharing new results, we hope the workshop will foster a lively discussion of future directions and methodologies for behavioral research related to economics and computation as well as fruitful cross-pollination of behavioral economics, cognitive psychology and computer science. </span></div>
<div><span><br/>
We welcome studies at the intersection of economic behavior and computation from a rich set of theoretical, experimental and empirical perspectives. The topics of interest for the workshop are behavioral research in all settings covered by EC, including but not limited to:</span></div>
<ul>
<li><span>Behavioral mechanism design and applied mechanism design</span></li>
<li><span>Boundedly-rational models of economic decision making</span></li>
<li><span>Empirical studies of human economic behavior</span></li>
<li><span>Model evaluation and selection based on behavioral data</span></li>
<li><span>Data-driven modelling</span></li>
<li><span>Online prediction markets, online experiments, and crowdsourcing platforms</span></li>
<li><span>Hybrid human-machine systems</span></li>
<li><span>Models and experiments about social considerations (e.g. fairness and trust) in decision making</span></li>
<li><span>Methods for behavioral EC: information aggregation, probability elicitation, quality control</span></li>
</ul>
<div/>
<div/>
<div><span><strong>Submission Instructions</strong><br/>
</span></div>
<div/>
<div><span style="color: #ff0000;">Submission deadline: May 18, 2020, 11:59pm PDT.</span></div>
<div><span style="color: #ff0000;">Notification: June 11, 2020</span></div>
<div/>
<div><span>All submissions will be peer reviewed. We will give priority to new (unpublished) research papers but will also consider ongoing research and recently published papers that may be of interest to the workshop audience. For submissions of published papers, authors must clearly state the venue of publication. Position papers and panel discussion proposals are also welcome. Papers will be reviewed for relevance, significance, originality, research contribution, and likelihood to catalyze discussion. </span></div>
<div><span><br/>
Submissions can be in any format and any length. We recommend the EC submission format. </span><span>The workshop will not have archival proceedings but will post accepted papers on the workshop website. At least one author of each accepted paper will be expected to attend and present their findings at the workshop.<p/>
<p/></span></div>
<div><span>Submissions should be uploaded to Easychair no later than May 18th, 2020, 11:59pm PDT. </span></div>
<div/>
<div/>
<div><span><strong>Organizing Committee</strong><br/>
</span></div>
<div><span><br/>
Yiling Chen, Harvard University<br/>
Dan Goldstein, Microsoft Research<br/>
Kevin Leyton-Brown, University of British Columbia<br/>
Shengwu Li, Harvard University<br/>
Gali Noti, Hebrew University</span></div>
<div/>
<div><span><br/>
<strong>More Information</strong><br/>
</span></div>
<div><span><br/>
For more information or questions, visit the workshop website:<br/>
<a href="https://sites.google.com/view/behavioralec2020/">https://sites.google.com/view/behavioralec2020/</a><br/>
or email the organizing committee: <a href="mailto:behavioralec2020@easychair.org">behavioralec2020@easychair.org</a></span></div>
			<div id="atatags-26942-5e8be264da029"/></div>
    </content>
    <updated>2020-04-07T02:15:59Z</updated>
    <published>2020-04-07T02:15:59Z</published>
    <category term="Uncategorized"/>
    <category term="Conferences"/>
    <author>
      <name>Kevin Leyton-Brown</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-04-09T06:20:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3272927110650993879</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3272927110650993879/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/return-of-vidcast.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3272927110650993879" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3272927110650993879" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/return-of-vidcast.html" rel="alternate" type="text/html"/>
    <title>Return of the Vidcast</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Bill and I just have a <a href="https://youtu.be/VTX3yiPri5c">discussion</a>, virtually of course.
<br/><br/>
<div><br/></div><div><br/></div></div>
    </content>
    <updated>2020-04-06T21:36:00Z</updated>
    <published>2020-04-06T21:36:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-04-08T20:36:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/04/06/professorship-in-theoretical-computer-science-at-university-of-copenhagen-apply-by-may-24-2020/</id>
    <link href="https://cstheory-jobs.org/2020/04/06/professorship-in-theoretical-computer-science-at-university-of-copenhagen-apply-by-may-24-2020/" rel="alternate" type="text/html"/>
    <title>Professorship in Theoretical Computer Science at University of Copenhagen (apply by May 24, 2020)</title>
    <summary>University of Copenhagen is seeking candidates for a full professorship in Theoretical Computer Science. More specifically, we are inviting exceptional candidates from the broad fields of algorithms, complexity, and cryptography including privacy. The application deadline is May 24, 2020. Enquiries are welcome and may be sent to Mikkel Thorup (mthorup@di.ku.dk) or Mads Nielsen (madsn@di.ku.dk). Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>University of Copenhagen is seeking candidates for a full professorship in Theoretical Computer Science. More specifically, we are inviting exceptional candidates from the broad fields of algorithms, complexity, and cryptography including privacy. The application deadline is May 24, 2020. Enquiries are welcome and may be sent to Mikkel Thorup (mthorup@di.ku.dk) or Mads Nielsen (madsn@di.ku.dk).</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18971&amp;ProjectId=151668">https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18971&amp;ProjectId=151668</a><br/>
Email: mthorup@di.ku.dk</p></div>
    </content>
    <updated>2020-04-06T19:13:00Z</updated>
    <published>2020-04-06T19:13:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-04-09T06:20:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16911</id>
    <link href="https://rjlipton.wordpress.com/2020/04/05/not-as-easy-as-abc/" rel="alternate" type="text/html"/>
    <title>Not As Easy As ABC</title>
    <summary>Is the claimed proof of the ABC conjecture correct? [ Photo courtesy of Kyodo University ] Shinichi Mochizuki is about to have his proof of the ABC conjecture published in a journal. The proof needs more than a ream of paper—that is, it is over 500 pages long. Today I thought we would discuss his […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Is the claimed proof of the ABC conjecture correct?</em><br/>
<font color="#000000"/></p><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/05/not-as-easy-as-abc/screen-shot-2020-04-05-at-8-22-38-pm/" rel="attachment wp-att-16914"><img alt="" class="alignright size-medium wp-image-16914" height="238" src="https://rjlipton.files.wordpress.com/2020/04/screen-shot-2020-04-05-at-8.22.38-pm.png?w=300&amp;h=238" width="300"/></a><p/>
<p>
<font color="#0044cc">
</font></p></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Photo courtesy of Kyodo University ]</font></td>
</tr>
</tbody>
</table>
<p>
Shinichi Mochizuki is about to have his proof of the ABC conjecture <a href="https://futurism.com/the-byte/mathematicians-shocked-paper-published">published</a> in a journal. The proof needs more than a ream of paper—that is, it is over 500 pages long. </p>
<p>
Today I thought we would discuss his claimed proof of this famous conjecture.</p>
<p>
The decision to published is also discussed in an <a href="https://www.nature.com/articles/d41586-020-00998-2">article</a> in <em>Nature</em>. Some of the discussion we have seen <a href="https://www.math.columbia.edu/~woit/wordpress/?p=11709">elsewhere</a> has been about personal factors. We will just comment briefly on the problem, the proof, and how to tell if a proof has problems. </p>
<p>
</p><p/><h2> The Problem </h2><p/>
<p/><p>
Number theory is hard because addition and multiplication do not play well together. Adding numbers is not too complex by its self; multiplication by its self is also not too hard. For those into formal logic the theory of addition for example is decidable. So in principle there is no hard problem that only uses addition. None. A similar point follows for multiplication. </p>
<p>
But together addition and multiplication is hard. Of course Kurt Gödel proved that the formal theory of arithmetic is hard. It is not complete, for example. There must be statements about addition and multiplication that are unprovable in Peano Arithmetic. </p>
<p>
The ABC conjecture states a property that is between addition and multiplication. Suppose that 	</p>
<p align="center"><img alt="\displaystyle  A + B = C, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A+%2B+B+%3D+C%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A + B = C, "/></p>
<p>for some integers <img alt="{1 \le A \le B \le C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+A+%5Cle+B+%5Cle+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \le A \le B \le C}"/>. Then 	</p>
<p align="center"><img alt="\displaystyle  C \le ABC " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+ABC+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le ABC "/></p>
<p>is trivial. The ABC conjecture says that one can do better and get 	</p>
<p align="center"><img alt="\displaystyle  C \le F(ABC), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+F%28ABC%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le F(ABC), "/></p>
<p>for a function <img alt="{F(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(X)}"/> that is sometimes much smaller than <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>. The function <img alt="{F(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(X)}"/> depends not on the size of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> but on the multiplicative structure of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>. That is the function depends on the multiplicative structure of the integers. Note, the bound 	</p>
<p align="center"><img alt="\displaystyle  C \le ABC " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+ABC+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le ABC "/></p>
<p>only needed that <img alt="{A,B,C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B,C}"/> were numbers larger than <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. The stronger bound 	</p>
<p align="center"><img alt="\displaystyle  C \le F(ABC), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+F%28ABC%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le F(ABC), "/></p>
<p>relies essentially on the finer structure of the integers. </p>
<p>
Roughly <img alt="{F(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(X)}"/> operates as follows: Compute all the primes <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> that divide <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>. Let <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> be the product of all these primes. Then <img alt="{F(X) \le Q^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28X%29+%5Cle+Q%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(X) \le Q^{2}}"/> works: 	</p>
<p align="center"><img alt="\displaystyle  C \le Q^{2}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+Q%5E%7B2%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le Q^{2}. "/></p>
<p>The key point is: <i>Even if <img alt="{p^{100}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B100%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p^{100}}"/>, for example, divides <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>, we only include <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> in the product <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/>.</i> This is where the savings all comes from. This is why the ABC conjecture is hard: repeated factors are thrown away.</p>
<p>
Well not exactly, there is a constant missing here, the bound is 	</p>
<p align="center"><img alt="\displaystyle  C \le \alpha Q^{2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+%5Calpha+Q%5E%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le \alpha Q^{2} "/></p>
<p>where <img alt="{\alpha&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha&gt;0}"/> is a universal constant. We can replace <img alt="{Q^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q^{2}}"/> by a smaller number—the precise statement can be found <a href="https://en.wikipedia.org/wiki/Abc_conjecture">here</a>. This is the ABC conjecture. </p>
<p>
The point here is that in many cases <img alt="{F(ABC)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28ABC%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(ABC)}"/> is vastly smaller than <img alt="{ABC}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BABC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ABC}"/> and so that inequality 	</p>
<p align="center"><img alt="\displaystyle  C \le F(ABC), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+F%28ABC%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le F(ABC), "/></p>
<p>is much better than the obvious one of 	</p>
<p align="center"><img alt="\displaystyle  C \le ABC. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+ABC.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le ABC. "/></p>
<p>For example, suppose that one wishes to know if 	</p>
<p align="center"><img alt="\displaystyle  5^{z} = 2^{x} + 3^{y}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++5%5E%7Bz%7D+%3D+2%5E%7Bx%7D+%2B+3%5E%7By%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  5^{z} = 2^{x} + 3^{y}, "/></p>
<p>is possible. The ABC conjecture shows that this cannot happen for <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> large enough. Note 	</p>
<p align="center"><img alt="\displaystyle  F(2^{x} 3^{y} 5^{z}) = 30 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%282%5E%7Bx%7D+3%5E%7By%7D+5%5E%7Bz%7D%29+%3D+30+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(2^{x} 3^{y} 5^{z}) = 30 "/></p>
<p>for positive integers <img alt="{x,y,z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y,z}"/>.</p>
<p>
</p><p/><h2> Is He Correct? </h2><p/>
<p/><p>
Eight years ago Mochizuki announced his <a href="http://www.kurims.kyoto-u.ac.jp/~motizuki/papers-english.html">proof</a>. Now it is about to be published in a journal. He is famous for work in part of number theory. He solved a major open problem there years ago. This gave him instant credibility and so his claim of solving the ABC conjecture was taken seriously. </p>
<p>
For example, one of his papers is <a href="https://www.math.uni-bielefeld.de/documenta/vol-kato/mochizuki.dm.pdf">The Absolute Anabelian Geometry of Canonical Curves</a>. The paper says: </p>
<blockquote><p><b> </b> <em> How much information about the isomorphism class of the variety <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X}"/> is contained in the knowledge of the étale fundamental group? </em>
</p></blockquote>
<p/><p>
A glance at this paper shows that it is for specialists only. But it does seem to be math of the type that we see all the time. And indeed the proof in his paper is long believed to be correct. This is in sharp contrast to his proof of the ABC conjecture. </p>
<p>
</p><p/><h2> Indicators of Correctness </h2><p/>
<p/><p>
The question is: Are there ways to detect if a proof is (in)correct? Especially <a href="https://en.wikipedia.org/wiki/List_of_long_mathematical_proofs">long</a> proofs? Are there ways that rise above just checking the proof line by line? By the way:</p>
<blockquote><p><b> </b> <em> The length of unusually long proofs has increased with time. As a rough rule of thumb, 100 pages in 1900, or 200 pages in 1950, or 500 pages in 2000 is unusually long for a proof. </em>
</p></blockquote>
<p/><p>
There are some ways to gain confidence. Here are some in my opinion that are useful.</p>
<ol>
<li>
Is the proof understood by the experts? <p/>
</li><li>
Has the proof been generalized? <p/>
</li><li>
Have new proofs been found? <p/>
</li><li>
Does the proof have a clear roadmap?
</li></ol>
<p>
The answer to the first question (1) seems to be no for the ABC proof. At least two world experts have raised concerns—see this <a href="https://www.quantamagazine.org/titans-of-mathematics-clash-over-epic-proof-of-abc-conjecture-20180920/">article</a> in <em>Quanta</em>—that appear serious. The proof has not yet been generalized. This is an important milestone for any proof. Andrew Wiles famous proof that the Fermat equation 	</p>
<p align="center"><img alt="\displaystyle  x^{p} + y^{p} = z^{p}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7Bp%7D+%2B+y%5E%7Bp%7D+%3D+z%5E%7Bp%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^{p} + y^{p} = z^{p}, "/></p>
<p>has no solutions in integers for <img alt="{xyz \neq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bxyz+%5Cneq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{xyz \neq 0}"/> and <img alt="{p \ge 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%5Cge+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p \ge 3}"/> a prime has been extended. This certainly adds confidence to our belief that it is correct.</p>
<p>
Important problems eventually get other proofs. This can take some time. But there is almost always success in finding new and different proofs. Probably it is way too early for the ABC proof, but we can hope. Finally the roadmap issue: This means does the argument used have a nice logical flow. Proofs, even long proofs, often have a logic flow that is not too complex. A proof that says: Suppose there is a object <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> with this property. Then it follows that there must be an object <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/> so that <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/> Is more believable than one with a much more convoluted logical flow. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Ivan Fesenko of Nottingham has written an <a href="https://www.maths.nottingham.ac.uk/plp/pmzibf/rpp.pdf">essay</a> about the proof and the decision to publish. Among factors he notes is “the potential lack of mathematical infrastructure and language to communicate novel concepts and methods”—noting the steep learning curve of trying to grasp the language and framework in which Mochizuki has set his proof. Will the decision to publish change the dynamics of this effort?</p>
<p>[Fixed typo]</p></font></div>
    </content>
    <updated>2020-04-06T02:30:51Z</updated>
    <published>2020-04-06T02:30:51Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="ABC"/>
    <category term="claimed proof"/>
    <category term="conjecture"/>
    <category term="long proof"/>
    <category term="open problems"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-04-09T06:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/043</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/043" rel="alternate" type="text/html"/>
    <title>TR20-043 |  A combinatorial MA-complete problem | 

	Dorit Aharonov, 

	Alex Bredariol Grilo</title>
    <summary>Despite the interest in the complexity class MA, the randomized analog of NP, there is just a couple of known natural (promise-)MA-complete problems, the first due to Bravyi and Terhal (SIAM Journal of Computing 2009) and the second due to Bravyi (Quantum Information and Computation 2015). Surprisingly, both problems are stated using terminology from quantum computation. This fact makes it hard for classical complexity theorists to study these problems, and prevents possible progress, e.g., on the important question of derandomizing MA.

In this note we define a natural combinatorial problem called SetCSP and prove its MA-completeness. The problem generalizes the constraint satisfaction problem (CSP) into constraints on sets of strings. This note is, in fact, a combination of previously known works: the brilliant work of Bravyi and Terhal, together with an observation made in our previous work (Aharonov and Grilo, FOCS 2019) that a restricted case of the Bravyi and Terhal's MA complete problem (namely, the uniform case) is already complete, and moreover, that this restricted case can be stated using a classical, combinatorial description. Here we flesh out this observation.

This note, along with a translation of the main result of Aharonov and Grilo to the SetCSP language, implies that finding a gap-amplification procedure for SetCSP problems (namely a generalization to SetCSPs of the gap-amplification used in Dinur's PCP proof) would imply MA=NP. This would provide a resolution of the major problem of derandomizing MA; in fact, the problem of finding gap-amplification for SetCSP is in fact equivalent to the MA=NP problem.</summary>
    <updated>2020-04-05T19:37:00Z</updated>
    <published>2020-04-05T19:37:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-04-09T06:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1285</id>
    <link href="https://ptreview.sublinear.info/?p=1285" rel="alternate" type="text/html"/>
    <title>News for March 2020</title>
    <summary>I hope all of you are keeping safe and healthy in these difficult times. Thank heavens we have our research and our math to keep our sanity… This month has seen two papers, one on testing variable partitions and one on distributed isomorphism testing. Learning and Testing Variable Partitions by Andrej Bogdanov and Baoxiang Wang […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I hope all of you are keeping safe and healthy in these difficult times. Thank heavens we have our research and our math to keep our sanity…</p>



<p>This month has seen two papers, one on testing variable partitions and one on distributed isomorphism testing.</p>



<p><strong>Learning and Testing Variable Partitions</strong> by Andrej Bogdanov and Baoxiang Wang (<a href="https://arxiv.org/abs/2003.12990">arXiv</a>). Consider a function \(f:\Sigma^n \to G\), where \(G\) is Abelian group. Let \(V\) denote the set of variables. The function \(f\) is \(k\)-separable if there is a partition \(V_1, V_2, \ldots, V_k\) of $V$ such that \(f(V)\) can be expressed as the sum \(f_1(V_1) + f_2(V_2) + \ldots + f_k(V_k)\). This is an obviously natural property to study, though the specific application mentioned in the paper is high-dimensional reinforcement learning control. There are a number of learning results, but we’ll focus on the main testing result. The property of \(k\)-separability can be tested with \(O(kn^3/\varepsilon)\) queries, for \(\Sigma = \mathbb{Z}_q\) (and distance between functions is the usual Hamming distance). There is an analogous result (with different query complexity) for \(\Sigma = \mathbb{R}\). It is also shown that testing 2-separability requires \(\Omega(n)\) queries, even with 2-sided error. The paper, to its credit, also has empirical studies of the learning algorithm with applications to reinforcement learning.</p>



<p><strong>Distributed Testing of Graph Isomorphism in the CONGEST model </strong>by Reut Levi and Moti Medina (<a href="https://arxiv.org/pdf/2003.00468.pdf">arXiv</a>). This result follows a recent line of research in distributed property testing algorithms. The main aim is to minimize the number of rounds of (synchronous) communication for a property testing problem. Let \(G_U\) denote the graph representing the distributive network. The aim is to test whether an input graph \(G_K\) is isomorphic to \(G_U\). The main property testing results are as follows. For the dense graph case, isomorphism can be property tested (with two-sided error) in \(O(D + (\varepsilon^{-1}\log n)^2) \) rounds, where \(D\) is the diameter of the graph and \(n\) is the number of nodes. (And, as a reader of this blog, you probably know what \(\varepsilon\) is already…). There is a standard \(\Omega(D)\) lower bound for distributed testing problems. For various classes of sparse graphs (like bounded-degree minor-free classes), constant time isomorphism (standard) property testers are known. This paper provides a simulation argument showing that standard/centralized \(q\)-query property testers can be implemented in the distributed model, in \(O(Dq)\) rounds (this holds for any property, not just isomorphism). Thus, these simulations imply \(O(D)\)-round property testers for isomorphism for bounded-degree minor-free classes.</p></div>
    </content>
    <updated>2020-04-05T05:22:05Z</updated>
    <published>2020-04-05T05:22:05Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-04-08T23:42:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4714</id>
    <link href="https://www.scottaaronson.com/blog/?p=4714" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4714#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4714" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">If I used Twitter…</title>
    <summary xml:lang="en-US">I’m thinking of writing a novel where human civilization is threatened by a global pandemic, and is then almost singlehandedly rescued by one man … a man who reigned for decades as the world’s prototypical ruthless and arrogant tech billionaire, but who was then transformed by the love of his wife. That is, if the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>I’m thinking of writing a novel where human civilization is threatened by a global pandemic, and is then almost singlehandedly rescued by one man … a man who reigned for decades as the world’s prototypical ruthless and arrogant tech billionaire, but who was then transformed by the love of his wife.  That is, <em>if</em> the billionaire can make it past government regulators as evil as they are stupid.  I need some advice: how can I make my storyline a bit subtler, so critics don’t laugh it off as some immature nerd fantasy?</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Updates (April 5):</span></strong> Thanks to several commenters for emphasizing that the wife needs to be a central character here: I agree!  The other thing is, I don’t want Fox News cheering my novel for its <em>Atlas Shrugged</em> vibe.  So maybe the pandemic is only surging out of control in the US because of the incompetence of a Republican president?  I don’t want to go ridiculously overboard, but like, maybe the president is some thuggish conman with the diction of a 5-year-old, who the deluded Republicans cheer anyway?  And maybe he’s also a Bible-thumping fundamentalist?  OK, that’s too much, so maybe the fundamentalist is like the <em>vice</em> president or something, and he gets put in charge of the pandemic response and then sets about muzzling the scientists?  As I said, I really need advice on making the messages subtler.</p></div>
    </content>
    <updated>2020-04-04T23:34:39Z</updated>
    <published>2020-04-04T23:34:39Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Obviously I'm Not Defending Aaronson"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-04-08T06:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=413</id>
    <link href="https://tcsplus.wordpress.com/2020/04/04/tcs-talk-wednesday-april-8-ramon-van-handel-princeton/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, April 8 — Ramon van Handel, Princeton</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, April 8th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Ramon van Handel from Princeton will speak about how “Rademacher type and Enflo type coincide” (abstract below). You can reserve a spot as an individual or a group […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, April 8th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Ramon van Handel</strong> from Princeton will speak about how “<em>Rademacher type and Enflo type coincide</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. (The link will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to join, until the maximum capacity of 300 seats is reached.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: In Banach space theory, Rademacher type is an important invariant that controls many geometric and probabilistic properties of normed spaces. It is of considerable interest in various settings to understand to what extent such powerful tools extend to general metric spaces. A natural metric analogue of Rademacher type was proposed by Enflo in the 1960s-70s, and has found a number of interesting applications. Despite much work in the intervening years, however, the relationship between Rademacher type and Enflo type has remained unclear. This basic question is settled in joint work with Paata Ivanisvili and Sasha Volberg: in the setting of Banach spaces, Rademacher type and Enflo type coincide. The proof is based on a very simple but apparently novel insight on how to prove dimension-free inequalities on the Boolean cube. I will not assume any prior background in Banach space theory in the talk.</p></blockquote></div>
    </content>
    <updated>2020-04-04T22:46:48Z</updated>
    <published>2020-04-04T22:46:48Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-04-09T06:21:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7673</id>
    <link href="https://windowsontheory.org/2020/04/04/in-defense-of-expertise/" rel="alternate" type="text/html"/>
    <title>In defense of expertise</title>
    <summary>Scott Aaronson blogged in defense of “armchair epidemiology”. Scott makes several points I agree with, but he also advocates that rather than discounting ideas from “contrarians” who have no special expertise in the matter, each one of us should evaluate the input of such people on its merits. I disagree. I can judge on their […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Scott Aaronson blogged in <a href="https://www.scottaaronson.com/blog/?p=4695">defense of “armchair epidemiology”</a>. Scott makes several points I agree with, but he also advocates that rather than discounting ideas from “contrarians” who have no special expertise in the matter, each one of us should evaluate the input of such people on its merits.</p>



<p>I disagree. I can judge on their merits the validity of a proposed P vs NP proof or a quantum algorithm for SAT, but I have seen time and again smart and educated non-experts misjudge such proposals. As much as I’d like to think otherwise, I would probably be fooled just as easily by a well-presented proposal in area X that “brushes under the rug” subtleties that experts in X would immediately notice.</p>



<p>This is not to say that non experts should stay completely out of the matter. Just like scientific journalists such as Erica Klarreich and Kevin Hartnett of quanta can do a great job of explaining computer science topics to lay audience, so can other well-read people serve as “signal boosters” and highlight works of experts in epidemiology. Journalist Helen Branswell of <a href="https://www.statnews.com/">stat news</a> has been following the <a href="https://www.nytimes.com/2020/03/30/business/media/stat-news-boston-coronavirus.html">novel coronavirus since January 4th</a>. </p>



<p>The difference is that these journalists don’t pretend to see what the experts are missing but rather to highlight and simplify the works that experts are already doing. This is unlike “contrarians” such as Robin Hanson that do their own analysis on a spreadsheet and come up with a “home brewed” policy proposal such as <a href="http://www.overcomingbias.com/2020/02/consider-controlled-infection.html">deliberate infection</a> or <a href="http://www.overcomingbias.com/2020/03/variolation-may-cut-covid19-deaths-3-30x.html">variolation</a> (with “hero hotels” in which people go to be deliberately infected). I am not saying that such proposals are necessarily wrong, but I am saying that I (or anyone else without the experience in this field) am not qualified to judge them. Even if they did “make sense” to me (they don’t) I would not feel any more confident in judging them than I would in reviewing a paper in astronomy. There is a reason why Wikipedia has a <a href="https://en.wikipedia.org/wiki/Wikipedia:No_original_research">“no original research”</a> policy.</p>



<p>Moreover, the attitude of dismissing expertise can be dangerous, whether it comes in the form of <a href="https://en.wikipedia.org/wiki/Teach_the_Controversy">“teach the debate”</a> in the context of evolution, or <a href="https://en.wikipedia.org/wiki/Climatic_Research_Unit_email_controversy">“ClimateGate”</a> in the context of climate change. Unlike the narrative of few brave “dissenters” or “contrarians”, in the case of COVID-19, experts as well as the world health organization have been literally sounding the alarm (see also <a href="https://www.nytimes.com/article/coronavirus-timeline.html">timeline</a>, as well as this NPR story on the <a href="https://www.npr.org/2020/04/03/826945368/how-the-united-states-failed-to-see-the-coronavirus-crisis-coming">US response</a>). Yes, some institutions, and especially the U.S., failed in several aspects (most importantly in the early production of testing). But one of the most troubling aspects is the constant sense of  <a href="https://www.nytimes.com/2020/04/03/us/politics/coronavirus-trump-medical-advisers.html">“daylight”</a>  and <a href="https://www.politico.com/news/2020/02/26/trump-backers-coronavirus-conspiracy-117781">distrust</a> between the current U.S. administration and its own medical experts. Moreover, the opinions of people such as  <a href="https://www.newyorker.com/news/q-and-a/the-contrarian-coronavirus-theory-that-informed-the-trump-administration">law professor Richard Epstein</a> are listened to even when they are far out of their depth. It is one thing to entertain the opinion of non-expert contrarians when we have all the time in the world to debate, discuss and debunk. It’s quite another to do so in the context of a fast-moving health emergency.  COVID-19 is an emergency that has medical, social, economical, and technological aspects, but it would best be addressed if each person contributes according to their skill set and collaborates with people of complementary backgrounds.</p>



<p/>



<p/></div>
    </content>
    <updated>2020-04-04T16:54:24Z</updated>
    <published>2020-04-04T16:54:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-04-09T06:21:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-6367105878835308659</id>
    <link href="http://processalgebra.blogspot.com/feeds/6367105878835308659/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=6367105878835308659" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6367105878835308659" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6367105878835308659" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/04/an-interview-with-nancy-lynch-and.html" rel="alternate" type="text/html"/>
    <title>An interview with Nancy Lynch and Roberto Segala, CONCUR Test-of-Time Award recipients</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post is devoted to the second interview with the <a href="https://concur2020.forsyte.at/test-of-time/index.html">colleagues</a> who were selected for the first edition of the CONCUR  Test-of-Time Award. (See <a href="https://processalgebra.blogspot.com/2020/04/an-interview-with-davide-sangiorgi.html">here</a> for the interview with Davide Sangiorgi.) I asked <a href="https://people.csail.mit.edu/lynch/">Nancy Lynch</a> (MIT, USA) and <a href="http://profs.sci.univr.it/~segala/">Roberto Segala</a> (University of Verona, Italy) a few questions via email and I copy their answers below. Let  me thank Nancy and Roberto for their answers. I trust that readers of this blog will find them interesting and inspiring.<br/><br/>Luca:  You receive one of the two CONCUR ToT Awards for the period 1992-1995 for your paper "<a href="https://groups.csail.mit.edu/tds/papers/Segala/CONCUR94.pdf">Probabilistic simulations for probabilistic processes</a>", presented at CONCUR 1994. Could you tell us briefly what spurred you to develop the “simple” probabilistic automaton model introduced in that paper and how the ideas underlying that notion came about?<br/><br/>Roberto: We were studying randomized distributed algorithms and we noted that several algorithms suffered from subtle errors. We were looking for a formal, rigorous way to study the correctness of those algorithms and in particular we were trying to extend the hierarchical approach used for <a href="https://en.wikipedia.org/wiki/Input/output_automaton">I/O automata</a>.  We knew that there was already an extensive literature within concurrency theory, but we were not able to use the theory to express even the simple idea that you may choose internally between flipping a fair or a biased coin; the existing approaches were extending labeled transition systems by adding probabilities to the arcs, losing the ability to distinguish between probabilistic and nondeterministic choice. The fact that the axioms for nondeterministic choice were not valid in most probabilistic calculi was further evidence of the problem. Starting from the observation that in the non-probabilistic world a step of a distributed algorithm corresponds to a transition of an I/O automaton, we tried to extend the notion of transition so that the process of flipping a coin could be represented by the "new" entity.  This led to the idea of "replacing points with measures" within ordinary automata and/or ordinary labeled transition systems. The idea worked very well for our analysis of randomized distributed algorithms, but we also wanted to validate it by checking whether it would simplify the concurrency-theoretical approach to probability. We decided to use simulation relations for our test, and indeed it worked and led to the CONCUR 1994 paper.<br/><br/>Nancy: For background for this paper, we should recall that, at the time,  there was a divergence in styles of modeling for distributed systems, between different concurrency theory research communities.   The style I had been using since the late 70s for modeling distributed algorithms involved interactive state machines, such as I/O automata, which are based on a set-theoretic foundation.  Such models are good for describing distributed algorithms, and for analyzing them for both correctness and costs.  On the other hand, the predominant style in the European concurrency community was more syntactic, based on logical languages like PCTL or various process algebras.  These were good for formal verification, and for studying expressive power of different formalisms, but not great for analyzing complicated distributed algorithms.  Our models were in the general spirit of the Labeled Transition System (LTS) models previously studied in Europe.  When Roberto came to MIT, he and I set about to extend prior modeling work for distributed systems to the probabilistic setting.  To do this, we considered both the set-theoretic and logical approaches.  We   needed to bridge the gap between them, which led us to the ideas in this paper.<br/><br/>Luca:  How much of your later work has built on your CONCUR 1994 paper? What follow-up results of yours are you most proud of and why?<br/><br/>Roberto: Besides using the results of the paper for the analysis of several randomized distributed algorithms, we worked jointly as well as independently on the study of the theory and on the analysis of security protocols. In collaboration with <a href="http://www.cs.ru.nl/~fvaan/">Frits Vaandrager</a>, we were able to discover different ways to analyze security in a hierarchical and compositional way. Furthermore, since in simple probabilistic automata probability and nondeterminism are well separated, it was easy to include computational complexity into the analysis of security protocols. This is work I did in collaboration with Andrea Turrini. The clear separation between probability and nondeterminism turned out to extend our approach to real-time models, leading to notions like Probabilistic Timed Automata and to several of the elements behind the first sketches of the probabilistic model checker <a href="https://www.prismmodelchecker.org/">PRISM</a> in collaboration with the group of <a href="https://www.cs.ox.ac.uk/people/marta.kwiatkowska/">Marta Kwiatkowska</a>.<br/><br/>Nancy: Roberto and I continued working on probabilistic models, in the set-theoretic style, for several years.  As Roberto notes, Frits Vaandrager also became  a major collaborator.  Some of our more recent papers following this direction are:<br/><ul><li><a href="https://www.sws.cs.ru.nl/publications/papers/fvaan/CPA.html">Compositionality for Probabilistic Automata</a>, CONCUR 2003, </li><li><a href="https://www.sws.cs.ru.nl/publications/papers/fvaan/LSVsiam/">Observing Branching Structure through Probabilistic Contexts</a>, SIAM  Journal of Computing 2007, </li><li><a href="https://groups.csail.mit.edu/tds/papers/Kirli/dilsun-icalp-tr.pdf">Task-Structured Probabilistic I/O Automata</a>, JCSS 2018.  </li></ul>This last  paper provides a kind of compositional foundation for combining  security protocols.  The key idea was to weaken the power of the adversary, making it more "oblivious".  In other related work, my students and I have worked extensively on  probabilistic distributed algorithms since then.  The models are similar to those developed in this early paper.  Examples include  wireless network algorithms, and more recently, biologically-inspired  algorithms, such as insect colony algorithms and neural network   algorithms.  I can't pinpoint a specific result that relies heavily on  the 1994 paper, but that paper certainly provided inspiration and foundation for the  later work.<br/><br/>Luca:  Did you imagine at the time that the CONCUR 1994 paper and its journal version would have so much impact? In your opinion, what is the most interesting or unexpected use in the literature of the notions and techniques you developed in your award-winning paper?<br/><br/>Roberto: We knew that the change of view proposed in the paper would have simplified the study and extension of several other concepts within concurrency theory, but we did not have any specific expectations. The collaboration on the PRISM project and on Probabilistic Timed Automata made more evident that there was a connection with Markov Decision Processes, which lead to a fruitful cross-fertilization between artificial intelligence, model checking, and concurrency theory.  It was a long exchange of ideas between a world interested in existential quantification, that is, find an optimal policy to achieve a goal, and universal quantification, that is to make sure that under any scheduler, policy, adversary, a system behaves correctly. The consequences of such exchange were many and unexpected.<br/><br/>Nancy: No, I did not anticipate that it would have broad impact, though of course I thought the ideas were important.  But in general, I am  bad at predicting what will appeal to others and inspire them to further work.<br/><br/>Luca:  The journal version of your paper appears in the Nordic Journal on Computing, which is not a prime venue. Did you ever regret not publishing that work somewhere else?  What is your take on the trend of using the perceived quality of a publication venue to evaluate research quality?<br/><br/>Roberto: Within semantics I know of a few technical reports that are much more influential than most journal papers; I do not think that the venue of a paper should be given much importance. On the other hand, venue as an evaluation tool is more objective, allows an evaluator to consider many papers without reading them, and protects the evaluator from any type of claim against decisions, which sometimes may have even legal consequences.  Needless to say that I do not agree with any of the ideas above.  I do not agree as well with the other emerging trend of counting papers to evaluate quality. One good paper is much better than ten mediocre papers. What worries me the most is that young researchers may have a hard time to concentrate on quality if they have to focus on venue and quantity. I feel I was lucky not to have to worry about these issues when we submitted our paper for the special issue of the Nordic Journal of Computing.<br/><br/>Nancy:   I certainly never regretted the venue for this paper.  In general, I haven't paid too much attention to choice of publication venue.  The main thing is to reach the audience you want to reach, which can be done through prestigious journals, less prestigious journals, conferences, or even ArXiv technical reports and some publicity.  It’s good to get feedback from referees, though. For evaluations, say for hiring or promotion, I think it’s fair to take many factors into account in evaluating research quality.  Venues, number of citations, special factors about different fields,… all can be discussed by hiring and promotion committees.  But I hope that those committees also take the time to actually read and consider the work itself.<br/><br/>Luca:  The last thirty years have seen a huge amount of work on probabilistic models of computation and on the development of proof techniques and tools based on them. What advice would you give to a young researcher interested in working on probabilistic models in concurrency today?<br/><br/>Roberto: My first advice would be to keep things simple and to focus on key ideas, possibly under the guidance of multiple application scenarios. When we study probabilistic concurrency, especially in contexts where the external world is governed by non-discrete laws, the interplay between different features of the model may become overwhelmingly complex. Of course it is a nice exercise to dive into all the details and see what it leads to, or to extend known results to the new scenarios, but how about checking whether some simple changes to one of the building blocks could make life easier?  Unfortunately, working on building blocks is risky. So, my second advice is to dive into details and write papers, but take some breaks and think whether there may be nicer ways to solve the same problems.<br/><br/>Nancy: Pick some particular application domain, and make sure that the models  and techniques you develop work well for that application.  Don't work  on the theory in a vacuum.  The models will turn out much better!  Perhaps simpler, as Roberto says.<br/><br/>Luca:  What are the research topics that currently excite you the most?<br/><br/>Roberto: Difficult question. There are many things I like to look at, but at the moment I am very curious about how quantum computing can fit in a nice and elementary way into a concurrency framework.<br/><br/>Nancy: Probabilistic algorithms, especially those that are flexible (work in  different environments), robust to failures and noise, and adaptive to  changes.  This includes such topics as wireless network algorithms,   robot swarms, and biologically-inspired algorithms.<br/><br/><b>Acknowledgements:</b> Many thanks to <a href="http://www-sop.inria.fr/members/Ilaria.Castellani/">Ilaria Castellani</a>, who pointed out some typos in the original version of this text.  </div>
    </content>
    <updated>2020-04-03T15:22:00Z</updated>
    <published>2020-04-03T15:22:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-04-04T14:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2445</id>
    <link href="https://francisbach.com/computer-aided-analyses/" rel="alternate" type="text/html"/>
    <title>Computer-aided analyses in optimization</title>
    <summary>In this blog post, I want to illustrate how computers can be great allies in designing (and verifying) convergence proofs for first-order optimization methods. This task can be daunting, and highly non-trivial, but nevertheless usually unavoidable when performing complexity analyses. A notable example is probably the convergence analysis of the stochastic average gradient (SAG) [1],...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">In this blog post, I want to illustrate how computers can be great allies in designing (and verifying) convergence proofs for first-order optimization methods. This task can be daunting, and highly non-trivial, but nevertheless usually unavoidable when performing complexity analyses. A notable example is probably the convergence analysis of the stochastic average gradient (SAG) [<a href="https://arxiv.org/pdf/1309.2388.pdf">1</a>], whose original proof was computer assisted.</p>



<p class="justify-text">To this end, we will mostly spend time on what is referred to as <em>performance estimation problems</em> (PEPs), introduced by Yoel Drori and Marc Teboulle [<a href="https://link.springer.com/article/10.1007/s10107-013-0653-0">2</a>]. Performance estimation is also closely related to the topic of <em>integral quadratic constraints</em> (IQCs), introduced in the context of optimization by Laurent Lessard, Benjamin Recht and Andrew Packard [<a href="https://epubs.siam.org/doi/abs/10.1137/15M1009597">3</a>]. In terms of presentations, IQCs  leverages control theory, whereas PEPs might seem more natural in the optimization community. This blog post essentially presents PEPs from the point of view of [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>], instantiated on a running example.</p>



<h2>Overview, motivations</h2>



<p class="justify-text">First-order methods for continuous optimization belong to the large panel of algorithms that are usually approached via worst-case analyses. In this context, analyses rely on combining inequalities (that are due to assumptions on the problem classes), in potentially long, non-intuitive, and technical, proofs. For the insiders, those proofs all look very similar. For the outsiders, those proofs all look rather repelling, technical (long pages of chained inequalities), probably not interesting, and like computer codes: usually intuitive mostly for their authors.</p>



<p class="justify-text">In what follows, I want to show how (and why) those proofs are indeed all very similar. On the way, I want to emphasize how those combinations of inequalities are related to the “true essence” of worst-case analyses (which rely on computing worst-case scenarios), and to provide examples on how to constructively obtain them.</p>



<p class="justify-text">We take the stand of illustrating the PEP approach on a single iteration of gradient descent, as it essentially contains all necessary ingredients to understand the methodology in other contexts as well. Certain details of the following text are (probably unavoidably) a bit technical. However, going through the detailed computations is not essential, and the text should contain the necessary ingredients for understanding the essence of the methodology.</p>



<h2>Running example: gradient descent</h2>



<p class="justify-text">Let us consider a naive, but standard, example: unconstrained convex minimization $$x_\star= \underset{x\in\mathbb{R}^d}{\mathrm{arg min}} f(x)$$with gradient descent: \(x_{k+1}=x_k-\gamma \nabla f(x_k)\). Let us assume \(f(\cdot)\) to be continuously differentiable, to have a \(L\)-Lipschitz gradient (a.k.a., \(L\)-smoothness), and to be \(\mu\)-strongly convex. Those functions satisfy, for all \(x,y\in\mathbb{R}^d\):<br/>– strong convexity, see e.g., [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>, Definition 2.1.2]: $$\tag{1}f(x) \geqslant      f(y)+\langle{\nabla f(y)}; {x-y}\rangle+\tfrac{\mu}{2} \lVert x-y\rVert^2,$$- smoothness, see e.g., [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>, Theorem 2.1.5]: $$\tag{2} f(x) \leqslant      f(y)+\langle{\nabla f(y)}; {x-y}\rangle+\tfrac{L}{2}\lVert x-y\rVert^2.$$Let us recall that in the case of twice continuously differentiable functions, smoothness and strong convexity amount to requiring  that $$\mu I \preccurlyeq \nabla^2 f(x) \preccurlyeq L I,$$ for some \(0&lt; \mu&lt;L&lt; \infty\) and for all \(x\in\mathbb{R}^d\) (in other words, all eigenvalues of \(\nabla^2 f(x)\) are between \(\mu\) and \(L\)). In what follows, we denote by \(\mathcal{F}_{\mu,L}\) the class of \(L\)-smooth \(\mu\)-strongly convex functions (irrespective of the dimension \(d\)).</p>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="204" src="https://www.di.ens.fr/~ataylor/BlogPost/SmoothStronglyConvex.png" width="473"/>Figure 1: the blue function is \(L\)-smooth and \(\mu\)-strongly convex (it is possible to create respectively global upper and lower quadratic bounds from every \(x\in\mathbb{R}^d\) with respectively curvatures \(L\) and \(\mu\)).</figure></div>
</div></div>



<p class="justify-text">In this context, convergence of gradient descent can be studied in many ways. Here, for the sake of the example, we will do it in terms of two base quantities: distance to optimality \(\lVert x_k-x_\star\rVert\), and function value accuracy \(f(x_k)-f(x_\star)\). There are, of course, infinitely many other possibilities, such as gradient norm \(\rVert \nabla f(x_k)\lVert\), Bregman divergence \(f(x_\star)-f(x_k)-\langle{\nabla f(x_k)};{x_\star-x_k}\rangle\), or even best function value observed throughout the iterations \(\min_{0\leq i\leq k} \{f(x_i)-f(x_\star)\}\): the reader can adapt the lines below for his/her favorite criterion. </p>



<p class="justify-text">For later reference, let us provide another inequality that is known to  hold for all \(x,y\in\mathbb{R}^d\) for any \(L\)-smooth \( \mu\)-strongly convex function: <br/>– bound on inner product, see, e.g., [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>,  Theorem 2.1.11]: $$\langle{\nabla f(x)-\nabla f(y)};{x-y}\rangle  \geqslant        \tfrac{1}{L+\mu} \lVert{\nabla f(x)-\nabla f(y)}\rVert^2+\tfrac{\mu  L}{L+\mu}\lVert{x-y}\rVert^2.\tag{3}$$ In the case \(\mu=0\) this inequality is known as “cocoercivity”. This (perhaps mysterious) inequality happens to play an important role in convergence proofs.</p>



<h3>A standard convergence result</h3>



<p class="justify-text">Let us start by stating two known results along with their simple proofs (see, e.g.,  [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>, Theorem 2.1.14] or [6, Section 1.4.2, Theorems 2 &amp; 3]):<br/>– convergence in distance:  $$\begin{array}{rl}    \rVert{x_{k+1}-x_\star}\lVert^2&amp;= \lVert{x_k-x_\star}\rVert^2+\gamma^2\lVert{\nabla f(x_k)}\rVert-2\gamma\langle{\nabla f(x_k)};{x_k-x_\star}\rangle \\ \     &amp; \leqslant      \left(1-\tfrac{2\gamma L \mu}{L+\mu}\right)\lVert{x_k-x_\star}\rVert^2+\gamma\left(\gamma-\tfrac2{L+\mu}\right)\lVert{\nabla f(x_k)}\rVert^2, \end{array} $$ where the second line follows from smoothness and strong convexity of \(f\) via the bound (3) on the inner product (with \(x=x_k\) and \(y=x_\star\)). For the particular choice \(\gamma=\tfrac2{L+\mu}\), the second term on the right hand side disappears, and we end up with<br/>     $$\lVert{x_{k+1}-x_\star}\rVert^2 \leqslant      \left(\tfrac{L-\mu}{L+\mu}\right)^2\lVert{x_k-x_\star}\rVert^2,$$ which, following from \(0&lt;\mu&lt;L&lt;\infty\), satisfies \(0&lt; \tfrac{L-\mu}{L+\mu}&lt;1\), hence proving linear convergence of gradient descent in this setup, by recursively applying the previous inequality: $$ \lVert{x_{k}-x_\star}\rVert^2 \leqslant      \left(\tfrac{L-\mu}{L+\mu}\right)^{2k}\lVert{x_0-x_\star}\rVert^2.$$ – Convergence in function values: one can simply use the result in distance along with the previous basic inequalities (1) and (2) characterizing smoothness and strong convexity (both with \(y=x_\star\)):<br/>     $$f(x_k)-f(x_\star) \leqslant \hspace{-.15cm}\tfrac{L}{2}\hspace{-.1cm}\rVert{x_k-x_\star}\lVert^2  \leqslant  \hspace{-.15cm}    \tfrac{L}{2}\hspace{-.1cm}\left(\tfrac{L-\mu}{L+\mu}\right)^{\hspace{-.1cm}2k} \rVert{x_0-x_\star}\lVert^2 \leqslant  \hspace{-.15cm}     \tfrac{L}{\mu}\hspace{-.1cm} \left(\tfrac{L-\mu}{L+\mu}\right)^{\hspace{-.1cm}2k}(f(x_0)-f(x_\star)).$$  It is also possible to directly look for convergence in terms of function values, but it is then usually unclear in the literature what inequalities to use, and I am not aware of any such proof leading to the same rate without the leading \(\tfrac{L}{\mu}\) (except the proof presented below).</p>



<p class="justify-text">At this point, even in this toy example, a few very legitimate questions can be raised:<br/>– can we improve anything? Can gradient descent really behaves like that on this class of functions?<br/>– How could we have guessed the inequality to use, and the shape of the corresponding proof? Obviously, the obscure fact is to arrive to inequality (1).  Therefore, is there a principled way for choosing the right inequalities to use, for example for studying convergence in terms of other quantities, such as  function values?<br/>– Is this the unique way to arrive to the desired result? If yes, how likely are we to find such proofs for more complicated cases (algorithms and/or function class)?</p>



<p class="justify-text">For the specific step size choice \(\gamma=\tfrac2{L+\mu}\), a partial answer to the first question is obtained by the observation that the rate is actually achieved on the quadratic function<br/> $$f(x)=\tfrac12 \, x^\top \begin{bmatrix}<br/> L &amp; 0\\ 0 &amp; \mu<br/> \end{bmatrix}x.$$ The following lines precisely target the missing answers.</p>



<h2>Worst-case analysis through worst-case scenarios</h2>



<p class="justify-text">Let us start by rephrasing our goal, and restrict ourselves to the study of a single iteration. We fix our target to finding the smallest possible value of \(\rho\) such that the inequality<br/> $$ \lVert{x_{k+1}-x_\star}\rVert^2  \leqslant       \rho^2 \lVert{x_k-x_\star}\rVert^2 $$ is valid for all \(x_k\) and \(x_{k+1}=x_k-\gamma \nabla f(x_k)\) (hence \(\rho\) is a function of \(\gamma\)). In other words, our goal is to solve<br/>$$ \rho^2(\gamma):= \sup \left\{ \frac{\lVert{x_{k+1}-x_\star}\rVert^2}{\lVert{x_k-x_\star}\rVert^2}\, \big|\, f\in\mathcal{F}_{\mu,L},\, x_{k+1}=x_k-\gamma \nabla f(x_k),\, \nabla f(x_\star)=0\right\}.$$<br/>Alternatively, we could be interested in studying convergence in other forms: for function values, we could target to solve the slightly modified problem:<br/> $$ \sup  \left\{ \frac{f(x_{k+1})-f(x_\star)}{f(x_{k})-f(x_\star)}\, \big|\, f\in\mathcal{F}_{\mu,L},\, x_{k+1}=x_k-\gamma \nabla f(x_k),\, \nabla f(x_\star)=0 \right\}.$$ It turns out that in both cases, the problem can be solved both numerically to high precision, and analytically, and that the answer is \(\rho^2(\gamma)=\max\{(1-\mu\gamma)^2,(1-L\gamma)^2\}\).</p>



<p class="justify-text">The only thing we did, so far, was to explicitly reformulate the problem of finding the best (smallest) convergence rate as the problem of finding the worst-case scenario, nothing more. In what follows, some parts might become slightly technical, but the overall idea is only to reformulate this problem of finding the worst-case scenarios, for solving it.</p>



<h3>Dealing with an infinite-dimensional variable: the function \(f\)</h3>



<p class="justify-text">The first observation is that the problem of computing \(\rho\) is stated as an infinite-dimensional optimization problem: we are looking for the worst possible problem instance (a function \(f\) and an initial point \(x_k\)) within a predefined class of problems. The first step we take to work around this is to reformulate it in the following equivalent  form (note that we maximize also over the dimension \(d\)—we discuss later how to remove it):<br/>$$\begin{array}{rl} \rho^2:= \underset{f,\, x_k,\,x_\star,\, g_k,\,d}{\sup} &amp;\displaystyle \frac{\lVert{x_{k}-\gamma g_k-x_\star}\rVert^2}{\lVert{x_k-x_\star}\rVert^2}\\<br/> \text{s.t. }    &amp; \exists f\in\mathcal{F}_{\mu,L}:\, g_k= \nabla f(x_k),\, 0=\nabla f(x_\star).<br/>\end{array}$$<br/>This problem intrinsically does not look better (it contains an  existence constraint), but it allows using mathematical tools which are referred to as  <em>interpolation,</em> or <em>extension</em>, theorems [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, <a href="https://arxiv.org/pdf/1603.00241.pdf">7</a>, <a href="https://hal.archives-ouvertes.fr/hal-01530908/file/DHLL_appendix.pdf">8</a>]. The problem is depicted on Figure 2:</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="209" src="https://www.di.ens.fr/~ataylor/BlogPost/Interpolation.png" width="490"/>Figure 2: discrete interpolation (or extension) problem: given a set of triplets \(\{(\text{coordinate}, \text{gradient}, \text{function value})\}\) can we recover a function within a determined class that explains those triplets?</figure></div>



<p class="justify-text">It turns out that convex interpolation (that is, neglecting smoothness and strong convexity) is actually rather simple:</p>



<ul class="justify-text"><li>given a convex function and an index set \(I\), any set of samples \(\{(x_i,g_i,f_i)\}_{i\in I}\) of the form \(\{(\text{coordinate}, \text{(sub)gradient}, \text{function value})\}\)) satisfies, for all \(i,j\in I\): $$f_i \geqslant      f_j+\langle g_j; x_i-x_j\rangle,$$ by definition of subgradient, as illustrated on Figure 3.</li></ul>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" height="205" src="https://www.di.ens.fr/~ataylor/BlogPost/SamplingCvx.png" width="485"/>Figure 3: sampling from a convex function.</figure></div>



<ul class="justify-text"><li>In the other direction, given a set of triplets \(\{(x_i,g_i,f_i)\}_{i\in I}\) satisfying the previous inequality for all pairs \(i,j\in I\), one can simply recover a  convex function by the following construction: $$f(x)=\underset{i\in I}{\max}\{ f_i+\langle g_i;x-x_i\rangle\},$$ which is depicted on Figure 4.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="207" src="https://www.di.ens.fr/~ataylor/BlogPost/InterpolateCvx.png" width="487"/>Figure 4: some set \(\{(x_i,g_i,f_i)\}_{i\in I}\) and its piecewise affine interpolant \(f(x)=\underset{i\in I}{\max}\{ f_i+\langle g_i;x-x_i\rangle\}\).</figure></div>



<ul class="justify-text"><li>Formally, the reasoning allows arriving to the following “convex interpolation” (or “convex extension”) result, where we denote the set of (closed, proper) convex functions by \(\mathcal{F}_{0,\infty}\) (to be understood as \(L\)-smooth \(\mu\)-strongly convex functions with \(\mu=0\) and \(L=\infty\)): $$\begin{array}{c}\exists f\in\mathcal{F}_{0,\infty}: \,  g_k\in\partial f(x_k) \text{ and } f_k=f(x_k) \ \ \forall k\in  I\\ \Leftrightarrow\\ f_i \geqslant       f_j+\langle{g_j};{x_i-x_j}\rangle\quad \forall i,j\in I,\end{array}$$ where \(\partial f(x)\) denotes the subdifferential of \(f\) at \(x\).</li></ul>



<p class="justify-text">In the next section, we use a similar interpolation result for taking smoothness and strong convexity into account. The result is a bit more technical, but follows from similar constructions as those for convex interpolation—the main difference being that the interpolation is done on the Fenchel conjugate instead, in order to incorporate smoothness, see [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, Section 2].</p>



<h3>Reformulation through convex interpolation</h3>



<p class="justify-text">Back to the problem of computing worst-case scenarios, we can now reformulate the existence constraint <em>exactly</em> using the following result (see [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>,  Theorem 4]): let \(I\) be a finite index set and let \( S=\{(x_i,g_i,f_i)\}_{i\in I}\) be a set of triplets, then<br/>  $$\begin{array}{c}\exists f\in\mathcal{F}_{\mu,L}: \,  g_i=\nabla f(x_i) \text{ and } f_i=f(x_i) \text{ for all } i\in  I\\ \Leftrightarrow\\  f_i \geqslant      f_j+\langle{g_j};{x_i-x_j}\rangle+\frac{1}{2L}\lVert{g_i-g_j}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_i-x_j-\frac{{1}}{L}(g_i-g_j)}\rVert^2  \,\,\, \forall i,j\in I.\end{array}$$ Therefore, the previous problem can be reformulated as (recalling that \(g_\star=0\))<br/>$$  \begin{array}{rl} \underset{{f_k,\,f_\star,\, x_k,\,x_\star,\, g_k,\,d}} {\sup}&amp;\displaystyle\frac{\rVert{x_{k}-\gamma  g_k-x_\star}\lVert^2}{\rVert{x_k-x_\star}\lVert^2}\\<br/>      \text{s.t. }    &amp; f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2\\<br/>      &amp; f_k \geqslant      f_\star+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2.<br/>\end{array}$$ </p>



<h3>Quadratic reformulation</h3>



<p class="justify-text">The next step is to remove the ratio appearing in the objective function, which we do via an homogeneity argument, as follows.</p>



<p class="justify-text">Starting from a feasible point, scale \(x_k,\,x_\star,g_k\) by some \(\alpha&gt;0\) and \(f_k,\,f_\star\) by \(\alpha^2\) and observe it does not change the value of the objective, while still being a feasible point. Therefore, the problem can be reformulated as a nonconvex QCQP (quadratically constrained quadratic program): $$  \begin{array}{rl} \underset{{f_k,\,f_\star,\, x_k,\,x_\star,\, g_k,\,d}} {\sup}&amp;\displaystyle{\rVert{x_{k}-\gamma  g_k-x_\star}\lVert^2}\\<br/>       \text{s.t. }    &amp; f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2\\<br/>       &amp; f_k \geqslant      f_\star+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2\\ &amp;{\rVert{x_k-x_\star}\lVert^2} \leqslant      1,<br/> \end{array}$$  which is quadratic in \(x_k\), \(x_\star\) and  \(g_k\), and linear in \(f_\star\) and \(f_k\). Actually, in the current form, nonconvexity comes from the term “\(\langle{g_k};{x_\star-x_k}\rangle\)” in the second constraint (and from the objective, due to maximization). It turns out that this problem can be reformulated <em>losslessly</em> using semidefinite programming (this is due to the maximization over \(d\), as commented at the end of the next section). </p>



<h3>Semidefinite reformulation</h3>



<p class="justify-text">At the end of this section, we will be able to compute, numerically, the values of the rate \(\rho^2(\gamma)\) for given values of the parameters \(\mu,\,L\), and \(\gamma\).</p>



<p class="justify-text">The last step in the reformulation goes as follows: the previous problem can be reformulated as a semidefinite program, as it is linear in terms of the entries of the following Gram matrix<br/>$$G = \begin{pmatrix}<br/>     \lVert{x_k-x_\star}\rVert^2 &amp; \langle{g_k};{x_k-x_\star}\rangle \\ \langle{g_k};{x_k-x_\star}\rangle &amp; \lVert{g_k}\rVert^2<br/>     \end{pmatrix}\succcurlyeq 0,$$ and in terms of the function values \(f_k\) and \(f_\star\). From those variables, one reformulate the previous problem as $$\begin{array}{rl} \underset{f_k,\,f_\star,\, G\succeq 0}{\sup} \, &amp;{\mathrm{Tr} (A_\text{num} G)}\\<br/>      \text{s.t. }    &amp; f_k-f_\star+\mathrm{Tr} (A_1 G)\leqslant 0\\<br/>      &amp;  f_\star-f_k+\mathrm{Tr} (A_2 G)\leqslant 0 \\<br/>      &amp;\mathrm{Tr} (A_\text{denom} G) \leqslant      1,\end{array}$$ which is a gentle semidefinite program where we picked matrices \(A_{\text{num}}\), \(A_{\text{denom}}\), \(A_1\) and \(A_2\) for encoding the previous terms. That is, we choose those matrices such that<br/> $$\begin{array}{rl}<br/>\mathrm{Tr}(A_{\text{denom}} G)&amp;=\lVert{x_k-x_\star}\rVert^2,\\  \mathrm{Tr}(A_{\text{num}} G)&amp;=\lVert{x_k-\gamma g_k-x_\star}\rVert^2,\\ \mathrm{Tr}(A_1G)&amp;=\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2,\\ \mathrm{Tr}(A_2G)&amp;=\tfrac{1}{2L}\lVert g_k\rVert^2+\tfrac{\mu}{2(1-\mu/L)}\lVert x_k-x_\star-\tfrac1L g_k\rVert^2.\end{array}$$ One possibility is to choose \(A_{\text{num}}\), \(A_{\text{denom}}\), \(A_1\) and \(A_2\) as symmetric matrices, as follows: $$\begin{array}{cc}<br/> A_{\text{denom}}=\begin{pmatrix}     1 &amp; 0\\ 0 &amp; 0     \end{pmatrix}, &amp; A_{\text{num}}=\begin{pmatrix}     1 &amp; -\gamma\\ -\gamma &amp; \gamma^2     \end{pmatrix}, \\ A_1=\begin{pmatrix}\tfrac{\mu}{2(1-\mu/L)} &amp; -\tfrac12-\tfrac{\mu}{2(L-\mu)} \\ -\tfrac12-\tfrac{\mu}{2(L-\mu)} &amp; \tfrac{1}{2L}+\tfrac{\mu}{2L(L-\mu)}\end{pmatrix}, &amp;  A_2=\begin{pmatrix}\tfrac{\mu}{2(1-\mu/L)} &amp; -\tfrac{\mu}{2(L-\mu)} \\ -\tfrac{\mu}{2(L-\mu)} &amp; \tfrac{1}{2L}+\tfrac{\mu}{2L(L-\mu)} \end{pmatrix}.\end{array}$$</p>



<p class="justify-text">All those steps can be carried out in the exact same way for the problem of computing the convergence rate for function values, reaching a similar problem with \(6\) inequality constraints instead—because interpolation conditions have to be imposed on all pairs of points in a set of \(3\) points: \(x_k\), \(x_{k+1}\) and \(x_\star\), instead of only \(2\) for the distance problem. The objective function is then \(f_{k+1}-f_\star\), the de-homogenization constraint (arising from the denominator of the objective function) is \(f_{k}-f_\star \leqslant     1\), and the Gram matrix is \(3\times 3\):<br/>$$G=\begin{pmatrix}<br/>     \lVert{x_k-x_\star}\rVert^2 &amp; \langle{g_k};{x_k-x_\star}\rangle&amp; \langle{g_{k+1}};{x_k-x_\star}\rangle\\ \langle{g_k};{x_k-x_\star}\rangle &amp; \lVert{g_k}\rVert^2 &amp; \langle{g_k};{g_{k+1}}\rangle \\<br/>     \langle{g_{k+1}};{x_k-x_\star}\rangle &amp; \langle{g_{k+1}};{g_k}\rangle &amp; \lVert{g_{k+1}}\rVert^2<br/>     \end{pmatrix}\succcurlyeq 0, $$ and the function values variables are \(f_k\), \(f_{k+1}\) and \(f_\star\).</p>



<p class="justify-text">We provide the numerical optimal values of those semidefinite programs on Figure 5 for both convergence in distances and in function values. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="324" src="https://www.di.ens.fr/~ataylor/BlogPost/ObjectiveValues.png" width="534"/>Figure 5: worst-cases of the ratio \(\frac{\lVert{x_{k+1}-x_\star}\rVert^2}{\lVert{x_k-x_\star}\rVert^2}\) (red) and \(\frac{f(x_{k+1})-f_\star}{f(x_k)-f_\star}\) (dashed blue) as functions of the step size \(\gamma\), for the case where \(f\) is \(1\)-smooth and \(0.1\)-strongly convex. The results match exactly the expected \(\max\{(1-\gamma L)^2,(1-\gamma\mu)^2\}\) in both cases. Note that the corresponding SDPs can be solved both for “good and bad” choices of step sizes: if the step size is chosen wisely then \(\rho(\gamma)&lt;1\), and otherwise \(\rho(\gamma)\geqslant 1\). The SDP confirms the common knowledge that \(\gamma\in (0,2/L)\Rightarrow \rho(\gamma)&lt; 1\). Numerical values obtained through YALMIP [<a href="https://yalmip.github.io/">9</a>] and Mosek [<a href="https://www.mosek.com/">10</a>].</figure></div>



<p class="justify-text">As a conclusion for this section, let us note that we showed how to compute the “best” rates that are dimension independent. In general, requiring the iterates and gradient (e.g., \(x_k\) and \(g_k\) for the problem in terms of distance, and \(x_k\), \(g_k\) and \(g_{k+1}\) for function values, and potentially more vectors when dealing with more complex settings) to lie in \(\mathbb{R}^d\) is equivalent to adding a rank constraint in the SDP. </p>



<h2>Duality between worst-case scenarios and combinations of inequalities</h2>



<p class="justify-text">Any feasible point to the previous SDP corresponds to a <em>lower bound</em>: a sampled version of a potentially difficult function for gradient descent. If we want to find <em>upper bounds</em> on the rate, a natural way to proceed is to go to the dual side of the previous SDPs, where any feasible point will naturally correspond to an upper bound on the convergence rate (by <em>weak duality</em>). As the primal problems were SDPs, their Lagrangian duals are SDPs as well. Let us associate one multiplier per constraint: $$ \begin{array}{rl}<br/>f_k-f_\star+\mathrm{Tr} (A_1 G)\leqslant 0&amp;:\lambda_1\\<br/>f_\star-f_k+\mathrm{Tr} (A_2 G)\leqslant 0&amp;:\lambda_2\\<br/>\mathrm{Tr}(A_\text{denom} G) \leqslant 1&amp;: \tau.<br/>\end{array}$$The dual is then<br/>$$\begin{array}{rl}<br/> \underset{\tau,\,\lambda_1,\,\lambda_2}{\min} &amp; \, \tau \\<br/> \text{s.t. } &amp; \lambda_1=\lambda_2,\\<br/> &amp; S:=A_\text{num}-\tau A_\text{denom}-\lambda_1A_1-\lambda_2A_2 \preccurlyeq 0,\\<br/> &amp;\tau,\lambda_1,\lambda_2 \geqslant  0.<br/> \end{array}$$ Hence, by weak duality, any feasible point to this last SDP corresponds to an upper bound on the rate: \(\tau \geqslant \rho^2\). A mere rephrasing of weak duality can be obtained through the following reasoning: assume we received some feasible \(\tau,\lambda_1,\lambda_2\) (and hence \(\lambda_1=\lambda_2\) and a corresponding \(S\preccurlyeq 0\)), we then get, for any primal feasible \(G\succcurlyeq0\):<br/>$$\begin{array}{rl}\mathrm{Tr}(SG)&amp;=\mathrm{Tr}(A_{\text{num}}G)-\tau\mathrm{Tr}(A_{\text{denom}}G)-\lambda_1\mathrm{Tr}(A_1G)-\lambda_2\mathrm{Tr}(A_2G)\\&amp;=\lVert x_{k+1}-x_\star\rVert^2-\tau\lVert x_{k}-x_\star\rVert^2\\ \,&amp;\,\,\,-\lambda_1[     f_k-f_\star+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2]\\ &amp;\,\,\,-\lambda_2 [f_\star-f_k+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2]\\ &amp;\geqslant \lVert x_{k+1}-x_\star\rVert^2-\tau\lVert x_{k}-x_\star\rVert^2, \end{array}$$ where the first equality follows from the definition of \(S\), the second equality corresponds to the definitions of \(A_{\text{num}}\), \(A_{\text{denom}}\), \(A_1\) and \(A_2\), and the last inequality follows from the sign of the interpolation inequalities (constraints in the primal) for any primal feasible point. Hence, we indeed have that any feasible \(\tau\) corresponds to a valid upper bound on the convergence rate, as $$S\preccurlyeq 0 \,\,\Rightarrow \,\, \mathrm{Tr}(SG)\leqslant 0\,\,\Rightarrow  \lVert x_{k+1}-x_\star\rVert^2-\tau\lVert x_{k}-x_\star\rVert^2\leqslant 0.$$ In order to obtain analytical proofs, we therefore need to find analytical dual feasible points, and numerics can of course help in this process! Let’s look at what the optimal dual solutions look like for our two running examples.</p>



<ul class="justify-text"><li> in Figure 6, we provide the numerical values for \(\lambda_1\) and \(\lambda_2\) for the distance problem.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="316" src="https://www.di.ens.fr/~ataylor/BlogPost/Multipliers_distance.png" width="467"/>Figure 6: numerical values of optimal dual variables: \(\lambda_1\) (red) and \(\lambda_2\) (dashed blue) as functions of the step size \(\gamma\), for the case where \(f\) is \(1\)-smooth and \(0.1\)-strongly convex. The results match \(\lambda_1=\lambda_2=2\gamma \rho(\gamma)\) with \(\rho(\gamma)=\max\{|1-\gamma L|,|1-\gamma\mu|\}\). Numerical values obtained with YALMIP [<a href="https://yalmip.github.io/">9</a>] and Mosek [<a href="https://www.mosek.com/">10</a>].</figure></div>



<ul class="justify-text"><li>For function values, the SDP is slightly more complicated, as more inequalities are involved (6 interpolation inequalities). We provide raw numerical values for the six multipliers in Figure 7.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="348" src="https://www.di.ens.fr/~ataylor/BlogPost/Multipliers_function.png" width="463"/>Figure 7: numerical values of optimal dual variables (for the rate in function values): \(\lambda_1,\lambda_2, …,\lambda_6\) as functions of the step size \(\gamma\), for the case where \(f\) is \(1\)-smooth and \(0.1\)-strongly convex. Numerical values obtained with YALMIP [<a href="https://yalmip.github.io/">9</a>] and Mosek [<a href="https://www.mosek.com/">10</a>].</figure></div>



<p>For those who want a bit more details, here are a few additional pointers:</p>



<ul class="justify-text"><li>Strong duality holds—a way to prove it is to show that there exists a Slater point in the primal, see e.g., [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, Theorem 6]—, and hence primal and dual optimal values match.</li><li>There might be different ways to optimaly combine the interpolation inequalities for proving the desired results. In other words: dual optimal solutions are often not unique—which is, in fact, quite a good news: I am sure nobody want to find the analytical version of the multipliers provided in Figure 7.</li><li>It is often possible to simplify the proofs by using fewer, or weaker, inequalities. This might lead to ”cleaner” results, typically (but not always) at the cost of ”weaker” rates. This was done for designing the proof for function values, later in this text.</li></ul>



<h2>Combinations of inequalities: same proofs without SDPs</h2>



<p class="justify-text">So far, we showed that computing convergence rates can be done in a very principled way. To this end, one can solve semidefinite programs—which may have arbitrarily complicated analytical solutions. Here, I want to emphasize that the process of <em>verifying</em> a solution can be quite different to that of<em> finding</em> a solution. Put in other words, although the dual certificates (a.k.a., the proofs) might have been found by solving SDPs, they can be formulated in ways that do not require the reader to know anything about the PEP methodology, nor on any SDP material, for verifying them. This fact might actually not be very surprising to the reader, as many proofs arising in the first-order optimization literature actually “only consists” in linear combinations of (quadratic) inequalities. On the one hand, those proofs can be seen as feasible points to “dual SDPs”, although generally not explicitely proved as such. On the other hand, proofs arising from the SDPs might therefore be expected to be writable without any explicit reference to semidefinite programing and performance estimation problems.<br/></p>



<p class="justify-text">In what follows, we provide the proofs for gradient descent, using the previous numerical inspiration, but without explicitly relying on any semidefinite program. The reader is not expected to verify any of those computations, as our goal is rather to emphasize that the principles underlying both proofs are exactly the same: reformulating linear combinations of inequalities.</p>



<p class="justify-text">For both proofs below, we limit ourselves to the step size regime \(0\leq   \gamma \leq \tfrac{2}{L+\mu}\), and we prove  that, in  this regime, \(\rho(\gamma)=(1-\gamma\mu)\)—actually we only proof the upper bounds, but one can easily verify that they are <em>tight</em> on simple quadratic functions.  The complete proofs (for the proximal gradient method),  can be found in [<a href="https://arxiv.org/pdf/1705.04398.pdf">11</a>]. </p>



<h3>Example 1: distance to optimality</h3>



<p class="justify-text">Recall the notations: \(g_k:=\nabla f(x_k)\), \(f_k:= f(x_k)\), \(g_\star:=\nabla f(x_\star)\), and \(f_\star:= f(x_\star)\).</p>



<p class="justify-text">For distance to optimality, sum the following inequalities with their corresponding weights: $$\begin{array}{r}     f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k-g_\star}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}(g_k-g_\star)}\rVert^2  :\lambda_1,  \\     f_k \geqslant      f_\star+\langle{g_\star};{x_k-x_\star}\rangle+\frac{1}{2L}\lVert{g_k-g_\star}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}(g_k-g_\star)}\rVert^2:\lambda_2.     \end{array}$$ We use the following values for the multipliers: \(\lambda_1=\lambda_2=2\gamma\rho(\gamma) \geqslant      0\) (see Figure 6). </p>



<p class="justify-text">After appropriate substitutions of \(g_\star\), \(x_{k+1}\), and \(\rho(\gamma)\), using respectively \(g_\star=0\), \(x_{k+1}=x_k-\gamma g_k\) and \(\rho(\gamma)=(1-\gamma\mu)\), and with little effort, one can check that the previous weighted sum of inequalities can be written in the form: $$ \begin{array}{rl}    \lVert{x_{k+1}-x_\star}\rVert^2  \leqslant      &amp; \left(1-\gamma \mu \right)^2\lVert{x_{k}-x_\star}\rVert^2 -\frac{\gamma(2-\gamma (L+\mu))}{L-\mu} \lVert{\mu {(x_k  -x_\star)} – g_k}\rVert^2. \end{array}$$ This statement can be checked simply by expanding both expressions (i.e., the weighted sum and its reformulation) and verifying that all terms indeed match.</p>



<p class="justify-text">Finally, using $$\gamma(2-\gamma (L+\mu)) \geqslant      0,  \text{ and } L-\mu \geqslant      0,$$ which are nonnegative by assumptions on the values of \(L\in(0,\infty)\), \(\mu\in (0,L)\) and \(\gamma\in(0,2/(L+\mu))\), we arrive to the desired $$ \lVert{x_{k+1}-x_\star}\rVert^2 \leqslant      \left(1-\gamma \mu \right)^2\lVert{x_{k}-x_\star}\rVert^2.$$</p>



<p class="justify-text">Note that, by using \(\lambda_1=\lambda_2\), the weighted sum exactly corresponds to the (scaled by a positive constant) inequality introduced in the early stage of this note for studying distance to optimality. However, the resulting expression is tight for all values of the step size here, whereas it was only tight for \(\gamma=2/(L+\mu)\) earlier, due to a different choice of weights! </p>



<p class="justify-text">The curious reader might wonder how to find such a reformulation. Actually, back in terms of SDPs, and using the expressions for the multipliers, it simply corresponds to $$\mathrm{Tr}(SG)=-\frac{\gamma(2-\gamma (L+\mu))}{L-\mu} \lVert{\mu {(x_k  -x_\star)} – g_k}\rVert^2.$$ In the example below, the reformulation is a bit more tricky—as \(\mathrm{Tr}(SG)\)  has two nonnegative terms, which were simply obtained by doing an analytical Cholesky factorization of the term \(\mathrm{Tr}(SG)\)—, but the idea is exactly the same.</p>



<h3>Example 2: function values</h3>



<p class="justify-text">For function values, we combine the following inequalities after multiplication with their respective coefficients:    </p>



<p class="justify-text">$$\scriptsize \begin{array}{lr}     f_k \geqslant      f_{k+1}+\langle{g_{k+1}};{x_k-x_{k+1}}\rangle+\frac{1}{2L}\lVert{g_k-g_{k+1}}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_{k+1}-\frac{1}{L}(g_k-g_{k+1})}\rVert^2    &amp;:\lambda_1,\\<br/>f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k-g_\star}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{1}{L}(g_k-g_\star)}\rVert^2  &amp;:\lambda_2, \\<br/>f_\star \geqslant      f_{k+1}+\langle{g_{k+1}};{x_\star-x_{k+1}}\rangle+\frac{1}{2L}\lVert{g_\star-g_{k+1}}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_\star-x_{k+1}-\frac{1}{L}(g_\star-g_{k+1})}\rVert^2  &amp;:\lambda_3.     \end{array}$$ We use the following multipliers \(\lambda_1=\rho(\gamma)\), \(\lambda_2=(1-\rho(\gamma))\rho(\gamma)\), and  \(\lambda_3=1-\rho(\gamma)\) (obtained by greedily trying to set different combinations of multipliers to \(0\) in the SDP—see Figure 7 for the values without such simplifications).</p>



<p class="justify-text">Again, after appropriate substitutions of \(g_\star\), \(x_{k+1}\), and \(\rho(\gamma)\), using respectively \(g_\star=0\), \(x_{k+1}=x_k-\gamma g_k\) and \(\rho(\gamma)=(1-\gamma\mu)\), we obtain that the weighted sum of inequalities can be reformulated exactly as $$  \begin{array}{rl}          f(x_{k+1})-f_\star \leqslant      &amp;\left(1-\gamma \mu\right)^2 \left(f(x_k)-f_\star\right)\\&amp;-\frac{1}{2 (L-\mu)}\lVert \nabla f(x_{k+1})-(1-\gamma  (L+\mu))\nabla f(x_k) +\gamma  \mu  L (x_\star-x_k)\rVert^2\\<br/>&amp;-\frac{\gamma  L(2- \gamma  (L+\mu))}{2 (L-\mu )}\lVert \nabla f(x_k)+\mu  (x_\star-x_k)\rVert^2.\end{array}$$ Again, this statement can be checked simply by expanding both expressions (i.e., the weighted sum and its reformulation), and verifying that all terms match. The desired conclusion $$ f(x_{k+1})-f_\star \leqslant \left(1-\gamma \mu\right)^2 \left(f(x_k)-f_\star\right), $$ follows from the signs of the leading coefficients: \(\gamma(2-\gamma (L+\mu)) \geqslant      0\), and \(L-\mu \geqslant      0\).</p>



<h3>To go further</h3>



<p class="justify-text">Before finishing, let us mention that we only dealt with linear convergence through a single iteration of gradient descent.</p>



<p class="justify-text">There are quite a few ways to handle both more iterations and sublinear convergence rates. Using SDPs, probably the most natural approach is to directly incorporate several iterations in the problem by  studying, for example, ratios of the form  $$\sup_{f\in\mathcal{F}_{\mu,L},\, x_0}  \frac{f(x_{N})-f_\star}{\lVert x_0-x_\star\rVert^2}. $$ This type of approach was used in the work of Drori and Teboulle [<a href="https://arxiv.org/pdf/1206.3209.pdf">2</a>] and in most consecutive PEP-related works: it has the advantage of providing  comfortable “non-improvable results” [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, <a href="https://arxiv.org/pdf/1512.07516.pdf">12</a>]   (by providing matching lower bounds) for any given \(N\), but requires solving larger and larger SDPs. Alternatively, simpler proofs can often be obtained through the use of  Lyapunov (or potential) functions—i.e., study a single iteration to produce recursable inequalities; a nice introduction is provided in [<a href="http://www.theoryofcomputing.org/articles/v015a004/v015a004.pdf">13</a>]. This idea can be exploited in PEPs [<a href="https://arxiv.org/pdf/1902.00947.pdf">14</a>] by enforcing the proofs to have a certain structure. Those principles are also at the heart of the related approach using integral quadratic constraints [<a href="https://epubs.siam.org/doi/abs/10.1137/15M1009597">3</a>, <a href="http://proceedings.mlr.press/v70/hu17a/hu17a.pdf">15</a>]. </p>



<h2>Take-home message and conclusions</h2>



<p class="justify-text">The overall message of this note is that first-order methods can often be studied directly using the definition of their “worst-cases” (i.e., by trying to find worst-case scenarios), along with their dual counterparts (linear combinations of inequalities), by translating them into semidefinite programs.</p>



<p class="justify-text">What we saw might look like an overkill for studying gradient descent. However, as long as we deal with Euclidean spaces, the same approach actually works beyond this simple case. In particular, the same technique applies to first-order methods performing explicit, projected, proximal, conditional, and inexact (sub)gradient steps [<a href="https://arxiv.org/pdf/1512.07516.pdf">12</a>].</p>



<p class="justify-text">Finally, let us mention a few previous works illustrating that the use of such computer-assisted proofs allowed obtaining results that are apparently too complicated for us to find bare-handed—even in apparently simple contexts.  Reasonable examples include the direct proof for convergence rates in  function values [<a href="https://arxiv.org/pdf/1705.04398.pdf">11</a>] presented above, but also proofs arising in the context of optimized numerical schemes [<a href="https://arxiv.org/pdf/1206.3209.pdf">2</a>, <a href="https://arxiv.org/abs/1409.2636">16</a>, <a href="https://arxiv.org/pdf/1406.5468.pdf">17</a>, <a href="https://arxiv.org/pdf/1803.06600.pdf">18</a>]—in particular [<a href="https://arxiv.org/pdf/1803.06600.pdf">18</a>] presents a method for minimizing the gradient norm at the last iterate, in smooth convex minimization—,  in the context of monotone inclusions,  and even for more general fixed-point problems (e.g., for Halpern iterations [<a href="http://www.optimization-online.org/DB_FILE/2017/11/6336.pdf">19</a>]).</p>



<h3>Toolbox</h3>



<p class="justify-text">The PErformance EStimation TOolbox (PESTO) [<a href="https://perso.uclouvain.be/julien.hendrickx/availablepublications/PESTO_CDC_2017.pdf">20</a>, see <a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox/graphs/traffic">Github</a>] allows a quick access to the methodology without worrying about details of semidefinite reformulations. The toolbox contains many  examples (about 50) in different settings, and include progresses on the approach, and results, by other groups (which are much more thoroughly referenced in the <a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox/blob/master/UserGuide.pdf">user guide</a>). In particular, we included standard classes of functions and operators, along with examples for analyzing recent optimized methods.</p>



<h2>References</h2>



<p class="justify-text">[1] Mark Schmidt, Nicolas Le Roux, Francis Bach. <a href="https://arxiv.org/pdf/1309.2388.pdf">Minimizing finite sums with the stochastic average gradient</a>. <em>Mathematical Programming</em>, <em>162</em>(1-2), 83-112, 2017.<br/>[2] Yoel Drori, Marc Teboulle. <a href="https://arxiv.org/pdf/1206.3209.pdf">Performance of first-order methods for smooth convex minimization: a novel approach</a>. <em>Mathematical Programming</em>, 145(1-2), 451-482, 2014.<br/>[3] Laurent Lessard, Benjamin Recht, Andrew Packard. <a href="https://epubs.siam.org/doi/abs/10.1137/15M1009597">Analysis and design of  optimization algorithms via integral quadratic constraints</a>. <em>SIAM Journal on Optimization</em>, 26(1), 57-95, 2016.<br/>[4] Adrien Taylor,  Julien Hendrickx, François Glineur. <a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">Smooth strongly convex interpolation and exact worst-case performance of first-order methods</a>. <em>Mathematical Programming</em>, 161(1-2), 307-345, 2017.<br/>[5] Yurii Nesterov. <a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">Introductory Lectures on Convex Optimization : a Basic Course</a>. <em>Applied optimization</em>. Kluwer Academic Publishing, 2004.<br/>[6]  Boris Polyak. Introduction to Optimization. Optimization Software New York, 1987.<br/>[7] Daniel Azagra, Carlos Mudarra. <a href="https://arxiv.org/pdf/1603.00241.pdf">An extension theorem for convex functions of class \(C^{1,1}\) on Hilbert spaces</a>. <em>Journal of Mathematical Analysis and Applications</em>, 446(2):1167–1182, 2017.<br/>[8] Aris Daniilidis, Mounir Haddou, Erwan Le Gruyer, Olivier Ley. <a href="https://hal.archives-ouvertes.fr/hal-01530908/file/DHLL_appendix.pdf">Explicit formulas for \(C^{1,1}\) Glaeser-Whitney extensions of 1-Taylor fields in Hilbert spaces</a>. <em>Proceedings of the American Mathematical Society</em>, 146(10):4487–4495, 2018.<br/>[9] Johan Löfberg. <a href="https://yalmip.github.io/">YALMIP : A toolbox for modeling and optimization in MATLAB</a>. <em>Proceedings of the CACSD Conference</em>, 2004.<br/>[10] APS Mosek. <a href="https://www.mosek.com/">The MOSEK optimization software</a>. Online at http://www.mosek.com, 54, 2010.<br/>[11] Adrien Taylor, Julien Hendrickx, François Glineur. <a href="https://arxiv.org/pdf/1705.04398.pdf">Exact worst-case convergence rates of the proximal gradient method for  composite convex minimization</a>. <em>Journal of Optimization Theory and Applications</em>, vol. 178, no 2, p. 455-476, 2018.<br/>[12] Adrien Taylor, Julien Hendrickx, François Glineur. <a href="https://arxiv.org/pdf/1512.07516.pdf">Exact worst-case performance of first-order methods for composite convex optimization</a>. <em>SIAM Journal on Optimization</em>, vol. 27, no 3, p. 1283-1313, 2017.<br/>[13] Nikhil Bansal, Anupam Gupta. <a href="http://www.theoryofcomputing.org/articles/v015a004/v015a004.pdf">Potential-Function Proofs for Gradient Methods. <em>Theory of Computing</em></a>, <em>15</em>(1), 1-32, 2019.<br/>[14] Adrien Taylor, Francis Bach. <a href="https://arxiv.org/pdf/1902.00947.pdf">Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions</a>, <em>Proceedings of the 32nd Conference on Learning Theory (COLT)</em>, 99:2934-2992, 2019.  <br/>[15] Bin Hu, Laurent Lessard. <a href="http://proceedings.mlr.press/v70/hu17a/hu17a.pdf">Dissipativity theory for Nesterov’s accelerated method</a>, <em>Proceedings of the 34th International Conference on Machine Learning</em>, 70:1549-1557, 2017. <br/>[16] Yoel Drori, Marc Teboulle. <a href="https://arxiv.org/abs/1409.2636">An optimal variant of Kelley’s cutting-plane method</a>. <em>Mathematical Programming</em> 160.1-2: 321-351, 2016.<br/>[17] Donghwan<strong> </strong>Kim, Jeffrey Fessler. <a href="https://arxiv.org/pdf/1406.5468.pdf">Optimized first-order methods for smooth convex minimization</a>, <em>Mathematical programming</em>, <em>159</em>(1-2), 81-107, 2016.<br/>[18] Donghwan Kim, Jeffrey Fessler. <a href="https://arxiv.org/pdf/1803.06600.pdf">Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions</a>, <em>preprint arXiv:1803.06600</em>, 2018.   <br/>[19] Felix Lieder. <a href="http://www.optimization-online.org/DB_FILE/2017/11/6336.pdf">On the convergence rate of the Halpern-iteration</a>. Technical Report, 2019.<br/>[20] Adrien Taylor, Julien Hendrickx, François Glineur.  <a href="https://perso.uclouvain.be/julien.hendrickx/availablepublications/PESTO_CDC_2017.pdf">Performance estimation toolbox (PESTO): automated worst-case analysis of  first-order optimization methods</a>,<em> Proceedings of the 56th Annual Conference on Decision and Control (CDC)</em>, pp. 1278-1283, 2017.</p></div>
    </content>
    <updated>2020-04-03T11:37:27Z</updated>
    <published>2020-04-03T11:37:27Z</published>
    <category term="Machine learning"/>
    <category term="Tools"/>
    <author>
      <name>Adrien Taylor</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-04-09T06:21:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=19411</id>
    <link href="https://gilkalai.wordpress.com/2020/04/03/trees-not-cubes-memories-of-boris-tsirelson/" rel="alternate" type="text/html"/>
    <title>Trees not Cubes! Memories of Boris Tsirelson</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post is devoted to a few memories of Boris Tsirelson who passed away at the end of January. I would like to mention that a few days ago graph theorist Robin Thomas passed away after long battle with ALS. … <a href="https://gilkalai.wordpress.com/2020/04/03/trees-not-cubes-memories-of-boris-tsirelson/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post is devoted to a few memories of Boris Tsirelson who passed away at the end of January. I would like to mention that a few days ago graph theorist Robin Thomas passed away after long battle with ALS. I hope  to tell about Robin’s stunning mathematics in a future post.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/03/tsirelson_boris.jpg"><img alt="" class="alignnone size-full wp-image-19552" height="480" src="https://gilkalai.files.wordpress.com/2020/03/tsirelson_boris.jpg?w=640&amp;h=480" width="640"/></a></p>
<p>Boris Tsirelson (1950 – 2020); <a href="http://www.math.tau.ac.il/~tsirel/">Boris’ home-page</a>, and <a href="https://en.wikipedia.org/wiki/Boris_Tsirelson">Wikipedia</a>. (More links, below.)</p>
<p>The title of the post is taken from the title of a very interesting 1999 paper by Boris Tsirelson and Oded Schramm: <a href="https://arxiv.org/abs/math/9902116">Trees, not cubes: hypercontractivity, cosiness, and noise stability</a></p>
<p>I was very sad and shocked to hear that Boris Tsirelson had passed away. Boris was one of the greatest Israeli mathematicians, and since 1997 or so we established mathematical connections surrounding several matters of common interest.  Here are a few memories.</p>
<h3>Love for coding</h3>
<p>1) One thing that Boris told me was that he loves to code. Being a “refusnik”, he could not get into Academia and (luckily) he could work as a programmer. And he told me that afterwards deciding what he liked more – programming or doing mathematical research – was no longer a trivial question for him.  Boris chose to go back to mathematical research, but he continued to enjoy programming, and when he needed it in his mathematical research, he could easily program.</p>
<h3>Love for quantum</h3>
<p>2) Another thing that Boris loved is “quantum”, the mathematics and physics of quantum mechanics and various connections to mathematics. Early on he proved his famous Tsirelson’s bound related to Bell’s inequalities, and later he was enthusiastic about the area of quantum computing. (And he learned it quickly, taught a course about it in 1997, and his 1997 lecture notes are still considered very useful.)</p>
<h3>Black Noise and noise sensitivity</h3>
<p>3) Perhaps the most significant mathematical connection between us was in the late 90s, and was centered around the theory of noise stability and noise sensitivity by Benjamini, Schramm and myself, which was closely related to a theory initiated by Boris Tsirelson and Anatoly Vershik. The translation between the different languages that we used and that Boris used was awkward, since the analog of Boolean functions that we studied was the “noise” that Boris studied, and the analog of noise sensitive Boolean functions in our language was “black noise” in Boris’s language. In any case, we had email discussions and we also met a few times with Itai and Oded regarding this connection.</p>
<h3>Black Noise and noise sensitivity II</h3>
<p>4)  Boris developed a very rich theory of black noise with relations to various areas of probability theory and operator algebras. He also found hypercontractivity that we used in our work quite useful to his applications, and also in this theory, he considered both classical and quantum aspects. I know only a little about Boris Tsirelson’s theory and its applications, but as far as tangent points with our Boolean interests are concerned, I can mention that Boris was enthusiastic by the <a href="https://arxiv.org/abs/1101.5820">result</a> of Schramm and Stas Smirnov that percolation is a “black noise” and also that, in 1999, Boris and Oded Schramm wrote a paper whose title started with “Trees not cubes!”, presenting a different angle on this theory.</p>
<p>Tsirelson saw white noise (what we call noise stability) as manifesting “linearity” while “black noise” (what we call noise sensitivity) as manifesting “non-linearity”. Over the years, I often asked him to explain this to me.</p>
<h3>Tsireslon’s Banach spaces</h3>
<p>5) Geometry of Banach spaces is a very strong area in Israel so naturally I heard as a graduate student about “Tsirelson’s space” from 1974 and some subsequent developments in the 80s. Boris Tsirelson constructed a  Banach space that does not contain an imbedding of <img alt="\ell_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ell_p"/> or <img alt="c_0" class="latex" src="https://s0.wp.com/latex.php?latex=c_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_0"/>.</p>
<h3>Bible codes</h3>
<p>6) My first personal connection with Boris was related to claims regarding a hidden Bible Code, and a 1994 paper claiming a statistical proof of the existence of these Bible codes. For many years my attitude was that these claims should be ignored, but around 1997, I changed my mind and did some work to see what was going on. Now, Boris kept a site linked in his homepage devoted to developments regarding the Bible Code claims. In this site Boris kindly reported about my first 1997 paper on the topic, my observation that the proximity of two reported p-values for the two Bible code experiments was “too good to be true”, and my interpretation that this suggests that the claimed results manifest naïve statistical expectations rather than scientific reality.  A few weeks later, Boris reported about a much stronger evidence (by McKay and Bar Nathan) against the Bible Code claims (they demonstrated codes of similar quality in Tolstoy’s “War and Peace”) and subsequently after some time he lost interest in this topic.</p>
<h3>Quantum computing skepticism</h3>
<p>6) In 2005 we had some correspondence and meetings regarding my quantum computing skepticism. In his first email he told me that my reference to “decoherence” seemed strange and I realized that I consistently referred to “entanglement” as “decoherence” and to “decoherence” as “entanglement”.</p>
<p>7) In his 1997 lecture notes on quantum computing (that I cannot find on the web, so I count on my memory), Boris addressed the concerns of early quantum computers skeptics like Rolf Landauer. He did not accept the analogy between quantum computing and analog computation, but he also regarded the analogy with digital computation as problematic. Rather, he regarded quantum information based on qubits as something (at least a priori) different from both these examples. (Update: I found one non-broken link to the lecture notes; indeed the subtitle of Chapter 9 is “neither analog nor digital”.)</p>
<p>A joke that I heard from Boris at that time</p>
<p>8) I remember that once when I asked him about some aspects of quantum fault tolerance he told me the following joke: A student is entitled to a special exam, he arrives at the professor’s office, is given three questions to answer and he fails to do so. He request and is granted a make-up exam two weeks later. When the student shows up at the office two weeks later the professor, who forgot all about it, gave him the same three questions. “This is extremely unfair”, said the student “you ask me questions that you already know that I cannot answer.”</p>
<h3>Noise sensitivity and high energy physics</h3>
<p>9) In 2006 I came up with the idea that noise sensitivity might be a great idea for physics. Knowing very little physics, I wrote a little manifesto with this idea and tried it, among other people, on Boris. As it turned out, Boris had the idea that noise sensitivity could add a useful modeling power to physics (especially high energy physics) well before that time. (And by 2006 he was already a bit skeptical regarding his own idea.) He also told me that one of the motivations of his 1998 paper with Tolya Vershik came from some mathematical ideas related to physics of the big bang. When I asked him if this was written somewhere in the paper itself, he answered: “Of course, not!”</p>
<h3>Boris Tsirelson’s lecture at Oded’s memorial school</h3>
<p>10) in 2009 we organized a meeting in memory of Oded Schramm and Boris gave a lecture related to the Schramm-Smirnov “percolation is black noise” result with a single theorem. And what was remarkable about it that it was that he presented a classical theorem with a quantum proof. You can find the videotaped lecture here  (And here are the slides. Boris never wrote up this result.) Following this lecture we had a short correspondence with Scott A. and Greg K. about quantum proofs to classical theorems. (Namely theorems that do not mention quantum in the statement).</p>
<h3>Tsirelson’s problem</h3>
<p>11) Our last correspondence in 2019 was about Thomas Vidick’s  Notices AMS article about Tsirelson’s problem. (This was a couple of months before the announcement of the solution.) Boris was pleased to hear about these developments, as he was regarding earlier developments in this area. He humorously refers to the history of his problem on his homepage and this interview.</p>
<p>12) People who knew Boris regarded him as a genius from a very early age, and former students have fond memories of his classes.</p>
<p> </p>
<h3>More resources:</h3>
<p>Boris’s <a href="http://www.math.tau.ac.il/~tsirel/">home page</a> contains  “Museum to my courses” with many useful lecture notes; link to a small <a href="http://www.math.tau.ac.il/~tsirel/Research/qcomp/main.html">page on quantum computation</a> with a link to Boris’ <a href="http://www.math.tau.ac.il/~tsirel/Courses/QuantInf/syllabus.html">1997 lecture notes on quantum computing</a>.  Links to comments on some of Tsirelson’s famous papers. <a href="http://www.math.tau.ac.il/~tsirel/Research/mybound/main.html">Tsirelson’s 1980 bound</a>. Boris <a href="http://www.math.tau.ac.il/~tsirel/Research/refereed.html">published papers</a>, and his “<a href="http://www.math.tau.ac.il/~tsirel/Research/self-publ.html">self-published</a>” papers.</p>
<p>Boris was a devoted Wikipedian and his Wikipidea <a href="https://en.wikipedia.org/wiki/User:Tsirel">user page</a> is now devoted to his memory; Here is <a href="https://www.iqoqi-vienna.at/en/blog/article/boris-tsirelson/?fbclid=IwAR1PrVvK0u5XmnFLLoPMzMN3x9rY1WIdp1wrYZ_yYPlqSGRpXDkollYTCR0">a great interview</a> with Boris; A very nice <a href="https://www.scottaaronson.com/blog/?p=4626">memorial post</a> on Freeman Dyson and Boris Tsirelson on the Shtetl Optimized; Tim Gowers explains some ideas behind Tsirelson’s space <a href="https://twitter.com/wtgowers/status/1231704267641249794">over Twitter;</a> and here in <a href="https://gowers.wordpress.com/2009/02/17/must-an-explicitly-defined-banach-space-contain-c_0-or-ell_p/">Polymath2</a>.</p>
<p>Below the fold some emails of interest from Boris, mainly where he explained to me various mathematics. (More can be found in this page.)</p>
<p><span id="more-19411"/></p>
<h2>Some email correspondence</h2>
<h3>Oct. 2019 Vidick’s paper</h3>
<p>Dear Boris<br/>
<a href="https://www.ams.org/journals/notices/201910/rnoti-p1618.pdf">This paper</a> by Thomas Vidick may interest you,<br/>
best regards and shana tova Gil</p>
<p>Oh yes, sure!<br/>
Thank you.<br/>
Shana metuka, Boris</p>
<p>(Remark: “metuka” means “sweet” in Hebrew.)</p>
<h3>Dec 2006 (about noise sensitivity and physics)</h3>
<p>(Dec 2006) My very first idea in this field (inspired by conversations with Vershik) was<br/>
rather physical (that Big Bang could be a natural occurrence of black noise),<br/>
and in fact the main example of “Tsirelson and Vershik 1998” follows this line<br/>
(not explicitly, of course).</p>
<p>In local (not Big Bang related) physics, I think, nonlinearity could produce<br/>
such effect. And then the very idea of `the field operator at a point’ (on<br/>
the level of operator-valued Schwartz distributions or something like that)<br/>
will fail. However, physicists do not want to consider this possibility<br/>
without very serious indications that it really is used by the nature. And<br/>
they are right…</p>
<h3>2005 quantum computer skepticism</h3>
<p>Subject: Re: Noise and more</p>
<p>Dear Gil:<br/>
Yes, of course, we can meet and speak.</p>
<p>For now, I am not much bothered. I am not an expert in quantum error<br/>
correction, but anyway, my feeling is that all physically reasonable<br/>
“attacks” of Nature are repelled. Especially, your three-qubit attack<br/>
looks to me not dangerous. And, “der Herr Gott is raffiniert, aber<br/>
boschaft ist er nicht”; Nature never attacks like an enemy.</p>
<p>Yours, Boris.</p>
<h3>Quick 2000 comments on a (sloppy) draft of my survey paper</h3>
<p>Dear Gil:</p>
<p>Thank you for the text; I am reading it.<br/>
For now, only a trivial remark: “Tsilerson” should be “Tsirelson” in<br/>
[133] and [134]; and in [132] “” should be “Tsirelson”…</p>
<p>Shabat shalom,<br/>
Yours, Boris.</p>
<h3>November 1998: Noise sensitivity and black noise</h3>
<p>Dear Gil,</p>
<p>I am reading your (with Itai and Oded) paper. Thanks.</p>
<p>Moreover, I am thinking about changing the title of my future talk in<br/>
Vien accordingly: from “The five noises” to “The six noises” (or even<br/>
more).<br/>
To this end, however, I need to answer the following question.</p>
<p>Is there a mesh refinement limit for the percolation?</p>
<p>That is, take the lattice with a small pitch \eps. Choose two<br/>
“electrodes”, say, two vertical intervals on two parallel vertical<br/>
lines, and ask about the probability that they are connected (via the<br/>
bond percolation on the whole band between the two vertical lines).<br/>
Let the electrodes be macroscopic; that is, they do not depend on<br/>
\eps. Does the probability of the event have a limit for \eps \to 0 ?<br/>
If it does, then one more question: what about the joint distribution<br/>
for a finite collection of such events? That is, I want to see a weak<br/>
limit of these “discrete” random processes. It seems to me, the<br/>
question is well-known and was discussed. However, do you know the<br/>
answer?</p>
<p>Yours, Boris.</p>
<p>Dear Itai, Oded, and Gil,</p>
<p>Thank you for the information. I see that for now we have a<br/>
conditional result: if there exists “the noise of percolation”, then<br/>
it is not a white noise.<br/>
Yours, Boris.</p>
<p>Dear Gil:<br/>
&gt; I dont understand yet the concept of “noise” precisely</p>
<p>One of ways is this. A noise is a scaling limit for coin tossing.<br/>
You choose a class of “macroscopic observables” and look, whether<br/>
their joint distribution converges, when n\to\infty. If it does, you<br/>
get a noise. (For percolation we do not know, does it or not.)<br/>
Now, if all “macroscopic observables” are noise insensitive, it means<br/>
that the noise is white. For a white noise, there is only one<br/>
invariant, its dimension (or multiplicity).<br/>
If all “macroscopic observables” are noise sensitive, the noise is<br/>
black. Probably, there are a lot of black noises, but for now we have<br/>
only two examples, without knowing, whether they are isomorphic, or<br/>
not. Spectra may be used for classifying black noises. Say, it may<br/>
happen that for each “macroscopic observable”, its spectrum is<br/>
concentrated on sets having Hausdorf dimension less than something.<br/>
If some “macroscopic observables” are noise sensitive but some others<br/>
are not (except for constants, of course), then the noise is neither<br/>
white nor black. It may happen that it is a direct sum of a white<br/>
noise and a black noise. However, it may happen that it is not. We<br/>
have for now two such examples: “noise if splitting” and “noise of<br/>
stickiness” (they are probably non-isomorphic); both are found by Jonathan<br/>
Warren. I am trying to understand, whether your matter can give more<br/>
examples.</p>
<p> </p>
<h3>August 1998: Bible code story</h3>
<p>Dear Gil,</p>
<p>Thanks for the text.<br/>
As for me, it is already an `overkill’, since for me the WRR94<br/>
is basically dead. But maybe for others…</p>
<p>Yours, Boris.</p>
<p> </p></div>
    </content>
    <updated>2020-04-03T08:03:55Z</updated>
    <published>2020-04-03T08:03:55Z</published>
    <category term="Combinatorics"/>
    <category term="Obituary"/>
    <category term="Probability"/>
    <category term="Quantum"/>
    <category term="Boris Tsirelson"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-04-09T06:20:29Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6956243767390574114</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6956243767390574114/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/lets-hear-it-for-cloud.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6956243767390574114" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6956243767390574114" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/lets-hear-it-for-cloud.html" rel="alternate" type="text/html"/>
    <title>Let's Hear It for the Cloud</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>Since March 19th I have worked out of home. I've had virtual meetings, sometimes seven or eight a day, on Zoom, Bluejeans, Google Hangouts, Google Meet, Blackboard Collaborate Ultra and Microsoft Teams. I take notes on my iPad using Penultimate which syncs with Evernote. I store my files in Dropbox and collaborate in Google Drive. I communicate by Google Chat, Gmail, Facebook messenger and a dozen other platforms. I continue to tweet and occasionally post in this blog. </div><div><br/></div><div>A billion of my closest friends around the world are also working out of home and using the same and similar tools. Yet outside of some pretty minor issues, all of these services continue to work and work well. Little of this would have been possible fifteen years ago. </div><div><br/></div><div>As Amazon scaled up their web operations to handle their growing business in the early 2000's they realized they could sell computing services. AWS, Amazon Web Services, started in 2006. Microsoft Azure, Google and others followed. These sites powered smartphones and their apps that push heavy processing to the cloud, small startups who don't need to run their own servers, and companies like Zoom when they need to scale up quickly and scale down like Expedia when they don't need as much use. Amazon and Microsoft makes most of their profit on cloud services. Amazon can't get me toilet paper but they can make sure Blackboard continues to work when all of our classes move online. </div><div><br/></div><div>Just for fun I like to occasionally look over the large collection of <a href="https://aws.amazon.com/products">Amazon Cloud Products</a>. Transcribe an audio recording and translate to Portuguese, not a problem. </div><div><br/></div><div>The cloud can't allow all of us to work from home. We have many who still go to work including front-line health care workers putting their lives on the line. Many have lost their jobs. Then of course there are those sick with the virus, many of whom will never recover. We can't forget about the reason we stay indoors.</div><div><br/></div><div>But every now and then it's good to look back and see how a technology has changed our world in a very short time. If we had this virus in the 90's we'd still be having to go to work, or simply stop teaching and other activities all together.</div><div><br/></div><div>And how will our universities and other work spaces look like in the future now that we find we can work reasonably well from home and even better technologies develop? Only time will tell.</div><div><br/></div><div><br/></div><div><br/></div></div>
    </content>
    <updated>2020-04-02T19:20:00Z</updated>
    <published>2020-04-02T19:20:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-04-08T20:36:36Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-6301869508435164660</id>
    <link href="http://processalgebra.blogspot.com/feeds/6301869508435164660/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=6301869508435164660" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6301869508435164660" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6301869508435164660" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/04/an-interview-with-davide-sangiorgi.html" rel="alternate" type="text/html"/>
    <title>An interview with Davide Sangiorgi, CONCUR Test-of-Time Award recipient</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The <a href="https://concur2020.forsyte.at/">International Conference on Concurrency Theory (CONCUR)</a> and the I<a href="https://concurrency-theory.org/organizations/ifip">FIP 1.8 Working Group on Concurrency Theory</a> are happy to announce the first edition of the CONCUR  Test-of-Time Award. The purpose of the award is to recognize important  achievements in Concurrency Theory that were published at the CONCUR  conference and have stood the test of time. All papers published in  CONCUR between 1990 and 1995 were eligible.<br/><br/>The award winners for the  CONCUR ToT Awards 2020 may be found <a href="https://concur2020.forsyte.at/test-of-time/index.html">here</a>, together with the citations for the awars. They were selected by a jury composed of Jos Baeten, Patricia  Bouyer-Decitre, Holger Hermanns, Alexandra Silva and myself.<br/><br/>This post is the first of a series in which I interview the recipients of the  CONCUR ToT Awards 2020. I asked <a href="http://www.cs.unibo.it/~sangio/">Davide Sangiorgi </a>(University of Bologna, Italy) a small number of questions via email and I report his answers below in a slightly edited form. Let  me thank Davide for his willingness to take part in an interview and for his inspiring answers, which I hope will be of interest to readers of this blog and will inspire young researchers to take up work on Concurrency Theory.<br/><br/>In what follows, LA stands for Luca Aceto and DS for Davide Sangiorgi.<br/><br/>LA: You receive one of the two CONCUR ToT Awards for the period 1992-1995 for your paper "<a href="http://www.lfcs.inf.ed.ac.uk/reports/93/ECS-LFCS-93-270/">A Theory of Bisimulation for the pi-Calculus</a>", presented at CONCUR 1993. Could you tell us briefly what spurred you to develop open bisimilarity and how the ideas underlying that notion came about?<br/><br/>DS: I developed the paper on open bisimulation during 1992 and 1993.  I was in Robin Milner's group, in Edinburgh.  We were studying and questioning basic aspects of the theory of the <a href="https://en.wikipedia.org/wiki/%CE%A0-calculus">pi-calculus</a>. One such aspect was the definition of equality on processes; thus a very fundamental aspect, underlying the whole theory.  The equality had to be a form of bisimulation, in the same way as it was for CCS.  The two forms of bisimilarity that were around for the pi-calculus, late and early bisimilarity, are not congruence relations.  In both cases, the input clause of bisimilarity uses a universal quantification on the possible instantiations of the bound name. As  a consequence, neither bisimilarity is preserved by the input prefix (forbidding substitutions in the input clause would make things worse, as congruence  would fail for parallel composition). Therefore, one has to introduce separately the induced congruence, by universally quantifying the bisimilarities over all name substitutions.  In other words,  the two bisimilarities are  not fully substitutive ('equal' terms cannot be replaced, one for the other,  in an arbitrary context).   On the other hand, the  congruences induced by the bisimilarities  are not themselves  bisimilarities. Hence in this case 'equal' terms, after some actions,  need not be 'equal' anymore.  Thus, for instance, such relations do not support dynamic modifications of the context surrounding related terms.<br/><br/>This situation was not fully satisfactory. The same could be said for the algebraic theory: there were proof systems for the two bisimilarities (of course, on finitary processes) but, because of the above congruence issue, there were no axiomatisations.  (In those years I was also working with Joachim Parrow on axiomatisations of these relations.)<br/><br/>The universal quantification on substitutions in the input clause of the bisimilarites and in the definitions of the induced congruences was also unsatisfactory because it  could make checking equalities cumbersome.<br/><br/>All these were  motivations for looking at possible variations of the definition of bisimulation. The specific hints towards open bisimulation came from thinking at two key facets of the pi-calculus model that were somehow neglected in the definitions of  early and late bisimilarities.  The first facet has to do with the pi-calculus rejection of the separation between channels and variables (`channel' here meaning a 'constant identifier'). In the pi-calculus ,there is only one syntactic category, that of <i>names</i>, with no formal distinction between channels and variables. This contributes to the elegance of the model and its theory. However, both in early and in late bisimilarity, the free names of processes are treated as channels, whereas the bound names of  inputs are treated as variables because of their immediate  instantiation  in the bisimilarity clause.   There was somehow a discrepancy between the syntax and the semantics. <br/><br/>The second facet of the pi-calculus that contributed to the definition of open bisimilarity is the lack of the mismatch operator: the pi-calculus, at least in its original proposal, has a match operator to test equality between names, but not the dual mismatch, to test for inequality.  Mismatching had been excluded for the preservation of a monotonicity property on transitions, intuitively asserting that substitutions may only increase the action capabilities of a  process.  (Both facets above represented major differences between the pi-calculus and its closest ancestor----Engberg and Nielsen's Extended CCS.)  Thus I started playing with the idea of avoiding the name instantiation in the input clause and, instead, allowing, at any moment, for arbitrary instantiations (i.e., substitutions) of the names of the processes---the latter justified by the above monotonicity property of transitions. By adding the requirement of being a congruence, the definition of open bisimilarity came about.<br/><br/>Still, I was not sure that such a bisimulation could be interesting and robust.  Two further developments helped here. One was the axiomatisation (over recursion-free terms).  It was a pure axiomatisation, it was  simple,  and with a completeness proof that leads to the construction of canonical and minimal (in some syntactic sense) representatives for the equivalence classes of the bisimilarity.  For other bisimilarities, or related congruences, obtaining canonical representatives seems hard; at best such representatives are parametrised upon a set of free names and even in these cases minimality is not guaranteed.<br/><br/>The other development has to do with a symbolic or "efficient" characterisation of the bisimilarity. The initial definition of open bisimulation makes heavy use of substitutions.  In the symbolic characterisation, substitutions are performed only when needed (for instance, the unification of two names <i>a</i> and <i>b</i> is required if there is an input at <i>a</i> and an output at <i>b</i> that can interact), somehow echoing the call-by-need style of  functional languages.  Such a characterisation seemed promising for automated or semi-automated verification. <br/><br/>LA: How much of your later work has built on your CONCUR 1993 paper? What results of yours are you most proud of and why?<br/><br/>DS: The most basic idea in open bisimulation is to avoid the instantiation of the bound name of an input, possibly making such a bound name a free name of the derivative term.  The use of substitutions, elsewhere in the definition, is necessary to obtain a congruence relation for the pi-calculus.  I was surprised to discover, in the following years, that such substitutions are not necessary in two relevant subsets of the pi-calculus.  I called the variant of open bisimulation without substitutions <i>ground</i> bisimulation (I think the name came from Robin).  One subset is Honda and <a href="https://hal.inria.fr/inria-00076939/en">Boudol</a>'s Asynchronous pi-calculus, whose main constraint is to make outputs floating particles that do not trigger the activation of a continuation (other limitations concern sum and matching).  The other subset is the Internal (or Private) pi-calculus, in which only private (i.e., restricted) names may be transmitted.  I designed the Internal pi-calculus with ground bisimilarity in mind.  People seem to have found this calculus useful in many ways, partly because of its expressiveness combined with its  simple theory (in many aspects similar to that of CCS), partly because it allows one to limit or control aliasing between names, which can be useful for carrying out proofs about behavioural properties of processes, or for designing and reasoning about type systems, or for representing the calculus in logical systems.<br/><br/>Sometimes, the possibility of using ground bisimulation can considerably simplify proofs of equalities of terms. For instance, in my works on comparisons between pi-calculus and lambda-calculus, when I had to translate the latter into the former I have always used one of the above subcalculi (sometimes even combining them, e.g., the Asynchronous Internal pi-calculus), precisely for being able to use ground bisimilarity.<br/><br/>I  consider both ground bisimilarity and the Internal pi-calculus spin-offs of the work on open bisimilarity.<br/><br/>While working on open bisimilarity for the pi-calculus, in a different paper, I applied the idea of open bisimilarity to the lambda-calculus.  I kept the name 'open'  but the bisimulation is really 'ground', as there are no substitutions involved.  I remember Robin encouraging me to keep the name 'open' because it conveyed well the idea of setting a bisimulation on open terms, rather than on closed terms as usually done. In  open bisimulation for the lambda-calculus, a lambda-abstraction<i> lambda x. M</i> yields an action with label <i>lambda x</i> that should be matched (modulo  alpha-conversion) by the same action by a bisimilar term. (Of course additional bisimulation clauses are needed when a free variable is found in evaluation position.) In contrast, in the ordinary bisimulation for the lambda-calculus,  Abramsky's applicative bisimilarity, the bound variable of an abstraction has to be instantiated with all closed terms, which is heavy. In general, open bisimilarity is finer than applicative bisimilarity and contextual equivalence (the reference equivalence in the lambda-calculus) but they often coincide in examples of concrete interest. Moreover, open bisimilarity does coincide with contextual equivalence in appropriate extensions of the lambda-calculus.  In short, open bisimilarity offers us a technique for reasoning on higher-order languages  using  'first-order' tools, somehow similarly to what game semantics does.<br/><br/>This line of work about open bisimilarity in higher-order languages has been very fruitful, and is still studied a lot, for various forms of higher-order languages, sometimes under the name of 'normal-form' bisimulation. <br/><br/>LA: In your opinion, what is the most interesting or unexpected use in the literature of the notions and techniques you developed in your award-winning paper?<br/><br/>DS: I mentioned the hope, when working on open bisimilarity, that its symbolic "efficient" characterisation could be useful for automated or semi-automated tools for reasoning about behavioural properties.  Shortly after introducing open bisimulation,   Björn Victor and Faron Moller, both in Edinburgh at the time,   exploited it to design the Mobility Workbench. I also worked on an algorithm and a prototype tool for on-the-fly checking, with Marco Pistore.  <br/><br/>However the most surprising applications in this direction have arrived later, when the 'lazy' instantiation of bound names of open bisimilarity has been applied to languages richer than pi-calculus.  For instance, Chaki, Rajamani, Rehof have used open similarity in their methods and tools for model checking distributed message-passing software.  Others have applied open bisimilarity to languages for security and cryptography, like the spi-calculus and applied pi-calculus. These include S. Brias, U. Nestmann and colleagues at EPFL and Berlin.   R. Horne, A. Tiu and colleagues in Singapore and Luxembourg have pushed significantly in this direction, with verification techniques and tools. For instance very recently they have discovered a privacy vulnerability for e-passports.  <br/><br/>Similarly, Yuxin Deng and colleagues have applied the idea of open bisimulation to quantum processes, with analogous motivations --- avoiding the universal quantification in the  instantiation of variables, algorithmically unfeasible in the quantum setting as quantum states constitute a continuum.<br/><br/>Another line of work that I found interesting and surprising concerns abstract frameworks for concurrency, including logical frameworks. Here forms of open bisimulation are often the 'natural' bisimulation that come up. These frameworks may be based, for instance on  coalgebras and category theory  (e.g., works by M. Fiore and S. Staton, N. Ghani,   K. Yemane, and B. Victor), category theory for reactive systems (e.g., works by F. Bonchi, B. König and U. Montanari), nominal SOS rule formats  (e.g., works by M. Cimini,  M. R. Mousavi, and  M. A. Reniers), higher-order logic languages (e.g., works by A.  Tiu, G. Nadathur, and D. Miller). <br/><br/>There have been works that have pushed the idea of open bisimulation of avoiding the instantiation of  bound names in interactions with an external observer one step further: such bound names are not instantiated even in interactions<i> internal </i>to the processes. The substitutions produced by the interactions are added to the calculus, producing particles sometimes called fusions.  This mechanism resembles the explicit substitutions of the lambda-calculus, but it goes beyond that; for instance the addition of fusions leads to modifications of  input and output prefixes that produce pleasant syntactic and semantic symmetry properties.  Various people have worked on this, including B. Victor, J. Parrow, Y. Fu, C. Laneve, P. Gardner and L. Wischik.<br/><br/>I should also mention the recent work by K. Y. Ahn, R. Horne, and A. Tiu  on logical interpretations of open bisimilarity. Remarkably, they explain the difference between the original (late and early)  formulations of bisimilarity in the pi-calculus  and open bisimilarity as the difference between  intuitionistic and classical versions of modal logics. <br/><br/>Apologies for not mentioning everybody!<br/><br/>LA: To your mind, how has the focus of CONCUR changed since its first edition in 1990?<br/><br/>DS: I remember that in the early years of CONCUR there was  a tremendous excitement about the conference.  A forum for grouping the (fast-growing) community had been long-awaited.  In Edinburgh, every year after the conference,   the people with an interest in concurrency   would meet (multiple times!)  and discuss the contents of the proceedings.  Several of us every year would attend the conference. Attending the conference was very useful and interesting: one was sure to meet a lot of people, hear about excellent papers, have lively discussions.  We would  try to go,  even without  a paper in the proceedings.   I vividly remember the 1993 edition, where I presented the paper on open bisimulation.  It had been organised by Eike Best in Hildesheim, Germany. It was an  enjoyable and exciting week,  I met and knew a number of people of our community, and learned a lot. (How sad and unwise that the Computer Science department in Hildesheim, so strong in concurrency  at the time, was shut down a few years later.)  Over the years, the CONCUR community has kept increasing its size. The conference has substantially broadened its scope,  rightly including new emerging topics. Perhaps it is more difficult than in the past to (more or less) understand most of the presented papers, both because of the diversity of the topics and of their technical specialisation.  On the other hand, there are now satellite events, covering  a number of areas. Hence  there are always plenty of interesting presentations and talks to attend (this definitely occurred to me in the last edition, in Amsterdam!).  I should also mention here the activity on the IFIP WG 1.8 on Concurrency Theory, currently chaired by Ilaria Castellani, that in many ways  supports and promotes CONCUR.<br/><br/>The quality of the papers at CONCUR is still very high. This is very important. As a community we should strive to maintain, and possibly even increase, the excellence and prestige of the conference, first of all, by submitting our best papers to the conference. CONCUR must be a reference conference in Computer Science, which is essential for injecting new people and energy into the community.<br/><br/><b>Acknowledgements:</b> Many thanks to <a href="http://www-sop.inria.fr/members/Ilaria.Castellani/">Ilaria Castellani</a>, who pointed out a number of typos in the original version of this text. </div>
    </content>
    <updated>2020-04-02T16:16:00Z</updated>
    <published>2020-04-02T16:16:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-04-04T14:46:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-04-02-bilinear-accumulators-for-cryptocurrency/</id>
    <link href="https://decentralizedthoughts.github.io/2020-04-02-bilinear-accumulators-for-cryptocurrency/" rel="alternate" type="text/html"/>
    <title>Bilinear Accumulators for Cryptocurrency Enthusiasts</title>
    <summary>Accumulator schemes are an alternative to Merkle Hash Trees (MHTs) for committing to sets of elements. Their main advantages are: Constant-sized membership and non-membership proofs, an improvement over logarithmic-sized proofs in MHTs, Algebraic structure that enables more efficient proofs about committed elements1 (e.g., ZeroCoin2 uses RSA accumulators for anonymity), Constant-sized...</summary>
    <updated>2020-04-02T08:10:00Z</updated>
    <published>2020-04-02T08:10:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-04-08T23:42:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1644</id>
    <link href="https://theorydish.blog/2020/04/01/approx-random-2020-is-virtual-from-the-get-go/" rel="alternate" type="text/html"/>
    <title>APPROX/RANDOM 2020 is Virtual From the Get Go</title>
    <summary>While APPROX/RANDOM 2020 was scheduled for September, we decided to reduce uncertainty in these stormy days and declare it to be virtual already in the CFP. We hope to have APPROX/RANDOM 2021 hosted in Seattle by UW, instead of this year,  So if you never sent papers to APPROX/RANDOM because you hate travel, this is your year to start!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>While <a href="https://randomconference.com/random-2020-home/">APPROX/RANDOM 2020</a> was scheduled for September, we decided to reduce uncertainty in these stormy days and declare it to be virtual already in the <a href="https://randomconference.files.wordpress.com/2020/04/randomapprox2020cfp-3.pdf">CFP</a>. We hope to have APPROX/RANDOM 2021 hosted in Seattle by UW, instead of this year,  So if you never sent papers to APPROX/RANDOM because you hate travel, this is your year to start!</p></div>
    </content>
    <updated>2020-04-01T22:37:07Z</updated>
    <published>2020-04-01T22:37:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-04-09T06:21:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-7256726075360411284</id>
    <link href="http://processalgebra.blogspot.com/feeds/7256726075360411284/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=7256726075360411284" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7256726075360411284" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7256726075360411284" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/04/magnus-m-halldorsson-eatcs-fellow-2020.html" rel="alternate" type="text/html"/>
    <title>Magnus M. Halldorsson: EATCS Fellow 2020</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">As announced <a href="http://eatcs.org/index.php/component/content/article/1-news/2851--eatcs-fellows-class-of-2020-named">here</a>, the EATCS Fellows vintage 2020 are<br/><ul><li><a href="http://pages.di.unipi.it/degano/">Pierpaolo Degano</a>, Universita di Pisa, Italy: for his contributions in concurrency theory and applications in security and for biological systems. </li><li><a href="http://www.cs.umd.edu/~hajiagha/">Mohammad Taghi Hajiaghayi,</a> University of Maryland, USA: for his contributions to the theory of algorithms, in particular algorithmic graph theory, game theory, and distributed computing. </li><li><a href="https://www.ru.is/~mmh/">Magnus Mar Halldorsson</a>, Reyjavik University, Iceland: for his contributions to the theory of approximation and graph algorithms as well as to the study of wireless algorithmics. </li></ul>Congratulations to all of them! However, I trust that Mohammad and Pierpaolo will forgive me if I devote this post to celebrate Magnus, his work and his contributions to the TCS community.<br/><br/>So, why was Magnus chosen as one of the EATCS Fellows 2020? Here are some reasons why.<br/><br/>Magnús has offered seminal contributions to the theory of approximation and graph algorithms as well as to the study of wireless algorithmics. His research career and contributions so far can be roughly divided into two phases. The first phase spans the time from the beginning of his career until roughly ten years ago. During that time, Magnús made significant contributions to approximation algorithms for maximum independent set and graph colouring, amongst many other problems. In the second phase, which started a bit more than ten years ago, he has worked on the algorithmics of realistic models for wireless computation. I think that it is fair to say that Magnús is currently <i>the</i> expert on wireless algorithmics based on the SINR model.<br/><br/>These two phases are not at all disjoint. Indeed, the typical problems studied in the SINR model, such as determining the capacity of wireless networks or how to schedule messages in such networks, can be seen as independent set and colouring problems, respectively, and his experience with those problems in graph algorithmics certainly helped Magnús in obtaining breakthrough results in wireless algorithmics. Throughout his career, Magnús has also given significant contributions to the computation of independent sets and colourings in restricted computational models, such as the online model of computation and the data streaming model. His sustained research productivity, both in quality and in quantity, is all the more remarkable since it has largely been achieved working in the difficult research environment in Iceland, where he was largely isolated until the establishment of the <a href="http://icetcs.ru.is/">Icelandic Centre of Excellent in Theoretical Computer Science (ICE-TCS)</a> in 2005. (Magnus has been the scientific director of the centre for 15 years.)<br/><br/>In addition to his seminal research achievements, Magnús has served the theoretical computer science community by sitting on prestigious award committees, organizing conferences and workshops in Iceland and elsewhere, serving on steering committees and by acting as an inspiring mentor for young researchers in computer science who have come to Iceland explicitly to work with him. By way of example, Magnús is a member of the steering committees for SIROCCO (chair), ALGOSENSORS and SWAT, Scandinavian Symposium and Workshops on Algorithm Theory (chair). He was a member of the Council of the EATCS and has organized the best attended ICALP conference to date (ICALP 2008). Amongst many other such duties, he was PC chair for Track C of ICALP 2015 and of ESA 2011.<br/><br/>Magnús was also one of the initiators and first editors of “<a href="http://www.nada.kth.se/%20%CC%83viggo/wwwcompendium/">A compendium of NP optimization problems”</a>, which is catalog of approximability results for NP optimization problems and has been a useful resource for researchers in that field for a long time. <br/><br/>Summing up, Magnús is a true stalwart of the algorithmics research community, and a great example for many of us. In my, admittedly biased, opinion, he richly deserves the recognition of being named an EATCS Fellow. I have no doubt that he will continue to lead by example in the coming years.<br/><br/><br/><br/></div>
    </content>
    <updated>2020-04-01T21:54:00Z</updated>
    <published>2020-04-01T21:54:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-04-04T14:46:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=19719</id>
    <link href="https://gilkalai.wordpress.com/2020/04/01/an-update-from-israel-and-memories-from-singapore-partha-dasgupta-robin-mason-frank-ramsey-and-007/" rel="alternate" type="text/html"/>
    <title>A small update from Israel and memories from Singapore: Partha Dasgupta, Robin Mason, Frank Ramsey, and 007</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A small update about the situation here in Israel Eight weeks ago I wrote that my heart goes out to the people of Wuhan and China, and these days my heart goes out to people in Italy, Spain, the US, … <a href="https://gilkalai.wordpress.com/2020/04/01/an-update-from-israel-and-memories-from-singapore-partha-dasgupta-robin-mason-frank-ramsey-and-007/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3>A small update about the situation here in Israel</h3>
<p>Eight weeks ago <a href="https://gilkalai.wordpress.com/2020/02/03/thinking-about-the-people-of-wuhan-and-china/">I wrote</a> that my heart goes out to the people of Wuhan and China, and these days my heart goes out to people in Italy, Spain, the US, Iran, France, the United Kingdom, Germany, Netherland and many other countries all over the world. Of course, I am especially worried about the situation here in my beloved country Israel, and let me tell you a little about it.</p>
<p>The pandemic started here late but it hit us pretty hard with 5,358 identified cases yesterday. Severe measures of social distancing were gradually introduced, and right now it is too early to tell if the pandemic is under control.</p>
<p>My part in this struggle is to stay at home. (Many Israeli scientists are making various endeavors and proposing ideas of various kind for fighting the disease and I salute them all for their efforts.) Like all of us I am very thankful to medical and other essential workers who are in the front-lines. As a scientist, I am especially impressed by the people from the Ministry of Health who manage the crisis and communicate with the public. They represent the very best we can offer in terms of science and medicine, decision making, gathering information, communicating with the public, and managing the crisis. In the picture below you can see three of the leaders – Moshe Bar Siman Tov (middle) Prof. Itamar Grotto (right) and Professor Sigal Sadetzki (left).</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/03/listen.png"><img alt="" class="alignnone size-full wp-image-19695" height="371" src="https://gilkalai.files.wordpress.com/2020/03/listen.png?w=640&amp;h=371" width="640"/></a></p>
<h2>And now for today’s post</h2>
<p>We had a tradition of sharing entertaining taxi-and-more stories and this post belongs to this category. We note that our highest quality story teller Michal Linial, a prominent Israeli biologist, is now involved in various aspects of the struggle against the disease. Our post today is part of<a href="https://gilkalai.files.wordpress.com/2020/03/gil-michal.docx"> a report by Michal Feldman and me on our experience from the ICA3 conferences in Singapore and Birmingham</a>.</p>
<h2>Partha Dasgupta, Robin Mason, Frank Ramsey, and James Bond</h2>
<p>After hearing about him for many years, it was a great pleasure for both Michal Feldman and myself to finally meet Partha Dasgupta in person and to listen to his lecture. Partha who is the Frank Ramsey Professor of Economics at Cambridge was introduced by a person, who entered the room directly from an intercontinental flight, whom we did not know but who made a strong impression on us. He devoted part of his introduction to Frank Ramsey who was a mathematician, philosopher and economist, and who had made fundamental contributions to algebra and had developed the canonical model of saving in economic growth, before he died at the young age of 26. (And yes! also Ramsey’s theorem!)</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/03/james-bond.png"><img alt="" class="alignnone size-full wp-image-19699" height="423" src="https://gilkalai.files.wordpress.com/2020/03/james-bond.png?w=640&amp;h=423" width="640"/></a></p>
<p>Seeing the introducer, Robin Mason, three words came into our minds (more precisely two words, one repeated twice): “Bond, James Bond.”</p>
<p>Indeed, this has led to the following sequence of profound ideas:</p>
<p>1) Robin Mason is a perfect choice for a new generation James Bond.</p>
<p>2) The name “James Bond” is overused. “Robin Mason” is a perfect name to replace the name “James Bond”.</p>
<p>3) Espionage is a little obsolete and it lost much of its prestige and charm. Science and academia is the new thing! An international interdisciplinary academics is the perfect profession which, at present, deserves the prestige formely associated with espionage.</p>
<p>In summary, we came a full circle. Robin Mason is the perfect new choice for James Bond, “Robin Mason” is the perfect new name to replace the name “James Bond,” and Mason’s academic activities and title of Pro-Vice-Chancellor (International) are the perfect replacement for Bond’s activities and the title ‘007’.</p>
<p>(The option of Mason playing his role on the movies rather than in real life should be considered. ‘Q’ could be handy for science as well. )</p>
<p><a href="https://youtu.be/dMSHmHc0z-E">Clique here</a> for Robin’s introduction and Partha’s lectur</p></div>
    </content>
    <updated>2020-04-01T19:44:35Z</updated>
    <published>2020-04-01T19:44:35Z</published>
    <category term="Algebra"/>
    <category term="Combinatorics"/>
    <category term="Conferences"/>
    <category term="Economics"/>
    <category term="Taxi-and-other-stories"/>
    <category term="Updates"/>
    <category term="Frank Ramsey"/>
    <category term="Itamar Grotto"/>
    <category term="Michal Feldman"/>
    <category term="Michal Linial"/>
    <category term="Moshe Bar Siman Tov"/>
    <category term="Partha Dasgupta"/>
    <category term="Robin Mason"/>
    <category term="Sigal Sadetzki"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-04-09T06:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blog.simons.berkeley.edu/?p=146</id>
    <link href="https://blog.simons.berkeley.edu/2020/04/lattice-blog-reduction-part-i-bkz/" rel="alternate" type="text/html"/>
    <title>Lattice Blog Reduction – Part I: BKZ</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is the first entry in a (planned) series of at least three, potentially four or five, posts about lattice block reduction. The purpose of this series is to give a high level introduction to the most popular algorithms and … <a href="https://blog.simons.berkeley.edu/2020/04/lattice-blog-reduction-part-i-bkz/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is the first entry in a (planned) series of at least three, potentially four or five, posts about lattice block reduction. The purpose of this series is to give a high level introduction to the most popular algorithms and their analysis, with pointers to the literature for more details. The idea is to start with the obvious – the classic BKZ algorithm. In the next two posts we will look at two lesser known algorithm, which allow to highlight useful tools in lattice reduction. These three posts will focus on provable results. I have not decided how to proceed from there, but I could see the series being extended to topics involving heuristic analyses, practical considerations, and/or a survey of more exotic algorithms that have been considered in the literature.</p>
<h4 id="target-audience">Target Audience</h4>
<p>I will assume that readers of this series are already familiar with basic concepts of lattices, e.g. bases, determinants, successive minima, Minkowski’s bound, Gram-Schmidt orthogonalization, dual lattices and dual bases, etc. If any of these concepts seem new to you, there are great resources to familiarize yourself with them first (see e.g. lecture notes by <a href="http://cseweb.ucsd.edu/classes/fa19/cse206A-a/">Daniele</a>, <a href="https://cims.nyu.edu/~regev/teaching/lattices_fall_2009/index.html">Oded</a>, <a href="https://homepages.cwi.nl/~dadush/teaching/lattices-2018/">Daniel/Léo</a>). It will probably help if you are familiar with the LLL algorithm (also covered in aforementioned notes), but I’ll try to phrase everything so it is understandable even if if you aren’t.</p>
<p>Ok, so let’s get started. Before we look at BKZ in particular, first some comments about lattice block reduction in general.</p>
<h1 id="sec:basics">The Basics</h1>
<h4 id="the-goal">The Goal</h4>
<p>Why would anyone use block reduction? There are (at least) two reasons.</p>
<h4 id="section"/>
<p>1) Block reduction allows you to find short vectors in a lattice. Recall that finding the shortest vector in a lattice (i.e. solving SVP) is really hard (as far as we know, this takes at least <span class="math inline">\(2^{\Omega(n)}\)</span> time or even <span class="math inline">\(n^{\Omega(n)}\)</span> if you are not willing to also spend exponential amounts of memory). On the other hand, finding somewhat short vectors that are longer than the shortest vector by “only” an exponential factor is really easy (see LLL). So what do you do if you need something that is shorter than what LLL gives you, but you don’t have enough time to actually find the shortest vector? (This situation arises practically every time you use lattice reduction for cryptanalysis.) You can try to find something in between and hope that it doesn’t take as long. This is where lattice reduction comes in: it gives you a smooth trade-off between the two settings. It is worth mentioning that when it comes to approximation algorithms, block reduction is essentially the only game in town, i.e. there are, as far as I know, no non-trivial approximation algorithms that cannot be viewed as block reduction. (In fact, this is related to an open problem that Noah stated during the program: to come up with a non-trivial approximation algorithm that does not rely on a subroutine to find the shortest lattice vector in smaller dimensions.) The only exception to this are quantum algorithms that are able to find subexponential approximations in polynomial time in lattices with certain (cryptographically highly relevant) structure (see <span class="citation">[CDPR16]</span> and follow up work).</p>
<h4 id="section-1"/>
<p>2) Block reduction actually gives you more than just short vectors. It gives you guarantees on the “quality” of the basis. What do we mean by the quality of the basis? Consider the Gram-Schmidt vectors <span class="math inline">\({\mathbf{b}}_i^*\)</span> (GSO vectors) associated to a lattice basis <span class="math inline">\({\mathbf{B}}\)</span>. What we want is that the length of these Gram-Schmidt vectors (the GSO norms) does not drop off too quickly. The reason why this is a useful measure of quality for lattice bases is that it gives a sense of how orthogonal the basis vectors are: conditioned on being bases of the same lattice, the less accentuated the drop off in the GSO vectors, the more orthogonal the basis, and the more useful this basis is to solve several problems in a lattice. In fact, recall that the product of the GSO norms is equal to the determinant of the lattice and thus remains constant. Accordingly, if the GSO norms do not drop off too quickly, the first vector can be shown to be relatively short. So by analyzing the quality of the basis that block reduction achieves, a guarantee on the length of the first vector comes for free (see goal 1)). If you are familiar with the analysis of LLL, this should not come as a surprise to you.</p>
<h4 id="tools">Tools</h4>
<p>In order to ensure that the GSO norms do not drop off to quickly, it seems useful to be able to reduce them locally. To this end, we will work with projected lattice blocks (this is where the term “block” in block reduction comes from). More formally, given a basis <span class="math inline">\({\mathbf{B}}\)</span> we will consider the block <span class="math inline">\({\mathbf{B}}_{[i,j]}\)</span> for <span class="math inline">\(i &lt; j\)</span> as the basis formed by the basis vectors <span class="math inline">\({\mathbf{b}}_i, {\mathbf{b}}_{i+1}, \dots, {\mathbf{b}}_{j}\)</span> <em>projected orthogonally to the first <span class="math inline">\(i-1\)</span> basis vectors</em>. So <span class="math inline">\({\mathbf{B}}_{[i,j]}\)</span> is a basis for the lattice given by the sublattice formed by <span class="math inline">\({\mathbf{b}}_1, {\mathbf{b}}_{2}, \dots, {\mathbf{b}}_{j}\)</span> projected onto the orthogonal subspace of the vectors <span class="math inline">\({\mathbf{b}}_1, {\mathbf{b}}_{2}, \dots, {\mathbf{b}}_{i-1}\)</span>. Notice that the first vector of <span class="math inline">\({\mathbf{B}}_{[i,j]}\)</span> is exactly <span class="math inline">\({\mathbf{b}}^*_i\)</span> – the <span class="math inline">\(i\)</span>-th GSO vector. Another way to view this is to consider the QR-factorization of <span class="math inline">\({\mathbf{B}} = {\mathbf{Q}} {\mathbf{R}}\)</span>, where <span class="math inline">\({\mathbf{B}}\)</span> is the matrix whose columns are the basis vectors <span class="math inline">\({\mathbf{b}}_i\)</span>. Since <span class="math inline">\({\mathbf{Q}}\)</span> is orthonormal, it represents a rotation of the lattice and we can consider the lattice generated by the columns of <span class="math inline">\({\mathbf{R}}\)</span> instead, which is an upper triangular matrix. For an upper triangular basis, the projection of a basis vector orthogonal to the previous basis vectors simply results in dropping the first entries from the vector. So considering a projected block <span class="math inline">\({\mathbf{R}}_{i,j}\)</span> is simply to consider the square submatrix of <span class="math inline">\({\mathbf{R}}\)</span> consisting of the rows and columns with index <span class="math inline">\(k\)</span> between <span class="math inline">\(i \leq k \leq j\)</span>.</p>
<p>Now we need a tool that allows us to control these GSO vectors, which we view as the first basis vectors in projected sublattices. For this, we will fall back to algorithms that solve SVP. Recall that this is very expensive, so we will not call this on the basis <span class="math inline">\({\mathbf{B}}\)</span> but rather on the projected blocks <span class="math inline">\({\mathbf{B}}_{[i,j]}\)</span>, where we ensure that the dimension <span class="math inline">\(k = j-i+1\)</span> of the lattice generated by this projected block is not too large. In fact, the maximum dimension <span class="math inline">\(k\)</span> that we call the SVP algorithm on will control the time/quality trade-off achieved by our block reduction algorithms and is usually denoted by the block size. So we will assume that we have access to such an SVP algorithm. Actually, we will assume something slightly stronger: we will assume access to a subroutine that takes as input the basis <span class="math inline">\({\mathbf{B}}\)</span> and indices <span class="math inline">\(i,j\)</span> and outputs a basis <span class="math inline">\({\mathbf{C}}\)</span> such that</p>
<ul>
<li><p>the lattice generated by the basis remains the same</p></li>
<li><p>the first <span class="math inline">\(i-1\)</span> and the last vectors starting from <span class="math inline">\(j+1\)</span> remain unchanged</p></li>
<li><p>the projected block <span class="math inline">\({\mathbf{C}}_{[i,j]}\)</span> is <em>SVP reduced</em>, meaning that <span class="math inline">\({\mathbf{c}}^*_i\)</span> is the shortest vector in the lattice generated by <span class="math inline">\({\mathbf{C}}_{[i,j]}\)</span>. Additionally, if <span class="math inline">\({\mathbf{B}}_{[i,j]}\)</span> is already SVP reduced, we assume that the basis <span class="math inline">\({\mathbf{B}}\)</span> is left unchanged.</p></li>
</ul>
<p>We will call an algorithm that achieves this an <em>SVP oracle</em>. Such an oracle can be implemented given any algorithm that solves SVP (for arbitrary lattices). The technical detail of filling in the gap is left as homework to the reader.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-156" height="315" src="https://blog.simons.berkeley.edu/wp-content/uploads/2020/04/svp.png" width="267"/>Effect of a call to the SVP oracle. GSO log norms of the input in black, of the output in red. Note that the sum of the GSO log norms is a constant, so reducing the first vector, increases the (average of the) remaining vectors.</figure></div>



<p>For the analysis we need to know what such an SVP oracle buys us. This is where Minkowski’s theorem comes in: we know that for any <span class="math inline">\(n\)</span>-dimensional lattice <span class="math inline">\(\Lambda\)</span> we have <span class="math inline">\(\lambda_1(\Lambda) \leq \sqrt{\gamma_n} \det(\Lambda)^{1/n}\)</span> (where <span class="math inline">\(\lambda_1(\Lambda)\)</span> is the length of the shortest vector in <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\gamma_n = \Theta(n)\)</span> is Hermite’s constant). This tells us that after we’ve applied the SVP oracle to a projected block <span class="math inline">\({\mathbf{B}}_{[i,i+k-1]}\)</span>, we have <span class="math display">\[\|{\mathbf{b}}^*_i \| \leq \sqrt{\gamma_{k}} \left(\prod_{j = i}^{i+k-1} \|{\mathbf{b}}_j^* \| \right)^{1/k}.\]</span> Almost all of the analyses of block reduction algorithms, at least in terms of their output quality, rely on this single inequality.</p>
<h4 id="disclaimer">Disclaimer</h4>
<p>Before we finally get to talk about BKZ, I want to remark that throughout this series I will punt on a technical (but very important) topic: the number of arithmetic operations (outside of the oracle calls) and the size of the numbers. The number of arithmetic operations is usually not a problem, since it will be dominated by the calls to the SVP oracle. We will only compute projections of sublattices corresponding to projected blocks as described above to pass them to the oracle, which can be done efficiently using the Gram-Schmidt orthogonalization. The size of the numbers is a more delicate issue. We need to ensure that the required precision for these projections does not explode somehow. This is usually addressed by interleaving the calls to the SVP oracle with calls to LLL. If you are familiar with the LLL algorithm, it should be intuitive that this allows to control the size of the number. For a clean example of how this can be handled, we refer to e.g. <span class="citation">[GN08a]</span>. So, in summary, we will measure the running time of our algorithms thoughout simply in the number of calls to the SVP oracle.</p>
<h1 id="sec:bkz">BKZ</h1>
<p>Schnorr <span class="citation">[S87]</span> introduced the concept of BKZ reduction in the 80’s as a generalization of LLL. The first version of the BKZ algorithm as we consider it today was proposed by Schnorr and Euchner <span class="citation">[SE94]</span> a few years later. With our setup above, the algorithm can be described in a very simple way. Let <span class="math inline">\({\mathbf{B}}\)</span> be a lattice basis of an <span class="math inline">\(n\)</span>-dimensional lattice and <span class="math inline">\(k\)</span> be the block size. Recall that this is a parameter that will determine the time/quality trade-off as we shall see in the analysis. We start by calling the SVP oracle on the first block <span class="math inline">\({\mathbf{B}}_{[1,k]}\)</span> of size <span class="math inline">\(k\)</span>. Once this block is SVP reduced, we shift our attention to the next block <span class="math inline">\({\mathbf{B}}_{[2,k+1]}\)</span> and call the oracle on that. Notice that SVP reduction of <span class="math inline">\({\mathbf{B}}_{[2,k+1]}\)</span> may change the lattice generated by <span class="math inline">\({\mathbf{B}}_{[1,k]}\)</span> and <span class="math inline">\({\mathbf{b}}_1\)</span> may not be the shortest vector in the first block anymore, i.e. it can potentially be reduced even further. However, instead of going back and fixing that, we will simply leave this as a problem to “future us”. For now, we continue in this fashion until we reach the end of the basis, i.e. until we called the oracle on <span class="math inline">\({\mathbf{B}}_{n-k,n}\)</span>. Note that so far this can be viewed as considering a constant sized window moving from the start of the basis to the end and reducing the first vector of the projected block in this window as much as possible using the oracle. Once we have reached the end of the basis, we start reducing the window size, i.e. we call the oracle on <span class="math inline">\({\mathbf{B}}_{n-k+1,n}\)</span>, then on <span class="math inline">\({\mathbf{B}}_{n-k+2,n}\)</span>, etc. This whole process is called a <em>BKZ tour</em>.</p>
<p>Now that we have finished a tour, it is time to go back and fix the blocks that are not SVP reduced anymore. We do this simply by running another tour. Again, if the second tour modified the basis, there is no guarantee that all the blocks are SVP redcued. So we simply repeat, and repeat, and … you get the idea. We run as many tours as required until the basis does not change anymore. That’s it. If this looks familiar to you, that’s not a coincidence: if we plug in <span class="math inline">\(k=2\)</span> as our block size, we obtain (a version of) LLL! So BKZ is a proper generalization of LLL.</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-157" src="https://blog.simons.berkeley.edu/wp-content/uploads/2020/04/bkz.png"/>BKZ in one picture: apply the SVP oracle to the projected blocks from start to finish and when you reach the end, repeat.</figure>



<p>The obvious questions now are: what can we expect from the output? And how long does it take?</p>
<h4 id="the-good">The Good</h4>
<p>We will now take a closer look at the approximation factor achieved by BKZ. If you want to follow this analysis along, you might want to get out pen and paper. Otherwise, feel free to trust me on the calculations (I wouldn’t!) and/or jump ahead to the end of this section for the result (no spoilers!). Let’s assume for now that the BKZ algorithm terminates. If it does, we know that the projected block <span class="math inline">\({\mathbf{B}}_{[i, i+k-1]}\)</span> is SVP reduced for every <span class="math inline">\(i \in [1,\dots,n-k+1]\)</span>. This means that we have <span class="math display">\[\|{\mathbf{b}}^*_i \|^k \leq \gamma_{k}^{k/2} \prod_{j = i}^{i+k-1} \|{\mathbf{b}}_j^* \|\]</span> for all these <span class="math inline">\(n-k+1\)</span> values of <span class="math inline">\(i\)</span>. Multiplying all of these inequalities and canceling terms gives the inequality <span class="math display">\[\|{\mathbf{b}}^*_1 \|^{k-1}\|{\mathbf{b}}^*_2 \|^{k-2} \dots \|{\mathbf{b}}^*_{k-1} \| \leq \gamma_{k}^{\frac{(n-k+1)k}{2}} \|{\mathbf{b}}_{n-k+2}^* \|^{k-1} \|{\mathbf{b}}_{n-k+3}^* \|^{k-2} \dots \|{\mathbf{b}}_{n}^* \|.\]</span> Now we make two more observations: 1) not only is <span class="math inline">\({\mathbf{B}}_{[1, k]}\)</span> SVP reduced, but so is <span class="math inline">\({\mathbf{B}}_{[1, i]}\)</span> for every <span class="math inline">\(i &lt; k\)</span>. (Why? Think about it for 2 seconds!) This means we can multiply the inequalities <span class="math display">\[\|{\mathbf{b}}^*_1 \|^i \leq \gamma_{i}^{i/2} \prod_{j = 1}^{i} \|{\mathbf{b}}_j^* \|\]</span> for all <span class="math inline">\(i \in [2,k-1]\)</span> together with the trivial inequality <span class="math inline">\(\|{\mathbf{b}}^*_1 \| \leq \|{\mathbf{b}}^*_1 \|\)</span>, which gives <span class="math display">\[\|{\mathbf{b}}^*_1 \|^{\frac{k(k-1)}{2}} \leq \left(\prod_{i = 2}^{k-1} \gamma_{i}^{i/2} \right) \prod_{i = 1}^{k-1} \|{\mathbf{b}}_i^* \|^{k-1}\]</span> Now we use the fact that <span class="math inline">\(\gamma_k^k \geq \gamma_i^i\)</span> for all <span class="math inline">\(i \leq k\)</span> (Why? Homework!) and combine with our long inequality above to get <span class="math display">\[\|{\mathbf{b}}^*_1 \|^{\frac{k(k-1)}{2}} \leq \gamma_k^{\frac{k(n-1)}{2}} \|{\mathbf{b}}_{n-k+2}^* \|^{k-1} \|{\mathbf{b}}_{n-k+3}^* \|^{k-2} \dots \|{\mathbf{b}}_{n}^* \|.\]</span> (I’m aware that this is a lengthy calculation for a blog post, but we’re almost there, so bear with me. It’s worth it!)</p>
<p>We now use one final observation, which is a pretty common trick in lattice algorithms: w.l.o.g. assume that for some shortest vector <span class="math inline">\({\mathbf{v}}\)</span> in our lattice its projection orthogonal to the first <span class="math inline">\(n-1\)</span> basis vectors is non-zero (if it is zero for all of the shortest vectors, simply drop the last vector from the basis, the result is still BKZ reduced, so use induction). Then we must have that <span class="math inline">\(\lambda_1 = \| {\mathbf{v}} \| \geq \|{\mathbf{b}}_i^* \|\)</span> for all <span class="math inline">\(i \in [n-k+2, \dots, n]\)</span>, since otherwise the projected block <span class="math inline">\({\mathbf{B}}_{i,n}\)</span> would not be SVP reduced. This means, we have <span class="math inline">\(\lambda_1 \geq \max_{i \in [n-k+2, \dots, n]} \|{\mathbf{b}}_i^* \|\)</span>. This is the final puzzle piece to get our approximation bound: <span class="math display">\[\|{\mathbf{b}}^*_1 \| \leq \gamma_{k}^{\frac{n-1}{k-1}} \lambda_1.\]</span> Note that this analysis (dating back to Schnorr <span class="citation">[S94]</span>) is reminiscent of the analysis of LLL and if we plug in <span class="math inline">\(k=2\)</span>, we get exactly what we’d expect from LLL. Though we do note a gap in the other extreme: if we plug in <span class="math inline">\(k=n\)</span>, we know that the approximation factor is <span class="math inline">\(1\)</span> (we are solving SVP in the entire lattice), but the bound above yields a factor <span class="math inline">\(\gamma_n = \Theta(n)\)</span>.</p>
<h4 id="the-bad">The Bad</h4>
<p>Now that we’ve looked at the output quality of the basis, let’s see what we can say about the running time (recall that our focus is on the number of calls to the SVP oracle). The short answer is: not much and that’s very unfortunate. Ideally, we’d want a bound on the number of SVP calls that is polynomial in <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span>. This would mean that the overall running time for large <span class="math inline">\(k\)</span> is dominated by the running time of the SVP oracle in dimension <span class="math inline">\(k\)</span> and the block size would give us exactly the expected trade-off. However, an LLL style analysis has so far only yielded a bound on the number of tours which is <span class="math inline">\(O(k^n)\)</span> <span class="citation">[HPS11, Appendix]</span>. This is quite bad – for large <span class="math inline">\(k\)</span> the number of calls will be the dominating factor in the running time.</p>
<h4 id="the-ugly">The Ugly</h4>
<p>Recall that the analysis of LLL does not only provide a bound on the approximation factor, but also on the Hermite factor, i.e. on the ratio of <span class="math inline">\(\| {\mathbf{b}}_1\|/\det(\Lambda)^{1/n}\)</span>. Since an LLL-style analysis worked out nicely for the approximation factor of BKZ, it stands to reason that a similar analysis should yield a similar bound for BKZ. By extrapolating from LLL, one could expect a bound along the lines of <span class="math inline">\(\| {\mathbf{b}}_1\|/\det(\Lambda)^{1/n} \leq \gamma_{k}^{n/2k}\)</span> (note the square root improvement w.r.t. the trivial bound obtained from the approximation factor). And, in fact, a bound of <span class="math inline">\(\gamma_{k}^{\frac{n-1}{2(k-1)} + 1}\)</span> has been claimed in <span class="citation">[GN08b]</span> but without proof (as pointed out in <span class="citation">[HPS11]</span>) and it is not clear, how one would prove this. (<span class="citation">[GN08b]</span> claims that one can use a similar argument as we did for the approximation factor, but I don’t see it.)</p>
<h4 id="the-rescue">The Rescue</h4>
<p>So it seems different techniques are necessary to complete the analysis of BKZ. The work of <span class="citation">[HPS11]</span> introduced such a new technique based on the analysis of dynamical systems. This work applied the technique successfully to BKZ, but the analysis is quite involved. What it shows is that one can terminate BKZ after a polynomial number of tours and still get a guarantee on the output quality, which is very close to the conjectured bound on the Hermite factor above. (Caveat: Technically, <span class="citation">[HPS11]</span> only showed this result for a slight variant of BKZ, but the difference to the standard BKZ algorithm only lies in the scope of the interleaving LLL applications, which is something that we glossed over above.) This is in line with experimental studies <span class="citation">[SE94,GN08b,MW16]</span>, which show that BKZ produces high quality bases after a few tours already.</p>
<p>We will revisit this approach when considering a different block reduction variant, SDBKZ, where the analysis is much cleaner. As a teaser for the next post though, recall that BKZ can be viewed as a generalization of LLL (which corresponds to BKZ with block size <span class="math inline">\(k=2\)</span>). Since the analysis of LLL did not carry entirely to BKZ, one could wonder if there is a different generalization of LLL such that an LLL-style analysis also generalizes naturally. The answer to this is yes, and we will consider such an algorithm in the next post.</p>



<ul><li>[CDPR16] Cramer, Ducas, Peikert, Regev. <em>Recovering short generators of principal ideals in cyclotomic rings.</em> EUROCRYPT 2016</li><li>[GN08a] Gama, Nguyen. <em>Finding short lattice vectors within Mordell’s inequality</em>. STOC 2008</li><li>[GN08b] Gama, Nguyen. <em>Predicting lattice reduction</em>. EUROCRYPT 2008</li><li>[HPS11] Hanrot, Pujol, Stehlé.<em> Analyzing blockwise lattice algorithms using dynamical systems.</em> CRYPTO 2011</li><li>[MW16] Micciancio, Walter. <em>Practical, predictable lattice basis reduction.</em> EUROCRYPT 2016</li><li>[SE94] Schnorr, Euchner. <em>Lattice basis reduction: Improved practical algorithms and solving subset sum problems.</em> Mathematical Programming 1994</li><li>[S87] Schnorr. <em>A hierarchy of polynomial time lattice basis reduction algorithms.</em> Theoretical Computer Science 1987</li><li>[S94] Schnorr. <em>Block reduced lattice bases and successive minima.</em> Combinatorics, Probability and Computing 1994</li></ul></div>
    </content>
    <updated>2020-04-01T15:17:39Z</updated>
    <published>2020-04-01T15:17:39Z</published>
    <category term="General"/>
    <category term="BKZ"/>
    <category term="Lattice Block Reduction"/>
    <author>
      <name>Michael Walter</name>
    </author>
    <source>
      <id>https://blog.simons.berkeley.edu</id>
      <link href="https://blog.simons.berkeley.edu/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blog.simons.berkeley.edu" rel="alternate" type="text/html"/>
      <subtitle>What's New at the Simons Institute for the Theory of Computing.</subtitle>
      <title>Calvin Café: The Simons Institute Blog</title>
      <updated>2020-04-08T23:41:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16904</id>
    <link href="https://rjlipton.wordpress.com/2020/04/01/research-at-home/" rel="alternate" type="text/html"/>
    <title>Research at Home</title>
    <summary>An idea for human-interest interviews Pixabay free src Dr. Lofa Polir is, like many of us, working from home. When we last wrote about her two years ago, she had started work for the Livingston, Louisiana branch of LIGO. They sent her and the rest of the staff home on March 19 and suspended observations […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>An idea for human-interest interviews</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/01/research-at-home/femalefool/" rel="attachment wp-att-16906"><img alt="" class="alignright wp-image-16906" height="213" src="https://rjlipton.files.wordpress.com/2020/03/femalefool.png?w=137&amp;h=213" width="137"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Pixabay free <a href="https://pixabay.com/illustrations/toon-figure-female-fool-funny-4292442/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Dr. Lofa Polir is, like many of us, working from home. When we last <a href="https://rjlipton.wordpress.com/2018/04/01/the-entropy-of-baseball/">wrote</a> about her two years ago, she had started work for the Livingston, Louisiana branch of <a href="https://www.ligo.caltech.edu/page/about">LIGO</a>. They sent her and the rest of the staff home on March 19 and <a href="https://www.ligo.caltech.edu/news/ligo20200326">suspended</a> observations on the 26th. Since Polir’s duties already included public outreach, she is looking to continue that online.</p>
<p>
Today we helped Dr. Polir interview another pandemic-affected researcher.</p>
<p>
We liked her idea of interviewing young people just starting their careers, who are facing unexpected uncertainties. Her first choice was a new graduate of Cambridge University doing fundamental work related to LIGO. Unfortunately, he had been unable to install a current version of Zoom on his handheld device, or maybe afraid owing to security <a href="https://twitter.com/random_walker/status/1244987617676050434?s=11">issues</a>. So she requested the special equipment we have used to <a href="https://rjlipton.wordpress.com/2011/10/31/an-interview-with-kurt-gdel/">interview</a> people <a href="https://rjlipton.wordpress.com/2012/11/03/more-interview-with-kurt-godel/">in</a> the <a href="https://rjlipton.wordpress.com/2013/11/15/the-graph-of-math/">past</a>. </p>
<p>
He replied at the speed of light that he was willing to do the interview so long as we respected some privacy measures. As for what name to use, he said we could just call him Izzy—Izzy Jr., in fact. So Dick, I, and Dr. Polir all used our own Zoom to port into our machine’s console room. The connection worked right away as Izzy’s head glimmered into view.</p>
<p>
</p><p/><h2> Starting The Interview </h2><p/>
<p/><p>
At first glimpse, all we could see was his long, light-brown hippie hair. This really surprised us—not the image we had of Cambridge—and we gasped about it before even saying hello. He replied that it was fashion from the Sixties. We asked how his family was doing and he said his fathers had passed on but mother and young siblings were at home and fine. We think he said “fathers” plural—the machine rendered him in a drawl like Mick Jagger and he was hard to follow.</p>
<p>
Izzy picked up on our discomfort and immediately assured us he hadn’t been doing any drugs: “You can’t get them anyway because they’re all being diverted to treat the sick.” But he did open up to us that he was in some kind of withdrawal. He confessed that he had resorted to looking at the sun with one eye. “It was ecstasy but bad—I still can see only reds and blues with that eye, and I need to use an extra-large rectangular cursor to read text.” We were curious what brand of handheld device he was using because of his problems with Zoom, and he told us it was a Napier 1660 by <a href="http://www.oughtred.org/history.shtml">Oughtred, Ltd.</a> We hadn’t heard of that model but he said he’d connected three of them into a good home lab setup.</p>
<p>
We asked how he was coping with distance teaching, but he said he hadn’t yet started his faculty position at Trinity College. We were surprised to learn that lecture attendance at Cambridge University is optional. “I shall be required to give the lectures but nobody will come to them so that’s all the same now—at least here I’ll have a cat for audience. No dogs and not my mother or siblings—I’d sooner burn the house down.” He quickly added, “Oh, my mother and I get along fine now and I love playing teatime with my little sisters.”</p>
<p>
We really didn’t want to go into Izzy’s personal life, and I tried to shift the small-talk by noting a little chess set on a shelf behind him. He snapped that he shouldn’t have spent money on it and he was a poor player anyway. We thought, wow, either this guy’s really down on himself or the cabin fever of the pandemic is getting to him. So Dick, always quick to pick up on things and find ways of encouragement, said:</p>
<p>
“Dr. Polir here works on gravity and we’re told you have some great new ideas about it. We’d love to hear them.”</p>
<p>
“Yes, I do—or did. But something happened yesterday that is making me realize that it’s all wrong, rubbish really…”</p>
<p>
</p><p/><h2> In the Garden </h2><p/>
<p/><p>
Izzy started by explaining that it’s a basic principle of alchemy that all objects have humors that can manifest as kinds of magnetism. (“Alchemy”? did we hear him right?) If you realize that the Earth and Sun are objects just like any other then you can model gravity that way. You just need to assign each object a number called its “mass” and then you get the equation </p>
<p align="center"><img alt="\displaystyle  F = G \frac{m_1 m_2}{r^2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F+%3D+G+%5Cfrac%7Bm_1+m_2%7D%7Br%5E2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F = G \frac{m_1 m_2}{r^2} "/></p>
<p>for the force of attraction, where <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> is the distance between the objects with masses <img alt="{m_1,m_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%2Cm_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1,m_2}"/> and <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is a constant that depends on your units.</p>
<p>
“We understand all that,” said Dr. Polir.</p>
<p>
Izzy said the point is that <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> depends <b>only</b> on your units and is the same regardless of where you are on Earth or on the Moon or wherever. It is very small, though. Then he went into his story of yesterday.</p>
<blockquote><p><b> </b> <em> “I was in our garden by the path to the neighbor’s farm. I was supposed to be watching my little brother Benjamin who wanted to help harvest squash but I hate farming so I let him go without me. I was lying under an apple tree for shade when an apple fell and I realized all my mistakes.” </em>
</p></blockquote>
<p/><p>
“What?,” we thought silently. We didn’t need to speak up—Izzy launched right into his litany of error:</p>
<blockquote><p><b> </b> <em> “First, I’d thought the force was in what made the apple fall, but that’s nonsense. The apple would fall naturally because down is the shortest path it would be on if the tree branch were not holding it back. The only force is the tensile strength of the branch which was restraining it. I think that the tensile force really is magnetism, by the way.”</em></p><em>
<p>
“Second, it’s ridiculous to think the force is coming <b>from</b> the Earth. On first principles, it could come from the ground, but that’s not what the equations say. They could have it all coming from one point in the <b>center</b> of the Earth. Just one point—four thousand miles deep!”</p>
</em><p><em>
“Third and worst, though, is when you apply it to the Sun and the Earth. My equation means they are exerting force on each other instantaneously. But they are millions of miles apart. Whereas, the tree was <b>touching</b> the apple. Force can work only by touch, not by some kind of spooky action at a distance.” </em>
</p></blockquote>
<p/><p>
We realized what he was driving at. Dick again always likes to encourage, so he said:</p>
<p>
“But the math you developed for this force theory—surely it is good for calculations…?”</p>
<blockquote><p><b> </b> <em> “No it’s not—it’s the Devil’s own box. I can calculate two bodies—the Earth and the Sun, or the Moon and the Earth if you suppose there is no Sun, but as soon as you have all three bodies it’s a bog. Worst of all, I can arrange five bodies so that one of them gets accelerated to infinite velocity—<a href="https://rjlipton.wordpress.com/2019/10/31/hobgoblins-in-our-equations/">in finite time</a>. This is a clear impossibility, a contradiction, so by <em>modus tollens</em>… it can all go in the bin.” </em>
</p></blockquote>
<p/><p>
We didn’t think it would help to tell him that his math was good enough to <a href="https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19720022040.pdf">calculate</a> a Moon landing but not to locate a friend’s house while driving. He supplied his own <em>coup-de-grâce</em> anyway:</p>
<blockquote><p><b> </b> <em> “And even the two-body calculations are tainted. I can calculate the orbits of the planets but the equations I get aren’t stable. I would wind up having to postulate something like God keeps the planets on their tracks. Yes, you need an intelligent Agent to start the planets going—all in one plane, basically—but to need such intervention all the time defeats the point of having equations.” </em>
</p></blockquote>
<p>
</p><p/><h2> Inklings </h2><p/>
<p/><p>
We asked Izzy what he was going to do. He said that the one blessing of enforced solitude is that one gets time to reflect on things and deepen the foundations. And he said he’d had an idea later that afternoon.</p>
<blockquote><p><b> </b> <em> “Toward supper I realized I needed to get Benjamin home. The path to the farm is straight except it goes over a mound. I was sauntering along and when I got to the hill I realized that if I didn’t watch it I’d have fallen right into it. So that got me thinking. First, what I thought was straight on the path was really a curve—the Earth is after all a ball. We think space is straight, but maybe it too is curved. So when I’m standing here, perhaps I would really be moving in a diagonally down direction, but the Earth is stopping me. The Irish blessing says, ‘may the road rise up to meet you.’ Perhaps it does.” </em>
</p></blockquote>
<p/><p>
“So are you doing math to work that out?,” I ventured.</p>
<blockquote><p><b> </b> <em> “I started after supper. One good thing is that it allows light to be affected by gravity—which I was already convinced of—even if light has no mass. But a problem is that it appears Time would have to be included as curved. That does not make sense either.” </em>
</p></blockquote>
<p/><p>
We asked when he might write up all this. He said he didn’t want to be quick to publish something so flawed on the one hand, or incomplete on the other, “unless someone else be about to publish the same.” We noted that there weren’t going to be any in-person conferences to present papers at for awhile anyway.</p>
<blockquote><p><b> </b> <em> “Besides, that’s not what I’m most eager to do. What the respite is really giving me time for is to start writing up my work on Theology. That’s most important—it could have stopped thirty years of war. For one thing, <a href="https://en.wikipedia.org/wiki/Homoiousios">homoiousios</a>, not <a href="https://en.wikipedia.org/wiki/Homoousios">homoousios</a>, is the right rendering. There will be a time and times and the dividing of times in under 400 years <a href="https://www.questia.com/library/journal/1G1-116141910/a-time-and-times-and-the-dividing-of-time-isaac">anyway</a>.” </em>
</p></blockquote>
<p/><p>
That last statement somehow did not reassure us. We thanked Izzy Jr. for the interview and he gave consent to publish it posthumously.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We hope that your April Fool’s Day is such as to allow a time to laugh. But also seriously, would you be interested in the idea of our interviewing people during these times? Is there anyone you would like to suggest?</p>
<p/></font></font></div>
    </content>
    <updated>2020-04-01T05:10:02Z</updated>
    <published>2020-04-01T05:10:02Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Teaching"/>
    <category term="April Fool"/>
    <category term="gravity"/>
    <category term="interview"/>
    <category term="Lofa Polir"/>
    <category term="outreach"/>
    <category term="pandemic"/>
    <category term="Physics"/>
    <category term="working from home"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-04-09T06:20:44Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/03/31/linkage</id>
    <link href="https://11011110.github.io/blog/2020/03/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Still at home, hoping the coffee arrives tomorrow as scheduled.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Still at home, hoping the coffee arrives tomorrow as scheduled.</p>

<ul>
  <li>
    <p><a href="https://twitter.com/scienceshitpost">Science diagrams that look like shitposts</a> (<a href="https://mathstodon.xyz/@11011110/103834852903279134"/>, <a href="https://www.metafilter.com/186088/Science-diagrams-that-look-like-shitposts">via</a>).</p>
  </li>
  <li>
    <p><a href="https://www.cambridge.org/core/what-we-publish/textbooks">All Cambridge University Press textbooks are free-to-read until May</a> (<a href="https://mathstodon.xyz/@JordiGH/103841002082854377"/>).</p>
  </li>
  <li>
    <p><a href="https://www.rweber.net/projects/non-gray-grayscales/">Non-gray grayscales</a> (<a href="https://mathstodon.xyz/@11011110/103848490584549963"/>). Rebecca Weber finds a method to produce off-gray colors in a range of lightness with visually-matching hues and saturations. It’s not just a matter of plotting a straight line in HSL colorspace. (Found while looking for more information on her book <em>Computability Theory</em>, but nowadays there isn’t much actual math on her website.)</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.08456">Convex hulls of random order types</a> (<a href="https://mathstodon.xyz/@11011110/103853512504928724"/>). This is one of my favorite papers in the list accepted to SoCG 2020, which now will be online-only. Point sets whose order type is uniformly random are different from randomly drawn points, and harder to study. Xavier Goaoc and Emo Welzl observe that the projective transformations of a random order type are equally likely, and use this idea to prove that these point sets typically have very small convex hulls.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Greedy_coloring">Greedy coloring</a> (<a href="https://mathstodon.xyz/@11011110/103856462402785120"/>). Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.theguardian.com/science/2019/oct/21/can-you-solve-it-the-four-points-two-distances-problem">The four points, two distances problem</a> (<a href="https://mathstodon.xyz/@11011110/103864906436854951"/>). Can you find all of the ways of arranging four distinct points in the plane so that they form only two distances? The link is not a spoiler but it has a separate link to the solution. “Nearly everyone misses at least one” says Peter Winkler; can you guess the one I missed?</p>
  </li>
  <li>
    <p><a href="https://mastodon.social/@sarielhp/103853624792571796">Sariel Har-Peled makes some suggestions for online conferences</a>: all papers above threshold should be accepted, rather than imposing artificial acceptance rates, and authors should provide versions of their talks in multiple lengths.</p>
  </li>
  <li>
    <p><a href="https://community.wolfram.com/groups/-/m/t/1904335">Minimal-stick examples of the knots , , , , and </a> (<a href="https://mathstodon.xyz/@shonk/103868182994018206"/>).</p>
  </li>
  <li>
    <p>Two linocut interpretations of a rhombic dodecahedron by the same artist, Josh Millard: <a href="https://mastodon.social/@joshmillard/103876129272051551">abstract</a> and <a href="https://mastodon.social/@joshmillard/103881150094999086">physical</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/103887968806441789"/>).</span></p>
  </li>
  <li>
    <p>Three recent “Did you know?” (<a href="https://mathstodon.xyz/@11011110/103893665729668173"/>):</p>

    <ul>
      <li>
        <p>… that <a href="https://en.wikipedia.org/wiki/Chiara_Daraio">Chiara Daraio</a> used Newton’s cradle to create sound bullets, and ball bearing filled walls to create one-way sound barriers?</p>
      </li>
      <li>
        <p>… that <a href="https://en.wikipedia.org/wiki/Heronian_tetrahedron">a tetrahedron with integer edge lengths, face areas, and volume</a> can be given integer coordinates?</p>
      </li>
      <li>
        <p>… that former college basketball star <a href="https://en.wikipedia.org/wiki/Amy_Langville">Amy Langville</a> is an expert in ranking systems, and has applied her ranking expertise to basketball bracketology?</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="http://www.mi.sanu.ac.rs/vismath/mart.htm">VisMath MathArt</a> (<a href="https://mathstodon.xyz/@11011110/103898916402221482"/>). Many linked galleries of images of mathematical art, from the 1990s-style web (occasional broken links and all).</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2020/03/robin-thomas.html">Sadly, graph theorist Robin Thomas has died</a> (<a href="https://mathstodon.xyz/@11011110/103901957462987629"/>).</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=IK7nBOLYzdE">What happens when half a cellular automaton runs Conway’s Game of Life and the other half runs a rolling version of Rule 30 pushing chaos across the border</a> (<a href="https://mathstodon.xyz/@11011110/103915202241542224"/>)? I wish I could see a larger scale of time and space to get an idea of how far the effects penetrate. If the boundary emitted gliders at a constant rate they’d collide far away in a form of ballistic annihilation but the boundary junk and glider-collision junk makes it more complicated.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/356220/440">Monotone subsets of uncountable plane sets</a> (<a href="https://mathstodon.xyz/@11011110/103919594138003386"/>). I ask on MathOverflow about infinite generalizations of the Erdős–Szekeres theorem on the existence of square-root-sized monotone subsets of finite sets of points in the plane.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-03-31T21:25:00Z</updated>
    <published>2020-03-31T21:25:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-04-01T04:49:01Z</updated>
    </source>
  </entry>
</feed>
