<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-11-12T04:22:16Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/11/tenure-track-faculty-at-stanford-university-apply-by-november-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/11/tenure-track-faculty-at-stanford-university-apply-by-november-15-2021/" rel="alternate" type="text/html"/>
    <title>Tenure-track Faculty at Stanford University (apply by November 15, 2021)</title>
    <summary>Stanford Data Science and the School of Engineering invite applications for a tenure-track appointment at the assistant professor or untenured associate professor level. We welcome candidates engaged in all aspects of data science and applications in one or more engineering disciplines. Application review begins Nov 15, 2021; search will remain open until the position is […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Stanford Data Science and the School of Engineering invite applications for a tenure-track appointment at the assistant professor or untenured associate professor level. We welcome candidates engaged in all aspects of data science and applications in one or more engineering disciplines.</p>
<p>Application review begins Nov 15, 2021; search will remain open until the position is filled.</p>
<p>Website: <a href="https://datascience.stanford.edu/tenure-track-faculty-position-soe">https://datascience.stanford.edu/tenure-track-faculty-position-soe</a><br/>
Email: SDSSoEfacultysearch@stanford.edu</p></div>
    </content>
    <updated>2021-11-11T17:15:22Z</updated>
    <published>2021-11-11T17:15:22Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-12T04:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/11/postdoc-at-northwestern-and-ttic-apply-by-january-1-2022/</id>
    <link href="https://cstheory-jobs.org/2021/11/11/postdoc-at-northwestern-and-ttic-apply-by-january-1-2022/" rel="alternate" type="text/html"/>
    <title>Postdoc at Northwestern and TTIC (apply by January 1, 2022)</title>
    <summary>Northwestern University and Toyota Technology Institute at Chicago invite applications for a postdoctoral position. The postdoc will conduct research in approximation algorithms and beyond-worst-case analysis of algorithms, working with Konstantin Makarychev and Yury Makarychev. The position will be based at Northwestern University, but the postdoc will be encouraged to spend some time at TTIC. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Northwestern University and Toyota Technology Institute at Chicago invite applications for a postdoctoral position. The postdoc will conduct research in approximation algorithms and beyond-worst-case analysis of algorithms, working with Konstantin Makarychev and Yury Makarychev. The position will be based at Northwestern University, but the postdoc will be encouraged to spend some time at TTIC.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/20466">https://academicjobsonline.org/ajo/jobs/20466</a><br/>
Email: yury@ttic.edu</p></div>
    </content>
    <updated>2021-11-11T15:12:06Z</updated>
    <published>2021-11-11T15:12:06Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-12T04:20:45Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2727537320196855723</id>
    <link href="http://blog.computationalcomplexity.org/feeds/2727537320196855723/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/20-years-of-algorithmic-game-theory.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/2727537320196855723" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/2727537320196855723" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/20-years-of-algorithmic-game-theory.html" rel="alternate" type="text/html"/>
    <title>20 Years of Algorithmic Game Theory</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Twenty years ago DIMACS hosted a <a href="http://dimacs.rutgers.edu/archive/Workshops/gametheory/program.html">Workshop on Computational Issues in Game Theory and Mechanism Design</a>. This wasn't the very beginning of algorithmic game theory, but it was quite the coming out party. From the <a href="http://dimacs.rutgers.edu/archive/Workshops/gametheory/announcement.html">announcement</a></p><p/><blockquote><p>The research agenda of computer science is undergoing significant changes due to the influence of the Internet. Together with the emergence of a host of new computational issues in mathematical economics, as well as electronic commerce, a new research agenda appears to be emerging. This area of research is collectively labeled under various titles, such as "Foundations of Electronic Commerce", Computational Economics", or "Economic Mechanisms in Computation" and deals with various issues involving the interplay between computation, game-theory and economics.</p><p>This workshop is intended to not only summarize progress in this area and attempt to define future directions for it, but also to help the interested but uninitiated, of which there seem many, understand the language, the basis principles and the major issues.</p><p/></blockquote><p>Working at the nearby NEC Research Institute at the time I attended as one of those "interested but unititated."</p><p>The workshop had talks from the current and rising stars in the field in both the theoretical computer science, AI and economics communities. The presentations included some classic early results including <a href="https://doi.org/10.1016/S0304-3975(03)00391-8">Competitive Analysis of Incentive Compatible Online Auctions</a>, <a href="https://doi.org/10.1145/506147.506153">How Bad is Selfish Routing?</a> and the seminal work on <a href="https://doi.org/10.1016/j.geb.2006.02.003">Competitive Auctions</a>. </p><p>Beyond the talks, just having the powerhouse of people at the meeting, established players, like Noam Nisan, Vijay Vazirani, Eva Tardos and Christos Papadimitriou, with several newcomers who are now the established players including Tim Roughgarden and Jason Hartline just to mention a few from theoretical computer science. </p><p>The highlight was a panel discussion on how to overcome the methodological differences between computer scientists and economic game theorists. The panelists were an all-star collection of  John Nash, Andrew Odlyzko, Christos Papadimitriou, Mark Satterthwaite, Scott Shenker and Michael Wellman. The discussion focused on things like competitive analysis though to me, in hindsight, the real difference is between the focus on models (game theory) vs theorems (CS). </p><div>Interest in these connections exploded after the workshop and a new field blossomed.</div></div>
    </content>
    <updated>2021-11-11T12:57:00Z</updated>
    <published>2021-11-11T12:57:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-11-11T15:44:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05800</id>
    <link href="http://arxiv.org/abs/2111.05800" rel="alternate" type="text/html"/>
    <title>Arbitrary order principal directions and how to compute them</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Digne:Julie.html">Julie Digne</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Valette:S=eacute=bastien.html">Sébastien Valette</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chaine:Rapha=euml=lle.html">Raphaëlle Chaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=eacute=arzi:Yohann.html">Yohann Béarzi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05800">PDF</a><br/><b>Abstract: </b>Curvature principal directions on geometric surfaces are a ubiquitous concept
of Geometry Processing techniques. However they only account for order 2
differential quantities, oblivious of higher order differential behaviors. In
this paper, we extend the concept of principal directions to higher orders for
surfaces in R^3 by considering symmetric differential tensors. We further show
how they can be explicitly approximated on point set surfaces and that they
convey valuable geometric information, that can help the analysis of 3D
surfaces.
</p></div>
    </summary>
    <updated>2021-11-11T23:06:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05780</id>
    <link href="http://arxiv.org/abs/2111.05780" rel="alternate" type="text/html"/>
    <title>Approximating bottleneck spanning trees on partitioned tuples of points</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biniaz:Ahmad.html">Ahmad Biniaz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maheshwari:Anil.html">Anil Maheshwari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smid:Michiel.html">Michiel Smid</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05780">PDF</a><br/><b>Abstract: </b>We present approximation algorithms for the following NP-hard optimization
problems related to bottleneck spanning trees in metric spaces.
</p>
<p>1. The disjoint bottleneck spanning tree problem: Given $n$ pairs of points
in a metric space, find two disjoint trees each containing exactly one point
from each pair and minimize the largest edge length (over all edges of both
trees). It is known that approximating this problem by a factor better than 2
is NP-hard. We present a 4-approximation algorithm for this problem. This
improves upon the previous best known approximation ratio of $9$. Our algorithm
extends to a $(3k-2)$-approximation for a more general case where points are
partitioned into $k$-tuples and we seek $k$ disjoint trees.
</p>
<p>2. The generalized bottleneck spanning tree problem: Given $n$ points in some
metric space that are partitioned into clusters of size at most 2, find a tree
that contains exactly one point from each cluster and minimizes the largest
edge length. We show that it is NP-hard to approximate this problem by a factor
better than 2, and present a 3-approximation algorithm.
</p>
<p>3. The partitioned bottleneck spanning tree problem: Given $kn$ points in
some metric space, find $k$ trees each containing exactly $n$ points and
minimize the largest edge length (over all edges of the $k$ trees). We show
that it is NP-hard to approximate this problem by a factor better than 2 for
any $k\ge 2$. We present an $\alpha$-approximation algorithm for this problem
where $\alpha=2$ for $k=2,3$ and $\alpha=3$ for $k\ge 4$. Towards obtaining
these approximation ratios we present tight upper bounds on the edge lengths of
$k$ equal-size disjoint trees that can be obtained from the nodes of a given
tree. This result is of independent interest.
</p>
<p>Our hardness proofs imply that it is NP-hard to approximate the non-metric
version of the above problems within any constant factor.
</p></div>
    </summary>
    <updated>2021-11-11T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05778</id>
    <link href="http://arxiv.org/abs/2111.05778" rel="alternate" type="text/html"/>
    <title>Theoretical and empirical analysis of a fast algorithm for extracting polygons from signed distance bounds</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Nenad Markuš <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05778">PDF</a><br/><b>Abstract: </b>We investigate an asymptotically fast method for transforming signed distance
bounds into polygon meshes. This is achieved by combining sphere tracing (also
known as ray marching) and one of the traditional polygonization schemes (e.g.,
Marching cubes). Let us call this approach Gridhopping. We provide theoretical
and experimental evidence that it is of the $O(N^2\log N)$ computational
complexity for a polygonization grid with $N^3$ cells. The algorithm is tested
on both a set of primitive shapes as well as signed distance fields generated
from point clouds by machine learning. Given its speed, simplicity and
portability, we argue that it could prove useful during the modelling stage as
well as in shape compression for storage.
</p>
<p>The code is available here: https://github.com/nenadmarkus/gridhopping
</p></div>
    </summary>
    <updated>2021-11-11T23:00:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05687</id>
    <link href="http://arxiv.org/abs/2111.05687" rel="alternate" type="text/html"/>
    <title>Non-Adaptive Stochastic Score Classification and Explainable Halfspace Evaluation</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghuge:Rohan.html">Rohan Ghuge</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nagarajan:Viswanath.html">Viswanath Nagarajan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05687">PDF</a><br/><b>Abstract: </b>We consider the stochastic score classification problem. There are several
binary tests, where each test $i$ is associated with a probability $p_i$ of
being positive and a cost $c_i$. The score of an outcome is a weighted sum of
all positive tests, and the range of possible scores is partitioned into
intervals corresponding to different classes. The goal is to perform tests
sequentially (and possibly adaptively) so as to identify the class at the
minimum expected cost. We provide the first constant-factor approximation
algorithm for this problem, which improves over the previously-known
logarithmic approximation ratio. Moreover, our algorithm is $non$ $adaptive$:
it just involves performing tests in a $fixed$ order until the class is
identified. Our approach also extends to the $d$-dimensional score
classification problem and the "explainable" stochastic halfspace evaluation
problem (where we want to evaluate some function on $d$ halfspaces). We obtain
an $O(d^2\log d)$-approximation algorithm for both these extensions. Finally,
we perform computational experiments that demonstrate the practical performance
of our algorithm for score classification. We observe that, for most instances,
the cost of our algorithm is within $50\%$ of an information-theoretic lower
bound on the optimal value.
</p></div>
    </summary>
    <updated>2021-11-11T22:54:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05663</id>
    <link href="http://arxiv.org/abs/2111.05663" rel="alternate" type="text/html"/>
    <title>The Impact of Changes in Resolution on the Persistent Homology of Images</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heiss:Teresa.html">Teresa Heiss</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tymochko:Sarah.html">Sarah Tymochko</a>, Brittany Story, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garin:Ad=eacute=lie.html">Adélie Garin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bui:Hoa.html">Hoa Bui</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bleile:Bea.html">Bea Bleile</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Robins:Vanessa.html">Vanessa Robins</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05663">PDF</a><br/><b>Abstract: </b>Digital images enable quantitative analysis of material properties at micro
and macro length scales, but choosing an appropriate resolution when acquiring
the image is challenging. A high resolution means longer image acquisition and
larger data requirements for a given sample, but if the resolution is too low,
significant information may be lost. This paper studies the impact of changes
in resolution on persistent homology, a tool from topological data analysis
that provides a signature of structure in an image across all length scales.
Given prior information about a function, the geometry of an object, or its
density distribution at a given resolution, we provide methods to select the
coarsest resolution yielding results within an acceptable tolerance. We present
numerical case studies for an illustrative synthetic example and samples from
porous materials where the theoretical bounds are unknown.
</p></div>
    </summary>
    <updated>2021-11-11T23:00:05Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05578</id>
    <link href="http://arxiv.org/abs/2111.05578" rel="alternate" type="text/html"/>
    <title>Conversational Recommendation:Theoretical Model and Complexity Analysis</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Noia:Tommaso_Di.html">Tommaso Di Noia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Donini:Francesco.html">Francesco Donini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jannach:Dietmar.html">Dietmar Jannach</a>, FedelucioNarducci, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pomo:Claudio.html">Claudio Pomo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05578">PDF</a><br/><b>Abstract: </b>Recommender systems are software applications that help users find items of
interest in situations of information overload in a personalized way, using
knowledge about the needs and preferences of individual users. In
conversational recommendation approaches, these needs and preferences are
acquired by the system in an interactive, multi-turn dialog. A common approach
in the literature to drive such dialogs is to incrementally ask users about
their preferences regarding desired and undesired item features or regarding
individual items. A central research goal in this context is efficiency,
evaluated with respect to the number of required interactions until a
satisfying item is found. This is usually accomplished by making inferences
about the best next question to ask to the user. Today, research on dialog
efficiency is almost entirely empirical, aiming to demonstrate, for example,
that one strategy for selecting questions is better than another one in a given
application. With this work, we complement empirical research with a
theoretical, domain-independent model of conversational recommendation. This
model, which is designed to cover a range of application scenarios, allows us
to investigate the efficiency of conversational approaches in a formal way, in
particular with respect to the computational complexity of devising optimal
interaction strategies. Through such a theoretical analysis we show that
finding an efficient conversational strategy is NP-hard, and in PSPACE in
general, but for particular kinds of catalogs the upper bound lowers to
POLYLOGSPACE. From a practical point of view, this result implies that catalog
characteristics can strongly influence the efficiency of individual
conversational strategies and should therefore be considered when designing new
strategies. A preliminary empirical analysis on datasets derived from a
real-world one aligns with our findings.
</p></div>
    </summary>
    <updated>2021-11-11T22:38:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05553</id>
    <link href="http://arxiv.org/abs/2111.05553" rel="alternate" type="text/html"/>
    <title>Matrix anti-concentration inequalities with applications</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nie:Zipei.html">Zipei Nie</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05553">PDF</a><br/><b>Abstract: </b>We provide a polynomial lower bound on the minimum singular value of an
$m\times m$ random matrix $M$ with jointly Gaussian entries, under a polynomial
bound on the matrix norm and a global small-ball probability bound
$$\inf_{x,y\in S^{m-1}}\mathbb{P}\left(\left|x^* M y\right|&gt;m^{-O(1)}\right)\ge
\frac{1}{2}.$$ With the additional assumption that $M$ is self-adjoint, the
global small-ball probability bound can be replaced with a weaker version.
</p>
<p>We establish two matrix anti-concentration inequalities, which lower bound
the minimum singular values of the sum of independent positive semidefinite
self-adjoint matrices and the linear combination of independent random matrices
with independent Gaussian coefficients. Both are under a global small-ball
probability assumption. As a major application, we prove a better singular
value bound for the Krylov space matrix, which leads to a faster and simpler
algorithm for solving sparse linear systems. Our algorithm runs in
$\tilde{O}\left(n^{\frac{3\omega-4}{\omega-1}}\right)=O(n^{2.27159})$ time,
improving on the previous fastest one in
$\tilde{O}\left(n^{\frac{5\omega-4}{\omega+1}}\right)=O(n^{2.33165})$ time by
Peng and Vempala.
</p></div>
    </summary>
    <updated>2021-11-11T22:43:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05545</id>
    <link href="http://arxiv.org/abs/2111.05545" rel="alternate" type="text/html"/>
    <title>Defensive Alliances in Graphs</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gaikwad:Ajinkya.html">Ajinkya Gaikwad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maity:Soumen.html">Soumen Maity</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05545">PDF</a><br/><b>Abstract: </b>A set $S$ of vertices of a graph is a defensive alliance if, for each element
of $S$, the majority of its neighbours are in $S$. We study the parameterized
complexity of the Defensive Alliance problem, where the aim is to find a
minimum size defensive alliance. Our main results are the following: (1) The
Defensive Alliance problem has been studied extensively during the last twenty
years, but the question whether it is FPT when parameterized by feedback vertex
set has still remained open. We prove that the problem is W[1]-hard
parameterized by a wide range of fairly restrictive structural parameters such
as the feedback vertex set number, treewidth, pathwidth, and treedepth of the
input graph; (2) the problem parameterized by the vertex cover number of the
input graph does not admit a polynomial compression unless coNP $\subseteq$
NP/poly, (3) it does not admit $2^{o(n)}$ algorithm under ETH, and (4) the
Defensive Alliance problem on circle graphs is NP-complete.
</p></div>
    </summary>
    <updated>2021-11-11T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05518</id>
    <link href="http://arxiv.org/abs/2111.05518" rel="alternate" type="text/html"/>
    <title>Applications of Random Algebraic Constructions to Hardness of Approximation</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bukh:Boris.html">Boris Bukh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/S=:Karthik_C=.html">Karthik C. S.</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narayanan:Bhargav.html">Bhargav Narayanan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05518">PDF</a><br/><b>Abstract: </b>In this paper, we show how one may (efficiently) construct two types of
extremal combinatorial objects whose existence was previously conjectural.
</p>
<p>(*) Panchromatic Graphs: For fixed integer k, a k-panchromatic graph is,
roughly speaking, a balanced bipartite graph with one partition class
equipartitioned into k colour classes in which the common neighbourhoods of
panchromatic k-sets of vertices are much larger than those of k-sets that
repeat a colour. The question of their existence was raised by Karthik and
Manurangsi [Combinatorica 2020].
</p>
<p>(*) Threshold Graphs: For fixed integer k, a k-threshold graph is, roughly
speaking, a balanced bipartite graph in which the common neighbourhoods of
k-sets of vertices on one side are much larger than those of (k+1)-sets. The
question of their existence was raised by Lin [JACM 2018].
</p>
<p>As applications of our constructions, we show the following conditional time
lower bounds on the parameterized set intersection problem where, given a
collection of n sets over universe [n] and a parameter k, the goal is to find k
sets with the largest intersection.
</p>
<p>(*) Assuming ETH, for any computable function F, no $n^{o(k)}$-time algorithm
can approximate the parameterized set intersection problem up to factor F(k).
This improves considerably on the previously best-known result under ETH due to
Lin [JACM 2018], who ruled out any $n^{o(\sqrt{k})}$ time approximation
algorithm for this problem.
</p>
<p>(*) Assuming SETH, for every $\varepsilon&gt;0$ and any computable function F,
no $n^{k-\varepsilon}$-time algorithm can approximate the parameterized set
intersection problem up to factor F(k). No result of comparable strength was
previously known under SETH, even for solving this problem exactly.
</p></div>
    </summary>
    <updated>2021-11-11T22:37:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05425</id>
    <link href="http://arxiv.org/abs/2111.05425" rel="alternate" type="text/html"/>
    <title>Disjoint edges in geometric graphs</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Nikita Chernega, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Polyanskii:Alexandr.html">Alexandr Polyanskii</a>, Rinat Sadykov <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05425">PDF</a><br/><b>Abstract: </b>A geometric graph is a graph drawn in the plane so that its vertices and
edges are represented by points in general position and straight line segments,
respectively. A vertex of a geometric graph is called convex if it lies outside
of the convex hull of its neighbors. We show that for a geometric graph with
$n$ vertices and $e$ edges there are at least $\frac{n}{2}\binom{2e/n}{3}$
pairs of disjoint edges provided that $2e\geq n$ and all the vertices of the
graph are convex. Besides, we prove that if any edge of a geometric graph with
$n$ vertices is disjoint from at most $m$ edges, then the number of edges of
this graph does not exceed $n(\sqrt{1+8m}+3)/4$ provided that $n$ is
sufficiently large.
</p>
<p>These two results are tight for an infinite family of graphs.
</p></div>
    </summary>
    <updated>2021-11-11T23:07:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05386</id>
    <link href="http://arxiv.org/abs/2111.05386" rel="alternate" type="text/html"/>
    <title>Computing Area-Optimal Simple Polygonizations</title>
    <feedworld_mtime>1636588800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fekete:S=aacute=ndor_P=.html">Sándor P. Fekete</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haas:Andreas.html">Andreas Haas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Keldenich:Phillip.html">Phillip Keldenich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perk:Michael.html">Michael Perk</a>, Arne Schmidt <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05386">PDF</a><br/><b>Abstract: </b>We consider methods for finding a simple polygon of minimum (Min-Area) or
maximum (Max-Area) possible area for a given set of points in the plane. Both
problems are known to be NP-hard; at the center of the recent CG Challenge,
practical methods have received considerable attention. However, previous
methods focused on heuristic methods, with no proof of optimality. We develop
exact methods, based on a combination of geometry and integer programming. As a
result, we are able to solve instances of up to n=25 points to provable
optimality. While this extends the range of solvable instances by a
considerable amount, it also illustrates the practical difficulty of both
problem variants.
</p></div>
    </summary>
    <updated>2021-11-11T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4588</id>
    <link href="https://lucatrevisan.wordpress.com/2021/11/10/online-optimization-post-7-matrix-multiplicative-weights-update/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 7: Matrix Multiplicative Weights Update</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is the seventh in a series of posts on online optimization, where we alternate one post explaining a result from the theory of online convex optimization and one post explaining an “application” in computational complexity or combinatorics. The first … <a href="https://lucatrevisan.wordpress.com/2021/11/10/online-optimization-post-7-matrix-multiplicative-weights-update/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is the seventh in a series of posts on online optimization, where we alternate one post explaining a result from the theory of online convex optimization and one post explaining an “application” in computational complexity or combinatorics. The first two posts were about the technique of <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">Multiplicative Weights Updates</a> and its application to <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/">“derandomizing” probabilistic arguments</a> based on combining a Chernoff bound and a union bound. The third and fourth post were about the <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/">Follow-the-Regularized-Leader</a> framework, which unifies multiplicative weights and gradient descent, and a <a href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/">“gradient descent view” of the Frieze-Kannan Weak Regularity Lemma</a>. The fifth and sixth post were about the <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/">constrained version of the Follow-the-Regularized-Leader</a> framework, and the <a href="https://lucatrevisan.wordpress.com/2021/10/20/online-optimization-post-6-the-impagliazzo-hard-core-set-lemma/">Impagliazzo Hard-Core Set Lemma</a>. Today we shall see the technique of Matrix Multiplicative Weights Updates.</p>
<p><b>1. Matrix Multiplicative Weights Update </b></p>
<p>In this post we consider the following generalization, introduced and studied by <a href="https://dl.acm.org/doi/10.1145/1250790.1250823">Arora and Kale</a>, of the “learning from expert advice” setting and the multiplicative weights update method. In the “experts” model, we have a repeated game in which, at each time step <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we have the option of following the advice of one of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> experts; if we follow the advice of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we incur a loss of <img alt="{\ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is unknown to us (although, at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> we know the loss functions <img alt="{\ell_1(\cdot),\ldots,\ell_{t-1}(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1%28%5Ccdot%29%2C%5Cldots%2C%5Cell_%7Bt-1%7D%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>). We are allowed to choose a probabilistic strategy, whereby we follow the advice of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with probability <img alt="{x_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, so that our expected loss at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{\sum_{i=1}^n x_t(i) \ell_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>In the matrix version, instead of choosing an expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> we are allowed to choose a unit <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-dimensional vector <img alt="{v_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and the loss incurred in choosing the vector <img alt="{v_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{v_t ^T L_t v_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t+%5ET+L_t+v_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="{L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an unknown symmetric <img alt="{n\times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> matrix. We are also allowed to choose a probabilistic strategy, so that with probability <img alt="{x_t(j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> we choose the unit vector <img alt="{v_t^{(j)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and we incur the expected loss</p>
<p align="center"><img alt="\displaystyle  \sum_j x_t (j) \cdot (v_t^{(j)})^T L_t v_t^{(j)} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_j+x_t+%28j%29+%5Ccdot+%28v_t%5E%7B%28j%29%7D%29%5ET+L_t+v_t%5E%7B%28j%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p><span id="more-4588"/></p>
<p>The above expression can also be written as</p>
<p align="center"><img alt="\displaystyle  X_t \bullet L_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_t+%5Cbullet+L_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> where <img alt="{X_t = \sum_j x_t(j) v_t^{(j)}(v_t^{(j)})^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_t+%3D+%5Csum_j+x_t%28j%29+v_t%5E%7B%28j%29%7D%28v_t%5E%7B%28j%29%7D%29%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and we used the Frobenius inner product among square matrices defined as <img alt="{A \bullet B = \sum_{i,j} A_{i,j} B_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Cbullet+B+%3D+%5Csum_%7Bi%2Cj%7D+A_%7Bi%2Cj%7D+B_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The matrices <img alt="{X_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that can be obtained as convex combinations of rank-1 matrices of the form <img alt="{vv^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bvv%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a unit vector are called <em>density matrices</em> and can be characterized as the set of positive semidefinite matrices whose trace is 1.</p>
<p>It is possible to see the above game as the “quantum version” of the experts settings. A choice of a unit vector <img alt="{v_t^{(j)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a <em>pure quantum state</em>, a probability distribution of pure quantum states, described by a density matrix, is a <em>mixed quantum state</em>. If <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a density matrix describing a mixed quantum state, <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a symmetric matrix, and <img alt="{L = \sum_i \lambda_i w_i w_i^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL+%3D+%5Csum_i+%5Clambda_i+w_i+w_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the spectral decomposition of <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in terms of its eigenvalues <img alt="{\lambda_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and orthonormal eigenvectors <img alt="{w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then <img alt="{X \bullet L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%5Cbullet+L%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the expected outcome of a measurement of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the basis <img alt="{w_1,\ldots,w_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_1%2C%5Cldots%2Cw_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and such that <img alt="{\lambda_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the value of the measurement if the outcome is <img alt="{w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>If you have no idea what the above paragraph means, that is perfectly ok because this view will not be particularly helpful in motivating the algorithm and analysis that we will describe. (Here I am reminded of the joke about the way people from Naples give directions: “How do I get to the post office?”, “Well, you see that road over there? After the a couple of blocks there is a pharmacy, where my uncle used to work, though now he is retired.” “Ok?” “Now, if you turn left after the pharmacy, after a while you get to a square with a big fountain and the church of St. Anthony where my niece got married. It was a beautiful ceremony, but the food at the reception was not great.” “Yes, I know that square”, “Good, don’t go there, the post office is not that way. Now, if you instead take that other road over there …”)</p>
<p>The main point of the above game, and of the Matrix Multiplicative Weights Update (MMWU) algorithm that plays it with bounded regret, is that it provides useful generalizations of the standard “experts” game and of the Multiplicative Weights Update (MWU) algorithm. For example, as we have already seen, MWU can provide a “derandomization” of the Chernoff bound; we will see that MMWU provides a derandomization of the <em>matrix</em> Chernoff bound. MWU can be used to approximate certain Linear Programming problems; MMWU can be used to approximate certain <em>Semidefinite Programming</em> problems.</p>
<p>To define and analyze the MMWU algorithm, we need to introduce certain operations on matrices. We will always work with real-valued symmetric matrices, but everything generalizes to complex-valued Hermitian matrices. If <img alt="{M = \sum_i \lambda_i w_i w_i^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM+%3D+%5Csum_i+%5Clambda_i+w_i+w_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a symmetric matrix, <img alt="{\lambda_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are the eigenvalues of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="{w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are corresponding orthonormal eigenvectors, then we will define a number of operations and functions on <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that operate on the eigenvalues while leaving the eigenvectors unchanged.</p>
<p>The first operation is <em>matrix exponentiation</em>: we define</p>
<p align="center"><img alt="\displaystyle  e^X := \sum_i e^{\lambda_i} w_i w_i^T " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%3A%3D+%5Csum_i+e%5E%7B%5Clambda_i%7D+w_i+w_i%5ET+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> The operation always defines a positive definite matrix, and the resulting matrix satisfies a “Taylor expansion”</p>
<p align="center"><img alt="\displaystyle  e^X = \sum_{k=0}^\infty \frac1 {k!} X^k " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%3D+%5Csum_%7Bk%3D0%7D%5E%5Cinfty+%5Cfrac1+%7Bk%21%7D+X%5Ek+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> Indeed, it is more common to use the above expansion as the definition of the matrix exponential, and then derive the expression in terms of eigenvalues.</p>
<p>We also have the useful bounds</p>
<p align="center"><img alt="\displaystyle  e^X \succeq I + X" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%5Csucceq+I+%2B+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> which is true for every <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and</p>
<p align="center"><img alt="\displaystyle  e^X \preceq I + X +X^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%5Cpreceq+I+%2B+X+%2BX%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> which is true for all <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that <img alt="{X \preceq I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%5Cpreceq+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>Analogously, if <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is positive definite, we can define</p>
<p align="center"><img alt="\displaystyle  \log X := \sum_i (\log \lambda_i) \cdot w_i w_i^T " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog+X+%3A%3D+%5Csum_i+%28%5Clog+%5Clambda_i%29+%5Ccdot+w_i+w_i%5ET+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>and we have a number of identities like <img alt="{\log e^X = X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+e%5EX+%3D+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{\log X^k = k \log X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+X%5Ek+%3D+k+%5Clog+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{e^{k X} = e^k \cdot X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bk+X%7D+%3D+e%5Ek+%5Ccdot+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a scalar. We should be careful, however, not to take the analogy with real numbers too far: for example, if <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are two symmetric matrices, in general it is not trues that <img alt="{e^{A+B} = e^A \cdot e^B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7BA%2BB%7D+%3D+e%5EA+%5Ccdot+e%5EB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, in fact the above expression is actually always false except when <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> commute, in which case it is trivially true. We have, however, the following extremely useful fact.</p>
<blockquote><p><b>Theorem 1 (Golden-Thompson Inequality)</b> <em> </em></p>
<p><em/></p><em>
<p align="center"><img alt="\displaystyle  {\rm tr}(e^{A+B}) \leq {\rm tr}(e^A \cdot e^B) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+tr%7D%28e%5E%7BA%2BB%7D%29+%5Cleq+%7B%5Crm+tr%7D%28e%5EA+%5Ccdot+e%5EB%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
</em><p><em/><em> </em></p></blockquote>
<p>The Golden-Thompson inequality will be all we need to generalize to this matrix setting everything we have proved about multiplicative weights. See <a href="https://terrytao.wordpress.com/2010/07/15/the-golden-thompson-inequality/">this post by Terry Tao</a> for a proof.</p>
<p>The <em>Von Neumann entropy</em> of a density matrix <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with eigenvalues <img alt="{\lambda_1,\cdots,\lambda_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_1%2C%5Ccdots%2C%5Clambda_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is defined as</p>
<p align="center"><img alt="\displaystyle  S(X) = \sum_i \lambda_i \log \frac 1 {\lambda_i} = - {\rm tr}(X\log X) = - X \bullet \log X " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28X%29+%3D+%5Csum_i+%5Clambda_i+%5Clog+%5Cfrac+1+%7B%5Clambda_i%7D+%3D+-+%7B%5Crm+tr%7D%28X%5Clog+X%29+%3D+-+X+%5Cbullet+%5Clog+X+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> that is, if we view <img alt="{X = \sum_i \lambda_i v_i v_i^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5Csum_i+%5Clambda_i+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as the mixed quantum state in which the pure state <img alt="{v_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has probability <img alt="{\lambda_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then <img alt="{S(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the entropy of the distribution over the pure states. Again, this is not a particularly helpful point of view, and in fact we will be interested in defining <img alt="{S(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> not just for density matrices <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> but for arbitrary positive definite matrices, and even positive semidefinite (with the convention that <img alt="{0 \log 0 = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%5Clog+0+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is used also in the standard definition of entropy of a distribution).</p>
<p>We will be interested in using Von Neumann entropy as a regularizer, and hence we will want to know what is its Bregman divergence. Some calculations show that the Bregman divergence of the Von Neumann entropy, which is called the quantum relative entropy, is</p>
<p align="center"><img alt="\displaystyle  S(X_1|| X_2) = {\rm tr} (X_1 \cdot ( \log X_1 - \log X_2)) + {\rm tr}(X_2) - {\rm tr}(X_1) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28X_1%7C%7C+X_2%29+%3D+%7B%5Crm+tr%7D+%28X_1+%5Ccdot+%28+%5Clog+X_1+-+%5Clog+X_2%29%29+%2B+%7B%5Crm+tr%7D%28X_2%29+-+%7B%5Crm+tr%7D%28X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p align="center"><img alt="\displaystyle  = X_1 \bullet(\log X_1 - \log X_2) + I \bullet (X_2 - X_1) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+X_1+%5Cbullet%28%5Clog+X_1+-+%5Clog+X_2%29+%2B+I+%5Cbullet+%28X_2+-+X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> If <img alt="{X_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{X_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are density matrices, the terms <img alt="{{\rm tr}(X_2) - {\rm tr}(X_1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+tr%7D%28X_2%29+-+%7B%5Crm+tr%7D%28X_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> cancel out; the above definition is valid for arbitrary positive definite matrices.</p>
<p>We will have to study the minima of various functions that take a matrix as an input, so it is good to understand how to compute the gradient of such functions. For example what is the gradient of the function <img alt="{{\rm tr}(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+tr%7D%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? Working through the definition we see that <img alt="{\nabla {\rm tr}(X) = I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+%7B%5Crm+tr%7D%28X%29+%3D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and indeed we always have that the gradient of the function <img alt="{X \rightarrow A\bullet X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%5Crightarrow+A%5Cbullet+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> everywhere. Somewhat less obvious is the calculation of the gradient of the Von Neumann entropy, which is</p>
<p align="center"><img alt="\displaystyle  \nabla (X \bullet \log X) = I + \log X " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+%28X+%5Cbullet+%5Clog+X%29+%3D+I+%2B+%5Clog+X+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p><b>2. Analysis in the Constrained FTRL Framework </b></p>
<p>Suppose that we play that we described above using agile mirror descent and using negative Von Neumann entropy (appropriately scaled) as a regularizer. That is, for some <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that we will choose later, we use the regularizer</p>
<p align="center"><img alt="\displaystyle  R(X) = c X \bullet \log X" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28X%29+%3D+c+X+%5Cbullet+%5Clog+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> which has the Bregman divergence</p>
<p align="center"><img alt="\displaystyle  D(X_1,X_2) = c S(X_1 || X_2) = c X_1 \bullet (\log X_1 - \log X_2) + cI \bullet (X_2 - X_1) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_1%2CX_2%29+%3D+c+S%28X_1+%7C%7C+X_2%29+%3D+c+X_1+%5Cbullet+%28%5Clog+X_1+-+%5Clog+X_2%29+%2B+cI+%5Cbullet+%28X_2+-+X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and our feasible set is the set of density matrices</p>
<p align="center"><img alt="\displaystyle  \Delta := \{ X\in {\mathbb R}^{n\times n} : X \succeq {\bf 0} \wedge {\rm tr}(X) = 1 \} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CDelta+%3A%3D+%5C%7B+X%5Cin+%7B%5Cmathbb+R%7D%5E%7Bn%5Ctimes+n%7D+%3A+X+%5Csucceq+%7B%5Cbf+0%7D+%5Cwedge+%7B%5Crm+tr%7D%28X%29+%3D+1+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> To bound the regret, we just have to plug the above definitions into the machinery that we developed <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/">in our fifth post</a>.</p>
<p>At time 1, we play the identity matrix scaled by n, which is a density matrix of maximum Von Neumann entropy <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>:</p>
<p align="center"><img alt="\displaystyle  X_1 := \arg\min_{X \in \Delta} R(X) = \frac 1n I " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_1+%3A%3D+%5Carg%5Cmin_%7BX+%5Cin+%5CDelta%7D+R%28X%29+%3D+%5Cfrac+1n+I+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> At time <img alt="{t+1\geq 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%2B1%5Cgeq+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we play the matrix <img alt="{X_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> obtained as</p>
<p align="center"><img alt="\displaystyle  \hat X_{t+1} = \arg\min_{X} D(X,X_{t}) + X\bullet L_{t} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+X_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7BX%7D+D%28X%2CX_%7Bt%7D%29+%2B+X%5Cbullet+L_%7Bt%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p align="center"><img alt="\displaystyle  X_{t+1} = \arg\min_{X\in \Delta} D(X,\hat X_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7BX%5Cin+%5CDelta%7D+D%28X%2C%5Chat+X_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and recall that we proved that, after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> steps,</p>
<p align="center"><img alt="\displaystyle  Regret_T(X) \leq D(X,X_1) + \sum_{t=1}^T D(X_t,\hat X_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T%28X%29+%5Cleq+D%28X%2CX_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+D%28X_t%2C%5Chat+X_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>If <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a density matrix with eigenvalues <img alt="{\lambda_1,\ldots,\lambda_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_1%2C%5Cldots%2C%5Clambda_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then the first term is</p>
<p align="center"><img alt="\displaystyle  D \left( X, \frac 1n I \right) = c X \bullet (\log X - \log n^{-1} I) = c \sum_i \lambda_i \log \frac n{\lambda_i} = c \log n - c \sum_i \lambda_i \frac 1 {\lambda_i} \leq c\log n " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D+%5Cleft%28+X%2C+%5Cfrac+1n+I+%5Cright%29+%3D+c+X+%5Cbullet+%28%5Clog+X+-+%5Clog+n%5E%7B-1%7D+I%29+%3D+c+%5Csum_i+%5Clambda_i+%5Clog+%5Cfrac+n%7B%5Clambda_i%7D+%3D+c+%5Clog+n+-+c+%5Csum_i+%5Clambda_i+%5Cfrac+1+%7B%5Clambda_i%7D+%5Cleq+c%5Clog+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> To complete the analysis we have to understand <img alt="{\hat X_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+X_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We need to compute the gradient <img alt="{X \rightarrow D(X,X_{t}) + X\bullet L_{t} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%5Crightarrow+D%28X%2CX_%7Bt%7D%29+%2B+X%5Cbullet+L_%7Bt%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and set it to zero. The gradient of <img alt="{X\bullet L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%5Cbullet+L_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is just <img alt="{L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The gradient of <img alt="{D(X,X_{t})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%28X%2CX_%7Bt%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is</p>
<p align="center"><img alt="\displaystyle  \nabla D(X,X_t) = c \nabla X \bullet \log X - c \nabla X \bullet \log X_t - \nabla c X \bullet I " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+D%28X%2CX_t%29+%3D+c+%5Cnabla+X+%5Cbullet+%5Clog+X+-+c+%5Cnabla+X+%5Cbullet+%5Clog+X_t+-+%5Cnabla+c+X+%5Cbullet+I+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p align="center"><img alt="\displaystyle  = cI + c \log X - c \log X_t - cI " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+cI+%2B+c+%5Clog+X+-+c+%5Clog+X_t+-+cI+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> Meaning that we want to solve for</p>
<p align="center"><img alt="\displaystyle  c \log X - c\log X_t + L_t = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c+%5Clog+X+-+c%5Clog+X_t+%2B+L_t+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and <img alt="{\hat X_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+X_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> satisfies</p>
<p align="center"><img alt="\displaystyle \log X_t - \log \hat X_{t+1} = \frac 1c L_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clog+X_t+-+%5Clog+%5Chat+X_%7Bt%2B1%7D+%3D+%5Cfrac+1c+L_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p align="center"><img alt="\displaystyle  \hat X_{t+1} = e^{\log X_t - \frac 1c L_t } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+X_%7Bt%2B1%7D+%3D+e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and we can write</p>
<p align="center"><img alt="\displaystyle  D(X_t,\hat X_{t+1} ) = c \cdot \left( X_t \bullet (\log X_t - \log \hat X_{t+1})\right) + c {\rm tr}( \hat X_{t+1} )" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_t%2C%5Chat+X_%7Bt%2B1%7D+%29+%3D+c+%5Ccdot+%5Cleft%28+X_t+%5Cbullet+%28%5Clog+X_t+-+%5Clog+%5Chat+X_%7Bt%2B1%7D%29%5Cright%29+%2B+c+%7B%5Crm+tr%7D%28+%5Chat+X_%7Bt%2B1%7D+%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p align="center"><img alt="\displaystyle  = c \cdot X_t \bullet \frac 1c L_t + c \cdot {\rm tr}(e^{\log X_t - \frac 1c L_t}) + c {\rm tr} \hat X_{t+1} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+c+%5Ccdot+X_t+%5Cbullet+%5Cfrac+1c+L_t+%2B+c+%5Ccdot+%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t%7D%29+%2B+c+%7B%5Crm+tr%7D+%5Chat+X_%7Bt%2B1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> Then we can use Golden-Thompson and the fact that <img alt="{e^-\frac 1c L_t \preceq I - \frac 1c L_t + \frac 1{c^2} L^2_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E-%5Cfrac+1c+L_t+%5Cpreceq+I+-+%5Cfrac+1c+L_t+%2B+%5Cfrac+1%7Bc%5E2%7D+L%5E2_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which holds if <img alt="{L_t \preceq cI}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, to write</p>
<p align="center"><img alt="\displaystyle  {\rm tr}(e^{\log X_t - \frac 1c L_t}) \leq {\rm tr}(e^{\log X_t} \cdot e^{-\frac 1c L_t} ) = X_t \bullet e^{-\frac 1c L_t} \leq X_t \bullet \left( I - \frac 1c L_t + \frac 1{c^2} L^2_t \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t%7D%29+%5Cleq+%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t%7D+%5Ccdot+e%5E%7B-%5Cfrac+1c+L_t%7D+%29+%3D+X_t+%5Cbullet+e%5E%7B-%5Cfrac+1c+L_t%7D+%5Cleq+X_t+%5Cbullet+%5Cleft%28+I+-+%5Cfrac+1c+L_t+%2B+%5Cfrac+1%7Bc%5E2%7D+L%5E2_t+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> Combining everything together we have</p>
<p align="center"><img alt="\displaystyle  D(X_t,\hat X_{t+1} ) \leq \frac 1c X_t \bullet L_t^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_t%2C%5Chat+X_%7Bt%2B1%7D+%29+%5Cleq+%5Cfrac+1c+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and so, provided <img alt="{\lambda_{\max} (L_t) \leq c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_%7B%5Cmax%7D+%28L_t%29+%5Cleq+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,</p>
<p align="center"><img alt="\displaystyle  Regret_T \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> This is the best bound we can hope for, and it matches Theorem 1 in <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">our first post</a> about the Xultiplicative Weights Update algorithm.</p>
<p>If we have <img alt="{L_t \preceq I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we can simplify it to</p>
<p align="center"><img alt="\displaystyle  Regret_T \leq c \log n + \frac T c = 2 \sqrt{T \log n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+T+c+%3D+2+%5Csqrt%7BT+%5Clog+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> where the last step comes from optimizing <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>We can also write, under the condition <img alt="{L_t \preceq c I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+c+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,</p>
<p align="center"><img alt="\displaystyle  Regret_T (X) \leq c \log n + \frac 1c \sum_{t=1}^T (X_t \bullet |L_t| )||L_t|| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%28X%29+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+%28X_t+%5Cbullet+%7CL_t%7C+%29%7C%7CL_t%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> where <img alt="{|L_t|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CL_t%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the “absolute value” of the matrix <img alt="{L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> defined in the following way: if <img alt="{X = \sum_i \lambda_i v_i v_i^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5Csum_i+%5Clambda_i+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a symmetric matrix, then its absolute value is <img alt="{|X| = \sum_i |\lambda_i| \cdot v_i v_i^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CX%7C+%3D+%5Csum_i+%7C%5Clambda_i%7C+%5Ccdot+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Allen-Zhu, Liao and Orecchia state the analysis in this way in their <a href="https://arxiv.org/abs/1506.04838">on generalizations of Matrix Multiplicative Weights</a>.</p>
<p>Our next post will discuss applications at length, but for now let us gain a bit of intuition about the usefulness of these regret bounds. Recall that, for every symmetric matrix <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we have</p>
<p align="center"><img alt="\displaystyle  \lambda_{\min} (M) = \min_{X \rm\ density\ matrix} \ \ X \bullet M " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_%7B%5Cmin%7D+%28M%29+%3D+%5Cmin_%7BX+%5Crm%5C+density%5C+matrix%7D+%5C+%5C+X+%5Cbullet+M+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and so the regret bound can be reintepreted in the following way: if we let <img alt="{L_1,\ldots,L_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_1%2C%5Cldots%2CL_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be the loss functions used in a game played against a MMWU algorithm, and the algorithm selects density matrices <img alt="{X_1,\ldots,X_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_1%2C%5Cldots%2CX_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then</p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T X_t \bullet L_t - \min_{X \rm \ density \ matrix} \ \ X \bullet \sum_{t=1}^T L_t \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+-+%5Cmin_%7BX+%5Crm+%5C+density+%5C+matrix%7D+%5C+%5C+X+%5Cbullet+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> that is,</p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T X_t \bullet L_t - \lambda_{\min} \left( \sum_{t=1}^T L_t \right) \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+-+%5Clambda_%7B%5Cmin%7D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cright%29+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> provided that <img alt="{L_t \preceq cI}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. For example, switching <img alt="{L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with <img alt="{-L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-L_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we have <a name="main"/></p>
<p><a name="main"/></p><a name="main">
<p align="center"><img alt="\displaystyle   \lambda_{\max} \left( \sum_{t=1}^T L_t \right) \leq \sum_{t=1}^T X_t \bullet L_t + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 + c\log n \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Clambda_%7B%5Cmax%7D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cright%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+%2B+c%5Clog+n+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
</a><p><a name="main"/><a name="main"/> provided that <img alt="{L_t \succeq -cI}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Csucceq+-cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which means that if we can choose a sequence of loss matrices that make the MMWU have small loss at each step, then we are guaranteed that the sum of such matrices cannot have any large eigenvalue.</p></div>
    </content>
    <updated>2021-11-10T12:11:57Z</updated>
    <published>2021-11-10T12:11:57Z</published>
    <category term="theory"/>
    <category term="matrix multiplicative weights update"/>
    <category term="online optimization"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2021-11-12T04:20:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/154</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/154" rel="alternate" type="text/html"/>
    <title>TR21-154 |  Explicit Binary Tree Codes with Sub-Logarithmic Size Alphabet | 

	Gil Cohen, 

	Inbar Ben Yaacov, 

	Tal Yankovitz</title>
    <summary>Since they were first introduced by Schulman (STOC 1993), the construction of tree codes remained an elusive open problem. The state-of-the-art construction by Cohen, Haeupler and Schulman (STOC 2018) has constant distance and $(\log n)^{e}$ colors for some constant $e &gt; 1$ that depends on the distance, where $n$ is the depth of the tree. Insisting on a constant number of colors at the expense of having vanishing distance, Gelles, Haeupler, Kol, Ron-Zewi, and Wigderson (SODA 2016) constructed a distance $\Omega(\frac1{\log n})$ tree code.

In this work we improve upon these prior works and construct a distance-$\delta$ tree code with $(\log{n})^{O(\sqrt{\delta})}$ colors. This is the first construction of a constant distance tree code with sub-logarithmic number of colors. Moreover, as a direct corollary we obtain a tree code with a constant number of colors and distance $\Omega\left(\frac1{(\log\log{n})^{2}}\right)$, exponentially improving upon the above-mentioned work by Gelles et al.</summary>
    <updated>2021-11-10T11:16:50Z</updated>
    <published>2021-11-10T11:16:50Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-12T04:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/09/christopher-strachey-professorship-of-computing-at-university-of-oxford-apply-by-february-28-2022/</id>
    <link href="https://cstheory-jobs.org/2021/11/09/christopher-strachey-professorship-of-computing-at-university-of-oxford-apply-by-february-28-2022/" rel="alternate" type="text/html"/>
    <title>Christopher Strachey Professorship of Computing at University of Oxford (apply by February 28, 2022)</title>
    <summary>The Strachey Professorship is the oldest chair in the Department of Computer Science, and is named for Christopher Strachey, who founded Oxford’s Programming Research Group in 1965. The Department seeks an internationally recognised research leader who will further the academic and strategic development of the department. Website: http://www.cs.ox.ac.uk/news/1988-full.html Email: head-of-dept@cs.ox.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Strachey Professorship is the oldest chair in the Department of Computer Science, and is named for Christopher Strachey, who founded Oxford’s Programming Research Group in 1965. The Department seeks an internationally recognised research leader who will further the academic and strategic development of the department.</p>
<p>Website: <a href="http://www.cs.ox.ac.uk/news/1988-full.html">http://www.cs.ox.ac.uk/news/1988-full.html</a><br/>
Email: head-of-dept@cs.ox.ac.uk</p></div>
    </content>
    <updated>2021-11-09T18:24:49Z</updated>
    <published>2021-11-09T18:24:49Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-12T04:20:45Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/neurips2021/</id>
    <link href="https://differentialprivacy.org/neurips2021/" rel="alternate" type="text/html"/>
    <title>Conference Digest - NeurIPS 2021</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The accepted papers for <a href="https://neurips.cc/Conferences/2020">NeurIPS 2021</a> were recently announced, and there’s a huge amount of differential privacy content. 
We found one relevant workshop and 48 papers.
This is up from 31 papers last year, an over 50% increase!
It looks like there’s huge growth in interest on differentially private machine learning.
Impressively, at the time of this writing, all but five papers are already posted on arXiv!
For the full list of accepted papers, see <a href="https://neurips.cc/Conferences/2021/AcceptedPapersInitial">here</a>.
Please let us know if we missed relevant papers on differential privacy!</p>

<h2 id="workshops">Workshops</h2>

<ul>
  <li><a href="https://priml2021.github.io/">Privacy in Machine Learning (PriML) 2021</a></li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2103.08721">A Central Limit Theorem for Differentially Private Query Answering</a><br/>
Jinshuo Dong, Weijie Su, Linjun Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2108.02391">Adapting to function difficulty and growth conditions in private optimization</a><br/>
Hilal Asi, Daniel Levy, John Duchi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.04378">Adaptive Machine Unlearning</a><br/>
Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, Chris Waites</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.13239">An Uncertainty Principle is a Price of Privacy-Preserving Microdata</a><br/>
John Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Daniel Kifer, Philip Leclerc, William Sexton, Ashley Simpson, Christine Task, Pavel Zhuravlev</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03408">Antipodes of Label Differential Privacy: PATE and ALIBI</a><br/>
Mani Malek Esmaeili, Ilya Mironov, Karthik Prasad, Igor Shilov, Florian Tramer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.13329">Covariance-Aware Private Mean Estimation Without Private Covariance Estimation</a><br/>
Gavin Brown, Marco Gaboardi, Adam Smith, Jonathan Ullman, Lydia Zakynthinou</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.06062">Deep Learning with Label Differential Privacy</a><br/>
Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, Chiyuan Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.05855">Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient Descent</a><br/>
Rishav Chourasia, Jiayuan Ye, Reza Shokri</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02674">Differentially Private Empirical Risk Minimization under the Fairness Lens</a><br/>
Cuong Tran, My Dinh, Ferdinando Fioretto</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.14153">Differentially Private Federated Bayesian Optimization with Distributed Exploration</a><br/>
Zhongxiang Dai, Bryan Kian Hsiang Low, Patrick Jaillet</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.03871">Differentially Private Learning with Adaptive Clipping</a><br/>
Galen Andrew, Om Thakkar, Swaroop Ramaswamy, Brendan McMahan</p>
  </li>
  <li>
    <p>Differentially Private Model Personalization<br/>
Prateek Jain, John Rush, Adam Smith, Shuang Song, Abhradeep Guha Thakurta</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02900">Differentially Private Multi-Armed Bandits in the Shuffle Model</a><br/>
Jay Tenenbaum, Haim Kaplan, Yishay Mansour, Uri Stemmer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2108.02831">Differentially Private n-gram Extraction</a><br/>
Kunho Kim, Sivakanth Gopi, Janardhan Kulkarni, Sergey Yekhanin</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.02516">Differential Privacy Over Riemannian Manifolds</a><br/>
Matthew Reimherr, Karthik Bharath, Carlos Soto</p>
  </li>
  <li>
    <p>Differentially Private Sampling from Distributions<br/>
Sofya Raskhodnikova, Satchit Sivakumar, Adam Smith, Marika Swanberg</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.05585">Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings</a><br/>
Raef Bassily, Cristóbal Guzmán, Michael Menart</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.01177">Don’t Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence</a><br/>
Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, Karsten Kreis</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.09063">Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization</a><br/>
Pranav Subramani, Nicholas Vadivelu, Gautam Kamath</p>
  </li>
  <li>
    <p>Exact Privacy Guarantees for Markov Chain Implementations of the Exponential Mechanism with Artificial Atoms<br/>
Jeremy Seeman, Matthew Reimherr, Aleksandra Slavković</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.03013">Fast and Memory Efficient Differentially Private-SGD via JL Projections</a><br/>
Zhiqi Bu, Sivakanth Gopi, Janardhan Kulkarni, Yin Tat Lee, Hanwen Shen, Uthaipon Tantipongpipat</p>
  </li>
  <li>
    <p>G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators<br/>
Yunhui Long, Boxin Wang, Zhuolin Yang, Bhavya Kailkhura, Aston Zhang, Carl Gunter, Bo Li</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03365">Generalized Linear Bandits with Local Differential Privacy</a><br/>
Yuxuan Han, Zhipeng Liang, Yang Wang, Jiheng Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.11193">Individual Privacy Accounting via a Rényi Filter</a><br/>
Vitaly Feldman, Tijana Zrnic</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2104.00979">Information-constrained optimization: can adaptive processing of gradients help?</a><br/>
Jayadev Acharya, Clement Canonne, Prathamesh Mayekar, Himanshu Tyagi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.00463">Instance-optimal Mean Estimation Under Differential Privacy</a><br/>
Ziyue Huang, Yuting Liang, Ke Yi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.07153">Iterative Methods for Private Synthetic Data: Unifying Framework and New Methods</a><br/>
Terrance Liu, Giuseppe Vietri, Steven Wu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.11845">Learning with User-Level Privacy</a><br/>
Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, Ananda Theertha Suresh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.13513">Littlestone Classes are Privately Online Learnable</a><br/>
Noah Golowich, Roi Livni</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.07778">Local Differential Privacy for Regret Minimization in Reinforcement Learning</a><br/>
Evrard Garcelon, Vianney Perchet, Ciara Pike-Burke, Matteo Pirotta</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.03940">Locally differentially private estimation of functionals of discrete distributions</a><br/>
Cristina Butucea, Yann Issartel</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2105.10675">Locally private online change point detection</a><br/>
Tom Berrett, Yi Yu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.10870">Multiclass versus Binary Differentially Private PAC Learning</a><br/>
Satchit Sivakumar, Mark Bun, Marco Gaboardi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02848">Numerical Composition of Differential Privacy</a><br/>
Sivakanth Gopi, Yin Tat Lee, Lukas Wutschitz</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.11526">On the Sample Complexity of Privately Learning Axis-Aligned Rectangles</a><br/>
Menachem Sadigurschi, Uri Stemmer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03645">Photonic Differential Privacy with Direct Feedback Alignment</a><br/>
Ruben Ohana, Hamlet Medina, Julien Launay, Alessandro Cappelli, Iacopo Poli, Liva Ralaivola, Alain Rakotomamonjy</p>
  </li>
  <li>
    <p>Private and Non-private Uniformity Testing for Ranking Data<br/>
Róbert Busa-Fekete, Dimitris Fotakis, Emmanouil Zampetakis</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.07171">Private learning implies quantum stability</a><br/>
Yihui Quek, Srinivasan Arunachalam, John A Smolin</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.15352">Private Non-smooth ERM and SCO in Subquadratic Steps</a><br/>
Janardhan Kulkarni, Yin Tat Lee, Daogao Liu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02162">Privately Learning Mixtures of Axis-Aligned Gaussians</a><br/>
Ishaq Aden-Ali, Hassan Ashtiani, Christopher Liaw</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.00001">Privately Learning Subspaces</a><br/>
Vikrant Singhal, Thomas Steinke</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.02281">Privately Publishable Per-instance Privacy</a><br/>
Rachel Redberg, Yu-Xiang Wang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2109.06153">Relaxed Marginal Consistency for Differentially Private Query Answering</a><br/>
Ryan McKenna, Siddhant Pradhan, Daniel Sheldon, Gerome Miklau</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.03279">Remember What You Want to Forget: Algorithms for Machine Unlearning</a><br/>
Ayush Sekhari, Jayadev Acharya, Gautam Kamath, Ananda Theertha Suresh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.08763">Renyi Differential Privacy of The Subsampled Shuffle Model In Distributed Learning</a><br/>
Antonious Girgis, Deepesh Data, Suhas Diggavi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.09159">Robust and differentially private mean estimation</a><br/>
Xiyang Liu, Weihao Kong, Sham Kakade, Sewoong Oh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.04995">The Skellam Mechanism for Differentially Private Federated Learning</a><br/>
Naman Agarwal, Peter Kairouz, Ken Liu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.11208">User-Level Differentially Private Learning via Correlated Sampling</a><br/>
Badih Ghazi, Ravi Kumar, Pasin Manurangsi</p>
  </li>
</ul></div>
    </summary>
    <updated>2021-11-09T15:00:00Z</updated>
    <published>2021-11-09T15:00:00Z</published>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-11-11T23:09:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/09/postdoc-positions-in-quantum-algorithms-at-irif-cnrs-paris-apply-by-december-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/09/postdoc-positions-in-quantum-algorithms-at-irif-cnrs-paris-apply-by-december-15-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc positions in quantum algorithms at IRIF, CNRS (Paris) (apply by December 15, 2021)</title>
    <summary>IRIF (Paris, France) is offering multiple postdoc positions to work on the theory of quantum computing. Emphasis is on the development of quantum algorithms for optimization, machine learning, massive data, and cryptography. You will be working with permanent members S. Apers, I. Kerenidis, S. Laplante and F. Magniez, and a strong team of postdocs and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>IRIF (Paris, France) is offering multiple postdoc positions to work on the theory of quantum computing. Emphasis is on the development of quantum algorithms for optimization, machine learning, massive data, and cryptography. You will be working with permanent members S. Apers, I. Kerenidis, S. Laplante and F. Magniez, and a strong team of postdocs and PhDs. Starting date: around September 2022.</p>
<p>Website: <a href="https://www.irif.fr/postes/postdoc#quantum-computing">https://www.irif.fr/postes/postdoc#quantum-computing</a><br/>
Email: simon.apers@inria.fr</p></div>
    </content>
    <updated>2021-11-09T14:43:53Z</updated>
    <published>2021-11-09T14:43:53Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-12T04:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/153</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/153" rel="alternate" type="text/html"/>
    <title>TR21-153 |  On Hardness Assumptions Needed for &amp;quot;Extreme High-End&amp;quot; PRGs and Fast Derandomization | 

	Ronen Shaltiel, 

	Emanuele Viola</title>
    <summary>The hardness vs.~randomness paradigm aims to explicitly construct pseudorandom generators $G:\{0,1\}^r \to \{0,1\}^m$ that fool circuits of size $m$, assuming the existence of explicit hard functions. A ``high-end PRG'' with seed length $r=O(\log m)$ (implying BPP=P) was achieved in a seminal work of Impagliazzo and Wigderson (STOC 1997), assuming \textsc{the high-end hardness assumption}: there exist constants $0&lt;\beta &lt; 1&lt; B$, and functions computable in time $2^{B \cdot n}$ that cannot be computed by circuits of size $2^{\beta \cdot n}$.

Recently, motivated by fast derandomization of randomized algorithms, Doron et al.~(FOCS 2020) and Chen and Tell (STOC 2021), construct ``extreme high-end PRGs'' with seed length $r=(1+o(1))\cdot \log m$, under qualitatively stronger assumptions.

We study whether extreme high-end PRGs can be constructed from the corresponding hardness assumption in which $\beta=1-o(1)$ and $B=1+o(1)$, which we call \textsc{the extreme high-end hardness assumption}. We give a partial negative answer:

\begin{itemize}
\item The construction of Doron et al. composes a PEG (pseudo-entropy generator) with an extractor.  The PEG is constructed starting from a function that is hard for MA-type circuits.  We show that black-box PEG constructions from \textsc{the extreme high-end hardness assumption} must have large seed length (and so cannot be used to obtain extreme high-end PRGs by applying an extractor).

To prove this, we establish a new property of (general) black-box PRG constructions from hard functions: it is possible to fix many output bits of the construction while fixing few bits of the hard function. This property distinguishes PRG constructions from typical extractor constructions, and this may explain why it is difficult to design PRG constructions.

\item The construction of Chen and Tell composes two PRGs: $G_1:\{0,1\}^{(1+o(1)) \cdot \log m} \to \{0,1\}^{r_2=m^{\Omega(1)}}$ and $G_2:\{0,1\}^{r_2} \to \{0,1\}^m$.  The first PRG is constructed from \textsc{the extreme high-end hardness assumption}, and the second PRG needs to run in time $m^{1+o(1)}$, and is constructed assuming one way functions. We show that in black-box proofs of hardness amplification to $\frac{1}{2}+1/m$, reductions must make $\Omega(m)$ queries, even in the extreme high-end. Known PRG constructions from hard functions are black-box and use (or imply) hardness amplification, and so cannot be used to construct a PRG $G_2$ from \textsc{the extreme high-end hardness assumption}.

The new feature of our hardness amplification result is that it applies even to the extreme high-end setting of parameters, whereas past work does not. Our techniques also improve recent lower bounds of Ron-Zewi, Shaltiel and Varma (ITCS 2021) on the number of queries of local list-decoding algorithms.
\end{itemize}</summary>
    <updated>2021-11-09T11:08:53Z</updated>
    <published>2021-11-09T11:08:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-12T04:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/152</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/152" rel="alternate" type="text/html"/>
    <title>TR21-152 |  Min-Entropic Optimality | 

	Tomer Grossman, 

	Gal Arnon</title>
    <summary>We introduce the notion of \emph{Min-Entropic Optimality} thereby providing a framework for arguing that a given algorithm computes a function better than any other algorithm. An algorithm is $k(n)$ Min-Entropic Optimal if for every distribution $D$ with min-entropy at least $k(n)$, its expected running time when its input is drawn from $D$ is at most a multiplicative constant larger than the expected running time (also with respect to $D$) of any other algorithm that computes the same function. Min-Entropic Optimality is a relaxation of the well established notion of instance optimality (when $k(n) = 0$). Thereby, Min-Entropic Optimality provides a meaningful notion of optimality, even in scenarios where instance optimality is inherently impossible to achieve (for instance, in the super-linear regime).

We analyze basic properties of this notion and prove that for many values of $k(n)$ there exist functions that have Min-Entropic Optimal algorithms. We further show that some natural search problems, such as $k$-sum, are unlikely to have optimal algorithms under this notion.</summary>
    <updated>2021-11-09T11:01:40Z</updated>
    <published>2021-11-09T11:01:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-12T04:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19306</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/" rel="alternate" type="text/html"/>
    <title>The Artificial Intelligence Historian</title>
    <summary>The past actually happened but history is only what someone wrote down—Whitney Brown CMU tribute Pamela McCorduck passed away last month. The New York Times obituary notes her interactions with many builders of the field of Artificial Intelligence from its infancy to its present state. Today we remember her and talk about AI’s near future. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>The past actually happened but history is only what someone wrote down—Whitney Brown</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/mccorduck-obit-700x700-min/" rel="attachment wp-att-19317"><img alt="" class="alignright wp-image-19317" height="156" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/mccorduck-obit-700x700-min.jpeg?resize=125%2C156&amp;ssl=1" width="125"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">CMU <a href="https://www.cmu.edu/news/stories/archives/2021/october/mccorduck-obit.html">tribute</a></font></td>
</tr>
</tbody>
</table>
<p>
Pamela McCorduck passed away last month. The New York Times <a href="https://www.nytimes.com/2021/11/04/technology/pamela-mccorduck-dead.html">obituary</a> notes her interactions with many builders of the field of Artificial Intelligence from its infancy to its present state. </p>
<p>
Today we remember her and talk about AI’s near future.</p>
<p>
I knew McCorduck through her late husband, Joe Traub, who we <a href="https://rjlipton.wpcomstaging.com/2015/08/31/how-joe-traub-beat-the-street/">memorialized</a> in 2015. He became the head of the CS department at Carnegie Mellon University in 1971. She also moved to CMU where she became an English teacher. Per the above quote by Brown, she helped make AI real by writing a number of books on its history. </p>
<p>
The NYT obit quotes something McCorduck wrote in her 2019 <a href="https://press.etc.cmu.edu/index.php/product/this-could-be-important/">memoir</a>, <em>This Could Be Important: My Life and Times With the Artificial Intelligentsia.</em> </p>
<blockquote><p><b> </b> <em> “For 60 years, I’ve lived in AI’s exponential. I’ve watched computers evolve from plodding sorcerer’s apprentices to machines that can best any humans at checkers, then chess, then the guessing game Jeopardy!, and now the deeply complex game of Go.” </em>
</p></blockquote>
<p/><p>
It is hard to project the future of an exponential, however. The best way I can try is to align it with my own field.</p>
<p>
</p><p/><h2> AI Movers and Shakers </h2><p/>
<p/><p>
At CMU McCorduck got to know the AI pioneers like Turing Award recipients Herbert Simon and Allen Newell and Raj Reddy. She already knew Edward Feigenbaum who said:</p>
<blockquote><p><b> </b> <em> She was dumped into this saturated milieu of the great and greatest in AI at Carnegie Mellon—some of the same people whose papers she’d helped us assemble—and decided to write a history of the field. </em>
</p></blockquote>
<p/><p>
The <a href="https://www.routledge.com/Machines-Who-Think-A-Personal-Inquiry-into-the-History-and-Prospects-of/McCorduck/p/book/9781568812052">book</a> was <em>Machines Who Think: A Personal Inquiry Into the History and Prospects of Artificial Intelligence.</em> Said Simon: </p>
<blockquote><p><b> </b> <em> She was interacting with all the movers and shakers of AI. She was in the middle of it, an eyewitness to history. </em>
</p></blockquote>
<p/><p>
I wish I knew more of her thoughts on the movers and shakers in Theory. She was well-versed in complexity of the dynamical-systems kind, and her husband’s work bridged to “our kind” of complexity. The title of her third <a href="https://www.amazon.com/Bounded-Rationality-Novel-Pamela-McCorduck/dp/0865348839">novel</a>, <em>Bounded Rationality</em>, speaks to both kinds of complexity from its setting at the Santa Fe Institute. This has led me to musing on the difference between AI and Theory.</p>
<p>
</p><p/><h2> AI Beats Theory </h2><p/>
<p/><p>
I never have worked on AI problems of any kind. The closest I ever came is I took a class at CMU as graduate student from Newell. He was a fun lecturer and the class was interesting. But I always worked on Theory. I must reflect a bit on why AI is so successful and Theory is less so. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/see/" rel="attachment wp-att-19311"><img alt="" class="aligncenter size-full wp-image-19311" height="195" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/see.png?resize=258%2C195&amp;ssl=1" width="258"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Pinterest <a href="https://www.pinterest.com/pin/55872851604309287/">source</a></font>
</td>
</tr>
</tbody></table>
<p>
Let’s start by saying that a field of research is determined not by who works in the field. Not by the tools that the field uses. It is determined by the problems that the field works on. AI is different from Theory because of the problems that it studies. This is the fundamental difference:</p>
<blockquote><p><b> </b> <em> AI looks at whole problems; Theory looks at sub-problems. </em>
</p></blockquote>
<p/><p>
What do I mean? AI studies problems that are concrete, that are big, that are as close to real problems as possible. For example, how to play Go or how to recognize images of faces. AI looks at problems that humans actually wish to solve: </p>
<blockquote><p><b> </b> <em> What move to make in this Go position? Or is this an image of X or Y? </em>
</p></blockquote>
<p/><p>
Theory looks at sub-problems. We look at a real problem and then identify some part of the problem that is hard to solve. We then try to invoke clever methods that show that this sub-problem can be done more efficiently that was previous known. This is hard in general. Is fun to work on in general. And leads to a beautiful field of study. One that is deep and rewarding. </p>
<p>
But Theory loses to AI. The issue is that no one really may wish to solve the sub-problem. This is real demand to solve the whole problem, but not the sub-problem. This is the fundamental advantage that AI holds over Theory.</p>
<p>
</p><p/><h2> AI Future </h2><p/>
<p/><p>
Public figures such as Stephen Hawking and Elon Musk have expressed concern that full artificial intelligence (AI) could result in human extinction. The consequences of the technological <a href="https://en.wikipedia.org/wiki/Technological_singularity">singularity</a> and its potential benefit or harm to the human race have been intensely debated.</p>
<p>
For our own part, we have <a href="https://rjlipton.wpcomstaging.com/2011/02/17/are-mathematicians-in-jeopardy/">wondered</a> whether an AI can take over in theory research. This puts a second light on possible meanings of “problems” in another quotation by McCorduck from her memoir, as related <a href="https://computerhistory.org/blog/the-future-humans-and-ai/">here</a>:</p>
<blockquote><p><b> </b> <em> “We can’t now say what living beside other, in some ways superior, intelligences will mean to us. Will it widen and raise our own individual and collective intelligence? In significant ways, it already has. Find solutions to problems we could never solve? Probably. Find solutions to problems we lack the wit even to propose? Maybe. Cause problems? Surely. AI has already shattered some of our fondest myths about ourselves and has shone unwelcome light on others. This will continue.</em></p><em>
<p>
…</p>
</em><p><em>
When people ask me my greatest worry about AI, I say: what we aren’t smart enough even to imagine.” </em>
</p></blockquote>
<p/><p>
Well, we can only talk about things we can imagine now. We can discuss facets of life that already outsource decisions to technology, such as high-speed stock trading and <a href="https://en.wikipedia.org/wiki/2010_flash_crash">several</a> <a href="https://www.technologyreview.com/2016/10/07/244656/algorithms-probably-caused-a-flash-crash-of-the-british-pound/">flash</a>–<a href="https://www.motherjones.com/politics/2013/02/high-frequency-trading-danger-risk-wall-street/">crashes</a> it has caused. </p>
<p>
But looking ahead, what is one near-term application area as a litmus test for the impact of AI? We think many will agree with our looking to <em>self-driving cars</em>. In taking over driving decisions, the AI expressly aims to reduce the evils of impaired or aggressive drivers. There have been <a href="https://www.washingtonpost.com/technology/2021/11/08/tesla-regulation-elon-musk/">mishaps</a> during development, sure, and the algorithms have not yet demonstrated robustness against possible deceptions. That is to say:</p>
<ul>
<li>
We can already see teething problems with this tech and imagine more along the same lines. <p/>
</li><li>
But can we project structural problems with the driverless paradigm whose concrete forms we have not imagined?
</li></ul>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What are your thoughts on the near future of AI? </p>
<p>
Our condolences go out to Pamela’s family and associates.</p>
<p/></font></font></div>
    </content>
    <updated>2021-11-08T22:23:34Z</updated>
    <published>2021-11-08T22:23:34Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="AI"/>
    <category term="artificial intelligence"/>
    <category term="in memoriam"/>
    <category term="Pamela McCorduck"/>
    <category term="singularity"/>
    <category term="Theory"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-11-12T04:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/151</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/151" rel="alternate" type="text/html"/>
    <title>TR21-151 |  Locally Testable Codes with constant rate, distance, and locality  | 

	Irit Dinur, 

	Shai Evra, 

	Ron Livne, 

	Alexander Lubotzky, 

	Shahar Mozes</title>
    <summary>A locally testable code (LTC) is an error correcting code that has a property-tester. The tester reads $q$ bits that are randomly chosen, and rejects words with probability proportional to their distance from the code. The parameter $q$ is called the locality of the tester.

LTCs were initially studied as important components of PCPs, and since then the topic has evolved on its own. High rate LTCs could be useful in practice: before attempting to decode a received word, one can save time by first quickly testing if it is close to the code.

An outstanding open question has been whether there exist "$c^3$-LTCs", namely LTCs with *c*onstant rate, *c*onstant distance, and *c*onstant locality.

In this work we construct such codes based on a new two-dimensional complex which we call a left-right Cayley complex. This is essentially a graph which, in addition to vertices and edges, also has squares. Our codes can be viewed as a two-dimensional version of (the one-dimensional) expander codes, where the codewords are functions on the squares rather than on the edges.</summary>
    <updated>2021-11-08T20:01:24Z</updated>
    <published>2021-11-08T20:01:24Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-12T04:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8225</id>
    <link href="https://windowsontheory.org/2021/11/08/new-quantum-science-and-engineering-phd-program/" rel="alternate" type="text/html"/>
    <title>New Quantum Science and Engineering PhD Program</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[Please forward this information to any undergraduate students in your program, or any relevant mailing lists or organizations you are aware of; we also have a postdoctoral fellowship in quantum computing. –Boaz] This year Harvard started a new Ph.D program in Quantum Science and Engineering. We are now accepting application for the first cohort of … <a class="more-link" href="https://windowsontheory.org/2021/11/08/new-quantum-science-and-engineering-phd-program/">Continue reading <span class="screen-reader-text">New Quantum Science and Engineering PhD Program</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Please forward this information to any undergraduate students in your program, or any relevant mailing lists or organizations you are aware of; we also have <a href="https://quantum.harvard.edu/external-candidates">a postdoctoral fellowship in quantum computing</a>. –Boaz]</em></p>



<p>This year Harvard started a new <a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering">Ph.D program in Quantum Science and Engineering</a>. We are now accepting application for the first cohort of students which will start in the 2022-2023 academic year.  This program can be an excellent fit for CS majors that are interested in quantum computation and information, and are looking for an interdisciplinary environment, with students from CS, physics, and other backgrounds, and with a curriculum and program that is designed with quantum computing in mind. Students can apply to both the QSE program and the <a href="https://gsas.harvard.edu/programs-of-study/all/computer-science">CS PhD program</a>, in parallel.</p>



<p>Some more information below:</p>



<p>The <strong><a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering">Harvard Quantum Science and Engineering (QSE) PhD program</a></strong> is designed for students like yours, as well as those studying engineering, physics and chemistry. The program brings these students from diverse undergraduate programs together through a world-class, integrated QSE PhD program that uniquely prepares them to become intellectual leaders and innovators in the burgeoning field of QSE. </p>



<p><strong>The curriculum. </strong>The integrated curriculum provides a shared foundation and QSE language that enables students to make discoveries and collaborate fluently beyond traditional disciplinary boundaries. Students enjoy the freedom to broadly explore their interests, specialize in their area of greatest interest, and can choose PhD advisors from across all quantum departments including computer science, physics, engineering, and chemistry.</p>



<p><strong>The community. </strong>The Harvard QSE PhD program is built around a supportive environment and collaborative research community that helps nurture each student and ensure their success. It’s an unprecedented opportunity to work with world leaders in the field of QSE in state-of-the-art educational and computational facilities while participating in cutting-edge research. </p>



<p><strong>The opportunity. </strong>Graduates of the program will be trained as the next generation of world leaders in the field of QSE. With their broad, yet deep educational foundation, guided by their own interests, they’ll be ready to take on exciting roles in industry, academia, and national laboratories. </p>



<p>Deadline to apply is <strong>December 15, 2021</strong>. Apply via <a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering" rel="noreferrer noopener" target="_blank">https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering</a> . Students can also apply in parallel to the CS PhD program via <a href="https://gsas.harvard.edu/programs-of-study/all/computer-science" rel="noreferrer noopener" target="_blank">https://gsas.harvard.edu/programs-of-study/all/computer-science</a> . Email <a href="mailto:qse-admissions@fas.harvard.edu" rel="noreferrer noopener" target="_blank">qse-admissions@fas.harvard.edu</a> with any questions.</p>



<p/></div>
    </content>
    <updated>2021-11-08T15:43:33Z</updated>
    <published>2021-11-08T15:43:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-11-12T04:21:02Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-6759625364649289630</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/6759625364649289630/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=6759625364649289630" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/6759625364649289630" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/6759625364649289630" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2021/11/postdoc-call-for-fodsi.html" rel="alternate" type="text/html"/>
    <title>Postdoc call for FODSI</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As a member of FODSI (Foundations of Data Science Institute -- an NSF funded institute with the aim of advancing theoretical foundations for data science), I'm self-interestedly posting the call for postdocs for this year.  Two of the areas are  a) Sketching, Sampling, and Sublinear-Time Algorithms   and b)  Machine Learning for Algorithms (which includes what I call "Algorithms with Predictions.")  I'd be happy to see postdoc applications in those areas from people who want to spend some time at Harvard, for example.... but of course there are lots of other exciting things going on with FODSI too and you should take a look.</p><p>The call is at <a href="https://academicjobsonline.org/ajo/jobs/20132">https://academicjobsonline.org/ajo/jobs/20132</a>  </p><p>Call text below:</p><table align="center" border="1" class="ads" style="border-collapse: collapse; border: 1px solid rgb(204, 204, 204); color: #2a2a2a; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 13.3333px; padding: 5px; width: 95%px;"><tbody><tr><td style="border-collapse: collapse; border: 1px solid rgb(204, 204, 204); padding: 5px;">The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College. It provides a structured environment for exploring interdisciplinary research in foundations of Data Science spanning Mathematics, Statistics, Theoretical Computer Science and other fields.<p>We are looking for multiple postdoctoral team members who will collaborate with <a href="https://fodsi.us/team.html" style="color: #005f6f; font-weight: 700;">FODSI researchers</a> at one or more of the participating institutions. These positions emphasize strong mentorship, flexibility, and breadth of collaboration opportunities with other team members -- senior and junior faculty, postdocs, and graduate students at various nodes around the country. Furthermore, postdoctoral fellows will be able to participate in workshops and other activities organized by FODSI.</p><p>The fellowship is a one-year full-time appointment, with the possibility of renewal for a second year (based upon mutual agreement) either at the same or at another FODSI institution. The start date is flexible, although most appointments are expected to start in summer 2022. Candidates are encouraged to apply to work with more than one faculty mentor <b>at one or more participating institutions</b> (in-person mentoring is preferred, but remote options will be also considered). The applicants should have an excellent theoretical background and a doctorate in a related field, including Mathematics, Statistics, Computer Science, Electrical Engineering or Economics. We particularly encourage applications from women and minority candidates.</p><p>The review process will start on November 15, 2021 and will continue until positions are filled.</p></td></tr></tbody></table></div>
    </content>
    <updated>2021-11-08T14:45:00Z</updated>
    <published>2021-11-08T14:45:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2021-11-10T02:05:19Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6799651120968614396</id>
    <link href="http://blog.computationalcomplexity.org/feeds/6799651120968614396/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/reflections-on-trusting-trustlessness.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6799651120968614396" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6799651120968614396" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/reflections-on-trusting-trustlessness.html" rel="alternate" type="text/html"/>
    <title>Reflections on Trusting ``Trustlessness'' in the era of ``Crypto'' Blockchains (Guest Post)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> </p><p><i>I trust Evangelos Georgiadis to do a guest post on Trust and Blockchain. </i></p><div>Today we have a guest post by Evangelos Georgiadis on Trust. It was written before Lance's post on trust <a href="https://blog.computationalcomplexity.org/2021/08/trusting-scientists.html">here</a> but it can be viewed as a followup to it. </div><div><br/></div><div>And now, here's E.G:</div><div><div><br/></div><div>==========================================================</div><div><br/></div><div>Trust is a funny concept, particularly in the realm of blockchains and "crypto".</div><div><br/></div><div>Do you trust the consensus mechanism of a public blockchain?</div><div><br/></div><div>Do you trust the architects that engineered the consensus mechanism?</div><div><br/></div><div>Do you trust the software engineers that implemented the code for the consensus mechanism?</div><div><br/></div><div>Do you trust the language that the software engineers used?</div><div><br/></div><div>Do you trust the underlying hardware that that the software is running?</div><div><br/></div><div>Theoretical Computer Science provides tools for some of this. But then the question becomes</div><div>Do you trust the program verifier?</div><div>Do you trust the proof of security?</div><div><br/></div><div>I touch on these issues in: </div><div><br/></div><div>                   <i>Reflections on Trusting ‘Trustlessness’ in the era of ”Crypto”/Blockchains</i></div><div><br/></div><div> which is <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/cbit-4-2.pdf">here</a>. Its only 3 pages so enjoy!</div></div></div>
    </content>
    <updated>2021-11-07T20:47:00Z</updated>
    <published>2021-11-07T20:47:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-11-11T15:44:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/150</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/150" rel="alternate" type="text/html"/>
    <title>TR21-150 |  Extractors: Low Entropy Requirements Colliding With Non-Malleability | 

	Eldon Chung, 

	Maciej Obremski, 

	Divesh Aggarwal</title>
    <summary>The known constructions of negligible error (non-malleable) two-source extractors can be broadly classified in three categories:

(1) Constructions where one source has min-entropy rate about $1/2$, the other source can have small min-entropy rate, but the extractor doesn't guarantee non-malleability.
(2) Constructions where one source is uniform, and the other can have small min-entropy rate, and the extractor guarantees non-malleability when the uniform source is tampered.
(3) Constructions where both sources have entropy rate very close to $1$ and the extractor guarantees non-malleability against the tampering of both sources. 

We introduce a new notion of collision resistant extractors and in using it we obtain a strong two source non-malleable extractor where we require the first source to have $0.8$ entropy rate and the other source can have min-entropy polylogarithmic in the length of the source.  

We show how the above extractor can be applied to obtain a non-malleable extractor with output rate $\frac 1 2$, which is optimal. We also show how, by using our extractor and extending the known protocol, one can  obtain a privacy amplification secure against memory tampering where the size of the secret output is almost optimal.</summary>
    <updated>2021-11-07T20:13:16Z</updated>
    <published>2021-11-07T20:13:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-12T04:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/149</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/149" rel="alternate" type="text/html"/>
    <title>TR21-149 |  On polynomially many queries to NP or QMA oracles | 

	Dorian Rudolph, 

	Sevag Gharibian</title>
    <summary>We study the complexity of problems solvable in deterministic polynomial time with access to an NP or Quantum Merlin-Arthur (QMA)-oracle, such as $P^{NP}$ and $P^{QMA}$, respectively.
The former allows one to classify problems more finely than the Polynomial-Time Hierarchy (PH), whereas the latter characterizes physically motivated problems such as Approximate Simulation (APX-SIM) [Ambainis, CCC 2014].
In this area, a central role has been played by the classes $P^{NP[\log]}$ and $P^{QMA[\log]}$, defined identically to $P^{NP}$ and $P^{QMA}$, except that only logarithmically many oracle queries are allowed. Here, [Gottlob, FOCS 1993] showed that if the adaptive queries made by a $P^{NP}$ machine have a "query graph" which is a tree, then this computation can be simulated in $P^{NP[\log]}$.

 In this work, we first show that for any verification class $C\in\{NP,MA,QCMA,QMA,QMA(2),NEXP,QMA_{\exp}\}$, any $P^C$ machine with a query graph of "separator number" $s$ can be simulated using deterministic time $\exp(s\log n)$ and $s\log n$ queries to a $C$-oracle.
When $s\in O(1)$ (which includes the case of $O(1)$-treewidth, and thus also of trees), this gives an upper bound of $P^{C[\log]}$, and when $s\in O(\log^k(n))$, this yields bound $QP^{C[\log^{k+1}]}$ (QP meaning quasi-polynomial time).
We next show how to combine Gottlob's "admissible-weighting function" framework with the "flag-qubit" framework of [Watson, Bausch, Gharibian, 2020], obtaining a unified approach for embedding $P^C$ computations directly into APX-SIM instances in a black-box fashion.
Finally, we formalize a simple no-go statement about polynomials (c.f. [Krentel, STOC 1986]): Given a multi-linear polynomial $p$ specified via an arithmetic circuit, if one can "weakly compress" $p$ so that its optimal value requires $m$ bits to represent, then $P^{NP}$ can be decided with only $m$ queries to an NP-oracle.</summary>
    <updated>2021-11-07T12:55:09Z</updated>
    <published>2021-11-07T12:55:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-12T04:20:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/</id>
    <link href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/" rel="alternate" type="text/html"/>
    <title>IDEAL mini-workshop on “New Directions on Robustness in ML”</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">November 16, 2021 Virtual https://www.ideal.northwestern.edu/events/mini-workshop-on-new-directions-on-robustness-in-ml/ As machine learning systems are being deployed in almost every aspect of decision-making, it is vital for them to be reliable and secure to adversarial corruptions and perturbations of various kinds. This workshop will explore newer notions of robustness and the different challenges that arise in designing reliable ML algorithms. … <a class="more-link" href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/">Continue reading <span class="screen-reader-text">IDEAL mini-workshop on “New Directions on Robustness in ML”</span></a></div>
    </summary>
    <updated>2021-11-07T04:05:12Z</updated>
    <published>2021-11-07T04:05:12Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-11-12T04:21:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1584</id>
    <link href="https://ptreview.sublinear.info/2021/11/news-for-october-2021/" rel="alternate" type="text/html"/>
    <title>News for October 2021</title>
    <summary>The month of September was quite busy, with seven papers, spanning (hyper)graphs, proofs, probability distributions, and sampling. Better Sum Estimation via Weighted Sampling, by Lorenzo Beretta and Jakub Tětek (arXiv). This paper considers the following question: “given a large universe of items, each with an unknown weight, estimate the total weight to a multiplicative \(1\pm […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The month of September was quite busy, with seven papers, spanning (hyper)graphs, proofs, probability distributions, and sampling.</p>



<p><strong>Better Sum Estimation via Weighted Sampling</strong>, by Lorenzo Beretta and Jakub Tětek (<a href="https://arxiv.org/abs/2110.14948">arXiv</a>). This paper considers the following question: “given a large universe of items, each with an unknown weight, estimate the total weight to a multiplicative \(1\pm \varepsilon\).” The key is in the type of access you have to those items: here, the authors consider the setting where items can be sampled proportionally to their unknown weights, and show improved bounds on the sample/query complexity in this model. And there something for everyone: they also discuss connections to edge estimation in graphs (assuming random edge queries) and to distribution testing (specifically, in the “dual” or “probability-revealing” models of Canonne–Rubinfeld and Onak–Sun).</p>



<p>This gives us an easy segue to distribution testing, which is the focus of the next two papers.</p>



<p><strong>As Easy as ABC: Adaptive Binning Coincidence Test for Uniformity Testing</strong>, by Sudeep Salgia, Qing Zhao, and Lang Tong (<a href="https://arxiv.org/abs/2110.06325">arXiv</a>). Most of the work in distribution testing (from the computer science community) focuses on discrete probability distributions, for several reasons. Including a technical one: total variation distance is rather fickle with continuous distributions, unless one makes some assumption on the unknown distribution. This paper does exactly this: assuming the unknown distribution has a Lipschitz density function, it shows how to test uniformity by adaptively discretizing the domain, achieving (near) sample complexity.</p>



<p><strong>Exploring the Gap between Tolerant and Non-tolerant Distribution Testing,</strong> by Sourav Chakraborty, Eldar Fischer, Arijit Ghosh, Gopinath Mishra, and Sayantan Sen (<a href="https://arxiv.org/abs/2110.09972">arXiv</a>). It is known that tolerant testing of distributions can be much harder than “standard” testing – for instance, for identity testing, the sample complexity can blow up by nearly a quadratic factor, from \(\sqrt{n}\) to \(\frac{n}{\log n}\)! But is it the worse that can happen, in general, for other properties? This work explores this question, and answers it in some notable cases of interest, such as for label-invariant (symmetric) properties.</p>



<p>And now, onto graphs!</p>



<p><strong>Approximating the Arboricity in Sublinear Time</strong>, by Talya Eden, Saleet Mossel, and Dana Ron (<a href="https://arxiv.org/abs/2110.15260">arXiv</a>). The arboricity of a graph is the minimal number of spanning forests required to cover all its edges. Many graph algorithms, especially sublinear-time ones, can be parameterized by this quantity: which is very useful, but what do you do if you don’t know the arboricity of your graph? Well, then you estimate it. Which this paper shows how to do efficiently, given degree and neighbor queries. Moreover, the bound they obtain — \(\tilde{O}(n/\alpha)\) queries to obtain a constant-factor approximation of the unknown arboricity \(\alpha\) — is optimal, up to logarithmic factors in the number of vertices \(n\).</p>



<p><strong>Sampling Multiple Nodes in Large Networks: Beyond Random Walks,</strong> by Omri Ben-Eliezer, Talya Eden, Joel Oren, and Dimitris Fotakis (<a href="https://arxiv.org/abs/2110.13324">arXiv</a>). Another thing which one typically wants to do with very large graphs is <em>sample nodes</em> from them, either uniformly or according to some prescribed distribution. This is a core building block in many other algorithms; unfortunately, approaches to do so via random walks will typically require a number of queries scaling with the mixing time \(t_{\rm mix}(G)\) of the graph \(G\), which might be very small for nicely expanding graphs, but not so great in many practical settings. This paper proposes and experimentally evaluates a different algorithm which bypasses this linear dependence on \(t_{\rm mix}(G)\), by first going through a random-walk-based “learning” phase (learn something about the structure of the graph) before using this learned structure to perform faster sampling, focusing on small connected components.</p>



<p>Why stop at graphs? <em>Hypergraphs</em>!</p>



<p><strong>Hypergraph regularity and random sampling,</strong> by Felix Joos, Jaehoon Kim, Daniela Kühn, Deryk Osthus (<a href="https://arxiv.org/abs/2110.01570">arXiv</a>). The main result in this paper is a hypergraph analogue of a result of Alon, Fischer, Newman and Shapira (for graphs), which roughly states that if a hypergraph satisfies some regularity condition, then so does with high probability a randomly sampled sub-hypergraph — and conversely. This in turn has direct implications to characterizing which hypergraph properties are testable: see the <a href="https://arxiv.org/abs/1707.03303">companion paper</a>, <em>b</em>y the same authors.<em><br/>(Note: this paper is a blast from the past, as the result it shows was originally established in the linked companion paper, from 2017; however, the authors split this paper in two this October, leading to this new, standalone paper.)</em></p>



<p>And, to conclude, Arthur, Merlin, and proofs:</p>



<p><strong>Sample-Based Proofs of Proximity,</strong> by Guy Goldberg, Guy Rothblum (<a href="https://eccc.weizmann.ac.il/report/2021/146/">ECCC</a>). Finally, consider the setting of interactive proofs of proximities (IPPs), where the prover is as usual computationally unbounded, but the verifier must run in sublinear time (à la property testing). This has received significant interest in the past years: but what if the verifier didn’t even get to make queries, but only got access to <em>uniformly random location</em>s of the input? These “SIPP” (Sample-based IPPs), and their non-interactive counterpart SAMPs (Sample-based Merlin-Arthur Proofs of Proximity) are the object of study of this paper, which it introduces and motivates in the context, for instance, of delegation of computation for sample-based algorithms.</p></div>
    </content>
    <updated>2021-11-07T02:17:23Z</updated>
    <published>2021-11-07T02:17:23Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-11-11T23:09:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at Foundations of Data Science Institute (FODSI) (apply by November 15, 2021)</title>
    <summary>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College. Website: https://academicjobsonline.org/ajo/jobs/20132 Email: See the url</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/20132">https://academicjobsonline.org/ajo/jobs/20132</a><br/>
Email: See the url</p></div>
    </content>
    <updated>2021-11-05T22:17:02Z</updated>
    <published>2021-11-05T22:17:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-12T04:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at Computer Science, University of Victoria (apply by November 20, 2021)</title>
    <summary>Bruce Kapron invites applications for a postdoc in CS at the University of Victoria. Applicants with a background or interest in higher-order complexity theory, including models and techniques related to theory of programming languages, feasible analysis, cryptography, and ordinary complexity theory are encouraged. Applicants should have a Ph.D. in CS, Mathematics, Logic or a related […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Bruce Kapron invites applications for a postdoc in CS at the University of Victoria. Applicants with a background or interest in higher-order complexity theory, including models and techniques related to theory of programming languages, feasible analysis, cryptography, and ordinary complexity theory are encouraged. Applicants should have a Ph.D. in CS, Mathematics, Logic or a related field.</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/list/18864">https://www.mathjobs.org/jobs/list/18864</a><br/>
Email: bmkapron@uvic.ca</p></div>
    </content>
    <updated>2021-11-05T16:54:41Z</updated>
    <published>2021-11-05T16:54:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-12T04:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/</id>
    <link href="https://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/" rel="alternate" type="text/html"/>
    <title>Assistant Professor at Charles University (apply by January 31, 2022)</title>
    <summary>The Computer Science Institute of Charles University, Prague, Czech Republic, invites applications for an assistant professor in the area of theoretical computer science to complement and/or strengthen existing research areas (which include computational complexity, cryptography, algorithms, combinatorics, and discrete mathematics). Strong candidates from all areas of TCS will be considered. Website: https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022 Email: koucky@iuuk.mff.cuni.cz</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science Institute of Charles University, Prague, Czech Republic, invites applications for an assistant professor in the area of theoretical computer science to complement and/or strengthen existing research areas (which include computational complexity, cryptography, algorithms, combinatorics, and discrete mathematics). Strong candidates from all areas of TCS will be considered.</p>
<p>Website: <a href="https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022">https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022</a><br/>
Email: koucky@iuuk.mff.cuni.cz</p></div>
    </content>
    <updated>2021-11-05T13:35:13Z</updated>
    <published>2021-11-05T13:35:13Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-12T04:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/" rel="alternate" type="text/html"/>
    <title>Summer Research Intern at Adobe Research  (apply by December 31, 2021)</title>
    <summary>Summer (TCS) Research Intern positions are available to work with Zhao Song at Adobe Research. The position is for 3-4 months in summer 2022, start date flexible. Applications will be reviewed on a rolling basis, with preference given to ones submitted before ddl. Potential project topics include but are not limited to general algorithmic topics. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Summer (TCS) Research Intern positions are available to work with Zhao Song at Adobe Research. The position is for 3-4 months in summer 2022, start date flexible. Applications will be reviewed on a rolling basis, with preference given to ones submitted before ddl. Potential project topics include but are not limited to general algorithmic topics. Interested candidates should send their CV to Zhao.</p>
<p>Website: <a href="https://scholar.google.com/citations?user=yDZct7UAAAAJ&amp;hl=en">https://scholar.google.com/citations?user=yDZct7UAAAAJ&amp;hl=en</a><br/>
Email: zsong@adobe.com</p></div>
    </content>
    <updated>2021-11-05T04:50:32Z</updated>
    <published>2021-11-05T04:50:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-12T04:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/04/faculty-at-university-of-haifa-at-oranim-college-apply-by-december-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/04/faculty-at-university-of-haifa-at-oranim-college-apply-by-december-31-2021/" rel="alternate" type="text/html"/>
    <title>Faculty at University of Haifa at Oranim College (apply by December 31, 2021)</title>
    <summary>The Department of Mathematics-Physics-Computer Science of the University of Haifa at Oranim College invites applications for a tenure-track faculty position in all areas of Computer Science, to begin October 1st 2022. Website: https://mathphys.haifa.ac.il/en/announcements/ Email: ackerman@sci.haifa.ac.il</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Mathematics-Physics-Computer Science of the University of Haifa at Oranim College invites applications for a tenure-track faculty position in all areas of Computer Science, to begin October 1st 2022.</p>
<p>Website: <a href="https://mathphys.haifa.ac.il/en/announcements/">https://mathphys.haifa.ac.il/en/announcements/</a><br/>
Email: ackerman@sci.haifa.ac.il</p></div>
    </content>
    <updated>2021-11-04T13:48:18Z</updated>
    <published>2021-11-04T13:48:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-12T04:20:45Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-3367673395710171015</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/3367673395710171015/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=3367673395710171015" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/3367673395710171015" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/3367673395710171015" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2021/11/hotnets-presentation-zero-cpu.html" rel="alternate" type="text/html"/>
    <title>HotNets Presentation : Zero-CPU Collection with Direct Telemetry Access</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>HotNets has asked that we let people know that the 2021 presentations <a href="https://www.youtube.com/channel/UCZZ5nf4RNDIIe4nifI8grwQ/videos">are available here</a>.  I'm using that an excuse to highlight our paper on Zero-CPU Collection with Direct Telemetry Access (<a href="https://arxiv.org/abs/2110.05438">arxiv version here</a>), but really I want to highlight the talk by graduate student Jonatan Langlet (Queen Mary University of London) who, as is the nature of graduate students, did all of the real work, and who really did a great job on <a href="https://www.youtube.com/watch?v=_M8AbF_f8Kk&amp;t=2s">the talk (direct link)</a>.  If you guessed from my involvement this involves hashing in some way, your maximum likelihood estimate turns out to be correct.</p><p>I think our work fits the HotNets call, which asks for new approaches and preliminary work.  Specifically, the call for the HotNets workshop says this:</p><p/><blockquote><p>We invite researchers and practitioners to submit short position papers. We encourage papers that identify fundamental open questions, advocate a new approach, offer a constructive critique of the state of networking research, re-frame or debunk existing work, report unexpected early results from a deployment, report on promising but unproven ideas, or propose new evaluation methods. Novel ideas need not be supported by full evaluations; well-reasoned arguments or preliminary evaluations can support the possibility of the paper’s claims.</p><p>We seek early-stage work, where the authors can benefit from community feedback. An ideal submission has the potential to open a line of inquiry for the community that results in multiple conference papers in related venues (SIGCOMM, NSDI, CoNEXT, SOSP, OSDI, MobiCom, MobiSys, etc.), rather than a single follow-on conference paper. The program committee will explicitly favor early work and papers likely to stimulate reflection and discussion over “conference papers in miniature”.</p></blockquote><p>There are similar other "Hot" workshops in other areas, and it was about <a href="http://mybiasedcoin.blogspot.com/2007/08/hottheory-workshop.html">14 years ago that I asked whether CS theory should have a HotTheory workshop</a>.  There's been a proliferation of new conferences and workshops in theory since then, but none of them really seem to have this flavor.  So maybe it's worth asking again whether a HotTheory workshop would make sense?  Or do existing theory events meet the theory community needs?</p><p/><p><br/></p></div>
    </content>
    <updated>2021-11-04T13:01:00Z</updated>
    <published>2021-11-04T13:01:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2021-11-10T02:05:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/04/lecturer-in-theoretical-computer-science-sheffield-uk-at-university-of-sheffield-apply-by-november-16-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/04/lecturer-in-theoretical-computer-science-sheffield-uk-at-university-of-sheffield-apply-by-november-16-2021/" rel="alternate" type="text/html"/>
    <title>Lecturer in Theoretical Computer Science, Sheffield (UK) at University of Sheffield (apply by November 16, 2021)</title>
    <summary>The University of Sheffield has an opening for a Lecturer in Theoretical Computer Science. Researchers in the area of computational complexity, where the interests of the Algorithms and Verification groups in the Department overlap, are particularly encouraged to apply. Website: https://www.jobs.ac.uk/job/CKB031/lecturer-in-theoretical-computer-science Email: g.j.brown@sheffield.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The University of Sheffield has an opening for a Lecturer in Theoretical Computer Science. Researchers in the area of computational complexity, where the interests of the Algorithms and Verification groups in the Department overlap, are particularly encouraged to apply.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CKB031/lecturer-in-theoretical-computer-science">https://www.jobs.ac.uk/job/CKB031/lecturer-in-theoretical-computer-science</a><br/>
Email: g.j.brown@sheffield.ac.uk</p></div>
    </content>
    <updated>2021-11-04T12:46:50Z</updated>
    <published>2021-11-04T12:46:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-12T04:20:44Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2021/11/04/perceptron/</id>
    <link href="http://benjamin-recht.github.io/2021/11/04/perceptron/" rel="alternate" type="text/html"/>
    <title>The Perceptron as a prototype for machine learning theory.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Just as many of the algorithms and community practices of machine learning were invented <a href="http://www.argmin.net/2021/10/20/highleyman/">in the late 1950s and early 1960s</a>, the foundations of machine learning theory were also established during this time. Many of the analyses of this period were strikingly simple, had surprisingly precise constants, and provided prescient guidelines for contemporary machine learning practice. Here, I’ll summarize the study of the Perceptron, highlighting both its algorithmic and statistical analyses, and using it as a prototype to illustrate further how prediction deviates from the umbrella of classical statistics.</p>

<p>Let’s begin with a classification problem where each individual from some population has a feature vector $x$ and an associated binary label $y$ that we take as valued $\pm 1$ for notational convenience. The goal of the Perceptron is to find a linear separator such that $\langle w, x \rangle&gt;0$ for when $y=1$ and $\langle w, x \rangle&lt;0$ when $y=-1$. We can write this compactly as saying that we want to find a $w$ for which $y \langle w, x \rangle &gt;0$ for as many individuals in the population as possible.</p>

<p>Rosenblatt’s Perceptron provides a simple algorithm for finding such a $w$. The Perceptron inputs an example, checks if it makes the correct classification. If yes, it does nothing and proceeds to the next example. If no, the decision boundary is nudged in the direction of classifying the example correctly next time.</p>

<p><strong>Perceptron</strong></p>

<ul>
  <li>Start from the initial solution $w_0=0$</li>
  <li>At each step $t=0,1,2,…$:
    <ul>
      <li>Select an individual from the population and look up their attributes: (x_t,y_t).</li>
      <li>Case 1: If $y_t\langle w_t, x_t\rangle \leq 0$, put
\(w_{t+1} = w_t + y_t x_t\)</li>
      <li>Case 2: Otherwise put $w_{t+1} = w_t$.</li>
    </ul>
  </li>
</ul>

<p>If the examples were selected at random, machine learners would recognize this algorithm as an instance of stochastic gradient descent, still the most ubiquitous way to train classifiers whether they be deep or shallow. Stochastic gradient descent minimizes sums of functions</p>

\[f(w) = \frac{1}{N} \sum_{i=1}^N \mathit{loss}( f(x_i; w) , y_i)\]

<p>with the update</p>

\[w_{t+1} = w_t - \alpha_t \nabla_w \mathit{loss}( f(x_t; w_t) , y_t)\,.\]

<p>When the examples are sampled randomly, the Perceptron is stochastic gradient descent with $\alpha_t=1$, $f(x;w) = \langle w,x \rangle$, and loss function $\mathit{loss}(\hat{y},y) = \max(-\hat{y} y, 0)$.</p>

<p>Stochastic gradient methods were invented a few years before the Perceptron. And the relations between these methods were noted by the mid-60s. Vapnik discusses some of this history in Chapter 1.11 of <a href="https://link.springer.com/book/10.1007/978-1-4757-3264-1"><em>The Nature of Statistical Learning Theory</em></a>.</p>

<p>While we might be tempted to use a standard stochastic gradient analysis to understand the optimization properties of the Perceptron, it turns out that a more rarified proof technique applies that uses no randomization whatsoever. Moreover, the argument will not only bound errors in optimization but also in generalization. Optimization is concerned with errors on a training data set. Generalization is concerned with errors on data we haven’t seen. The analysis from the 1960s links these two by first understanding the dynamics of the algorithm.</p>

<p><a href="https://cs.uwaterloo.ca/~y328yu/classics/novikoff.pdf">A celebrated result by Al Novikoff in 1962</a> showed that under reasonable conditions the algorithm makes a bounded number of updates no matter how large the sample size. Novikoff’s result is typically referred to as a <em>mistake bound</em> as it bounds the number of total misclassifications made when running the Perceptron on some data set. The key assumption in Novikoff’s argument is that the positive and negative examples are cleanly separated by a linear function. People often dismiss the Perceptron because of this <em>separability</em> assumption. But for any finite data set, can always add features and end up with a linearly separable problem. And if we add enough features, we’ll usually be separable no matter how many points we have.</p>

<p>This has been the trend in modern machine learning: don’t fear big models and don’t fear getting zero errors on your training set. This is no different than what was being proposed in the Perceptron. In fact, <a href="https://cs.uwaterloo.ca/~y328yu/classics/kernel.pdf">Aizerman, Braverman, and Roeznoer</a> recognized the power of such overparameterization, and extended Novikoff’s argument to “potential functions” that we now recognize as functions belonging to an infinite dimensional Reproducing Kernel Hilbert Space.</p>

<p>To state Novikoff’s result, we make the following assumptions: First, we assume as input a set of examples $S$. We assume every data point has norm at most $R(S)$ and that there exists a hyperplane that correctly classifies all of the data points and is of distance at least $\gamma(S)$ from every data point. This second assumption is called a <em>margin condition</em> that quantifies how separated the given data is. With these assumptions, Novikoff proved the Perceptron algorithm makes at most</p>

\[{\small
\frac{R(S)^2}{\gamma(S)^{2}}
}\]

<p>mistakes when run on $S$. No matter what the ordering of the data points in $S$, the algorithm makes a bounded number of errors.</p>

<p>The algorithmic analysis of Novikoff has many implications. First, if the data is separable, we can conclude that the Perceptron will terminate if it is run over the data set several times. This is because we can think of $k$ epochs of the Perceptron as running on the union of $k$ distinct copies of $S$, and the Perceptron eventually stops updating when run on this enlarged data set. Hence, the mistake bound tells us something particular about optimization: the Perceptron converges to a solution with zero training errors and hence a global minimizer of the empirical risk.</p>

<p>Second, we can think of the Perceptron algorithm as an <em>online learning algorithm</em>. We need not assume anything distributional about the sequence $S$. We can instead think about how long it takes for the Perceptron to converge to a solution that would have been as good as the optimal classifier. We can quantify this convergence by measuring the <em>regret</em>, equal to</p>

\[\mathcal{R}_T = \sum_{t=1}^T \mathrm{error}(w_t, (x_t,y_t)) - \sum_{t=1}^T \mathrm{error}(w_\star, (x_t,y_t))\,,\]

<p>where $w_\star$ denotes the optimal hyperplane. That is, the regret counts how frequently the classifier at step $t$ misclassifies the next example in the sequence. Novikoff’s argument shows that, if a sequence is perfectly classifiable, then the accrued regret is a constant that does not scale with T.</p>

<p>A third, less well known application of Novikoff’s bound is as a building block for a  <em>generalization bound</em>. A generalization bound estimates the probability of making an error on a new example given that the new example is sampled from the same population as the data thus far sceen. To state the generalization bound for the Perceptron, I <em>now</em> need to return to statistics. Generalization theory concerns statistical validity, and hence we need to define some notion of sampling from the population. I will use the same sampling model I have been using in this blog series. Rather than assuming a statistical model of the population, I will assume we have some population of data from which we can uniformly sample. Our training data will consist of $n$ points sampled uniformly from this population: $S={(x_1,y_1)\ldots, (x_n,y_n) }$.</p>

<p>We know that the Perceptron will find a good linear predictor for the training data if it exists. What we now show is that this predictor also works on new data sampled uniformly from the same population.</p>

<p>To analyze what happens on new data, I will employ an elegant argument I learned from Sasha Rakhlin. This argument appears in a book on Learning Theory by Vapnik and Chervonenkis from 1974, which, to my knowledge, is only available in Russian. Sasha also believes this argument is considerably older as <a href="http://www.mit.edu/~rakhlin/papers/chervonenkis_chapter.pdf">Aizermann and company were making similar “online to batch” constructions in the 1960s</a>. The proof here leverages the assumption that the data are sampled in such a way that they are identically distributed, so we can swap the roles of training and test examples in the analysis. It foreshadows later studies of stability and generalization that would be revisited decades later.</p>

<p><strong>Theorem</strong> <em>Let $w(S)$ be the output of the Perceptron on a dataset $S$ after running until the hyperplane makes no more mistakes on $S$. Let $S_n$ denote a training set of $n$ samples uniformly at random from some population. And let $(x,y)$ be an additional independent uniform sample from the same population. Then, the probability of making a mistake on $(x,y)$ is bounded as</em></p>

\[\Pr[y \langle w(S_n), x \rangle \leq 0] \leq \frac{1}{n+1} {\mathbb{E}}_{S_{n+1}}\left[ \frac{R(S_{n+1})^2}{\gamma(S_{n+1})^2} \right]\,.\]

<p>To prove the theorem, define the “leave-one-out set” to be the set where we drop $(x_k,y_k)$:</p>

\[{\scriptsize
S^{-k}=\{(x_1,y_1),\dots,(x_{k-1},y_{k-1}),(x_{k+1},y_{k+1}),...,(x_{n+1},y_{n+1})\}\,.
}\]

<p>With this notation, since all of the data are sampled identically and independently, we can rewrite the probability of a mistake on the final data point as the expectation of the leave-one-out error</p>

\[{\small
\Pr[y \langle w(S_n), x \rangle   \leq 0]
= \frac1{n+1}\sum_{k=1}^{n+1} \mathbb{E}[\mathbb{1}\{y_k \langle w(S^{-k}), x_k \rangle \leq 0\}]\,.
}\]

<p>Novikoff’s mistake bound asserts the Perceptron makes at most</p>

\[{\small
m=\frac{R(S_{n+1})^2}{\gamma(S_{n+1})^2}
}\]

<p>mistakes when run on the entire sequence $S_{n+1}$. Let $I={i_1,\dots,i_m}$ denote the indices on which the algorithm makes a mistake in any of its cycles over the data. If $k$ is not in $I$, the output of the algorithm remains the same after we remove the $k$-th sample from the sequence. It follows that such $k \in S_{n+1}\setminus I$ satisfy  $y_k w(S^{-k})x_k \geq 0$ and therefore do not contribute to the right hand side of the summation. The other terms can at most contribute $1$ to the summation.
Hence,</p>

\[\Pr[y \langle w(S_n), x \rangle \leq 0] \le \frac{\mathbb{E}[m]}{n+1}\,,\]

<p>which is what we wanted to prove.</p>

<p>What’s most stunning to me about this argument is that there are no numerical constants or logarithms. The generalization error is perfectly quantified by a simple formula of $R$, $\gamma$, and $n$. There are a variety of other arguments that get the $\tilde{O}(R/(n\gamma))$ scaling with far more complex arguments and large constants and logarithmic terms. For example, one can show that the set of hyperplanes in Euclidean space with norm bounded by $\gamma^{-1}$ has <a href="https://www.wiley.com/en-us/Statistical+Learning+Theory-p-9780471030034">VC dimension $R/\gamma$</a>. Similarly, a <a href="https://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf">Rademacher complexity argument will achieve a similar scaling</a>. These arguments apply to far more algorithms than the Perceptron, but it’s frustrating how this simple algorithm from 1956 gets such a tight bound with such a short argument whereas analyzing more “powerful” algorithms often takes pages of derivations.</p>

<p>It’s remarkable that these bounds on optimization, regret, and generalization worked out in the 1960s all turned out to be optimal for classification theory. This strikes me as particularly odd because when I was in graduate school I was taught that the Perceptron was a failed enterprise. But as fads in AI have come and gone, the role of the Perceptron has remained central for 65 years. We’ve made more progress in machine learning theory since then, but it’s not always at the front of our minds just how long ago we had established our modern learning theory framework.</p></div>
    </summary>
    <updated>2021-11-04T00:00:00Z</updated>
    <published>2021-11-04T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2021-11-11T23:09:02Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8862914568867887724</id>
    <link href="http://blog.computationalcomplexity.org/feeds/8862914568867887724/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/a-complexity-view-of-machine-learning.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/8862914568867887724" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/8862914568867887724" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/a-complexity-view-of-machine-learning.html" rel="alternate" type="text/html"/>
    <title>A Complexity View of Machine Learning?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Complexity is at its best when it models new technologies so we can study it in a principled way. Quantum computing comes to mind as a good relatively recent example. With machine learning playing an every growing role in computing, how can complexity play a role?</p><p>The theory community questions about machine learning typically look at finding mathematical reasons to explain why the models well with little overfitting or trying to get good definitions of privacy, fairness, explainability to mitigate the social challenges of ML. But what about from a computational complexity point of view? I don't have a great answer yet but here are some thoughts.</p><p>In much of structural complexity, we use relativization to understand the relative power of complexity classes. We define an oracle as a set A where a machine can ask questions about membership to A and magically get an answer. Relativization can be used to help us define classes like Σ<sub>2</sub><sup>P</sup> = NP<sup>NP</sup> or allow us to succinctly state <a href="https://doi.org/10.1137/0220053">Toda's theorem</a> as PH in P<sup>#P</sup>.</p><p>As I <a href="https://twitter.com/fortnow/status/1453827400383488002">tweeted</a> last week, machine learning feels like an oracle, after all machine learning models and algorithms are typically accessed through APIs and Python modules. What kind of oracle? Definitely not an NP-complete problem like SAT since machine learning fails miserably if you try to use it to break cryptography. </p><p>The real information in machine learning comes from the data. For a length parameter n, consider a string x which might be exponential in n. Think of x as a list of labeled or unlabeled examples of some larger set S. Machine learning creates a model M from x that tries to predict whether x is in S. Think of M as the oracle, as some compressed version of S.</p><p>Is there a computational view of M? We can appeal to Ockham's razor and consider the simplest model consistent with the data for which x as a set are random in the S that M generates. One can formalize this Minimum Description Length approach using <a href="https://doi.org/10.1109/18.825807">Kolmogorov Complexity</a>. This model is too ideal, for one it can also break cryptography, and typical deep learning models are not simple at all with sometimes millions of parameters.</p><p>This is just a start. One could try time bounds on the Kolmogorov definitions or try something different completely. Adversarial and foundational learning models might yield different kinds of oracles. </p><p>If we can figure out even a rough complexity way to understand learning, we can start to get a hold of learning's computational power and limitations, which is the purpose of studying complexity complexity in the first place. </p></div>
    </content>
    <updated>2021-11-03T14:39:00Z</updated>
    <published>2021-11-03T14:39:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-11-11T15:44:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/148</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/148" rel="alternate" type="text/html"/>
    <title>TR21-148 |  Explicit Exponential Lower Bounds for Exact Hyperplane Covers | 

	Benjamin Diamond, 

	Amir Yehudayoff</title>
    <summary>We describe an explicit and simple subset of the discrete hypercube which cannot be exactly covered by fewer than exponentially many hyperplanes. The proof exploits a connection to communication complexity, and relies heavily on Razborov's lower bound for disjointness.</summary>
    <updated>2021-11-03T11:39:43Z</updated>
    <published>2021-11-03T11:39:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-12T04:20:29Z</updated>
    </source>
  </entry>
</feed>
