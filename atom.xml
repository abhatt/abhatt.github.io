<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-06-24T16:22:04Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>http://offconvex.github.io/2020/06/24/equilibrium-min-max/</id>
    <link href="http://offconvex.github.io/2020/06/24/equilibrium-min-max/" rel="alternate" type="text/html"/>
    <title>An equilibrium in nonconvex-nonconcave min-max optimization</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>While there has been incredible progress in convex and nonconvex minimization, a multitude of problems in ML today are in need of efficient algorithms to solve min-max optimization problems. 
 Unlike minimization, where algorithms can always be shown to converge to some local minimum, there is no notion of a local equilibrium in min-max optimization that exists for general nonconvex-nonconcave functions.
    In two recent papers, we give  two notions of local equilibria that are guaranteed to exist and efficient algorithms to compute them.
In this post we present the key ideas behind a second-order notion of local min-max equilibrium from <a href="https://arxiv.org/abs/2006.12363">this paper</a> and in the next we will talk about a different notion along with the algorithm and show its implications to GANs from <a href="https://arxiv.org/abs/2006.12376">this paper</a>.</p>

<h2 id="min-max-optimization">Min-max optimization</h2>

<p>Min-max optimization of an objective function $f:\mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$</p>



<p>is a powerful framework in optimization, economics, and ML as it allows one to model learning in the presence of multiple agents with competing objectives.
In ML applications, such as <a href="https://arxiv.org/abs/1406.2661">GANs</a> and <a href="https://adversarial-ml-tutorial.org">adversarial robustness</a>, the min-max objective function may be nonconvex-nonconcave.
We know that min-max optimization is at least as hard as minimization, hence, we cannot hope to find a globally optimal solution to min-max problems for general functions.</p>

<h2 id="approximate-local-minima-for-minimization">Approximate local minima for minimization</h2>

<p>Let us first revisit the special case of minimization, where there is a natural notion of an approximate second-order local minimum.</p>

<blockquote>
  <p>$x$ is a second-order $\varepsilon$-local minimum of $\mathcal{L}:\mathbb{R}^d\rightarrow \mathbb{R}$ if
</p>
</blockquote>

<p>Now suppose we just wanted to minimize a function $\mathcal{L}$, and we start from any point which is <em>not</em> at an $\varepsilon$-local minimum of $\mathcal{L}$.
Then we can always find a direction to travel in along which either $\mathcal{L}$ decreases rapidly, or the second derivative of $\mathcal{L}$ is large.
 By searching in such a direction we can easily find a new point which has a smaller value of $\mathcal{L}$ using only local information about the gradient and Hessian of $\mathcal{L}$.
 This means that we can keep decreasing $\mathcal{L}$ until we reach an $\varepsilon$-local minimum (see <a href="https://www.researchgate.net/profile/Boris_Polyak2/publication/220589612_Cubic_regularization_of_Newton_method_and_its_global_performance/links/09e4150dd2f0320879000000/Cubic-regularization-of-Newton-method-and-its-global-performance.pdf">Nesterov and Polyak</a>,  <a href="https://dl.acm.org/doi/10.1145/3055399.3055464">here</a>,  <a href="http://proceedings.mlr.press/v40/Ge15.pdf">here</a>,  and also an earlier <a href="https://www.offconvex.org/2016/03/22/saddlepoints">blog post</a> for how to do this with only access to gradients of $\mathcal{L}$).
 If $\mathcal{L}$ is Lipschitz smooth and bounded, we will reach an $\varepsilon$-local minimum in polynomial time from any starting point.</p>

<blockquote>
  <p>Is there an analogous definition with similar properties for min-max optimization?</p>
</blockquote>

<h2 id="problems-with-current-local-optimality-notions">Problems with current local optimality notions</h2>
<p>There has been much recent work on extending theoretical results in nonconvex minimization to min-max optimization (see <a href="https://arxiv.org/abs/1906.00331">here</a>, <a href="https://papers.nips.cc/paper/9430-efficient-algorithms-for-smooth-minimax-optimization">here</a>, <a href="https://arxiv.org/pdf/1807.02629.pdf">here</a>,  <a href="https://papers.nips.cc/paper/9631-solving-a-class-of-non-convex-min-max-games-using-iterative-first-order-methods.pdf">here</a>, <a href="https://arxiv.org/abs/1910.07512">here</a>.
One way to extend the notion of local minimum to the min-max setting is to seek a solution point called a “local saddle”–a point $(x,y)$ where 1) $y$ is a local maximum for $f(x, \cdot)$ and 2) $x$ is a local minimum for $f(\cdot, y).$</p>

<p>For instance,
 this is used  <a href="https://arxiv.org/abs/1706.08500">here</a>, <a href="https://arxiv.org/pdf/1901.00838.pdf">here</a>, <a href="https://arxiv.org/pdf/1705.10461.pdf">here</a>, and <a href="http://proceedings.mlr.press/v89/adolphs19a.html">here</a>.
But, there are very simple examples of two-dimensional bounded functions where a local saddle does not exist.</p>

<blockquote>
  <p>For instance, consider $f(x,y) = sin(x+y)$ from <a href="https://arxiv.org/abs/1902.00618">here</a>. Check that none of the points on this function are simultaneously a local minimum for $x$ and local maximum for $y$.</p>
</blockquote>

<p>The fact that no local saddle exists may be surprising, since an $\varepsilon$-global solution to a min-max optimization problem <em>is</em> guaranteed to exist as long as the objective function is uniformly bounded.
Roughly, this is because, in a global min-max setting, the max-player is empowered to globally maximize the function $f(x,\cdot)$, and the min-player is empowered to minimize the “global max” function $\max_y(f(x, \cdot))$.</p>

<p>The ability to compute the global max  allows the min-player to  predict the max-player’s response.
If $x$ is a global minimum of $\max_y(f(x, \cdot))$, the min-player is aware of this fact and will have no incentive to update $x$.
On the other hand, if the min-player can only simulate the max-player’s updates locally (as in local saddle),
then the min-player may try to update her strategy even when it leads to a net increase in $f$.
This can happen because the min-player is not powerful enough to accurately simulate the max-player’s response. (See  a  <a href="https://arxiv.org/abs/1902.00618">related notion</a> of local optimality with similar issues due to vanishingly small updates.)</p>

<p>The fact that players who can only make local predictions are
unable to predict their opponents’ responses can lead to convergence problems in many popular algorithms such as<br/>
gradient descent ascent (GDA). This non-convergence behavior can occur if the function has no local saddle point (e.g. the function $sin(x+y)$  mentioned above), and can even happen on some functions, like $f(x,y) = xy$ which do have a local saddle point.</p>

<div style="text-align: center;">
<img alt="" src="http://www.offconvex.org/assets/GDA_spiral_fast.gif"/>
<br/>
<b>Figure 1.</b> GDA spirals off to infinity from almost every starting point on the objective function $f(x,y) = xy$. 
</div>
<p><br/></p>

<h2 id="greedy-max-a-computationally-tractable-alternative-to-global-max">Greedy max: a computationally tractable alternative to global max</h2>

<p>To allow for a more stable min-player, and a more stable notion of local optimality, we would like to empower the min-player to more effectively simulate the max-player’s response. 
While the notion of global min-max does exactly this by having the min-player compute the global max function $\max_y(f(\cdot,y))$, computing the global maximum may be intractable.</p>

<p>Instead, we replace the global max function $\max_y (f(\cdot ,y))$ with a computationally tractable alternative. 
Towards this end, we restrict the max-player’s response, and the min-player’s simulation of this response, to updates which can be computed using any algorithm from a class of second-order optimization algorithms.
More specifically, we restrict the max-player to updating $y$ by traveling along continuous paths which start at the current value of $y$ and along which either $f$ is increasing or the second derivative of $f$ is positive.  We refer to such paths as greedy paths since they model a class of second-order “greedy” optimization algorithms.</p>

<blockquote>
  <p><strong>Greedy path:</strong> A unit-speed path $\varphi:[0,\tau] \rightarrow \mathbb{R}^d$ is greedy if $f$ is non-decreasing over this path, and for every $t\in[0,\tau]$
</p>
</blockquote>

<p>Roughly speaking, when restricted to updates obtained from greedy paths, the max-player will always be able to reach a point which is an approximate local maximum for $f(x,\cdot)$, although there may not be a greedy path which leads the max-player to a global maximum.</p>

<div style="text-align: center;">
<img alt="" src="http://www.offconvex.org/assets/greedy_region_omega_t.png" style="width: 400px;"/> <img alt="" src="http://www.offconvex.org/assets/global_max_path_no_axes_t.png" style="width: 400px;"/> 
<br/>
 <b>Figure 2.</b> <i>Left:</i> The light-colored region $\Omega$ is reachable from the initial point $A$ by a greedy path; the dark region is not reachable. <i>Right:</i> There is always a greedy path from any point $A$ to a local maximum ($B$), but a global maximum ($C$) may not be reachable by any greedy path.
</div>
<p><br/></p>

<p>To define an alternative to $\max_y(f(\cdot,y))$, we consider the local maximum point with the largest value of $f(x,\cdot)$ attainable from a given starting point $y$ by any greedy path.
We refer to the value of $f$ at this point as the <em>greedy max function</em>, and denote this value by $g(x,y)$.</p>

<blockquote>
  <p><strong>Greedy max function:</strong> 
    $g(x,y) = \max_{z \in \Omega} f(x,z),$
where $\Omega$ is points reachable from $y$ by greedy path.</p>
</blockquote>

<h2 id="our-greedy-min-max-equilibrium">Our greedy min-max equilibrium</h2>
<p>We use the greedy max function to define a new second-order notion of local optimality for min-max optimization, which we refer to as a greedy min-max equilibrium.
Roughly speaking, we say that $(x,y)$ is a greedy min-max equilibrium if 
1) $y$ is a local maximum for $f(x,\cdot)$ (and hence the endpoint of a greedy path), and 
2) if $x$ is a local minimum of the greedy max function $g(\cdot,y)$.</p>

<p>In other words, $x$ is a local minimum of $\max_y f(\cdot, y)$ under the constraint that the maximum is computed only over the set of greedy paths starting at $y$.
Unfortunately, even if $f$ is smooth, the greedy max function may not be differentiable with respect to $x$ and may even be discontinuous.</p>

<div style="text-align: center;">
<img alt="" src="http://www.offconvex.org/assets/discontinuity2_grid_t.png" width="400"/> <img alt="" src="http://www.offconvex.org/assets/discontinuity2g_grid_t.png" width="400"/> 
<br/>
 <b>Figure 3.</b> <i>Left:</i> If we change $x$ from one value $x$ to a very close value $\hat{x}$, the largest value of $f$ reachable by greedy path undergoes a discontinuous change.  <i>Right:</i>  This means the greedy max function $g(x,y)$ is discontinuous in $x$.</div>
<p><br/></p>

<p>This creates a problem, since the definition of $\varepsilon$-local minimum only applies to smooth functions.</p>

<p>To solve this problem we would ideally like to smooth $g$ by convolution with a Gaussian.
Unfortunately, convolution can cause the local minima of a function to “shift”– a point which is a local minimum for $g$ may no longer be a local minimum for the convolved version of $g$ (to see why, try convolving the function $f(x) = x - 3x I(x\leq 0) + I(x \leq 0)$ with a Gaussian $N(0,\sigma^2)$ for any $\sigma&gt;0$).
To avoid this, we instead consider a “truncated” version of $g$, and then convolve this function in the $x$ variable with a Gaussian to obtain our smoothed version of $g$.</p>

<p>This allows us to define a notion of greedy min-max equilibrium.  We say that a point $(x^\star, y^\star)$ is a greedy min-max equilibrium if $y^\star$ is an approximate local maximum of $f(x^\star, \cdot)$, and $x^\star$ is an $\varepsilon$-local minimum of this smoothed version of $g(\cdot, y^\star)$.</p>

<blockquote>
  <p><b>Greedy min-max equilibrium:</b>
$(x^{\star}, y^{\star})$ is a greedy min-max equilibrium if

 
where $S(x,y):= \mathrm{smooth}_x(\mathrm{truncate}(g(x, y))$.</p>
</blockquote>

<h2 id="greedy-min-max-equilibria-always-exist-and-can-be-found-efficiently">Greedy min-max equilibria always exist! (And can be found efficiently)</h2>
<p>In <a href="https://arxiv.org/abs/2006.12363">this paper</a> we show: A greedy min-max equilibrium is always guaranteed to exist provided that $f$ is uniformly bounded with Lipschitz Hessian. We do so by providing an algorithm which converges to a greedy min-max equilibrium, and, moreover, we show that it is able to do this in polynomial time from any initial point:</p>

<blockquote>
  <p><b>Main theorem:</b> Suppose that we are given access to a smooth function $f:\mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$ and to its gradient and Hessian.  And suppose that $f$ is unformly bounded by $b&gt;0$ and has $L$-Lipschitz Hessian.
Then given any initial point, our algorithm returns an $\varepsilon$-greedy min-max equilibrium $(x^\star,y^\star)$ of $f$ in $\mathrm{poly}(b, L, d, \frac{1}{\varepsilon})$ time.</p>
</blockquote>

<p>There are a number of difficulties that our algorithm and proof must overcome:
One difficulty in designing an algorithm is that the greedy max function may be discontinuous. 
To find an approximate local minimum of a discontinuous function, our algorithm combines a Monte-Carlo hill climbing algorithm with a <a href="https://arxiv.org/abs/cs/0408007">zeroth-order optimization version</a> of stochastic gradient descent.
Another difficulty is that, while one can easily compute a greedy path from any starting point, there may be many different greedy paths which end up at different local maxima.
Searching for the greedy path which leads to the local maximum point with the largest value of $f$ may be infeasible.
In other words the greedy max function $g$ may be intractable to compute.</p>

<div style="text-align: center;">
<img alt="" src="http://www.offconvex.org/assets/greedy_paths_no_axes_t.png" width="400"/> 
<br/>
 <b>Figure 4.</b>There are many different greedy paths that start at the same point $A$.  They can end up at different local maxima ($B$, $D$), with different values of $f$.  In many cases it may be intractable to search over all these paths to compute the greedy max function.
 </div>
<p><br/></p>

<p>To get around this problem, rather than computing the exact value of $g(x,y)$, we instead compute a lower bound $h(x,y)$ for the greedy max function. Since we are able to obtain this lower bound by computing only a <em>single</em> greedy path, it is much easier to compute than greedy max function.</p>

<p>In our paper, we prove that if 1) $x^\star$ is an approximate local minimum for the this lower bound $h(\cdot, y^\star)$, and  2) $y^\star$ is a an approximate local maximum for $f(x^\star, \cdot)$, then $x^\star$ is also an approximate local minimum for the greedy max $g(\cdot, y^\star)$.
This allows us to design an algorithm which obtains a greedy min-max point by minimizing the computationally tractable lower bound $h$, instead of the greedy max function which may be intractable to compute.</p>

<h2 id="to-conclude">To conclude</h2>

<p>In this post we have shown how to extend a notion of second-order equilibrium for minimization to min-max optimization which is guaranteed to exist for any function which is bounded and Lipschitz, with Lipschitz gradient and Hessian.
We have also shown that our algorithm is able to find this equilibrium in  polynomial time from any initial point.</p>

<blockquote>
  <p>Our results do not require any additional assumptions such as convexity, monotonicity, or sufficient bilinearity.</p>
</blockquote>

<p>In an upcoming blog post we will show how one can use some of the ideas from here to obtain a new min-max optimization algorithm with applications to stably training GANs.</p></div>
    </summary>
    <updated>2020-06-24T10:00:00Z</updated>
    <published>2020-06-24T10:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2020-06-24T14:21:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/094</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/094" rel="alternate" type="text/html"/>
    <title>TR20-094 |  Is it possible to improve Yao’s XOR lemma using reductions that exploit the efficiency of their oracle? | 

	Ronen Shaltiel</title>
    <summary>Yao's XOR lemma states that for every function $f:\set{0,1}^k \ar \set{0,1}$, if $f$ has hardness $2/3$ for $P/poly$ (meaning that for every circuit $C$ in $P/poly$, $\Pr[C(X)=f(X)] \le 2/3$ on a uniform input $X$), then the task of computing $f(X_1) \oplus \ldots \oplus f(X_t)$ for sufficiently large $t$ has hardness $\half +\epsilon$ for $P/poly$.

Known proofs of this lemma cannot achieve $\epsilon=\frac{1}{k^{\omega(1)}}$, and even for $\epsilon=\frac{1}{k}$, we do not know how to replace
$P/poly$ by AC$^0[\textsc{parity}]$ (the class of constant depth circuits with the gates $\set{\textsc{and,or,not,parity}}$ of unbounded fan-in).

Recently, Grinberg, Shaltiel and Viola (FOCS 2018) (building on a sequence of earlier works) showed that these limitations cannot be circumvented by \emph{black-box reductions}. Namely, by reductions $\Red^{(\cdot)}$ that given oracle access to a function $D$ that violates the conclusion of Yao's XOR lemma, implement a circuit that violates the assumption of Yao's XOR lemma.

There are a few known reductions in the related literature on worst-case to average case reductions that are \emph{non-black box}. Specifically, the reductions of Gutfreund, Shaltiel and Ta Shma (Computational Complexity 2007) and  Hirahara (FOCS 2018)) are ``class reductions'' that are only guaranteed to succeed when given oracle access to an oracle $D$ from some efficient class of algorithms. These works seem to circumvent some black-box impossibility results.

In this paper we extend the previous limitations of Grinberg, Shaltiel and Viola to class reductions, giving evidence that class reductions cannot yield the desired improvements in Yao's XOR lemma.  To the best of our knowledge, this is the first limitation on reductions for hardness amplification that applies to class reductions.

Our technique imitates the previous lower bounds for black-box reductions, replacing the inefficient oracle used in that proof, with an efficient one that is based on limited independence, and developing tools to deal with the technical difficulties that arise following this replacement.</summary>
    <updated>2020-06-24T05:25:21Z</updated>
    <published>2020-06-24T05:25:21Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-24T16:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13166</id>
    <link href="http://arxiv.org/abs/2006.13166" rel="alternate" type="text/html"/>
    <title>A Family of Constant-Areas Deltoids Associated with the Ellipse</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garcia:Ronaldo.html">Ronaldo Garcia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reznik:Dan.html">Dan Reznik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stachel:Hellmuth.html">Hellmuth Stachel</a>, Mark Helman <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13166">PDF</a><br/><b>Abstract: </b>The Negative Pedal Curve (NPC) of the Ellipse with respect to a boundary
point M is a 3-cusp deltoid which is the affine image of the Steiner Curve.
Over all M the family has invariant area and displays an array of interesting
properties.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.13073</id>
    <link href="http://arxiv.org/abs/2006.13073" rel="alternate" type="text/html"/>
    <title>Reduction From Non-Unique Games To Boolean Unique Games</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eldan:Ronen.html">Ronen Eldan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moshkovitz:Dana.html">Dana Moshkovitz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.13073">PDF</a><br/><b>Abstract: </b>We reduce the problem of proving a "Boolean Unique Games Conjecture" (with
gap 1-delta vs. 1-C*delta, for any C&gt; 1, and sufficiently small delta&gt;0) to the
problem of proving a PCP Theorem for a certain non-unique game. In a previous
work, Khot and Moshkovitz suggested an inefficient candidate reduction (i.e.,
without a proof of soundness). The current work is the first to provide an
efficient reduction along with a proof of soundness. The non-unique game we
reduce from is similar to non-unique games for which PCP theorems are known.
Our proof relies on a new concentration theorem for functions in Gaussian space
that are restricted to a random hyperplane. We bound the typical Euclidean
distance between the low degree part of the restriction of the function to the
hyperplane and the restriction to the hyperplane of the low degree part of the
function.
</p></div>
    </summary>
    <updated>2020-06-24T01:21:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12943</id>
    <link href="http://arxiv.org/abs/2006.12943" rel="alternate" type="text/html"/>
    <title>Learning Based Distributed Tracking</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Hao.html">Hao Wu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gan:Junhao.html">Junhao Gan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Rui.html">Rui Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12943">PDF</a><br/><b>Abstract: </b>Inspired by the great success of machine learning in the past decade, people
have been thinking about the possibility of improving the theoretical results
by exploring data distribution. In this paper, we revisit a fundamental problem
called Distributed Tracking (DT) under an assumption that the data follows a
certain (known or unknown) distribution, and propose a number data-dependent
algorithms with improved theoretical bounds. Informally, in the DT problem,
there is a coordinator and k players, where the coordinator holds a threshold N
and each player has a counter. At each time stamp, at most one counter can be
increased by one. The job of the coordinator is to capture the exact moment
when the sum of all these k counters reaches N. The goal is to minimise the
communication cost. While our first type of algorithms assume the concrete data
distribution is known in advance, our second type of algorithms can learn the
distribution on the fly. Both of the algorithms achieve a communication cost
bounded byO(k log log N) with high probability, improving the state-of-the-art
data-independent bound O(k log N/k). We further propose a number of
implementation optimisation heuristics to improve both efficiency and
robustness of the algorithms. Finally, we conduct extensive experiments on
three real datasets and four synthetic datasets. The experimental results show
that the communication cost of our algorithms is as least as 20% of that of the
state-of-the-art algorithms.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12929</id>
    <link href="http://arxiv.org/abs/2006.12929" rel="alternate" type="text/html"/>
    <title>Approximation algorithms for general cluster routing problem</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Xiaoyan.html">Xiaoyan Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Du:Donglei.html">Donglei Du</a>, Gregory Gutin, Qiaoxia Ming, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Jian.html">Jian Sun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12929">PDF</a><br/><b>Abstract: </b>Graph routing problems have been investigated extensively in operations
research, computer science and engineering due to their ubiquity and vast
applications. In this paper, we study constant approximation algorithms for
some variations of the general cluster routing problem. In this problem, we are
given an edge-weighted complete undirected graph $G=(V,E,c),$ whose vertex set
is partitioned into clusters $C_{1},\dots ,C_{k}.$ We are also given a subset
$V'$ of $V$ and a subset $E'$ of $E.$ The weight function $c$ satisfies the
triangle inequality. The goal is to find a minimum cost walk $T$ that visits
each vertex in $V'$ only once, traverses every edge in $E'$ at least once and
for every $i\in [k]$ all vertices of $C_i$ are traversed consecutively.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12897</id>
    <link href="http://arxiv.org/abs/2006.12897" rel="alternate" type="text/html"/>
    <title>Polynomial Time Approximation Schemes for Clustering in Low Highway Dimension Graphs</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldmann:Andreas_Emil.html">Andreas Emil Feldmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saulpic:David.html">David Saulpic</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12897">PDF</a><br/><b>Abstract: </b>We study clustering problems such as k-Median, k-Means, and Facility Location
in graphs of low highway dimension, which is a graph parameter modeling
transportation networks. It was previously shown that approximation schemes for
these problems exist, which either run in quasi-polynomial time (assuming
constant highway dimension) [Feldmann et al. SICOMP 2018] or run in FPT time
(parameterized by the number of clusters $k$, the highway dimension, and the
approximation factor) [Becker et al. ESA~2018, Braverman et al. 2020]. In this
paper we show that a polynomial-time approximation scheme (PTAS) exists
(assuming constant highway dimension). We also show that the considered
problems are NP-hard on graphs of highway dimension 1.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12881</id>
    <link href="http://arxiv.org/abs/2006.12881" rel="alternate" type="text/html"/>
    <title>BETULA: Numerically Stable CF-Trees for BIRCH Clustering</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lang:Andreas.html">Andreas Lang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schubert:Erich.html">Erich Schubert</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12881">PDF</a><br/><b>Abstract: </b>BIRCH clustering is a widely known approach for clustering, that has
influenced much subsequent research and commercial products. The key
contribution of BIRCH is the Clustering Feature tree (CF-Tree), which is a
compressed representation of the input data. As new data arrives, the tree is
eventually rebuilt to increase the compression. Afterward, the leaves of the
tree are used for clustering. Because of the data compression, this method is
very scalable. The idea has been adopted for example for k-means, data stream,
and density-based clustering.
</p>
<p>Clustering features used by BIRCH are simple summary statistics that can
easily be updated with new data: the number of points, the linear sums, and the
sum of squared values. Unfortunately, how the sum of squares is then used in
BIRCH is prone to catastrophic cancellation.
</p>
<p>We introduce a replacement cluster feature that does not have this numeric
problem, that is not much more expensive to maintain, and which makes many
computations simpler and hence more efficient. These cluster features can also
easily be used in other work derived from BIRCH, such as algorithms for
streaming data. In the experiments, we demonstrate the numerical problem and
compare the performance of the original algorithm compared to the improved
cluster features.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12838</id>
    <link href="http://arxiv.org/abs/2006.12838" rel="alternate" type="text/html"/>
    <title>Analytic Solution to the Piecewise Linear Interface Construction Problem and its Application in Curvature Calculation for Volume-of-Fluid Simulation Codes</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Moritz Lehmann, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gekle:Stephan.html">Stephan Gekle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12838">PDF</a><br/><b>Abstract: </b>The plane-cube intersection problem has been around in literature since 1984
and iterative solutions to it have been used as part of piecewise linear
interface construction (PLIC) in computational fluid dynamics simulation codes
ever since. In many cases, PLIC is the bottleneck of these simulations
regarding compute time, so a faster, analytic solution to the plane-cube
intersection would greatly reduce compute time for such simulations. We derive
an analytic solution for all intersection cases and compare it to the one
previous solution from Scardovelli and Zaleski (Ruben Scardovelli and Stephane
Zaleski. "Analytical relations connecting linear interfaces and volume
fractions in rectangular grids". In: Journal of Computational Physics 164.1
(2000), pp. 228-237.), which we further improve to include edge cases and
micro-optimize to reduce arithmetic operations and branching. We then extend
our comparison regarding compute time and accuracy to include two different
iterative solutions as well. We find that the best choice depends on the
employed hardware platform: on the CPU, Newton-Raphson is fastest with
vectorization while analytic solutions perform better without. The reason for
this is that vectorization instruction sets do not include trigonometric
functions as used in the analytic solutions. On the GPU, the fastest method is
our optimized version of the analytic SZ solution. We finally provide details
on one of the applications of PLIC: curvature calculation for the
Volume-of-Fluid model used for free surface fluid simulations in combination
with the lattice Boltzmann method.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12772</id>
    <link href="http://arxiv.org/abs/2006.12772" rel="alternate" type="text/html"/>
    <title>Combinatorial Pure Exploration of Dueling Bandit</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Wei.html">Wei Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Du:Yihan.html">Yihan Du</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Longbo.html">Longbo Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Haoyu.html">Haoyu Zhao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12772">PDF</a><br/><b>Abstract: </b>In this paper, we study combinatorial pure exploration for dueling bandits
(CPE-DB): we have multiple candidates for multiple positions as modeled by a
bipartite graph, and in each round we sample a duel of two candidates on one
position and observe who wins in the duel, with the goal of finding the best
candidate-position matching with high probability after multiple rounds of
samples. CPE-DB is an adaptation of the original combinatorial pure exploration
for multi-armed bandit (CPE-MAB) problem to the dueling bandit setting.
</p>
<p>We consider both the Borda winner and the Condorcet winner cases. For Borda
winner, we establish a reduction of the problem to the original CPE-MAB setting
and design PAC and exact algorithms that achieve both the sample complexity
similar to that in the CPE-MAB setting (which is nearly optimal for a subclass
of problems) and polynomial running time per round.
</p>
<p>For Condorcet winner, we first design a fully polynomial time approximation
scheme (FPTAS) for the offline problem of finding the Condorcet winner with
known winning probabilities, and then use the FPTAS as an oracle to design a
novel pure exploration algorithm ${\sf CAR}$-${\sf Cond}$ with sample
complexity analysis. ${\sf CAR}$-${\sf Cond}$ is the first algorithm with
polynomial running time per round for identifying the Condorcet winner in
CPE-DB.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12760</id>
    <link href="http://arxiv.org/abs/2006.12760" rel="alternate" type="text/html"/>
    <title>Symmetries, graph properties, and quantum speedups</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Ben=David:Shalev.html">Shalev Ben-David</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Childs:Andrew_M=.html">Andrew M. Childs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gily=eacute=n:Andr=aacute=s.html">András Gilyén</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kretschmer:William.html">William Kretschmer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Podder:Supartha.html">Supartha Podder</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Daochen.html">Daochen Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12760">PDF</a><br/><b>Abstract: </b>Aaronson and Ambainis (2009) and Chailloux (2018) showed that fully symmetric
(partial) functions do not admit exponential quantum query speedups. This
raises a natural question: how symmetric must a function be before it cannot
exhibit a large quantum speedup?
</p>
<p>In this work, we prove that hypergraph symmetries in the adjacency matrix
model allow at most a polynomial separation between randomized and quantum
query complexities. We also show that, remarkably, permutation groups
constructed out of these symmetries are essentially the only permutation groups
that prevent super-polynomial quantum speedups. We prove this by fully
characterizing the primitive permutation groups that allow super-polynomial
quantum speedups.
</p>
<p>In contrast, in the adjacency list model for bounded-degree graphs (where
graph symmetry is manifested differently), we exhibit a property testing
problem that shows an exponential quantum speedup. These results resolve open
questions posed by Ambainis, Childs, and Liu (2010) and Montanaro and de Wolf
(2013).
</p></div>
    </summary>
    <updated>2020-06-24T01:20:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12748</id>
    <link href="http://arxiv.org/abs/2006.12748" rel="alternate" type="text/html"/>
    <title>Approximation Algorithms for Sparse Principal Component Analysis</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chowdhury:Agniva.html">Agniva Chowdhury</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Drineas:Petros.html">Petros Drineas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12748">PDF</a><br/><b>Abstract: </b>We present three provably accurate, polynomial time, approximation algorithms
for the Sparse Principal Component Analysis (SPCA) problem, without imposing
any restrictive assumptions on the input covariance matrix. The first algorithm
is based on randomized matrix multiplication; the second algorithm is based on
a novel deterministic thresholding scheme; and the third algorithm is based on
a semidefinite programming relaxation of SPCA. All algorithms come with
provable guarantees and run in low-degree polynomial time. Our empirical
evaluations confirm our theoretical findings.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12670</id>
    <link href="http://arxiv.org/abs/2006.12670" rel="alternate" type="text/html"/>
    <title>An Efficient PTAS for Stochastic Load Balancing with Poisson Jobs</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/De:Anindya.html">Anindya De</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khanna:Sanjeev.html">Sanjeev Khanna</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Huan.html">Huan Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nikpey:Hesam.html">Hesam Nikpey</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12670">PDF</a><br/><b>Abstract: </b>We give the first polynomial-time approximation scheme (PTAS) for the
stochastic load balancing problem when the job sizes follow Poisson
distributions. This improves upon the 2-approximation algorithm due to Goel and
Indyk (FOCS'99). Moreover, our approximation scheme is an efficient PTAS that
has a running time double exponential in $1/\epsilon$ but nearly-linear in $n$,
where $n$ is the number of jobs and $\epsilon$ is the target error. Previously,
a PTAS (not efficient) was only known for jobs that obey exponential
distributions (Goel and Indyk, FOCS'99).
</p>
<p>Our algorithm relies on several probabilistic ingredients including some
(seemingly) new results on scaling and the so-called "focusing effect" of
maximum of Poisson random variables which might be of independent interest.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12608</id>
    <link href="http://arxiv.org/abs/2006.12608" rel="alternate" type="text/html"/>
    <title>Similarity Search with Tensor Core Units</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahle:Thomas_D=.html">Thomas D. Ahle</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silvestri:Francesco.html">Francesco Silvestri</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12608">PDF</a><br/><b>Abstract: </b>Tensor Core Units (TCUs) are hardware accelerators developed for deep neural
networks, which efficiently support the multiplication of two dense
$\sqrt{m}\times \sqrt{m}$ matrices, where $m$ is a given hardware parameter. In
this paper, we show that TCUs can speed up similarity search problems as well.
We propose algorithms for the Johnson-Lindenstrauss dimensionality reduction
and for similarity join that, by leveraging TCUs, achieve a $\sqrt{m}$ speedup
up with respect to traditional approaches.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12589</id>
    <link href="http://arxiv.org/abs/2006.12589" rel="alternate" type="text/html"/>
    <title>Distributional Individual Fairness in Clustering</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Nihesh Anderson, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bera:Suman_K=.html">Suman K. Bera</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Das:Syamantak.html">Syamantak Das</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yang.html">Yang Liu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12589">PDF</a><br/><b>Abstract: </b>In this paper, we initiate the study of fair clustering that ensures
distributional similarity among similar individuals. In response to improving
fairness in machine learning, recent papers have investigated fairness in
clustering algorithms and have focused on the paradigm of statistical
parity/group fairness. These efforts attempt to minimize bias against some
protected groups in the population. However, to the best of our knowledge, the
alternative viewpoint of individual fairness, introduced by Dwork et al. (ITCS
2012) in the context of classification, has not been considered for clustering
so far. Similar to Dwork et al., we adopt the individual fairness notion which
mandates that similar individuals should be treated similarly for clustering
problems. We use the notion of $f$-divergence as a measure of statistical
similarity that significantly generalizes the ones used by Dwork et al. We
introduce a framework for assigning individuals, embedded in a metric space, to
probability distributions over a bounded number of cluster centers. The
objective is to ensure (a) low cost of clustering in expectation and (b)
individuals that are close to each other in a given fairness space are mapped
to statistically similar distributions.
</p>
<p>We provide an algorithm for clustering with $p$-norm objective ($k$-center,
$k$-means are special cases) and individual fairness constraints with provable
approximation guarantee. We extend this framework to include both group
fairness and individual fairness inside the protected groups. Finally, we
observe conditions under which individual fairness implies group fairness. We
present extensive experimental evidence that justifies the effectiveness of our
approach.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12561</id>
    <link href="http://arxiv.org/abs/2006.12561" rel="alternate" type="text/html"/>
    <title>Better approximation algorithms for maximum weight internal spanning trees in cubic graphs and claw-free graphs</title>
    <feedworld_mtime>1592956800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biniaz:Ahmad.html">Ahmad Biniaz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12561">PDF</a><br/><b>Abstract: </b>Given a connected vertex-weighted graph $G$, the maximum weight internal
spanning tree (MaxwIST) problem asks for a spanning tree of $G$ that maximizes
the total weight of internal nodes. This problem is NP-hard and APX-hard, with
the currently best known approximation factor $1/2$ (Chen et al., Algorithmica
2019). For the case of claw-free graphs, Chen et al. present an involved
approximation algorithm with approximation factor $7/12$. They asked whether it
is possible to improve these ratios, in particular for claw-free graphs and
cubic graphs.
</p>
<p>We improve the approximation factors for the MaxwIST problem in cubic graphs
and claw-free graphs. For cubic graphs we present an algorithm that computes a
spanning tree whose total weight of internal vertices is at least
$\frac{3}{4}-\frac{3}{n}$ times the total weight of all vertices, where $n$ is
the number of vertices of $G$. This ratio is almost tight for large values of
$n$. For claw-free graphs of degree at least three, we present an algorithm
that computes a spanning tree whose total internal weight is at least
$\frac{3}{5}-\frac{1}{n}$ times the total vertex weight. The degree constraint
is necessary as this ratio may not be achievable if we allow vertices of degree
less than three.
</p>
<p>With the above ratios, we immediately obtain better approximation algorithms
with factors $\frac{3}{4}-\epsilon$ and $\frac{3}{5}-\epsilon$ for the MaxwIST
problem in cubic graphs and claw-free graphs of degree at least three, for any
$\epsilon&gt;0$. In addition to improving the approximation factors, the new
algorithms are relatively short compared to that of Chen et al.. The new
algorithms are fairly simple, and employ a variant of the depth-first search
algorithm that selects a relatively-large-weight vertex in every branching
step. Moreover, the new algorithms take linear time while previous algorithms
for similar problem instances are super-linear.
</p></div>
    </summary>
    <updated>2020-06-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4870</id>
    <link href="https://www.scottaaronson.com/blog/?p=4870" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4870#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4870" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Pseudonymity as a trivial concession to genius</title>
    <summary xml:lang="en-US">Update (6/24): For further thoughts and context about this unfolding saga, see this excellent piece by Tom Chivers (author of The AI Does Not Hate You, so far the only book about the rationalist community, one that I reviewed here). This morning, like many others, I woke up to the terrible news that Scott Alexander—the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Update (6/24):</span></strong> For further thoughts and context about this unfolding saga, see <a href="https://unherd.com/2020/06/slate-star-codex-must-remain-anonymous/">this excellent piece by Tom Chivers</a> (author of <em>The AI Does Not Hate You</em>, so far the only book about the rationalist community, one that I <a href="https://www.scottaaronson.com/blog/?p=4361">reviewed here</a>).</p>



<p/><hr/><p/>



<p>This morning, like many others, I woke up to the terrible news that Scott Alexander—the man I call “the greatest Scott A. of the Internet”—has <a href="https://slatestarcodex.com/">deleted SlateStarCodex in its entirety</a>.  The reason, Scott explains, is that the <em>New York Times</em> was planning to run an article about SSC.  Even though the article was going to be <em>positive</em>, NYT decided that by policy, it would need to include Scott’s real surname (Alexander is his middle name).  Scott felt that revealing his name to the world would endanger himself and his psychiatry patients.  Taking down his entire blog was the only recourse that he saw.</p>



<p>The NYT writer, Cade Metz, was someone who I’d previously known and trusted from his reporting on Google’s quantum supremacy experiment.  So in recent weeks, I’d spent a couple hours on the phone with Cade, answering his questions about the rationality community, the history of my interactions with it, and why I thought SlateStarCodex spoke to so many readers.  Alas, when word got around the rationality community that Cade was writing a story, a huge panic arose that he was planning on some sort of <em>Gawker</em>-style hit piece or takedown.  Trying to tamp down the fire, I told Scott Alexander and others that I knew Cade, his intentions were good, he was only trying to understand the community, and everyone should help him by talking to him openly.</p>



<p>In a year of historic ironies, here’s another one: that it was the decent, reasonable, and well-meaning Cade Metz, rather than any of the SneerClubbers or Twitter-gangsters who despised Scott Alexander for sharing his honest thoughts on hot-button issues, who finally achieved the latter’s dark dream of exiling Scott from the public sphere.</p>



<p>The recent news had already been bad enough: Trump’s “temporary suspension” of J1 and H1B visas (which will deal a body blow to American universities this year, and to all the foreign scientists who planned to work at them), on top of the civil unrest, on top of the economic collapse, on top of the now-resurgent coronavirus.  But with no more SlateStarCodex, now I <em>really</em> feel like my world is coming to an end.</p>



<p>I’ve considered SSC to be the best blog on the Internet since not long after discovering it five years ago.  Of course my judgment is colored by one of the most notorious posts in SSC’s history (“Untitled”) being a ferocious defense of me, when thousands were attacking me and it felt like my life was finished.  But that’s merely what brought me there in the first place.  I stayed because of Scott’s insights about everything else, and because of the humor and humanity and craftsmanship of his prose.  Since then I had the privilege to become friends with Scott, not only virtually but in real life, and to meet dozens of others in the SSC community, in its Bay Area epicenter and elsewhere.</p>



<p>In my view, for SSC to be <em>permanently</em> deleted would be an intellectual loss on the scale of, let’s say, John Stuart Mill or Mark Twain burning their collected works.  That might sound like hyperbole, but not (I don’t think) to the tens of thousands who read Scott’s essays and fiction, particularly during their 2013-2016 heyday, and who went from casual enjoyment to growing admiration to the gradual recognition that they were experiencing, “live,” the works that future generations of teachers will assign their students when they cover the early twenty-first century.  The one thing that mitigates this tragedy is the hope that it will yet be reversed (and, of course, the fact that backups still exist in the bowels of the Internet).</p>



<p>When I discovered Scott Alexander in early 2015, the one issue that gave me pause was his strange insistence on maintaining pseudonymity, even as he was already then becoming more and more of a public figure.  In effect, Scott was trying to erect a firewall between his Internet persona and his personal and professional identities, and was <em>relying on the entire world’s goodwill</em> not to breach that firewall.  I thought to myself, “this can’t <em>possibly</em> last!  Scott simply writes too well to evade mainstream notice forever—and once he’s on the world’s radar, he’ll need to make a choice, about who he is and whether he’s ready to own his gifts to posterity under his real name.”  In retrospect, what astonishes me is that Scott has been able to maintain the “double life” for as long as he has!</p>



<p>In his takedown notice, Scott writes that it’s considered vitally important in psychiatry for patients to know almost nothing about their doctors, beyond their names and their areas of expertise.  That caused me to wonder: OK, but doesn’t the world already have enough psychiatrists who are ciphers to their patients?  Would it be so terrible to have one psychiatrist with a clear public persona—possibly even one who patients sought out <em>because</em> of his public persona, because his writings gave evidence that he’d have sympathy or insight about their conditions?  To become a psychiatrist, does one really need to take a lifelong vow of boringness—a vow never to do or say anything notable enough that one would be “outed” to one’s patients?  What would Freud, or Jung, or any of the other famous therapist-intellectuals of times past have thought about such a vow?</p>



<p>Scott also mentions that he’s gotten death threats, and harassing calls to his workplace, from people who hate him because of his blog (and who found his real name by sleuthing).  I wish I knew a solution to that.  For what it’s worth, my blogging has <em>also</em> earned me a death threat, and threats to sue me, and accusatory letters to the president of my university—although in my case, the worst threats came neither from Jew-hating neo-Nazis nor from nerd-bashing SJWs, but from crackpots enraged that I wouldn’t use my blog to credit their proof of P≠NP or their refutation of quantum mechanics.</p>



<p>When I started <em>Shtetl-Optimized</em> back in 2005, I remember thinking: this is it.  From now on, the only secrets I’ll have in life will be ephemeral and inconsequential ones.  From this day on, every student in my class, every prospective employer, every woman who I ask on a date (I wasn’t married yet), can know whatever they want to know about my political sympathies, my deepest fears and insecurities, any of it, with a five-second Google search.  Am I ready for that?  I decided that I was—partly just because I‘ve never had the mental space to maintain multiple partitioned identities <em>anyway</em>, to remember what each one is or isn’t allowed to know and say!  I won’t pretend that this is the right decision for everyone, but it was my decision, and I stuck with it, and it wasn’t always easy but I’m still here and so evidently are you.</p>



<p>I’d be <em>overjoyed</em> if Scott Alexander were someday to reach a place in his life where he felt comfortable deciding similarly.  That way, not only could he enjoy the full acclaim that he’s earned for what he’s given to the world, but (much more importantly) his tens of thousands of fans would be able to continue benefitting from his insights.</p>



<p>For now, though, the brute fact is that Scott is obviously <em>not</em> comfortable making that choice.  That being so, it seems to me that, if the NYT was able to respect the pseudonymity of Banksy and many others who it’s reported on in the past, when revealing their real names would serve no public interest, then it should also be able to respect Scott Alexander’s pseudonymity.  Especially now that Scott has sent the most credible signal imaginable of how much he values that pseudonymity, a signal that astonished even me.  The world does not exist only to serve its rare geniuses, but surely it can make such trivial concessions to them.</p></div>
    </content>
    <updated>2020-06-23T17:41:53Z</updated>
    <published>2020-06-23T17:41:53Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-06-24T07:33:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/093</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/093" rel="alternate" type="text/html"/>
    <title>TR20-093 |  Reduction From Non-Unique Games To Boolean Unique Games | 

	Dana Moshkovitz, 

	Ronen Eldan</title>
    <summary>We reduce the problem of proving a "Boolean Unique Games Conjecture" (with gap $1-\delta$ vs. $1-C\delta$, for any $C&gt; 1$, and sufficiently small $\delta&gt;0$) to the problem of proving a PCP Theorem for a certain non-unique game.
In a previous work, Khot and Moshkovitz suggested an inefficient candidate reduction (i.e., without a proof of soundness). 
The current work is the first to provide an efficient reduction along with a proof of soundness. 
The non-unique game we reduce from is similar to non-unique games for which PCP theorems are known.
Our proof relies on a new concentration theorem for functions in Gaussian space that are restricted to a random hyperplane. We bound the typical Euclidean distance between the low degree part of the restriction of the function to the hyperplane and the restriction to the hyperplane of the low degree part of the function.</summary>
    <updated>2020-06-23T14:41:45Z</updated>
    <published>2020-06-23T14:41:45Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-06-24T16:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12028</id>
    <link href="http://arxiv.org/abs/2006.12028" rel="alternate" type="text/html"/>
    <title>Constructing Driver Hamiltonians for Several Linear Constraints</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Hannes Leipold, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Spedalieri:Federico_M=.html">Federico M. Spedalieri</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12028">PDF</a><br/><b>Abstract: </b>Recent advances in the field of adiabatic quantum computing and the closely
related field of quantum annealers has centered around using more advanced and
novel Hamiltonian representations to solve optimization problems. One of these
advances has centered around the development of driver Hamiltonians that
commute with the constraints of an optimization problem - allowing for another
avenue to satisfying those constraints instead of imposing penalty terms for
each of them. In particular, the approach is able to use sparser connectivity
to embed several practical problems on quantum devices than other common
practices. However, designing the driver Hamiltonians that successfully commute
with several constraints has largely been based on strong intuition for
specific problems and with no simple general algorithm to generate them for
arbitrary constraints. In this work, we develop a simple and intuitive
algebraic framework for reasoning about the commutation of Hamiltonians with
linear constraints - one that allows us to classify the complexity of finding a
driver Hamiltonian for an arbitrary set of constraints as NP-hard.
</p></div>
    </summary>
    <updated>2020-06-23T23:20:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12023</id>
    <link href="http://arxiv.org/abs/2006.12023" rel="alternate" type="text/html"/>
    <title>The space of sections of a smooth function</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carlsson:Gunnar.html">Gunnar Carlsson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filippenko:Benjamin.html">Benjamin Filippenko</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12023">PDF</a><br/><b>Abstract: </b>Given a compact manifold $X$ with boundary and a submersion $f : X
\rightarrow Y$ whose restriction to the boundary of $X$ has isolated critical
points with distinct critical values and where $Y$ is $[0,1]$ or $S^1$, the
connected components of the space of sections of $f$ are computed from $\pi_0$
and $\pi_1$ of the fibers of $f$. This computation is then leveraged to provide
new results on a smoothed version of the evasion path problem for mobile sensor
networks: From the time-varying homology of the covered region and the
time-varying cup-product on cohomology of the boundary, a necessary and
sufficient condition for existence of an evasion path and a lower bound on the
number of homotopy classes of evasion paths are computed. No connectivity
assumptions are required.
</p></div>
    </summary>
    <updated>2020-06-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.12011</id>
    <link href="http://arxiv.org/abs/2006.12011" rel="alternate" type="text/html"/>
    <title>Superpolynomial Lower Bounds for Learning One-Layer Neural Networks using Gradient Descent</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goel:Surbhi.html">Surbhi Goel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gollakota:Aravind.html">Aravind Gollakota</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Zhihan.html">Zhihan Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karmalkar:Sushrut.html">Sushrut Karmalkar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klivans:Adam.html">Adam Klivans</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.12011">PDF</a><br/><b>Abstract: </b>We prove the first superpolynomial lower bounds for learning one-layer neural
networks with respect to the Gaussian distribution using gradient descent. We
show that any classifier trained using gradient descent with respect to
square-loss will fail to achieve small test error in polynomial time given
access to samples labeled by a one-layer neural network. For classification, we
give a stronger result, namely that any statistical query (SQ) algorithm
(including gradient descent) will fail to achieve small test error in
polynomial time. Prior work held only for gradient descent run with small batch
sizes, required sharp activations, and applied to specific classes of queries.
Our lower bounds hold for broad classes of activations including ReLU and
sigmoid. The core of our result relies on a novel construction of a simple
family of neural networks that are exactly orthogonal with respect to all
spherically symmetric distributions.
</p></div>
    </summary>
    <updated>2020-06-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11978</id>
    <link href="http://arxiv.org/abs/2006.11978" rel="alternate" type="text/html"/>
    <title>Fast Preprocessing for Optimal Orthogonal Range Reporting and Range Successor with Applications to Text Indexing</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Younan Gao, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Meng.html">Meng He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nekrich:Yakov.html">Yakov Nekrich</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11978">PDF</a><br/><b>Abstract: </b>Under the word RAM model, we design three data structures that can be
constructed in $O(n\sqrt{\lg n})$ time over $n$ points in an $n \times n$ grid.
The first data structure is an $O(n\lg^{\epsilon} n)$-word structure supporting
orthogonal range reporting in $O(\lg\lg n+k)$ time, where $k$ denotes output
size and $\epsilon$ is an arbitrarily small constant. The second is an
$O(n\lg\lg n)$-word structure supporting orthogonal range successor in
$O(\lg\lg n)$ time, while the third is an $O(n\lg^{\epsilon} n)$-word structure
supporting sorted range reporting in $O(\lg\lg n+k)$ time. The query times of
these data structures are optimal when the space costs must be within $O(n\
polylog\ n)$ words. Their exact space bounds match those of the best known
results achieving the same query times, and the $O(n\sqrt{\lg n})$ construction
time beats the previous bounds on preprocessing. Previously, among 2d range
search structures, only the orthogonal range counting structure of Chan and
P\v{a}tra\c{s}cu (SODA 2010) and the linear space, $O(\lg^{\epsilon} n)$ query
time structure for orthogonal range successor by Belazzougui and Puglisi (SODA
2016) can be built in the same $O(n\sqrt{\lg n})$ time. Hence our work is the
first that achieve the same preprocessing time for optimal orthogonal range
reporting and range successor. We also apply our results to improve the
construction time of text indexes.
</p></div>
    </summary>
    <updated>2020-06-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11947</id>
    <link href="http://arxiv.org/abs/2006.11947" rel="alternate" type="text/html"/>
    <title>How to Count Triangles, without Seeing the Whole Graph</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bera:Suman_K=.html">Suman K. Bera</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seshadhri:C=.html">C. Seshadhri</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11947">PDF</a><br/><b>Abstract: </b>Triangle counting is a fundamental problem in the analysis of large graphs.
There is a rich body of work on this problem, in varying streaming and
distributed models, yet all these algorithms require reading the whole input
graph. In many scenarios, we do not have access to the whole graph, and can
only sample a small portion of the graph (typically through crawling). In such
a setting, how can we accurately estimate the triangle count of the graph?
</p>
<p>We formally study triangle counting in the {\em random walk} access model
introduced by Dasgupta et al (WWW '14) and Chierichetti et al (WWW '16). We
have access to an arbitrary seed vertex of the graph, and can only perform
random walks. This model is restrictive in access and captures the challenges
of collecting real-world graphs. Even sampling a uniform random vertex is a
hard task in this model.
</p>
<p>Despite these challenges, we design a provable and practical algorithm,
TETRIS, for triangle counting in this model. TETRIS is the first provably
sublinear algorithm (for most natural parameter settings) that approximates the
triangle count in the random walk model, for graphs with low mixing time. Our
result builds on recent advances in the theory of sublinear algorithms. The
final sample built by TETRIS is a careful mix of random walks and degree-biased
sampling of neighborhoods. Empirically, TETRIS accurately counts triangles on a
variety of large graphs, getting estimates within 5\% relative error by looking
at 3\% of the number of edges.
</p></div>
    </summary>
    <updated>2020-06-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11876</id>
    <link href="http://arxiv.org/abs/2006.11876" rel="alternate" type="text/html"/>
    <title>Personalized PageRank to a Target Node, Revisited</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Hanzhi.html">Hanzhi Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei:Zhewei.html">Zhewei Wei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gan:Junhao.html">Junhao Gan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Sibo.html">Sibo Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Zengfeng.html">Zengfeng Huang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11876">PDF</a><br/><b>Abstract: </b>Personalized PageRank (PPR) is a widely used node proximity measure in graph
mining and network analysis. Given a source node $s$ and a target node $t$, the
PPR value $\pi(s,t)$ represents the probability that a random walk from $s$
terminates at $t$, and thus indicates the bidirectional importance between $s$
and $t$. The majority of the existing work focuses on the single-source
queries, which asks for the PPR value of a given source node $s$ and every node
$t \in V$. However, the single-source query only reflects the importance of
each node $t$ with respect to $s$. In this paper, we consider the {\em
single-target PPR query}, which measures the opposite direction of importance
for PPR. Given a target node $t$, the single-target PPR query asks for the PPR
value of every node $s\in V$ to a given target node $t$. We propose RBS, a
novel algorithm that answers approximate single-target queries with optimal
computational complexity. We show that RBS improves three concrete
applications: heavy hitters PPR query, single-source SimRank computation, and
scalable graph neural networks. We conduct experiments to demonstrate that RBS
outperforms the state-of-the-art algorithms in terms of both efficiency and
precision on real-world benchmark datasets.
</p></div>
    </summary>
    <updated>2020-06-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11827</id>
    <link href="http://arxiv.org/abs/2006.11827" rel="alternate" type="text/html"/>
    <title>Refined bounds for algorithm configuration: The knife-edge of dual class approximability</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balcan:Maria=Florina.html">Maria-Florina Balcan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sandholm:Tuomas.html">Tuomas Sandholm</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vitercik:Ellen.html">Ellen Vitercik</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11827">PDF</a><br/><b>Abstract: </b>Automating algorithm configuration is growing increasingly necessary as
algorithms come with more and more tunable parameters. It is common to tune
parameters using machine learning, optimizing performance metrics such as
runtime and solution quality. The training set consists of problem instances
from the specific domain at hand. We investigate a fundamental question about
these techniques: how large should the training set be to ensure that a
parameter's average empirical performance over the training set is close to its
expected, future performance? We answer this question for algorithm
configuration problems that exhibit a widely-applicable structure: the
algorithm's performance as a function of its parameters can be approximated by
a "simple" function. We show that if this approximation holds under the
L-infinity norm, we can provide strong sample complexity bounds. On the flip
side, if the approximation holds only under the L-p norm for p smaller than
infinity, it is not possible to provide meaningful sample complexity bounds in
the worst case. We empirically evaluate our bounds in the context of integer
programming, one of the most powerful tools in computer science. Via
experiments, we obtain sample complexity bounds that are up to 700 times
smaller than the previously best-known bounds.
</p></div>
    </summary>
    <updated>2020-06-23T23:21:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11726</id>
    <link href="http://arxiv.org/abs/2006.11726" rel="alternate" type="text/html"/>
    <title>Continuous Submodular Maximization: Beyond DR-Submodularity</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldman:Moran.html">Moran Feldman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karbasi:Amin.html">Amin Karbasi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11726">PDF</a><br/><b>Abstract: </b>In this paper, we propose the first continuous optimization algorithms that
achieve a constant factor approximation guarantee for the problem of monotone
continuous submodular maximization subject to a linear constraint. We first
prove that a simple variant of the vanilla coordinate ascent, called
Coordinate-Ascent+, achieves a $(\frac{e-1}{2e-1}-\varepsilon)$-approximation
guarantee while performing $O(n/\varepsilon)$ iterations, where the
computational complexity of each iteration is roughly
$O(n/\sqrt{\varepsilon}+n\log n)$ (here, $n$ denotes the dimension of the
optimization problem). We then propose Coordinate-Ascent++, that achieves the
tight $(1-1/e-\varepsilon)$-approximation guarantee while performing the same
number of iterations, but at a higher computational complexity of roughly
$O(n^3/\varepsilon^{2.5} + n^3 \log n / \varepsilon^2)$ per iteration. However,
the computation of each round of Coordinate-Ascent++ can be easily parallelized
so that the computational cost per machine scales as
$O(n/\sqrt{\varepsilon}+n\log n)$.
</p></div>
    </summary>
    <updated>2020-06-23T23:28:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11687</id>
    <link href="http://arxiv.org/abs/2006.11687" rel="alternate" type="text/html"/>
    <title>PFP Data Structures</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boucher:Christina.html">Christina Boucher</a>, Ondřej Cvacho, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gagie:Travis.html">Travis Gagie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Holub:Jan.html">Jan Holub</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manzini:Giovanni.html">Giovanni Manzini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rossi:Massimiliano.html">Massimiliano Rossi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11687">PDF</a><br/><b>Abstract: </b>Prefix-free parsing (PFP) was introduced by Boucher et al. (2019) as a
preprocessing step to ease the computation of Burrows-Wheeler Transforms (BWTs)
of genomic databases. Given a string $S$, it produces a dictionary $D$ and a
parse $P$ of overlapping phrases such that $\mathrm{BWT} (S)$ can be computed
from $D$ and $P$ in time and workspace bounded in terms of their combined size
$|\mathrm{PFP} (S)|$. In practice $D$ and $P$ are significantly smaller than
$S$ and computing $\mathrm{BWT} (S)$ from them is more efficient than computing
it from $S$ directly, at least when $S$ consists of genomes from individuals of
the same species. In this paper, we consider $\mathrm{PFP} (S)$ as a {\em data
structure} and show how it can be augmented to support the following queries
quickly, still in $O (|\mathrm{PFP} (S)|)$ space: longest common extension
(LCE), suffix array (SA), longest common prefix (LCP) and BWT. Lastly, we
provide experimental evidence that the PFP data structure can be efficiently
constructed for very large repetitive datasets: it takes one hour and 54 GB
peak memory for $1000$ variants of human chromosome 19, initially occupying
roughly 56 GB.
</p></div>
    </summary>
    <updated>2020-06-23T23:31:05Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11648</id>
    <link href="http://arxiv.org/abs/2006.11648" rel="alternate" type="text/html"/>
    <title>Training (Overparametrized) Neural Networks in Near-Linear Time</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brand:Jan_van_den.html">Jan van den Brand</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Binghui.html">Binghui Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Song:Zhao.html">Zhao Song</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinstein:Omri.html">Omri Weinstein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11648">PDF</a><br/><b>Abstract: </b>The slow convergence rate and pathological curvature issues of first-order
gradient methods for training deep neural networks, initiated an ongoing effort
for developing faster $\mathit{second}$-$\mathit{order}$ optimization
algorithms beyond SGD, without compromising the generalization error. Despite
their remarkable convergence rate ($\mathit{independent}$ of the training batch
size $n$), second-order algorithms incur a daunting slowdown in the
$\mathit{cost}$ $\mathit{per}$ $\mathit{iteration}$ (inverting the Hessian
matrix of the loss function), which renders them impractical. Very recently,
this computational overhead was mitigated by the works of [ZMG19, CGH+19],
yielding an $O(Mn^2)$-time second-order algorithm for training overparametrized
neural networks with $M$ parameters.
</p>
<p>We show how to speed up the algorithm of [CGH+19], achieving an
$\tilde{O}(Mn)$-time backpropagation algorithm for training (mildly
overparametrized) ReLU networks, which is near-linear in the dimension ($Mn$)
of the full gradient (Jacobian) matrix. The centerpiece of our algorithm is to
reformulate the Gauss-Newton iteration as an $\ell_2$-regression problem, and
then use a Fast-JL type dimension reduction to $\mathit{precondition} $ the
underlying Gram matrix in time independent of $M$, allowing to find a
sufficiently good approximate solution via $\mathit{first}$-$\mathit{order}$
conjugate gradient. Our result provides a proof-of-concept that advanced
machinery from randomized linear algebra-which led to recent breakthroughs in
$\mathit{convex}$ $\mathit{optimization}$ (ERM, LPs, Regression)-can be carried
over to the realm of deep learning as well.
</p></div>
    </summary>
    <updated>2020-06-23T23:29:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11607</id>
    <link href="http://arxiv.org/abs/2006.11607" rel="alternate" type="text/html"/>
    <title>Knapsack Secretary with Bursty Adversary</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kesselheim:Thomas.html">Thomas Kesselheim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Molinaro:Marco.html">Marco Molinaro</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11607">PDF</a><br/><b>Abstract: </b>The random-order or secretary model is one of the most popular beyond-worst
case model for online algorithms. While it avoids the pessimism of the
traditional adversarial model, in practice we cannot expect the input to be
presented in perfectly random order. This has motivated research on ``best of
both worlds'' (algorithms with good performance on both purely stochastic and
purely adversarial inputs), or even better, on inputs that are a mix of both
stochastic and adversarial parts. Unfortunately the latter seems much harder to
achieve and very few results of this type are known.
</p>
<p>Towards advancing our understanding of designing such robust algorithms, we
propose a random-order model with bursts of adversarial time steps. The
assumption of burstiness of unexpected patterns is reasonable in many contexts,
since changes (e.g. spike in a demand for a good) are often triggered by a
common external event. We then consider the Knapsack Secretary problem in this
model: there is a knapsack of size $k$ (e.g., available quantity of a good),
and in each of the $n$ time steps an item comes with its value and size in
$[0,1]$ and the algorithm needs to make an irrevocable decision whether to
accept or reject the item.
</p>
<p>We design an algorithm that gives an approximation of $1 -
\tilde{O}(\Gamma/k)$ when the adversarial time steps can be covered by $\Gamma
\ge \sqrt{k}$ intervals of size $\tilde{O}(\frac{n}{k})$. In particular,
setting $\Gamma = \sqrt{k}$ gives a $(1 - O(\frac{\ln^2
k}{\sqrt{k}}))$-approximation that is resistant to up to a $\frac{\ln^2
k}{\sqrt{k}}$-fraction of the items being adversarial, which is almost optimal
even in the absence of adversarial items. Also, setting $\Gamma =
\tilde{\Omega}(k)$ gives a constant approximation that is resistant to up to a
constant fraction of items being adversarial.
</p></div>
    </summary>
    <updated>2020-06-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11589</id>
    <link href="http://arxiv.org/abs/2006.11589" rel="alternate" type="text/html"/>
    <title>Multicritera Cuts and Size-Constrained $k$-cuts in Hypergraphs</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beideman:Calvin.html">Calvin Beideman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chandrasekaran:Karthekeyan.html">Karthekeyan Chandrasekaran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Chao.html">Chao Xu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11589">PDF</a><br/><b>Abstract: </b>We address counting and optimization variants of multicriteria global min-cut
and size-constrained min-$k$-cut in hypergraphs.
</p>
<p>1. For an $r$-rank $n$-vertex hypergraph endowed with $t$ hyperedge-cost
functions, we show that the number of multiobjective min-cuts is
$O(r2^{tr}n^{3t-1})$. In particular, this shows that the number of parametric
min-cuts in constant rank hypergraphs for a constant number of criteria is
strongly polynomial, thus resolving an open question by Aissi, Mahjoub,
McCormick, and Queyranne (Math Programming, 2015). In addition, we give
randomized algorithms to enumerate all multiobjective min-cuts and all
pareto-optimal cuts in strongly polynomial-time.
</p>
<p>2. We also address node-budgeted multiobjective min-cuts: For an $n$-vertex
hypergraph endowed with $t$ vertex-weight functions, we show that the number of
node-budgeted multiobjective min-cuts is $O(r2^{r}n^{t+2})$, where $r$ is the
rank of the hypergraph, and the number of node-budgeted $b$-multiobjective
min-cuts for a fixed budget-vector $b$ is $O(n^2)$.
</p>
<p>3. We show that min-$k$-cut in hypergraphs subject to constant lower bounds
on part sizes is solvable in polynomial-time for constant $k$, thus resolving
an open problem posed by Queyranne. Our technique also shows that the number of
optimal solutions is polynomial.
</p>
<p>All of our results build on the random contraction approach of Karger (SODA,
1993). Our techniques illustrate the versatility of the random contraction
approach to address counting and algorithmic problems concerning multiobjective
min-cuts and size-constrained $k$-cuts in hypergraphs.
</p></div>
    </summary>
    <updated>2020-06-23T23:26:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11580</id>
    <link href="http://arxiv.org/abs/2006.11580" rel="alternate" type="text/html"/>
    <title>Finite-size scaling, phase coexistence, and algorithms for the random cluster model on random graphs</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Helmuth:Tyler.html">Tyler Helmuth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jenssen:Matthew.html">Matthew Jenssen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perkins:Will.html">Will Perkins</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11580">PDF</a><br/><b>Abstract: </b>For $\Delta \ge 5$ and $q$ large as a function of $\Delta$, we give a
detailed picture of the phase transition of the random cluster model on random
$\Delta$-regular graphs. In particular, we determine the limiting distribution
of the weights of the ordered and disordered phases at criticality and prove
exponential decay of correlations away from criticality.
</p>
<p>Our techniques are based on using polymer models and the cluster expansion to
control deviations from the ordered and disordered ground states. These
techniques also yield efficient approximate counting and sampling algorithms
for the Potts and random cluster models on random $\Delta$-regular graphs at
all temperatures when $q$ is large. Our algorithms apply more generally to
$\Delta$-regular graphs satisfying a small set expansion condition.
</p></div>
    </summary>
    <updated>2020-06-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2006.11523</id>
    <link href="http://arxiv.org/abs/2006.11523" rel="alternate" type="text/html"/>
    <title>Cycle-based formulations in Distance Geometry</title>
    <feedworld_mtime>1592870400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liberti:Leo.html">Leo Liberti</a>, Gabriele Iommazzo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lavor:Carlile.html">Carlile Lavor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maculan:Nelson.html">Nelson Maculan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2006.11523">PDF</a><br/><b>Abstract: </b>The distance geometry problem asks to find a realization of a given simple
edge-weighted graph in a Euclidean space of given dimension K, where the edges
are realized as straight segments of lengths equal (or as close as possible) to
the edge weights. The problem is often modelled as a mathematical programming
formulation involving decision variables that determine the position of the
vertices in the given Euclidean space. Solution algorithms are generally
constructed using local or global nonlinear optimization techniques. We present
a new modelling technique for this problem where, instead of deciding vertex
positions, formulations decide the length of the segments representing the
edges in each cycle in the graph, projected in every dimension. We propose an
exact formulation and a relaxation based on a Eulerian cycle. We then compare
computational results from protein conformation instances obtained with
stochastic global optimization techniques on the new cycle-based formulation
and on the existing edge-based formulation. While edge-based formulations take
less time to reach termination, cycle-based formulations are generally better
on solution quality measures.
</p></div>
    </summary>
    <updated>2020-06-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-06-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/06/22/postdoc-at-technion-israel-institute-of-technology-apply-by-august-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/06/22/postdoc-at-technion-israel-institute-of-technology-apply-by-august-1-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc at Technion Israel Institute of Technology (apply by August 1, 2020)</title>
    <summary>Looking for excellent CS theory graduates for a postdoctoral position at the Computer Science Faculty of Technion Israel Institute of Technology, in Prof. Nir Ailon’s group. Research topics include theory of learning, optimization, information theory and their intersection. Website: https://nailon.net.technion.ac.il/ Email: mayasidis@cs.technion.ac.il</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Looking for excellent CS theory graduates for a postdoctoral position at the Computer Science Faculty of Technion Israel Institute of Technology, in Prof. Nir Ailon’s group. Research topics include theory of learning, optimization, information theory and their intersection.</p>
<p>Website: <a href="https://nailon.net.technion.ac.il/">https://nailon.net.technion.ac.il/</a><br/>
Email: mayasidis@cs.technion.ac.il</p></div>
    </content>
    <updated>2020-06-22T08:35:15Z</updated>
    <published>2020-06-22T08:35:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-06-24T16:20:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5075242632528707140</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5075242632528707140/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/06/winner-of-ramsey-meme-contest.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5075242632528707140" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5075242632528707140" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/06/winner-of-ramsey-meme-contest.html" rel="alternate" type="text/html"/>
    <title>Winner of Ramsey Meme Contest</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My REU program had a Ramsey Meme Contest.<br/>
<br/>
The winner was Saadiq Shaik with this entry:<br/>
<br/>
<a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/idont.jpg">I Don't Always...</a><br/>
<br/>
I challenge my readers to come up with other Ramsey Memes! or Complexity Memes! or point me to some that are already out there.<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2020-06-22T05:15:00Z</updated>
    <published>2020-06-22T05:15:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-06-24T03:30:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1791</id>
    <link href="https://theorydish.blog/2020/06/21/free-registeration-to-tcs-women-rising-star-talks/" rel="alternate" type="text/html"/>
    <title>(Free) Registeration to TCS Women Rising Star talks</title>
    <summary>TCS Women Rising Star talks are happening as part of TCS Women STOC Spotlight Workshop. Seven Rising Star speakers are lined up, all of whom are planning to be on the job market this year. In addition, Shafi Goldwasser will give a longer talk. Everybody is invited, but (free) registration is required. (Attendees don’t even have to pay the STOC registration fee.) More information is on the website:https://sigact.org/tcswomen/.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>TCS Women Rising Star talks are happening as part of TCS Women STOC Spotlight Workshop. Seven Rising Star speakers are lined up, all of whom are planning to be <em><strong>on the job market this year</strong></em>. In addition, Shafi Goldwasser will give a longer talk. Everybody is invited, but (free) registration is required. (Attendees don’t even have to pay the STOC registration fee.) More information is on the website:<br/><a href="https://sigact.org/tcswomen/" rel="noreferrer noopener" target="_blank">https://sigact.org/tcswomen/</a>. </p></div>
    </content>
    <updated>2020-06-22T04:16:26Z</updated>
    <published>2020-06-22T04:16:26Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-06-24T16:21:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2020/06/22/virtual-conferences/</id>
    <link href="http://benjamin-recht.github.io/2020/06/22/virtual-conferences/" rel="alternate" type="text/html"/>
    <title>The Uncanny Valley of Virtual Conferences</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We wrapped up two amazing days of <a href="http://www.l4dc.org/">L4DC 2020</a> last Friday. It’s pretty wild to watch this community grow so quickly: starting as a <a href="https://kgatsis.github.io/learning_for_control_workshop_CDC2018/">workshop</a> at <a href="https://kgatsis.github.io/learning_for_control_workshop_CDC2018/">CDC 2018</a>, the conference organizers put together an <a href="https://l4dc.mit.edu/">inaugural event at MIT</a> in only a few months and were overwhelmed by nearly 400 attendees. Based on a groundswell of support from the participants, we decided to add contributed talks and papers this year. We had passionate volunteers for our <a href="https://sites.google.com/berkeley.edu/l4dc/organizers-pc">70-person program committee</a>, and they did heroic work of reviewing 135 submissions for this year’s program.</p>

<p>Then, of course, the pandemic hit forcing us to cancel our in-person event. As most conferences in a similar situation as ours, we decided to move to a virtual setting. I think that had we not had contributed papers, we would have simply canceled this year (I’ll return to this later). But to respect the passion and hard-work of our contributors, we tried to come up with a reasonable plan for running this conference virtually.</p>

<p>When we started planning to go virtual, there were too many options to sort through: Zoom webinars and breakout rooms? Sli.do Q&amp;As? Google Hangouts? Slack channels? We had so many tools for virtual community building, each with their own pluses and minuses. Our main constraints were that we wanted to highlight the best contributed papers as talks in some way, to give visibility to the wonderful set of accepted papers without burdening the authors with more work, to be inclusive to the broader community of folks interested in learning and automation, and, importantly, to not charge registration fees.</p>

<p>We eventually settled on the following scheme:</p>
<ol>
  <li>We had a Zoom room for invited and contributed speakers and moderators.</li>
  <li>This Zoom was <a href="https://www.youtube.com/watch?v=b_sJb1k9dVY">live streamed to Youtube</a>.</li>
  <li>Questions were gathered by grad student moderators who scanned the YouTube live chat and then relayed inquiries back to the speakers.</li>
  <li>We tried to keep the live part under four hours per day and to provide ample breaks. We recognize how hard it is to sit in front of a live stream for much more than that.</li>
  <li>Further discussion was then done on <a href="https://openreview.net/group?id=L4DC.org/2020/Conference">OpenReview</a>, where we hosted all accepted papers of the conference.</li>
  <li>The proceedings of the conference were subsequently archived by <a href="http://proceedings.mlr.press/">Proceedings of Machine Learning Research</a>.</li>
</ol>

<p>Though it took a lot of work to tie all these pieces together, everything went super smoothly in the end. I was basically able to run the entire AV setup from my garage.</p>

<p class="center"><img alt="where the magic happens" src="http://www.argmin.net/assets/command_station.jpg" width="250px"/></p>

<p>The only thing that cost money here was the Zoom account (20 dollars/month, though subsidized by Berkeley) and my home internet connection. I know that Zoom and YouTube have well documented issues, and I think it’s imperative that they continue to strive to fix these problems, but I also think it’s easy to forget how empowering this technology is. This format opens up conferences to those who can’t travel for financial or logistical reasons, and lowers the energy to engage with cutting edge research. Being able to sit in my garage and run a virtual conference with speakers spanning 10 time zones and nearly 2000 viewers is a wonder of modern times.</p>

<h2 id="second-life-still-has-a-long-way-to-go">Second Life still has a long way to go.</h2>

<p>There are still many parts of the online conference that felt cheated and incomplete. I still don’t know how to run a virtual poster session effectively. Most of our papers have not yet received any comments on <a href="https://openreview.net/group?id=L4DC.org/2020/Conference">OpenReview</a>, though comments are still open and I’d encourage you to drop by and ask questions! Partially, I think this lack of engagement stems from the considerable amount of effort required to participate, especially when it is compared to somewhat aimlessly ambling through a poster session.</p>

<p>Indeed, many aspects of live conferences are simply not replicable with our current tools, whether they be chance encounters or meetings with friends from far away. On the other hand, maybe we shouldn’t try to replicate this experience! Maybe we need to think harder about what opportunities our technology has for building communities and how we can better support these facets of academic interaction. When I think back on the decades of conferences I’ve attended, I can think of only a few posters that really got me interested in reading a paper, and <a href="https://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf">one later won a test of time award at NeurIPS</a>. Poster sessions always felt like an anachronistic means to justify a work travel expense rather than an effective means of academic knowledge dissemination. Is there a better way forward that uses our current technological constraints to amplify the voices of young scholars with cutting edge ideas? I don’t have great ideas for how to do this yet, but new interaction structures may emerge as we deal with at least one more year without meetings with hundreds of people.</p>

<h2 id="how-much-should-conferences-cost">How much should conferences cost?</h2>

<p>We were able to do L4DC, with the proceedings and all, for free. Obviously, the program committee put in tons of work in reviewing and organizing the logistics. But reviewing labor isn’t compensated by any conference. All peer reviewed conferences rely on the volunteer service labor of a dedicated program committee. The main line items we expected for L4DC were for renting physical space, paying an AV crew, and food. But in the virtual world, these expenses drop to near zero.</p>

<p>I’m supposed to give a plenary talk at the <a href="https://www.ifac2020.org/">Virtual IFAC Congress</a> in July. I have to say, I am troubled: IFAC is charging <a href="https://www.ifac2020.org/registration/">380 euros per person</a> for registration. What does one get for this sum? Access to video streams and the ability to publish papers. This seems exorbitantly expensive. Why would anyone watch a talk I give at IFAC when I promise to just release it on YouTube at the same time? What value is IFAC providing back to the academic community?</p>

<h2 id="decoupling-papers-from-talks">Decoupling papers from talks</h2>

<p>One of the main things the registration fee at many conferences provides is a stamp of academic approval. It is a de facto publication fee. Led by computer science, conferences in engineering are replacing journals as the archives where CV-building work is cataloged. Though this wasn’t the initial purpose of conferences in computer science, conferences do have many attractive features over journals for rapidly evolving fields: Conferences have speedy turn-around times and clearly delineated submission and decision dates. This archival aspect of conferences, however, has nothing to do with community building or scholarly dissemination. Why do we need to couple a talk to a publication? Can’t we separate these two as is done in every other academic field?</p>

<p>Our collective pandemic moment gives us an opportunity not only to rethink community-building but also our publication model. With 10000-person mega-conferences like <a href="http://icml.cc">AI Summer</a> and <a href="http://neurips.cc">AI Winter</a>, why can’t we keep all of the deadlines the same but remove all of the talks? We’d still have the same reviewing architecture, which has been wholly virtual for over a decade. And we could still publish all of the proceedings online for free, which has been done for multiple decades.</p>

<p>The decoupling proposal here would have effectively zero overhead on our communities: the deadlines, CMTs, program committees, and proceedings could all function exactly the same way (though, to be fair, these systems all have warts worth improving upon). New archival, fast-turnaround journals could easily start using the same tools. Indeed, I’ve always been enamored with the idea of an arxiv-overlay journal that simply is a table of contents that points towards particular versions of arxiv papers as “accepted.” And a really radical idea would be to solicit <em>talks</em>—not papers—for virtual conferences where potential speakers would submit slides or videos to demonstrate proficiency in the medium in which they’d present.</p>

<p>I tend to dismiss most of the bloviation about how coronavirus permanently changes everything about how we live our lives. But it does provide us an opportunity to pause and assess whether current systems are functioning well. I’d argue that the current conference system hasn’t been functioning well for a while, but this simple decoupling of papers and talks might clear up a lot of the issues currently facing the hard-charging computing world.</p>

<p><em>Many thanks to my dedicated, passionate L4DC Co-organizers: Alex Bayen, Ali Jadbababie, George Pappas, Pablo Parrilo, Claire Tomlin, and Melanie Zeilinger. I’d also like to thank Rediet Abebe, Jordan Ellenberg, Eric Jonas, Angjoo Kanazawa, Adam Klivans, Nik Matni, Chris Re, and Tom Ristenpart for their helpful feedback on this post.</em></p></div>
    </summary>
    <updated>2020-06-22T00:00:00Z</updated>
    <published>2020-06-22T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2020-06-23T23:49:58Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/06/21/subpract</id>
    <link href="https://11011110.github.io/blog/2020/06/21/subpract.html" rel="alternate" type="text/html"/>
    <title>Subpract</title>
    <summary>I’ve written here before about subtraction games, two-player games in which the players remove tokens from a pile of tokens, the number of removed tokens is required to belong to a designated subtraction set, and the goal is to make the last move. For instance, subtract a square, a game I studied at FUN 2018, is of this type, with the subtraction set being the square numbers.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I’ve <a href="https://11011110.github.io/blog/2018/04/18/subtraction-games.html">written here before</a> about <a href="https://en.wikipedia.org/wiki/Subtraction_game">subtraction games</a>, two-player games in which the players remove tokens from a pile of tokens, the number of removed tokens is required to belong to a designated <em>subtraction set</em>, and the goal is to make the last move. For instance, <a href="https://en.wikipedia.org/wiki/Subtract_a_square">subtract a square</a>, a game <a href="https://doi.org/10.4230/LIPIcs.FUN.2018.20">I studied at FUN 2018</a>, is of this type, with the subtraction set being the square numbers.</p>

<p>At some point in studying these games I also briefly looked at the subtraction game whose subtraction set is the set of <a href="https://en.wikipedia.org/wiki/Practical_number">practical numbers</a>, the numbers whose sums of divisors include all values up to the given numbers. The sequence of these numbers begins</p>



<p>and it turns out to be important here that, after the first one, they’re all even. Let’s call the subtraction game with this subtraction set <em>subpract</em>.</p>

<p>For a subtraction game, or more generally any <a href="https://en.wikipedia.org/wiki/Impartial_game">impartial game</a>, the game states can be partitioned into -positions (where the player who played previously is winning with optimal play) and -positions (where the next player to move can force a win); the -positions tend to be rarer than the -positions, and it’s important to know where they are because the optimal strategy in the game is to move to a -position whenever possible.</p>

<p><a href="http://oeis.org/A275432">OEIS A275432</a> lists the -positions for subpract. They are:</p>



<p>For instance, it’s a winning move in subpract to move to a pile of ten tokens (if you can), because  whatever move your opponent makes from there lets you win. If your opponent takes an even number of tokens, you will be able to take all the remaining tokens and win immediately. And if your opponent takes one token, leaving a pile of nine tokens, you can win by taking six more and leaving a pile of three tokens. Then, regardless of how your opponent responds, you will be able to take all the tokens on your next move.</p>

<p>An obvious pattern jumps out from this list of -positions: they come in pairs, spaced three apart. More precisely, an even number  is a -position if and only if the odd number  is a -position. It’s not just a coincidence, true at the start of the sequence and then false later on: it carries on throughout the entire sequence of -positions. More strongly, this same three-apart pairing of  -positions holds for any subtraction game whose subtraction set contains <span style="white-space: nowrap;">, , and ,</span> and does not contain any other odd numbers.</p>

<h1 id="proof-of-the-pairing-property">Proof of the pairing property</h1>

<p>To prove this, I need to show that  is a -position if and only if  is a winning position. We can prove this by induction, where we assume that all the -positions below  and  are paired up in the same way, and use it to prove that the same pairing holds for  and . The basic idea of the proof is to assume that one of the two players has a winning strategy for the <span style="white-space: nowrap;">position ,</span> and to copy that strategy for , most of the time playing the same moves and responses that you would play for the smaller position. Whenever a sequence of moves is applicable to both  and  and preserves the parity of the starting position, the induction hypothesis shows that it has the same outcome for both starting positions. However, there are a few cases where you may be forced to deviate from this strategy:</p>

<ul>
  <li>
    <p>If  is an -position, but its winning move is to subtract one token leading to an odd -position , then copying that move from the starting position  would lead to the position  which may not be a -position. Instead you should subtract four tokens to get to the position  directly.</p>
  </li>
  <li>
    <p>If  is a -position, and you’re trying to copy its winning strategy in the position , your opponent may be able to subtract , a move that is not possible in , so you have no response to copy. But in this case the result is a pile of just one token, from which you can immediately win.</p>
  </li>
  <li>
    <p>Again, if you’re trying to copy the winning strategy for -position  in the position , your opponent may subtract only one token. In this case, the winning response when starting from  might be to subtract an even number, leading to an odd -position . If you copy this response, you will end up at  which may not be a -position. But instead of copying the -strategy, you can simply subtract two tokens, leading to the position  itself.</p>
  </li>
  <li>
    <p>Similarly, it may be the case that the winning response to an opponent’s even move from  is to take a single token, leading to odd -position . Copying this strategy from the starting position  would again lead to . But in this case you can subtract four tokens leading to  again.</p>
  </li>
</ul>

<p>It’s tempting to guess that, more strongly than pairing -positions and -positions in this way, subpract and similar subtraction games have a pairing of their <a href="https://en.wikipedia.org/wiki/Sprague%E2%80%93Grundy_theorem">nim-values</a>, where the nim-value of an odd position always equals the nim-value of the even position three units smaller. But it’s not true. For instance, in subpract, a pile of four tokens has nim-value 1 while a pile of seven tokens has nim-value 4.</p>

<h1 id="other-subtraction-sets">Other subtraction sets</h1>

<p>Probably the most obvious choice of another subtraction set that begins  and has no larger odd numbers would be the powers of two, but they don’t give rise to an interesting subtraction game: the -positions are just the multiples of three. The same thing happens whenever there are no multiples of three in the subtraction set, as happens for instance with the <a href="https://11011110.github.io/blog/2020/06/21/Telephone number (mathematics)">telephone numbers</a> and <a href="http://oeis.org/A003422">left factorials</a>.</p>

<p>Another natural subtraction set to which this theory applies is the sequence of <a href="http://oeis.org/A025487">Hardy–Ramanujan integers (A025487)</a>, the numbers whose prime factorization  has a non-increasing sequence of exponents . They are:</p>



<p>These are a subset of the practical numbers so one would expect their subtraction game to have more-dense -positions. My implementation found that these -positions are:</p>



<p>again obeying the offset-by-three pairing as it should, and otherwise having somewhat irregular intervals between its -positions.</p>

<p>The <a href="http://oeis.org/A000084">enumeration function of the series-parallel graphs and the cographs</a> is even after its first term because of series-parallel duality; it begins</p>



<p>These are not all practical; for instance, 10, 1532, and 43930 are not practical. The sequence of -positions for their subtraction game begins</p>



<p>mostly differing by three between consecutive values but with occasional glitches where the larger multiple-of-three subtraction set values kick in.</p>

<p>And finally, if we subtract numbers that are one less than a prime, we get the subtraction set</p>



<p>and the sequence of -positions</p>



<p>Its small values have many five-unit gaps but that pattern appears to die out after the quadruple of -positions .</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104384624632242432">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-06-21T16:29:00Z</updated>
    <published>2020-06-21T16:29:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-06-21T23:43:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17214</id>
    <link href="https://rjlipton.wordpress.com/2020/06/21/some-real-and-some-virtual-news/" rel="alternate" type="text/html"/>
    <title>Some Real and Some Virtual News</title>
    <summary>Gossip and more. Composite of , src1, src3 Jessica Deters, Izabel Aguiar, and Jacqueline Feuerborn are the authors of the paper, “The Mathematics of Gossip.” They use infection models—specifically the Susceptible-Infected-Recovered (SIR) model—to discuss gossip. Their work was done before the present pandemic, in 2017–2019. It is also described in a nice profile of Aguiar. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Gossip and more.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/06/detersaguiarfeuerborn-1.png"><img alt="" class="alignright size-full wp-image-17221" src="https://rjlipton.files.wordpress.com/2020/06/detersaguiarfeuerborn-1.png?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://jessicadeters.wordpress.com/">, </a><a href="https://izabelpaguiar.com/about/">src1</a>, <a href="https://www.linkedin.com/in/jacqueline-feuerborn-87b7b3106/">src3</a></font></td>
</tr>
</tbody>
</table>
<p>
Jessica Deters, Izabel Aguiar, and Jacqueline Feuerborn are the authors of the <a href="https://scholarship.claremont.edu/cgi/viewcontent.cgi?article=1036&amp;context=codee">paper</a>, “The Mathematics of Gossip.” They use infection models—specifically the Susceptible-Infected-Recovered (<a href="https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIR_model">SIR</a>) model—to discuss gossip. Their work was done <em>before</em> the present pandemic, in 2017–2019.  It is also described in a nice <a href="https://sciencebuffs.org/2018/05/14/to-gossip-is-human-to-math-divine/">profile</a> of Aguiar. Their analogy is expressed by a drawing in their paper: </p>
<p><a href="https://rjlipton.files.wordpress.com/2020/06/gossipcough.jpg"><img alt="" class="aligncenter wp-image-17218" height="125" src="https://rjlipton.files.wordpress.com/2020/06/gossipcough.jpg?w=400&amp;h=125" width="400"/></a></p>
<p/><p><br/>
Not just for today, but for the summer at least, Ken and I want to share some gossip, share some problems, and ask our readers a question.</p>
<p>
The question first. Ken and I wonder if GLL should start a virtual theory “lunch” meeting, that would meet periodically via Zoom. It would be like meeting for a theory lunch in the old days—just not all in the same room. Some topic might be agreed on, perhaps a short presentation, and always a chance to swap some gossip. Plus maybe ask the group for advice on a problem.</p>
<p>
I do miss the old meetings: </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/06/meet.png"><img alt="" class="aligncenter size-medium wp-image-17219" height="224" src="https://rjlipton.files.wordpress.com/2020/06/meet.png?w=300&amp;h=224" width="300"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">AcademicKeys #78 <a href="https://www.academickeys.com/all/cartoon.php?dothis=display&amp;cartoon[IDX]=78">source</a></font>
</td>
</tr>
</tbody></table>
<p>
What do you think? Should we have such meetings?</p>
<p>
</p><p/><h2> Some Gossip </h2><p/>
<p/><p>
The study of gossip feeds into other issues of the spread of information and misinformation during an election year. Ken’s Buffalo colleague Kenny Joseph has <a href="https://kennyjoseph.github.io/">research</a> on the spread of fake news and Twitter sentiment using mathematical tools adjacent to those of the trio above. Ken and I still intend to say more about epidemiology models ourselves when we get time. But no, here by “gossip” we just mean actual pieces of gossip—just as at a conference or other kind of in-person meeting.</p>
<p>
Here are two examples of the kind of gossip we might exchange. </p>
<p>
Anna Gilbert is moving from Michigan math to Yale math and statistics. She will be the John C. Malone Professor of Mathematics, Professor of Statistics &amp; Data Science. Pretty impressive. She was at Michigan math for <img alt="{2^{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{4}}"/> years. She told me that she could not wait for the next power of <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>, and thus had to try a new place, with new challenges. </p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/06/gilbert-1.png"><img alt="" class="aligncenter wp-image-17233" height="163" src="https://rjlipton.files.wordpress.com/2020/06/gilbert-1.png?w=127&amp;h=163" width="127"/></a></p>
<p>
Rich DeMillo is a long time friend who is at Georgia Institute of Technology and is not moving. He continues working on making voting fair, secure, and efficient. He is referenced in a recent article on voting issues in Georgia. See <a href="https://www.voanews.com/usa/us-politics/activists-cite-tabulation-flaw-georgia-mail-ballots">here</a> for details. </p>
<p>
<a href="https://rjlipton.files.wordpress.com/2020/06/demillo-1.jpg"><img alt="" class="aligncenter wp-image-17234" height="138" src="https://rjlipton.files.wordpress.com/2020/06/demillo-1.jpg?w=149&amp;h=138" width="149"/></a></p>
<p>
Note: we have in mind pleasant and factual gossip. The most useful kind is about probable directions and emphases to make projects attractive to pursue. This leads into our other component.</p>
<p>
</p><p/><h2> Possible Problems To Present </h2><p/>
<p/><p>
Here are a problem from each of us as examples of what could be discussed in these meetings and why that might give advantage over just hunting the literature. Both are about factoring—always factoring…</p>
<p>
</p><p/><h3> Factoring: Ken </h3><p/>
<p/><p>
I, Ken, would like to know about field tests of approximative methods in quantum computing, specifically of shortcuts to Shor’s Algorithm. The approximations I have in mind are rougher than those I find in the literature and need not be physically natural.</p>
<p>
To explain, the way Shor’s algorithm is proven correct in Shor’s paper and all textbooks we know—including ours—uses an exact analysis involving the quantum Fourier transform, in which exponentially fine phase angles appear in terms. Approximation can be argued in several ways. Circuits of Hadamard, CNOT, and <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>-gates, in which no phase angle finer than <img alt="{\pi/4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%2F4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi/4}"/> appears, can approximate arbitrary quantum circuits to exponential precision with polynomial overhead. With just Hadamard and Toffoli gates, hence <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> and <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi}"/> as the only phases, one can approximate the data returned by measurements in the algorithm, though without approximating the algorithm’s result vectors in complex Hilbert space. There are other ways to approximate those vectors while eliding the finer phase components. We would like to see more attention to the concrete overheads of all these methods.</p>
<p>
What I would really like to discuss, however, is efforts toward more-brusque approximations that could yield new classical attempts on factoring. For a broad example, note that not only does <img alt="{\mathsf{BQP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{BQP}}"/> reduce to <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{\#P}}"/> but also individual steps in Shor’s algorithm can be broken down as reductions to counting. Now suppose we apply approximate counting heuristics to those steps. The stock answer for why this doesn’t work to approximate quantum measurement properties <em>globally</em> is that those probabilities have the form </p>
<p align="center"><img alt="\displaystyle  p = \frac{f_1(x) - f_2(x)}{D} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p+%3D+%5Cfrac%7Bf_1%28x%29+-+f_2%28x%29%7D%7BD%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p = \frac{f_1(x) - f_2(x)}{D} "/></p>
<p>where <img alt="{f_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1}"/> and <img alt="{f_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_2}"/> are <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{\#P}}"/> functions and <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is something like <img alt="{2^{n/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{n/2}}"/>. Note the knowledge beforehand that <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> is between <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> and <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. The point is that <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is exponential yet smaller than the additive approximations possible in polynomial time for <img alt="{f_1(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1(x)}"/> and <img alt="{f_2(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_2%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_2(x)}"/> individually, so the approximation gives no help to the difference. </p>
<p>
However, this does not prevent using approximations of magnitudes that are not differences at intermediate steps. For a vein of more particular examples, I raise the translation from quantum gates to Boolean formulas in my 2018 <a href="https://link.springer.com/chapter/10.1007/978-3-662-56499-8_4">paper</a> with Amlan Chakrabarti and my recent PhD graduate Chaowen Guan. This translation can encode the state <img alt="{\Phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi}"/> at any intermediate stage of an execution of Shor’s algorithm by a Boolean formula <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/>. The size of <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> stays linear in the size of the quantum circuit being simulated—the exponential explosion happens only when we try to count solutions to <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/>. The formula <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> encodes all the information in <img alt="{\Phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi}"/>, including the implicit presence of fine phase angles. Now suppose we alter <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> to a <img alt="{\phi'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi'}"/> whose corresponding <img alt="{\Phi'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi'}"/> is a simplified approximation of <img alt="{\Phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi}"/>. The kicker is that <img alt="{\Phi'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi'}"/> might not need to be a legal quantum state. The transformations in our paper for later stages of the circuit will still apply building on <img alt="{\phi'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi'}"/>. </p>
<p>
Is there any chance of this working? Heuristic approaches applying SAT to factoring have been tried and found to be <a href="https://toughsat.appspot.com/">tough</a>. The nice site <a href="http://beyondnp.org/">BeyondNP</a> includes <a href="http://beyondnp.org/pages/solvers/model-counters-exact/">links</a> to #SAT counters such as <a href="https://sites.google.com/site/marcthurley/sharpsat">sharpSAT</a> and <a href="https://www.cs.rochester.edu/u/kautz/Cachet/index.htm">Cachet</a>. Thus we are not asking anything outlandish. Leveraging Shor’s algorithm might be a new approach. Has anyone tried it? That’s the kind of question I would visit a conference to ask, where wider arity might work better than asking people individually. Thus also for raising it in a meeting.</p>
<p>
</p><p/><h3> Factoring: Dick </h3><p/>
<p/><p>
I have recently been thinking about the power of weak sub-theories of Peano Arithmetic. There are many proofs known that there are an infinite number of prime numbers. The usual proofs use this:</p>
<blockquote><p><b> </b> <em> For all <img alt="{x&gt;1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%3E1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x&gt;1}"/> there is some prime <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{p}"/> that divides <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x}"/>. </em>
</p></blockquote>
<p>Given this it is not hard to prove, in many ways, that there are an infinite number of primes. Euclid’s original proof uses it in the step: Let <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> divide <img alt="{p_{1}\cdots p_{n} + 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7B1%7D%5Ccdots+p_%7Bn%7D+%2B+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{1}\cdots p_{n} + 1}"/>. The idea is suppose some weak theory can prove the above. This means that it can prove: 	</p>
<p align="center"><img alt="\displaystyle  \forall x&gt;1 \ \exists y \ y|x \text{ and } \mathsf{prime}(y). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x%3E1+%5C+%5Cexists+y+%5C+y%7Cx+%5Ctext%7B+and+%7D+%5Cmathsf%7Bprime%7D%28y%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall x&gt;1 \ \exists y \ y|x \text{ and } \mathsf{prime}(y). "/></p>
<p>	 I believe that this shows that if the theory is weak enough that this implies that factoring is in polynomial time. Is this known? Is it true? </p>
<p>
Once again, we can hunt for literature on this. We can ask individual people, such as Avi Wigderson and various co-authors of his. But our hunch is that this topic was explored in the 1990s without a definitive resolution. It could be more effective to get up to speed on it and share ideas in a meeting.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Should we start a virtual theory lunch? Would you attend?</p>
<p>
<b>Added 6/21:</b> Getting back to gossip, we wonder if the all-to-all nature of a Zoom meeting, versus few-to-few in a conference hallway, would filter out the badder kinds of gossip.</p></font></font></div>
    </content>
    <updated>2020-06-21T15:17:56Z</updated>
    <published>2020-06-21T15:17:56Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Factoring"/>
    <category term="Gossip"/>
    <category term="infection models"/>
    <category term="Izabel Aguiar"/>
    <category term="Jacqueline Feuerborn"/>
    <category term="Jessica Deters"/>
    <category term="Logic"/>
    <category term="meetings"/>
    <category term="quantum"/>
    <category term="Shor's algorithm"/>
    <category term="videoconferencing"/>
    <category term="witness functions"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-06-24T16:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4867</id>
    <link href="https://www.scottaaronson.com/blog/?p=4867" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4867#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4867" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Quantum Computing Since Democritus: New Foreword!</title>
    <summary xml:lang="en-US">Time for a non-depressing post. Quantum Computing Since Democritus, which is already available in English and Russian, is about to be published in both Chinese and Japanese. (So if you read this blog, but have avoided tackling QCSD because your Chinese or Japanese is better than your English, today’s your day!) To go along with […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Time for a non-depressing post.  <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing Since Democritus</a>, which is already available in English and Russian, is about to be published in both Chinese and Japanese.  (So if you read this blog, but have avoided tackling QCSD because your Chinese or Japanese is better than your English, today’s your day!)  To go along with the new editions, Cambridge University Press asked me to write a new foreword, reflecting on what happened in the seven years since the book was published.  The editor, Paul Dobson, kindly gave me permission to share the new foreword on my blog.  So without further ado…</p>



<p/><hr/><p/>



<p><em>Quantum Computing Since Democritus</em> began its life as a course that I taught at the University of Waterloo in 2006.  Seven years later, it became the book that you now hold.  Its preface ended with the following words:</p>



<blockquote class="wp-block-quote"><p>Here’s hoping that, in 2020, this book will be as badly in need of revision as the 2006 lecture notes were in 2013.</p></blockquote>



<p>As I write this, in June 2020, a lot has happened that I would never have predicted in 2013.  Donald Trump is the President of the United States, and is up for reelection shortly.  This is not a political book, so let me resist the urge to comment further.  Meanwhile, the coronavirus pandemic is ravaging the world, killing hundreds of thousands of people, crashing economies, and shutting down schools and universities (including mine).  And in the past few weeks, protests against racism and police brutality started in America and then spread to the world, despite the danger of protesting during a pandemic.</p>



<p>Leaving aside the state of the world, my own life is also very different than it was seven years ago.  Along with my family, I’ve moved from MIT to the University of Texas in Austin.  My daughter, who was born at almost exactly the same time as <em>Quantum Computing Since Democritus</em>, is now a first-grader, and is joined by a 3-year-old son.  When my daughter’s school shut down due to the coronavirus, I began home-schooling her in math, computer science, and physics—in some of the exact same topics covered in this book.  I’m now engaged in an experiment to see what portion of this material can be made accessible to a 7-year-old.</p>



<p>But what about the material itself?  How has it held up over seven years?  Both the bad news and the (for you) good news, I suppose, is that it’s <em>not</em> particularly out of date.  The intellectual underpinnings of quantum computing and its surrounding disciplines remain largely as they were.  Still, let me discuss what <em>has</em> changed.</p>



<p>Between 2013 and 2020, the field of quantum computing made a striking transition, from a mostly academic pursuit to a major technological arms race.  The Chinese government, the US government, and the European Union have all pledged billions of dollars for quantum computing research.  Google, Microsoft, IBM, Amazon, Alibaba, Intel, and Honeywell also now all have well-funded groups tasked with building quantum computers, or providing quantum-computing-related software and services, or even just doing classical computing that’s “quantum-inspired.”  These giants are joined by dozens of startups focused entirely on quantum computing.</p>



<p>The new efforts vary greatly in caliber; some efforts seem rooted in visions of what quantum computers will be able to help with, and how soon, that I find to be wildly overoptimistic or even irresponsible.  But perhaps it’s always this way when a new technology moves from an intellectual aspiration to a commercial prospect.  Having joined the field around 1999, before there were <em>any</em> commercial efforts in quantum computing, I’ve found the change disorienting.</p>



<p>But while some of the new excitement is based on pure hype—on marketers now mixing some “quantum” into their word-salad of “blockchain,” “deep learning,” etc., with no particular understanding of any of the ingredients—there really have been some scientific advances in quantum computing since 2013, a fire underneath the smoke.</p>



<p>Surely the crowning achievement of quantum computing during this period was the achievement of “quantum supremacy,” which a team at Google announced in the fall of 2019.  For the first time, a programmable quantum computer was used to outperform any classical computer on earth, running any currently known algorithm.  Google’s device, called “Sycamore,” with 53 superconducting qubits cooled to a hundredth of a degree above absolute zero, solved a well-defined albeit probably useless sampling problem in about 3 minutes.  To compare, current state-of-the-art simulations on classical computers need a few days, even with hundreds of thousands of parallel processors.  Ah, but will a better classical simulation be possible?  That’s an open question in quantum complexity!  The discussion of that question draws on theoretical work that various colleagues and I did over the past decade.  That work in turn draws on my so-called <strong>PostBQP</strong>=<strong>PP</strong> theorem from 2004, explained in this book.</p>



<p>In the past seven years, there were also several breakthroughs in quantum computing theory—some of which resolved open problems mentioned in this book. </p>



<p>In 2018, Ran Raz and Avishay Tal gave an oracle relative to which <strong>BQP</strong> (Bounded-Error Quantum Polynomial-Time) is not contained in <strong>PH</strong> (the Polynomial Hierarchy).  This solved one of the main open questions, since 1993, about where <strong>BQP</strong> fits in with classical complexity classes, at least in the black-box setting.  (What does that mean?  Read the book!)  Raz and Tal’s proof used a candidate problem that I had defined in 2009 and called “Forrelation.”</p>



<p>Also in 2018, Urmila Mahadev gave a protocol, based on cryptography, by which a polynomial-time quantum computer (i.e., a <strong>BQP</strong> machine) could always prove the results of its computation to a classical polynomial-time skeptic, purely by exchanging classical messages with the skeptic.  Following Urmila’s achievement, I was delighted to give her a $25 prize for solving the problem that I’d announced on my blog back in 2007.</p>



<p>Perhaps most spectacularly of all, in 2020, Zhengfeng Ji, Anand Natarajan, Thomas Vidick, John Wright, and Henry Yuen proved that <strong>MIP*</strong>=<strong>RE</strong>.  Here <strong>MIP*</strong> means the class of problems solvable using multi-prover interactive proof systems with quantumly entangled provers (and classical polynomial-time verifiers), while <strong>RE</strong> means Recursively Enumerable: a class that includes not only all the computable problems, but even the infamous halting problem (!).  To say it more simply, <em>entangled provers can convince a polynomial-time verifier that an arbitrary Turing machine halts</em>.  Besides its intrinsic interest, a byproduct of this breakthrough was to answer a decades-old question in pure math, the so-called Connes Embedding Conjecture (by <em>refuting</em> the conjecture).  To my knowledge, the new result represents the first time that quantum computing has reached “all the way up the ladder of hardness” to touch uncomputable problems.  It’s also the first time that non-relativizing techniques, like the ones central to the study of interactive proofs, were ever used in computability theory.</p>



<p>In a different direction, the last seven years have witnessed an astonishing convergence between quantum information and quantum gravity—something that was just starting when <em>Quantum Computing Since Democritus</em> appeared in 2013, and that I mentioned as an exciting new direction.  Since then, the so-called “It from Qubit” collaboration has brought together quantum computing theorists with string theorists and former string theorists—experts in things like the black hole information problem—to develop a shared language.  One striking proposal that’s emerged from this is a fundamental role for <em>quantum circuit complexity</em>—that is, the smallest number of 1- and 2-qubit gates needed to prepare a given n-qubit state from the all-0 state—in the so-called AdS/CFT (Anti de Sitter / Conformal Field Theory) correspondence.  AdS/CFT is a duality between physical theories involving different numbers of spatial dimensions; for more than twenty years, it’s been a central testbed for ideas about quantum gravity.  But the duality is extremely nonlocal: a “simple” quantity in the AdS theory, like the volume of a wormhole, can correspond to an incredibly “complicated” quantity in the dual CFT.  The new proposal is that the CFT quantity might be not just complicated, but literally circuit complexity itself.  Fanciful as that sounds, the truth is that no one has come up with any other proposal that passes the same sanity checks.  A related new insight is that the nonlocal mapping between the AdS and CFT theories is not merely analogous to, but literally an example of, a quantum error-correcting code: the same mathematical objects that will be needed to build scalable quantum computers.</p>



<p>When <em>Quantum Computing Since Democritus</em> was first published, some people thought it went too far in elevating computer science, and computational complexity in particular, to fundamental roles in understanding the physical world.  But even I wasn’t audacious enough to posit connections like the ones above, which are now more-or-less mainstream in quantum gravity research.</p>



<p>I’m proud that I wrote <em>Quantum Computing Since Democritus</em>, but as the years go by, I find that I have no particular desire to revise it, or even reread it.  It seems far better for the book to stand as a record of what I knew and believed and cared about at a certain moment in time.</p>



<p>The intellectual quest that’s defined my life—the quest to wrap together computation, physics, math, and philosophy into some sort of coherent picture of the world—might never end.  But it does need to start somewhere.  I’m honored that you chose <em>Quantum Computing Since Democritus</em> as a place to start or continue your own quest.  I hope you enjoy it.</p>



<p>Scott Aaronson<br/>Austin, Texas<br/>June 2020</p></div>
    </content>
    <updated>2020-06-20T23:48:04Z</updated>
    <published>2020-06-20T23:48:04Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum Computing Since Democritus"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-06-24T07:33:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7753</id>
    <link href="https://windowsontheory.org/2020/06/19/stoc-2020-information-guest-post-by-madhur-tulsiani/" rel="alternate" type="text/html"/>
    <title>STOC 2020 information (guest post by Madhur Tulsiani)</title>
    <summary>Dear fellow theorists, As you already know, STOC 2020 this year will be a virtual conference. If you are interested in attending the conference, but haven’t registered yet, please do so soon (students: $25, regular: $50). This will help us ensure we have capacity for various online events.  Upon registration, you should receive a confirmation […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Dear fellow theorists,</p>



<p>As you already know, STOC 2020 this year will be a virtual conference. If you are interested in attending the conference, but haven’t registered yet, <a href="http://www.cvent.com/events/52nd-annual-acm-symposium-on-theory-of-computing-stoc-2020-/event-summary-ea5fa7861d1a476d82bc10f667a1c0f4.aspx" rel="noreferrer noopener" target="_blank">please do so soon</a> (students: $25, regular: $50). This will help us ensure we have capacity for various online events. </p>



<p>Upon registration, you should receive a confirmation email from CVENT, also containing access information for various conference events. Also, if you are a student looking to register for STOC but the cost is a burden, please email us at <a>stoc2020@ttic.edu</a>.</p>



<p><strong>How will the conference work?</strong></p>



<ul><li><strong>Videos</strong>: The videos for all conference talks are now available on YouTube, and can be accessed through the links in the <a href="http://acm-stoc.org/stoc2020/STOCprogram.html" rel="noreferrer noopener" target="_blank">conference program</a>. Registration is <em>not required</em> to view the talks on Youtube.</li></ul>



<ul><li>Slack: The conference has a Slack workspace, with one channel for every paper and workshop, and additional channels for information, announcements, social events, help, etc. The invitations for the Slack workspace will be sent to registered participants. Authors are also encouraged to monitor the channels for their papers. All access information for the conference will also be available here. The workspace is currently active, and will remain active for at least one week after the conference.</li></ul>



<ul><li><strong>Zoom sessions</strong>: The conference will feature Zoom sessions with short presentations by the speakers. The total time for each paper is 10 minutes. Given that participants have access to the full talks by the speakers on Youtube, these can be thought of as being analogues of poster sessions. The workshops will also be held as separate sessions. The links for the Zoom sessions are available via information in the registration confirmation email.</li></ul>



<ul><li><strong>Social events</strong>: The conference will include junior/senior “lunches”, breakout tables for impromptu and scheduled hangouts, and a group event using <a href="https://gather.town/" rel="noreferrer noopener" target="_blank">gather.town</a>. The timings for the events can be found in the conference program. Sign-up links for various events will be sent to all registered participants – please do sign-up soon!</li></ul>



<p>See you all at (virtual) STOC 2020. Please do let us know if you have any questions or suggestions.</p>



<p>TheoryFest organization team</p>



<p>(<a>stoc2020@ttic.edu</a>)</p></div>
    </content>
    <updated>2020-06-19T22:05:41Z</updated>
    <published>2020-06-19T22:05:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-06-24T16:21:12Z</updated>
    </source>
  </entry>
</feed>
