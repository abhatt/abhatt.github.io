<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-11-09T06:39:13Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03611</id>
    <link href="http://arxiv.org/abs/2111.03611" rel="alternate" type="text/html"/>
    <title>Approximately Efficient Bilateral Trade</title>
    <feedworld_mtime>1636416000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deng:Yuan.html">Yuan Deng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mao:Jieming.html">Jieming Mao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sivan:Balasubramanian.html">Balasubramanian Sivan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Kangning.html">Kangning Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03611">PDF</a><br/><b>Abstract: </b>We study bilateral trade between two strategic agents. The celebrated result
of Myerson and Satterthwaite states that in general, no incentive-compatible,
individually rational and weakly budget balanced mechanism can be efficient.
I.e., no mechanism with these properties can guarantee a trade whenever buyer
value exceeds seller cost. Given this, a natural question is whether there
exists a mechanism with these properties that guarantees a constant fraction of
the first-best gains-from-trade, namely a constant fraction of the
gains-from-trade attainable whenever buyer's value weakly exceeds seller's
cost. In this work, we positively resolve this long-standing open question on
constant-factor approximation, mentioned in several previous works, using a
simple mechanism.
</p></div>
    </summary>
    <updated>2021-11-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03492</id>
    <link href="http://arxiv.org/abs/2111.03492" rel="alternate" type="text/html"/>
    <title>Fast FPT-Approximation of Branchwidth</title>
    <feedworld_mtime>1636416000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Korhonen:Tuukka.html">Tuukka Korhonen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03492">PDF</a><br/><b>Abstract: </b>Branchwidth determines how graphs, and more generally, arbitrary connectivity
(basically symmetric and submodular) functions could be decomposed into a
tree-like structure by specific cuts. We develop a general framework for
designing fixed-parameter tractable (FPT) 2-approximation algorithms for
branchwidth of connectivity functions. The first ingredient of our framework is
combinatorial. We prove a structural theorem establishing that either a
sequence of particular refinement operations could decrease the width of a
branch decomposition or that the width of the decomposition is already within a
factor of 2 from the optimum. The second ingredient is an efficient
implementation of the refinement operations for branch decompositions that
support efficient dynamic programming. We present two concrete applications of
our general framework.
</p>
<p>$\bullet$ An algorithm that for a given $n$-vertex graph $G$ and integer $k$
in time $2^{2^{O(k)}} n^2$ either constructs a rank decomposition of $G$ of
width at most $2k$ or concludes that the rankwidth of $G$ is more than $k$. It
also yields a $(2^{2k+1}-1)$-approximation algorithm for cliquewidth within the
same time complexity, which in turn, improves to $f(k)n^2$ the running times of
various algorithms on graphs of cliquewidth $k$. Breaking the "cubic barrier"
for rankwidth and cliquewidth was an open problem in the area.
</p>
<p>$\bullet$ An algorithm that for a given $n$-vertex graph $G$ and integer $k$
in time $2^{O(k)} n$ either constructs a branch decomposition of $G$ of width
at most $2k$ or concludes that the branchwidth of $G$ is more than $k$. This
improves over the 3-approximation that follows from the recent treewidth
2-approximation of Korhonen [FOCS 2021].
</p></div>
    </summary>
    <updated>2021-11-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03361</id>
    <link href="http://arxiv.org/abs/2111.03361" rel="alternate" type="text/html"/>
    <title>Fast Deterministic Fully Dynamic Distance Approximation</title>
    <feedworld_mtime>1636416000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brand:Jan_van_den.html">Jan van den Brand</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Forster:Sebastian.html">Sebastian Forster</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nazari:Yasamin.html">Yasamin Nazari</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03361">PDF</a><br/><b>Abstract: </b>In this paper, we develop deterministic fully dynamic algorithms for
computing approximate distances in a graph with worst-case update time
guarantees. In particular we obtain improved dynamic algorithms that, given an
unweighted and undirected graph $G=(V,E)$ undergoing edge insertions and
deletions, and a parameter $0 &lt; \epsilon \leq 1$, maintain
$(1+\epsilon)$-approximations of the $st$ distance of a single pair of nodes,
the distances from a single source to all nodes ("SSSP"), the distances from
multiple sources to all nodes ("MSSP''), or the distances between all nodes
("APSP").
</p>
<p>Our main result is a deterministic algorithm for maintaining
$(1+\epsilon)$-approximate single-source distances with worst-case update time
$O(n^{1.529})$ (for the current best known bound on the matrix multiplication
coefficient $\omega$). This matches a conditional lower bound by [BNS, FOCS
2019]. We further show that we can go beyond this SSSP bound for the problem of
maintaining approximate $st$ distances by providing a deterministic algorithm
with worst-case update time $O(n^{1.447})$. This even improves upon the fastest
known randomized algorithm for this problem.
</p>
<p>At the core, our approach is to combine algebraic distance maintenance data
structures with near-additive emulator constructions. This also leads to novel
dynamic algorithms for maintaining $(1+\epsilon, \beta)$-emulators that improve
upon the state of the art, which might be of independent interest. Our
techniques also lead to improvements for randomized approximate diameter
maintenance.
</p></div>
    </summary>
    <updated>2021-11-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03280</id>
    <link href="http://arxiv.org/abs/2111.03280" rel="alternate" type="text/html"/>
    <title>Geometric construction of canonical 3D gadgets in origami extrusions</title>
    <feedworld_mtime>1636416000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Doi:Mamoru.html">Mamoru Doi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03280">PDF</a><br/><b>Abstract: </b>In a series of our three previous papers, we presented several constructions
of positive and negative 3D gadgets in origami extrusions which create with two
simple outgoing pleats a top face parallel to the ambient paper and two side
faces sharing a ridge, where a 3D gadget is said to be positive (resp.
negative) if the top face of the resulting gadget seen from the front side lies
above (resp. below) the ambient paper. For any possible set of angle
parameters, we obtained an infinite number of positive 3D gadgets in our second
paper, while we obtained a unique negative 3D gadget by our third construction
in our third paper. In this paper we present a geometric (ruler and compass)
construction of our third negative 3D gadgets, while the construction presented
in our third paper was a numerical one using a rather complicated formula.
Also, we prove that there exists a unique positive 3D gadget corresponding to
each of our third negative ones. Thus we obtain a canonical pair of a positive
and a negative 3D gadget. The proof is based on a geometric redefinition of the
critical angles which we introduced in constructing our positive 3D gadgets.
This redefinition also enables us to give a simplified proof of the existence
theorem of our positive 3D gadgets in our second paper. As an application, we
can construct a positive and a negative extrusion from a common crease pattern
by using the canonical counterparts, as long as there arise no interferences.
</p></div>
    </summary>
    <updated>2021-11-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03247</id>
    <link href="http://arxiv.org/abs/2111.03247" rel="alternate" type="text/html"/>
    <title>Entropic Independence II: Optimal Sampling and Concentration via Restricted Modified Log-Sobolev Inequalities</title>
    <feedworld_mtime>1636416000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anari:Nima.html">Nima Anari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jain:Vishesh.html">Vishesh Jain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koehler:Frederic.html">Frederic Koehler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pham:Huy_Tuan.html">Huy Tuan Pham</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vuong:Thuy=Duong.html">Thuy-Duong Vuong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03247">PDF</a><br/><b>Abstract: </b>We introduce a framework for obtaining tight mixing times for Markov chains
based on what we call restricted modified log-Sobolev inequalities. Modified
log-Sobolev inequalities (MLSI) quantify the rate of relative entropy
contraction for the Markov operator, and are notoriously difficult to
establish. However, infinitesimally close to stationarity, entropy contraction
becomes equivalent to variance contraction, a.k.a. a Poincare inequality, which
is significantly easier to establish through, e.g., spectral analysis.
Motivated by this observation, we study restricted modified log-Sobolev
inequalities that guarantee entropy contraction not for all starting
distributions, but for those in a large neighborhood of the stationary
distribution. We show how to sample from the hardcore and Ising models on
$n$-node graphs that have a constant $\delta$ relative gap to the
tree-uniqueness threshold, in nearly-linear time $\widetilde O_{\delta}(n)$.
Notably, our bound does not depend on the maximum degree $\Delta$, and is
therefore optimal even for high-degree graphs. This improves on prior mixing
time bounds of $\widetilde O_{\delta, \Delta}(n)$ and $\widetilde
O_{\delta}(n^2)$, established via (non-restricted) modified log-Sobolev and
Poincare inequalities respectively. We further show that optimal concentration
inequalities can still be achieved from the restricted form of modified
log-Sobolev inequalities. To establish restricted entropy contraction, we
extend the entropic independence framework of Anari, Jain, Koehler, Pham, and
Vuong to the setting of distributions that are spectrally independent under a
restricted set of external fields. We also develop an orthogonal trick that
might be of independent interest: utilizing Bernoulli factories we show how to
implement Glauber dynamics updates on high-degree graphs in $O(1)$ time,
assuming standard adjacency array representation of the graph.
</p></div>
    </summary>
    <updated>2021-11-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03198</id>
    <link href="http://arxiv.org/abs/2111.03198" rel="alternate" type="text/html"/>
    <title>On the Complexity of Dynamic Submodular Maximization</title>
    <feedworld_mtime>1636416000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xi.html">Xi Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Binghui.html">Binghui Peng</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03198">PDF</a><br/><b>Abstract: </b>We study dynamic algorithms for the problem of maximizing a monotone
submodular function over a stream of $n$ insertions and deletions. We show that
any algorithm that maintains a $(0.5+\epsilon)$-approximate solution under a
cardinality constraint, for any constant $\epsilon&gt;0$, must have an amortized
query complexity that is $\mathit{polynomial}$ in $n$. Moreover, a linear
amortized query complexity is needed in order to maintain a $0.584$-approximate
solution. This is in sharp contrast with recent dynamic algorithms of [LMNF+20,
Mon20] that achieve $(0.5-\epsilon)$-approximation with a
$\mathsf{poly}\log(n)$ amortized query complexity.
</p>
<p>On the positive side, when the stream is insertion-only, we present efficient
algorithms for the problem under a cardinality constraint and under a matroid
constraint with approximation guarantee $1-1/e-\epsilon$ and amortized query
complexities $\smash{O(\log (k/\epsilon)/\epsilon^2)}$ and
$\smash{k^{\tilde{O}(1/\epsilon^2)}\log n}$, respectively, where $k$ denotes
the cardinality parameter or the rank of the matroid.
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03193</id>
    <link href="http://arxiv.org/abs/2111.03193" rel="alternate" type="text/html"/>
    <title>Explainable k-means. Don't be greedy, plant bigger trees!</title>
    <feedworld_mtime>1636416000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makarychev:Konstantin.html">Konstantin Makarychev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shan:Liren.html">Liren Shan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03193">PDF</a><br/><b>Abstract: </b>We provide a new bi-criteria $\tilde{O}(\log^2 k)$ competitive algorithm for
explainable $k$-means clustering. Explainable $k$-means was recently introduced
by Dasgupta, Frost, Moshkovitz, and Rashtchian (ICML 2020). It is described by
an easy to interpret and understand (threshold) decision tree or diagram. The
cost of the explainable $k$-means clustering equals to the sum of costs of its
clusters; and the cost of each cluster equals the sum of squared distances from
the points in the cluster to the center of that cluster. Our randomized
bi-criteria algorithm constructs a threshold decision tree that partitions the
data set into $(1+\delta)k$ clusters (where $\delta\in (0,1)$ is a parameter of
the algorithm). The cost of this clustering is at most $\tilde{O}(1/\delta
\cdot \log^2 k)$ times the cost of the optimal unconstrained $k$-means
clustering. We show that this bound is almost optimal.
</p></div>
    </summary>
    <updated>2021-11-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03174</id>
    <link href="http://arxiv.org/abs/2111.03174" rel="alternate" type="text/html"/>
    <title>Single-Sample Prophet Inequalities via Greedy-Ordered Selection</title>
    <feedworld_mtime>1636416000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Caramanis:Constantine.html">Constantine Caramanis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uuml=tting:Paul.html">Paul Dütting</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Faw:Matthew.html">Matthew Faw</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fusco:Federico.html">Federico Fusco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lazos:Philip.html">Philip Lazos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leonardi:Stefano.html">Stefano Leonardi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Papadigenopoulos:Orestis.html">Orestis Papadigenopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pountourakis:Emmanouil.html">Emmanouil Pountourakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reiffenh=auml=user:Rebecca.html">Rebecca Reiffenhäuser</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03174">PDF</a><br/><b>Abstract: </b>We study single-sample prophet inequalities (SSPIs), i.e., prophet
inequalities where only a single sample from each prior distribution is
available. Besides a direct, and optimal, SSPI for the basic single choice
problem [Rubinstein et al., 2020], most existing SSPI results were obtained via
an elegant, but inherently lossy, reduction to order-oblivious secretary (OOS)
policies [Azar et al., 2014]. Motivated by this discrepancy, we develop an
intuitive and versatile greedy-based technique that yields SSPIs directly
rather than through the reduction to OOSs. Our results can be seen as
generalizing and unifying a number of existing results in the area of prophet
and secretary problems. Our algorithms significantly improve on the competitive
guarantees for a number of interesting scenarios (including general matching
with edge arrivals, bipartite matching with vertex arrivals, and certain
matroids), and capture new settings (such as budget additive combinatorial
auctions). Complementing our algorithmic results, we also consider mechanism
design variants. Finally, we analyze the power and limitations of different
SSPI approaches by providing a partial converse to the reduction from SSPI to
OOS given by Azar et al.
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03137</id>
    <link href="http://arxiv.org/abs/2111.03137" rel="alternate" type="text/html"/>
    <title>Big-Step-Little-Step: Efficient Gradient Methods for Objectives with Multiple Scales</title>
    <feedworld_mtime>1636416000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Jonathan Kelner, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marsden:Annie.html">Annie Marsden</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharan:Vatsal.html">Vatsal Sharan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sidford:Aaron.html">Aaron Sidford</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Valiant:Gregory.html">Gregory Valiant</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuan:Honglin.html">Honglin Yuan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03137">PDF</a><br/><b>Abstract: </b>We provide new gradient-based methods for efficiently solving a broad class
of ill-conditioned optimization problems. We consider the problem of minimizing
a function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ which is implicitly
decomposable as the sum of $m$ unknown non-interacting smooth, strongly convex
functions and provide a method which solves this problem with a number of
gradient evaluations that scales (up to logarithmic factors) as the product of
the square-root of the condition numbers of the components. This complexity
bound (which we prove is nearly optimal) can improve almost exponentially on
that of accelerated gradient methods, which grow as the square root of the
condition number of $f$. Additionally, we provide efficient methods for solving
stochastic, quadratic variants of this multiscale optimization problem. Rather
than learn the decomposition of $f$ (which would be prohibitively expensive),
our methods apply a clean recursive "Big-Step-Little-Step" interleaving of
standard methods. The resulting algorithms use $\tilde{\mathcal{O}}(d m)$
space, are numerically stable, and open the door to a more fine-grained
understanding of the complexity of convex optimization beyond condition number.
</p></div>
    </summary>
    <updated>2021-11-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03135</id>
    <link href="http://arxiv.org/abs/2111.03135" rel="alternate" type="text/html"/>
    <title>Scaffolding Sets</title>
    <feedworld_mtime>1636416000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Burhanpurkar:Maya.html">Maya Burhanpurkar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deng:Zhun.html">Zhun Deng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dwork:Cynthia.html">Cynthia Dwork</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Linjun.html">Linjun Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03135">PDF</a><br/><b>Abstract: </b>Predictors map individual instances in a population to the interval $[0,1]$.
For a collection $\mathcal C$ of subsets of a population, a predictor is
multi-calibrated with respect to $\mathcal C$ if it is simultaneously
calibrated on each set in $\mathcal C$. We initiate the study of the
construction of scaffolding sets, a small collection $\mathcal S$ of sets with
the property that multi-calibration with respect to $\mathcal S$ ensures
correctness, and not just calibration, of the predictor. Our approach is
inspired by the folk wisdom that the intermediate layers of a neural net learn
a highly structured and useful data representation.
</p></div>
    </summary>
    <updated>2021-11-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19306</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/" rel="alternate" type="text/html"/>
    <title>The Artificial Intelligence Historian</title>
    <summary>The past actually happened but history is only what someone wrote down—Whitney Brown CMU tribute Pamela McCorduck passed away last month. The New York Times obituary notes her interactions with many builders of the field of Artificial Intelligence from its infancy to its present state. Today we remember her and talk about AI’s near future. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>The past actually happened but history is only what someone wrote down—Whitney Brown</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/mccorduck-obit-700x700-min/" rel="attachment wp-att-19317"><img alt="" class="alignright wp-image-19317" height="156" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/mccorduck-obit-700x700-min.jpeg?resize=125%2C156&amp;ssl=1" width="125"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">CMU <a href="https://www.cmu.edu/news/stories/archives/2021/october/mccorduck-obit.html">tribute</a></font></td>
</tr>
</tbody>
</table>
<p>
Pamela McCorduck passed away last month. The New York Times <a href="https://www.nytimes.com/2021/11/04/technology/pamela-mccorduck-dead.html">obituary</a> notes her interactions with many builders of the field of Artificial Intelligence from its infancy to its present state. </p>
<p>
Today we remember her and talk about AI’s near future.</p>
<p>
I knew McCorduck through her late husband, Joe Traub, who we <a href="https://rjlipton.wpcomstaging.com/2015/08/31/how-joe-traub-beat-the-street/">memorialized</a> in 2015. He became the head of the CS department at Carnegie Mellon University in 1971. She also moved to CMU where she became an English teacher. Per the above quote by Brown, she helped make AI real by writing a number of books on its history. </p>
<p>
The NYT obit quotes something McCorduck wrote in her 2019 <a href="https://press.etc.cmu.edu/index.php/product/this-could-be-important/">memoir</a>, <em>This Could Be Important: My Life and Times With the Artificial Intelligentsia.</em> </p>
<blockquote><p><b> </b> <em> “For 60 years, I’ve lived in AI’s exponential. I’ve watched computers evolve from plodding sorcerer’s apprentices to machines that can best any humans at checkers, then chess, then the guessing game Jeopardy!, and now the deeply complex game of Go.” </em>
</p></blockquote>
<p/><p>
It is hard to project the future of an exponential, however. The best way I can try is to align it with my own field.</p>
<p>
</p><p/><h2> AI Movers and Shakers </h2><p/>
<p/><p>
At CMU McCorduck got to know the AI pioneers like Turing Award recipients Herbert Simon and Allen Newell and Raj Reddy. She already knew Edward Feigenbaum who said:</p>
<blockquote><p><b> </b> <em> She was dumped into this saturated milieu of the great and greatest in AI at Carnegie Mellon—some of the same people whose papers she’d helped us assemble—and decided to write a history of the field. </em>
</p></blockquote>
<p/><p>
The <a href="https://www.routledge.com/Machines-Who-Think-A-Personal-Inquiry-into-the-History-and-Prospects-of/McCorduck/p/book/9781568812052">book</a> was <em>Machines Who Think: A Personal Inquiry Into the History and Prospects of Artificial Intelligence.</em> Said Simon: </p>
<blockquote><p><b> </b> <em> She was interacting with all the movers and shakers of AI. She was in the middle of it, an eyewitness to history. </em>
</p></blockquote>
<p/><p>
I wish I knew more of her thoughts on the movers and shakers in Theory. She was well-versed in complexity of the dynamical-systems kind, and her husband’s work bridged to “our kind” of complexity. The title of her third <a href="https://www.amazon.com/Bounded-Rationality-Novel-Pamela-McCorduck/dp/0865348839">novel</a>, <em>Bounded Rationality</em>, speaks to both kinds of complexity from its setting at the Santa Fe Institute. This has led me to musing on the difference between AI and Theory.</p>
<p>
</p><p/><h2> AI Beats Theory </h2><p/>
<p/><p>
I never have worked on AI problems of any kind. The closest I ever came is I took a class at CMU as graduate student from Newell. He was a fun lecturer and the class was interesting. But I always worked on Theory. I must reflect a bit on why AI is so successful and Theory is less so. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/see/" rel="attachment wp-att-19311"><img alt="" class="aligncenter size-full wp-image-19311" height="195" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/see.png?resize=258%2C195&amp;ssl=1" width="258"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Pinterest <a href="https://www.pinterest.com/pin/55872851604309287/">source</a></font>
</td>
</tr>
</tbody></table>
<p>
Let’s start by saying that a field of research is determined not by who works in the field. Not by the tools that the field uses. It is determined by the problems that the field works on. AI is different from Theory because of the problems that it studies. This is the fundamental difference:</p>
<blockquote><p><b> </b> <em> AI looks at whole problems; Theory looks at sub-problems. </em>
</p></blockquote>
<p/><p>
What do I mean? AI studies problems that are concrete, that are big, that are as close to real problems as possible. For example, how to play Go or how to recognize images of faces. AI looks at problems that humans actually wish to solve: </p>
<blockquote><p><b> </b> <em> What move to make in this Go position? Or is this an image of X or Y? </em>
</p></blockquote>
<p/><p>
Theory looks at sub-problems. We look at a real problem and then identify some part of the problem that is hard to solve. We then try to invoke clever methods that show that this sub-problem can be done more efficiently that was previous known. This is hard in general. Is fun to work on in general. And leads to a beautiful field of study. One that is deep and rewarding. </p>
<p>
But Theory loses to AI. The issue is that no one really may wish to solve the sub-problem. This is real demand to solve the whole problem, but not the sub-problem. This is the fundamental advantage that AI holds over Theory.</p>
<p>
</p><p/><h2> AI Future </h2><p/>
<p/><p>
Public figures such as Stephen Hawking and Elon Musk have expressed concern that full artificial intelligence (AI) could result in human extinction. The consequences of the technological <a href="https://en.wikipedia.org/wiki/Technological_singularity">singularity</a> and its potential benefit or harm to the human race have been intensely debated.</p>
<p>
For our own part, we have <a href="https://rjlipton.wpcomstaging.com/2011/02/17/are-mathematicians-in-jeopardy/">wondered</a> whether an AI can take over in theory research. This puts a second light on possible meanings of “problems” in another quotation by McCorduck from her memoir, as related <a href="https://computerhistory.org/blog/the-future-humans-and-ai/">here</a>:</p>
<blockquote><p><b> </b> <em> “We can’t now say what living beside other, in some ways superior, intelligences will mean to us. Will it widen and raise our own individual and collective intelligence? In significant ways, it already has. Find solutions to problems we could never solve? Probably. Find solutions to problems we lack the wit even to propose? Maybe. Cause problems? Surely. AI has already shattered some of our fondest myths about ourselves and has shone unwelcome light on others. This will continue.</em></p><em>
<p>
…</p>
</em><p><em>
When people ask me my greatest worry about AI, I say: what we aren’t smart enough even to imagine.” </em>
</p></blockquote>
<p/><p>
Well, we can only talk about things we can imagine now. We can discuss facets of life that already outsource decisions to technology, such as high-speed stock trading and <a href="https://en.wikipedia.org/wiki/2010_flash_crash">several</a> <a href="https://www.technologyreview.com/2016/10/07/244656/algorithms-probably-caused-a-flash-crash-of-the-british-pound/">flash</a>–<a href="https://www.motherjones.com/politics/2013/02/high-frequency-trading-danger-risk-wall-street/">crashes</a> it has caused. </p>
<p>
But looking ahead, what is one near-term application area as a litmus test for the impact of AI? We think many will agree with our looking to <em>self-driving cars</em>. In taking over driving decisions, the AI expressly aims to reduce the evils of impaired or aggressive drivers. There have been <a href="https://www.washingtonpost.com/technology/2021/11/08/tesla-regulation-elon-musk/">mishaps</a> during development, sure, and the algorithms have not yet demonstrated robustness against possible deceptions. That is to say:</p>
<ul>
<li>
We can already see teething problems with this tech and imagine more along the same lines. <p/>
</li><li>
But can we project structural problems with the driverless paradigm whose concrete forms we have not imagined?
</li></ul>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What are your thoughts on the near future of AI? </p>
<p>
Our condolences go out to Pamela’s family and associates.</p>
<p/></font></font></div>
    </content>
    <updated>2021-11-08T22:23:34Z</updated>
    <published>2021-11-08T22:23:34Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="AI"/>
    <category term="artificial intelligence"/>
    <category term="in memoriam"/>
    <category term="Pamela McCorduck"/>
    <category term="singularity"/>
    <category term="Theory"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-11-09T06:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/151</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/151" rel="alternate" type="text/html"/>
    <title>TR21-151 |  Locally Testable Codes with constant rate, distance, and locality  | 

	Irit Dinur, 

	Shai Evra, 

	Ron Livne, 

	Alexander Lubotzky, 

	Shahar Mozes</title>
    <summary>A locally testable code (LTC) is an error correcting code that has a property-tester. The tester reads $q$ bits that are randomly chosen, and rejects words with probability proportional to their distance from the code. The parameter $q$ is called the locality of the tester.

LTCs were initially studied as important components of PCPs, and since then the topic has evolved on its own. High rate LTCs could be useful in practice: before attempting to decode a received word, one can save time by first quickly testing if it is close to the code.

An outstanding open question has been whether there exist "$c^3$-LTCs", namely LTCs with *c*onstant rate, *c*onstant distance, and *c*onstant locality.

In this work we construct such codes based on a new two-dimensional complex which we call a left-right Cayley complex. This is essentially a graph which, in addition to vertices and edges, also has squares. Our codes can be viewed as a two-dimensional version of (the one-dimensional) expander codes, where the codewords are functions on the squares rather than on the edges.</summary>
    <updated>2021-11-08T20:01:24Z</updated>
    <published>2021-11-08T20:01:24Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-09T06:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8225</id>
    <link href="https://windowsontheory.org/2021/11/08/new-quantum-science-and-engineering-phd-program/" rel="alternate" type="text/html"/>
    <title>New Quantum Science and Engineering PhD Program</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[Please forward this information to any undergraduate students in your program, or any relevant mailing lists or organizations you are aware of; we also have a postdoctoral fellowship in quantum computing. –Boaz] This year Harvard started a new Ph.D program in Quantum Science and Engineering. We are now accepting application for the first cohort of … <a class="more-link" href="https://windowsontheory.org/2021/11/08/new-quantum-science-and-engineering-phd-program/">Continue reading <span class="screen-reader-text">New Quantum Science and Engineering PhD Program</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Please forward this information to any undergraduate students in your program, or any relevant mailing lists or organizations you are aware of; we also have <a href="https://quantum.harvard.edu/external-candidates">a postdoctoral fellowship in quantum computing</a>. –Boaz]</em></p>



<p>This year Harvard started a new <a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering">Ph.D program in Quantum Science and Engineering</a>. We are now accepting application for the first cohort of students which will start in the 2022-2023 academic year.  This program can be an excellent fit for CS majors that are interested in quantum computation and information, and are looking for an interdisciplinary environment, with students from CS, physics, and other backgrounds, and with a curriculum and program that is designed with quantum computing in mind. Students can apply to both the QSE program and the <a href="https://gsas.harvard.edu/programs-of-study/all/computer-science">CS PhD program</a>, in parallel.</p>



<p>Some more information below:</p>



<p>The <strong><a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering">Harvard Quantum Science and Engineering (QSE) PhD program</a></strong> is designed for students like yours, as well as those studying engineering, physics and chemistry. The program brings these students from diverse undergraduate programs together through a world-class, integrated QSE PhD program that uniquely prepares them to become intellectual leaders and innovators in the burgeoning field of QSE. </p>



<p><strong>The curriculum. </strong>The integrated curriculum provides a shared foundation and QSE language that enables students to make discoveries and collaborate fluently beyond traditional disciplinary boundaries. Students enjoy the freedom to broadly explore their interests, specialize in their area of greatest interest, and can choose PhD advisors from across all quantum departments including computer science, physics, engineering, and chemistry.</p>



<p><strong>The community. </strong>The Harvard QSE PhD program is built around a supportive environment and collaborative research community that helps nurture each student and ensure their success. It’s an unprecedented opportunity to work with world leaders in the field of QSE in state-of-the-art educational and computational facilities while participating in cutting-edge research. </p>



<p><strong>The opportunity. </strong>Graduates of the program will be trained as the next generation of world leaders in the field of QSE. With their broad, yet deep educational foundation, guided by their own interests, they’ll be ready to take on exciting roles in industry, academia, and national laboratories. </p>



<p>Deadline to apply is <strong>December 15, 2021</strong>. Apply via <a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering" rel="noreferrer noopener" target="_blank">https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering</a> . Students can also apply in parallel to the CS PhD program via <a href="https://gsas.harvard.edu/programs-of-study/all/computer-science" rel="noreferrer noopener" target="_blank">https://gsas.harvard.edu/programs-of-study/all/computer-science</a> . Email <a href="mailto:qse-admissions@fas.harvard.edu" rel="noreferrer noopener" target="_blank">qse-admissions@fas.harvard.edu</a> with any questions.</p>



<p/></div>
    </content>
    <updated>2021-11-08T15:43:33Z</updated>
    <published>2021-11-08T15:43:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-11-09T06:38:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-6759625364649289630</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/6759625364649289630/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=6759625364649289630" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/6759625364649289630" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/6759625364649289630" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2021/11/postdoc-call-for-fodsi.html" rel="alternate" type="text/html"/>
    <title>Postdoc call for FODSI</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As a member of FODSI (Foundations of Data Science Institute -- an NSF funded institute with the aim of advancing theoretical foundations for data science), I'm self-interestedly posting the call for postdocs for this year.  Two of the areas are  a) Sketching, Sampling, and Sublinear-Time Algorithms   and b)  Machine Learning for Algorithms (which includes what I call "Algorithms with Predictions.")  I'd be happy to see postdoc applications in those areas from people who want to spend some time at Harvard, for example.... but of course there are lots of other exciting things going on with FODSI too and you should take a look.</p><p>The call is at <a href="https://academicjobsonline.org/ajo/jobs/20132">https://academicjobsonline.org/ajo/jobs/20132</a>  </p><p>Call text below:</p><table align="center" border="1" class="ads" style="border-collapse: collapse; border: 1px solid rgb(204, 204, 204); color: #2a2a2a; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 13.3333px; padding: 5px; width: 95%px;"><tbody><tr><td style="border-collapse: collapse; border: 1px solid rgb(204, 204, 204); padding: 5px;">The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College. It provides a structured environment for exploring interdisciplinary research in foundations of Data Science spanning Mathematics, Statistics, Theoretical Computer Science and other fields.<p>We are looking for multiple postdoctoral team members who will collaborate with <a href="https://fodsi.us/team.html" style="color: #005f6f; font-weight: 700;">FODSI researchers</a> at one or more of the participating institutions. These positions emphasize strong mentorship, flexibility, and breadth of collaboration opportunities with other team members -- senior and junior faculty, postdocs, and graduate students at various nodes around the country. Furthermore, postdoctoral fellows will be able to participate in workshops and other activities organized by FODSI.</p><p>The fellowship is a one-year full-time appointment, with the possibility of renewal for a second year (based upon mutual agreement) either at the same or at another FODSI institution. The start date is flexible, although most appointments are expected to start in summer 2022. Candidates are encouraged to apply to work with more than one faculty mentor <b>at one or more participating institutions</b> (in-person mentoring is preferred, but remote options will be also considered). The applicants should have an excellent theoretical background and a doctorate in a related field, including Mathematics, Statistics, Computer Science, Electrical Engineering or Economics. We particularly encourage applications from women and minority candidates.</p><p>The review process will start on November 15, 2021 and will continue until positions are filled.</p></td></tr></tbody></table></div>
    </content>
    <updated>2021-11-08T14:45:00Z</updated>
    <published>2021-11-08T14:45:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2021-11-08T14:46:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03639</id>
    <link href="http://arxiv.org/abs/2111.03639" rel="alternate" type="text/html"/>
    <title>Randomized Communication and the Implicit Graph Conjecture</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Harms:Nathaniel.html">Nathaniel Harms</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wild:Sebastian.html">Sebastian Wild</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zamaraev:Viktor.html">Viktor Zamaraev</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03639">PDF</a><br/><b>Abstract: </b>The most basic lower-bound question in randomized communication complexity
is: Does a given problem have constant cost, or non-constant cost? We observe
that this question has a deep connection to the Implicit Graph Conjecture (IGC)
in structural graph theory. Specifically, constant-cost communication problems
correspond to a certain subset of hereditary graph families that satisfy the
IGC: those that admit constant-size probabilistic universal graphs (PUGs), or,
equivalently, those that admit constant-size adjacency sketches.
</p>
<p>We initiate the study of the hereditary graph families that admit
constant-size PUGs, with the two (equivalent) goals of (1) giving a structural
characterization of randomized constant-cost communication problems, and (2)
resolving a probabilistic version of the IGC. For each family $\mathcal F$
studied in this paper (including the monogenic bipartite families, product
graphs, interval and permutation graphs, families of bounded twin-width, and
others), it holds that the subfamilies $\mathcal H \subseteq \mathcal F$ are
either stable (in a sense relating to model theory), in which case they admit
constant-size PUGs (i.e. adjacency sketches), or they are not stable, in which
case they do not. We conjecture that this always holds, i.e. that constant-cost
randomized communication problems correspond to the set of stable families that
satisfy the IGC.
</p>
<p>As a consequence of our results, we also obtain constant-size adjacency
sketches, and an $O(\log n)$ adjacency labeling scheme, for the induced
subgraphs of arbitrary Cartesian products, as well as constant-size
small-distance sketches for Cartesian products and stable families of bounded
twin-width (including planar graphs, answering a question from earlier work).
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03560</id>
    <link href="http://arxiv.org/abs/2111.03560" rel="alternate" type="text/html"/>
    <title>Optimal Approximate Distance Oracle for Planar Graphs</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Hung.html">Hung Le</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wulff=Nilsen:Christian.html">Christian Wulff-Nilsen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03560">PDF</a><br/><b>Abstract: </b>A $(1+\epsilon)$-approximate distance oracle of an edge-weighted graph is a
data structure that returns an approximate shortest path distance between any
two query vertices up to a $(1+\epsilon)$ factor. Thorup (FOCS 2001, JACM 2004)
and Klein (SODA 2002) independently constructed a $(1+\epsilon)$-approximate
distance oracle with $O(n\log n)$ space, measured in number of words, and
$O(1)$ query time when $G$ is an undirected planar graph with $n$ vertices and
$\epsilon$ is a fixed constant. Many follow-up works gave
$(1+\epsilon)$-approximate distance oracles with various trade-offs between
space and query time. However, improving $O(n\log n)$ space bound without
sacrificing query time remains an open problem for almost two decades. In this
work, we resolve this problem affirmatively by constructing a
$(1+\epsilon)$-approximate distance oracle with optimal $O(n)$ space and $O(1)$
query time for undirected planar graphs and fixed $\epsilon$.
</p>
<p>We also make substantial progress for planar digraphs with non-negative edge
weights. For fixed $\epsilon &gt; 0$, we give a $(1+\epsilon)$-approximate
distance oracle with space $o(n\log(Nn))$ and $O(\log\log(Nn)$ query time; here
$N$ is the ratio between the largest and smallest positive edge weight. This
improves Thorup's (FOCS 2001, JACM 2004) $O(n\log(Nn)\log n)$ space bound by
more than a logarithmic factor while matching the query time of his structure.
This is the first improvement for planar digraphs in two decades, both in the
weighted and unweighted setting.
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03559</id>
    <link href="http://arxiv.org/abs/2111.03559" rel="alternate" type="text/html"/>
    <title>Computability and Beltrami fields in Euclidean space</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cardona:Robert.html">Robert Cardona</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miranda:Eva.html">Eva Miranda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peralta=Salas:Daniel.html">Daniel Peralta-Salas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03559">PDF</a><br/><b>Abstract: </b>In this article, we pursue our investigation of the connections between the
theory of computation and hydrodynamics. We prove the existence of stationary
solutions of the Euler equations in Euclidean space, of Beltrami type, that can
simulate a universal Turing machine. In particular, these solutions possess
undecidable trajectories. Heretofore, the known Turing complete constructions
of steady Euler flows in dimension 3 or higher were not associated to a
prescribed metric. Our solutions do not have finite energy, and their
construction makes crucial use of the non-compactness of $\mathbb R^3$, however
they can be employed to show that an arbitrary tape-bounded Turing machine can
be robustly simulated by a Beltrami flow on $\mathbb T^3$ (with the standard
flat metric). This shows that there exist steady solutions to the Euler
equations on the flat torus exhibiting dynamical phenomena of (robust)
arbitrarily high computational complexity. We also quantify the energetic cost
for a Beltrami field on $\mathbb T^3$ to simulate a tape-bounded Turing
machine, thus providing additional support for the space-bounded Church-Turing
thesis. Another implication of our construction is that a Gaussian random
Beltrami field on Euclidean space exhibits arbitrarily high computational
complexity with probability $1$. Finally, our proof also yields Turing complete
flows and maps on $\mathbb{S}^2$ with zero topological entropy, thus disclosing
a certain degree of independence within different hierarchies of complexity.
</p></div>
    </summary>
    <updated>2021-11-08T22:38:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03528</id>
    <link href="http://arxiv.org/abs/2111.03528" rel="alternate" type="text/html"/>
    <title>New Streaming Algorithms for High Dimensional EMD and MST</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xi.html">Xi Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jayaram:Rajesh.html">Rajesh Jayaram</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levi:Amit.html">Amit Levi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Waingarten:Erik.html">Erik Waingarten</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03528">PDF</a><br/><b>Abstract: </b>We study streaming algorithms for two fundamental geometric problems:
computing the cost of a Minimum Spanning Tree (MST) of an $n$-point set $X
\subset \{1,2,\dots,\Delta\}^d$, and computing the Earth Mover Distance (EMD)
between two multi-sets $A,B \subset \{1,2,\dots,\Delta\}^d$ of size $n$. We
consider the turnstile model, where points can be added and removed. We give a
one-pass streaming algorithm for MST and a two-pass streaming algorithm for
EMD, both achieving an approximation factor of $\tilde{O}(\log n)$ and using
polylog$(n,d,\Delta)$-space only. Furthermore, our algorithm for EMD can be
compressed to a single pass with a small additive error. Previously, the best
known sublinear-space streaming algorithms for either problem achieved an
approximation of $O(\min\{ \log n , \log (\Delta d)\} \log n)$
[Andoni-Indyk-Krauthgamer '08, Backurs-Dong-Indyk-Razenshteyn-Wagner '20]. For
MST, we also prove that any constant space streaming algorithm can only achieve
an approximation of $\Omega(\log n)$, analogous to the $\Omega(\log n)$ lower
bound for EMD of [Andoni-Indyk-Krauthgamer '08].
</p>
<p>Our algorithms are based on an improved analysis of a recursive space
partitioning method known generically as the Quadtree. Specifically, we show
that the Quadtree achieves an $\tilde{O}(\log n)$ approximation for both EMD
and MST, improving on the $O(\min\{ \log n , \log (\Delta d)\} \log n)$
approximation of [Andoni-Indyk-Krauthgamer '08,
Backurs-Dong-Indyk-Razenshteyn-Wagner '20].
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03479</id>
    <link href="http://arxiv.org/abs/2111.03479" rel="alternate" type="text/html"/>
    <title>Long paths make pattern-counting hard, and deep trees make it harder</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jel=iacute=nek:V=iacute=t.html">Vít Jelínek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Opler:Michal.html">Michal Opler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pek=aacute=rek:Jakub.html">Jakub Pekárek</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03479">PDF</a><br/><b>Abstract: </b>We study the counting problem known as #PPM, whose input is a pair of
permutations $\pi$ and $\tau$ (called pattern and text, respectively), and the
task is to find the number of subsequences of $\tau$ that have the same
relative order as $\pi$. A simple brute-force approach solves #PPM for a
pattern of length $k$ and a text of length $n$ in time $O(n^{k+1})$, while
Berendsohn, Kozma and Marx have recently shown that under the exponential time
hypothesis (ETH), it cannot be solved in time $f(k) n^{o(k/\log k)}$ for any
function $f$. In this paper, we consider the restriction of #PPM, known as
$\mathcal{C}$-Pattern #PPM, where the pattern $\pi$ must belong to a hereditary
permutation class $\mathcal{C}$. Our goal is to identify the structural
properties of $\mathcal{C}$ that determine the complexity of
$\mathcal{C}$-Pattern #PPM.
</p>
<p>We focus on two such structural properties, known as the long path property
(LPP) and the deep tree property (DTP). Assuming ETH, we obtain these results:
</p>
<p>1. If $C$ has the LPP, then $\mathcal{C}$-Pattern #PPM cannot be solved in
time $f(k)n^{o(\sqrt{k})}$ for any function $f$, and
</p>
<p>2. if $C$ has the DTP, then $\mathcal{C}$-Pattern #PPM cannot be solved in
time $f(k)n^{o(k/\log^2 k)}$ for any function $f$.
</p>
<p>Furthermore, when $\mathcal{C}$ is one of the so-called monotone grid
classes, we show that if $\mathcal{C}$ has the LPP but not the DTP, then
$\mathcal{C}$-Pattern #PPM can be solved in time $f(k)n^{O(\sqrt k)}$. In
particular, the lower bounds above are tight up to the polylog terms in the
exponents.
</p></div>
    </summary>
    <updated>2021-11-08T22:37:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03450</id>
    <link href="http://arxiv.org/abs/2111.03450" rel="alternate" type="text/html"/>
    <title>Directed flow-augmentation</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Eun Jung Kim, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kratsch:Stefan.html">Stefan Kratsch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pilipczuk:Marcin.html">Marcin Pilipczuk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wahlstr=ouml=m:Magnus.html">Magnus Wahlström</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03450">PDF</a><br/><b>Abstract: </b>We show a flow-augmentation algorithm in directed graphs: There exists a
polynomial-time algorithm that, given a directed graph $G$, two integers $s,t
\in V(G)$, and an integer $k$, adds (randomly) to $G$ a number of arcs such
that for every minimal $st$-cut $Z$ in $G$ of size at most $k$, with
probability $2^{-\mathrm{poly}(k)}$ the set $Z$ becomes a minimum $st$-cut in
the resulting graph.
</p>
<p>The directed flow-augmentation tool allows us to prove fixed-parameter
tractability of a number of problems parameterized by the cardinality of the
deletion set, whose parameterized complexity status was repeatedly posed as
open problems: (1) Chain SAT, defined by Chitnis, Egri, and Marx [ESA'13,
Algorithmica'17], (2) a number of weighted variants of classic directed cut
problems, such as Weighted $st$-Cut, Weighted Directed Feedback Vertex Set, or
Weighted Almost 2-SAT.
</p>
<p>By proving that Chain SAT is FPT, we confirm a conjecture of Chitnis, Egri,
and Marx that, for any graph $H$, if the List $H$-Coloring problem is
polynomial-time solvable, then the corresponding vertex-deletion problem is
fixed-parameter tractable.
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03360</id>
    <link href="http://arxiv.org/abs/2111.03360" rel="alternate" type="text/html"/>
    <title>Maintaining Exact Distances under Multiple Edge Failures</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Duan:Ran.html">Ran Duan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ren:Hanlin.html">Hanlin Ren</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03360">PDF</a><br/><b>Abstract: </b>We present the first compact distance oracle that tolerates multiple failures
and maintains exact distances. Given an undirected weighted graph $G = (V, E)$
and an arbitrarily large constant $d$, we construct an oracle that given
vertices $u, v \in V$ and a set of $d$ edge failures $D$, outputs the exact
distance between $u$ and $v$ in $G - D$ (that is, $G$ with edges in $D$
removed). Our oracle has space complexity $O(d n^4)$ and query time $d^{O(d)}$.
Previously, there were compact approximate distance oracles under multiple
failures [Chechik, Cohen, Fiat, and Kaplan, SODA'17; Duan, Gu, and Ren,
SODA'21], but the best exact distance oracles under $d$ failures require
essentially $\Omega(n^d)$ space [Duan and Pettie, SODA'09]. Our distance oracle
seems to require $n^{\Omega(d)}$ time to preprocess; we leave it as an open
question to improve this preprocessing time.
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03257</id>
    <link href="http://arxiv.org/abs/2111.03257" rel="alternate" type="text/html"/>
    <title>Tight Bounds for Differentially Private Anonymized Histograms</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manurangsi:Pasin.html">Pasin Manurangsi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03257">PDF</a><br/><b>Abstract: </b>In this note, we consider the problem of differentially privately (DP)
computing an anonymized histogram, which is defined as the multiset of counts
of the input dataset (without bucket labels). In the low-privacy regime
$\epsilon \geq 1$, we give an $\epsilon$-DP algorithm with an expected
$\ell_1$-error bound of $O(\sqrt{n} / e^\epsilon)$. In the high-privacy regime
$\epsilon &lt; 1$, we give an $\Omega(\sqrt{n \log(1/\epsilon) / \epsilon})$ lower
bound on the expected $\ell_1$ error. In both cases, our bounds asymptotically
match the previously known lower/upper bounds due to [Suresh, NeurIPS 2019].
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03221</id>
    <link href="http://arxiv.org/abs/2111.03221" rel="alternate" type="text/html"/>
    <title>Breaking the $n^k$ Barrier for Minimum $k$-cut on Simple Graphs</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Zhiyang.html">Zhiyang He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jason.html">Jason Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03221">PDF</a><br/><b>Abstract: </b>In the minimum $k$-cut problem, we want to find the minimum number of edges
whose deletion breaks the input graph into at least $k$ connected components.
The classic algorithm of Karger and Stein runs in $\tilde O(n^{2k-2})$ time,
and recent, exciting developments have improved the running time to $O(n^k)$.
For general, weighted graphs, this is tight assuming popular hardness
conjectures.
</p>
<p>In this work, we show that perhaps surprisingly, $O(n^k)$ is not the right
answer for simple, unweighted graphs. We design an algorithm that runs in time
$O(n^{(1-\epsilon)k})$ where $\epsilon&gt;0$ is an absolute constant, breaking the
natural $n^k$ barrier. This establishes a separation of the two problems in the
unweighted and weighted cases.
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03213</id>
    <link href="http://arxiv.org/abs/2111.03213" rel="alternate" type="text/html"/>
    <title>The Fourier Transform of Restrictions of Functions on the Slice</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rao:Shravas.html">Shravas Rao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03213">PDF</a><br/><b>Abstract: </b>This paper considers the Fourier transform over the slice of the Boolean
hypercube. We prove a relationship between the Fourier coefficients of a
function over the slice, and the Fourier coefficients of its restrictions. As
an application, we prove a Goldreich-Levin theorem for functions on the slice
based on the Kushilevitz-Mansour algorithm for the Boolean hypercube.
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03171</id>
    <link href="http://arxiv.org/abs/2111.03171" rel="alternate" type="text/html"/>
    <title>A New Framework for Matrix Discrepancy: Partial Coloring Bounds via Mirror Descent</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dadush:Daniel.html">Daniel Dadush</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Haotian.html">Haotian Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reis:Victor.html">Victor Reis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03171">PDF</a><br/><b>Abstract: </b>Motivated by the Matrix Spencer conjecture, we study the problem of finding
signed sums of matrices with a small matrix norm. A well-known strategy to
obtain these signs is to prove, given matrices $A_1, \dots, A_n \in
\mathbb{R}^{m \times m}$, a Gaussian measure lower bound of $2^{-O(n)}$ for a
scaling of the discrepancy body $\{x \in \mathbb{R}^n: \| \sum_{i=1}^n x_i
A_i\| \leq 1\}$. We show this is equivalent to covering its polar with
$2^{O(n)}$ translates of the cube $\frac{1}{n} B^n_\infty$, and construct such
a cover via mirror descent. As applications of our framework, we show:
</p>
<p>$\bullet$ Matrix Spencer for Low-Rank Matrices. If the matrices satisfy
$\|A_i\|_{\mathrm{op}} \leq 1$ and $\mathrm{rank}(A_i) \leq r$, we can
efficiently find a coloring $x \in \{\pm 1\}^n$ with discrepancy
$\|\sum_{i=1}^n x_i A_i \|_{\mathrm{op}} \lesssim \sqrt{n \log (\min(rm/n,
r))}$. This improves upon the naive $O(\sqrt{n \log r})$ bound for random
coloring and proves the matrix Spencer conjecture when $r m \leq n$.
</p>
<p>$\bullet$ Matrix Spencer for Block Diagonal Matrices. For block diagonal
matrices with $\|A_i\|_{\mathrm{op}} \leq 1$ and block size $h$, we can
efficiently find a coloring $x \in \{\pm 1\}^n$ with $\|\sum_{i=1}^n x_i A_i
\|_{\mathrm{op}} \lesssim \sqrt{n \log (hm/n)}$. Using our proof, we reduce the
matrix Spencer conjecture to the existence of a $O(\log(m/n))$ quantum relative
entropy net on the spectraplex.
</p>
<p>$\bullet$ Matrix Discrepancy for Schatten Norms. We generalize our
discrepancy bound for matrix Spencer to Schatten norms $2 \le p \leq q$. Given
$\|A_i\|_{S_p} \leq 1$ and $\mathrm{rank}(A_i) \leq r$, we can efficiently find
a partial coloring $x \in [-1,1]^n$ with $|\{i : |x_i| = 1\}| \ge n/2$ and
$\|\sum_{i=1}^n x_i A_i\|_{S_q} \lesssim \sqrt{n \min(p, \log(rk))} \cdot
k^{1/p-1/q}$, where $k := \min(1,m/n)$.
</p></div>
    </summary>
    <updated>2021-11-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.03142</id>
    <link href="http://arxiv.org/abs/2111.03142" rel="alternate" type="text/html"/>
    <title>Inapproximability of Positive Semidefinite Permanents and Quantum State Tomography</title>
    <feedworld_mtime>1636329600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meiburg:Alex.html">Alex Meiburg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.03142">PDF</a><br/><b>Abstract: </b>Quantum State Tomography is the task of estimating a quantum state, given
many measurements in different bases. We discuss a few variants of what exactly
``estimating a quantum state" means, including maximum likelihood estimation
and computing a Bayesian average. We show that, when the measurements are
fixed, this problem is NP-Hard to approximate within any constant factor. In
the process, we find that it reduces to the problem of approximately computing
the permanent of a Hermitian positive semidefinite (HPSD) matrix. This implies
that HPSD permanents are also NP-Hard to approximate, resolving a standing
question with applications in quantum information and BosonSampling.
</p></div>
    </summary>
    <updated>2021-11-08T22:39:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6799651120968614396</id>
    <link href="http://blog.computationalcomplexity.org/feeds/6799651120968614396/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/reflections-on-trusting-trustlessness.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6799651120968614396" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6799651120968614396" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/reflections-on-trusting-trustlessness.html" rel="alternate" type="text/html"/>
    <title>Reflections on Trusting ``Trustlessness'' in the era of ``Crypto'' Blockchains (Guest Post)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> </p><p><i>I trust Evangelos Georgiadis to do a guest post on Trust and Blockchain. </i></p><div>Today we have a guest post by Evangelos Georgiadis on Trust. It was written before Lance's post on trust <a href="https://blog.computationalcomplexity.org/2021/08/trusting-scientists.html">here</a> but it can be viewed as a followup to it. </div><div><br/></div><div>And now, here's E.G:</div><div><div><br/></div><div>==========================================================</div><div><br/></div><div>Trust is a funny concept, particularly in the realm of blockchains and "crypto".</div><div><br/></div><div>Do you trust the consensus mechanism of a public blockchain?</div><div><br/></div><div>Do you trust the architects that engineered the consensus mechanism?</div><div><br/></div><div>Do you trust the software engineers that implemented the code for the consensus mechanism?</div><div><br/></div><div>Do you trust the language that the software engineers used?</div><div><br/></div><div>Do you trust the underlying hardware that that the software is running?</div><div><br/></div><div>Theoretical Computer Science provides tools for some of this. But then the question becomes</div><div>Do you trust the program verifier?</div><div>Do you trust the proof of security?</div><div><br/></div><div>I touch on these issues in: </div><div><br/></div><div>                   <i>Reflections on Trusting ‘Trustlessness’ in the era of ”Crypto”/Blockchains</i></div><div><br/></div><div> which is <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/cbit-4-2.pdf">here</a>. Its only 3 pages so enjoy!</div></div></div>
    </content>
    <updated>2021-11-07T20:47:00Z</updated>
    <published>2021-11-07T20:47:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-11-08T21:45:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/150</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/150" rel="alternate" type="text/html"/>
    <title>TR21-150 |  Extractors: Low Entropy Requirements Colliding With Non-Malleability | 

	Eldon Chung, 

	Maciej Obremski, 

	Divesh Aggarwal</title>
    <summary>The known constructions of negligible error (non-malleable) two-source extractors can be broadly classified in three categories:

(1) Constructions where one source has min-entropy rate about $1/2$, the other source can have small min-entropy rate, but the extractor doesn't guarantee non-malleability.
(2) Constructions where one source is uniform, and the other can have small min-entropy rate, and the extractor guarantees non-malleability when the uniform source is tampered.
(3) Constructions where both sources have entropy rate very close to $1$ and the extractor guarantees non-malleability against the tampering of both sources. 

We introduce a new notion of collision resistant extractors and in using it we obtain a strong two source non-malleable extractor where we require the first source to have $0.8$ entropy rate and the other source can have min-entropy polylogarithmic in the length of the source.  

We show how the above extractor can be applied to obtain a non-malleable extractor with output rate $\frac 1 2$, which is optimal. We also show how, by using our extractor and extending the known protocol, one can  obtain a privacy amplification secure against memory tampering where the size of the secret output is almost optimal.</summary>
    <updated>2021-11-07T20:13:16Z</updated>
    <published>2021-11-07T20:13:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-09T06:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/149</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/149" rel="alternate" type="text/html"/>
    <title>TR21-149 |  On polynomially many queries to NP or QMA oracles | 

	Dorian Rudolph, 

	Sevag Gharibian</title>
    <summary>We study the complexity of problems solvable in deterministic polynomial time with access to an NP or Quantum Merlin-Arthur (QMA)-oracle, such as $P^{NP}$ and $P^{QMA}$, respectively.
The former allows one to classify problems more finely than the Polynomial-Time Hierarchy (PH), whereas the latter characterizes physically motivated problems such as Approximate Simulation (APX-SIM) [Ambainis, CCC 2014].
In this area, a central role has been played by the classes $P^{NP[\log]}$ and $P^{QMA[\log]}$, defined identically to $P^{NP}$ and $P^{QMA}$, except that only logarithmically many oracle queries are allowed. Here, [Gottlob, FOCS 1993] showed that if the adaptive queries made by a $P^{NP}$ machine have a "query graph" which is a tree, then this computation can be simulated in $P^{NP[\log]}$.

 In this work, we first show that for any verification class $C\in\{NP,MA,QCMA,QMA,QMA(2),NEXP,QMA_{\exp}\}$, any $P^C$ machine with a query graph of "separator number" $s$ can be simulated using deterministic time $\exp(s\log n)$ and $s\log n$ queries to a $C$-oracle.
When $s\in O(1)$ (which includes the case of $O(1)$-treewidth, and thus also of trees), this gives an upper bound of $P^{C[\log]}$, and when $s\in O(\log^k(n))$, this yields bound $QP^{C[\log^{k+1}]}$ (QP meaning quasi-polynomial time).
We next show how to combine Gottlob's "admissible-weighting function" framework with the "flag-qubit" framework of [Watson, Bausch, Gharibian, 2020], obtaining a unified approach for embedding $P^C$ computations directly into APX-SIM instances in a black-box fashion.
Finally, we formalize a simple no-go statement about polynomials (c.f. [Krentel, STOC 1986]): Given a multi-linear polynomial $p$ specified via an arithmetic circuit, if one can "weakly compress" $p$ so that its optimal value requires $m$ bits to represent, then $P^{NP}$ can be decided with only $m$ queries to an NP-oracle.</summary>
    <updated>2021-11-07T12:55:09Z</updated>
    <published>2021-11-07T12:55:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-09T06:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/</id>
    <link href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/" rel="alternate" type="text/html"/>
    <title>IDEAL mini-workshop on “New Directions on Robustness in ML”</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">November 16, 2021 Virtual https://www.ideal.northwestern.edu/events/mini-workshop-on-new-directions-on-robustness-in-ml/ As machine learning systems are being deployed in almost every aspect of decision-making, it is vital for them to be reliable and secure to adversarial corruptions and perturbations of various kinds. This workshop will explore newer notions of robustness and the different challenges that arise in designing reliable ML algorithms. … <a class="more-link" href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/">Continue reading <span class="screen-reader-text">IDEAL mini-workshop on “New Directions on Robustness in ML”</span></a></div>
    </summary>
    <updated>2021-11-07T04:05:12Z</updated>
    <published>2021-11-07T04:05:12Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-11-09T06:38:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1584</id>
    <link href="https://ptreview.sublinear.info/2021/11/news-for-october-2021/" rel="alternate" type="text/html"/>
    <title>News for October 2021</title>
    <summary>The month of September was quite busy, with seven papers, spanning (hyper)graphs, proofs, probability distributions, and sampling. Better Sum Estimation via Weighted Sampling, by Lorenzo Beretta and Jakub Tětek (arXiv). This paper considers the following question: “given a large universe of items, each with an unknown weight, estimate the total weight to a multiplicative \(1\pm […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The month of September was quite busy, with seven papers, spanning (hyper)graphs, proofs, probability distributions, and sampling.</p>



<p><strong>Better Sum Estimation via Weighted Sampling</strong>, by Lorenzo Beretta and Jakub Tětek (<a href="https://arxiv.org/abs/2110.14948">arXiv</a>). This paper considers the following question: “given a large universe of items, each with an unknown weight, estimate the total weight to a multiplicative \(1\pm \varepsilon\).” The key is in the type of access you have to those items: here, the authors consider the setting where items can be sampled proportionally to their unknown weights, and show improved bounds on the sample/query complexity in this model. And there something for everyone: they also discuss connections to edge estimation in graphs (assuming random edge queries) and to distribution testing (specifically, in the “dual” or “probability-revealing” models of Canonne–Rubinfeld and Onak–Sun).</p>



<p>This gives us an easy segue to distribution testing, which is the focus of the next two papers.</p>



<p><strong>As Easy as ABC: Adaptive Binning Coincidence Test for Uniformity Testing</strong>, by Sudeep Salgia, Qing Zhao, and Lang Tong (<a href="https://arxiv.org/abs/2110.06325">arXiv</a>). Most of the work in distribution testing (from the computer science community) focuses on discrete probability distributions, for several reasons. Including a technical one: total variation distance is rather fickle with continuous distributions, unless one makes some assumption on the unknown distribution. This paper does exactly this: assuming the unknown distribution has a Lipschitz density function, it shows how to test uniformity by adaptively discretizing the domain, achieving (near) sample complexity.</p>



<p><strong>Exploring the Gap between Tolerant and Non-tolerant Distribution Testing,</strong> by Sourav Chakraborty, Eldar Fischer, Arijit Ghosh, Gopinath Mishra, and Sayantan Sen (<a href="https://arxiv.org/abs/2110.09972">arXiv</a>). It is known that tolerant testing of distributions can be much harder than “standard” testing – for instance, for identity testing, the sample complexity can blow up by nearly a quadratic factor, from \(\sqrt{n}\) to \(\frac{n}{\log n}\)! But is it the worse that can happen, in general, for other properties? This work explores this question, and answers it in some notable cases of interest, such as for label-invariant (symmetric) properties.</p>



<p>And now, onto graphs!</p>



<p><strong>Approximating the Arboricity in Sublinear Time</strong>, by Talya Eden, Saleet Mossel, and Dana Ron (<a href="https://arxiv.org/abs/2110.15260">arXiv</a>). The arboricity of a graph is the minimal number of spanning forests required to cover all its edges. Many graph algorithms, especially sublinear-time ones, can be parameterized by this quantity: which is very useful, but what do you do if you don’t know the arboricity of your graph? Well, then you estimate it. Which this paper shows how to do efficiently, given degree and neighbor queries. Moreover, the bound they obtain — \(\tilde{O}(n/\alpha)\) queries to obtain a constant-factor approximation of the unknown arboricity \(\alpha\) — is optimal, up to logarithmic factors in the number of vertices \(n\).</p>



<p><strong>Sampling Multiple Nodes in Large Networks: Beyond Random Walks,</strong> by Omri Ben-Eliezer, Talya Eden, Joel Oren, and Dimitris Fotakis (<a href="https://arxiv.org/abs/2110.13324">arXiv</a>). Another thing which one typically wants to do with very large graphs is <em>sample nodes</em> from them, either uniformly or according to some prescribed distribution. This is a core building block in many other algorithms; unfortunately, approaches to do so via random walks will typically require a number of queries scaling with the mixing time \(t_{\rm mix}(G)\) of the graph \(G\), which might be very small for nicely expanding graphs, but not so great in many practical settings. This paper proposes and experimentally evaluates a different algorithm which bypasses this linear dependence on \(t_{\rm mix}(G)\), by first going through a random-walk-based “learning” phase (learn something about the structure of the graph) before using this learned structure to perform faster sampling, focusing on small connected components.</p>



<p>Why stop at graphs? <em>Hypergraphs</em>!</p>



<p><strong>Hypergraph regularity and random sampling,</strong> by Felix Joos, Jaehoon Kim, Daniela Kühn, Deryk Osthus (<a href="https://arxiv.org/abs/2110.01570">arXiv</a>). The main result in this paper is a hypergraph analogue of a result of Alon, Fischer, Newman and Shapira (for graphs), which roughly states that if a hypergraph satisfies some regularity condition, then so does with high probability a randomly sampled sub-hypergraph — and conversely. This in turn has direct implications to characterizing which hypergraph properties are testable: see the <a href="https://arxiv.org/abs/1707.03303">companion paper</a>, <em>b</em>y the same authors.<em><br/>(Note: this paper is a blast from the past, as the result it shows was originally established in the linked companion paper, from 2017; however, the authors split this paper in two this October, leading to this new, standalone paper.)</em></p>



<p>And, to conclude, Arthur, Merlin, and proofs:</p>



<p><strong>Sample-Based Proofs of Proximity,</strong> by Guy Goldberg, Guy Rothblum (<a href="https://eccc.weizmann.ac.il/report/2021/146/">ECCC</a>). Finally, consider the setting of interactive proofs of proximities (IPPs), where the prover is as usual computationally unbounded, but the verifier must run in sublinear time (à la property testing). This has received significant interest in the past years: but what if the verifier didn’t even get to make queries, but only got access to <em>uniformly random location</em>s of the input? These “SIPP” (Sample-based IPPs), and their non-interactive counterpart SAMPs (Sample-based Merlin-Arthur Proofs of Proximity) are the object of study of this paper, which it introduces and motivates in the context, for instance, of delegation of computation for sample-based algorithms.</p></div>
    </content>
    <updated>2021-11-07T02:17:23Z</updated>
    <published>2021-11-07T02:17:23Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-11-09T03:21:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at Foundations of Data Science Institute (FODSI) (apply by November 15, 2021)</title>
    <summary>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College. Website: https://academicjobsonline.org/ajo/jobs/20132 Email: See the url</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/20132">https://academicjobsonline.org/ajo/jobs/20132</a><br/>
Email: See the url</p></div>
    </content>
    <updated>2021-11-05T22:17:02Z</updated>
    <published>2021-11-05T22:17:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-09T06:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at Computer Science, University of Victoria (apply by November 20, 2021)</title>
    <summary>Bruce Kapron invites applications for a postdoc in CS at the University of Victoria. Applicants with a background or interest in higher-order complexity theory, including models and techniques related to theory of programming languages, feasible analysis, cryptography, and ordinary complexity theory are encouraged. Applicants should have a Ph.D. in CS, Mathematics, Logic or a related […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Bruce Kapron invites applications for a postdoc in CS at the University of Victoria. Applicants with a background or interest in higher-order complexity theory, including models and techniques related to theory of programming languages, feasible analysis, cryptography, and ordinary complexity theory are encouraged. Applicants should have a Ph.D. in CS, Mathematics, Logic or a related field.</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/list/18864">https://www.mathjobs.org/jobs/list/18864</a><br/>
Email: bmkapron@uvic.ca</p></div>
    </content>
    <updated>2021-11-05T16:54:41Z</updated>
    <published>2021-11-05T16:54:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-09T06:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/</id>
    <link href="https://cstheory-jobs.org/2021/11/05/assistant-professor-at-charles-university-apply-by-january-31-2022/" rel="alternate" type="text/html"/>
    <title>Assistant Professor at Charles University (apply by January 31, 2022)</title>
    <summary>The Computer Science Institute of Charles University, Prague, Czech Republic, invites applications for an assistant professor in the area of theoretical computer science to complement and/or strengthen existing research areas (which include computational complexity, cryptography, algorithms, combinatorics, and discrete mathematics). Strong candidates from all areas of TCS will be considered. Website: https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022 Email: koucky@iuuk.mff.cuni.cz</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science Institute of Charles University, Prague, Czech Republic, invites applications for an assistant professor in the area of theoretical computer science to complement and/or strengthen existing research areas (which include computational complexity, cryptography, algorithms, combinatorics, and discrete mathematics). Strong candidates from all areas of TCS will be considered.</p>
<p>Website: <a href="https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022">https://www.mff.cuni.cz/en/faculty/job-opportunities/open-competition/academic-positions-application-deadline-january-31-2022</a><br/>
Email: koucky@iuuk.mff.cuni.cz</p></div>
    </content>
    <updated>2021-11-05T13:35:13Z</updated>
    <published>2021-11-05T13:35:13Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-09T06:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/05/summer-research-intern-at-adobe-research-apply-by-december-31-2021/" rel="alternate" type="text/html"/>
    <title>Summer Research Intern at Adobe Research  (apply by December 31, 2021)</title>
    <summary>Summer (TCS) Research Intern positions are available to work with Zhao Song at Adobe Research. The position is for 3-4 months in summer 2022, start date flexible. Applications will be reviewed on a rolling basis, with preference given to ones submitted before ddl. Potential project topics include but are not limited to general algorithmic topics. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Summer (TCS) Research Intern positions are available to work with Zhao Song at Adobe Research. The position is for 3-4 months in summer 2022, start date flexible. Applications will be reviewed on a rolling basis, with preference given to ones submitted before ddl. Potential project topics include but are not limited to general algorithmic topics. Interested candidates should send their CV to Zhao.</p>
<p>Website: <a href="https://scholar.google.com/citations?user=yDZct7UAAAAJ&amp;hl=en">https://scholar.google.com/citations?user=yDZct7UAAAAJ&amp;hl=en</a><br/>
Email: zsong@adobe.com</p></div>
    </content>
    <updated>2021-11-05T04:50:32Z</updated>
    <published>2021-11-05T04:50:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-09T06:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/04/faculty-at-university-of-haifa-at-oranim-college-apply-by-december-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/04/faculty-at-university-of-haifa-at-oranim-college-apply-by-december-31-2021/" rel="alternate" type="text/html"/>
    <title>Faculty at University of Haifa at Oranim College (apply by December 31, 2021)</title>
    <summary>The Department of Mathematics-Physics-Computer Science of the University of Haifa at Oranim College invites applications for a tenure-track faculty position in all areas of Computer Science, to begin October 1st 2022. Website: https://mathphys.haifa.ac.il/en/announcements/ Email: ackerman@sci.haifa.ac.il</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Mathematics-Physics-Computer Science of the University of Haifa at Oranim College invites applications for a tenure-track faculty position in all areas of Computer Science, to begin October 1st 2022.</p>
<p>Website: <a href="https://mathphys.haifa.ac.il/en/announcements/">https://mathphys.haifa.ac.il/en/announcements/</a><br/>
Email: ackerman@sci.haifa.ac.il</p></div>
    </content>
    <updated>2021-11-04T13:48:18Z</updated>
    <published>2021-11-04T13:48:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-09T06:37:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-3367673395710171015</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/3367673395710171015/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=3367673395710171015" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/3367673395710171015" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/3367673395710171015" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2021/11/hotnets-presentation-zero-cpu.html" rel="alternate" type="text/html"/>
    <title>HotNets Presentation : Zero-CPU Collection with Direct Telemetry Access</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>HotNets has asked that we let people know that the 2021 presentations <a href="https://www.youtube.com/channel/UCZZ5nf4RNDIIe4nifI8grwQ/videos">are available here</a>.  I'm using that an excuse to highlight our paper on Zero-CPU Collection with Direct Telemetry Access (<a href="https://arxiv.org/abs/2110.05438">arxiv version here</a>), but really I want to highlight the talk by graduate student Jonatan Langlet (Queen Mary University of London) who, as is the nature of graduate students, did all of the real work, and who really did a great job on <a href="https://www.youtube.com/watch?v=_M8AbF_f8Kk&amp;t=2s">the talk (direct link)</a>.  If you guessed from my involvement this involves hashing in some way, your maximum likelihood estimate turns out to be correct.</p><p>I think our work fits the HotNets call, which asks for new approaches and preliminary work.  Specifically, the call for the HotNets workshop says this:</p><p/><blockquote><p>We invite researchers and practitioners to submit short position papers. We encourage papers that identify fundamental open questions, advocate a new approach, offer a constructive critique of the state of networking research, re-frame or debunk existing work, report unexpected early results from a deployment, report on promising but unproven ideas, or propose new evaluation methods. Novel ideas need not be supported by full evaluations; well-reasoned arguments or preliminary evaluations can support the possibility of the paper’s claims.</p><p>We seek early-stage work, where the authors can benefit from community feedback. An ideal submission has the potential to open a line of inquiry for the community that results in multiple conference papers in related venues (SIGCOMM, NSDI, CoNEXT, SOSP, OSDI, MobiCom, MobiSys, etc.), rather than a single follow-on conference paper. The program committee will explicitly favor early work and papers likely to stimulate reflection and discussion over “conference papers in miniature”.</p></blockquote><p>There are similar other "Hot" workshops in other areas, and it was about <a href="http://mybiasedcoin.blogspot.com/2007/08/hottheory-workshop.html">14 years ago that I asked whether CS theory should have a HotTheory workshop</a>.  There's been a proliferation of new conferences and workshops in theory since then, but none of them really seem to have this flavor.  So maybe it's worth asking again whether a HotTheory workshop would make sense?  Or do existing theory events meet the theory community needs?</p><p/><p><br/></p></div>
    </content>
    <updated>2021-11-04T13:01:00Z</updated>
    <published>2021-11-04T13:01:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2021-11-08T14:46:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/04/lecturer-in-theoretical-computer-science-sheffield-uk-at-university-of-sheffield-apply-by-november-16-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/04/lecturer-in-theoretical-computer-science-sheffield-uk-at-university-of-sheffield-apply-by-november-16-2021/" rel="alternate" type="text/html"/>
    <title>Lecturer in Theoretical Computer Science, Sheffield (UK) at University of Sheffield (apply by November 16, 2021)</title>
    <summary>The University of Sheffield has an opening for a Lecturer in Theoretical Computer Science. Researchers in the area of computational complexity, where the interests of the Algorithms and Verification groups in the Department overlap, are particularly encouraged to apply. Website: https://www.jobs.ac.uk/job/CKB031/lecturer-in-theoretical-computer-science Email: g.j.brown@sheffield.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The University of Sheffield has an opening for a Lecturer in Theoretical Computer Science. Researchers in the area of computational complexity, where the interests of the Algorithms and Verification groups in the Department overlap, are particularly encouraged to apply.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CKB031/lecturer-in-theoretical-computer-science">https://www.jobs.ac.uk/job/CKB031/lecturer-in-theoretical-computer-science</a><br/>
Email: g.j.brown@sheffield.ac.uk</p></div>
    </content>
    <updated>2021-11-04T12:46:50Z</updated>
    <published>2021-11-04T12:46:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-09T06:37:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/" rel="alternate" type="text/html"/>
    <title>Professor, Associate Professor, Assistant Professor at George Washington University (apply by December 1, 2021)</title>
    <summary>We invite applications to multiple faculty positions at all ranks. Our search is focused on machine learning; artificial intelligence; computer and distributed systems; security and privacy; and candidates that can support our multidisciplinary initiatives in Smart and Trustworthy Systems and Meaningful Computing, broadly defined. Website: https://www.gwu.jobs/postings/87400 Email: cssearch@gwu.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We invite applications to multiple faculty positions at all ranks. Our search is focused on machine learning; artificial intelligence; computer and distributed systems; security and privacy; and candidates that can support our multidisciplinary initiatives in Smart and Trustworthy Systems and Meaningful Computing, broadly defined.</p>
<p>Website: <a href="https://www.gwu.jobs/postings/87400">https://www.gwu.jobs/postings/87400</a><br/>
Email: cssearch@gwu.edu</p></div>
    </content>
    <updated>2021-11-04T02:26:32Z</updated>
    <published>2021-11-04T02:26:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-09T06:37:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2021/11/04/perceptron/</id>
    <link href="http://benjamin-recht.github.io/2021/11/04/perceptron/" rel="alternate" type="text/html"/>
    <title>The Perceptron as a prototype for machine learning theory.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Just as many of the algorithms and community practices of machine learning were invented <a href="http://www.argmin.net/2021/10/20/highleyman/">in the late 1950s and early 1960s</a>, the foundations of machine learning theory were also established during this time. Many of the analyses of this period were strikingly simple, had surprisingly precise constants, and provided prescient guidelines for contemporary machine learning practice. Here, I’ll summarize the study of the Perceptron, highlighting both its algorithmic and statistical analyses, and using it as a prototype to illustrate further how prediction deviates from the umbrella of classical statistics.</p>

<p>Let’s begin with a classification problem where each individual from some population has a feature vector $x$ and an associated binary label $y$ that we take as valued $\pm 1$ for notational convenience. The goal of the Perceptron is to find a linear separator such that $\langle w, x \rangle&gt;0$ for when $y=1$ and $\langle w, x \rangle&lt;0$ when $y=-1$. We can write this compactly as saying that we want to find a $w$ for which $y \langle w, x \rangle &gt;0$ for as many individuals in the population as possible.</p>

<p>Rosenblatt’s Perceptron provides a simple algorithm for finding such a $w$. The Perceptron inputs an example, checks if it makes the correct classification. If yes, it does nothing and proceeds to the next example. If no, the decision boundary is nudged in the direction of classifying the example correctly next time.</p>

<p><strong>Perceptron</strong></p>

<ul>
  <li>Start from the initial solution $w_0=0$</li>
  <li>At each step $t=0,1,2,…$:
    <ul>
      <li>Select an individual from the population and look up their attributes: (x_t,y_t).</li>
      <li>Case 1: If $y_t\langle w_t, x_t\rangle \leq 0$, put
\(w_{t+1} = w_t + y_t x_t\)</li>
      <li>Case 2: Otherwise put $w_{t+1} = w_t$.</li>
    </ul>
  </li>
</ul>

<p>If the examples were selected at random, machine learners would recognize this algorithm as an instance of stochastic gradient descent, still the most ubiquitous way to train classifiers whether they be deep or shallow. Stochastic gradient descent minimizes sums of functions</p>

\[f(w) = \frac{1}{N} \sum_{i=1}^N \mathit{loss}( f(x_i; w) , y_i)\]

<p>with the update</p>

\[w_{t+1} = w_t - \alpha_t \nabla_w \mathit{loss}( f(x_t; w_t) , y_t)\,.\]

<p>When the examples are sampled randomly, the Perceptron is stochastic gradient descent with $\alpha_t=1$, $f(x;w) = \langle w,x \rangle$, and loss function $\mathit{loss}(\hat{y},y) = \max(-\hat{y} y, 0)$.</p>

<p>Stochastic gradient methods were invented a few years before the Perceptron. And the relations between these methods were noted by the mid-60s. Vapnik discusses some of this history in Chapter 1.11 of <a href="https://link.springer.com/book/10.1007/978-1-4757-3264-1"><em>The Nature of Statistical Learning Theory</em></a>.</p>

<p>While we might be tempted to use a standard stochastic gradient analysis to understand the optimization properties of the Perceptron, it turns out that a more rarified proof technique applies that uses no randomization whatsoever. Moreover, the argument will not only bound errors in optimization but also in generalization. Optimization is concerned with errors on a training data set. Generalization is concerned with errors on data we haven’t seen. The analysis from the 1960s links these two by first understanding the dynamics of the algorithm.</p>

<p><a href="https://cs.uwaterloo.ca/~y328yu/classics/novikoff.pdf">A celebrated result by Al Novikoff in 1962</a> showed that under reasonable conditions the algorithm makes a bounded number of updates no matter how large the sample size. Novikoff’s result is typically referred to as a <em>mistake bound</em> as it bounds the number of total misclassifications made when running the Perceptron on some data set. The key assumption in Novikoff’s argument is that the positive and negative examples are cleanly separated by a linear function. People often dismiss the Perceptron because of this <em>separability</em> assumption. But for any finite data set, can always add features and end up with a linearly separable problem. And if we add enough features, we’ll usually be separable no matter how many points we have.</p>

<p>This has been the trend in modern machine learning: don’t fear big models and don’t fear getting zero errors on your training set. This is no different than what was being proposed in the Perceptron. In fact, <a href="https://cs.uwaterloo.ca/~y328yu/classics/kernel.pdf">Aizerman, Braverman, and Roeznoer</a> recognized the power of such overparameterization, and extended Novikoff’s argument to “potential functions” that we now recognize as functions belonging to an infinite dimensional Reproducing Kernel Hilbert Space.</p>

<p>To state Novikoff’s result, we make the following assumptions: First, we assume as input a set of examples $S$. We assume every data point has norm at most $R(S)$ and that there exists a hyperplane that correctly classifies all of the data points and is of distance at least $\gamma(S)$ from every data point. This second assumption is called a <em>margin condition</em> that quantifies how separated the given data is. With these assumptions, Novikoff proved the Perceptron algorithm makes at most</p>

\[{\small
\frac{R(S)^2}{\gamma(S)^{2}}
}\]

<p>mistakes when run on $S$. No matter what the ordering of the data points in $S$, the algorithm makes a bounded number of errors.</p>

<p>The algorithmic analysis of Novikoff has many implications. First, if the data is separable, we can conclude that the Perceptron will terminate if it is run over the data set several times. This is because we can think of $k$ epochs of the Perceptron as running on the union of $k$ distinct copies of $S$, and the Perceptron eventually stops updating when run on this enlarged data set. Hence, the mistake bound tells us something particular about optimization: the Perceptron converges to a solution with zero training errors and hence a global minimizer of the empirical risk.</p>

<p>Second, we can think of the Perceptron algorithm as an <em>online learning algorithm</em>. We need not assume anything distributional about the sequence $S$. We can instead think about how long it takes for the Perceptron to converge to a solution that would have been as good as the optimal classifier. We can quantify this convergence by measuring the <em>regret</em>, equal to</p>

\[\mathcal{R}_T = \sum_{t=1}^T \mathrm{error}(w_t, (x_t,y_t)) - \sum_{t=1}^T \mathrm{error}(w_\star, (x_t,y_t))\,,\]

<p>where $w_\star$ denotes the optimal hyperplane. That is, the regret counts how frequently the classifier at step $t$ misclassifies the next example in the sequence. Novikoff’s argument shows that, if a sequence is perfectly classifiable, then the accrued regret is a constant that does not scale with T.</p>

<p>A third, less well known application of Novikoff’s bound is as a building block for a  <em>generalization bound</em>. A generalization bound estimates the probability of making an error on a new example given that the new example is sampled from the same population as the data thus far sceen. To state the generalization bound for the Perceptron, I <em>now</em> need to return to statistics. Generalization theory concerns statistical validity, and hence we need to define some notion of sampling from the population. I will use the same sampling model I have been using in this blog series. Rather than assuming a statistical model of the population, I will assume we have some population of data from which we can uniformly sample. Our training data will consist of $n$ points sampled uniformly from this population: $S={(x_1,y_1)\ldots, (x_n,y_n) }$.</p>

<p>We know that the Perceptron will find a good linear predictor for the training data if it exists. What we now show is that this predictor also works on new data sampled uniformly from the same population.</p>

<p>To analyze what happens on new data, I will employ an elegant argument I learned from Sasha Rakhlin. This argument appears in a book on Learning Theory by Vapnik and Chervonenkis from 1974, which, to my knowledge, is only available in Russian. Sasha also believes this argument is considerably older as <a href="http://www.mit.edu/~rakhlin/papers/chervonenkis_chapter.pdf">Aizermann and company were making similar “online to batch” constructions in the 1960s</a>. The proof here leverages the assumption that the data are sampled in such a way that they are identically distributed, so we can swap the roles of training and test examples in the analysis. It foreshadows later studies of stability and generalization that would be revisited decades later.</p>

<p><strong>Theorem</strong> <em>Let $w(S)$ be the output of the Perceptron on a dataset $S$ after running until the hyperplane makes no more mistakes on $S$. Let $S_n$ denote a training set of $n$ samples uniformly at random from some population. And let $(x,y)$ be an additional independent uniform sample from the same population. Then, the probability of making a mistake on $(x,y)$ is bounded as</em></p>

\[\Pr[y \langle w(S_n), x \rangle \leq 0] \leq \frac{1}{n+1} {\mathbb{E}}_{S_{n+1}}\left[ \frac{R(S_{n+1})^2}{\gamma(S_{n+1})^2} \right]\,.\]

<p>To prove the theorem, define the “leave-one-out set” to be the set where we drop $(x_k,y_k)$:</p>

\[{\scriptsize
S^{-k}=\{(x_1,y_1),\dots,(x_{k-1},y_{k-1}),(x_{k+1},y_{k+1}),...,(x_{n+1},y_{n+1})\}\,.
}\]

<p>With this notation, since all of the data are sampled identically and independently, we can rewrite the probability of a mistake on the final data point as the expectation of the leave-one-out error</p>

\[{\small
\Pr[y \langle w(S_n), x \rangle   \leq 0]
= \frac1{n+1}\sum_{k=1}^{n+1} \mathbb{E}[\mathbb{1}\{y_k \langle w(S^{-k}), x_k \rangle \leq 0\}]\,.
}\]

<p>Novikoff’s mistake bound asserts the Perceptron makes at most</p>

\[{\small
m=\frac{R(S_{n+1})^2}{\gamma(S_{n+1})^2}
}\]

<p>mistakes when run on the entire sequence $S_{n+1}$. Let $I={i_1,\dots,i_m}$ denote the indices on which the algorithm makes a mistake in any of its cycles over the data. If $k$ is not in $I$, the output of the algorithm remains the same after we remove the $k$-th sample from the sequence. It follows that such $k \in S_{n+1}\setminus I$ satisfy  $y_k w(S^{-k})x_k \geq 0$ and therefore do not contribute to the right hand side of the summation. The other terms can at most contribute $1$ to the summation.
Hence,</p>

\[\Pr[y \langle w(S_n), x \rangle \leq 0] \le \frac{\mathbb{E}[m]}{n+1}\,,\]

<p>which is what we wanted to prove.</p>

<p>What’s most stunning to me about this argument is that there are no numerical constants or logarithms. The generalization error is perfectly quantified by a simple formula of $R$, $\gamma$, and $n$. There are a variety of other arguments that get the $\tilde{O}(R/(n\gamma))$ scaling with far more complex arguments and large constants and logarithmic terms. For example, one can show that the set of hyperplanes in Euclidean space with norm bounded by $\gamma^{-1}$ has <a href="https://www.wiley.com/en-us/Statistical+Learning+Theory-p-9780471030034">VC dimension $R/\gamma$</a>. Similarly, a <a href="https://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf">Rademacher complexity argument will achieve a similar scaling</a>. These arguments apply to far more algorithms than the Perceptron, but it’s frustrating how this simple algorithm from 1956 gets such a tight bound with such a short argument whereas analyzing more “powerful” algorithms often takes pages of derivations.</p>

<p>It’s remarkable that these bounds on optimization, regret, and generalization worked out in the 1960s all turned out to be optimal for classification theory. This strikes me as particularly odd because when I was in graduate school I was taught that the Perceptron was a failed enterprise. But as fads in AI have come and gone, the role of the Perceptron has remained central for 65 years. We’ve made more progress in machine learning theory since then, but it’s not always at the front of our minds just how long ago we had established our modern learning theory framework.</p></div>
    </summary>
    <updated>2021-11-04T00:00:00Z</updated>
    <published>2021-11-04T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2021-11-09T01:16:09Z</updated>
    </source>
  </entry>
</feed>
