<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-07-16T00:22:29Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06876</id>
    <link href="http://arxiv.org/abs/2107.06876" rel="alternate" type="text/html"/>
    <title>Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klicpera:Johannes.html">Johannes Klicpera</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lienen:Marten.html">Marten Lienen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=uuml=nnemann:Stephan.html">Stephan GÃ¼nnemann</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06876">PDF</a><br/><b>Abstract: </b>The current best practice for computing optimal transport (OT) is via entropy
regularization and Sinkhorn iterations. This algorithm runs in quadratic time
as it requires the full pairwise cost matrix, which is prohibitively expensive
for large sets of objects. In this work we propose two effective log-linear
time approximations of the cost matrix: First, a sparse approximation based on
locality-sensitive hashing (LSH) and, second, a Nystr\"om approximation with
LSH-based sparse corrections, which we call locally corrected Nystr\"om (LCN).
These approximations enable general log-linear time algorithms for
entropy-regularized OT that perform well even for the complex, high-dimensional
spaces common in deep learning. We analyse these approximations theoretically
and evaluate them experimentally both directly and end-to-end as a component
for real-world applications. Using our approximations for unsupervised word
embedding alignment enables us to speed up a state-of-the-art method by a
factor of 3 while also improving the accuracy by 3.1 percentage points without
any additional model changes. For graph distance regression we propose the
graph transport network (GTN), which combines graph neural networks (GNNs) with
enhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales
log-linearly in the number of nodes.
</p></div>
    </summary>
    <updated>2021-07-15T22:42:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06817</id>
    <link href="http://arxiv.org/abs/2107.06817" rel="alternate" type="text/html"/>
    <title>Efficient Set of Vectors Search</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leybovich:Michael.html">Michael Leybovich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shmueli:Oded.html">Oded Shmueli</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06817">PDF</a><br/><b>Abstract: </b>We consider a similarity measure between two sets $A$ and $B$ of vectors,
that balances the average and maximum cosine distance between pairs of vectors,
one from set $A$ and one from set $B$. As a motivation for this measure, we
present lineage tracking in a database. To practically realize this measure, we
need an approximate search algorithm that given a set of vectors $A$ and sets
of vectors $B_1,...,B_n$, the algorithm quickly locates the set $B_i$ that
maximizes the similarity measure. For the case where all sets are singleton
sets, essentially each is a single vector, there are known efficient
approximate search algorithms, e.g., approximated versions of tree search
algorithms, locality-sensitive hashing (LSH), vector quantization (VQ) and
proximity graph algorithms. In this work, we present approximate search
algorithms for the general case. The underlying idea in these algorithms is
encoding a set of vectors via a "long" single vector.
</p></div>
    </summary>
    <updated>2021-07-15T22:59:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06800</id>
    <link href="http://arxiv.org/abs/2107.06800" rel="alternate" type="text/html"/>
    <title>Signed Barcodes for Multi-Parameter Persistence via Rank Decompositions and Rank-Exact Resolutions</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Botnan:Magnus_Bakke.html">Magnus Bakke Botnan</a>, Steffen Oppermann, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oudot:Steve.html">Steve Oudot</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06800">PDF</a><br/><b>Abstract: </b>In this paper we introduce the signed barcode, a new visual representation of
the global structure of the rank invariant of a multi-parameter persistence
module or, more generally, of a poset representation. Like its unsigned
counterpart in one-parameter persistence, the signed barcode encodes the rank
invariant as a $\mathbb{Z}$-linear combination of rank invariants of indicator
modules supported on segments in the poset. It can also be enriched to encode
the generalized rank invariant as a $\mathbb{Z}$-linear combination of
generalized rank invariants in fixed classes of interval modules. In the paper
we develop the theory behind these rank invariant decompositions, showing under
what conditions they exist and are unique -- so the signed barcode is
canonically defined. We also connect them to the line of work on generalized
persistence diagrams via M\"obius inversions, deriving explicit formulas to
compute a rank decomposition and its associated signed barcode. Finally, we
show that, similarly to its unsigned counterpart, the signed barcode has its
roots in algebra, coming from a projective resolution of the module in some
exact category. To complete the picture, we show some experimental results that
illustrate the contribution of the signed barcode in the exploration of
multi-parameter persistence modules.
</p></div>
    </summary>
    <updated>2021-07-15T23:02:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06785</id>
    <link href="http://arxiv.org/abs/2107.06785" rel="alternate" type="text/html"/>
    <title>Large-Scale News Classification using BERT Language Model: Spark NLP Approach</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Kuncahyo Setyo Nugroho, Kuncahyo Setyo Nugroho, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yudistira:Novanto.html">Novanto Yudistira</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06785">PDF</a><br/><b>Abstract: </b>The rise of big data analytics on top of NLP increases the computational
burden for text processing at scale. The problems faced in NLP are very high
dimensional text, so it takes a high computation resource. The MapReduce allows
parallelization of large computations and can improve the efficiency of text
processing. This research aims to study the effect of big data processing on
NLP tasks based on a deep learning approach. We classify a big text of news
topics with fine-tuning BERT used pre-trained models. Five pre-trained models
with a different number of parameters were used in this study. To measure the
efficiency of this method, we compared the performance of the BERT with the
pipelines from Spark NLP. The result shows that BERT without Spark NLP gives
higher accuracy compared to BERT with Spark NLP. The accuracy average and
training time of all models using BERT is 0.9187 and 35 minutes while using
BERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will
take more computation resources and need a longer time to complete the tasks.
However, the accuracy of BERT with Spark NLP only decreased by an average of
5.7%, while the training time was reduced significantly by 62.9% compared to
BERT without Spark NLP.
</p></div>
    </summary>
    <updated>2021-07-15T22:37:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06715</id>
    <link href="http://arxiv.org/abs/2107.06715" rel="alternate" type="text/html"/>
    <title>ETH Tight Algorithms for Geometric Intersection Graphs: Now in Polynomial Space</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovach:Petr_A=.html">Petr A. Golovach</a>, Tanmay Inamdar, Saket Saurabh <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06715">PDF</a><br/><b>Abstract: </b>De Berg et al. in [SICOMP 2020] gave an algorithmic framework for
subexponential algorithms on geometric graphs with tight (up to ETH) running
times. This framework is based on dynamic programming on graphs of weighted
treewidth resulting in algorithms that use super-polynomial space. We introduce
the notion of weighted treedepth and use it to refine the framework of de Berg
et al. for obtaining polynomial space (with tight running times) on geometric
graphs. As a result, we prove that for any fixed dimension $d \ge 2$ on
intersection graphs of similarly-sized fat objects many well-known graph
problems including Independent Set, $r$-Dominating Set for constant $r$, Cycle
Cover, Hamiltonian Cycle, Hamiltonian Path, Steiner Tree, Connected Vertex
Cover, Feedback Vertex Set, and (Connected) Odd Cycle Transversal are solvable
in time $2^{O(n^{1-1/d})}$ and within polynomial space.
</p></div>
    </summary>
    <updated>2021-07-15T22:47:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06649</id>
    <link href="http://arxiv.org/abs/2107.06649" rel="alternate" type="text/html"/>
    <title>Polynomial Time Algorithms to Find an Approximate Competitive Equilibrium for Chores</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boodaghians:Shant.html">Shant Boodaghians</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chaudhury:Bhaskar_Ray.html">Bhaskar Ray Chaudhury</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mehta:Ruta.html">Ruta Mehta</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06649">PDF</a><br/><b>Abstract: </b>Competitive equilibrium with equal income (CEEI) is considered one of the
best mechanisms to allocate a set of items among agents fairly and efficiently.
In this paper, we study the computation of CEEI when items are chores that are
disliked (negatively valued) by agents, under 1-homogeneous and concave utility
functions which includes linear functions as a subcase. It is well-known that,
even with linear utilities, the set of CEEI may be non-convex and disconnected,
and the problem is PPAD-hard in the more general exchange model. In contrast to
these negative results, we design FPTAS: A polynomial-time algorithm to compute
$\epsilon$-approximate CEEI where the running-time depends polynomially on
$1/\epsilon$.
</p>
<p>Our algorithm relies on the recent characterization due to Bogomolnaia et
al.~(2017) of the CEEI set as exactly the KKT points of a non-convex
minimization problem that have all coordinates non-zero. Due to this non-zero
constraint, naive gradient-based methods fail to find the desired local minima
as they are attracted towards zero. We develop an exterior-point method that
alternates between guessing non-zero KKT points and maximizing the objective
along supporting hyperplanes at these points. We show that this procedure must
converge quickly to an approximate KKT point which then can be mapped to an
approximate CEEI; this exterior point method may be of independent interest.
When utility functions are linear, we give explicit procedures for finding the
exact iterates, and as a result show that a stronger form of approximate CEEI
can be found in polynomial time. Finally, we note that our algorithm extends to
the setting of un-equal incomes (CE), and to mixed manna with linear utilities
where each agent may like (positively value) some items and dislike (negatively
value) others.
</p></div>
    </summary>
    <updated>2021-07-15T22:42:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06626</id>
    <link href="http://arxiv.org/abs/2107.06626" rel="alternate" type="text/html"/>
    <title>Optimality of the Johnson-Lindenstrauss Dimensionality Reduction for Practical Measures</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bartal:Yair.html">Yair Bartal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fandina:Ora_Nova.html">Ora Nova Fandina</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Larsen:Kasper_Green.html">Kasper Green Larsen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06626">PDF</a><br/><b>Abstract: </b>It is well known that the Johnson-Lindenstrauss dimensionality reduction
method is optimal for worst case distortion. While in practice many other
methods and heuristics are used, not much is known in terms of bounds on their
performance. The question of whether the JL method is optimal for practical
measures of distortion was recently raised in \cite{BFN19} (NeurIPS'19). They
provided upper bounds on its quality for a wide range of practical measures and
showed that indeed these are best possible in many cases. Yet, some of the most
important cases, including the fundamental case of average distortion were left
open. In particular, they show that the JL transform has $1+\epsilon$ average
distortion for embedding into $k$-dimensional Euclidean space, where
$k=O(1/\eps^2)$, and for more general $q$-norms of distortion, $k =
O(\max\{1/\eps^2,q/\eps\})$, whereas tight lower bounds were established only
for large values of $q$ via reduction to the worst case.
</p>
<p>In this paper we prove that these bounds are best possible for any
dimensionality reduction method, for any $1 \leq q \leq O(\frac{\log (2\eps^2
n)}{\eps})$ and $\epsilon \geq \frac{1}{\sqrt{n}}$, where $n$ is the size of
the subset of Euclidean space.
</p>
<p>Our results imply that the JL method is optimal for various distortion
measures commonly used in practice, such as {\it stress, energy} and {\it
relative error}. We prove that if any of these measures is bounded by $\eps$
then $k=\Omega(1/\eps^2)$, for any $\epsilon \geq \frac{1}{\sqrt{n}}$, matching
the upper bounds of \cite{BFN19} and extending their tightness results for the
full range moment analysis.
</p>
<p>Our results may indicate that the JL dimensionality reduction method should
be considered more often in practical applications, and the bounds we provide
for its quality should be served as a measure for comparison when evaluating
the performance of other methods and heuristics.
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06615</id>
    <link href="http://arxiv.org/abs/2107.06615" rel="alternate" type="text/html"/>
    <title>Oblivious sketching for logistic regression</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munteanu:Alexander.html">Alexander Munteanu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Omlor:Simon.html">Simon Omlor</a>, David Woodruff <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06615">PDF</a><br/><b>Abstract: </b>What guarantees are possible for solving logistic regression in one pass over
a data stream? To answer this question, we present the first data oblivious
sketch for logistic regression. Our sketch can be computed in input sparsity
time over a turnstile data stream and reduces the size of a $d$-dimensional
data set from $n$ to only $\operatorname{poly}(\mu d\log n)$ weighted points,
where $\mu$ is a useful parameter which captures the complexity of compressing
the data. Solving (weighted) logistic regression on the sketch gives an $O(\log
n)$-approximation to the original problem on the full data set. We also show
how to obtain an $O(1)$-approximation with slight modifications. Our sketches
are fast, simple, easy to implement, and our experiments demonstrate their
practicality.
</p></div>
    </summary>
    <updated>2021-07-15T22:41:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06582</id>
    <link href="http://arxiv.org/abs/2107.06582" rel="alternate" type="text/html"/>
    <title>Towards a Decomposition-Optimal Algorithm for Counting and Sampling Arbitrary Motifs in Sublinear Time</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biswas:Amartya_Shankha.html">Amartya Shankha Biswas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eden:Talya.html">Talya Eden</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinfeld:Ronitt.html">Ronitt Rubinfeld</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06582">PDF</a><br/><b>Abstract: </b>We consider the problem of sampling an arbitrary given motif $H$ in a graph
$G$, where access to $G$ is given via queries: degree, neighbor, and pair, as
well as uniform edge sample queries. Previous algorithms for the uniform
sampling task were based on a decomposition of $H$ into a collection of odd
cycles and stars, denoted $\mathcal{D}^*(H)=\{O_{k_1}, \ldots, O_{k_q},
S_{p_1}, \ldots, S_{p_\ell}\}$. These algorithms were shown to be optimal for
the case where $H$ is a clique or an odd-length cycle, but no other lower
bounds were known.
</p>
<p>We present a new algorithm for sampling arbitrary motifs which, up to
$\poly(\log n)$ factors, for any motif $H$ whose decomposition contains at
least two components or at least one star, is always preferable. The main
ingredient leading to this improvement is an improved uniform algorithm for
sampling stars, which might be of independent interest, as it allows to sample
vertices according to the $p$-th moment of the degree distribution. We further
show how to use our sampling algorithm to get an approximate counting
algorithm, with essentially the same complexity.
</p>
<p>Finally, we prove that this algorithm is \emph{decomposition-optimal} for
decompositions that contain at least one odd cycle. That is, we prove that for
any decomposition $D$ that contains at least one odd cycle, there exists a
motif $H_{D}$ with decomposition $D$, and a family of graphs $\mathcal{G}$, so
that in order to output a uniform copy of $H$ in a uniformly chosen graph in
$\mathcal{G}$, the number of required queries matches our upper bound. These
are the first lower bounds for motifs $H$ with a nontrivial decomposition,
i.e., motifs that have more than a single component in their decomposition.
</p></div>
    </summary>
    <updated>2021-07-15T22:40:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06571</id>
    <link href="http://arxiv.org/abs/2107.06571" rel="alternate" type="text/html"/>
    <title>A QPTAS for stabbing rectangles</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eisenbrand:Friedrich.html">Friedrich Eisenbrand</a>, Martina Gallato, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svensson:Ola.html">Ola Svensson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Venzin:Moritz.html">Moritz Venzin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06571">PDF</a><br/><b>Abstract: </b>We consider the following geometric optimization problem: Given $ n $
axis-aligned rectangles in the plane, the goal is to find a set of horizontal
segments of minimum total length such that each rectangle is stabbed. A segment
stabs a rectangle if it intersects both its left and right edge. As such, this
stabbing problem falls into the category of weighted geometric set cover
problems for which techniques that improve upon the general ${\Theta}(\log
n)$-approximation guarantee have received a lot of attention in the literature.
Chan at al. (2018) have shown that rectangle stabbing is NP-hard and that it
admits a constant-factor approximation algorithm based on Varadarajan's
quasi-uniform sampling method. In this work we make progress on rectangle
stabbing on two fronts. First, we present a quasi-polynomial time approximation
scheme (QPTAS) for rectangle stabbing. Furthermore, we provide a simple
$8$-approximation algorithm that avoids the framework of Varadarajan. This
settles two open problems raised by Chan et al. (2018).
</p></div>
    </summary>
    <updated>2021-07-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06490</id>
    <link href="http://arxiv.org/abs/2107.06490" rel="alternate" type="text/html"/>
    <title>Greedy Spanners in Euclidean Spaces Admit Sublinear Separators</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Hung.html">Hung Le</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Than:Cuong.html">Cuong Than</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06490">PDF</a><br/><b>Abstract: </b>The greedy spanner in a low dimensional Euclidean space is a fundamental
geometric construction that has been extensively studied over three decades as
it possesses the two most basic properties of a good spanner: constant maximum
degree and constant lightness. Recently, Eppstein and Khodabandeh showed that
the greedy spanner in $\mathbb{R}^2$ admits a sublinear separator in a strong
sense: any subgraph of $k$ vertices of the greedy spanner in $\mathbb{R}^2$ has
a separator of size $O(\sqrt{k})$. Their technique is inherently planar and is
not extensible to higher dimensions. They left showing the existence of a small
separator for the greedy spanner in $\mathbb{R}^d$ for any constant $d\geq 3$
as an open problem. In this paper, we resolve the problem of Eppstein and
Khodabandeh by showing that any subgraph of $k$ vertices of the greedy spanner
in $\mathbb{R}^d$ has a separator of size $O(k^{1-1/d})$. We introduce a new
technique that gives a simple characterization for any geometric graph to have
a sublinear separator that we dub $\tau$-lanky: a geometric graph is
$\tau$-lanky if any ball of radius $r$ cuts at most $\tau$ edges of length at
least $r$ in the graph. We show that any $\tau$-lanky geometric graph of $n$
vertices in $\mathbb{R}^d$ has a separator of size $O(\tau n^{1-1/d})$. We then
derive our main result by showing that the greedy spanner is $O(1)$-lanky. We
indeed obtain a more general result that applies to unit ball graphs and point
sets of low fractal dimensions in $\mathbb{R}^d$. Our technique naturally
extends to doubling metrics. We use the $\tau$-lanky characterization to show
that there exists a $(1+\epsilon)$-spanner for doubling metrics of dimension
$d$ with a constant maximum degree and a separator of size
$O(n^{1-\frac{1}{d}})$; this result resolves an open problem posed by Abam and
Har-Peled a decade ago.
</p></div>
    </summary>
    <updated>2021-07-15T23:03:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06406</id>
    <link href="http://arxiv.org/abs/2107.06406" rel="alternate" type="text/html"/>
    <title>A Theoretical Framework for Learning from Quantum Data</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heidari:Mohsen.html">Mohsen Heidari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Padakandla:Arun.html">Arun Padakandla</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Szpankowski:Wojciech.html">Wojciech Szpankowski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06406">PDF</a><br/><b>Abstract: </b>Over decades traditional information theory of source and channel coding
advances toward learning and effective extraction of information from data. We
propose to go one step further and offer a theoretical foundation for learning
classical patterns from quantum data. However, there are several roadblocks to
lay the groundwork for such a generalization. First, classical data must be
replaced by a density operator over a Hilbert space. Hence, deviated from
problems such as state tomography, our samples are i.i.d density operators. The
second challenge is even more profound since we must realize that our only
interaction with a quantum state is through a measurement which -- due to
no-cloning quantum postulate -- loses information after measuring it. With this
in mind, we present a quantum counterpart of the well-known PAC framework.
Based on that, we propose a quantum analogous of the ERM algorithm for learning
measurement hypothesis classes. Then, we establish upper bounds on the quantum
sample complexity quantum concept classes.
</p></div>
    </summary>
    <updated>2021-07-15T22:45:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06399</id>
    <link href="http://arxiv.org/abs/2107.06399" rel="alternate" type="text/html"/>
    <title>The Perfect Matching Cut Problem Revisited</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Van_Bang.html">Van Bang Le</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Telle:Jan_Arne.html">Jan Arne Telle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06399">PDF</a><br/><b>Abstract: </b>In a graph, a perfect matching cut is an edge cut that is a perfect matching.
Perfect Matching Cut (PMC) is the problem of deciding whether a given graph has
a perfect matching cut, and is known to be NP-complete. We revisit the problem
and show that PMC remains NP-complete when restricted to bipartite graphs of
maximum degree 3 and arbitrarily large girth. Complementing this hardness
result, we give two graph classes in which PMC is polynomial time solvable. The
first one includes claw-free graphs and graphs without an induced path on five
vertices, the second one properly contains all chordal graphs. Assuming the
Exponential Time Hypothesis, we show there is no $O^*(2^{o(n)})$-time algorithm
for PMC even when restricted to $n$-vertex bipartite graphs, and also show that
PMC can be solved in $O^*(1.2721^n)$ time by means of an exact branching
algorithm.
</p></div>
    </summary>
    <updated>2021-07-15T22:43:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06309</id>
    <link href="http://arxiv.org/abs/2107.06309" rel="alternate" type="text/html"/>
    <title>Tight bounds on the Fourier growth of bounded functions on the hypercube</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iyer:Siddharth.html">Siddharth Iyer</a>, Anup Rao, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reis:Victor.html">Victor Reis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rothvoss:Thomas.html">Thomas Rothvoss</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yehudayoff:Amir.html">Amir Yehudayoff</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06309">PDF</a><br/><b>Abstract: </b>We give tight bounds on the degree $\ell$ homogenous parts $f_\ell$ of a
bounded function $f$ on the cube. We show that if $f: \{\pm 1\}^n \rightarrow
[-1,1]$ has degree $d$, then $\| f_\ell \|_\infty$ is bounded by
$d^\ell/\ell!$, and $\| \hat{f}_\ell \|_1$ is bounded by $d^\ell
e^{\binom{\ell+1}{2}} n^{\frac{\ell-1}{2}}$. We describe applications to
pseudorandomness and learning theory. We use similar methods to generalize the
classical Pisier's inequality from convex analysis. Our analysis involves
properties of real-rooted polynomials that may be useful elsewhere.
</p></div>
    </summary>
    <updated>2021-07-15T22:38:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05690</id>
    <link href="http://arxiv.org/abs/2107.05690" rel="alternate" type="text/html"/>
    <title>Worst-Case Welfare of Item Pricing in the Tollbooth Problem</title>
    <feedworld_mtime>1626307200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Zihan.html">Zihan Tan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Teng:Yifeng.html">Yifeng Teng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Mingfei.html">Mingfei Zhao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05690">PDF</a><br/><b>Abstract: </b>We study the worst-case welfare of item pricing in the tollbooth problem. The
problem was first introduced by Guruswami et al, and is a special case of the
combinatorial auction in which (i) each of the $m$ items in the auction is an
edge of some underlying graph; and (ii) each of the $n$ buyers is single-minded
and only interested in buying all edges of a single path. We consider the
competitive ratio between the hindsight optimal welfare and the optimal
worst-case welfare among all item-pricing mechanisms, when the order of the
arriving buyers is adversarial. On the one hand, we prove an $\Omega(m^{1/8})$
lower bound of the competitive ratio for general graphs. We show that an
$m^{\Omega(1)}$ competitive ratio is unavoidable even if the graph is a grid,
or if the capacity of every edge is augmented by a constant $c$. On the other
hand, we study the competitive ratio for special families of graphs. In
particular, we improve the ratio when the input graph $G$ is a tree, from 8
(proved by Cheung and Swamy) to 3. We prove that the ratio is $2$ (tight) when
$G$ is a cycle and $O(\log^2 m)$ when $G$ is an outerplanar graph.
</p>
<p>All positive results above require that the seller can choose a proper
tie-breaking rule to maximize the welfare. In the paper we also consider the
setting where the tie-breaking power is on the buyers' side, i.e. the buyer can
choose whether or not to buy her demand path when the total price of edges in
the path equals her value. We show that the gap between the two settings is at
least a constant even when the underlying graph is a single path (this special
case is also known as the highway problem). Meanwhile, in this setting where
buyers have the tie-breaking power, we also prove an $O(1)$ upper bound of
competitive ratio for special families of graphs.
</p></div>
    </summary>
    <updated>2021-07-15T22:44:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18952</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/" rel="alternate" type="text/html"/>
    <title>Socially Reproduced Experiments</title>
    <summary>We must avoid becoming one Cropped from USA Today source JosÃ© Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseballâs most dramatic cheating accusation of 2019. Today, at baseballâs All-Star break, we review this and other social [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>We must avoid becoming one</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/altuvehr/" rel="attachment wp-att-18954"><img alt="" class="alignright size-full wp-image-18954" height="151" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AltuveHR.jpg?resize=151%2C151&amp;ssl=1" width="151"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from USA Today <a href="https://www.usatoday.com/story/sports/mlb/2021/07/11/astros-jose-altuve-walk-off-hr-shirt-ripped-off-yankees/7933018002/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
JosÃ© Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseballâs most dramatic cheating accusation of 2019.</p>
<p>
Today, at baseballâs All-Star break, we review this and other social experiments that have quite a bit more data.</p>
<p>
Altuve won the 2019 American League Championship series with a pinch-hit homer in the bottom of the ninth against the Yankeesâ closer, Aroldis Chapman. As he approached home plate, he was <a href="https://www.youtube.com/watch?v=MTNBnk1dz6g">seen</a> telling his teammates waiting to mob him at the plate not to rip off his jersey in celebration. He subsequently <a href="https://youtu.be/-ryvOya4PoE?t=247">scooted</a> into the corridor behind the dugout, then re-emerged into the on-field celebration. This fed accusations that he had been wired with a buzzer to know what kind of pitch was coming from Chapman, in line with <a href="https://en.wikipedia.org/wiki/Houston_Astros_sign_stealing_scandal">sign-stealing</a> by other means from the Astrosâ 2017 championship season and 2018 that was proven and punished by Major League Baseball. </p>
<p>
Almost the same scenario was reproduced Sunday: Houston down 7-5 against the Yankees with two out and two on base in the ninth, Altuve up against the Yankeesâ closer (Chad Green recently supplanting Chapman). Altuve <a href="https://www.youtube.com/watch?v=OHrc6OdYOpo">socked</a> a homer to the same part of the ballpark to complete a shocking six-run comeback. Immediately upon touching home, he had his shirt ripped off to reveal nothing but the top half of his birthday suit. This was the most direct way possible to witness that he could have hit the other homer without illegal information.</p>
<p>
</p><p/><h2> Examples and Non-Examples </h2><p/>
<p/><p>
I have dealt with chess-cheating cases in which electronic buzzing has been specifically alleged, including the two <a href="https://en.wikipedia.org/wiki/Borislav_Ivanov#Retirement_from_competitive_chess_and_brief_return_to_chess-related_activities">most</a> prominent <a href="https://www.nytimes.com/2013/08/18/crosswords/chess/a-master-is-disqualified-over-suspicions-of-cheating.html">cases</a> of 2013. I will not take this post further in this direction, however, but rather to pose this question:</p>
<blockquote><p><b> </b> <em> What is considered a âsocial proofâ of an assertionâespecially when there are elements of scientific control and reproduction? </em>
</p></blockquote>
<p/><p>
A simple example is a police lineup. This tries to control for whether a witness has previously seen the accused by including the accused among usually four or five similarly represented people. Picking the right person is considered to prove the previous encounter. Statistically, however, this is a <a href="https://en.wikipedia.org/wiki/P-value"><img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-value</a> of only 0.20 or 0.167, which are not considered significant at even the weakest level of âstatistical proof.â Allowing <a href="https://www.apa.org/monitor/julaug04/lineups">null</a> lineups does not change the statistics much.</p>
<p>
Baseball gives a non-example that surprises me. One of the bad performances that cost Chapman his closer role was losing an 8-4 lead against the Los Angeles Angels on June 30. As a fantasy-baseball player, Iâve regularly observed poor pitching (by the closers on my âfantasy teamâ) when the lead is too large to earn credit for a coveted <a href="https://en.wikipedia.org/wiki/Save_(baseball)">save</a>. Does the data reproduce a phenomenon of closers bearing down less when way ahead, with no âsaveâ to gain? A <a href="https://www.beyondtheboxscore.com/2014/1/27/5344580/the-closer-mentality-part-1-closers-in-non-save-situations">study</a> after the 2013 season, which cleverly represented performances by the same <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scores I use in chess, found none. âMeltdownsâ like Chapmanâs are offset by cases where closers pitched better. The <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scores in the study are all in the range -1.25 to +1.50 anyway, which count as statistically random.</p>
<p>
This study used a reasonably large data set, one that is well-defined and admits controlling factors such as normalizing for game circumstances and the quality of the opposing hitters. At least it is more than the two instances of Altuve. In-between would be an attempt to determine whether certain national soccer teams are consistently worse at penalty-kick tiebreaks. Englandâs and Italyâs teams brought their long tortured histories together in the tiebreak of Sundayâs European Cup final. The Italians missed two of five kicks, a score that often spells doom, but the English missed three.</p>
<p>
</p><p/><h2> Larger Scale </h2><p/>
<p/><p>
Dick and I are really interested in âexperimentsâ that have spilled into society, with minimal controls but large data. One sphere of this is cybersecurity. </p>
<p>
It seems to us that only in the past decade have security experts begun formalizing their research as experimental science with repeatability and reproducibility as explicit criteria. The NSA devoted a special 2012 <a href="https://www.nsa.gov/Portals/70/documents/resources/everyone/digital-media-center/publications/the-next-wave/TNW-19-2.pdf">issue</a> of their <em>Next Wave</em> series to what they titled as âDeveloping a blueprint for a science of cybersecurity.â Among the contents are:</p>
<ul>
<li>
An introductory essay by Carl Landwehr titled, âCybersecurity: From engineering to science.â <p/>
</li><li>
A linchpin <a href="https://www.cs.dartmouth.edu/~ccpalmer/teaching/cs55/Resources/Papers/TNW_19_2_BlueprintScienceCybersecurity_Schneider.pdf">paper</a> by Fred Schneider titled, âBlueprint for a science of cybersecurity.â <p/>
</li><li>
A <a href="https://www.cs.cmu.edu/~maxion/pubs/Maxion12.pdf">paper</a> by Roy Maxion titled, âMaking experiments dependable,â which came from a 2011 Springer LNCS <a href="https://link.springer.com/book/10.1007/978-3-642-24541-1">Festschrift</a>.
</li></ul>
<p>
Maxionâs main example is <a href="https://en.wikipedia.org/wiki/Keystroke_dynamics">keystroke biometrics</a>. This covers inferences made from typing style on a computer keyboard or mouse or similar handheld input device. This can be used to verify identity or screen for malfeasant activities. Online chess playing platforms collect data of this natureâokay we could not resist adding chess example. </p>
<p>
Another area is experiments designed to simulate attacks and test defenses against them. Schneiderâs paper begins with a contrast between <em>predictive</em> modeling versus <em>reactive</em> handling of them. About the latter, he draws an analogy with health care:</p>
<blockquote><p><b> </b> <em> âSome health problems are best handled in a reactive manner. We know what to do when somebody breaks a finger, and each year we create a new influenza vaccine in anticipation of the flu season to come. But only after making significant investments in basic medical sciences are we starting to understand the mechanisms by which cancers grow, and a cure seems to require that kind of deep understanding.â </em>
</p></blockquote>
<p/><p>
He goes on to outline the kind of scientific foundation that could hopefully underlie a âcureâ for intrusion and malware and the like. </p>
<p>
What we have seen happen especially in the past months, howeverâin both health and securityâis uncontrolled experiments with society as the domain. Large-scale ransomware attacks are becoming as frequent as hurricanes and heat waves. And of course, the pandemic. These share with Altuve the property of being one-off instances, but have large data on the receiving end.</p>
<p>
</p><p/><h2> Summer Pandemic Update </h2><p/>
<p/><p>
The following chart updates our June 20 <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">post</a> on the state of the pandemic and its projection for the summerâfor Florida and the United Kingdom in particular:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/flukcases071321/" rel="attachment wp-att-18955"><img alt="" class="aligncenter wp-image-18955" height="440" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/FLUKCases071321.png?resize=450%2C440&amp;ssl=1" width="450"/></a></p>
<p>
The vertical line shows about where the charts were on June 20. The past few days are the first where we can point to a significant rise in Florida, though Missouri had a similar rise last week and it is showing up in some other states.  The charts are taken from the <em>Worldometer</em> coronavirus <a href="https://www.worldometers.info/coronavirus/">pages</a>. </p>
<p>
The UK rise looks ghastly. It was a subtext of our previous post to worry that allowing the large dense soccer crowds at Londonâs Wembley Stadium for the semis and finalâand anything similar in baseballâwould stoke the rise in our respective countries even more. However, the rate of hospitalizations in the UK has remained largely flat. This Fortune <a href="https://fortune.com/2021/07/08/kids-vulnerable-covid-delta-variant-vaccinated-europe/">article</a> last week is one of several attesting that the new cases are mostly in children or in vaccinated people with enough immunity to contain the âbreakthroughâ positive. The UK is going ahead with large-scale re-openings later this month, with the portion of those 18 and older who have had one dose approaching 90% and those fully vaccinated coming past 70%. The latter number in relation to the whole population is about 52%.</p>
<p>
The US looks like becoming an experiment in how the local vaccination rate affects the numbers. The rates of those fully vaccinated by state are currently eerily <a href="https://www.cnn.com/2021/07/08/politics/electoral-map-vaccine-map-covid-19/index.html">similar</a> to Joe Bidenâs vote percentage in the state. One aspect of scientific reproducibility is the size of the simplest classifier of the results. For a presidential vote to have simpler explaining power than any factors of biology or other life circumstances would make a strange experiment indeed.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Dick and I tried to come up with other examplesâfrom computer security in particularâto sustain what has been occupying our thoughts about standards of proof for policy. We would welcome some examples from you, our readers. </p>
<p>
And of course, we have been concerned about the present course of the pandemic amid re-openings since the referenced post last month. In the meantime, if it is your taste, please enjoy the All-Star Game, which Altuve is, ironically, <a href="https://calltothepen.com/2021/07/11/houston-astros-officially-skipping-star-game/">skipping</a>.</p>
<p/><p><br/>
[some word fixes and changes]</p></font></font></div>
    </content>
    <updated>2021-07-13T19:47:02Z</updated>
    <published>2021-07-13T19:47:02Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="baseball"/>
    <category term="coronavirus"/>
    <category term="Jose Altuve"/>
    <category term="pandemic"/>
    <category term="reproducibility"/>
    <category term="science"/>
    <category term="security"/>
    <category term="social process"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>GÃ¶del's Lost Letter and P=NP</title>
      <updated>2021-07-16T00:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/102</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/102" rel="alternate" type="text/html"/>
    <title>TR21-102 |  Tight bounds on the Fourier growth of bounded functions on the hypercube | 

	Siddharth Iyer, 

	Anup Rao, 

	Victor Reis, 

	Thomas Rothvoss, 

	Amir Yehudayoff</title>
    <summary>We give tight bounds on the degree $\ell$  homogenous parts $f_\ell$ of a bounded function $f$ on the cube. We show that if $f: \{\pm 1\}^n \rightarrow [-1,1]$ has degree $d$, then $\| f_\ell \|_\infty$ is bounded by $d^\ell/\ell!$, and $\| \hat{f}_\ell \|_1$ is bounded by $d^\ell e^{{\ell+1 \choose 2}} n^{\frac{\ell-1}{2}}$. We describe applications to pseudorandomness and learning theory. We use similar methods to generalize the classical Pisier's inequality from convex analysis. Our analysis  involves properties of real-rooted polynomials that may be useful elsewhere.</summary>
    <updated>2021-07-13T17:55:40Z</updated>
    <published>2021-07-13T17:55:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-16T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/101</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/101" rel="alternate" type="text/html"/>
    <title>TR21-101 |  A Parallel Repetition Theorem for the GHZ Game: A Simpler Proof | 

	Uma Girish, 

	Justin Holmgren, 

	Kunal Mittal, 

	Ran Raz, 

	Wei Zhan</title>
    <summary>We give a new proof of the fact that the parallel repetition of the (3-player) GHZ game reduces the value of the game to zero polynomially quickly. That is, we show that the value of the $n$-fold GHZ game is at most $n^{-\Omega(1)}$. This was first established by Holmgren and Raz [HR20]. We present a new proof of this theorem that we believe to be simpler and more direct. Unlike most previous works on parallel repetition, our proof makes no use of information theory, and relies on the use of Fourier analysis.

The GHZ game [GHZ89] has played a foundational role in the understanding of quantum information theory, due in part to the fact that quantum strategies can win the GHZ game with probability $1$. It is possible that improved parallel repetition bounds may find applications in this setting.

Recently, Dinur, Harsha, Venkat, and Yuen [DHVY17] highlighted the GHZ game as a simple three-player game, which is in some sense maximally far from the class of multi-player games whose behavior under parallel repetition is well understood. Dinur et al. conjectured that parallel repetition decreases the value of the GHZ game exponentially quickly, and speculated that progress on proving this would shed light on parallel repetition for general multi-player (multi-prover) games.</summary>
    <updated>2021-07-13T14:48:27Z</updated>
    <published>2021-07-13T14:48:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-16T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://sarielhp.org/blog/?p=9420</id>
    <link href="https://sarielhp.org/blog/?p=9420" rel="alternate" type="text/html"/>
    <title>FSTTCS 2021 deadline is this Mondayâ¦</title>
    <summary>The link is here. Due to the pandemic it is going to be virtual. Quoting Bob Dylan, this blog is not dead, it is just asleep.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The link is <a href="https://www.fsttcs.org.in/2021/" rel="noreferrer noopener" target="_blank">here</a>. Due to the pandemic it is going to be virtual.</p>



<p>Quoting Bob Dylan,  this blog is not dead, it is just asleep.</p></div>
    </content>
    <updated>2021-07-13T03:38:25Z</updated>
    <published>2021-07-13T03:38:25Z</published>
    <category term="Research"/>
    <category term="cs theory"/>
    <author>
      <name>Sariel</name>
    </author>
    <source>
      <id>https://sarielhp.org/blog</id>
      <link href="https://sarielhp.org/blog/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://sarielhp.org/blog" rel="alternate" type="text/html"/>
      <subtitle>Sariel's blog</subtitle>
      <title>Vanity of Vanities, all is Vanity</title>
      <updated>2021-07-16T00:20:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4532</id>
    <link href="https://lucatrevisan.wordpress.com/2021/07/12/what-a-difference-a-few-months-can-make/" rel="alternate" type="text/html"/>
    <title>What a difference a few months can make</title>
    <summary>Piazza Duomo, in Milan, on December 26, 2020 Piazza Duomo, in Milan, on July 11, 2021</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Piazza Duomo, in Milan, on December 26, 2020</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure>



<p>Piazza Duomo, in Milan, on July 11, 2021</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure></div>
    </content>
    <updated>2021-07-12T17:04:51Z</updated>
    <published>2021-07-12T17:04:51Z</published>
    <category term="Milan"/>
    <category term="things that are excellent"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2021-07-16T00:20:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/exponential-mechanism-bounded-range/</id>
    <link href="https://differentialprivacy.org/exponential-mechanism-bounded-range/" rel="alternate" type="text/html"/>
    <title>A Better Privacy Analysis of the Exponential Mechanism</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A basic and frequent task in data analysis is <em>selection</em> â given a set of options \(\mathcal{Y}\), output the (approximately) best one, where âbestâ is defined by some loss function \(\ell : \mathcal{Y} \times \mathcal{X}^n \to \mathbb{R}\) and a dataset \(x \in \mathcal{X}^n\). That is, we want to output some \(y \in \mathcal{Y}\) that approximately minimizes \(\ell(y,x)\). Naturally, we are interested in <em>private selection</em> â i.e., the output should be differentially private in terms of the dataset \(x\).
This post discusses algorithms for private selection â in particular, we give an improved privacy analysis of the popular exponential mechanism.</p>

<h2 id="the-exponential-mechanism">The Exponential Mechanism</h2>

<p>The most well-known algorithm for private selection is the <a href="https://en.wikipedia.org/wiki/Exponential_mechanism_(differential_privacy)"><em>exponential mechanism</em></a> <a href="https://doi.org/10.1109/FOCS.2007.66" title="Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007."><strong>[MT07]</strong></a>. The exponential mechanism \(M : \mathcal{X}^n \to \mathcal{Y} \) is a randomized algorithm given by \[\forall x \in \mathcal{X}^n ~ \forall y \in \mathcal{Y} ~~~~~ \mathbb{P}[M(x) = y] = \frac{\exp(-\frac{\varepsilon}{2\Delta} \ell(y,x))}{\sum_{yâ \in \mathcal{Y}} \exp(-\frac{\varepsilon}{2\Delta} \ell(yâ,x)) }, \tag{1}\] where \(\Delta\) is the sensitivity of the loss function \(\ell\) given by \[\Delta = \sup_{x,xâ \in \mathcal{X}^n : d(x,xâ) \le 1} \max_{y\in\mathcal{Y}} |\ell(y,x) - \ell(y,xâ)|,\tag{2}\] where the supremum is taken over all datasets \(x\) and \(xâ\) differing on the data of a single individual (which we denote by \(d(x,xâ)\le 1\)).</p>

<p>In terms of utility, we can easily show that <a href="https://arxiv.org/abs/1511.02513" title="Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, Jonathan Ullman. Algorithmic Stability for Adaptive Data Analysis. STOC 2016."><strong>[BNSSSU16]</strong></a> \[\mathbb{E}[\ell(M(x),x)] \le \min_{y \in \mathcal{Y}} \ell(y,x) + \frac{2\Delta}{\varepsilon} \log |\mathcal{Y}|\] for all \(x \in \mathcal{X}^n\) (and we can also give high probability bounds).</p>

<p>It is easy to show that the exponential mechanism satisfies \(\varepsilon\)-differential privacy.
But there is more to this story! Weâre going to look at a more refined privacy analysis.</p>

<h2 id="bounded-range">Bounded Range</h2>

<p>The privacy guarantee of the exponential mechanism is more precisely characterized by <em>bounded range</em>. This was observed and defined by David Durfee and Ryan Rogers <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a> and further analyzed later <a href="https://arxiv.org/abs/1909.13830" title="Jinshuo Dong, David Durfee, Ryan Rogers. Optimal Differential Privacy Composition for Exponential Mechanisms. ICML 2020."><strong>[DDR20]</strong></a>.</p>

<blockquote>
  <p><strong>Definition 1 (Bounded Range).</strong><sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1" rel="footnote">1</a></sup> 
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\eta\)-bounded range if, for all pairs of inputs \(x, xâ \in \mathcal{X}^n\) differing only on the data of a single individual, there exists some \(t \in \mathbb{R}\) such that \[\forall y \in \mathcal{Y} ~~~~~ \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(xâ)=y]}\right) \in [t, t+\eta].\] Here \(t\) may depend on the pair of input datasets \(x,xâ\), but not on the output \(y\).</p>
</blockquote>

<p>To interpret this definition, we <a href="https://differentialprivacy.org/flavoursofdelta/">recall the definition of the privacy loss random variable</a>: Define \(f : \mathcal{Y} \to \mathbb{R}\) by \[f(y) = \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(xâ)=y]}\right).\] Then the privacy loss random variable \(Z \gets \mathsf{PrivLoss}(M(x)\|M(xâ))\) is given by \(Z = f(M(x))\).</p>

<p>Pure \(\varepsilon\)-differential privacy is equivalent to demanding that the privacy loss is bounded by \(\varepsilon\) â i.e., \(\mathbb{P}[|Z|\le\varepsilon]=1\). Approximate \((\varepsilon,\delta)\)-differential privacy is, roughly, equivalent to demanding that \(\mathbb{P}[Z\le\varepsilon]\ge1-\delta\).<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2" rel="footnote">2</a></sup></p>

<p>Now \(\eta\)-bounded range is simply demanding that the privacy loss \(Z\) is supported on some interval of length \(\eta\). This interval \([t,t+\eta]\) may depend on the pair \(x,xâ\).</p>

<p>Bounded range and pure differential privacy are equivalent up to a factor of 2 in the parameters:</p>

<blockquote>
  <p><strong>Lemma 2 (Bounded Range versus Pure Differential Privacy).</strong></p>
  <ul>
    <li>\(\varepsilon\)-differential privacy implies \(\eta\)-bounded range with \(\eta \le 2\varepsilon\).</li>
    <li>\(\eta\)-bounded range implies \(\varepsilon\)-differential privacy with \(\varepsilon \le \eta\).</li>
  </ul>
</blockquote>

<p><em>Proof.</em> The first part of the equivalence follows from the fact that pure \(\varepsilon\)-differential privacy implies the privacy loss is supported on the interval \([-\varepsilon,\varepsilon]\). Thus, if we set \(t=-\varepsilon\) and \(\eta=2\varepsilon\), then \([t,t+\eta] = [-\varepsilon,\varepsilon]\).
The second part follows from the fact that the support of the privacy loss \([t,t+\eta]\) must straddle \(0\). That is, the privacy loss cannot be always positive nor always negative, so \(0 \in [t,t+\eta]\) and, hence, \([t,t+\eta] \subseteq [-\eta,\eta]\). Otherwise \(\forall y ~ f(y)&gt;0\) or \(\forall y ~ f(y)&lt;0\)  would imply \(\forall y ~ \mathbb{P}[M(x)=y]&gt;\mathbb{P}[M(xâ)=y]\) or \(\forall y ~ \mathbb{P}[M(x)=y]&lt;\mathbb{P}[M(xâ)=y]\), contradicting the fact that \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x)=y] = 1\) and \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(xâ)=y] = 1\). â</p>

<p>OK, back to the exponential mechanism:</p>

<blockquote>
  <p><strong>Lemma 3 (The Exponential Mechanism is Bounded Range).</strong>
The exponential mechanism (given in Equation 1 above) satisfies \(\varepsilon\)-bounded range .<sup id="fnref:3"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:3" rel="footnote">3</a></sup></p>
</blockquote>

<p><em>Proof.</em>
We have \[e^{f(y)} = \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(xâ)=y]} = \frac{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x))}{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,xâ))} \cdot \frac{\sum_{yâ} \exp(-\frac{\varepsilon}{2\Delta} \ell(yâ,xâ))}{\sum_{yâ} \exp(-\frac{\varepsilon}{2\Delta} \ell(yâ,x))}.\]
Setting \(t = \log\left(\frac{\sum_{yâ} \exp(-\frac{\varepsilon}{2\Delta} \ell(yâ,xâ))}{\sum_{yâ} \exp(-\frac{\varepsilon}{2\Delta} \ell(yâ,x))}\right) - \frac{\varepsilon}{2}\), we have \[ f(y) = \frac{\varepsilon}{2\Delta} (\ell(y,xâ)-\ell(y,x)+\Delta) + t.\]
By the definition of sensitivity (given in Equation 2), we have \( 0 \le \ell(y,xâ)-\ell(y,x)+\Delta \le 2\Delta\), whence \(t \le f(y) \le t + \varepsilon\). â</p>

<p>Bounded range is not really a useful privacy definition on its own. Thus weâre going to relate it to a relaxed version of differential privacy next.</p>

<h2 id="concentrated-differential-privacy">Concentrated Differential Privacy</h2>

<p>Concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a> and its variants <a href="https://arxiv.org/abs/1603.01887" title="Cynthia Dwork, Guy N. Rothblum. Concentrated Differential Privacy. 2016."><strong>[DR16]</strong></a> <a href="https://arxiv.org/abs/1702.07476" title="Ilya Mironov. R&#xE9;nyi Differential Privacy. CCS 2017."><strong>[M17]</strong></a> are relaxations of pure differential privacy with many nice properties. In particular, it composes very cleanly.</p>

<blockquote>
  <p><strong>Definition 4 (Concentrated Differential Privacy).</strong>
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, xâ \in \mathcal{X}^n\) differing only on the data of a single individual, 
\[\forall \lambda &gt; 0 ~~~~~ \mathbb{E}[\exp( \lambda Z)] \le \exp(\lambda(\lambda+1)\rho),\tag{3}\]
where \(Z \gets \mathsf{PrivLoss}(M(x)\|M(xâ))\) is the privacy loss random variable.<sup id="fnref:4"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:4" rel="footnote">4</a></sup></p>
</blockquote>

<p>Intuitively, concentrated differential privacy requires that the privacy loss is subgaussian. Specifically, the bound on the moment generating function of \(\rho\)-concentrated differential privacy is tight if the privacy loss \(Z\) follows the distribution \(\mathcal{N}(\rho,2\rho)\). Indeed, the privacy loss random variable of the Gaussian mechanism has such a distribution.<sup id="fnref:5"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:5" rel="footnote">5</a></sup></p>

<p>OK, back to the exponential mechanism:
We know that \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>.
This, of course, applies to the exponential mechaism. A cool fact â that we want to draw more attention to â is that we can do better! 
Specifically, \(\eta\)-bounded range implies \(\frac18 \eta^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/2004.07223" title="Mark Cesar, Ryan Rogers. Bounding, Concentrating, and Truncating: Unifying Privacy Loss Composition for Data Analytics. ALT 2021."><strong>[CR21]</strong></a>.
What follows is a proof of this fact following that of Mark Cesar and Ryan Rogers, but with some simplification.</p>

<blockquote>
  <p><strong>Theorem 5 (Bounded Range implies Concentrated Differential Privacy).</strong>
If \(M\) is \(\eta\)-bounded range, then it is \(\frac18\eta^2\)-concentrated differentially private.</p>
</blockquote>

<p><em>Proof.</em>
Fix datasets \(x,xâ \in \mathcal{X}^n\) differing on a single individualâs data.
Let \(Z \gets \mathsf{PrivLoss}(M(x)\|M(xâ))\) be the privacy loss random variable of the mechanism \(M\) on this pair of datasets.
By the definition of bounded range (Definition 1), there exists some \(t \in \mathbb{R}\) such that \(Z \in [t, t+\eta]\) with probability 1.
Now we employ <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_lemma">Hoeffdingâs Lemma</a> <a href="https://doi.org/10.1080%2F01621459.1963.10500830" title="Wassily Hoeffding. Probability inequalities for sums of bounded random variables. JASA 1963."><strong>[H63]</strong></a>:</p>
<blockquote>
  <p><strong>Lemma 6 (Hoeffdingâs Lemma).</strong>
Let \(X\) be a random variable supported on the interval \([a,b]\). Then, for all \(\lambda \in \mathbb{R}\), we have \[\mathbb{E}[\exp(\lambda X)] \le \exp \left( \mathbb{E}[X] \cdot \lambda + \frac{(b-a)^2}{8} \cdot \lambda^2 \right).\]</p>
</blockquote>

<p>Applying the lemma to the privacy loss gives \[\forall \lambda \in \mathbb{R} ~~~~~  \mathbb{E}[\exp(\lambda Z)] \le \exp \left( \mathbb{E}[Z] \cdot \lambda + \frac{\eta^2}{8} \cdot \lambda^2 \right).\]
The only remaining thing we need is to show is that \(\mathbb{E}[Z] \le \frac18 \eta^2\).<sup id="fnref:6"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:6" rel="footnote">6</a></sup></p>

<p>If we set \(\lambda = -1 \), then we get \( \mathbb{E}[\exp( - Z)] \le \exp \left( -\mathbb{E}[Z] + \frac{\eta^2}{8} \right)\), which rearranges to \(\mathbb{E}[Z] \le \frac18 \eta^2 - \log \mathbb{E}[\exp( - Z)]\). 
Now we have \[ \mathbb{E}[\exp( - Z)] \!=\! \sum_y \mathbb{P}[M(x)\!=\!y] \exp(-f(y)) \!=\! \sum_y \mathbb{P}[M(x)\!=\!y]  \!\cdot\! \frac{\mathbb{P}[M(xâ)\!=\!y]}{\mathbb{P}[M(x)\!=\!y]} \!=\! 1.\]
â</p>

<p>This brings us to the TL;DR of this post:</p>

<blockquote>
  <p><strong>Corollary 7.</strong> The exponential mechanism (given by Equation 1) is \(\frac18 \varepsilon^2\)-concentrated differentially private.</p>
</blockquote>

<p>This is great news. The standard analysis only gives \(\frac12 \varepsilon^2\)-concentrated differential privacy. Constants matter when applying differential privacy, and we save a factor of 4 in the concentrated differential privacy analysis of the exponential mechanism for free with this improved analysis.</p>

<p>Combining Lemma 2 with Theorem 5 also gives a simpler proof of the conversion from pure differential privacy to concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>:</p>

<blockquote>
  <p><strong>Corollary 8.</strong> \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy.</p>
</blockquote>

<h2 id="beyond-the-exponential-mechanism">Beyond the Exponential Mechanism</h2>

<p>The exponential mechanism is not the only algorithm for private selection. A closely-related algorithm is <em>report noisy max/min</em>:<sup id="fnref:7"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:7" rel="footnote">7</a></sup> Draw independent noise \(\xi_y\) from some distribution for each \(y \in \mathcal{Y}\) then output \[M(x) = \underset{y \in \mathcal{Y}}{\mathrm{argmin}} ~ \ell(y,x) - \xi_y.\]</p>

<p>If the noise distribution is an appropriate <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution</a>, then report noisy max is exactly the exponential mechanism. (This equivalence is known as the âGumbel max trick.â)</p>

<p>We can also use the Laplace distribution or the exponential distribution. Report noisy max with the exponential distribution is equivalent to the <em>permute and flip</em> algorithm <a href="https://arxiv.org/abs/2010.12603" title="Ryan McKenna, Daniel Sheldon. Permute-and-Flip: A new mechanism for differentially private selection . NeurIPS 2020."><strong>[MS20]</strong></a> <a href="https://arxiv.org/abs/2105.07260" title="Zeyu Ding, Daniel Kifer, Sayed M. Saghaian N. E., Thomas Steinke, Yuxin Wang, Yingtai Xiao, Danfeng Zhang. The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with Exponential Noise. 2021."><strong>[DKSSWXZ21]</strong></a>. However, these algorithms donât enjoy the same improved bounded range and concentrated differential privacy guarantees as the exponential mechanism.</p>

<p>There are also other variants of the selection problem. For example, in some cases we can assume that only a few options have low loss and the rest of the options have high loss â i.e., there is a gap between the minimum loss and the second-lowest loss (or, more generally, the \(k\)-th lowest loss). In this case there are algorithms that attain better accuracy than the exponential mechanism under relaxed privacy definitions <a href="https://arxiv.org/abs/1409.2177" title="Kamalika Chaudhuri, Daniel Hsu, Shuang Song. The Large Margin Mechanism for Differentially Private Maximization. NIPS 2014."><strong>[CHS14]</strong></a> <a href="https://dl.acm.org/doi/10.1145/3188745.3188946" title=" Mark Bun, Cynthia Dwork, Guy N. Rothblum, Thomas Steinke. Composable and versatile privacy via truncated CDP. STOC 2018."><strong>[BDRS18]</strong></a> <a href="https://arxiv.org/abs/1905.13229" title="Mark Bun, Gautam Kamath, Thomas Steinke, Zhiwei Steven Wu. Private Hypothesis Selection. NeurIPS 2019."><strong>[BKSW19]</strong></a>.</p>

<p>There are a lot of interesting aspects of private selection, including questions for further research! We hope to have further posts about some of these topics.</p>

<hr/>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For simplicity, we restrict our discussion here to finite sets of outputs, although the definitions, algorithms, and results can be extended to infinite sets.Â <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">â©</a></p>
    </li>
    <li id="fn:2">
      <p>To be more precise, \((\varepsilon,\delta)\)-differential privacy is equivalent to demanding that \(\mathbb{E}[\max\{0,1-\exp(\varepsilon-Z)\}]\le\delta\) <a href="https://arxiv.org/abs/2004.00010" title="Cl&#xE9;ment L. Canonne, Gautam Kamath, Thomas Steinke. The Discrete Gaussian for Differential Privacy. NeurIPS 2020."><strong>[CKS20]</strong></a>. (To be completely precise, we must appropriately deal with the \(Z=\infty\) case, which we ignore in this discussion for simplicity.)Â <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">â©</a></p>
    </li>
    <li id="fn:3">
      <p>This proof actually gives <a href="https://dongjs.github.io/2020/02/10/ExpMech.html">a slightly stronger result</a>: We can replace the sensitivity \(\Delta\) (defined in Equation 2) by half the range \[\hat\Delta = \frac12 \sup_{x,xâ \in \mathcal{X}^n : d(x,xâ) \le 1} \left( \max_{\overline{y}\in\mathcal{Y}} \ell(\overline{y},x) - \ell(\overline{y},xâ) - \min_{\underline{y}\in\mathcal{Y}} \ell(\underline{y},x) - \ell(\underline{y},xâ) \right).\] We always have \(\hat\Delta \le \Delta\) but it is possible that \(\hat\Delta &lt; \Delta\) and the privacy analysis of the exponential mechanism still works if we replace \(\Delta\) by \(\hat\Delta\).Â <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:3">â©</a></p>
    </li>
    <li id="fn:4">
      <p>Equivalently, a randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, xâ \in \mathcal{X}^n\) differing only on the data of a single individual, \[\forall \lambda &gt; 0 ~~~~~ \mathrm{D}_{\lambda+1}(M(x)\|M(xâ)) \le \lambda(\lambda+1)\rho,\] where \(\mathrm{D}_{\lambda+1}(M(x)\|M(xâ)))\) is the order \(\lambda+1\) RÃ©nyi divergence of \(M(x)\) from \(M(xâ)\).Â <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:4">â©</a></p>
    </li>
    <li id="fn:5">
      <p>To be precise, if \(M(x) = q(x) + \mathcal{N}(0,\sigma^2I)\), then \(M : \mathcal{X}^n \to \mathbb{R}^d\) satisfies \(\frac{\Delta_2^2}{2\sigma^2}\)-concentrated differential privacy, where \(\Delta_2 = \sup_{x,xâ\in\mathcal{X}^n : d(x,xâ)\le1} \|q(x)-q(xâ)\|_2\) is the 2-norm sensitivity of \(q:\mathcal{X}^n \to \mathbb{R}^d\). Furthermore, the privacy loss of the Gaussian mechanism is itself a Gaussian and it makes the inequality defining concentrated differential privacy (Equation 3) an equality for all \(\lambda\)Â <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:5">â©</a></p>
    </li>
    <li id="fn:6">
      <p>Note that the expectation of the privacy loss is simply the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>: \(\mathbb{E}[Z] = \mathrm{D}_1( M(x) \| M(xâ) )\).Â <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:6">â©</a></p>
    </li>
    <li id="fn:7">
      <p>We have presented selection here in terms of minimization, but most of the literature is in terms of maximization.Â <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:7">â©</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2021-07-12T17:00:00Z</updated>
    <published>2021-07-12T17:00:00Z</published>
    <author>
      <name>Thomas Steinke</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-07-15T23:06:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/100</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/100" rel="alternate" type="text/html"/>
    <title>TR21-100 |  Karchmer-Wigderson Games for Hazard-free Computation | 

	Christian Ikenmeyer, 

	Balagopal Komarath, 

	Nitin Saurabh</title>
    <summary>We present a Karchmer-Wigderson game to study the complexity of hazard-free formulas. This new game is both a generalization of the monotone Karchmer-Wigderson game and an analog of the classical Boolean Karchmer-Wigderson game. Therefore, it acts as a bridge between the existing monotone and general games.

Using this game, we prove hazard-free formula size and depth lower bounds that are provably stronger than those possible by the standard technique of transferring results from monotone complexity in a black-box fashion.
For the multiplexer function
we give (1) a hazard-free formula of optimal size and (2) an improved low-depth hazard-free formula of almost optimal size and (3) a hazard-free formula with alternation depth $2$ that has optimal depth.
We then use our optimal constructions to obtain an improved universal worst-case hazard-free formula size upper bound.
We see our results as a significant step towards establishing hazard-free computation as an independent missing link between Boolean complexity and monotone complexity.</summary>
    <updated>2021-07-12T07:35:35Z</updated>
    <published>2021-07-12T07:35:35Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-16T00:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6138760775046652444</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6138760775046652444/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6138760775046652444" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6138760775046652444" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html" rel="alternate" type="text/html"/>
    <title>Would you take this bet (Part 2) ?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Â Recall from my last post (<a href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html">here</a>)</p><p><br/></p><p>I offer you the following bet:Â </p><p>I will flip a coin.</p><p>IfÂ  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>IfÂ  HEADS you get 2 dollars and we end there.</p><p>IfÂ  TAILS I flip again</p><p><br/></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>The expected value is infinity.</p><p><br/></p><p>Would you pay $1000 to play this game?</p><p>Everyone who responded said NO. Most gave reasons similar to what I have below.Â </p><p>This is called The St Petersburg Paradox. Not sure it's a paradox, but it is odd. The concrete question of <i>would you pay $1000 to play</i>Â might be a paradox since most people would say NO even though the expected value is infinity.Â  SeeÂ <a href="https://en.wikipedia.org/wiki/St._Petersburg_paradox">here</a>Â for more background.</p><p>Shapley (seeÂ <a href="https://www.sciencedirect.com/science/article/abs/pii/0022053177901429">here</a>) gives a good reason why you would notÂ  pay $1000 to play the game, and also how much you should pay to play the game (spoiler alert: not much). I will summarize his argument and then add to it.Â </p><p><br/></p><p>1) Shapley's argument: Lets say the game goes for 40 rounds. Then you are owed 2^{40} dollars.Â </p><p>The amount of money in the world is, according toÂ <a href="https://bibloteka.com/how-much-money-is-there-in-the-world/#:~:text=Short%20Answer%3A%20Money%20in%20circulation%20in%20the%20world,the%20medium%20of%20trade%20for%20goods%20and%20services.">this article</a>Â around 1.2 quadrillion dollarsÂ  which is roughly 2^{40} dollars.Â </p><p>So the expected value calculation has to be capped at (say) 40 rounds. This means you expect to get 20 dollars! So pay 19 to play.Â </p><p><br/></p><p>2) My angle which is very similar: at what point is more money not going to change your life at all? For me it is way less than 2^{40} dollars. Hence I would not pay 1000. Or even 20.Â </p><p><i>Exercise</i>: If you think the game will go at most R rounds and you only wand D dollars, how much should you pay to play? You can also juggle more parameters - the bias of the coin, how much they pay out when you win.Â </p><p>Does Shapley's discussionsÂ Â <i>resolve</i> the paradox? It depends on what you consider paradoxical. If the paradox is that people would NOT pay 1000 even though the expected value is infinity, then ShapleyÂ  resolves the paradoxÂ  by contrasting the real world to the math world.Â </p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-07-12T00:53:00Z</updated>
    <published>2021-07-12T00:53:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-15T20:38:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/099</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/099" rel="alternate" type="text/html"/>
    <title>TR21-099 |  Improved Product-Based High-Dimensional Expanders | 

	Louis Golowich</title>
    <summary>High-dimensional expanders generalize the notion of expander graphs to higher-dimensional simplicial complexes. In contrast to expander graphs, only a handful of high-dimensional expander constructions have been proposed, and no elementary combinatorial construction with near-optimal expansion is known. In this paper, we introduce an improved combinatorial high-dimensional expander construction, by modifying a previous construction of Liu, Mohanty, and Yang (ITCS 2020), which is based on a high-dimensional variant of a tensor product. Our construction achieves a spectral gap of $\Omega(\frac{1}{k^2})$ for random walks on the $k$-dimensional faces, which is only quadratically worse than the optimal bound of $\Theta(\frac{1}{k})$. Previous combinatorial constructions, including that of Liu, Mohanty, and Yang, only achieved a spectral gap that is exponentially small in $k$. We also present reasoning that suggests our construction is optimal among similar product-based constructions.</summary>
    <updated>2021-07-11T07:39:29Z</updated>
    <published>2021-07-11T07:39:29Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-16T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/098</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/098" rel="alternate" type="text/html"/>
    <title>TR21-098 |  On the Probabilistic Degree of an $n$-variate Boolean Function | 

	Srikanth Srinivasan, 

	S Venkitesh</title>
    <summary>Nisan and Szegedy (CC 1994) showed that any Boolean function $f:\{0,1\}^n\to\{0,1\}$ that depends on all its input variables, when represented as a real-valued multivariate polynomial $P(x_1,\ldots,x_n)$, has degree at least $\log n - O(\log \log n)$. This was improved to a tight $(\log n - O(1))$ bound by Chiarelli, Hatami and Saks (Combinatorica 2020). Similar statements are also known for other Boolean function complexity measures such as Sensitivity (Simon (FCT 1983)), Quantum query complexity, and Approximate degree (Ambainis and de Wolf (CC 2014)). 

In this paper, we address this question for \emph{Probabilistic degree}. The function $f$ has probabilistic degree at most $d$ if there is a random real-valued polynomial of degree at most $d$ that agrees with $f$ at each input with high probability. Our understanding of this complexity measure is significantly weaker than those above: for instance, we do not even know the probabilistic degree of the OR function, the best-known bounds put it between $(\log n)^{1/2-o(1)}$ and $O(\log n)$ (Beigel, Reingold, Spielman (STOC 1991); Tarui (TCS 1993); Harsha, Srinivasan (RSA 2019)).

Here we can give a near-optimal understanding of the probabilistic degree of $n$-variate functions $f$, \emph{modulo} our lack of understanding of the probabilistic degree of OR. We show that if the probabilistic degree of OR is $(\log n)^c$, then the minimum possible probabilistic degree of such an $f$ is at least $(\log n)^{c/(c+1)-o(1)}$, and we show this is tight up to $(\log n)^{o(1)}$ factors.</summary>
    <updated>2021-07-11T04:50:01Z</updated>
    <published>2021-07-11T04:50:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-16T00:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/07/10/angles-arc-triangles</id>
    <link href="https://11011110.github.io/blog/2021/07/10/angles-arc-triangles.html" rel="alternate" type="text/html"/>
    <title>Angles of arc-triangles</title>
    <summary>Piecewise-circular curves or, if you like, arc-polygons are a very old topic in mathematics. Archimedes and Pappus studied the arbelos, a curved triangle formed from three semicircles, and Hippocrates of Chios found that the lune of Hippocrates, a two-sided figure bounded by a semicircle and a quarter-circle, has the same area as an isosceles right triangle stretched between the same two points. The history of the Reuleaux triangle, bounded by three sixths of circles, stretches back well past Reuleaux to the shapes of of Gothic church windows and its use by Leonardo da Vinci for fortress floor plans and world map projections. But despite their long history and frequent use (for instance in the design of machined parts), there are some basic properties of arc-polygons that seem to have been unexplored so far.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Piecewise-circular curves or, if you like, arc-polygons are a very old topic in mathematics. Archimedes and Pappus studied the <a href="https://en.wikipedia.org/wiki/Arbelos">arbelos</a>, a curved triangle formed from three semicircles, and Hippocrates of Chios found that the <a href="https://en.wikipedia.org/wiki/Lune_of_Hippocrates">lune of Hippocrates</a>, a two-sided figure bounded by a semicircle and a quarter-circle, has the same area as an isosceles right triangle stretched between the same two points. The history of the <a href="https://en.wikipedia.org/wiki/Reuleaux_triangle">Reuleaux triangle</a>, bounded by three sixths of circles, stretches back well past Reuleaux to the shapes of of Gothic church windows and its use by Leonardo da Vinci for fortress floor plans and world map projections. But despite their long history and frequent use (for instance in the design of machined parts), there are some basic properties of arc-polygons that seem to have been unexplored so far.</p>

<p>I looked at one of these properties, <a href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html">the ability of arc-triangles to tile the plane</a>, in an earlier post. Another of these properties involves the feasible combinations of angles of these shapes. As is well known, in a straight-sided triangle in the plane, the three interior angles always sum to exactly \(\pi\), and any three positive angles summing to \(\pi\) are possible. Let \(T\) be the set of triples of angles \((\theta_1,\theta_2,\theta_3)\) from triangles, and reinterpret these triples as coordinates of points in Euclidean space. Then \(T\) is itself an equilateral triangle, with corners at the three points \((\pi,0,0)\), \((0,\pi,0)\), and \((0,0\pi)\). (More precisely, itâs the relative interior of this triangle.)</p>

<p>What about arc-triangles? Are their angles similarly constrained? What shape do their triples of angles make? First of all, their angles donât have a fixed sum (except for the tilers, for which this sum is again \(\pi\)). The arbelos has three interior angles that are all zero, summing to zero. The Reuleux triangle has three angles of \(2\pi/3\), summing to \(2\pi\). <a href="https://11011110.github.io/blog/2021/05/15/linkage.html">Boscovitchâs cardioid</a>, below, uses three semicircles like the arbelos, but with one interior angle of \(2\pi\) and two others equal to \(\pi\), summing to \(4\pi\). The <a href="https://en.wikipedia.org/wiki/Trefoil">trefoil</a>, a common architectural motif, bulges outward from its three corners, forming interior angles that are much larger, up to \(2\pi\) each for a trefoil made from three \(5/6\)-circle arcs, for a total interior angle of \(6\pi\).</p>

<p style="text-align: center;"><img alt="Boscovich's cardioid" src="https://11011110.github.io/blog/assets/2021/boscovich.svg"/></p>

<p>Nevertheless, for a non-self-crossing arc-triangle, not all combinations of angles are possible. For instance, itâs not possible to have one angle that is zero and two that are \(2\pi\). My new preprint, âAngles of arc-polygons and lombardi drawings of cactiâ (<a href="https://arxiv.org/abs/2107.03615">arXiv:2107.03615
</a>, with UCI students Daniel Frishberg and Martha Osegueda, to appear at CCCG) proves a precise characterization: beyond the obvious requirement that each angle \(\theta_i\) be in the range \(0\le\theta_i\le 2\pi\), we have only the additional inequalities</p>

\[-\pi &lt; \frac{\pi - \theta_i + \theta_{i+1} - \theta_{i+2}}{2} &lt; \pi\]

<p>where the index arithmetic is done modulo three. The formula in the middle of each of these inequalities is itself an angle, the angle of incidence between one of the circular arcs of the arc-triangle and the circle through its three corners. Where the straight triangles had an equilateral-triangle feasible region, the arc-triangles have a more complicated shape. The obvious constraints \(0\le\theta_i\le 2\pi\) would produce a cubical feasible region \([0,2\pi]^3\), but the additional inequalities above cut off six corners of the cube, leaving a feasible region looking like this:</p>

<p style="text-align: center;"><img alt="The feasible region for triples of angles of arc-triangles" src="https://11011110.github.io/blog/assets/2021/feasible-arc-triangles.svg"/></p>

<p>The motivating application for all of this is graph drawing, and more specifically Lombardi drawing, in which edges are circular arcs meeting at equal angles at each vertex. Using our new understanding of arc-polygons, we prove that every <a href="https://en.wikipedia.org/wiki/Cactus_graph">cactus graph</a> has a planar Lombardi drawing for its natural embedding (the one in which each cycle of the cactus forms a face) but might not for some other embeddings, including the one below.</p>

<p style="text-align: center;"><img alt="An embedded cactus that has no planar Lombardi drawing" src="https://11011110.github.io/blog/assets/2021/badhat.svg"/></p>

<p>But beyond graph drawing, I think that the long history and many applications of arc-polygons justifies more study of their general properties. For instance, what about arc-polygons with more than three sides? What can their angles be? Our paper has a partial answer, enough to answer the questions we asked in our Lombardi drawing application, but a complete characterization for arc-polygons of more than three sides is still open.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106557711895868173">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-07-10T11:06:00Z</updated>
    <published>2021-07-10T11:06:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-07-10T18:31:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/097</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/097" rel="alternate" type="text/html"/>
    <title>TR21-097 |  Number of Variables for Graph Identification and the Resolution of GI Formulas | 

	Jacobo Toran, 

	Florian WÃ¶rz</title>
    <summary>We show that the number of variables and the quantifier depth needed to distinguish a pair of graphs by first-order logic sentences exactly match the complexity measures of clause width and positive depth needed to refute the corresponding graph isomorphism formula in propositional narrow resolution. 

Using this connection, we obtain upper and lower bounds for refuting graph  isomorphism formulas in (normal) resolution. In particular, we show that if $k$ is the minimum number of variables needed to distinguish two graphs with $n$ vertices each, then there is an $n^{\mathrm{O}(k)}$ resolution refutation size upper bound for the corresponding isomorphism formula, as well as lower bounds of $2^{k-1}$ and $k$ for the tree-like resolution size and resolution clause space for this formula. We also show a resolution size lower bound of ${\exp} \big( \Omega(k^2/n) \big)$ for the case of colored graphs with constant color class size.

Applying these results, we prove the first exponential lower bound for graph isomorphism formulas in the proof system SRC-1, a system that extends resolution with a global symmetry rule, thereby answering an open question posed by Schweitzer and Seebach.</summary>
    <updated>2021-07-09T15:37:56Z</updated>
    <published>2021-07-09T15:37:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-16T00:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/07/09/spanners-unit-ball</id>
    <link href="https://11011110.github.io/blog/2021/07/09/spanners-unit-ball.html" rel="alternate" type="text/html"/>
    <title>Spanners for unit ball graphs in doubling spaces</title>
    <summary>My student Hadi Khodabandeh had a paper with me on spanners earlier this year at SoCG, in which we showed that the greedy spanner algorithm for points in the Euclidean plane produces graphs with few crossings and small separators. Now we have another spanner preprint: âOptimal spanners for unit ball graphs in doubling metricsâ, arXiv:2106.15234.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My student Hadi Khodabandeh had a paper with me on spanners earlier this year at SoCG, in which we showed that <a href="https://11011110.github.io/blog/2020/02/17/spanners-have-sparse.html">the greedy spanner algorithm for points in the Euclidean plane produces graphs with few crossings and small separators</a>. Now we have another spanner preprint: <a href="https://arxiv.org/abs/2106.15234">âOptimal spanners for unit ball graphs in doubling metricsâ, arXiv:2106.15234</a>.</p>

<p><a href="https://en.wikipedia.org/wiki/Doubling_space">Doubling metrics</a> are a generalization of Euclidean spaces. Like Euclidean spaces, they have a dimension, the <em>doubling dimension</em>, but it might not be an integer. Even the doubling dimension of the Euclidean plane itself is \(\log_2 7\approx 2.807355\); this means that every circular disk of radius \(r\) in the plane can be covered by seven closed disks of radius \(r/2\).</p>

<p style="text-align: center;"><img alt="Seven disks of radius $$r/2$$ cover a single disk of radius $$r$$" src="https://11011110.github.io/blog/assets/2021/doubling-dim.svg"/></p>

<p>Analogously, the <em>doubling constant</em> of a metric space (if it exists) is the smallest number \(c\) such that every closed metric ball (the set of points within some radius \(r\) of a fixed point) can be covered by \(c\) balls of half the radius. The <em>doubling dimension</em> is the binary logarithm of the doubling constant. A metric space is a <em>doubling metric</em> or <em>doubling space</em> if it has a doubling constant and doubling dimension. This is true for all Euclidean spaces: for instance, if you cover a ball with a grid of hypercubes, sized small enough that their long diagonal has length at most the radius of the ball, you obtain doubling constant at most \(\lceil 2\sqrt{d}\rceil^d\), although it seems difficult to compute the precise doubling constant in general.</p>

<p style="text-align: center;"><img alt="Nine disks of radius $$r/2$$ cover a square grid that covers a single disk of radius $$r$$" src="https://11011110.github.io/blog/assets/2021/grid-doubling.svg"/></p>

<p>The hyperbolic plane provides a natural example of a space that is not a doubling space: arbitrarily large-radius disks require arbitrarily many half-radius disks to cover them.</p>

<p>Many results in computational geometry can be generalized to doubling metrics, but not always, and sometimes with difficulty. That includes results on spanners, as we consider in our paper. A spanner of a weighted graph is a subgraph whose shortest path distances approximate the distances in the full graph, and often for spanners of metric spaces one uses the complete graph, weighted by the metric distance between each pair of points. But here, we are using a different graph, the unit ball graph. The unit ball graph, for points in a continuous space, has an edge whenever two unit balls centered at two of the points have a nonempty intersection, and it can be extended to discrete point sets by instead including an edge whenever two points are at distance at most 2 (or, as in our paper, distance at most 1; scaling doesnât really change anything). The weights are the same as in the complete graph. If a spanner accurately approximates all edge weights, it approximates all paths.</p>

<p>The unit ball graph has fewer edges to approximate than the complete graph. But that actually makes it harder to approximate, because by the same token there are fewer edges that can be used in the spanner. Despite that, the greedy spanner algorithm still produces a spanner, but one of its key properties is lost when going from Euclidean to doubling spaces: Euclidean greedy spanners have bounded degree, but greedy spanners in doubling spaces do not. Instead, our paper provides different spanner algorithms that apply to unit ball graphs, approximate paths in these graphs arbitrarily well, have bounded degree, have total weight a constant times that of the minimum spanning tree, and can be constructed efficiently in a distributed model of computing. I think the details are too technical to go into here, so see the paper for more.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106551534535329957">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-07-09T09:06:00Z</updated>
    <published>2021-07-09T09:06:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-07-10T18:31:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/096</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/096" rel="alternate" type="text/html"/>
    <title>TR21-096 |  Keep That Card in Mind: Card Guessing with Limited Memory | 

	Boaz Menuhin, 

	Moni Naor</title>
    <summary>A card guessing game is played between two players, Guesser and Dealer. At the beginning of the game, the Dealer holds a deck of $n$ cards (labeled $1, ..., n$). ForÂ $n$ turns, the Dealer draws a card from the deck, the Guesser guesses which card was drawn, and then the card is discarded from the deck. The Guesser receives a point for each correctly guessed card. 

With perfect memory, a Guesser can keep track of all cards that were played so far and pick at random a card that has not appeared so far, yielding in expectation $\ln n$ correct guesses. With no memory, the best a Guesser can do will result in a single guess in expectation. 

We consider the case of a memory bounded Guesser that has $m &lt; n$ memory bits. We show that the performance of such a memory bounded Guesser depends much on the behavior of the Dealer. In more detail, we show that there is a gap between the static case, where the Dealer draws cards from a properly shuffled deck or a prearranged one, and the adaptive case, where the Dealer draws cards thoughtfully, in an adversarial manner. Specifically: 

1. We show a Guesser with $O(\log^2 n)$ memory bits that scores a near optimal result against any static Dealer. 

2. We show that no Guesser with $m$ bits of memory can score better than $O(\sqrt{m})$ correct guesses, thus, no Guesser can score better than $\min \{\sqrt{m}, \ln n\}$, i.e., the above Guesser is optimal. 

3. We show an efficient adaptive Dealer against which no Guesser withÂ $m$ memory bits can make more than $\ln m + 2 \ln \log n + O(1)$ correct guesses in expectation. 

These results are (almost) tight, and we prove them using compression arguments that harness the guessing strategy for encoding.</summary>
    <updated>2021-07-08T21:32:56Z</updated>
    <published>2021-07-08T21:32:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-16T00:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2021/07/08/imp-reg-tf/</id>
    <link href="http://offconvex.github.io/2021/07/08/imp-reg-tf/" rel="alternate" type="text/html"/>
    <title>Implicit Regularization in Tensor Factorization&amp;#58; Can Tensor Rank Shed Light on Generalization in Deep Learning?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In effort to understand implicit regularization in deep learning, a lot of theoretical focus is being directed at matrix factorization, which can be seen as linear neural networks.
This post is based on our <a href="https://arxiv.org/pdf/2102.09972.pdf">recent paper</a> (to appear at ICML 2021), where we take a step towards practical deep learning, by investigating <em>tensor factorization</em> â a model equivalent to a certain type of non-linear neural networks.
It is well known that <a href="https://arxiv.org/pdf/0911.1393.pdf">most tensor problems are NP-hard</a>, and accordingly, the common sentiment is that working with tensors (in both theory and practice) entails extreme difficulties.
However, by adopting a dynamical systems view, we manage to avoid such difficulties, and establish an implicit regularization towards low <em>tensor rank</em>.
Our results suggest that tensor rank may shed light on generalization in deep learning.</p>

<h2 id="challenge-finding-a-right-measure-of-complexity">Challenge: finding a right measure of complexity</h2>

<p>Overparameterized neural networks are mysteriously able to generalize even when trained without any explicit regularization.
Per conventional wisdom, this generalization stems from an <em>implicit regularization</em> â a tendency of gradient-based optimization to fit training examples with predictors of minimal ââcomplexity.ââ
A major challenge in translating this intuition to provable guarantees is that we lack measures for predictor complexity that are quantitative (admit generalization bounds), and at the same time, capture the essence of natural data (images, audio, text etc.), in the sense that it can be fit with predictors of low complexity.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/imp_reg_tf_data_complexity.png" style="width: 500px; padding-bottom: 10px; padding-top: 5px;"/>
<br/>
<i><b>Figure 1:</b> 
To explain generalization in deep learning, a complexity <br/>
measure must allow the fit of natural data with low complexity. On the <br/>
other hand, when fitting data which does not admit generalization, <br/>
e.g. random data, the complexity should be high. 
</i>
</div>
<p><br/></p>

<h2 id="a-common-testbed-matrix-factorization">A common testbed: matrix factorization</h2>

<p>Without a clear complexity measure for practical neural networks, existing analyses usually focus on simple settings where a notion of complexity is obvious. 
A common example of such a setting is <em>matrix factorization</em> â matrix completion via linear neural networks. 
This model was discussed pretty extensively in previous posts (see <a href="http://www.offconvex.org/2019/06/03/trajectories/">one</a> by Sanjeev, <a href="http://www.offconvex.org/2019/07/10/trajectories-linear-nets/">one</a> by Nadav and Wei and <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">another one</a> by Nadav), but for completeness we present it again here.</p>

<p>In <em>matrix completion</em> weâre given a subset of entries from an unknown matrix $W^* \in \mathbb{R}^{d, dâ}$, and our goal is to predict the unobserved entries. 
This can be viewed as a supervised learning problem with $2$-dimensional inputs, where the label of the input $( i , j )$ is $( W^* )_{i,j}$.
Under such a viewpoint, the observed entries are the training set, and the average reconstruction error over unobserved entries is the test error, quantifying generalization.
A predictor can then be thought of as a matrix, and a natural notion of complexity is its <em>rank</em>.
Indeed, in many real-world scenarios (a famous example is the <a href="https://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a>) one is interested in <a href="https://arxiv.org/pdf/1601.06422.pdf">recovering a low rank matrix from incomplete observations</a>.</p>

<p>A ââdeep learning approachââ to matrix completion is matrix factorization, where the idea is to use a linear neural network (fully connected neural network with no non-linearity), and fit observations via gradient descent (GD). 
This amounts to optimizing the following objective:</p>

<div style="text-align: center;">
\[
 \min\nolimits_{W_1 , \ldots , W_L} ~ \sum\nolimits_{(i,j) \in observations} \big[ ( W_L \cdots W_1 )_{i , j} - (W^*)_{i,j} \big]^2 ~.
\]
</div>

<p>It is obviously possible to constrain the rank of the produced solution by limiting the shared dimensions of the weight matrices $\{ W_j \}_j$. 
However, from an implicit regularization standpoint, the most interesting case is where rank is unconstrained and the factorization can express any matrix. 
In this case there is no explicit regularization, and the kind of solution we get is determined implicitly by the parameterization and the optimization algorithm.</p>

<p>As it turns out, in practice, matrix factorization with near-zero initialization and small step size tends to accurately recover low rank matrices.
This phenomenon (first identified in <a href="https://papers.nips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf">Gunasekar et al. 2017</a>) manifests some kind of implicit regularization, whose mathematical characterization drew a lot of interest.
It was initially conjectured that matrix factorization implicitly minimizes nuclear norm (<a href="https://papers.nips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf">Gunasekar et al. 2017</a>), but recent evidence points to implicit rank minimization, stemming from incremental learning dynamics (see <a href="https://papers.nips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf">Arora et al. 2019</a>; <a href="https://papers.nips.cc/paper/2020/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf">Razin &amp; Cohen 2020</a>; <a href="https://openreview.net/pdf/e29b53584bc9017cb15b9394735cd51b56c32446.pdf">Li et al. 2021</a>). 
Today, it seems we have a relatively firm understanding of generalization in matrix factorization.
There is a complexity measure for predictors â matrix rank â by which implicit regularization strives to lower complexity, and the data itself is of low complexity (i.e. can be fit with low complexity). 
Jointly, these two conditions lead to generalization.</p>

<h2 id="beyond-matrix-factorization-tensor-factorization">Beyond matrix factorization: tensor factorization</h2>

<p>Matrix factorization is interesting on its own behalf, but as a theoretical surrogate for deep learning it is limited.
First, it corresponds to <em>linear</em> neural networks, and thus misses the crucial aspect of non-linearity. 
Second, viewing matrix completion as a prediction problem, it doesnât capture tasks with more than two input variables.
As we now discuss, both of these limitations can be lifted if instead of matrices one considers tensors.</p>

<p>A tensor can be thought of as a multi-dimensional array.
The number of axes in a tensor is called its <em>order</em>.
In the task of <em>tensor completion</em>, a subset of entries from an unknown tensor $\mathcal{W}^* \in \mathbb{R}^{d_1, \ldots, d_N}$ are given, and the goal is to predict the unobserved entries. 
Analogously to how matrix completion can be viewed as a prediction problem over two input variables, order-$N$ tensor completion can be seen as a prediction problem over $N$ input variables (each corresponding to a different axis).
In fact, any multi-dimensional prediction task with discrete inputs and scalar output can be formulated as a tensor completion problem.
Consider for example the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>, and for simplicity assume that image pixels hold one of two values, i.e. are either black or white. 
The task of predicting labels for the $28$-by-$28$ binary images can be seen as an order-$784$ (one axis for each pixel) tensor completion problem, where all axes are of length $2$ (corresponding to the number of values a pixel can take). 
For further details on how general prediction tasks map to tensor completion problems see <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a>.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/pred_prob_to_tensor_comp.png" style="width: 550px; padding-bottom: 10px; padding-top: 5px;"/>
<br/>
<i><b>Figure 2:</b> 
Prediction tasks can be viewed as tensor completion problems. <br/>
For example, predicting labels for input images with $3$ pixels, each taking <br/>
one of $5$ grayscale values, corresponds to completing a $5 \times 5 \times 5$ tensor.
</i>
</div>
<p><br/>
Like matrices, tensors can be factorized. 
The most basic scheme for factorizing tensors, named CANDECOMP/PARAFAC (CP), parameterizes a tensor as a sum of outer products (for information on this scheme, as well as others, see the <a href="http://www.kolda.net/publication/TensorReview.pdf">excellent survey</a> of Kolda and Bader).
In <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a> and this post, we use the term <em>tensor factorization</em> to refer to solving tensor completion by fitting observations via GD over CP parameterization, i.e. over the following objective ($\otimes$ here stands for outer product):</p>

<div style="text-align: center;">
\[
\min\nolimits_{ \{ \mathbf{w}_r^n \}_{r , n} } \sum\nolimits_{ (i_1 , ... , i_N) \in observations } \big[ \big(  {\textstyle \sum}_{r = 1}^R \mathbf{w}_r^1 \otimes \cdots \otimes \mathbf{w}_r^N \big)_{i_1 , \ldots , i_N} - (\mathcal{W}^*)_{i_1 , \ldots , i_N} \big]^2 ~.
\]
</div>

<p>The concept of rank naturally extends from matrices to tensors.
The <em>tensor rank</em> of a given tensor $\mathcal{W}$ is defined to be the minimal number of components (i.e. of outer product summands) $R$ required for CP parameterization to express it.
Note that for order-$2$ tensors, i.e. for matrices, this exactly coincides with matrix rank.
We can explicitly constrain the tensor rank of solutions found by tensor factorization via limiting the number of components $R$. 
However, since our interest lies on implicit regularization, we consider the case where $R$ is large enough for any tensor to be expressed.</p>

<p>By now you might be wondering what does tensor factorization have to do with deep learning.
Apparently, as Nadav mentioned in an <a href="http://www.offconvex.org/2020/11/27/reg_dl_not_norm/">earlier post</a>, analogously to how matrix factorization is equivalent to matrix completion (two-dimensional prediction) via linear neural networks, tensor factorization is equivalent to tensor completion (multi-dimensional prediction) with a certain type of <em>non-linear</em> neural networks (for the exact details behind the latter equivalence see <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a>). 
It therefore represents a setting one step closer to practical neural networks.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/mf_lnn_tf_nonlinear.png" style="width: 900px; padding-bottom: 10px; padding-top: 5px;"/>
<br/>
<i><b>Figure 3:</b> 
While matrix factorization corresponds to a linear neural network, <br/>
tensor factorization corresponds to a certain non-linear neural network.
</i>
</div>
<p><br/>
As a final piece of the analogy between matrix and tensor factorizations, in a <a href="https://arxiv.org/pdf/2005.06398.pdf">previous paper</a> (described in an <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">earlier post</a>) Noam and Nadav demonstrated empirically that (similarly to the phenomenon discussed above for matrices) tensor factorization with near-zero initialization and small step size tends to accurately recover low rank tensors.
Our goal in the <a href="https://arxiv.org/pdf/2102.09972.pdf">current paper</a> was to mathematically explain this finding. 
To avoid the <a href="https://arxiv.org/pdf/0911.1393.pdf">notorious difficulty of tensor problems</a>, we chose to adopt a dynamical systems view, and analyze directly the trajectories induced by GD.</p>

<h2 id="dynamical-analysis-implicit-tensor-rank-minimization">Dynamical analysis: implicit tensor rank minimization</h2>

<p>So what can we say about the implicit regularization in tensor factorization? 
At the core of our analysis is the following dynamical characterization of component norms:</p>

<blockquote>
  <p><strong>Theorem:</strong>
Running gradient flow (GD with infinitesimal step size) over a tensor factorization with near-zero initialization leads component norms to evolve by:
[ \frac{d}{dt} || \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) || \propto \color{brown}{|| \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) ||^{2 - 2/N}} ~,
]
where $\mathbf{w}_r^1 (t), \ldots, \mathbf{w}_r^N (t)$ denote the weight vectors at time $t \geq 0$.</p>
</blockquote>

<p>According to the theorem above, component norms evolve at a rate proportional to their size exponentiated by $\color{brown}{2 - 2 / N}$ (recall that $N$ is the order of the tensor to complete).
Consequently, they are subject to a momentum-like effect, by which they move slower when small and faster when large. 
This suggests that when initialized near zero, components tend to remain close to the origin, and then, after passing a critical threshold, quickly grow until convergence. 
Intuitively, these dynamics induce an incremental process where components are learned one after the other, leading to solutions with a few large components and many small ones, i.e. to (approximately) low tensor rank solutions!</p>

<p>We empirically verified the incremental learning of components in many settings. 
Here is a representative example from one of our experiments (see <a href="https://arxiv.org/pdf/2102.09972.pdf">the paper</a> for more):</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/tf_dyn_exps.png" style="width: 800px; padding-bottom: 15px; padding-top: 10px;"/>
<br/>
<i><b>Figure 4:</b> 
Dynamics of component norms during GD over tensor factorization. <br/>
An incremental learning effect is enhanced as initialization scale decreases, <br/>
leading to accurate completion of a low rank tensor.
</i>
</div>
<p><br/>
Using our dynamical characterization of component norms, we were able to prove that with sufficiently small initialization, tensor factorization (approximately) follows a trajectory of rank one tensors for an arbitrary amount of time. 
This leads to:</p>

<blockquote>
  <p><strong>Theorem:</strong>
If tensor completion has a rank one solution, then under certain technical conditions, tensor factorization will reach it.</p>
</blockquote>

<p>Itâs worth mentioning that, in a way, our results extend to tensor factorization the incremental rank learning dynamics known for matrix factorization (cf. <a href="https://papers.nips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf">Arora et al. 2019</a> and <a href="https://arxiv.org/pdf/2012.09839v1.pdf">Li et al. 2021</a>). 
As typical when transitioning from matrices to tensors, this extension entailed various challenges that necessitated use of different techniques.</p>

<h2 id="tensor-rank-as-measure-of-complexity">Tensor rank as measure of complexity</h2>

<p>Going back to the beginning of the post, recall that a major challenge towards understanding implicit regularization in deep learning is that we lack measures for predictor complexity that capture natural data. 
Now, let us recap what we have seen thus far:
$(1)$ tensor completion is equivalent to multi-dimensional prediction; 
$(2)$ tensor factorization corresponds to solving the prediction task with certain non-linear neural networks; 
and 
$(3)$ the implicit regularization of these non-linear networks, i.e. of tensor factorization, minimizes tensor rank.
Motivated by these findings, we ask the following:</p>

<blockquote>
  <p><strong>Question:</strong> 
Can tensor rank serve as a measure of predictor complexity?</p>
</blockquote>

<p>We empirically explored this prospect by evaluating the extent to which tensor rank captures natural data, i.e. to which natural data can be fit with predictors of low tensor rank.
As testbeds we used <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> and <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> datasets, comparing the resulting errors against those obtained when fitting two randomized variants: one generated via shuffling labels (âârand labelââ), and the other by replacing inputs with noise (âârand imageââ).</p>

<p>The following plot, displaying results for Fashion-MNIST (those for MNIST are similar), shows that with predictors of low tensor rank the original data is fit way more accurately than the randomized datasets. 
Specifically, even with tensor rank as low as one the original data is fit relatively well, while the error in fitting random data is close to trivial (variance of the label). 
This suggests that tensor rank as a measure of predictor complexity has potential to capture aspects of natural data! 
Note also that an accurate fit with low tensor rank coincides with low test error, which is not surprising given that low tensor rank predictors can be described with a small number of parameters.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/exp_complexity_fmnist.png" style="width: 600px; padding-bottom: 15px; padding-top: 10px;"/>
<br/>
<i><b>Figure 5:</b> 
Evaluation of tensor rank as a measure of complexity â standard datasets <br/>
can be fit accurately with predictors of low tensor rank (far beneath what is required by <br/>
random datasets), suggesting it may capture aspects of natural data. Plot shows mean <br/>
error of predictors with low tensor rank over Fashion-MNIST. Markers correspond <br/>
to separate runs differing in the explicit constraint on the tensor rank.
</i>
</div>

<h2 id="concluding-thoughts">Concluding thoughts</h2>

<p>Overall, <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a> shows that tensor rank captures both the implicit regularization of a certain type of non-linear neural networks, and aspects of natural data. 
In light of this, we believe tensor rank (or more advanced notions such as hierarchical tensor rank) might pave way to explaining both implicit regularization in more practical neural networks, and the properties of real-world data translating this implicit regularization to generalization.</p>

<p><a href="https://noamrazin.github.io/">Noam Razin</a>, <a href="https://asafmaman101.github.io/">Asaf Maman</a>, <a href="http://www.cohennadav.com/">Nadav Cohen</a></p></div>
    </summary>
    <updated>2021-07-08T09:00:00Z</updated>
    <published>2021-07-08T09:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2021-07-15T23:05:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/095</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/095" rel="alternate" type="text/html"/>
    <title>TR21-095 |  LEARN-Uniform Circuit Lower Bounds and Provability in Bounded Arithmetic | 

	Valentine Kabanets, 

	Igor Oliveira, 

	Marco Carmosino, 

	Antonina Kolokolova</title>
    <summary>We investigate randomized LEARN-uniformity, which captures the power of randomness and equivalence queries (EQ) in the construction of Boolean circuits for an explicit problem. This is an intermediate notion between P-uniformity and non-uniformity motivated by connections to learning, complexity, and logic.  Building on a number of techniques, we establish the first unconditional lower bounds against LEARN-uniform circuits:

-- For all $c\geq 1$, there is $L \in P$ that is not computable by circuits of size $n \cdot (\log n)^c$ generated in deterministic polynomial time with $o(\log n/\log \log n)$ equivalence queries to $L$. In other words, small circuits for $L$ cannot be efficiently learned using a bounded number of EQs.
-- For each $k\geq 1$, there is $L \in NP$ such that circuits for $L$ of size $O(n^k)$ cannot be learned in deterministic polynomial time with access to $n^{o(1)}$ EQs.
--  For each $k\geq 1$, there is a problem in promise-ZPP that is not in FZPP-uniform $SIZE[n^k]$.
-- Conditional and unconditional lower bounds against LEARN-uniform circuits in the general setting that combines randomized uniformity and access to EQs.

In all these lower bounds, the learning algorithm is allowed to run in arbitrary polynomial time, while the hard problem is computed in some fixed polynomial time.

We employ these results to investigate the (un)provability of non-uniform circuit upper bounds (e.g., Is NP contained in $SIZE[n^3]$?) in theories of bounded arithmetic. Some questions of this form have been addressed in recent papers of Krajicek-Oliveira (2017), Muller-Bydzovsky (2020), and Bydzovsky-Krajicek-Oliveira (2020) via a mixture of techniques from proof theory, complexity theory, and model theory. In contrast, by extracting computational information from proofs via a direct translation to LEARN-uniformity, we establish robust unprovability theorems that unify, simplify, and extend nearly all previous results. In addition, our lower bounds against randomized LEARN-uniformity yield unprovability results for theories augmented with the \emph{dual weak pigeonhole principle}, such as $APC^1$ (Jerabek, 2007), which is known to formalize a large fragment of modern complexity theory.

Finally, we make precise potential limitations of theories of bounded arithmetic such as PV (Cook, 1975) and Jerabek's theory $APC^1$, by showing unconditionally that these theories cannot prove statements like ``$NP\not\subseteq BPP \wedge NP\subset io$-P/poly'', i.e., that NP is uniformly ``hard'' but non-uniformly ``easy'' on infinitely many input lengths. In other words, if we live in such a complexity world, then this cannot be established feasibly.</summary>
    <updated>2021-07-07T21:27:49Z</updated>
    <published>2021-07-07T21:27:49Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-16T00:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/open-problem-optimal-query-release/</id>
    <link href="https://differentialprivacy.org/open-problem-optimal-query-release/" rel="alternate" type="text/html"/>
    <title>Open Problem - Optimal Query Release for Pure Differential Privacy</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Releasing large sets of statistical queries is a centerpiece of the theory of differential privacy.  Here, we are given a <em>dataset</em> \(x = (x_1,\dots,x_n) \in [T]^n\), and a set of <em>statistical queries</em> \(f_1,\dots,f_k\), where each query is defined by some bounded function \(f_j : [T] \to [-1,1]\), and (abusing notation) is defined as
\[
f_j(x) = \frac{1}{n} \sum_{i=1}^{n} f_j(x_i).
\]
We use \(f(x) = (f_1(x),\dots,f_k(x))\) to denote the vector consisting of the true answers to all these queries.
Our goal is to design an \((\varepsilon, \delta)\)-differentially private algorithm \(M\) that takes a dataset \(x\in [T]^n\) and outputs a random vector \(M(x)\in \mathbb{R}^k\) such that \(\| M(x) - f(x) \|\) is small in expectation for some norm \(\|\cdot\|\). Usually algorithms for this problem also give high probability bounds on the error, but we focus on expected error for simplicity.</p>

<p>This problem has been studied for both <em>pure differential privacy</em> (\(\delta = 0\)) and <em>appproximate differential privacy</em> (\(\delta &gt; 0\)), and for both \(\ell_\infty\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{\infty} ) \leq \alpha,
\]
and \(\ell_2\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{2} ) \leq \alpha k^{1/2},
\]
giving four variants of the problem.  By now we know tight worst-case upper and lower bounds for two of these variants, and nearly tight bounds (up to logarithmic factors) for a third. The tightest known upper bounds are given in the following table.</p>

<table>
  <tbody>
    <tr>
      <td>Â </td>
      <td>Pure DP</td>
      <td>Approx DP</td>
    </tr>
    <tr>
      <td>\( \ell_2 \)<br/>error</td>
      <td>\( \alpha \lesssim \left(\frac{\log^2 k ~\cdot~ \log^{3/2}T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://arxiv.org/abs/1212.0297">NTZ13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://guyrothblum.files.wordpress.com/2014/11/drv10.pdf">DRV10</a>]</td>
    </tr>
    <tr>
      <td>\( \ell_\infty \)<br/>error</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log T}{\varepsilon n} \right)^{1/3} \)  <br/> [<a href="https://arxiv.org/abs/1109.2229">BLR13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://guyrothblum.files.wordpress.com/2014/11/hr10.pdf">HR10</a>, <a href="https://arxiv.org/abs/1107.3731">GRU12</a>]</td>
    </tr>
  </tbody>
</table>

<p>The bounds for approximate DP are known to be tight [<a href="https://arxiv.org/abs/1311.3158">BUV14</a>].  Our two open problems both involve improving the best known upper bounds for pure differential privacy.</p>

<blockquote>
  <p><b>Open Problem 1:</b> What is the best possible \(\ell_\infty\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the known upper bound in the table can be improved to
\[
\alpha = \left(\frac{\log k \cdot \log T}{\varepsilon n} \right)^{1/2},
\]
which is known to be the best possible [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1].</p>

<blockquote>
  <p><b>Open Problem 2:</b> What is the best possible \(\ell_2\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the upper bound can be improved to
\[
\alpha = \left(\frac{\log T}{\varepsilon n} \right)^{1/2}.
\]
The construction used in [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1] can be analyzed to show this bound would be tight. Note, in particular, that this conjecture implies that the tight upper bound has no dependence on the number of queries, similarly to the case of \(\ell_2\) error and approximate DP.</p></div>
    </summary>
    <updated>2021-07-07T17:45:00Z</updated>
    <published>2021-07-07T17:45:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-07-15T23:06:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8892467103290123546</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8892467103290123546/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8892467103290123546" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8892467103290123546" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html" rel="alternate" type="text/html"/>
    <title>Would you take this bet (Part 1) ?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Â I am going to present a well known paradox (I didn't know it until last week, but the source I read said it was well known) and ask your opinion in this post, and reveal my thoughts in my next post.</p><p>I don't want you to go to the web and find out about it, I want your natural thoughts. Of course I can't stop you, but note that I did not give the name of the paradox.Â </p><p>Here it is:</p><p>I offer you the following bet:Â </p><p>I will flip a coin.</p><p>IfÂ  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>IfÂ  HEADS you get 2 dollars and we end there.</p><p>IfÂ  TAILS I flip again</p><p><br/></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>etc.Â </p><p>1) Expected value:Â </p><p>Prob of getting 1 dollar is 1/2</p><p>Prob of getting 2 dollars is 1/2^2</p><p>Prob of getting 2^2 dollars is 1/2^3</p><p>etc</p><p>Hence the Expected Value isÂ </p><p>1/2 + 1/2 + 1/2 + ... = INFINITY</p><p><br/></p><p>QUESTION: Would you pay $1000 to play the game?</p><p>Leave your answer in the comments and you may say whatever you want as well,</p><p>but I request you don't give the name of the paradox if you know it.Â </p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-07-07T00:58:00Z</updated>
    <published>2021-07-07T00:58:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-15T20:38:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=879</id>
    <link href="https://emanueleviola.wordpress.com/2021/07/06/windows-never-changes/" rel="alternate" type="text/html"/>
    <title>Windows never changes</title>
    <summary>For months, Windows 10 complained that it didnât have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the âstorageâ function that supposedly tells you whatâs taking space wasnât even close to the truth. This became so bad that I was forced to remove some [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>For months, Windows 10 complained that it didnât have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the âstorageâ function that supposedly tells you whatâs taking space wasnât even close to the truth. This became so bad that I was forced to remove some things I didnât want to remove, often with a lot of effort, because space was so tight that Windows didnât even have enough to run the uninstaller! In the end I became so desperate that I installed <em>TreeSize Free</em>. It quickly revealed that <em>crash plan </em> was taking up a huge amount of space. This revealed to be associated to the <em>Code42</em> program â a program that the system was listing as taking 200MB. Well, uninstalling Code42 freed SIXTY PERCENT of the hard disk space, 140GB.</p></div>
    </content>
    <updated>2021-07-06T16:42:32Z</updated>
    <published>2021-07-06T16:42:32Z</published>
    <category term="Uncategorized"/>
    <category term="tech"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2021-07-16T00:21:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1546</id>
    <link href="https://ptreview.sublinear.info/?p=1546" rel="alternate" type="text/html"/>
    <title>News for June 2021</title>
    <summary>A quieter month after last monthâs bonanza. One (applied!) paper on distribution testing, a paper on tolerant distribution testing, and a compendium of open problems. (Ed: alas, I missed the paper on tolerant distribution testing, authored by one of our editors. Sorry, ClÃ©ment!) Learning-based Support Estimation in Sublinear Time by Talya Eden,Â Piotr Indyk,Â Shyam Narayanan,Â Ronitt Rubinfeld,Â Sandeep [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p id="block-79f77829-3f22-4392-8cc9-3f466129d855">A quieter month after last monthâs bonanza. One (applied!) paper on distribution testing, a paper on tolerant distribution testing, and a compendium of open problems. <em>(Ed: alas, I missed the paper on tolerant distribution testing, authored by one of our editors. Sorry, ClÃ©ment!)</em></p>



<p id="block-db4c65bf-46c4-4c3e-ad70-d3348de84ff3"><strong>Learning-based Support Estimation in Sublinear Time</strong> by Talya Eden,Â Piotr Indyk,Â Shyam Narayanan,Â Ronitt Rubinfeld,Â Sandeep Silwal,Â and Tal Wagner (<a href="https://arxiv.org/abs/2106.08396">arXiv</a>). A classic problem in distribution testing is that of estimating the support size \(n\) of an unknown distribution \(\mathcal{D}\). (Assume that all elements in the support have probability at least \(1/n\).) A fundamental result of <a href="http://theory.stanford.edu/~valiant/papers/VV_stoc11.pdf">Valiant-Valiant</a> (2011)  proves that the sample complexity of this problem is \(\Theta(n/\log n)\). A line of work has emerged in trying to reduce this complexity, with additional sources of information. <a href="http://dspace.mit.edu/bitstream/handle/1721.1/101001/Rubinfeld_Testing%20probability.pdf;sequence=1">Canonne-Rubinfeld (2014)</a> showed that, if one can query the exact probabilities of elements, then the complexity can be made independent of \(n\). This paper studies a robust version of this assumption: suppose, we can get constant factor approximations to the probabilities. Then, the main result is that we can get a query complexity of \(n^{1-1/\log(\varepsilon^{-1})} \ll n/\log n\) (where the constant \(\varepsilon\) denotes the additive approximation to the support size). This paper also does empirical experiments to show that the new algorithm is indeed better in practice. Moreover, it shows that existing methods degraded rapidly with poorer probability estimates, while the new algorithm maintains its accuracy even with such estimates. </p>



<p><strong>The Price of Tolerance in Distribution Testing</strong> by ClÃ©ment L. Canonne, Ayush Jain, Gautam Kamath, and Jerry Li (<a href="https://arxiv.org/abs/2106.13414">arXiv</a>). While we have seen many results in distribution testing, the subject of tolerance is one that hasnât received as much attention. Consider the problem of testing if unknown distribution \(\mathcal{p}\) (over domain \([n]\)) is the same as known distribution \(\mathcal{q}\). We wish to distinguish \(\varepsilon_1\)-close from \(\varepsilon_2\)-far, under total variation distance. When \(\varepsilon_1\) is zero, this is the standard property testing setting, and classic results yield \(\Theta(\sqrt{n})\) sample complexity. If \(\varepsilon_1 = \varepsilon_2/2\), then we are looking for a constant factor approximation to the distance. And the complexity is \(\Theta(n/\log n)\). Surprisingly, nothing was known in better. Until this paper, that is. The main result gives a complete characterization of sample complexity (up to log factors), for all values of \(\varepsilon_1, \varepsilon_2\). Remarkably, the sample complexity has an additive term \((n/\log n) \cdot (\varepsilon_1/\varepsilon^2_2)\). Thus, when \(\varepsilon_1 &gt; \sqrt{\varepsilon_2}\), the sample complexity is \(\Theta(n/\log n)\). When \(\varepsilon_1\) is smaller, the main result gives a smooth dependence on the sample complexity. One the main challenges is that existing results use two very different techniques for the property testing vs constant-factor approximation regimes. The former uses simpler \(\ell_2\)-statistics (e.g. collision counting), while the latter is based on polynomial approximations (estimating moments). The upper bound in this paper shows that simpler statistics based on just the first two moments suffice to getting results for all regimes of \(\varepsilon_1, \varepsilon_2\).</p>



<p><strong>Open Problems in Property Testing of Graphs</strong> by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2021/088/">ECCC</a>). As the title clearly states, this is a survey covering a number of open problems in graph property testing. The broad division is based on the query model: dense graphs, bounded degree graphs, and general graphs. A reader will see statements of various classic open problems, such as the complexity of testing triangle freeness for dense graphs and characterizing properties that can be tested in \(poly(\varepsilon^{-1})\) queries. Arguably, there are more open problems (and fewer results) for testing in bounded degree graphs, where we lack broad characterizations of testable properties. An important, though less famous (?), open problem is that of the complexity of testing isomorphism. It would appear that the setting of general graphs, where we know the least, may be the next frontier for graph property testing. A problem that really caught my eye: can we transform testers that work for bounded degree graphs into those that work for bounded arboricity graphs? The latter is a generalization of bounded degree that has appeared in a number of recent results on sublinear graph algorithms.</p></div>
    </content>
    <updated>2021-07-06T02:27:19Z</updated>
    <published>2021-07-06T02:27:19Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-07-15T23:06:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/094</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/094" rel="alternate" type="text/html"/>
    <title>TR21-094 |  New Non-FPT Lower Bounds for Some Arithmetic Formulas | 

	SÃ©bastien Tavenas, 

	Nutan Limaye, 

	Srikanth Srinivasan</title>
    <summary>An Algebraic Formula for a polynomial $P\in F[x_1,\ldots,x_N]$ is an algebraic expression for $P(x_1,\ldots,x_N)$ using variables, field constants, additions and multiplications.  Such formulas capture an algebraic analog of the Boolean complexity class $\mathrm{NC}^1.$ Proving lower bounds against this model is thus an important problem.


It is known that, to prove superpolynomial lower bounds against algebraic formulas, it suffices to prove good enough lower bounds against restricted kinds of formulas known as Set-Multilinear formulas, for computing a polynomial $P(x_1,...,x_N)$ of degree $O(\log N/\log \log N)$. In the past, many superpolynomial lower bounds were found, but they are of the form $\Omega(f(d) poly(N))$ (where $f$ is typically a subexponential function) which is insufficient to get lower bounds for general formulas. Recently, the authors proved the first non-FPT lower bounds, i.e., a lower bound of the form $N^{\Omega{(f(d)})}$, against small-depth set-multilinear formulas (and also for circuits). In this work, we extend this result in two directions. 

1) Large-depth set-multilinear formulas. In the setting of general set-multilinear formulas, we prove a lower bound of $(\log n)^{\Omega(\log d)}$ for computing the Iterated Matrix Multiplication polynomial $IMM_{n,d}.$ In particular, this implies the first superpolynomial lower bound against unbounded-depth set-multilinear formulas computing $IMM_{n,n}.$ 

As a corollary, this also resolves the homogeneous version of a question of Nisan (STOC 1991) regarding the relative power of Algebraic formulas and Branching programs in the non-commutative setting.

2) Stronger bounds for homogeneous non-commutative small-depth circuits. In the small-depth homogeneous non-commutative case, we prove  a lower bound of $n^{d^{1/\Delta}/2^{O(\Delta)}}$, which yields non-FPT bounds for depths up to $o(\sqrt{\log d}).$ In comparison, the previous bound works in the harder commutative set-multilinear setting, but only up to depths $o(\log \log d)$. 
Moreover, our lower bound holds for all values of $d$, as opposed to the previous set-multilinear lower bound, which holds as long as $d$ is small, i.e., $d = O(\log n)$.</summary>
    <updated>2021-07-06T02:24:48Z</updated>
    <published>2021-07-06T02:24:48Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-16T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5554</id>
    <link href="https://www.scottaaronson.com/blog/?p=5554" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5554#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5554" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Open thread on new quantum supremacy claims</title>
    <summary xml:lang="en-US">Happy 4th to those in the US! The group of Chaoyang Lu and Jianwei Pan, based at USTC in China, has been on a serious quantum supremacy tear lately. Recall that last December, USTC announced the achievement of quantum supremacy via Gaussian BosonSampling, with 50-70 detected photonsâthe second claim of sampling-based quantum supremacy, after Googleâs [â¦]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Happy 4th to those in the US!</p>



<p>The group of Chaoyang Lu and Jianwei Pan, based at USTC in China, has been on a <em>serious</em> quantum supremacy tear lately.  Recall that last December, USTC announced the achievement of quantum supremacy via Gaussian BosonSampling, with 50-70 detected photonsâthe second claim of sampling-based quantum supremacy, after Googleâs in Fall 2019.  However, skeptics then poked holes in the USTC claim, showing how they could spoof the results with a classical computer, basically by reproducing the k-photon correlations for relatively small values of k.  Debate over the details continues, but the Chinese group seeks to render the debate largely moot with a <a href="https://arxiv.org/abs/2106.15534">new and better Gaussian BosonSampling experiment</a>, with 144 modes and up to 113 detected photons.  They say they were able to measure k-photon correlations for k up to about 19, which if true would constitute a serious obstacle to the classical simulation strategies that people discussed for the previous experiment.</p>



<p>In the meantime, though, an overlapping group of authors had <a href="https://arxiv.org/abs/2106.14734">put out another paper the day before</a> (!) reporting a sampling-based quantum supremacy experiment using superconducting qubitsâextremely similar to what Google did (the same circuit depth and everything), except now with 56 qubits rather than 53.</p>



<p>I confess that I havenât yet studied either paper in detailâamong other reasons, because Iâm on vacation with my family at the beach, and because Iâm trying to spend what work-time I have on my own projects.  But anyone who <em>has</em> read them, please use the comments of this post to discuss!  Hopefully Iâll learn something.</p>



<p>To confine myself to some general comments: since Googleâs announcement in Fall 2019, Iâve consistently said that sampling-based quantum supremacy is <em>not</em> yet a done deal.  Iâve said that quantum supremacy seems important enough to want independent replications, and demonstrations in other hardware platforms like ion traps and photonics, and better gate fidelity, and better classical hardness, and better verification protocols.  Most of all, Iâve said that we needed a genuine dialogue between the âquantum supremacistsâ and the classical skeptics: the former doing experiments and releasing all their data, the latter trying to design efficient classical simulations for those experiments, and so on in an iterative process.  Just like in applied cryptography, weâd only have real confidence in a quantum supremacy claim once it had survived at least a few years of attacks by skeptics.  So Iâm delighted that this is precisely whatâs now happening.  USTCâs papers are two new volleys in this back-and-forth; we all eagerly await the next volley, whichever side it comes from.</p>



<p>While Iâve been trying for years to move away from the expectation that I blog about each and every QC announcement that someone messages me about, maybe Iâll also say a word about the recent announcement by IBM of a quantum advantage in space complexity (see <a href="https://www.zdnet.com/article/ibm-researchers-demonstrate-the-advantage-that-quantum-computers-have-over-classical-computers/?fbclid=IwAR0KHVoI8W83Kwmeq9XwmuLHknlKeHjxWcvIjrqjH0QUtLZ8kaIWi6z42yk">here</a> for popular article and <a href="https://arxiv.org/abs/2008.06478">here</a> for arXiv preprint).  There appears to be a nice theoretical result here, about the ability to evaluate any symmetric Boolean function with a single qubit in a branching-program-like model.  Iâd love to understand that result better.  But to answer the question I received, this is another case where, once you know the protocol, you know both that the experiment can be done <em>and</em> exactly what its result will be (namely, the thing predicted by QM).  So I think the interest is almost entirely in the protocol itself.</p></div>
    </content>
    <updated>2021-07-04T22:34:01Z</updated>
    <published>2021-07-04T22:34:01Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-07-04T22:34:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/093</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/093" rel="alternate" type="text/html"/>
    <title>TR21-093 |  Bounded Indistinguishability for Simple Sources | 

	Yuval Filmus, 

	Andrej Bogdanov, 

	Yuval Ishai, 

	Akshayaram Srinivasan, 

	Krishnamoorthy Dinesh, 

	Avi Kaplan</title>
    <summary>A pair of sources $\mathbf{X},\mathbf{Y}$ over $\{0,1\}^n$ are $k$-indistinguishable if their projections to any $k$ coordinates are identically distributed. Can some $\mathit{AC^0}$ function distinguish between two such sources when $k$ is big, say $k=n^{0.1}$? Braverman's theorem (Commun. ACM 2011) implies a negative answer when $\mathbf{X}$ is uniform, whereas Bogdanov et al. (Crypto 2016) observe that this is not the case in general.

We initiate a systematic study of this question for natural classes of low-complexity sources, including ones that arise in cryptographic applications, obtaining positive results, negative results, and barriers. In particular: 

â There exist $\Omega(\sqrt{n})$-indistinguishable $\mathbf{X},\mathbf{Y}$, samplable by degree $O(\log n)$ polynomial maps (over $\mathbb{F}_2$) and by $\mathit{poly}(n)$-size decision trees, that are $\Omega(1)$-distinguishable by OR.

â There exists a function $f$ such that all $f(d, \epsilon)$-indistinguishable $\mathbf{X},\mathbf{Y}$ that are samplable by degree-$d$ polynomial maps are $\epsilon$-indistinguishable by OR for all sufficiently large $n$.  Moreover, $f(1, \epsilon) = \lceil\log(1/\epsilon)\rceil + 1$ and $f(2, \epsilon) = O(\log^{10}(1/\epsilon))$. 

â Extending (weaker versions of) the above negative results to $\mathit{AC^0}$ distinguishers would require  settling a conjecture of Servedio and Viola (ECCC 2012).
Concretely, if every pair of $n^{0.9}$-indistinguishable $\mathbf{X},\mathbf{Y}$ that are samplable by linear maps is $\epsilon$-indistinguishable by $\mathit{AC^0}$ circuits, then the binary inner product function can have at most an $\epsilon$-correlation with $\mathit{AC^0}\circ\oplus$ circuits.</summary>
    <updated>2021-07-04T02:37:53Z</updated>
    <published>2021-07-04T02:37:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-16T00:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/</id>
    <link href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/" rel="alternate" type="text/html"/>
    <title>ADFOCS 2021: Convex Optimization and Graph Algorithms</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 26 â August 13, 2021 Online https://conferences.mpi-inf.mpg.de/adfocs-22/ ADFOCS is an international summer school held annually at the Max Planck Institute for Informatics (MPII). The topic of this yearâs edition is Convex Optimization and its applications on Graph Algorithms. The event will take place online over the span of three weeks from July 26 to â¦ <a class="more-link" href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/">Continue reading <span class="screen-reader-text">ADFOCS 2021: Convex Optimization and GraphÂ Algorithms</span></a></div>
    </summary>
    <updated>2021-07-03T13:59:31Z</updated>
    <published>2021-07-03T13:59:31Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-07-16T00:21:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8161</id>
    <link href="https://windowsontheory.org/2021/07/02/itc-2021-call-for-participation-guest-post-by-benny-applebaum/" rel="alternate" type="text/html"/>
    <title>ITC 2021: Call for participation (guest post by Benny Applebaum)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The second edition of the recently created conference on Information-Theoretic Cryptography (ITC 2021) will take place virtually on July 24-26, 2021. The final program is out and contains exciting new works and invited talks that highlight the recent advances in the area by Benny Applebaum, Elaine Shi, Irit Dinur, Salman Avestimehr, Matthieu Bloch, and Mark â¦ <a class="more-link" href="https://windowsontheory.org/2021/07/02/itc-2021-call-for-participation-guest-post-by-benny-applebaum/">Continue reading <span class="screen-reader-text">ITC 2021: Call for participation (guest post by BennyÂ Applebaum)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The second edition of the recently created conference on <em>Information-Theoretic Cryptography (ITC 2021)</em> will take place virtually on July 24-26, 2021. The final program is out and contains exciting new works and invited talks that highlight the recent advances in the area by Benny Applebaum, Elaine Shi, Irit Dinur, Salman Avestimehr, Matthieu Bloch, and Mark Zhandry.</p>



<p>Registration to the conference is free (but required!)</p>



<p>Visit the webpage <a href="https://itcrypto.github.io/2021/index.html" rel="noreferrer noopener" target="_blank">https://itcrypto.github.io/2021/index.html</a> for more information and for the full program.Â Â </p>



<p>Hope to see you there!</p>



<p>â The Organising Committee</p></div>
    </content>
    <updated>2021-07-02T13:08:48Z</updated>
    <published>2021-07-02T13:08:48Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-07-16T00:21:15Z</updated>
    </source>
  </entry>
</feed>
