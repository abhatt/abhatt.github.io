<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-02-06T12:21:57Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1062</id>
    <link href="http://corner.mimuw.edu.pl/?p=1062" rel="alternate" type="text/html"/>
    <title>HALG 2019 - Call For Submissions of Short Contributed Presentations</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The HALG 2019 conference seeks submissions for contributed presentations. Each presentation is expected to consist of a poster and a short talk (an invitation to the poster). There will be no conference proceedings, hence presenting work already published at a … <a href="http://corner.mimuw.edu.pl/?p=1062">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The HALG 2019 conference seeks submissions for contributed presentations. Each presentation is expected to consist of a poster and a short talk (an invitation to the poster). There will be no conference proceedings, hence presenting work already published at a different venue or journal (or to be submitted there) is welcome.</p>
<p>If you would like to present your results at HALG 2019, please submit their details the abstract of the talk or the contribution of the poster via EasyChair: <a href="https://easychair.org/conferences/?conf=halg2019" rel="noopener noreferrer" target="_blank">https://easychair.org/conferences/?conf=halg2019</a></p>
<p>The abstract should include (when relevant) information where the results have been published/accepted (e.g., conference), and where they are publicly available (e.g., arXiv). All submissions will be reviewed by the program committee, giving priority to new work not formally published yet, and to papers published in 2018 or later.</p>
<p>Submissions deadline: March 15th, 2019.<br/>
Late submissions will be accepted subject to space constraints.</p></div>
    </content>
    <updated>2019-02-06T11:41:10Z</updated>
    <published>2019-02-06T11:41:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2019-02-06T12:21:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01832</id>
    <link href="http://arxiv.org/abs/1902.01832" rel="alternate" type="text/html"/>
    <title>Inferring the strength of social ties: a community-driven approach</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rozenshtein:Polina.html">Polina Rozenshtein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tatti:Nikolaj.html">Nikolaj Tatti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gionis:Aristides.html">Aristides Gionis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01832">PDF</a><br/><b>Abstract: </b>Online social networks are growing and becoming denser. The social
connections of a given person may have very high variability: from close
friends and relatives to acquaintances to people who hardly know. Inferring the
strength of social ties is an important ingredient for modeling the interaction
of users in a network and understanding their behavior. Furthermore, the
problem has applications in computational social science, viral marketing, and
people recommendation.
</p>
<p>In this paper we study the problem of inferring the strength of social ties
in a given network. Our work is motivated by a recent approach [27], which
leverages the strong triadic closure (STC) principle, a hypothesis rooted in
social psychology [13]. To guide our inference process, in addition to the
network structure, we also consider as input a collection of tight communities.
Those are sets of vertices that we expect to be connected via strong ties. Such
communities appear in different situations, e.g., when being part of a
community implies a strong connection to one of the existing members.
</p>
<p>We consider two related problem formalizations that reflect the assumptions
of our setting: small number of STC violations and strong-tie connectivity in
the input communities. We show that both problem formulations are NP-hard. We
also show that one problem formulation is hard to approximate, while for the
second we develop an algorithm with approximation guarantee. We validate the
proposed method on real-world datasets by comparing with baselines that
optimize STC violations and community connectivity separately.
</p></div>
    </summary>
    <updated>2019-02-06T02:22:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01829</id>
    <link href="http://arxiv.org/abs/1902.01829" rel="alternate" type="text/html"/>
    <title>Hierarchical Matrix Operations on GPUs: Matrix-Vector Multiplication and Compression</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boukaram:Wajih_Halim.html">Wajih Halim Boukaram</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Turkiyyah:George.html">George Turkiyyah</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Keyes:David_E=.html">David E. Keyes</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01829">PDF</a><br/><b>Abstract: </b>Hierarchical matrices are space and time efficient representations of dense
matrices that exploit the low rank structure of matrix blocks at different
levels of granularity. The hierarchically low rank block partitioning produces
representations that can be stored and operated on in near-linear complexity
instead of the usual polynomial complexity of dense matrices. In this paper, we
present high performance implementations of matrix vector multiplication and
compression operations for the $\mathcal{H}^2$ variant of hierarchical matrices
on GPUs. This variant exploits, in addition to the hierarchical block
partitioning, hierarchical bases for the block representations and results in a
scheme that requires only $O(n)$ storage and $O(n)$ complexity for the mat-vec
and compression kernels. These two operations are at the core of algebraic
operations for hierarchical matrices, the mat-vec being a ubiquitous operation
in numerical algorithms while compression/recompression represents a key
building block for other algebraic operations, which require periodic
recompression during execution. The difficulties in developing efficient GPU
algorithms come primarily from the irregular tree data structures that underlie
the hierarchical representations, and the key to performance is to recast the
computations on flattened trees in ways that allow batched linear algebra
operations to be performed. This requires marshaling the irregularly laid out
data in a way that allows them to be used by the batched routines. Marshaling
operations only involve pointer arithmetic with no data movement and as a
result have minimal overhead. Our numerical results on covariance matrices from
2D and 3D problems from spatial statistics show the high efficiency our
routines achieve---over 550GB/s for the bandwidth-limited mat-vec and over
850GFLOPS/s in sustained performance for the compression on the P100 Pascal
GPU.
</p></div>
    </summary>
    <updated>2019-02-06T02:27:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01814</id>
    <link href="http://arxiv.org/abs/1902.01814" rel="alternate" type="text/html"/>
    <title>A non-iterative method for robustly computing the intersections between a line and a curve or surface</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xiao:Xiao.html">Xiao Xiao</a>, Laurent Buse, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cirak:Fehmi.html">Fehmi Cirak</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01814">PDF</a><br/><b>Abstract: </b>The need to compute the intersections between a line and a high-order curve
or surface arises in a large number of finite element applications. Such
intersection problems are easy to formulate but hard to solve robustly. We
introduce a non-iterative method for computing intersections by solving a
matrix singular value decomposition (SVD) and an eigenvalue problem. That is,
all intersection points and their parametric coordinates are determined in
one-shot using only standard linear algebra techniques available in most
software libraries. As a result, the introduced technique is far more robust
than the widely used Newton-Raphson iteration or its variants. The maximum size
of the considered matrices depends on the polynomial degree $q$ of the shape
functions and is $2q \times 3q$ for curves and $6 q^2 \times 8 q^2$ for
surfaces. The method has its origin in algebraic geometry and has here been
considerably simplified with a view to widely used high-order finite elements.
In addition, the method is derived from a purely linear algebra perspective
without resorting to algebraic geometry terminology. A complete implementation
is available from <a href="http://bitbucket.org/nitro-project/.">this http URL</a>
</p></div>
    </summary>
    <updated>2019-02-06T02:33:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01765</id>
    <link href="http://arxiv.org/abs/1902.01765" rel="alternate" type="text/html"/>
    <title>The Hardest Halfspace</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sherstov:Alexander_A=.html">Alexander A. Sherstov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01765">PDF</a><br/><b>Abstract: </b>We study the approximation of halfspaces $h:\{0,1\}^n\to\{0,1\}$ in the
infinity norm by polynomials and rational functions of any given degree. Our
main result is an explicit construction of the "hardest" halfspace, for which
we prove polynomial and rational approximation lower bounds that match the
trivial upper bounds achievable for all halfspaces. This completes a lengthy
line of work started by Myhill and Kautz (1961).
</p>
<p>As an application, we construct a communication problem that achieves
essentially the largest possible separation, of $O(n)$ versus $2^{-\Omega(n)},$
between the sign-rank and discrepancy. Equivalently, our problem exhibits a gap
of $\log n$ versus $\Omega(n)$ between the communication complexity with
unbounded versus weakly unbounded error, improving quadratically on previous
constructions and completing a line of work started by Babai, Frankl, and Simon
(FOCS 1986). Our results further generalize to the $k$-party
number-on-the-forehead model, where we obtain an explicit separation of $\log
n$ versus $\Omega(n/4^{n})$ for communication with unbounded versus weakly
unbounded error. This gap is a quadratic improvement on previous work and
matches the state of the art for number-on-the-forehead lower bounds.
</p></div>
    </summary>
    <updated>2019-02-06T02:22:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01745</id>
    <link href="http://arxiv.org/abs/1902.01745" rel="alternate" type="text/html"/>
    <title>Hamiltonicity below Dirac's condition</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jansen:Bart_M=_P=.html">Bart M. P. Jansen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kozma:L=aacute=szl=oacute=.html">László Kozma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nederlof:Jesper.html">Jesper Nederlof</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01745">PDF</a><br/><b>Abstract: </b>Dirac's theorem (1952) is a classical result of graph theory, stating that an
$n$-vertex graph ($n \geq 3$) is Hamiltonian if every vertex has degree at
least $n/2$. Both the value $n/2$ and the requirement for every vertex to have
high degree are necessary for the theorem to hold.
</p>
<p>In this work we give efficient algorithms for determining Hamiltonicity when
either of the two conditions are relaxed. More precisely, we show that the
Hamiltonian cycle problem can be solved in time $c^k \cdot n^{O(1)}$, for some
fixed constant $c$, if at least $n-k$ vertices have degree at least $n/2$, or
if all vertices have degree at least $n/2-k$. The running time is, in both
cases, asymptotically optimal, under the exponential-time hypothesis (ETH).
</p>
<p>The results extend the range of tractability of the Hamiltonian cycle
problem, showing that it is fixed-parameter tractable when parameterized below
a natural bound. In addition, for the first parameterization we show that a
kernel with $O(k)$ vertices can be found in polynomial time.
</p></div>
    </summary>
    <updated>2019-02-06T02:32:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01732</id>
    <link href="http://arxiv.org/abs/1902.01732" rel="alternate" type="text/html"/>
    <title>Classifying Convex Bodies by their Contact and Intersection Graphs</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aamand:Anders.html">Anders Aamand</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abrahamsen:Mikkel.html">Mikkel Abrahamsen</a>, Jakob Bæk Tejs Knudsen, Peter Michael Reichstein Rasmussen <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01732">PDF</a><br/><b>Abstract: </b>Suppose that $A$ is a convex body in the plane and that $A_1,\dots,A_n$ are
translates of $A$. Such translates give rise to an intersection graph of $A$,
$G=(V,E)$, with vertices $V=\{1,\dots,n\}$ and edges $E=\{uv\mid A_u\cap
A_v\neq \emptyset\}$. The subgraph $G'=(V, E')$ satisfying that $E'\subset E$
is the set of edges $uv$ for which the interiors of $A_u$ and $A_v$ are
disjoint is a unit distance graph of $A$. If furthermore $G'=G$, i.e., if the
interiors of $A_u$ and $A_v$ are disjoint whenever $u\neq v$, then $G$ is a
contact graph of $A$.
</p>
<p>In this paper we study which pairs of convex bodies have the same contact,
unit distance, or intersection graphs. We say that two convex bodies $A$ and
$B$ are equivalent if there exists a linear transformation $B'$ of $B$ such
that for any slope, the longest line segments with that slope contained in $A$
and $B'$, respectively, are equally long. For a broad class of convex bodies,
including all strictly convex bodies and linear transformations of regular
polygons, we show that the contact graphs of $A$ and $B$ are the same if and
only if $A$ and $B$ are equivalent. We prove the same statement for unit
distance and intersection graphs.
</p></div>
    </summary>
    <updated>2019-02-06T02:33:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01727</id>
    <link href="http://arxiv.org/abs/1902.01727" rel="alternate" type="text/html"/>
    <title>Discovering bursts revisited: guaranteed optimization of the model parameters</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tatti:Nikolaj.html">Nikolaj Tatti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01727">PDF</a><br/><b>Abstract: </b>One of the classic data mining tasks is to discover bursts, time intervals,
where events occur at abnormally high rate. In this paper we revisit
Kleinberg's seminal work, where bursts are discovered by using exponential
distribution with a varying rate parameter: the regions where it is more
advantageous to set the rate higher are deemed bursty. The model depends on two
parameters, the initial rate and the change rate. The initial rate, that is,
the rate that is used when there are no burstiness was set to the average rate
over the whole sequence. The change rate is provided by the user.
</p>
<p>We argue that these choices are suboptimal: it leads to worse likelihood, and
may lead to missing some existing bursts. We propose an alternative problem
setting, where the model parameters are selected by optimizing the likelihood
of the model. While this tweak is trivial from the problem definition point of
view, this changes the optimization problem greatly. To solve the problem in
practice, we propose efficient ($1 + \epsilon$) approximation schemes. Finally,
we demonstrate empirically that with this setting we are able to discover
bursts that would have otherwise be undetected.
</p></div>
    </summary>
    <updated>2019-02-06T02:31:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01704</id>
    <link href="http://arxiv.org/abs/1902.01704" rel="alternate" type="text/html"/>
    <title>A Sequential Importance Sampling Algorithm for Estimating Linear Extensions</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beichl:Isabel.html">Isabel Beichl</a>, Alathea Jensen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sullivan:Francis.html">Francis Sullivan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01704">PDF</a><br/><b>Abstract: </b>In recent decades, a number of profound theorems concerning approximation of
hard counting problems have appeared. These include estimation of the
permanent, estimating the volume of a convex polyhedron, and counting
(approximately) the number of linear extensions of a partially ordered set. All
of these results have been achieved using probabilistic sampling methods,
specifically Monte Carlo Markov Chain (MCMC) techniques. In each case, a
rapidly mixing Markov chain is defined that is guaranteed to produce, with high
probability, an accurate result after only a polynomial number of operations.
</p>
<p>Although of polynomial complexity, none of these results lead to a practical
computational technique, nor do they claim to. The polynomials are of high
degree and a non-trivial amount of computing is required to get even a single
sample. Our aim in this paper is to present practical Monte Carlo methods for
one of these problems, counting linear extensions. Like related work on
estimating the coefficients of the reliability polynomial, our technique is
based on improving the so-called Knuth counting algorithm by incorporating an
importance function into the node selection technique giving a sequential
importance sampling (SIS) method. We define and report performance on two
importance functions.
</p></div>
    </summary>
    <updated>2019-02-06T02:31:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01701</id>
    <link href="http://arxiv.org/abs/1902.01701" rel="alternate" type="text/html"/>
    <title>Network Resilience Assessment via QoS Degradation Metrics: An Algorithmic Approach</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Lan_N=.html">Lan N. Nguyen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thai:My_T=.html">My T. Thai</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01701">PDF</a><br/><b>Abstract: </b>This paper focuses on network resilience to perturbation of edge weight.
Other than connectivity, many network applications nowadays rely upon some
measure of network distance between a pair of connected nodes. In these
systems, a metric related to network functionality is associated to each edge.
A pair of nodes only being functional if the weighted, shortest-path distance
between the pair is below a given threshold \texttt{T}. Consequently, a natural
question is on which degree the change of edge weights can damage the network
functionality? With this motivation, we study a new problem, \textit{Quality of
Service Degradation}: given a set of pairs, find a minimum budget to increase
the edge weights which ensures the distance between each pair exceeds
$\mathtt{T}$. We introduce four algorithms with theoretical performance
guarantees for this problem. Each of them has its own strength in trade-off
between effectiveness and running time, which are illustrated both in theory
and comprehensive experimental evaluation.
</p></div>
    </summary>
    <updated>2019-02-06T02:26:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01698</id>
    <link href="http://arxiv.org/abs/1902.01698" rel="alternate" type="text/html"/>
    <title>Stochastic Enumeration with Importance Sampling</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alathea Jensen <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01698">PDF</a><br/><b>Abstract: </b>Many hard problems in the computational sciences are equivalent to counting
the leaves of a decision tree, or, more generally, summing a cost function over
the nodes. These problems include calculating the permanent of a matrix,
finding the volume of a convex polyhedron, and counting the number of linear
extensions of a partially ordered set. Many approximation algorithms exist to
estimate such sums. One of the most recent is Stochastic Enumeration (SE),
introduced in 2013 by Rubinstein. In 2015, Vaisman and Kroese provided a
rigorous analysis of the variance of SE, and showed that SE can be extended to
a fully polynomial randomized approximation scheme for certain cost functions
on random trees. We present an algorithm that incorporates an importance
function into SE, and provide theoretical analysis of its efficacy. We also
present the results of numerical experiments to measure the variance of an
application of the algorithm to the problem of counting linear extensions of a
poset, and show that introducing importance sampling results in a significant
reduction of variance as compared to the original version of SE.
</p></div>
    </summary>
    <updated>2019-02-06T02:27:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01691</id>
    <link href="http://arxiv.org/abs/1902.01691" rel="alternate" type="text/html"/>
    <title>Accuracy Evaluation of Overlapping and Multi-resolution Clustering Algorithms on Large Datasets</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lutov:Artem.html">Artem Lutov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khayati:Mourad.html">Mourad Khayati</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cudr=eacute==Mauroux:Philippe.html">Philippe Cudré-Mauroux</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01691">PDF</a><br/><b>Abstract: </b>Performance of clustering algorithms is evaluated with the help of accuracy
metrics. There is a great diversity of clustering algorithms, which are key
components of many data analysis and exploration systems. However, there exist
only few metrics for the accuracy measurement of overlapping and
multi-resolution clustering algorithms on large datasets. In this paper, we
first discuss existing metrics, how they satisfy a set of formal constraints,
and how they can be applied to specific cases. Then, we propose several
optimizations and extensions of these metrics. More specifically, we introduce
a new indexing technique to reduce both the runtime and the memory complexity
of the Mean F1 score evaluation. Our technique can be applied on large datasets
and it is faster on a single CPU than state-of-the-art implementations running
on high-performance servers. In addition, we propose several extensions of the
discussed metrics to improve their effectiveness and satisfaction to formal
constraints without affecting their efficiency. All the metrics discussed in
this paper are implemented in C++ and are available for free as open-source
packages that can be used either as stand-alone tools or as part of a
benchmarking system to compare various clustering algorithms.
</p></div>
    </summary>
    <updated>2019-02-06T02:30:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01668</id>
    <link href="http://arxiv.org/abs/1902.01668" rel="alternate" type="text/html"/>
    <title>Expressive Power of Oblivious Consensus Protocols</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blondin:Michael.html">Michael Blondin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esparza:Javier.html">Javier Esparza</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaax:Stefan.html">Stefan Jaax</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01668">PDF</a><br/><b>Abstract: </b>Population protocols are a formal model of computation by identical,
anonymous mobile agents interacting in pairs. It has been shown that their
computational power is rather limited: They can only compute the predicates
expressible in Presburger arithmetic. Population protocols are oblivious, in
the sense that their behavior only depends on the number of agents in each
state of the current configuration, and nothing else. Obliviousness has
advantages for applications where agents want to reveal as little as possible
about their trajectories in a computation. We investigate the computational
power of oblivious protocols. We first show that, under a weak assumption,
oblivious protocols can only compute number predicates $\varphi : \mathbb{N}^m
\rightarrow \{0, 1\}$ in NSPACE(n) (with the input written, as usual, in
binary), while all predicates computed by population protocols are in
DSPACE(log n), thus proving an exponential gap. Then we introduce broadcast
consensus protocols, in which agents can also broadcast signals to all other
agents. We prove that they compute all predicates in NSPACE(n), reaching the
theoretical limit for oblivious protocols. Finally, we conduct the first
systematic comparison of different models introduced in the literature
(population protocols, broadcast protocols, community protocols, and mediated
protocols) with respect to their computational power and their privacy
guarantees.
</p></div>
    </summary>
    <updated>2019-02-06T02:20:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01635</id>
    <link href="http://arxiv.org/abs/1902.01635" rel="alternate" type="text/html"/>
    <title>Randomized Riemannian Preconditioning for Quadratically Constrained Problems</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shustin:Boris.html">Boris Shustin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avron:Haim.html">Haim Avron</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01635">PDF</a><br/><b>Abstract: </b>Optimization problem with quadratic equality constraints are prevalent in
machine learning. Indeed, two important examples are Canonical Correlation
Analysis (CCA) and Linear Discriminant Analysis (LDA). Unfortunately, methods
for solving such problems typically involve computing matrix inverses and
decomposition. For the aforementioned problems, these matrices are actually
Gram matrices of input data matrices, and as such the computations are too
expensive for large scale datasets. In this paper, we propose a sketching based
approach for solving CCA and LDA that reduces the cost dependence on the input
size. The proposed algorithms feature randomized preconditioning combined with
Riemannian optimization.
</p></div>
    </summary>
    <updated>2019-02-06T02:22:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01609</id>
    <link href="http://arxiv.org/abs/1902.01609" rel="alternate" type="text/html"/>
    <title>An Optimal Algorithm for Online Freeze-tag</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Josh Brunner, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wellman:Julian.html">Julian Wellman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01609">PDF</a><br/><b>Abstract: </b>In the freeze-tag problem, one active robot must wake up many frozen robots.
The robots are considered as points in a metric space, where active robots move
at a constant rate and activate other robots by visiting them. In the
(time-dependent) online variant of the problem, frozen robots are not revealed
until a specified time. Hammar, Nilsson, and Persson have shown that no online
algorithm can achieve a competitive ratio better than $7/3$ for online
freeze-tag, and asked whether there is any $O(1)$-competitive algorithm. In
this paper, we provide a $(1+\sqrt{2})$-competitive algorithm for online
time-dependent freeze-tag, and show that no algorithm can achieve a lower
competitive ratio on every metric space.
</p></div>
    </summary>
    <updated>2019-02-06T02:32:12Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01543</id>
    <link href="http://arxiv.org/abs/1902.01543" rel="alternate" type="text/html"/>
    <title>Window-based Streaming Graph Partitioning Algorithm</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Md Anwarul kaium Patwary, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Saurabh.html">Saurabh Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kang:Byeong.html">Byeong Kang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01543">PDF</a><br/><b>Abstract: </b>In the recent years, the scale of graph datasets has increased to such a
degree that a single machine is not capable of efficiently processing large
graphs. Thereby, efficient graph partitioning is necessary for those large
graph applications. Traditional graph partitioning generally loads the whole
graph data into the memory before performing partitioning; this is not only a
time consuming task but it also creates memory bottlenecks. These issues of
memory limitation and enormous time complexity can be resolved using
stream-based graph partitioning. A streaming graph partitioning algorithm reads
vertices once and assigns that vertex to a partition accordingly. This is also
called an one-pass algorithm. This paper proposes an efficient window-based
streaming graph partitioning algorithm called WStream. The WStream algorithm is
an edge-cut partitioning algorithm, which distributes a vertex among the
partitions. Our results suggest that the WStream algorithm is able to partition
large graph data efficiently while keeping the load balanced across different
partitions, and communication to a minimum. Evaluation results with real
workloads also prove the effectiveness of our proposed algorithm, and it
achieves a significant reduction in load imbalance and edge-cut with different
ranges of dataset.
</p></div>
    </summary>
    <updated>2019-02-06T02:31:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01499</id>
    <link href="http://arxiv.org/abs/1902.01499" rel="alternate" type="text/html"/>
    <title>Differentially Private Release of High-Dimensional Datasets using the Gaussian Copula</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Asghar:Hassan_Jameel.html">Hassan Jameel Asghar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Ming.html">Ming Ding</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rakotoarivelo:Thierry.html">Thierry Rakotoarivelo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mrabet:Sirine.html">Sirine Mrabet</a>, Mohamed Ali Kaafar <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01499">PDF</a><br/><b>Abstract: </b>We propose a generic mechanism to efficiently release differentially private
synthetic versions of high-dimensional datasets with high utility. The core
technique in our mechanism is the use of copulas. Specifically, we use the
Gaussian copula to define dependencies of attributes in the input dataset,
whose rows are modelled as samples from an unknown multivariate distribution,
and then sample synthetic records through this copula. Despite the inherently
numerical nature of Gaussian correlations we construct a method that is
applicable to both numerical and categorical attributes alike. Our mechanism is
efficient in that it only takes time proportional to the square of the number
of attributes in the dataset. We propose a differentially private way of
constructing the Gaussian copula without compromising computational efficiency.
Through experiments on three real-world datasets, we show that we can obtain
highly accurate answers to the set of all one-way marginal, and two-and
three-way positive conjunction queries, with 99\% of the query answers having
absolute (fractional) error rates between 0.01 to 3\%. Furthermore, for a
majority of two-way and three-way queries, we outperform independent noise
addition through the well-known Laplace mechanism. In terms of computational
time we demonstrate that our mechanism can output synthetic datasets in around
6 minutes 47 seconds on average with an input dataset of about 200 binary
attributes and more than 32,000 rows, and about 2 hours 30 mins to execute a
much larger dataset of about 700 binary attributes and more than 5 million
rows. To further demonstrate scalability, we ran the mechanism on larger
(artificial) datasets with 1,000 and 2,000 binary attributes (and 5 million
rows) obtaining synthetic outputs in approximately 6 and 19 hours,
respectively.
</p></div>
    </summary>
    <updated>2019-02-06T02:28:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01483</id>
    <link href="http://arxiv.org/abs/1902.01483" rel="alternate" type="text/html"/>
    <title>Discovering Nested Communities</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tatti:Nikolaj.html">Nikolaj Tatti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gionis:Aristides.html">Aristides Gionis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01483">PDF</a><br/><b>Abstract: </b>Finding communities in graphs is one of the most well-studied problems in
data mining and social-network analysis. In many real applications, the
underlying graph does not have a clear community structure. In those cases,
selecting a single community turns out to be a fairly ill-posed problem, as the
optimization criterion has to make a difficult choice between selecting a tight
but small community or a more inclusive but sparser community.
</p>
<p>In order to avoid the problem of selecting only a single community we propose
discovering a sequence of nested communities. More formally, given a graph and
a starting set, our goal is to discover a sequence of communities all
containing the starting set, and each community forming a denser subgraph than
the next. Discovering an optimal sequence of communities is a complex
optimization problem, and hence we divide it into two subproblems: 1) discover
the optimal sequence for a fixed order of graph vertices, a subproblem that we
can solve efficiently, and 2) find a good order. We employ a simple heuristic
for discovering an order and we provide empirical and theoretical evidence that
our order is good.
</p></div>
    </summary>
    <updated>2019-02-06T02:24:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01477</id>
    <link href="http://arxiv.org/abs/1902.01477" rel="alternate" type="text/html"/>
    <title>Faster way to agony: Discovering hierarchies in directed graphs</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tatti:Nikolaj.html">Nikolaj Tatti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01477">PDF</a><br/><b>Abstract: </b>Many real-world phenomena exhibit strong hierarchical structure.
Consequently, in many real-world directed social networks vertices do not play
equal role. Instead, vertices form a hierarchy such that the edges appear
mainly from upper levels to lower levels. Discovering hierarchies from such
graphs is a challenging problem that has gained attention. Formally, given a
directed graph, we want to partition vertices into levels such that ideally
there are only edges from upper levels to lower levels. From computational
point of view, the ideal case is when the underlying directed graph is acyclic.
In such case, we can partition the vertices into a hierarchy such that there
are only edges from upper levels to lower edges. In practice, graphs are rarely
acyclic, hence we need to penalize the edges that violate the hierarchy. One
practical approach is agony, where each violating edge is penalized based on
the severity of the violation. The fastest algorithm for computing agony
requires $O(nm^2)$ time. In the paper we present an algorithm for computing
agony that has better theoretical bound, namely $O(m^2)$. We also show that in
practice the obtained bound is pessimistic and that we can use our algorithm to
compute agony for large datasets. Moreover, our algorithm can be used as
any-time algorithm.
</p></div>
    </summary>
    <updated>2019-02-06T02:32:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01461</id>
    <link href="http://arxiv.org/abs/1902.01461" rel="alternate" type="text/html"/>
    <title>(Near) Optimal Adaptivity Gaps for Stochastic Multi-Value Probing</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Domagoj Bradac, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zuzic:Goran.html">Goran Zuzic</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01461">PDF</a><br/><b>Abstract: </b>Consider a kidney-exchange application where we want to find a max-matching
in a random graph. To find whether an edge $e$ exists, we need to perform an
expensive test, in which case the edge $e$ appears independently with a
\emph{known} probability $p_e$. Given a budget on the total cost of the tests,
our goal is to find a testing strategy that maximizes the expected maximum
matching size.
</p>
<p>The above application is an example of the stochastic probing problem. In
general the optimal stochastic probing strategy is difficult to find because it
is \emph{adaptive}---decides on the next edge to probe based on the outcomes of
the probed edges. An alternate approach is to show the \emph{adaptivity gap} is
small, i.e., the best \emph{non-adaptive} strategy always has a value close to
the best adaptive strategy. This allows us to focus on designing non-adaptive
strategies that are much simpler. Previous works, however, have focused on
Bernoulli random variables that can only capture whether an edge appears or
not. In this work we introduce a multi-value stochastic probing problem, which
can also model situations where the weight of an edge has a probability
distribution over multiple values.
</p>
<p>Our main technical contribution is to obtain (near) optimal bounds for the
(worst-case) adaptivity gaps for multi-value stochastic probing over
prefix-closed constraints. For a monotone submodular function, we show the
adaptivity gap is at most $2$ and provide a matching lower bound. For a
weighted rank function of a $k$-extendible system (a generalization of
intersection of $k$ matroids), we show the adaptivity gap is between $O(k\log
k)$ and $k$. None of these results were known even in the Bernoulli case where
both our upper and lower bounds also apply, thereby resolving an open question
of Gupta et al.
</p></div>
    </summary>
    <updated>2019-02-06T02:26:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01454</id>
    <link href="http://arxiv.org/abs/1902.01454" rel="alternate" type="text/html"/>
    <title>External Labeling Techniques: A Taxonomy and Survey</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bekos:Michael_A=.html">Michael A. Bekos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niedermann:Benjamin.html">Benjamin Niedermann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/N=ouml=llenburg:Martin.html">Martin Nöllenburg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01454">PDF</a><br/><b>Abstract: </b>External labeling is frequently used for annotating features in graphical
displays and visualizations, such as technical illustrations, anatomical
drawings, or maps, with textual information. Such a labeling connects features
within an illustration by thin leader lines with their labels, which are placed
in the empty space surrounding the image. Over the last twenty years, a large
body of literature in diverse areas of computer science has been published that
investigated many different aspects, models, and algorithms for automatically
placing external labels for a given set of features. This state-of-the-art
report introduces a first unified taxonomy for categorizing the different
results in the literature and then presents a comprehensive survey of the state
of the art, a sketch of the most relevant algorithmic techniques for external
labeling algorithms, as well as a list of open research challenges in this
multidisciplinary research field.
</p></div>
    </summary>
    <updated>2019-02-06T02:33:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.00619</id>
    <link href="http://arxiv.org/abs/1902.00619" rel="alternate" type="text/html"/>
    <title>Parametric FEM for Shape Optimization applied to Golgi Stack</title>
    <feedworld_mtime>1549411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xinshi.html">Xinshi Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chung:Eric.html">Eric Chung</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.00619">PDF</a><br/><b>Abstract: </b>The thesis is about an application of the shape optimization to the
morphological evolution of Golgi stack. Golgi stack consists of multiple layers
of cisternae. It is an organelle in the biological cells. Inspired by the
Helfrich Model \cite{Helfrich}, which is a model for vesicles typically applied
to biological cells, a new model specially designed for Golgi stack is
developed and then implemented using FEM in this thesis.
</p>
<p>In the Golgi model, each cisternae of the Golgi stack is viewed as a closed
vesicle without topological changes, and our model is adaptable to both
single-vesicle case and multiple-vesicle case. The main idea of the math model
is to minimize the elastic energy(bending energy) of the vesicles, with some
constraints designed regarding the biological properties of Golgi stack. With
these constraints attached to the math model, we could extend this model to an
obstacle-type problem. Hence, in the thesis, not only the simulations of Golgi
stack are shown, but some interesting examples without biological meanings are
also demonstrated. Also, as multiple cisternaes are considered as a whole, this
is also a model handling multiple objects.
</p>
<p>A set of numerical examples is shown to compare with the observed shape of
Golgi stack, so we can lay down some possible explanations to the morphological
performance of trans-Golgi cisternae.
</p></div>
    </summary>
    <updated>2019-02-06T02:33:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-02-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=92</id>
    <link href="https://gilkalai.wordpress.com/2019/02/05/extremal-combinatorics-v-posets/" rel="alternate" type="text/html"/>
    <title>Extremal Combinatorics V: POSETS</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is the remaining post V on partially ordered sets of my series on extremal combinatorics (I,II,III,IV,VI).  We will talk here about POSETS – partially ordered sets. The study of order is very important in many areas of mathematics starting … <a href="https://gilkalai.wordpress.com/2019/02/05/extremal-combinatorics-v-posets/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>This is the remaining post V on partially ordered sets of my series on extremal combinatorics (<a href="https://gilkalai.wordpress.com/2008/05/01/extremal-combinatorics-i/">I</a>,<a href="https://gilkalai.wordpress.com/2008/07/17/extermal-combinatorics-ii-some-geometry-and-number-theory/">II</a>,<a href="https://gilkalai.wordpress.com/2008/09/28/extremal-combinatorics-iii-some-basic-theorems/">III</a>,<a href="https://gilkalai.wordpress.com/2008/10/06/extremal-combinatorics-iv-shifting/">IV</a>,<a href="https://gilkalai.wordpress.com/2009/05/21/extremal-combinatorics-vi-the-frankl-wilson-theorem/">VI</a>). </em></p>
<p>We will talk here about POSETS – partially ordered sets. The study of order is very important in many areas of mathematics starting with the order relation on the integers and reals in algebra and in Euclidean geometry. The set of all subsets of a set can be partially ordered by inclusion and this is a very basic example of posets. While the study of order and posets is a separate area on its own, parts of it are very important in extremal combinatorics and we will give a little taste here.</p>
<p style="text-align: center;"><span style="color: #0000ff;"><strong>Dear readers, please contribute your favorite result or problem on partially ordered sets (or Sorting) in the comment session.</strong></span></p>
<p>A chain <img alt="C \subset P" class="latex" src="https://s0.wp.com/latex.php?latex=C+%5Csubset+P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C \subset P"/> in a POSET is a set of elements so that every two of them are comparable. An antichain $A \subset P$ is a set of elements so that every two distinct elemenאs in <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> are incomparable.  (Antichains are also called independent sets.) An immediate but important Lemma is:</p>
<p><strong>The immediate lemma:</strong> The intersection of a chain <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> and an antichain <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> contains at most one element. <strong><span style="color: #993366;">Walla!</span></strong></p>
<h3>Dilworth’s theorem</h3>
<p>Dilworth’s theorem (DT): Every finite partially ordered <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> set can be covered by <img alt="a(P)" class="latex" src="https://s0.wp.com/latex.php?latex=a%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a(P)"/> chains.</p>
<p>(By the immediate lemma, at least <img alt="a(P)" class="latex" src="https://s0.wp.com/latex.php?latex=a%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a(P)"/> chains are needed.)</p>
<p>Dual Dilworth theorem: Every partially ordedrd sets can be cover by <img alt="c(P)" class="latex" src="https://s0.wp.com/latex.php?latex=c%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c(P)"/> untichains.</p>
<p>(By the immediate lemma, at least <img alt="c(P)" class="latex" src="https://s0.wp.com/latex.php?latex=c%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c(P)"/> antichains are needed.)</p>
<p>The proof of the dual Dilworth theorem is easy. Note that the set <img alt="A_1=MIN(P)" class="latex" src="https://s0.wp.com/latex.php?latex=A_1%3DMIN%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A_1=MIN(P)"/> of minimal elements of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> is an antichain. Let <img alt="A_k = MIN (P\backslash (A_1 \cup A_2 \cup \dots A_{k-1}))" class="latex" src="https://s0.wp.com/latex.php?latex=A_k+%3D+MIN+%28P%5Cbackslash+%28A_1+%5Ccup+A_2+%5Ccup+%5Cdots+A_%7Bk-1%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A_k = MIN (P\backslash (A_1 \cup A_2 \cup \dots A_{k-1}))"/>. We need two easy observations. First, <img alt="A_k is an antichain" class="latex" src="https://s0.wp.com/latex.php?latex=A_k+is+an+antichain&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A_k is an antichain"/> and second: If <img alt="A_k \ne \emptyset" class="latex" src="https://s0.wp.com/latex.php?latex=A_k+%5Cne+%5Cemptyset&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A_k \ne \emptyset"/> then there is a chain with one element from <img alt="A_i: 1 \le i\le k" class="latex" src="https://s0.wp.com/latex.php?latex=A_i%3A+1+%5Cle+i%5Cle+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A_i: 1 \le i\le k"/>. <strong><span style="color: #0000ff;">Walla!</span></strong></p>
<p>The proof of Dilworth theorem is by induction on $|P|$. For the induction step you first consider the case where every antichain of maximal size is either <img alt="MAX(P)" class="latex" src="https://s0.wp.com/latex.php?latex=MAX%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="MAX(P)"/> or <img alt="MIN (P)" class="latex" src="https://s0.wp.com/latex.php?latex=MIN+%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="MIN (P)"/>. In this case you consider a chain with one element in <img alt="MAX(P)" class="latex" src="https://s0.wp.com/latex.php?latex=MAX%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="MAX(P)"/> and one element in <img alt="MIN (P)" class="latex" src="https://s0.wp.com/latex.php?latex=MIN+%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="MIN (P)"/> and delete these elements from <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/>. For the resulting post <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/>, <img alt="a(Q)=a(P)-1" class="latex" src="https://s0.wp.com/latex.php?latex=a%28Q%29%3Da%28P%29-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a(Q)=a(P)-1"/> and we can use the induction hypothesis.</p>
<p>Otherwise there is an antichain <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> of maximum size <img alt="t=a(P)" class="latex" src="https://s0.wp.com/latex.php?latex=t%3Da%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t=a(P)"/> which is not <em>MAX(P)</em> or <em>MIN(P)</em>.  Put <img alt="A=\{a_1,a_2,\dots,a_t\}" class="latex" src="https://s0.wp.com/latex.php?latex=A%3D%5C%7Ba_1%2Ca_2%2C%5Cdots%2Ca_t%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A=\{a_1,a_2,\dots,a_t\}"/>. Let <img alt="P^+" class="latex" src="https://s0.wp.com/latex.php?latex=P%5E%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P^+"/> be the set of elements in <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> which are larger or equal some element in <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/>, and let <img alt="P^-" class="latex" src="https://s0.wp.com/latex.php?latex=P%5E-&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P^-"/> be the set of elements in <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> which are smaller or equal some element in <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/>.</p>
<p>Now,</p>
<ol>
<li><img alt="P^+ \cup P^-=P" class="latex" src="https://s0.wp.com/latex.php?latex=P%5E%2B+%5Ccup+P%5E-%3DP&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P^+ \cup P^-=P"/>. Otherwise we could add an element to <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> to form a larger antichain.</li>
<li><img alt="P^+ \cap P^- = A" class="latex" src="https://s0.wp.com/latex.php?latex=P%5E%2B+%5Ccap+P%5E-+%3D+A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P^+ \cap P^- = A"/>. Otherwise, there will be two elements of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> which are comparable.</li>
</ol>
<p>So by the induction hypothesis <img alt="P^+" class="latex" src="https://s0.wp.com/latex.php?latex=P%5E%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P^+"/> can be covered by <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> chains <img alt="C_1^+, C_2^+, \dots, C_t^+" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%5E%2B%2C+C_2%5E%2B%2C+%5Cdots%2C+C_t%5E%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1^+, C_2^+, \dots, C_t^+"/> and <img alt="P^-" class="latex" src="https://s0.wp.com/latex.php?latex=P%5E-&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P^-"/> can be covered by <img alt="a(P" class="latex" src="https://s0.wp.com/latex.php?latex=a%28P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a(P"/>$ chains <img alt="C_1^-, C_2^-, \dots, C_t^-" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%5E-%2C+C_2%5E-%2C+%5Cdots%2C+C_t%5E-&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1^-, C_2^-, \dots, C_t^-"/>. Bu re-indexing we can assume that both <img alt="C_i^+" class="latex" src="https://s0.wp.com/latex.php?latex=C_i%5E%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_i^+"/> and <img alt="C_i^-" class="latex" src="https://s0.wp.com/latex.php?latex=C_i%5E-&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_i^-"/> contains <img alt="a_i" class="latex" src="https://s0.wp.com/latex.php?latex=a_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_i"/>. It follows that <img alt="a_i" class="latex" src="https://s0.wp.com/latex.php?latex=a_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_i"/> is the minimal element in $C_i^+$ and the maximal element in $C_i^-$ and hence <img alt="C_i=:C_i^+ \cup C_i^-" class="latex" src="https://s0.wp.com/latex.php?latex=C_i%3D%3AC_i%5E%2B+%5Ccup+C_i%5E-&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_i=:C_i^+ \cup C_i^-"/> is a chain. The <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> chains <img alt="C_1, C_2, \dots, C_t" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%2C+C_2%2C+%5Cdots%2C+C_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1, C_2, \dots, C_t"/> cover <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/>. <span style="color: #993366;"><strong>Sababa!</strong></span></p>
<p>An <strong>important Corollary</strong> both from Dilworth’s theorem and its dual is that</p>
<p style="text-align: center;"><img alt="a(P) c(P) \ge |P|." class="latex" src="https://s0.wp.com/latex.php?latex=a%28P%29+c%28P%29+%5Cge+%7CP%7C.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a(P) c(P) \ge |P|."/></p>
<h3>Erdos-Szekeres theorem</h3>
<p>The fundamental Erdos Szekeres theorem asserts that if <img alt="n=ab+1" class="latex" src="https://s0.wp.com/latex.php?latex=n%3Dab%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n=ab+1"/> then every sequence <img alt="a_1,a_2,\dots ,a_n" class="latex" src="https://s0.wp.com/latex.php?latex=a_1%2Ca_2%2C%5Cdots+%2Ca_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_1,a_2,\dots ,a_n"/> of different real numbers contains a monotone increasing sequence of length <img alt="a+1" class="latex" src="https://s0.wp.com/latex.php?latex=a%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a+1"/> or a monotone decreasing sequence of length <img alt="b+1" class="latex" src="https://s0.wp.com/latex.php?latex=b%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b+1"/>.</p>
<p>There are simple proofs. For example, associate to every <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> a pair <img alt="(I_k,D_k)" class="latex" src="https://s0.wp.com/latex.php?latex=%28I_k%2CD_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(I_k,D_k)"/> of integers where  <img alt="I_k" class="latex" src="https://s0.wp.com/latex.php?latex=I_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="I_k"/> is the maximum length of the increasing subsequence starting with <img alt="a_k" class="latex" src="https://s0.wp.com/latex.php?latex=a_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_k"/> and <img alt="D_k" class="latex" src="https://s0.wp.com/latex.php?latex=D_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_k"/> is the maximum length of the deccreasing subsequence starting with <img alt="a_k" class="latex" src="https://s0.wp.com/latex.php?latex=a_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_k"/>. The result follows from the easy observation that all these pairs are different.</p>
<p>Both Dilworth’ theorem and its easy dual implies easily (in fact we need only the important corollary) the Erdos Szekeres theorem when we define the following partial order: <img alt="i &lt; k" class="latex" src="https://s0.wp.com/latex.php?latex=i+%3C+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i &lt; k"/> if both <img alt="i&lt;k" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Ck&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i&lt;k"/> and <img alt="a_i &lt; a_k" class="latex" src="https://s0.wp.com/latex.php?latex=a_i+%3C+a_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_i &lt; a_k"/>.</p>
<h3>Looking at Sperner’s theorem again</h3>
<p>Sperner’s theorem asserts that the maximal size of an antichain of subsets of an <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> elements set is <img alt="{{n} \choose {[n/2]}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%5Bn%2F2%5D%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{n} \choose {[n/2]}}"/>. By Dilworth theorem it follows that we can cover all sets by <img alt="{{n} \choose {[n/2]}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7B%5Bn%2F2%5D%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{n} \choose {[n/2]}}"/> chains (and, of course when we exhibit such a covering it reproves Sperner theorem). A symmetric saturated chain decomposition is a partition of <img alt="P(n)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(n)"/> (=all subsets of <img alt="[n]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[n]"/>) to saturated chains where each chain has, for some <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>, sets of sizes $k,k+1,\dots,d-k$. You can build such a decomposition inductively.</p>
<p>Start with a decomposition for <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> for each chain <img alt="C_i" class="latex" src="https://s0.wp.com/latex.php?latex=C_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_i"/> create a new chain <img alt="C'_i" class="latex" src="https://s0.wp.com/latex.php?latex=C%27_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C'_i"/> by adding the element <img alt="n+1" class="latex" src="https://s0.wp.com/latex.php?latex=n%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n+1"/> to every set. And then move the top set in <img alt="C'_i" class="latex" src="https://s0.wp.com/latex.php?latex=C%27_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C'_i"/> to <img alt="C_i" class="latex" src="https://s0.wp.com/latex.php?latex=C_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_i"/>.  <strong>Walla!</strong></p>
<p>This is a beginning of a very beautiful story related also to the Dedekind Problem about  number of antichains in <img alt="P(n)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(n)"/>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/02/untitledgreene-kleitman2.png"><img alt="" class="alignnone size-full wp-image-16834" height="489" src="https://gilkalai.files.wordpress.com/2019/02/untitledgreene-kleitman2.png?w=640&amp;h=489" width="640"/></a></p>
<p><strong><span style="color: #ff0000;">Curtis Greene and Danny Kleitman</span></strong></p>
<h3>The Greene-Kleitman theorem</h3>
<p>Let <img alt="a_k(P)" class="latex" src="https://s0.wp.com/latex.php?latex=a_k%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_k(P)"/> be the maximum size of the union <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/> of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> antichains in a poset <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/>. For every chain For every chain <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> we have <img alt="|C \cap X| \le \min\{|C|,k\}" class="latex" src="https://s0.wp.com/latex.php?latex=%7CC+%5Ccap+X%7C+%5Cle+%5Cmin%5C%7B%7CC%7C%2Ck%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|C \cap X| \le \min\{|C|,k\}"/>. Therefore for a partition of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> to chains <img alt="C_1,C_2,\dots,C_t" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%2CC_2%2C%5Cdots%2CC_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1,C_2,\dots,C_t"/> we   have <img alt="\sum\min\{|C_i|,k\ge |X|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum%5Cmin%5C%7B%7CC_i%7C%2Ck%5Cge+%7CX%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum\min\{|C_i|,k\ge |X|"/>. The <a href="https://www.encyclopediaofmath.org/index.php/Greene-Kleitman_theorem">Greene-Kleitman theorem</a> asserts that there always is a decomposition into chains with <img alt="\sum\min\{|C_i|,k\}=a_k(P)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum%5Cmin%5C%7B%7CC_i%7C%2Ck%5C%7D%3Da_k%28P%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum\min\{|C_i|,k\}=a_k(P)"/>.</p>
<h3>The perfect graph theorem.</h3>
<p>What is the relation between the very easy dual Dilworth theorem and the harder Dilworth theorem? As it turns out there is a very general theorem, Lovasz’ perfect graph theorem, that show that,  these two theorems are equivalent.</p>
<p>A graph G is perfect if for every induced subgraph H, the chromatic number equals the clique number. Lovasz’ theorem  (conjectured by Claude Berge) asserts that complements of perfect graphs are perfect. The perfectness of the comparability graph of a poset amounts to the dual Dilworth theorem, and for its complement it is the Dilworth theorem. Lovasz in fact proved that perfectness is equivalent to the relation $\latex \omega(H)\cdot \alpha (H) \ge |H|$ for every induced subgraph H. (For posets this is our important corollary above.)</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/02/kahn-linial-saks.png"><img alt="" class="alignnone size-full wp-image-16832" height="239" src="https://gilkalai.files.wordpress.com/2019/02/kahn-linial-saks.png?w=640&amp;h=239" width="640"/></a></p>
<p><strong><span style="color: #ff0000;">Jeff Kahn and Jake Baron </span></strong><span style="color: #ff0000;">(</span><span style="color: #ff0000;"><a href="http://archive.dimacs.rutgers.edu/DIMACS_highlights/tuza/tuza.html">see here on their 2016 asymptotic solution to Tusza’s conjecture</a></span><span style="color: #ff0000;">),</span><strong><span style="color: #ff0000;"> Mike Saks, and Nati Linial</span></strong></p>
<h3>Startling theorems on POSETS: Kahn-Saks,  Linial-Saks, Linial-Kahn, and Kahn-Saks</h3>
<p>Here  some beautiful and important theorems on posets. An order ideas <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="I"/> of a post is a set of elements so that if <img alt="x in I" class="latex" src="https://s0.wp.com/latex.php?latex=x+in+I&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x in I"/> and <img alt="y &lt; x" class="latex" src="https://s0.wp.com/latex.php?latex=y+%3C+x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y &lt; x"/> then <img alt="y \in I" class="latex" src="https://s0.wp.com/latex.php?latex=y+%5Cin+I&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y \in I"/>.</p>
<p><strong>Theorem (Linial-Saks, 1985):</strong> In every poset <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> there is an element which is contained in more than δ and less than 1-δ order ideas of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/>.  (<a href="http://www.cs.huji.ac.il/~nati/PAPERS/central_element.pdf">Paper</a>)</p>
<p><strong>Theorem (Kahn-Saks, 1984):</strong> For every Poset <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> which is not a chain there are two incomparable elements <img alt="x,y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y"/> such that the number of linear extensions of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> for which <img alt="x&lt;y" class="latex" src="https://s0.wp.com/latex.php?latex=x%3Cy&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x&lt;y"/> is between 3/11 and 8/11. (<a href="https://link.springer.com/article/10.1007/BF00565647">Paper</a>)</p>
<p>A <a href="http://www.cs.huji.ac.il/~nati/PAPERS/brunn_minkowski.pdf">simpler proof</a> was found in the late 80s by Kahn and Linial and by Karzanov and Khachiyan. It  is  based on the Brunn Minkowski theorem gives a weaker constant <img alt="1/2e" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1/2e"/> .</p>
<p><strong>Theorem (Kahn-Saks, 1987)</strong>: For every finite distributive lattice <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L"/> the maximum antichain is of size <img alt="o(|L|)" class="latex" src="https://s0.wp.com/latex.php?latex=o%28%7CL%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="o(|L|)"/>. (<a href="https://core.ac.uk/download/pdf/82629369.pdf">Paper</a>)</p>
<p>Lattices are special types of posets with the property that for every set of elements (pairs <img alt="\{x,y\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bx%2Cy%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{x,y\}"/> suffice in the finite case), there is a unique minimal elements above them all (denoted for pairs by <i>x</i> ∧ <i>y</i>) and a unique maximal element (denoted for pairs by <i>x</i> ∨ <i>y</i>) below them all.</p>
<p>A <a href="https://en.wikipedia.org/wiki/Distributive_lattice">distributive lattice</a> ia a lattice that satisfies for every <em>x, y</em> and <em>z</em>, the relation</p>
<p style="text-align: center;"><i>x</i> ∧ (<i>y</i> ∨ <i>z</i>) = (<i>x</i> ∧ <i>y</i>) ∨ (<i>x</i> ∧ <i>z</i>)</p>
<p>Birkhoff’s representation theorem asserts that finite distributive lattices can be represented as order ideals of posets (ordered by inclusion).</p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2019-02-05T11:18:54Z</updated>
    <published>2019-02-05T11:18:54Z</published>
    <category term="Combinatorics"/>
    <category term="Claude Berge"/>
    <category term="Curtis Greene"/>
    <category term="Daniel Kleitman"/>
    <category term="Dilworth's theorem"/>
    <category term="Extremal combinatorics"/>
    <category term="Jeff Kahn"/>
    <category term="Laci Lovasz"/>
    <category term="Mike Saks"/>
    <category term="Nati Linial"/>
    <category term="Posets"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-02-06T12:20:36Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-8542550481798271953</id>
    <link href="http://processalgebra.blogspot.com/feeds/8542550481798271953/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=8542550481798271953" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/8542550481798271953" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/8542550481798271953" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2019/02/two-awards-at-hicss19-for-csgssi.html" rel="alternate" type="text/html"/>
    <title>Two awards at HICSS’19 for CS@GSSI student Roberto Verdecchia</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div><div><a href="https://robertoverdecchia.github.io/" target="_blank">Roberto Verdecchia</a>, a third-year Ph.D. student of the Gran Sasso Science Institute (GSSI) and the Vrije Universiteit Amsterdam (VU) has received two distinct prizes at the 52nd Hawaii International Conference on System Sciences (HICSS’19;<span> </span><a href="http://hicss.hawaii.edu/" style="color: #1155cc;" target="_blank">http://hicss.hawaii.edu/</a>) for his research paper “<a href="https://robertoverdecchia.github.io/papers/HICSS_2019.pdf" style="color: #1155cc;" target="_blank">DecidArch: Playing Cards as Software Architects</a>”, which is <span style="font-family: Helvetica; font-size: 12px;">co-authored with</span><i style="font-family: Helvetica; font-size: 12px;"> </i>Patricia Lago, Jia F. Cai (both at VU Amsterdam), Remco C. de Boer (ArchiXL) and Philippe Kruchten (University of British Columbia). Out of over 780 papers presented at HICCS within 11 different research tracks, the study was presented with the “Best Paper award” of the Software Education and Training track. Additionally, the article was also selected as one of the five “ISSIP-IBM-CBA Student Paper Award for Best Industry Studies Paper” of HICCS’19.</div><div><br/>The study presents a novel educational game conceived to train students and practitioners in concepts related to software architecture and decision making. The game is currently used as an interactive session of the course “Software Architecture”, taught at the Vrije Universiteit Amsterdam.<br/><br/>The two prizes were adjudicated independently by two distinct committees.</div><div> </div><div>Congratulations to Roberto!</div><div><br/></div><div>Let me close by adding that I expect that Roberto will deliver his PhD thesis in the autumn 2019 and will soon be on the job market. If you have a postdoc or tenure-track  position in SE, keep him mind. </div></div></div>
    </content>
    <updated>2019-02-05T08:58:00Z</updated>
    <published>2019-02-05T08:58:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2019-02-05T08:58:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4206</id>
    <link href="https://lucatrevisan.wordpress.com/2019/02/04/%e6%81%ad%e5%96%9c%e5%8f%91%e8%b4%a2-11/" rel="alternate" type="text/html"/>
    <title>恭喜发财!</title>
    <summary>新年快乐！ Advertisements</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><img alt="8cxnLMGdi" class="alignnone size-full wp-image-4207" src="https://lucatrevisan.files.wordpress.com/2019/02/8cxnlmgdi.png?w=584"/></p>
<p>新年快乐！</p></div>
    </content>
    <updated>2019-02-05T06:45:35Z</updated>
    <published>2019-02-05T06:45:35Z</published>
    <category term="&#x65B0;&#x5E74;"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-02-06T12:20:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01334</id>
    <link href="http://arxiv.org/abs/1902.01334" rel="alternate" type="text/html"/>
    <title>Distances between Data Sets Based on Summary Statistics</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tatti:Nikolaj.html">Nikolaj Tatti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01334">PDF</a><br/><b>Abstract: </b>The concepts of similarity and distance are crucial in data mining. We
consider the problem of defining the distance between two data sets by
comparing summary statistics computed from the data sets. The initial
definition of our distance is based on geometrical notions of certain sets of
distributions. We show that this distance can be computed in cubic time and
that it has several intuitive properties. We also show that this distance is
the unique Mahalanobis distance satisfying certain assumptions. We also
demonstrate that if we are dealing with binary data sets, then the distance can
be represented naturally by certain parity functions, and that it can be
evaluated in linear time. Our empirical tests with real world data show that
the distance works well.
</p></div>
    </summary>
    <updated>2019-02-05T23:36:05Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01331</id>
    <link href="http://arxiv.org/abs/1902.01331" rel="alternate" type="text/html"/>
    <title>Safe projections of binary data sets</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tatti:Nikolaj.html">Nikolaj Tatti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01331">PDF</a><br/><b>Abstract: </b>Selectivity estimation of a boolean query based on frequent itemsets can be
solved by describing the problem by a linear program. However, the number of
variables in the equations is exponential, rendering the approach tractable
only for small-dimensional cases. One natural approach would be to project the
data to the variables occurring in the query. This can, however, change the
outcome of the linear program.
</p>
<p>We introduce the concept of safe sets: projecting the data to a safe set does
not change the outcome of the linear program. We characterise safe sets using
graph theoretic concepts and give an algorithm for finding minimal safe sets
containing given attributes. We describe a heuristic algorithm for finding
almost-safe sets given a size restriction, and show empirically that these sets
outperform the trivial projection.
</p>
<p>We also show a connection between safe sets and Markov Random Fields and use
it to further reduce the number of variables in the linear program, given some
regularity assumptions on the frequent itemsets.
</p></div>
    </summary>
    <updated>2019-02-05T23:35:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01280</id>
    <link href="http://arxiv.org/abs/1902.01280" rel="alternate" type="text/html"/>
    <title>A New Class of Searchable and Provably Highly Compressible String Transformations</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giancarlo:Raffaele.html">Raffaele Giancarlo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manzini:Giovanni.html">Giovanni Manzini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosone:Giovanna.html">Giovanna Rosone</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sciortino:Marinella.html">Marinella Sciortino</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01280">PDF</a><br/><b>Abstract: </b>The Burrows-Wheeler Transform is a string transformation that plays a
fundamental role for the design of self-indexing compressed data structures.
Over the years, researchers have successfully extended this transformation
outside the domains of strings. However, efforts to find non-trivial
alternatives of the original, now 25 years old, Burrows-Wheeler string
transformation have met limited success. In this paper we bring new lymph to
this area by introducing a whole new family of transformations that have all
the myriad virtues of the BWT: they can be computed and inverted in linear
time, they produce provably highly compressible strings, and they support
linear time pattern search directly on the transformed string. This new family
is a special case of a more general class of transformations based on context
adaptive alphabet orderings, a concept introduced here. This more general class
includes also the Alternating BWT, another invertible string transforms
recently introduced in connection with a generalization of Lyndon words.
</p></div>
    </summary>
    <updated>2019-02-05T23:24:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01192</id>
    <link href="http://arxiv.org/abs/1902.01192" rel="alternate" type="text/html"/>
    <title>Advances in the Treatment of Trimmed CAD Models due to Isogeometric Analysis</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marussig:Benjamin.html">Benjamin Marussig</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01192">PDF</a><br/><b>Abstract: </b>Trimming is a core technique in geometric modeling. Unfortunately, the
resulting objects do not take the requirements of numerical simulations into
account and yield various problems. This paper outlines principal issues of
trimmed models and highlights different analysis-suitable strategies to address
them. It is discussed that these concepts not only provide important
computational tools for isogeometric analysis, but can also improve the
treatment of trimmed models in a design context.
</p></div>
    </summary>
    <updated>2019-02-05T23:47:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01128</id>
    <link href="http://arxiv.org/abs/1902.01128" rel="alternate" type="text/html"/>
    <title>A Unified Framework for Marketing Budget Allocation</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Kui.html">Kui Zhao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hua:Junhao.html">Junhao Hua</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yan:Ling.html">Ling Yan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Qi.html">Qi Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Huan.html">Huan Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Cheng.html">Cheng Yang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01128">PDF</a><br/><b>Abstract: </b>While marketing budget allocation has been studied for decades in traditional
business, nowadays online business brings much more challenges due to the
dynamic environment and complex decision-making process. In this paper, we
present a novel unified framework for marketing budget allocation. By
leveraging abundant data, the proposed data-driven approach can help us to
overcome the challenges and make more informed decisions. In our approach, a
semi-black-box model is built to forecast the dynamic market response and an
efficient optimization method is proposed to solve the complex allocation task.
First, the response in each market-segment is forecasted by exploring
historical data through a semi-black-box model, where the capability of logit
demand curve is enhanced by neural networks. The response model reveals
relationship between sales and marketing cost. Based on the learned model,
budget allocation is then formulated as an optimization problem, and we design
efficient algorithms to solve it in both continuous and discrete settings.
Several kinds of business constraints are supported in one unified optimization
paradigm, including cost upper bound, profit lower bound, or ROI lower bound.
The proposed framework is easy to implement and readily to handle large-scale
problems. It has been successfully applied to many scenarios in Alibaba Group.
The results of both offline experiments and online A/B testing demonstrate its
effectiveness.
</p></div>
    </summary>
    <updated>2019-02-05T23:36:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01088</id>
    <link href="http://arxiv.org/abs/1902.01088" rel="alternate" type="text/html"/>
    <title>On Prefix-Sorting Finite Automata</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alanko:Jarno.html">Jarno Alanko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Policriti:Alberto.html">Alberto Policriti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prezza:Nicola.html">Nicola Prezza</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01088">PDF</a><br/><b>Abstract: </b>Being able to efficiently test the membership of a word in a formal language
is, arguably, one of the most fundamental problems in computer science. In this
paper, we apply string-processing techniques (specifically,
\emph{prefix-sorting}) to speed up solutions for this problem on languages
described by particular finite automata. Prefix sorting can be generalized to
objects more complex than strings: the recent notion of \emph{Wheeler graph}
extends this concept to labeled graphs (for example, finite-state automata). A
Wheeler graph admits a co-lexicographic ordering of its nodes and opens up the
possibility of building fast and small data structures supporting path queries
on the graph. However, while strings and trees can always be prefix-sorted in
linear time, the situation on graphs is more complicated: not all graphs are
Wheeler, and a recent result shows that the problem of identifying them is
NP-complete even for acyclic NFAs. In this paper, we present the following
results: (i) we show that the problem of recognizing and sorting Wheeler DFAs
(even cyclic) can be solved offline in optimal linear time, (ii) when the DFA
is acyclic, we give an online algorithm that, when fed with the DFA's states in
any topological order, with logarithmic delay computes the co-lexicographic
rank of a new incoming state or decides that such an ordering does not exist,
and (iii) we show that also Wheeler 2-NFAs---i.e. those with at most two
equally-labeled transitions leaving any state---admit a polynomial-time
identification and prefix-sorting algorithm. Our algorithms (i)-(ii) generalize
to labeled graphs existing prefix-sorting algorithms on strings and labeled
trees that have been previously described in the literature.
</p></div>
    </summary>
    <updated>2019-02-05T23:25:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01028</id>
    <link href="http://arxiv.org/abs/1902.01028" rel="alternate" type="text/html"/>
    <title>Can SGD Learn Recurrent Neural Networks with Provable Generalization?</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Allen=Zhu:Zeyuan.html">Zeyuan Allen-Zhu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yuanzhi.html">Yuanzhi Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01028">PDF</a><br/><b>Abstract: </b>Recurrent Neural Networks (RNNs) are among the most popular models in
sequential data analysis. However, due to the complexity raised by recurrent
structure, they remain one of the least theoretically understood neural-network
models. In particular, existing generalization bounds for RNNs mostly scale
exponentially with the length of the input sequence, which limited their
practical implications. In this paper, we show that if the input labels are
(approximately) realizable by certain classes of (non-linear) functions of the
input sequences, then using the vanilla stochastic gradient descent (SGD), RNNs
can actually learn them $\textit{efficiently}$, meaning that both the training
time and sample complexity only scale $\textit{polynomially}$ with the input
length (or almost polynomially, depending on the classes of non-linearity).
</p></div>
    </summary>
    <updated>2019-02-05T23:35:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.01002</id>
    <link href="http://arxiv.org/abs/1902.01002" rel="alternate" type="text/html"/>
    <title>Ranking Episodes using a Partition Model</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tatti:Nikolaj.html">Nikolaj Tatti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.01002">PDF</a><br/><b>Abstract: </b>One of the biggest setbacks in traditional frequent pattern mining is that
overwhelmingly many of the discovered patterns are redundant. A prototypical
example of such redundancy is a freerider pattern where the pattern contains a
true pattern and some additional noise events. A technique for filtering
freerider patterns that has proved to be efficient in ranking itemsets is to
use a partition model where a pattern is divided into two subpatterns and the
observed support is compared to the expected support under the assumption that
these two subpatterns occur independently.
</p>
<p>In this paper we develop a partition model for episodes, patterns discovered
from sequential data. An episode is essentially a set of events, with possible
restrictions on the order of events. Unlike with itemset mining, computing the
expected support of an episode requires surprisingly sophisticated methods. In
order to construct the model, we partition the episode into two subepisodes. We
then model how likely the events in each subepisode occur close to each other.
If this probability is high---which is often the case if the subepisode has a
high support---then we can expect that when one event from a subepisode occurs,
then the remaining events occur also close by. This approach increases the
expected support of the episode, and if this increase explains the observed
support, then we can deem the episode uninteresting. We demonstrate in our
experiments that using the partition model can effectively and efficiently
reduce the redundancy in episodes.
</p></div>
    </summary>
    <updated>2019-02-05T23:26:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.00975</id>
    <link href="http://arxiv.org/abs/1902.00975" rel="alternate" type="text/html"/>
    <title>Some Remarks on Real-Time Turing Machines</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Petersen:Holger.html">Holger Petersen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.00975">PDF</a><br/><b>Abstract: </b>The power of real-time Turing machines using sublinear space is investigated.
In contrast to a claim appearing in the literature, such machines can accept
non-regular languages, even if working in deterministic mode. While maintaining
a standard binary counter appears to be impossible in real-time, we present a
guess and check approach that yields a binary representation of the input
length. Based on this technique, we show that unary encodings of languages
accepted in exponential time can be recognized by nondeterministic real-time
Turing machines.
</p></div>
    </summary>
    <updated>2019-02-05T23:24:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.00947</id>
    <link href="http://arxiv.org/abs/1902.00947" rel="alternate" type="text/html"/>
    <title>Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Taylor:Adrien.html">Adrien Taylor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bach:Francis.html">Francis Bach</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.00947">PDF</a><br/><b>Abstract: </b>We provide a novel computer-assisted technique for systematically analyzing
first-order methods for optimization. In contrast with previous works, the
approach is particularly suited for handling sublinear convergence rates and
stochastic oracles. The technique relies on semidefinite programming and
potential functions. It allows simultaneously obtaining worst-case guarantees
on the behavior of those algorithms, and assisting in choosing appropriate
parameters for tuning their worst-case performances. The technique also
benefits from comfortable tightness guarantees, meaning that unsatisfactory
results can be improved only by changing the setting. We use the approach for
analyzing deterministic and stochastic first-order methods under different
assumptions on the nature of the stochastic noise. Among others, we treat
unstructured noise with bounded variance, different noise models arising in
over-parametrized expectation minimization problems, and randomized
block-coordinate descent schemes.
</p></div>
    </summary>
    <updated>2019-02-05T23:20:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.00919</id>
    <link href="http://arxiv.org/abs/1902.00919" rel="alternate" type="text/html"/>
    <title>Knapsack Problem With Cardinality Constraint: A Faster FPTAS Through the Lens of Numerical Analysis and Applications</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Wenxin.html">Wenxin Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Joohyun.html">Joohyun Lee</a>, Ness Shroff <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.00919">PDF</a><br/><b>Abstract: </b>We study the $K$-item knapsack problem (\ie, $1.5$-dimensional knapsack
problem), which is a generalization of the famous 0-1 knapsack problem (\ie,
$1$-dimensional knapsack problem) in which an upper bound $K$ is imposed on the
number of items selected. This problem is of fundamental importance and is
known to have a broad range of applications in various fields such as computer
science and operation research. It is well known that, there is no FPTAS for
the $d$-dimensional knapsack problem when $d\geq 2$, unless P $=$ NP. While the
$K$-item knapsack problem is known to admit an FPTAS, the complexity of all
existing FPTASs have a high dependency on the cardinality bound $K$ and
approximation error $\varepsilon$, which could result in inefficiencies
especially when $K$ and $\varepsilon^{-1}$ increase. The current best results
are due to \citep{mastrolilli2006hybrid}, in which two schemes are presented
exhibiting a space-time tradeoff--one scheme with time complexity
$O(n+Kz^{2}/\varepsilon^{2})$ and space complexity $O(n+z^{3}/\varepsilon)$,
while another scheme requires a run-time of
$O(n+(Kz^{2}+z^{4})/\varepsilon^{2})$ but only needs $O(n+z^{2}/\varepsilon)$
space, where $z=\min\{K,1/\varepsilon\}$. In this paper we close the space-time
tradeoff exhibited in \citep{mastrolilli2006hybrid} by designing a new FPTAS
with a running time of $O(n)+\widetilde{O}(z^{2}\cdot
\max\{K,\varepsilon^{-2}\})$, while simultaneously reaching the
$O(n+z^{2}/\varepsilon)$ space complexity. Our scheme provides
$\widetilde{O}(\min\{K,\varepsilon^{-2}\})$ and $O(z)$ improvements on the
long-established state-of-the-art algorithms in time and space complexity
respectively. An salient feature of our algorithm is that it is the
\emph{first} FPTAS, which achieves better time and space complexity bounds than
the very first standard FPTAS \emph{over all parameter regimes}.
</p></div>
    </summary>
    <updated>2019-02-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.00911</id>
    <link href="http://arxiv.org/abs/1902.00911" rel="alternate" type="text/html"/>
    <title>Study, representation and applications of hypergraph minimal transversals</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>M. Nidhal Jelassi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.00911">PDF</a><br/><b>Abstract: </b>This work is part of the field of the hypergraph theory and focuses on
hypergraph minimal transversal. The problem of extracting the minimal
transversals from a hypergraph received the interest of many researchers as
shown the number of algorithms proposed in the literature, and this is mainly
due to the solutions offered by the minimal transversal in various application
areas such as databases, artificial intelligence, e-commerce, semantic web,
etc. In view of the wide range of fields of minimal transversal application and
the interest they generate, the objective of this thesis is to explore new
application paths of minimal transversal by proposing methods to optimize the
extraction. This has led to three proposed contributions in this thesis. The
first approach takes advantage of the emergence of Web 2.0 and, therefore,
social networks using minimal transversal for the detection of important actors
within these networks. The second part of research in this thesis has focused
on reducing the number of hypergraph minimal transversal. A concise and
accurate representation of minimal transversal was proposed and is based on the
construction of an irredundant hypergraph, hence are calculated the irredundant
minimal transversal of the initial hypergraph. An application of this
representation to the dependency inference problem is presented to illustrate
the usefulness of this approach. The last approach includes the hypergraph
decomposition into partial hypergraph the local minimal transversal are
calculated and their Cartesian product can generate all the hypergraph
transversal sets. Different experimental studies have shown the value of these
proposed approaches.
</p></div>
    </summary>
    <updated>2019-02-05T23:25:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.00846</id>
    <link href="http://arxiv.org/abs/1902.00846" rel="alternate" type="text/html"/>
    <title>A Billion Updates per Second Using 30,000 Hierarchical In-Memory D4M Databases</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kepner:Jeremy.html">Jeremy Kepner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gadepally:Vijay.html">Vijay Gadepally</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Milechin:Lauren.html">Lauren Milechin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Samsi:Siddharth.html">Siddharth Samsi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Arcand:William.html">William Arcand</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bestor:David.html">David Bestor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bergeron:William.html">William Bergeron</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Byun:Chansup.html">Chansup Byun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hubbell:Matthew.html">Matthew Hubbell</a>, Micheal Houle, Micheal Jones, Anne Klein, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Michaleas:Peter.html">Peter Michaleas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mullen:Julie.html">Julie Mullen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prout:Andrew.html">Andrew Prout</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosa:Antonio.html">Antonio Rosa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yee:Charles.html">Charles Yee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reuther:Albert.html">Albert Reuther</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.00846">PDF</a><br/><b>Abstract: </b>Analyzing large scale networks requires high performance streaming updates of
graph representations of these data. Associative arrays are mathematical
objects combining properties of spreadsheets, databases, matrices, and graphs,
and are well-suited for representing and analyzing streaming network data. The
Dynamic Distributed Dimensional Data Model (D4M) library implements associative
arrays in a variety of languages (Python, Julia, and Matlab/Octave) and
provides a lightweight in-memory database. Associative arrays are designed for
block updates. Streaming updates to a large associative array requires a
hierarchical implementation to optimize the performance of the memory
hierarchy. Running 34,000 instances of a hierarchical D4M associative arrays on
1,100 server nodes on the MIT SuperCloud achieved a sustained update rate of
1,900,000,000 updates per second. This capability allows the MIT SuperCloud to
analyze extremely large streaming network data sets.
</p></div>
    </summary>
    <updated>2019-02-05T23:30:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.00804</id>
    <link href="http://arxiv.org/abs/1902.00804" rel="alternate" type="text/html"/>
    <title>Itemsets for Real-valued Datasets</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tatti:Nikolaj.html">Nikolaj Tatti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.00804">PDF</a><br/><b>Abstract: </b>Pattern mining is one of the most well-studied subfields in exploratory data
analysis. While there is a significant amount of literature on how to discover
and rank itemsets efficiently from binary data, there is surprisingly little
research done in mining patterns from real-valued data. In this paper we
propose a family of quality scores for real-valued itemsets. We approach the
problem by considering casting the dataset into a binary data and computing the
support from this data. This naive approach requires us to select thresholds.
To remedy this, instead of selecting one set of thresholds, we treat thresholds
as random variables and compute the average support. We show that we can
compute this support efficiently, and we also introduce two normalisations,
namely comparing the support against the independence assumption and, more
generally, against the partition assumption. Our experimental evaluation
demonstrates that we can discover statistically significant patterns
efficiently.
</p></div>
    </summary>
    <updated>2019-02-05T23:30:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1902.00732</id>
    <link href="http://arxiv.org/abs/1902.00732" rel="alternate" type="text/html"/>
    <title>Scheduling with Predictions and the Price of Misprediction</title>
    <feedworld_mtime>1549324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitzenmacher:Michael.html">Michael Mitzenmacher</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1902.00732">PDF</a><br/><b>Abstract: </b>In many traditional job scheduling settings, it is assumed that one knows the
time it will take for a job to complete service. In such cases, strategies such
as shortest job first can be used to improve performance in terms of measures
such as the average time a job waits in the system. We consider the setting
where the service time is not known, but is predicted by for example a machine
learning algorithm. Our main result is the derivation, under natural
assumptions, of formulae for the performance of several strategies for queueing
systems that use predictions for service times in order to schedule jobs. As
part of our analysis, we suggest the framework of the "price of misprediction,"
which offers a measure of the cost of using predicted information.
</p></div>
    </summary>
    <updated>2019-02-05T23:27:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-02-05T01:30:00Z</updated>
    </source>
  </entry>
</feed>
