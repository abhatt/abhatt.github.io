<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-02-21T21:22:26Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-1990979806491986425</id>
    <link href="http://processalgebra.blogspot.com/feeds/1990979806491986425/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=1990979806491986425" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/1990979806491986425" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/1990979806491986425" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/02/article-by-sergey-kitaev-and-anthony.html" rel="alternate" type="text/html"/>
    <title>Article by Sergey Kitaev and Anthony Mendes in Jeff Remmel's memory</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://www.strath.ac.uk/staff/kitaevsergeydr/" target="_blank">Sergey Kitaev</a> just shared with me <a href="http://ecajournal.haifa.ac.il/Volume2021/ECA2021_S1H2.pdf" target="_blank">an article</a> he wrote with Anthony Mendes in <a href="https://senate.universityofcalifornia.edu/in-memoriam/files/jeffrey-b-remmel.html" target="_blank">Jeff Remmel's memory</a>. Jeff Remmel was a distinguished mathematician with a very successful career in both logic and combinatorics. </p><p>The short biography at the start of the article paints a vivid picture of Jeff Remmel's  personality, and will be of interest and inspiration to many readers. His hiring as "an Assistant Professor in the Department of Mathematics at UC San Diego at age 25, without officially finishing his Ph.D. and without having published a single paper" was, in Jeff Remmel's own words, a "fluke that will never happen again."<br/></p><p>I had the pleasure of making Jeff Remmel's acquaintance when he visited Sergey in Reykjavik and thoroughly enjoyed talking to him about a variety of subjects. He was truly a larger-than-life academic. <br/></p></div>
    </content>
    <updated>2021-02-21T11:47:00Z</updated>
    <published>2021-02-21T11:47:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-02-21T11:47:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/025</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/025" rel="alternate" type="text/html"/>
    <title>TR21-025 |  Improved Maximally Recoverable LRCs using Skew Polynomials | 

	Sivakanth Gopi, 

	Venkatesan Guruswami</title>
    <summary>An $(n,r,h,a,q)$-Local Reconstruction Code is a linear code over $\mathbb{F}_q$ of length $n$, whose codeword symbols are partitioned into $n/r$ local groups each of size $r$. Each local group satisfies `$a$' local parity checks to recover from `$a$' erasures in that local group and there are further $h$ global parity checks to provide fault tolerance from more global erasure patterns. Such an LRC is Maximally Recoverable (MR), if it offers the best blend of locality and global erasure resilience---namely it can correct all erasure patterns whose recovery is information-theoretically feasible given the locality structure (these are precisely patterns with up to `$a$' erasures in each local group and an additional $h$ erasures anywhere in the codeword).

Random constructions can easily show the existence of MR LRCs over very large fields, but a major algebraic challenge is to construct MR LRCs, or even show their existence, over smaller fields, as well as understand inherent lower bounds on their field size. We give an explicit construction of $(n,r,h,a,q)$-MR LRCs with field size $q$ bounded by $\left(O\left(\max\{r,n/r\}\right)\right)^{\min\{h,r-a\}}$. This improves upon known constructions in many relevant parameter ranges. Moreover, it matches the lower bound from Gopi et al. (2020) in an interesting range of parameters where $r=\Theta(\sqrt{n})$, $r-a=\Theta(\sqrt{n})$ and $h$ is a fixed constant with $h\le a+2$, achieving the optimal field size of $\Theta_{h}(n^{h/2}).$

Our construction is based on the theory of skew polynomials.  We believe skew polynomials should have further applications in coding and complexity theory; as a small illustration we show how to capture algebraic results underlying list decoding folded Reed-Solomon and multiplicity codes in a unified way within this theory.</summary>
    <updated>2021-02-21T10:21:05Z</updated>
    <published>2021-02-21T10:21:05Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T21:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/024</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/024" rel="alternate" type="text/html"/>
    <title>TR21-024 |  A Majority Lemma for Randomised Query Complexity | 

	Mika Göös, 

	Gilbert Maystre</title>
    <summary>We show that computing the majority of $n$ copies of a boolean function $g$ has randomised query complexity $\mathrm{R}(\mathrm{Maj} \circ g^n) = \Theta(n\cdot \bar{\mathrm{R}}_{1/n}(g))$. In fact, we show that to obtain a similar result for any composed function $f\circ g^n$, it suffices to prove a sufficiently strong form of the result only in the special case $g=\mathrm{GapOr}$.</summary>
    <updated>2021-02-21T10:18:33Z</updated>
    <published>2021-02-21T10:18:33Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T21:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/023</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/023" rel="alternate" type="text/html"/>
    <title>TR21-023 |  $3.1n - o(n)$ Circuit Lower Bounds for Explicit Functions | 

	Tianqi Yang, 

	Jiatu Li</title>
    <summary>Proving circuit lower bounds has been an important but extremely hard problem for decades. Although one may show that almost every function $f:\mathbb{F}_2^n\to\mathbb{F}_2$ requires circuit of size $\Omega(2^n/n)$ by a simple counting argument, it remains unknown whether there is an explicit function (for example, a function in $NP$) not computable by circuits of size $10n$. In fact, a $3n-o(n)$ explicit lower bound by Blum (TCS, 1984) was unbeaten for over 30 years until a recent breakthrough by Find et al. (FOCS, 2016), which proved a $(3+\frac{1}{86})n-o(n)$ lower bound for affine dispersers, a class of functions known to be constructible in $P$.

In this paper, we prove a stronger lower bound $3.1n - o(n)$ for affine dispersers. To get this result, we strengthen the gate elimination approach for $(3+\frac{1}{86})n$ lower bound, by a more sophisticated case analysis that significantly decreases the number of bottleneck structures introduced during the elimination procedure. Intuitively, our improvement relies on three observations: adjacent bottleneck structures becomes less troubled; the gates eliminated are usually connected; and the hardest cases during gate elimination have nice local properties to prevent the introduction of new bottleneck structures.</summary>
    <updated>2021-02-21T06:59:55Z</updated>
    <published>2021-02-21T06:59:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T21:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/022</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/022" rel="alternate" type="text/html"/>
    <title>TR21-022 |  Depth lower bounds in Stabbing Planes for combinatorial principles | 

	Stefan Dantchev, 

	Nicola  Galesi, 

	Abdul Ghani, 

	Barnaby Martin</title>
    <summary>We prove logarithmic depth lower bounds in Stabbing Planes for the classes of  combinatorial principles known as  the Pigeonhole principle and the Tseitin contradictions. The depth lower bounds are new, obtained by giving almost linear length lower bounds which do not depend on the bit-size of the inequalities and in the case of  the Pigeonhole principle are tight. 

The technique known so far to prove depth lower bounds for Stabbing Planes is a generalization of that used for the Cutting Planes proof system.  In this work  we  introduce two  new approaches to prove length/depth lower bounds in Stabbing Planes: one relying on Sperner's Theorem which works for the Pigeonhole principle and Tseitin contradictions over the complete graph; a second proving the lower bound for Tseitin contradictions over a grid graph, which uses a result on essential  coverings of the boolean cube by linear polynomials, which in turn relies on Alon's combinatorial Nullenstellensatz</summary>
    <updated>2021-02-20T20:20:01Z</updated>
    <published>2021-02-20T20:20:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T21:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/021</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/021" rel="alternate" type="text/html"/>
    <title>TR21-021 |  Average-Case Perfect Matching Lower Bounds from Hardness of Tseitin Formulas | 

	Kilian Risse, 

	Per Austrin</title>
    <summary>We study the complexity of proving that a sparse random regular graph on an odd number of vertices does not have a perfect matching, and related problems involving each vertex being matched some pre-specified number of times. We show that this requires proofs of degree $\Omega(n/\log n)$ in the Polynomial Calculus (over fields of characteristic $\ne 2$) and Sum-of-Squares proof systems, and exponential size in the bounded-depth Frege proof system. This resolves a question by Razborov asking whether the Lovász-Schrijver proof system requires $n^\delta$ rounds to refute these formulas for some $\delta &gt; 0$. The results are obtained by a worst-case to average-case reduction of these formulas relying on a topological embedding theorem which may be of independent interest.</summary>
    <updated>2021-02-20T18:26:59Z</updated>
    <published>2021-02-20T18:26:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T21:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/020</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/020" rel="alternate" type="text/html"/>
    <title>TR21-020 |  Error Reduction For Weighted PRGs Against Read Once Branching Programs | 

	Gil Cohen, 

	Dean Doron, 

	Amnon Ta-Shma, 

	Ori Sberlo, 

	Oren Renard</title>
    <summary>Weighted pseudorandom generators (WPRGs), introduced by Braverman, Cohen and Garg [BCG20], is a generalization of pseudorandom generators (PRGs) in which arbitrary real weights are considered rather than a probability mass. Braverman et al. constructed WPRGs against read once branching programs (ROBPs) with near-optimal dependence on the error parameter. Chattopadhyay and Liao [CL20] somewhat simplified the technically involved BCG construction, also obtaining some improvement in parameters.

In this work we devise an error reduction procedure for PRGs against ROBPs. More precisely, our procedure transforms any PRG against length n width w ROBP with error 1/poly(n) having seed length s to a WPRG with seed length s + O(log(w/?)loglog(1/?)). By instantiating our procedure with Nisan’s PRG [Nis92] we obtain a WPRG with seed length O(log(n)log(nw) + log(w/?)loglog(1/?)). This improves upon [BCG20] and is incomparable with [CL20].

Our construction is significantly simpler on the technical side and is conceptually cleaner. Another advantage of our construction is its low space complexity O(log nw)+ poly(loglog(1/?)) which is logarithmic in n for interesting values of the error parameter ?. Previous constructions (like [BCG20, CL20]) specify the seed length but not the space complexity, though it is plausible they can also achieve such (or close) space complexity.</summary>
    <updated>2021-02-20T06:12:44Z</updated>
    <published>2021-02-20T06:12:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T21:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/019</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/019" rel="alternate" type="text/html"/>
    <title>TR21-019 |  Pseudodistributions That Beat All Pseudorandom Generators | 

	Edward Pyne, 

	Salil Vadhan</title>
    <summary>A recent paper of Braverman, Cohen, and Garg (STOC 2018) introduced the concept of a pseudorandom pseudodistribution generator (PRPG), which amounts to a pseudorandom generator (PRG) whose outputs are accompanied with real coefficients that scale the acceptance probabilities of any potential distinguisher. They gave an explicit construction of PRPGs for ordered branching programs whose seed length has a better dependence on the error parameter $\epsilon$ than the classic PRG construction of Nisan (STOC 1990 and Combinatorica 1992). 
    
    In this work, we give an explicit construction of PRPGs that achieve parameters that are impossible to achieve by a PRG.  In particular, we construct a PRPG for ordered permutation branching programs of unbounded width with a single accept state that has seed length $\tilde{O}(\log^{3/2} n)$ for error parameter $\epsilon=1/\text{poly}(n)$, where $n$ is the input length.  In contrast, recent work of Hoza et al. (ITCS 2021) shows that any PRG for this model requires seed length $\Omega(\log^2 n)$ to achieve error $\epsilon=1/\text{poly}(n)$.
    
    As a corollary, we obtain explicit PRPGs with seed length $\tilde{O}(\log^{3/2} n)$ and error $\epsilon=1/\text{poly}(n)$ for ordered permutation branching programs of width $w=\text{poly}(n)$ with an arbitrary number of accept states.  Previously, seed length $o(\log^2 n)$ was only known when both the width and the reciprocal of the error are subpolynomial, i.e. $w=n^{o(1)}$ and $\epsilon=1/n^{o(1)}$ (Braverman, Rao, Raz, Yehudayoff, FOCS 2010 and SICOMP 2014).
    
    The starting point for our results are the recent space-efficient algorithms for estimating random-walk probabilities in directed graphs by Ahmadenijad, Kelner, Murtagh, Peebles, Sidford, and Vadhan (FOCS 2020), which are based on spectral graph theory and space-efficient Laplacian solvers.  We interpret these algorithms as giving PRPGs with large seed length, which we then derandomize to obtain our results.  We also note that this approach gives a simpler proof of the original result of Braverman, Cohen, and Garg, as independently discovered by Cohen, Doron, Renard, Sberlo, and Ta-Shma (personal communication, January 2021).</summary>
    <updated>2021-02-20T06:10:41Z</updated>
    <published>2021-02-20T06:10:41Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T21:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/018</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/018" rel="alternate" type="text/html"/>
    <title>TR21-018 |  Monotone Branching Programs: Pseudorandomness and Circuit Complexity | 

	Dean Doron, 

	Raghu Meka, 

	Omer Reingold, 

	Avishay Tal, 

	Salil Vadhan</title>
    <summary>We study monotone branching programs, wherein the states at each time step can be ordered so that edges with the same labels never cross each other. Equivalently, for each fixed input, the transition functions are a monotone function of the state. 

We prove that constant-width monotone branching programs of polynomial size are equivalent in power to $AC^{0}$ circuits. This complements the celebrated theorem of Barrington, which states that constant-width branching programs, without the monotonicity constraint, are equivalent in power to $NC^{1}$ circuits.

Next we turn to read-once monotone branching programs of constant width, which we note are strictly more powerful than read-once $AC^0$.  Our main result is an explicit pseudorandom generator that $\varepsilon$-fools length $n$ programs with seed length $\widetilde{O}(\log(n/\varepsilon))$. This extends the families of constant-width read-once branching programs for which we have an explicit pseudorandom generator with near-logarithmic seed length. 

Our pseudorandom generator construction follows Ajtai and Wigderson's approach of iterated pseudorandom restrictions [AW89,GMRTV12]. We give a randomness-efficient width-reduction process which allows us to simplify the branching program after only $O(\log\log n)$ independent applications of the Forbes--Kelley pseudorandom restrictions [FK18].</summary>
    <updated>2021-02-20T06:07:23Z</updated>
    <published>2021-02-20T06:07:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T21:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09544</id>
    <link href="http://arxiv.org/abs/2102.09544" rel="alternate" type="text/html"/>
    <title>Combinatorial optimization and reasoning with graph neural networks</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cappart:Quentin.html">Quentin Cappart</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Ch=eacute=telat:Didier.html">Didier Chételat</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khalil:Elias.html">Elias Khalil</a>, Andrea Lodi, Christopher Morris, Petar Veličković <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09544">PDF</a><br/><b>Abstract: </b>Combinatorial optimization is a well-established area in operations research
and computer science. Until recently, its methods have focused on solving
problem instances in isolation, ignoring the fact that they often stem from
related data distributions in practice. However, recent years have seen a surge
of interest in using machine learning, especially graph neural networks (GNNs),
as a key building block for combinatorial tasks, either as solvers or as helper
functions. GNNs are an inductive bias that effectively encodes combinatorial
and relational input due to their permutation-invariance and sparsity
awareness. This paper presents a conceptual review of recent key advancements
in this emerging field, aiming at both the optimization and machine learning
researcher.
</p></div>
    </summary>
    <updated>2021-02-20T22:47:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09510</id>
    <link href="http://arxiv.org/abs/2102.09510" rel="alternate" type="text/html"/>
    <title>How we are leading a 3-XORSAT challenge: from the energy landscape to the algorithm and its efficient implementation on GPUs</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>M. Bernaschi, M. Bisson, M. Fatica, E. Marinari, V. Martin-Mayor, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parisi:G=.html">G. Parisi</a>, F. Ricci-Tersenghi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09510">PDF</a><br/><b>Abstract: </b>A recent 3-XORSAT challenge required to minimize a very complex and rough
energy function, typical of glassy models with a random first order transition
and a golf course like energy landscape. We present the ideas beyond the
quasi-greedy algorithm and its very efficient implementation on GPUs that are
allowing us to rank first in such a competition. We suggest a better protocol
to compare algorithmic performances and we also provide analytical predictions
about the exponential growth of the times to find the solution in terms of
free-energy barriers.
</p></div>
    </summary>
    <updated>2021-02-20T22:42:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09463</id>
    <link href="http://arxiv.org/abs/2102.09463" rel="alternate" type="text/html"/>
    <title>Range Minimum Queries in Minimal Space</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Russo:Lu=iacute=s_M=_S=.html">Luís M. S. Russo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09463">PDF</a><br/><b>Abstract: </b>We consider the problem of computing a sequence of range minimum queries. We
assume a sequence of commands that contains values and queries. Our goal is to
quickly determine the minimum value that exists between the current position
and a previous position $i$. Range minimum queries are used as a sub-routine of
several algorithms, namely related to string processing. We propose a data
structure that can process these commands sequences. We obtain efficient
results for several variations of the problem, in particular we obtain $O(1)$
time per command for the offline version and $O(\alpha(n))$ amortized time for
the online version, where $\alpha(n)$ is the inverse Ackermann function and $n$
the number of values in the sequence. This data structure also has very small
space requirements, namely $O(\ell)$ where $\ell$ is the maximum number active
$i$ positions. We implemented our data structure and show that it is
competitive against existing alternatives. We obtain comparable command
processing time, in the nano second range, and much smaller space requirements.
</p></div>
    </summary>
    <updated>2021-02-20T22:41:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09432</id>
    <link href="http://arxiv.org/abs/2102.09432" rel="alternate" type="text/html"/>
    <title>A Stronger Impossibility for Fully Online Matching</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eckl:Alexander.html">Alexander Eckl</a>, Anja Kirschbaum, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leichter:Marilena.html">Marilena Leichter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schewior:Kevin.html">Kevin Schewior</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09432">PDF</a><br/><b>Abstract: </b>We revisit the fully online matching model (Huang et al., J.\ ACM, 2020), an
extension of the classic online matching model due to Karp, Vazirani, and
Vazirani (STOC 1990), which has recently received a lot of attention (Huang et
al., SODA 2019 and FOCS 2020), partly due to applications in ride-sharing
platforms. It has been shown that the fully online version is harder than the
classic version for which the achievable competitive ratio is at most $0.6317$,
rather than precisely $1-\frac{1}{e}\approx 0.6321$. We introduce two new ideas
to the construction. By optimizing the parameters of the modified construction
numerically, we obtain an improved impossibility result of $0.6297$. Like the
previous bound, the new bound even holds for fractional (rather than
randomized) algorithms on bipartite graphs.
</p></div>
    </summary>
    <updated>2021-02-20T22:48:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09413</id>
    <link href="http://arxiv.org/abs/2102.09413" rel="alternate" type="text/html"/>
    <title>Locality in Online Algorithms</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pacut:Maciej.html">Maciej Pacut</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parham:Mahmoud.html">Mahmoud Parham</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rybicki:Joel.html">Joel Rybicki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmid:Stefan.html">Stefan Schmid</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Suomela:Jukka.html">Jukka Suomela</a>, Aleksandr Tereshchenko <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09413">PDF</a><br/><b>Abstract: </b>Online algorithms make decisions based on past inputs. In general, the
decision may depend on the entire history of inputs. If many computers run the
same online algorithm with the same input stream but are started at different
times, they do not necessarily make consistent decisions.
</p>
<p>In this work we introduce time-local online algorithms. These are online
algorithms where the output at a given time only depends on $T = O(1)$ latest
inputs. The use of (deterministic) time-local algorithms in a distributed
setting automatically leads to globally consistent decisions.
</p>
<p>Our key observation is that time-local online algorithms (in which the output
at a given time only depends on local inputs in the temporal dimension) are
closely connected to local distributed graph algorithms (in which the output of
a given node only depends on local inputs in the spatial dimension). This makes
it possible to interpret prior work on distributed graph algorithms from the
perspective of online algorithms.
</p>
<p>We describe an algorithm synthesis method that one can use to design optimal
time-local online algorithms for small values of $T$. We demonstrate the power
of the technique in the context of a variant of the online file migration
problem, and show that e.g. for two nodes and unit migration costs there exists
a $3$-competitive time-local algorithm with horizon $T=4$, while no
deterministic online algorithm (in the classic sense) can do better. We also
derive upper and lower bounds for a more general version of the problem; we
show that there is a $6$-competitive deterministic time-local algorithm and a
$2.62$-competitive randomized time-local algorithm for any migration cost
$\alpha \ge 1$.
</p></div>
    </summary>
    <updated>2021-02-20T22:42:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09384</id>
    <link href="http://arxiv.org/abs/2102.09384" rel="alternate" type="text/html"/>
    <title>Buffered Streaming Graph Partitioning</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Faraj:Marcelo_Fonseca.html">Marcelo Fonseca Faraj</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schulz:Christian.html">Christian Schulz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09384">PDF</a><br/><b>Abstract: </b>Partitioning graphs into blocks of roughly equal size is a widely used tool
when processing large graphs. Currently there is a gap in the space of
available partitioning algorithms. On the one hand, there are streaming
algorithms that have been adopted to partition massive graph data on small
machines. In the streaming model, vertices arrive one at a time including their
neighborhood and then have to be assigned directly to a block. These algorithms
can partition huge graphs quickly with little memory, but they produce
partitions with low quality. On the other hand, there are offline
(shared-memory) multilevel algorithms that produce partitions with high quality
but also need a machine with enough memory to partition a network. In this
work, we make a first step to close this gap by presenting an algorithm that
computes high-quality partitions of huge graphs using a single machine with
little memory. First, we extend the streaming model to a more reasonable
approach in practice: the buffered streaming model. In this model, a PE can
store a batch of nodes (including their neighborhood) before making assignment
decisions. When our algorithm receives a batch of nodes, we build a model graph
that represents the nodes of the batch and the already present partition
structure. This model enables us to apply multilevel algorithms and in turn
compute high-quality solutions of huge graphs on cheap machines. To partition
the model, we develop a multilevel algorithm that optimizes an objective
function that has previously shown to be effective for the streaming setting.
Surprisingly, this also removes the dependency on the number of blocks from the
running time. Overall, our algorithm computes on average 55% better solutions
than Fennel using a very small batch size. In addition, our algorithm is
significantly faster than one of the main one-pass partitioning algorithms for
larger amounts of blocks.
</p></div>
    </summary>
    <updated>2021-02-20T22:42:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09299</id>
    <link href="http://arxiv.org/abs/2102.09299" rel="alternate" type="text/html"/>
    <title>Theory meets Practice: worst case behavior of quantile algorithms</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cormode:Graham.html">Graham Cormode</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mishra:Abhinav.html">Abhinav Mishra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ross:Joseph.html">Joseph Ross</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vesel=yacute=:Pavel.html">Pavel Veselý</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09299">PDF</a><br/><b>Abstract: </b>Estimating the distribution and quantiles of data is a foundational task in
data mining and data science. We study algorithms which provide accurate
results for extreme quantile queries using a small amount of space, thus
helping to understand the tails of the input distribution. Namely, we focus on
two recent state-of-the-art solutions: $t$-digest and ReqSketch. While
$t$-digest is a popular compact summary which works well in a variety of
settings, ReqSketch comes with formal accuracy guarantees at the cost of its
size growing as new observations are inserted. In this work, we provide insight
into which conditions make one preferable to the other. Namely, we show how to
construct inputs for $t$-digest that induce an almost arbitrarily large error
and demonstrate that it fails to provide accurate results even on i.i.d.
samples from a highly non-uniform distribution. We propose practical
improvements to ReqSketch, making it faster than $t$-digest, while its error
stays bounded on any instance. Still, our results confirm that $t$-digest
remains more accurate on the "non-adversarial" data encountered in practice.
</p></div>
    </summary>
    <updated>2021-02-20T22:38:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09294</id>
    <link href="http://arxiv.org/abs/2102.09294" rel="alternate" type="text/html"/>
    <title>Data Structures Lower Bounds and Popular Conjectures</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Pavel Dvořák, Michal Koucký, Karel Král, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sl=iacute=vov=aacute=:Veronika.html">Veronika Slívová</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09294">PDF</a><br/><b>Abstract: </b>In this paper, we investigate the relative power of several conjectures that
attracted recently lot of interest. We establish a connection between the
Network Coding Conjecture (NCC) of Li and Li and several data structure like
problems such as non-adaptive function inversion of Hellman and the
well-studied problem of polynomial evaluation and interpolation. In turn these
data structure problems imply super-linear circuit lower bounds for explicit
functions such as integer sorting and multi-point polynomial evaluation.
</p></div>
    </summary>
    <updated>2021-02-20T22:37:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09149</id>
    <link href="http://arxiv.org/abs/2102.09149" rel="alternate" type="text/html"/>
    <title>Classically Verifiable (Dual-Mode) NIZK for QMA with Preprocessing</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morimae:Tomoyuki.html">Tomoyuki Morimae</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yamakawa:Takashi.html">Takashi Yamakawa</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09149">PDF</a><br/><b>Abstract: </b>We propose three constructions of classically verifiable non-interactive
proofs (CV-NIP) and non-interactive zero-knowledge proofs and arguments
(CV-NIZK) for QMA in various preprocessing models.
</p>
<p>- We construct an information theoretically sound CV-NIP for QMA in the
secret parameter model where a trusted party generates a quantum proving key
and classical verification key and gives them to the corresponding parties
while keeping it secret from the other party. Alternatively, we can think of
the protocol as one in a model where the verifier sends an instance-independent
quantum message to the prover as preprocessing.
</p>
<p>- We construct a CV-NIZK for QMA in the secret parameter model. It is
information theoretically sound and zero-knowledge.
</p>
<p>- Assuming the quantum hardness of the leaning with errors problem, we
construct a CV-NIZK for QMA in a model where a trusted party generates a CRS
and the verifier sends an instance-independent quantum message to the prover as
preprocessing. This model is the same as one considered in the recent work by
Coladangelo, Vidick, and Zhang (CRYPTO '20). Our construction has the so-called
dual-mode property, which means that there are two computationally
indistinguishable modes of generating CRS, and we have information theoretical
soundness in one mode and information theoretical zero-knowledge property in
the other. This answers an open problem left by Coladangelo et al, which is to
achieve either of soundness or zero-knowledge information theoretically. To the
best of our knowledge, ours is the first dual-mode NIZK for QMA in any kind of
model.
</p></div>
    </summary>
    <updated>2021-02-20T22:37:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09127</id>
    <link href="http://arxiv.org/abs/2102.09127" rel="alternate" type="text/html"/>
    <title>FrugalMCT: Efficient Online ML API Selection for Multi-Label Classification Tasks</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Lingjiao.html">Lingjiao Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zaharia:Matei.html">Matei Zaharia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zou:James.html">James Zou</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09127">PDF</a><br/><b>Abstract: </b>Multi-label classification tasks such as OCR and multi-object recognition are
a major focus of the growing machine learning as a service industry. While many
multi-label prediction APIs are available, it is challenging for users to
decide which API to use for their own data and budget, due to the heterogeneity
in those APIs' price and performance. Recent work shows how to select from
single-label prediction APIs. However the computation complexity of the
previous approach is exponential in the number of labels and hence is not
suitable for settings like OCR. In this work, we propose FrugalMCT, a
principled framework that adaptively selects the APIs to use for different data
in an online fashion while respecting user's budget. The API selection problem
is cast as an integer linear program, which we show has a special structure
that we leverage to develop an efficient online API selector with strong
performance guarantees. We conduct systematic experiments using ML APIs from
Google, Microsoft, Amazon, IBM, Tencent and other providers for tasks including
multi-label image classification, scene text recognition and named entity
recognition. Across diverse tasks, FrugalMCT can achieve over 90% cost
reduction while matching the accuracy of the best single API, or up to 8%
better accuracy while matching the best API's cost.
</p></div>
    </summary>
    <updated>2021-02-20T22:41:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09101</id>
    <link href="http://arxiv.org/abs/2102.09101" rel="alternate" type="text/html"/>
    <title>No-Substitution $k$-means Clustering with Low Center Complexity and Memory</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhattacharjee:Robi.html">Robi Bhattacharjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Imola:Jacob.html">Jacob Imola</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09101">PDF</a><br/><b>Abstract: </b>Clustering is a fundamental task in machine learning. Given a dataset $X =
\{x_1, \ldots x_n\}$, the goal of $k$-means clustering is to pick $k$ "centers"
from $X$ in a way that minimizes the sum of squared distances from each point
to its nearest center. We consider $k$-means clustering in the online, no
substitution setting, where one must decide whether to take $x_t$ as a center
immediately upon streaming it and cannot remove centers once taken.
</p>
<p>The online, no substitution setting is challenging for clustering--one can
show that there exist datasets $X$ for which any $O(1)$-approximation $k$-means
algorithm must have center complexity $\Omega(n)$, meaning that it takes
$\Omega(n)$ centers in expectation. Bhattacharjee and Moshkovitz (2020) refined
this bound by defining a complexity measure called $Lower_{\alpha, k}(X)$, and
proving that any $\alpha$-approximation algorithm must have center complexity
$\Omega(Lower_{\alpha, k}(X))$. They then complemented their lower bound by
giving a $O(k^3)$-approximation algorithm with center complexity
$\tilde{O}(k^2Lower_{k^3, k}(X))$, thus showing that their parameter is a tight
measure of required center complexity. However, a major drawback of their
algorithm is its memory requirement, which is $O(n)$. This makes the algorithm
impractical for very large datasets.
</p>
<p>In this work, we strictly improve upon their algorithm on all three fronts;
we develop a $36$-approximation algorithm with center complexity
$\tilde{O}(kLower_{36, k}(X))$ that uses only $O(k)$ additional memory. In
addition to having nearly optimal memory, this algorithm is the first known
algorithm with center complexity bounded by $Lower_{36, k}(X)$ that is a true
$O(1)$-approximation with its approximation factor being independent of $k$ or
$n$.
</p></div>
    </summary>
    <updated>2021-02-20T22:42:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09032</id>
    <link href="http://arxiv.org/abs/2102.09032" rel="alternate" type="text/html"/>
    <title>Consistent Lock-free Parallel Stochastic Gradient Descent for Fast and Stable Convergence</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=auml=ckstr=ouml=m:Karl.html">Karl Bäckström</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Walulya:Ivan.html">Ivan Walulya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Papatriantafilou:Marina.html">Marina Papatriantafilou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsigas:Philippas.html">Philippas Tsigas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09032">PDF</a><br/><b>Abstract: </b>Stochastic gradient descent (SGD) is an essential element in Machine Learning
(ML) algorithms. Asynchronous parallel shared-memory SGD (AsyncSGD), including
synchronization-free algorithms, e.g. HOGWILD!, have received interest in
certain contexts, due to reduced overhead compared to synchronous
parallelization. Despite that they induce staleness and inconsistency, they
have shown speedup for problems satisfying smooth, strongly convex targets, and
gradient sparsity. Recent works take important steps towards understanding the
potential of parallel SGD for problems not conforming to these strong
assumptions, in particular for deep learning (DL). There is however a gap in
current literature in understanding when AsyncSGD algorithms are useful in
practice, and in particular how mechanisms for synchronization and consistency
play a role. We focus on the impact of consistency-preserving non-blocking
synchronization in SGD convergence, and in sensitivity to hyper-parameter
tuning. We propose Leashed-SGD, an extensible algorithmic framework of
consistency-preserving implementations of AsyncSGD, employing lock-free
synchronization, effectively balancing throughput and latency. We argue
analytically about the dynamics of the algorithms, memory consumption, the
threads' progress over time, and the expected contention. We provide a
comprehensive empirical evaluation, validating the analytical claims,
benchmarking the proposed Leashed-SGD framework, and comparing to baselines for
training multilayer perceptrons (MLP) and convolutional neural networks (CNN).
We observe the crucial impact of contention, staleness and consistency and show
how Leashed-SGD provides significant improvements in stability as well as
wall-clock time to convergence (from 20-80% up to 4x improvements) compared to
the standard lock-based AsyncSGD algorithm and HOGWILD!, while reducing the
overall memory footprint.
</p></div>
    </summary>
    <updated>2021-02-20T22:45:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.09002</id>
    <link href="http://arxiv.org/abs/2102.09002" rel="alternate" type="text/html"/>
    <title>Impartial selection with prior information</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Caragiannis:Ioannis.html">Ioannis Caragiannis</a>, George Christodoulou, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Protopapas:Nicos.html">Nicos Protopapas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.09002">PDF</a><br/><b>Abstract: </b>We study the problem of {\em impartial selection}, a topic that lies at the
intersection of computational social choice and mechanism design. The goal is
to select the most popular individual among a set of community members. The
input can be modeled as a directed graph, where each node represents an
individual, and a directed edge indicates nomination or approval of a community
member to another. An {\em impartial mechanism} is robust to potential selfish
behavior of the individuals and provides appropriate incentives to voters to
report their true preferences by ensuring that the chance of a node to become a
winner does not depend on its outgoing edges. The goal is to design impartial
mechanisms that select a node with an in-degree that is as close as possible to
the highest in-degree. We measure the efficiency of such a mechanism by the
difference of these in-degrees, known as its {\em additive} approximation.
</p>
<p>In particular, we study the extent to which prior information on voters'
preferences could be useful in the design of efficient deterministic impartial
selection mechanisms with good additive approximation guarantees. We consider
three models of prior information, which we call the {\em opinion poll}, the
{\em a prior popularity}, and the {\em uniform} model. We analyze the
performance of a natural selection mechanism that we call {\em approval voting
with default} (AVD) and show that it achieves a $O(\sqrt{n\ln{n}})$ additive
guarantee for opinion poll and a $O(\ln^2n)$ for a priori popularity inputs,
where $n$ is the number of individuals. We consider this polylogarithmic bound
as our main technical contribution. We complement this last result by showing
that our analysis is close to tight, showing an $\Omega(\ln{n})$ lower bound.
This holds in the uniform model, which is the simplest among the three models.
</p></div>
    </summary>
    <updated>2021-02-20T22:44:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.03277</id>
    <link href="http://arxiv.org/abs/2102.03277" rel="alternate" type="text/html"/>
    <title>Minimum projective linearizations of trees in linear time</title>
    <feedworld_mtime>1613779200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alemany=Puig:Llu=iacute=s.html">Lluís Alemany-Puig</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esteban:Juan_Luis.html">Juan Luis Esteban</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferrer=i=Cancho:Ramon.html">Ramon Ferrer-i-Cancho</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.03277">PDF</a><br/><b>Abstract: </b>The minimum linear arrangement problem (MLA) consists of finding a mapping
$\pi$ from vertices of a graph to integers that minimizes $\sum_{uv\in
E}|\pi(u) - \pi(v)|$. For trees, various algorithms are available to solve the
problem in polynomial time; the best known runs in subquadratic time in
$n=|V|$. There exist variants of the MLA in which the arrangements are
constrained to certain classes of projectivity. Iordanskii, and later Hochberg
and Stallmann (HS), put forward $O(n)$-time algorithms that solve the problem
when arrangements are constrained to be planar. We also consider linear
arrangements of rooted trees that are constrained to be projective. Gildea and
Temperley (GT) sketched an algorithm for the projectivity constraint which, as
they claimed, runs in $O(n)$ but did not provide any justification of its cost.
In contrast, Park and Levy claimed that GT's algorithm runs in $O(n \log
d_{max})$ where $d_{max}$ is the maximum degree but did not provide sufficient
detail. Here we correct an error in HS's algorithm for the planar case, show
its relationship with the projective case, and derive an algorithm for the
projective case that runs undoubtlessly in $O(n)$-time.
</p></div>
    </summary>
    <updated>2021-02-20T22:40:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/017</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/017" rel="alternate" type="text/html"/>
    <title>TR21-017 |  Mixing in non-quasirandom groups | 

	Timothy Gowers, 

	Emanuele Viola</title>
    <summary>We initiate a systematic study of mixing in non-quasirandom groups.
Let $A$ and $B$ be two independent, high-entropy distributions over
a group $G$. We show that the product distribution $AB$ is statistically
close to the distribution $F(AB)$ for several choices of $G$ and
$F$, including:

(1) $G$ is the affine group of $2\times2$ matrices, and $F$ sets
the top-right matrix entry to a uniform value,

(2) $G$ is the lamplighter group, that is the wreath product of $\Z_{2}$
and $\Z_{n}$, and $F$ is multiplication by a certain subgroup,

(3) $G$ is $H^{n}$ where $H$ is non-abelian, and $F$ selects a
uniform coordinate and takes a uniform conjugate of it.

The obtained bounds for (1) and (2) are tight.

This work is motivated by and applied to problems in communication
complexity. We consider the 3-party communication problem of deciding
if the product of three group elements multiplies to the identity.
We prove lower bounds for the groups above, which are tight for the
affine and the lamplighter groups.</summary>
    <updated>2021-02-19T19:59:56Z</updated>
    <published>2021-02-19T19:59:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T21:20:54Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings</id>
    <link href="https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings.html" rel="alternate" type="text/html"/>
    <title>Loops, degrees, and matchings</title>
    <summary>A student in my graph algorithms class asked how self-loops in undirected graphs affect the vertex degrees and matchings of a graph. The standard answer is that a self-loop adds two to the degree (because each edge has two endpoints) and that they are useless in matching because matchings should have at most one incidence to each vertex, not two. But that’s just a convention; one could reasonably declare that the contribution of a self-loop to the degree is one, and I’m pretty sure I’ve seen sources that do just that. With that alternative convention, it should be possible to include a self-loop in a matching, and use it to match only a single vertex.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A student in my graph algorithms class asked how <a href="https://11011110.github.io/blog/2021/02/19/Loop (graph theory)">self-loops</a> in undirected graphs affect the vertex degrees and matchings of a graph. The standard answer is that a self-loop adds two to the degree (because each edge has two endpoints) and that they are useless in matching because matchings should have at most one incidence to each vertex, not two. But that’s just a convention; one could reasonably declare that the contribution of a self-loop to the degree is one, and I’m pretty sure I’ve seen sources that do just that. With that alternative convention, it should be possible to include a self-loop in a matching, and use it to match only a single vertex.</p>

<p>However, this turns out not to make much difference to many matching problems, because the following simple transformation turns a problem with self-loops (allowed in matchings in this way) into a problem with no self-loops (so it doesn’t matter whether they are allowed or not). Simply form a <a href="https://en.wikipedia.org/wiki/Covering_graph">double cover</a>\(^*\) of the given graph (let’s call it the “loopless double cover”) by making two copies of the graph and replacing all corresponding pairs of loops by simple edges from one copy to the other. In weighted matching problems, give the replacement edges for the loops the sum of the weights of the two loops they replace; all other edges keep their original weights.</p>

<p style="text-align: center;"><img alt="The loopless double cover of a graph and of one of its loopy matchings" src="https://11011110.github.io/blog/assets/2021/loopless-double-cover.svg"/></p>

<p>Then (unlike the <a href="https://en.wikipedia.org/wiki/Bipartite_double_cover">bipartite double cover</a>, which also eliminates loops) the cardinality or optimal weight of a matching in the loopy graph can be read off from the corresponding solution in its loopless double cover. Any matching of the original loopy graph can be translated into a matching of the loopless cover by applying the same loopless cover translation to the matching instead of to the whole graph; this doubles the total weight of the matching and the total number of matched vertices. And among matchings on the loopless cover, when trying to optimize weight or matched vertices, it is never helpful to match the two copies differently, so there is an optimal solution that can be translated back to the original graph without changing its optimality.</p>

<p>This doesn’t quite work for the problem of finding a matching that maximizes the total number of matched edges, rather than the total number of matched vertices. These two problems are the same in simple graphs, but different in loopy graphs. However, in a loopy graph, if you are trying to maximize matched edges, you might as well include all loops in the matching, and then search for a maximum matching of the simple graph induced by the remaining unmatched vertices. Again, in this case, you don’t get a problem that requires any new algorithms to solve it.</p>

<p>In the case of my student, I only provided the conventional answer, because really all they wanted to know was whether these issues affected how they answered one of the homework questions, and the answer was that the question didn’t involve and didn’t need loops. However it seems that the more-complicated answer is that even if you allow loops to count only one unit towards degree, and to be included in matchings, they don’t change the matching problem much.</p>

<p>\(^*\) This is only actually a covering graph under the convention that the degree of a loop is one. For the usual degree-2 convention for loops, you would need to replace each loop by a pair of parallel edges, forming a multigraph, to preserve the degrees of the vertices.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105762400402127534">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-02-19T18:57:00Z</updated>
    <published>2021-02-19T18:57:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-02-20T07:30:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21277</id>
    <link href="https://gilkalai.wordpress.com/2021/02/19/nostalgia-corner-john-riordans-referee-report-of-my-first-paper/" rel="alternate" type="text/html"/>
    <title>Nostalgia corner: John Riordan’s referee report of my first paper</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In 1971/1972 academic year, I was an undergraduate student at the Hebrew University of Jerusalem and toward the end of the year I wrote a paper about Abel’s sums. I sent it to John Riordan the author of the books  … <a href="https://gilkalai.wordpress.com/2021/02/19/nostalgia-corner-john-riordans-referee-report-of-my-first-paper/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In 1971/1972 academic year, I was an undergraduate student at the Hebrew University of Jerusalem and toward the end of the year I wrote a paper about Abel’s sums. I sent it to John Riordan the author of the books  “Combinatorial Identities” and “Combinatorial Analysis”.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan1.png"><img alt="" class="alignnone size-full wp-image-21279" height="834" src="https://gilkalai.files.wordpress.com/2021/02/riordan1.png?w=640&amp;h=834" width="640"/></a></p>
<p>I was surely very happy to read the sentence <span style="color: #0000ff;">“I think you have had a splendid idea”</span>.  Here is part of Riordan’s remarks. The full report is <a href="https://gilkalai.files.wordpress.com/2021/02/riordan-rep1.pdf">here</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan3.png"><img alt="" class="alignnone size-full wp-image-21280" height="827" src="https://gilkalai.files.wordpress.com/2021/02/riordan3.png?w=640&amp;h=827" width="640"/></a></p>
<p>It took me some time to revise the paper and get it printed. And here is the report for the second version.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan2.png"><img alt="" class="alignnone size-full wp-image-21281" height="839" src="https://gilkalai.files.wordpress.com/2021/02/riordan2.png?w=640&amp;h=839" width="640"/></a></p>
<p>And here is part of Riordan’s second round of remarks. The full report is <a href="https://gilkalai.files.wordpress.com/2021/02/riordan-rep2.pdf">here</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan4.png"><img alt="" class="alignnone size-full wp-image-21282" height="843" src="https://gilkalai.files.wordpress.com/2021/02/riordan4.png?w=640&amp;h=843" width="640"/></a></p>
<p>I was certainly happy to read the following sentence: <span style="color: #0000ff;">“I would remark that the result for  <em>p = -1 </em> is new and perhaps <span style="color: #ff0000;">the simplest derivation of Abel’s result</span>.”</span></p>
<p>In 1978 I actually visited John Riordan in his office at Rockefeller University, NYC. I remember him as very cheerful and he told me that when his first book appeared he was working at Bell Labs and his managers wanted to talk to him. He was a bit worried that they would not approve of him spending time and effort to write a book in pure mathematics. But actually, they gave him a salary raise!</p>
<p>(If you have a picture of John Riordan, please send me.)</p>
<p>In 1979 the paper <a href="https://gilkalai.files.wordpress.com/2021/02/1-s2.0-0097316579900475-main-1.pdf">appeared</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan5.png"><img alt="" class="alignnone size-full wp-image-21286" src="https://gilkalai.files.wordpress.com/2021/02/riordan5.png?w=640"/></a></p></div>
    </content>
    <updated>2021-02-19T09:23:53Z</updated>
    <published>2021-02-19T09:23:53Z</published>
    <category term="Combinatorics"/>
    <category term="personal"/>
    <category term="Abel sums"/>
    <category term="John Riordan"/>
    <category term="Niels Henrik Abel"/>
    <category term="refereeing"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-02-21T21:20:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18125</id>
    <link href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/" rel="alternate" type="text/html"/>
    <title>Computing’s Role In The Pandemic</title>
    <summary>How can we help? Joe Biden is the 46th president of the USA. Note is called a centered triangular number. These numbers obey the formula: and start with The previous one, the , was Herbert Hoover, hmmm. Biden has promised to make controlling the Covid-19 pandemic one of his top priorities. Today I thought we […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>How can we help?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/unknown-145/" rel="attachment wp-att-18132"><img alt="" class="alignright size-full wp-image-18132" src="https://rjlipton.files.wordpress.com/2021/02/unknown.jpeg?w=600"/></a></p>
<p>
Joe Biden is the 46th president of the USA. Note <img alt="{46}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B46%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{46}"/> is called a <a href="https://en.wikipedia.org/wiki/Centered_triangular_number">centered triangular number</a>. These numbers obey the formula: 	</p>
<p align="center"><img alt="\displaystyle  \frac{3n^2 + 3n + 2}{2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B3n%5E2+%2B+3n+%2B+2%7D%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \frac{3n^2 + 3n + 2}{2} "/></p>
<p>and start with <img alt="{1,4,10,19,31,46,\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2C4%2C10%2C19%2C31%2C46%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1,4,10,19,31,46,\dots}"/> The previous one, the <img alt="{31^{st}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B31%5E%7Bst%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{31^{st}}"/>, was Herbert Hoover, hmmm. Biden has promised to make controlling the Covid-19 <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic">pandemic</a> one of his top priorities. </p>
<p>
Today I thought we would discuss how he might use computer technology to help get the virus under control. </p>
<p>
First, we thank the drug companies since we now have <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States#Vaccines">vaccines</a> that work against the virus. Without these we would have little chance to bring the pandemic under control at all. </p>
<p>
Second, we must state that we are worried that the virus is mutating and this may render the current vaccines less useful, if not useless. We hope this is not happening, or that the drug companies will be able to respond with vaccine boosters. Today there seems to be <a href="https://www.usnews.com/news/health-news/articles/2021-02-18/pfizer-coronavirus-vaccine-protects-against-uk-south-africa-variants-study-shows">good news</a> and <a href="https://www.reuters.com/article/us-health-coronavirus-vaccines-variants/pfizer-says-south-african-variant-could-significantly-reduce-protective-antibodies-idUSKBN2AH2VG">bad news</a>.  </p>
<p>
Results will fluctuate, but in any case, vaccines will definitely play a key role in defeating the pandemic. We want to ask the same about computing technology.</p>
<p>
</p><p/><h2> Computing’s Role—I </h2><p/>
<p/><p>
There are many web sites that discuss how computing technology can play a role in defeating the pandemic. Here are some of the main points:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Tracking People:</i> Many places are interested in tracking who are sick. Tracking can by itself help stop the spreading of the virus, and thus help save lives. For example, <a href="https://www.computer.org/publications/tech-news/five-ways-tech-is-being-used-to-fight-covid-19">IEEE</a> says: </p>
<blockquote><p><b> </b> <em> “We believe software can help combat this global pandemic, and that’s why we’re launching the Code Against COVID-19 initiative…,” said Weiting Liu, founder and CEO of Arc. “From tracking outbreaks and reducing the spread to scaling testing and supporting healthcare, teams around the world are using software to flatten the curve. The eMask app (real-time mask inventory in Taiwan) and TraceTogether (contact tracing in Singapore) are just two of the many examples.” </em>
</p></blockquote>
<p/><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Changing Behavior:</i> A powerful idea is to avoid human to human contact and thus stop the spread of the virus. For example, here are <a href="https://www.weforum.org/agenda/2020/04/10-technology-trends-coronavirus-covid19-pandemic-robotics-telehealth">examples</a> from a longer list of ideas: </p>
<ul>
<li>
Robot Deliveries; <p/>
</li><li>
Digital and Contactless Payments; <p/>
</li><li>
Remote Work and Remote Learning and more.
</li></ul>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Changing Health Delivery:</i> An important idea is how can we reduce the risk of health delivery. A paradox is that health care may need to be avoided, since traditional delivery requires human contact. There are many examples of ways to make health care online, and therefore safer. Shwetak Patel won the 2018 ACM Prize in Computing for contributions to creative and practical sensing systems for sustainability and health. He outlined here <a href="https://cccblog.org/2020/09/24/what-role-can-computing-play-in-battling-the-covid-19-pandemic/">CCC blog</a> how health care could be made more online.</p>
<p>
</p><p/><h2> Computing’s Role—II </h2><p/>
<p/><p>
The above ideas are fine but I believe the real role for computing is simple: </p>
<blockquote><p><b> </b> <em> <i>Make signing up and obtaining an appointment for a vaccine easier, fairer, and sooner.</i> </em>
</p></blockquote>
<p/><p>
In the US each state is in charge of running web sites that allow people to try and get an appointment for a vaccine shot. <i>Try</i> is the key word. Almost all sites require an appointment to get a shot—walk-ins are mostly not allowed. </p>
<p>
I cannot speak for all states and all web sites, but my direct experience is that the sites are terrible. Signing up for a vaccination shot is a disaster. The web sites that I have seen are poorly written, clumsy, and difficult to use. They are some of the worst sites I have ever needed to use, for anything. Some of the top issues: </p>
<ol>
<li>
The sites require you to sign in each time from scratch. <p/>
</li><li>
The sites require you to sign in each time from scratch. <p/>
</li><li>
The sites require you to sign in each time from scratch. <p/>
</li><li>
The sites rules are confusing and unclear. <p/>
</li><li>
You may need to search for particular vaccine locations, rather than for any locations. <p/>
</li><li>
And more <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\dots}"/>
</li></ol>
<p>Repeating (1,2,3) is a poor joke, but one that reflects reality. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
If Amazon, Google, Apple had sites that worked this way, they would be out of business quickly. Perhaps this is the key: <i>Can our top companies help build the state sites?</i> Is it too late to help? See <a href="https://www.nytimes.com/2021/01/12/technology/the-problem-with-vaccine-websites.html">here</a> for a New York Times article on this issue: </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/game/" rel="attachment wp-att-18129"><img alt="" class="aligncenter wp-image-18129" src="https://rjlipton.files.wordpress.com/2021/02/game.png?w=300" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<blockquote><p><b> </b> <em> When you start to pull your hair out because you can’t register for a vaccine on a local website, remember that it’s not (only) the fault of a bad tech company or misguided choices by government leaders today. It’s a systematic failure years in the making. </em>
</p></blockquote>
<p/><p>
Also is the issue of <a href="https://medium.com/berkeleyischool/fairness-in-the-age-of-algorithms-feb11c56a709">algorithmic fairness</a> relevant here? We know that it is unfortunately easy to have web sites that are unfair—that assign vaccine sign up dates unfairly, that favor one class of people over another. </p>
<p/></font></font></div>
    </content>
    <updated>2021-02-19T03:03:24Z</updated>
    <published>2021-02-19T03:03:24Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="19"/>
    <category term="covid"/>
    <category term="states"/>
    <category term="vaccine"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-02-21T21:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5347</id>
    <link href="https://www.scottaaronson.com/blog/?p=5347" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5347#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5347" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Brief thoughts on the Texas catastrophe</title>
    <summary xml:lang="en-US">This past week, I spent so much mental energy worrying about the fate of Scott Alexander that I almost forgot that right here in Texas, I’m surrounded by historic scenes of Third-World-style devastation: snowstorms and sub-freezing temperatures for which our infrastructure was completely unprepared; impassable roads; burst gas and water pipes; millions without electricity or […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>This past week, I spent so much mental energy worrying about the fate of Scott Alexander that I almost forgot that right here in Texas, I’m surrounded by historic scenes of Third-World-style devastation: snowstorms and sub-freezing temperatures for which our infrastructure was completely unprepared; impassable roads; burst gas and water pipes; millions without electricity or heat or clean water; the UT campus a short walk from me converted into a giant refugee camp.</p>



<p>For all those who asked: my family and I are fine.  While many we know were without power for days (or are <em>still</em> without power), we lucked out by living close to a hospital, which means that they can’t shut off the electricity to our block.  We <em>are</em> now on a boil-water notice, like all of Austin, and we can’t take deliveries or easily go anywhere, and the university and schools and daycares are all closed (even for remote learning).  Which means: we’re simply holed up in our house, eating through our stockpiled food, the kids running around being crazy, Dana and I watching them with one eye and our laptops with the other.  Could be worse.</p>



<p>In some sense, it’s not surprising that the Texas infrastructure would buckle under weather stresses outside the envelope of anything it was designed for or saw for decades.  The central problem is that our elected leaders have shown zero indication of understanding the urgent need, for Texas’ economic viability, to do whatever it takes to make sure nothing like this ever happens again.  Ted Cruz, as everyone now knows, left for Cancun; the mayor of Colorado City angrily told everyone to fend for themselves (and then resigned); and Governor Abbott has been blaming frozen wind turbines, a tiny percentage of the problem (frozen gas pipes are a much bigger issue) but one that plays with the base.  The bare minimum of a sane response might be, I dunno,</p>



<ul><li>acknowledging the reality that climate change means that “once-per-century” weather events will be every couple years from now on,</li><li>building spare capacity (nuclear would be ideal … well, I can dream),</li><li>winterizing what we have now, and</li><li>connecting the Texas grid to the rest of the US.</li></ul>



<p>If I were a Texas Democrat, I’d consider making Republican incompetence on infrastructure, utilities, and public health my <em>only</em> campaign issues.</p>



<p>Alright, now back to watching the Mars lander, which is apparently easier to build and deploy than a reliable electric grid.</p></div>
    </content>
    <updated>2021-02-18T21:40:22Z</updated>
    <published>2021-02-18T21:40:22Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Adventures in Meatspace"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-02-19T21:03:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4494</id>
    <link href="https://lucatrevisan.wordpress.com/2021/02/18/this-year-for-lent-we-realized-it-has-been-lent-all-along/" rel="alternate" type="text/html"/>
    <title>This year, for Lent, we realized it has been Lent all along</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Yesterday was Ash Wednesday, the beginning of Lent, the 40-day period that precedes Easter and that is observed by Catholics and other Christians as a period of reflection. It is, often, a period in which the faithful choose to give … <a href="https://lucatrevisan.wordpress.com/2021/02/18/this-year-for-lent-we-realized-it-has-been-lent-all-along/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Yesterday was Ash Wednesday, the beginning of Lent, the 40-day period that precedes Easter and that is observed by Catholics and other Christians as a period of reflection. It is, often, a period in which the faithful choose to give something up as a penance, such as giving up eating meat.</p>



<p>The period that immediately precedes Lent is known as Carnival, and, perhaps incongruously, it is a time for having fun, playing pranks, and eating special sweets, often deep-fried ones. Traditionally kids, and also grownups, dress up in costumes and attend costume parties. The idea being, let’s have fun and eat now, because soon we are “entirely voluntarily”  going to fast and to reflect on sin and death, and stuff like that. The day before Ash Wednesday, indeed, is called “Fat Tuesday”.</p>



<p>In Milan, however, the tradition is to power through Ash Wednesday and to continue the Carnival festivities until the following Sunday. There are a number of legends that explain this unique tradition, that is apparently ancient. One such legend is that a plague epidemic had been ravaging Milan in the IV century around the time that should have been Carnival, and life was beginning to go back to normal right around Ash Wednesday. So people rebelled against Lent, and were like, haven’t we suffered enough, what more penance do we need, and celebrated Carnival later.</p>



<p>It has now been nearly a year since the first lockdown, and we still cannot travel between regions (for example, we cannot travel from Milan to Bologna, or to Venice), cannot eat dinner in a restaurant, cannot go see a movie, a play or a sporting event, cannot ski, and so on.</p>



<p>My proposal is that when (if?) we go back to a normal life, we shorten Lent to three days (start with “Ash Thursday” the day before Good Friday), and that we make Carnival  start on Easter Monday and last for 361 days. Not because we have had it worse than a IV century plague epidemic: indeed, even in the best of times, IV century people in Milan did not usually eat in restaurants, travel to Venice, see movies, or ski. We, however, are spoiled XXI century people, we are not used to inconveniences, and when (if?) this is over we will need a lot of self-care, especially the eating-deep-fried-sweets-and-partying kind of self-care.</p></div>
    </content>
    <updated>2021-02-18T13:28:07Z</updated>
    <published>2021-02-18T13:28:07Z</published>
    <category term="Milan"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2021-02-21T21:20:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/18/postdoc-at-imperial-college-london-in-complexity-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/18/postdoc-at-imperial-college-london-in-complexity-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at Imperial College London in Complexity  (apply by March 31, 2021) at Imperial College London (apply by March 31, 2021)</title>
    <summary>The Complexity group of Iddo Tzameret at Imperial College London invites expressions of interest for a postdoctoral position funded by the ERC. The position is for two years with a possible one-year extension. The start date is flexible, and the salary is generous and includes funding for equipment and travel. This position will be based […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Complexity group of Iddo Tzameret at Imperial College London invites expressions of interest for a postdoctoral position funded by the ERC. The position is for two years with a possible one-year extension. The start date is flexible, and the salary is generous and includes funding for equipment and travel. This position will be based at the South Kensington campus at the heart of London.</p>
<p>Website: <a href="https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html">https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html</a><br/>
Email: iddo.tzameret@gmail.com</p></div>
    </content>
    <updated>2021-02-18T11:52:18Z</updated>
    <published>2021-02-18T11:52:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-02-21T21:21:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/18/phd-positions-at-imperial-college-london-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/18/phd-positions-at-imperial-college-london-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/" rel="alternate" type="text/html"/>
    <title>PhD positions at Imperial College London (apply by March 31, 2021) at Imperial College London (apply by March 31, 2021)</title>
    <summary>Imperial’s Computing is seeking up to two highly motivated PhD students interested in computational complexity. The positions are based at the South Kensington campus at the heart of London, and include a generous stipend, as well as funding for equipment and travel. The successful candidate will join the complexity group at Imperial College led by […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Imperial’s Computing is seeking up to two highly motivated PhD students interested in computational complexity. The positions are based at the South Kensington campus at the heart of London, and include a generous stipend, as well as funding for equipment and travel. The successful candidate will join the complexity group at Imperial College led by Iddo Tzameret.</p>
<p>Website: <a href="https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html">https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html</a><br/>
Email: iddo.tzameret@gmail.com</p></div>
    </content>
    <updated>2021-02-18T11:51:55Z</updated>
    <published>2021-02-18T11:51:55Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-02-21T21:21:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7992</id>
    <link href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/" rel="alternate" type="text/html"/>
    <title>What do deep networks learn and when do they learn it</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Scribe notes by Manos Theodosis Previous post: A blitz through statistical learning theory Next post: TBD. See also all seminar posts and course webpage. Lecture video – Slides (pdf) – Slides (powerpoint with ink and animation) In this lecture, we talk about what neural networks end up learning (in terms of their weights) and when, … <a class="more-link" href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/">Continue reading <span class="screen-reader-text">What do deep networks learn and when do they learn it</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Scribe notes by <a href="https://manosth.github.io/">Manos Theodosis</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">A blitz through statistical learning theory</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c518b9e4-5f63-4278-871d-acc2017b8984">Lecture video</a> – <a href="https://boazbk.github.io/mltheoryseminar/lectures/seminar_lecture2.pdf">Slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_2.pptx">Slides (powerpoint with ink and animation)</a></p>



<p>In this lecture, we talk about <em>what</em> neural networks end up learning (in terms of their weights) and <em>when</em>, during training, they learn it.</p>



<p>In particular, we’re going to discuss</p>



<ul><li><strong>Simplicity bias</strong>: how networks favor “simple” features first.</li><li><strong>Learning dynamics</strong>: what is learned early in training.</li><li><strong>Different layers</strong>: do the different layers learn the same features?</li></ul>



<p>The type of results we will discuss are:</p>



<ul><li>Gradient-based deep learning algorithms have a bias toward learning simple classifiers. In particular this often holds when the optimization problem they are trying to solve is “underconstrained/overparameterized”, in the sense that there are exponentially many different models that fit the data.</li><li>Simplicity also affects the <em>timing</em> of learning. Deep learning algorithms tend to learn simple (but still predictive!) features first.</li><li>Such “simple predictive features” tend to be in lower (closer to input) levels of the network. Hence deep learning also tends to learn lower levels earlier.</li><li>On the other side, the above means that distributions that do not have “simple predictive features” pose significant challenges for deep learning. Even if there is a small neural network that works very well for the distribution, gradient-based algorithms will not “get off the ground” in such cases. We will see a lower bound for <em>learning parities</em> that makes this intuition formal.</li></ul>



<h2>What do neural networks learn, and when do they learn it?</h2>



<p>As a first example to showcase what is learned by neural networks, we’ll consider the following data distribution where we sample points <img alt="(X, Y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28X%2C+Y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(X, Y)"/>, with <img alt="Y \in {1, -1}" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%5Cin+%7B1%2C+-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Y \in {1, -1}"/> (<img alt="Y = 1" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Y = 1"/> corresponding to orange points and <img alt="Y=-1" class="latex" src="https://s0.wp.com/latex.php?latex=Y%3D-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Y=-1"/> corresponding to blue points).<br/><img alt="" src="https://i.imgur.com/xiXoqDj.png"/></p>



<p>If we <a href="http://tfmeter.icsi.berkeley.edu/#activation=tanh&amp;batchSize=10&amp;dataset=byod&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;trueLearningRate=0&amp;regularizationRate=0&amp;noise=35&amp;networkShape=10,6,4,2&amp;seed=0.36834921&amp;showTestData=false&amp;discretize=false&amp;percTrainData=74&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">train</a> a neural network to fit this distribution, we can see below that the neurons that are closest to the input data end up learning features that are highly correlated with the input (mostly linear subspaces at 45-degree angle, which correspond to one of the stripes). In the subsequent layers, the features learned are more sophisticated and have increased complexity.<br/><img alt="" src="https://i.imgur.com/p2FQdUd.png"/><br/></p>



<h3>Neural networks have simpler but useful features in lower layers</h3>



<p>Some people have spent a lot of time trying to understand what is learned by different layers. In a <a href="https://distill.pub/2020/circuits/early-vision/">recent</a> work, Olah et al. dig deep into a particular architecture for computer vision, trying to interpret the features learned by neurons at different layers.</p>



<p>They found that earlier layers learn features that resemble edge detectors.<br/><img alt="" src="https://i.imgur.com/I8veBVf.png"/><br/>However, as we go deeper, the neurons at those layers start learning more convoluted (for example, these features from layer <img alt="3b" class="latex" src="https://s0.wp.com/latex.php?latex=3b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="3b"/> resemble heads).<br/><img alt="" src="https://i.imgur.com/rJsmKHR.png"/></p>



<h3>SGD learns simple (but still predictive) features earlier.</h3>



<p>There is evidence that <a href="https://arxiv.org/abs/1905.11604">SGD learns simpler classifiers first</a>. The following figure tracks how much of a learned classifier’s performance can be accounted for by a linear classifier. We see that up to a certain point in training <em>all</em> of the performance of the neural network learned by SGD (measured as mutual information with the label or as accuracy) can be ascribed to the linear classifier. They diverge only very near the point where the linear classifier “saturates,” in the sense that the classifier reachers the best possible accuracy for linear models. (We use the quantity <img alt="I(f(x);y |L(x))" class="latex" src="https://s0.wp.com/latex.php?latex=I%28f%28x%29%3By+%7CL%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I(f(x);y |L(x))"/> – the mutual information of <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/> conditioned on the prediction of the linear classifier <img alt="L(x)" class="latex" src="https://s0.wp.com/latex.php?latex=L%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(x)"/> – to measure how much of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>‘s performance <em>cannot</em> be accounted for by <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L"/>.)</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/pGxap3e.png"/></figure>



<h3>The benefits and pitfalls of simplicity bias</h3>



<p>In general, simplicity bias is a very good thing. For example, the most “complex” function is a random function. However, if given some observed data <img alt="{ (x_i,y_i)}_{i\in [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29%7D_%7Bi%5Cin+%5Bn%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ (x_i,y_i)}_{i\in [n]}"/>, SFD were to find a random function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> that perfectly fits it, then it would never generalize (since for every fresh <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, the value of <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)"/> would be random).</p>



<p>At the same time, simplicity bias means that our algorithms might focus too much on simple solutions and miss more complex ones. Sometimes the complex solutions actually do perform better. In the following cartoon a person could go to the low-hanging fruit tree on the right-hand side and miss the bigger rewards on the left-hand side.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/eijQfpl.png"/></figure>



<p>This <a href="https://arxiv.org/abs/2006.07710">can actually happen</a> in neural networks. We also saw a simple example in class:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/LvONKsw.png"/></figure>



<p>The two datasets are equally easy to represent, but on the righthand side, there is a very strong “simple classifier” (the 45-degree halfspace) that SGD will “latch onto.” Once it gets stuck with that classifier, it is hard for SGD to get “unstuck.” As a result, SGD has a much harder time learning the righthand dataset than the lefthand dataset.</p>



<h2>Analysing SGD for over-parameterized linear regression</h2>



<p>So, what can we prove about the dynamics of gradient descent? Often we can gain insights by studying <em>linear regression</em>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/T9VGkXj.png"/></figure>



<p>Formally, given <img alt="(x_i, y_i)_{i=1}^n \in \mathbb{R}^{d+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_i%2C+y_i%29_%7Bi%3D1%7D%5En+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x_i, y_i)_{i=1}^n \in \mathbb{R}^{d+1}"/> with <img alt="d\gg n" class="latex" src="https://s0.wp.com/latex.php?latex=d%5Cgg+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d\gg n"/> we would like to find a vector <img alt="w\in\mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Cin%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w\in\mathbb{R}^d"/> such that <img alt="\langle w, x_i\rangle\approx y_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+w%2C+x_i%5Crangle%5Capprox+y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle w, x_i\rangle\approx y_i"/>.</p>



<p>In this setting, we can prove that running SGD (from zero or tiny initialization) on the loss <img alt="\mathcal{L}(w) =\lVert Xw -y \rVert^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28w%29+%3D%5ClVert+Xw+-y+%5CrVert%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{L}(w) =\lVert Xw -y \rVert^2"/> will converge to solution <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/> of minimum norm. To see whym note that SGD performs updates of the form<br/><img alt="w_{t+1} = w_t - \eta x_i^T(\langle x_i, w\rangle - y_i)" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D+%3D+w_t+-+%5Ceta+x_i%5ET%28%5Clangle+x_i%2C+w%5Crangle+-+y_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{t+1} = w_t - \eta x_i^T(\langle x_i, w\rangle - y_i)"/>.<br/>However note that <img alt="\eta(\langle x_i, w\rangle - y_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%28%5Clangle+x_i%2C+w%5Crangle+-+y_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta(\langle x_i, w\rangle - y_i)"/> is a scalar. Therefore all of the updates keep the updated vector <img alt="w_{t+1}" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{t+1}"/> within <img alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathrm{span}(x_1^T, \ldots, x_n^T)"/>. This implies that the converging solution <img alt="w_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{\infty}"/> will also lie in <img alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathrm{span}(x_1^T, \ldots, x_n^T)"/>.<br/><img alt="" src="https://i.imgur.com/G8tFMrF.png"/><br/>Geometrically this translates into <img alt="w_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{\infty}"/> being the projection of <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/> onto the the subspace <img alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathrm{span}(x_1^T, \ldots, x_n^T)"/> which results in the least norm solution.</p>



<p>Analyzing the dynamics of descent, we can write the distance between consecutive weight updates and the converging solution as<br/><img alt="w_{t+1} - w_{\infty} = (I - \eta X^TX)(w_t - w_{\infty})" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D+-+w_%7B%5Cinfty%7D+%3D+%28I+-+%5Ceta+X%5ETX%29%28w_t+-+w_%7B%5Cinfty%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{t+1} - w_{\infty} = (I - \eta X^TX)(w_t - w_{\infty})"/>.<br/>We see that we are applying the linear operator <img alt="(I - \eta X^TX)" class="latex" src="https://s0.wp.com/latex.php?latex=%28I+-+%5Ceta+X%5ETX%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(I - \eta X^TX)"/> at every step we take. As long as this operator is contractive, we will continue to progress and converge to <img alt="w_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{\infty}"/>. Formally, to make progress, we require<br/><img alt="0 \prec I -\eta X^TX\prec 1" class="latex" src="https://s0.wp.com/latex.php?latex=0+%5Cprec+I+-%5Ceta+X%5ETX%5Cprec+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0 \prec I -\eta X^TX\prec 1"/>.<br/>This directly translates into <img alt="\eta &lt; \frac{1}{\lambda_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta+%3C+%5Cfrac%7B1%7D%7B%5Clambda_1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta &lt; \frac{1}{\lambda_1}"/> and then the progress we make is approximately <img alt="\frac{\lambda_d}{\lambda_1}=\frac{1}{\kappa}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Clambda_d%7D%7B%5Clambda_1%7D%3D%5Cfrac%7B1%7D%7B%5Ckappa%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{\lambda_d}{\lambda_1}=\frac{1}{\kappa}"/>, where <img alt="\kappa" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ckappa&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\kappa"/> is the <em>condition number</em> of <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X"/>.</p>



<p>What happens now if the matrix <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X"/> is random? Then, results from random matrix theory (specifically the <a href="https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution">Marchenko-Pastur distribution</a>) state that</p>



<ul><li>if <img alt="d &lt; n" class="latex" src="https://s0.wp.com/latex.php?latex=d+%3C+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d &lt; n"/>, then the matrix <img alt="X^\top X" class="latex" src="https://s0.wp.com/latex.php?latex=X%5E%5Ctop+X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X^\top X"/> has <img alt="\mathrm{rank}(X^\top X)=d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Brank%7D%28X%5E%5Ctop+X%29%3Dd&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathrm{rank}(X^\top X)=d"/> and the eigenvalues are bounded away from <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/>. This means that the matrix is well conditioned.</li><li>if <img alt="d \approx n" class="latex" src="https://s0.wp.com/latex.php?latex=d+%5Capprox+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d \approx n"/>, then the spectrum of <img alt="X^\top X" class="latex" src="https://s0.wp.com/latex.php?latex=X%5E%5Ctop+X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X^\top X"/> starts shifting towards <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/>, with some eigenvalues being equal to zero, resulting in an ill-conditioned matrix.</li><li>if <img alt="d &gt; n" class="latex" src="https://s0.wp.com/latex.php?latex=d+%3E+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d &gt; n"/>, then the spectrum has some zero eigenvalues, but is otherwise bounded away from zero. If we restrict to the subspace of positive eigenvalues, we achieve again a good condition number.</li></ul>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/hLZtfz5.png"/></figure>



<p/>



<h2>Deep linear networks</h2>



<p>We now want to go beyond linear regression and talk about deep networks. As deep networks are very hard to understand, we will first start analyzing a depth <img alt="2" class="latex" src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2"/> network. We will also consider a <em>linear</em> network and omit the nonlinearity. This might seem strange, as we could consider the corresponding linear model, which has exactly the same expressiveness. However, note that these two models have a different parameter space. This means that gradient-based algorithms will travel on different paths when optimizing these two models.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/ekh9k8M.png"/></figure>



<p>Specifically, we can see that the minimum loss attained by the two models will coincide, i.e., <img alt="\min \mathcal{L}(A_1, A_2) = \min \mathcal{L}(B)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin+%5Cmathcal%7BL%7D%28A_1%2C+A_2%29+%3D+%5Cmin+%5Cmathcal%7BL%7D%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min \mathcal{L}(A_1, A_2) = \min \mathcal{L}(B)"/>, but the SGD path and the solution will be different.</p>



<p>We will analyze the gradient flow on these two networks (which is gradient descent with the learning rate <img alt="\eta \rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta \rightarrow 0"/>). We will make the simplifying assumption that <img alt="A_1 = A_2" class="latex" src="https://s0.wp.com/latex.php?latex=A_1+%3D+A_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A_1 = A_2"/> and symmetric. Then, we can see that <img alt="B = A^2 \Rightarrow A = \sqrt{B}" class="latex" src="https://s0.wp.com/latex.php?latex=B+%3D+A%5E2+%5CRightarrow+A+%3D+%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B = A^2 \Rightarrow A = \sqrt{B}"/>. We will try and compare the gradient flow of two different loss functions: <img alt="\mathcal{L}(B)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{L}(B)"/> (doing gradient flow on a linear model) and <img alt="\tilde{\mathcal{L}}(A) = \mathcal{L}(A^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%29+%3D+%5Cmathcal%7BL%7D%28A%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tilde{\mathcal{L}}(A) = \mathcal{L}(A^2)"/> (doing gradient flow on a depth linear model).</p>



<p>Gradient flow on the linear model simply gives <img alt="\frac{dB(t)}{dt}=-\nabla\mathcal{L}(B(t))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D%3D-%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dB(t)}{dt}=-\nabla\mathcal{L}(B(t))"/>, whereas for the deep linear network we have (using the chain rule)<br/><img alt="\frac{dA(t)}{dt}=-\nabla\tilde{\mathcal{L}}(A(t)) = \nabla\mathcal{L}(A^2)A = A\nabla\mathcal{L}(A^2)," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%28t%29%7D%7Bdt%7D%3D-%5Cnabla%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%28t%29%29+%3D+%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29A+%3D+A%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dA(t)}{dt}=-\nabla\tilde{\mathcal{L}}(A(t)) = \nabla\mathcal{L}(A^2)A = A\nabla\mathcal{L}(A^2),"/><br/>since <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> is symmetric.</p>



<p>For simplicity, let’s denote <img alt="\nabla\mathcal{L}(B) = \nabla\mathcal{L}(A^2) = \nabla" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5Cmathcal%7BL%7D%28B%29+%3D+%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29+%3D+%5Cnabla&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla\mathcal{L}(B) = \nabla\mathcal{L}(A^2) = \nabla"/> and <img alt="\nabla\tilde{\mathcal{L}}(A) = \tilde{\nabla}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%29+%3D+%5Ctilde%7B%5Cnabla%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla\tilde{\mathcal{L}}(A) = \tilde{\nabla}"/>. We then have<br/><img alt="\frac{dA^2(t)}{dt}=\frac{dA(t)}{dt}A=-\tilde{\nabla}A = -A \nabla A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%5E2%28t%29%7D%7Bdt%7D%3D%5Cfrac%7BdA%28t%29%7D%7Bdt%7DA%3D-%5Ctilde%7B%5Cnabla%7DA+%3D+-A+%5Cnabla+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dA^2(t)}{dt}=\frac{dA(t)}{dt}A=-\tilde{\nabla}A = -A \nabla A"/>.</p>



<p>Another way to view the comparison between the models of interest, <img alt="\frac{dA^2(t)}{dt}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%5E2%28t%29%7D%7Bdt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dA^2(t)}{dt}"/> and <img alt="\frac{dB(t)}{dt}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dB(t)}{dt}"/> is as follows: let <img alt="B = A^2" class="latex" src="https://s0.wp.com/latex.php?latex=B+%3D+A%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B = A^2"/>, then <img alt="\frac{dB(t)}{dt} = -A\nabla A = -\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D+%3D+-A%5Cnabla+A+%3D+-%5Csqrt%7BB%7D%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dB(t)}{dt} = -A\nabla A = -\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}"/>.<br/>We can view this as follows: when we multiply the gradient with <img alt="\sqrt{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sqrt{B}"/> we end up making the “big bigger and the small smaller”. Basically, this accenuates the differences between the eigenvalues and is biasing <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/> to become a low-rank matrix. </p>



<p>To see why, you can think of a low rank matrix has one that has few large eigenvalues and the others small. If <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/> is already close low rank, then replacing a gradient by <img alt="\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BB%7D%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}"/> encourages the gradient steps to mostly happen in the top eigenspace of <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/>. This result <a href="https://arxiv.org/abs/1910.05505">generalizes</a> to networks of greater depth, and the gradient evolves as <img alt="\frac{dB(t)}{dt} = -\psi_{B(t)}(\nabla\mathcal{L}(B(t)))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D+%3D+-%5Cpsi_%7BB%28t%29%7D%28%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dB(t)}{dt} = -\psi_{B(t)}(\nabla\mathcal{L}(B(t)))"/>, with <img alt="\psi_{B}(\nabla) = \sum B^{\alpha}\nabla B^{1-\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7BB%7D%28%5Cnabla%29+%3D+%5Csum+B%5E%7B%5Calpha%7D%5Cnabla+B%5E%7B1-%5Calpha%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\psi_{B}(\nabla) = \sum B^{\alpha}\nabla B^{1-\alpha}"/>.</p>



<p>This means that we end up doing gradient flow on a <em>Riemannian manifold</em>. An interesting result is that the flow induced by the operator <img alt="\psi_{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\psi_{B}"/> is provably not equivalent to a regularized minimization problem <img alt="\min\mathcal{L} + \lambda R(B)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin%5Cmathcal%7BL%7D+%2B+%5Clambda+R%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min\mathcal{L} + \lambda R(B)"/> for any <img alt="R(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=R%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="R(\cdot)"/>.</p>



<h2>What is learned at different layers?</h2>



<p>Finally, let’s discuss what is learned by the different layers in a neural network. Some intuition people have is that learning proceeds roughly like the following cartoon:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/rteesY5.png"/></figure>



<p>We can think of our data as being “built up” as a sequence of choices from higher level to lower level features. For example, the data is generated by first deciding that it would be a photo of a dog, then that it would be on the beach, and finally low-level details such as the type of fur and light. This is also how a human would describe this photo. In contrast, a neural network builds up the features in the opposite direction. It starts from the simplest (lowest-level) features in the image (edges, textures, etc.) and gradually builds up complexity until it finally classifies the image.</p>



<h2>How neural networks learn features?</h2>



<p>To build a bit of intuition, consider an example of combining different simple features. We can see that if we try to combine two good edge detectors with different orientations, the end result will hardly be an edge detector.<br/><img alt="" src="https://i.imgur.com/CsTB2LV.png"/></p>



<p>So the intuition is that there is competitive/evolutionary pressure on neurons to “specialize” and recognize useful features. Initially, all the neurons are random features, which can be thought of as random linear combination of the various detectors. However, after training, the symmetry will break between the neurons, and they will specialize (in this simple example, they will either become vertical or horizontal edge detectors).</p>



<p><a href="https://arxiv.org/abs/1706.05806">Raghu, Gilmer, Yosinski, and Sohl-Dickstein</a> tracked the speed at which features learned by different layers reach their final learned state. In the figure below the diagonal elements denote the similarity of the current state of a layer to its final one, where lighter color means that the state is more similar. We can see that earlier layer (more to the left) reach their final state earlier (with th exception of the 2 layers closest to the output that also converge very early).</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/qHTIAzz.png"/></figure>



<p>The “symmetry breaking” intuition is explored by a recent work of <a href="https://arxiv.org/abs/1912.05671">Frankle, Dziugaite, Roy, and Carbin</a>. Intuitively, because the average of two good features is generally <em>not</em> a good feature, averaging the weights of two neural networks with small loss will likely result in a network with large loss. That is, if we start from two random initializations <img alt="w_0" class="latex" src="https://s0.wp.com/latex.php?latex=w_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_0"/>, <img alt="w'_0" class="latex" src="https://s0.wp.com/latex.php?latex=w%27_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w'_0"/> and train two networks until we reach weights <img alt="w_\infty" class="latex" src="https://s0.wp.com/latex.php?latex=w_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_\infty"/> and <img alt="w'_\infty" class="latex" src="https://s0.wp.com/latex.php?latex=w%27_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w'_\infty"/> with small loss, then we expect the average of <img alt="w_\infty" class="latex" src="https://s0.wp.com/latex.php?latex=w_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_\infty"/> and <img alt="w'_\infty" class="latex" src="https://s0.wp.com/latex.php?latex=w%27_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w'_\infty"/> to result in a network with poor loss:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/uLPBPpN.png"/></figure>



<p>In contrast, Frankle et al showed that sometimes, when we start from the same initialization (especially after pruning) and use random SGD noise (obtained by randomly shuffling the training set) then we reach a “linear plateu” of the loss function in which averaging two networks yields a network with similar loss:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/owtRGUL.png"/></figure>



<h2>The contrapositive of simplicity: lower bounds for learning parities</h2>



<p>If we believe that networks learn simple features first, and learn them in the early layers, then this has an interesting consequence. If the data has the form that simple features (e.g. linear or low degree) are completely uninformative (have no correlation with the label) then we may expect that learning cannot “get off the ground”. That is, even if there exists a small neural network that can learn the class, gradient based algorithms such as SGD will never find it. (In fact, it is possible that <em>no</em> efficient algorithm could find it.) There are some settings where we can prove such conjectures. (For gradient-based algorithms that is; proving this for all efficient algorithms would require settling the P vs NP question.)</p>



<p>We discuss one of the canonical “hard” examples for neural networks: parities. Formally, for <img alt="I\subset [d]" class="latex" src="https://s0.wp.com/latex.php?latex=I%5Csubset+%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I\subset [d]"/>, the distribution <img alt="D_I" class="latex" src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D_I"/> is the distribution over <img alt="(x,y) \in { \pm 1 }^{d+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Cin+%7B+%5Cpm+1+%7D%5E%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y) \in { \pm 1 }^{d+1}"/> defined as follows: <img alt="x\sim {\pm 1}^d" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+%7B%5Cpm+1%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim {\pm 1}^d"/> and <img alt="y = \prod_{i\in I}x_i" class="latex" src="https://s0.wp.com/latex.php?latex=y+%3D+%5Cprod_%7Bi%5Cin+I%7Dx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y = \prod_{i\in I}x_i"/>. The “learning parity” problem is as follows: given <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> samples <img alt="{ (x_i,y_i) }_{i=1..n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29+%7D_%7Bi%3D1..n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ (x_i,y_i) }_{i=1..n}"/> drawn from <img alt="D_I" class="latex" src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D_I"/>, either recover <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/> or do the weaker task of finding a predictor <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> such that <img alt="f(x)=y" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=y"/> with high probability over future samples <img alt="(x,y) \sim D_I" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Csim+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y) \sim D_I"/>.</p>



<p>It turns out that if we don’t restrict ourselves to deep learning, given <img alt="2d" class="latex" src="https://s0.wp.com/latex.php?latex=2d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2d"/> samples we can recover <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/>. Consider the transformations <img alt="Z_{i,j} = (1 - x_{i,j})/2" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7Bi%2Cj%7D+%3D+%281+-+x_%7Bi%2Cj%7D%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z_{i,j} = (1 - x_{i,j})/2"/> and <img alt="b_i = (1 - y_i)/2" class="latex" src="https://s0.wp.com/latex.php?latex=b_i+%3D+%281+-+y_i%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="b_i = (1 - y_i)/2"/>. If we let <img alt="s_i=1" class="latex" src="https://s0.wp.com/latex.php?latex=s_i%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="s_i=1"/> if <img alt="i\in I" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cin+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\in I"/> and <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> otherwise, we can write <img alt="\sum_j Z_{i, j}s_j = b_i (\text{mod } 2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_j+Z_%7Bi%2C+j%7Ds_j+%3D+b_i+%28%5Ctext%7Bmod+%7D+2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_j Z_{i, j}s_j = b_i (\text{mod } 2)"/>. Basically, we transformed the problem of parity to a problem of counting if we have an odd or an even number of <img alt="-1" class="latex" src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-1"/>. In this setting, we can think of every sample <img alt="(x,y) \in D_I" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Cin+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y) \in D_I"/> as providing a <em>linear equation</em> moudlo 2 over the <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> unknown variables <img alt="s_1,\ldots,s_d" class="latex" src="https://s0.wp.com/latex.php?latex=s_1%2C%5Cldots%2Cs_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="s_1,\ldots,s_d"/>. When <img alt="n&gt;d" class="latex" src="https://s0.wp.com/latex.php?latex=n%3Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n&gt;d"/>, these linear equations will be very likely to be of full rank, and hence we can use Gaussian elimination to find <img alt="s_1,\ldots,s_d" class="latex" src="https://s0.wp.com/latex.php?latex=s_1%2C%5Cldots%2Cs_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="s_1,\ldots,s_d"/> and hence <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/>.</p>



<p>Switching to the learning setting, we can express parities by using few ReLUs. In particular, we’ve shown that we can create a step function using <img alt="4" class="latex" src="https://s0.wp.com/latex.php?latex=4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="4"/> ReLUs. Therefore for every <img alt="k \in {0,1,\ldots, d }" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cin+%7B0%2C1%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k \in {0,1,\ldots, d }"/>, there is a combination of four ReLUs that computes the function <img alt="f_k:\mathbb{R} :\rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=f_k%3A%5Cmathbb%7BR%7D+%3A%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_k:\mathbb{R} :\rightarrow \mathbb{R}"/> such that <img alt="f_k(s)" class="latex" src="https://s0.wp.com/latex.php?latex=f_k%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_k(s)"/> outputs <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/> for <img alt="s=k" class="latex" src="https://s0.wp.com/latex.php?latex=s%3Dk&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="s=k"/>, and <img alt="f_k(s)" class="latex" src="https://s0.wp.com/latex.php?latex=f_k%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_k(s)"/> outputs <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> if <img alt="|x-k|&gt;0.5" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cx-k%7C%3E0.5&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|x-k|&gt;0.5"/>. We can then write the parity function (for example for <img alt="I=[d]" class="latex" src="https://s0.wp.com/latex.php?latex=I%3D%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I=[d]"/>) as <img alt="\sum_{k \text{ odd } \in [d]} f_k(\sum_{i=1}^d (1-x_i)/2 )" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bk+%5Ctext%7B+odd+%7D+%5Cin+%5Bd%5D%7D+f_k%28%5Csum_%7Bi%3D1%7D%5Ed+%281-x_i%29%2F2+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_{k \text{ odd } \in [d]} f_k(\sum_{i=1}^d (1-x_i)/2 )"/>. This will be a linear combination of at most <img alt="4d" class="latex" src="https://s0.wp.com/latex.php?latex=4d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="4d"/> ReLUs.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/NwQ31Jy.png"/></figure>



<p>Parities are an example of a case where simple feature are uninformative. For example, if <img alt="|I|&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=%7CI%7C%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|I|&gt;1"/> then for every linear function <img alt="L:\mathbb{R}^d \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=L%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L:\mathbb{R}^d \rightarrow \mathbb{R}"/>,</p>



<p><img alt="\mathbb{E}_{(x,y) \sim D_I}[ L(x)y] = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%28x%2Cy%29+%5Csim+D_I%7D%5B+L%28x%29y%5D+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_{(x,y) \sim D_I}[ L(x)y] = 0"/></p>



<p>in other words, there is no correlation between the linear function and the label.<br/>To see why this is true, write <img alt="L(x) = \sum L_i x_i" class="latex" src="https://s0.wp.com/latex.php?latex=L%28x%29+%3D+%5Csum+L_i+x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(x) = \sum L_i x_i"/>. By linearity of expectation, it suffices to show that $latex \mathbb{E}<em>{(x,y) \sim D_I}[ L_ix_i y] = L_i \mathbb{E}</em>{(x,y) \sim D_I}[ x_i y] = 0&amp;bg=ffffff$. Both <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/> and <img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y_i"/> are just values in <img alt="{\pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{\pm 1 }"/>. To evaluate the expectation <img alt="\mathbb{E}[x_i y]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Bx_i+y%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}[x_i y]"/> we simply need to know the marginal distribution that <img alt="D_I" class="latex" src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D_I"/> induces on <img alt="{ \pm 1 }^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ \pm 1 }^2"/> when we restrict it to these two coordinates. This distribution is just the uniform distribution. To see why this is the case, consider a coordinate <img alt="j\in I \setminus { i }" class="latex" src="https://s0.wp.com/latex.php?latex=j%5Cin+I+%5Csetminus+%7B+i+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="j\in I \setminus { i }"/> and let’s condition on the values of all coordinates other than <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/> and <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="j"/>. After conditioning on these values, <img alt="y = \sigma x_i x_j" class="latex" src="https://s0.wp.com/latex.php?latex=y+%3D+%5Csigma+x_i+x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y = \sigma x_i x_j"/> for some <img alt="\sigma \in { \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sigma \in { \pm 1 }"/> and <img alt="x_i,x_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%2Cx_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i,x_j"/> are chosen uniformly and independently from <img alt="{ \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ \pm 1 }"/>. For every choice of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/>, if we flip <img alt="x_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_j"/> then that would flip the value of <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/>, and hence the marginal distribution on <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/> will be uniform.</p>



<p>This lack of correlation turns out to be a real obstacle for gradient-based algorithms. While small neural networks for parities exist, and Gaussian elimination can find them, it turns out that gradient-based algorithms such as SGD will <em>fail</em> to do so. Parities are hard to learn, and even if the capacity of the network is such that it can memorize the input, it will still perform poorly in a test set. Indeed, we can prove that for <em>every</em> neural network architecture <img alt="f_w(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f_w%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_w(x)"/>, running SGD on <img alt="\min\lVert f_w(x) -\prod_{i\in I} x_i\rVert^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin%5ClVert+f_w%28x%29+-%5Cprod_%7Bi%5Cin+I%7D+x_i%5CrVert%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min\lVert f_w(x) -\prod_{i\in I} x_i\rVert^2"/> will require <img alt="e^{\Omega(d)}" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B%5COmega%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="e^{\Omega(d)}"/> steps. (Note that if we add <em>noise</em> to parities, then Gaussian elimination will fail and it is believed that <em>no efficient algorithm</em> can learn the distribution in this case. This is known as the <a href="https://wiki.epfl.ch/edicpublic/documents/Candidacy%20exam/Cryptography_from_learning_parity_with_noise.pdf">learning parity with noise</a> problem, which is also related to the <a href="https://en.wikipedia.org/wiki/Learning_with_errors">learning with errors</a> problem that is the foundation of modern lattice-based cryptography.)</p>



<p>We now sketch the proof that gradient-based algorithms require exponentially many steps to learn parities, following Theorem 1 of <a href="https://arxiv.org/abs/1703.07950">Shalev-Shwartz,Shamir and Shammah</a>. We think of an idealized setting where we have an unlimited number of samples and only use a sample <img alt="(x,y) \sim D_I" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Csim+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y) \sim D_I"/> only once (this should only make learning easier). We will show that we make very little progress in learning <img alt="D_I" class="latex" src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D_I"/>, by showing that for any given <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/>, the expected gradient over <img alt="(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y)"/> will be exponentially small, and hence we make very little progress toward learning <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/>. Specifically, using the notation <img alt="\chi_I(x)=\prod_{i\in I}x_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29%3D%5Cprod_%7Bi%5Cin+I%7Dx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\chi_I(x)=\prod_{i\in I}x_i"/>, for any <img alt="w,x,I" class="latex" src="https://s0.wp.com/latex.php?latex=w%2Cx%2CI&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w,x,I"/>,</p>



<p><img alt="\nabla_w \parallel f_w(x) - \chi_I(x) \parallel^2 = 2\sum_{i=1}^d \left [ f_w(x) \tfrac{d}{d x_i}f_{w}(x)- \chi_I(x)\tfrac{d}{d x_i}f_{w}(x) \right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla_w+%5Cparallel+f_w%28x%29+-+%5Cchi_I%28x%29+%5Cparallel%5E2+%3D+2%5Csum_%7Bi%3D1%7D%5Ed+%5Cleft+%5B+f_w%28x%29+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29-+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla_w \parallel f_w(x) - \chi_I(x) \parallel^2 = 2\sum_{i=1}^d \left [ f_w(x) \tfrac{d}{d x_i}f_{w}(x)- \chi_I(x)\tfrac{d}{d x_i}f_{w}(x) \right]"/></p>



<p>The term <img alt="f_w(x) \tfrac{d}{d x_i}f_{w}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f_w%28x%29+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_w(x) \tfrac{d}{d x_i}f_{w}(x)"/> is independent of <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/> and so does not contribute toward learning <img alt="D_I" class="latex" src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D_I"/>. Hence intuitively to show we make exponentially small progress, it suffices to show that typically for every <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/>, <img alt="\left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29+%5D+%5Cright%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2"/> will be exponentially small. (That is, even if for a fixed <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> we make a large step, these all cancel out and give us exponentially small progress toward actually learning <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/>.)</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/lWCuan2.png"/></figure>



<p>Formally, we will prove the following lemma:</p>



<p><strong>Lemma:</strong> For every <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/>, <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/></p>



<p><img alt="\mathbb{E}_I \left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2 \leq \tfrac{poly(d,n)\mathbb{E}_x \frac{d}{d x_i}f{w}(x)^2}{2^d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_I+%5Cleft%28%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29+%5D+%5Cright%29%5E2+%5Cleq+%5Ctfrac%7Bpoly%28d%2Cn%29%5Cmathbb%7BE%7D_x+%5Cfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29%5E2%7D%7B2%5Ed%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_I \left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2 \leq \tfrac{poly(d,n)\mathbb{E}_x \frac{d}{d x_i}f{w}(x)^2}{2^d}"/></p>



<p><strong>Proof:</strong> Let us fix <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/> and define <img alt="g(x) = \tfrac{d}{d x_i}f_{w}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x%29+%3D+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g(x) = \tfrac{d}{d x_i}f_{w}(x)"/>. The quantity <img alt="\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x)]"/> can be written as <img alt="\langle \chi_I,g \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cchi_I%2Cg+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle \chi_I,g \rangle"/> with respect to the inner product <img alt="\langle f,g \rangle = \mathbb{E}_x f(x)g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+f%2Cg+%5Crangle+%3D+%5Cmathbb%7BE%7D_x+f%28x%29g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle f,g \rangle = \mathbb{E}_x f(x)g(x)"/>. However, <img alt="{ \chi_I }{I\subseteq [d]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Cchi_I+%7D%7BI%5Csubseteq+%5Bd%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ \chi_I }{I\subseteq [d]}"/> is an orhtonormal basis with respect to this inner product. To see this note that since <img alt="\chi_I(x) \in { \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\chi_I(x) \in { \pm 1 }"/>, <img alt="\mathbb{E}_x \chi_I(x)^2 = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x+%5Cchi_I%28x%29%5E2+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_x \chi_I(x)^2 = 1"/> for every <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/>, and for <img alt="I \neq J" class="latex" src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I \neq J"/>, <img alt="\chi_I(x)\chi_J(x) = (\prod{i \in I} x_i)(\prod_{j \in J} x_j) = \prod_{k \in I \oplus H} x_k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29%5Cchi_J%28x%29+%3D+%28%5Cprod%7Bi+%5Cin+I%7D+x_i%29%28%5Cprod_%7Bj+%5Cin+J%7D+x_j%29+%3D+%5Cprod_%7Bk+%5Cin+I+%5Coplus+H%7D+x_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\chi_I(x)\chi_J(x) = (\prod{i \in I} x_i)(\prod_{j \in J} x_j) = \prod_{k \in I \oplus H} x_k"/> where <img alt="I\oplus J" class="latex" src="https://s0.wp.com/latex.php?latex=I%5Coplus+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I\oplus J"/> is the symmetric difference of <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/> and <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/>. The reason is that <img alt="x_i^2 =1" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%5E2+%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i^2 =1"/> for all <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/> and so elements that appear in both <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/> and <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/> “cancel out”. Since the coordinates of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> are distributed independently and uniformly, the expectation of the product is the product of expectations. This means that as long as <img alt="I \oplus J" class="latex" src="https://s0.wp.com/latex.php?latex=I+%5Coplus+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I \oplus J"/> is not empty (i.e., <img alt="I \neq J" class="latex" src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I \neq J"/>) this will be a product of one or more terms of the form <img alt="(\mathbb{E} x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cmathbb%7BE%7D+x_k%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(\mathbb{E} x_k)"/>. Since <img alt="x_k" class="latex" src="https://s0.wp.com/latex.php?latex=x_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_k"/> is uniform over <img alt="\{ \pm 1 \}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Cpm+1+%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\{ \pm 1 \}"/>, <img alt="\mathbb{E} x_k = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+x_k+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E} x_k = 0"/> and so we get that if <img alt="I \neq J" class="latex" src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I \neq J"/>, <img alt="\langle \chi_I,\chi_J \rangle =0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cchi_I%2C%5Cchi_J+%5Crangle+%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle \chi_I,\chi_J \rangle =0"/>.</p>



<p>Given the above</p>



<p><img alt="\mathbb{E}_x g(x)^2 = \langle g,g\rangle = \sum_{I \subseteq [d]} \langle g , \chi_I \rangle^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x+g%28x%29%5E2+%3D+%5Clangle+g%2Cg%5Crangle+%3D+%5Csum_%7BI+%5Csubseteq+%5Bd%5D%7D+%5Clangle+g+%2C+%5Cchi_I+%5Crangle%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_x g(x)^2 = \langle g,g\rangle = \sum_{I \subseteq [d]} \langle g , \chi_I \rangle^2"/></p>



<p>which means that (since there are <img alt="2^d" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2^d"/> subsets of <img alt="[d]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="[d]"/>) on average <img alt="\langle g, \chi_I \rangle = \parallel g \parallel^2 / 2^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+g%2C+%5Cchi_I+%5Crangle+%3D+%5Cparallel+g+%5Cparallel%5E2+%2F+2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle g, \chi_I \rangle = \parallel g \parallel^2 / 2^d"/>. In other words, <img alt="\langle g,\chi_I \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+g%2C%5Cchi_I+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle g,\chi_I \rangle"/> is typically exponentially small which is what we wanted to prove.</p></div>
    </content>
    <updated>2021-02-17T21:30:14Z</updated>
    <published>2021-02-17T21:30:14Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-02-21T21:21:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18134</id>
    <link href="https://rjlipton.wordpress.com/2021/02/17/alan-selman/" rel="alternate" type="text/html"/>
    <title>Alan Selman</title>
    <summary>A special journal issue in his honor Elvira Mayordomo, Mitsu Ogihara, and Atri Rudra are going to be the editors of a special issue of the journal Theory of Computing Systems dedicated to Alan Selman. Alan passed away this January 2021. Today we circulate their request for contributions. The details of the call say: This […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A special journal issue in his honor</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2021/02/17/alan-selman/all-9/" rel="attachment wp-att-18136"><img alt="" class="alignright  wp-image-18136" src="https://rjlipton.files.wordpress.com/2021/02/all.png?w=200" width="200"/></a></p>
<p>
Elvira Mayordomo, Mitsu Ogihara, and Atri Rudra are going to be the editors of a special issue of the journal <a href="https://www.springer.com/journal/224">Theory of Computing Systems</a> dedicated to Alan Selman. Alan passed away this January 2021. </p>
<p>
Today we circulate their request for contributions.</p>
<p>
The details of the <a href="https://www.springer.com/journal/224/updates/18863610">call</a> say:  This special issue celebrates Alan’s life and commemorate his extraordinary contributions to the field. The topics of interest include but are not limited to: </p>
<ul>
<li>
average-case complexity <p/>
</li><li>
circuit complexity <p/>
</li><li>
comparison of reducibilities <p/>
</li><li>
complexity theoretic characterizations of models <p/>
</li><li>
function complexity <p/>
</li><li>
hierarchy theorems <p/>
</li><li>
	parameterized complexity <p/>
</li><li>
promise problems and disjoint NP-pairs <p/>
</li><li>
public-key cryptography <p/>
</li><li>
relativization <p/>
</li><li>
semi-feasible algorithms <p/>
</li><li>
sparse sets <p/>
</li><li>
structure of complete sets
</li></ul>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>Please look at <a href="https://www.springer.com/journal/224/updates/18863610">this</a> for details—the deadline for submission is 31st July 2021. You have 164 days to write your paper. Which is 3936 hours or 236160 minutes.</p>
<p>
Please send a contribution. </p>
<p/></font></font></div>
    </content>
    <updated>2021-02-17T17:07:25Z</updated>
    <published>2021-02-17T17:07:25Z</published>
    <category term="News"/>
    <category term="People"/>
    <category term="Alan Selman"/>
    <category term="special issue"/>
    <category term="TCD"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-02-21T21:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/02/17/stoc-2021-workshops/</id>
    <link href="https://cstheory-events.org/2021/02/17/stoc-2021-workshops/" rel="alternate" type="text/html"/>
    <title>STOC 2021 workshops</title>
    <summary>June 21-25, 2021 Online http://acm-stoc.org/stoc2021/callforworkshops.html Submission deadline: March 15, 2021 STOC 2021 will hold workshops during the conference week, June 21–25, 2021. We invite groups of interested researchers to submit workshop proposals. The due date for proposals is March 15.</summary>
    <updated>2021-02-17T04:07:12Z</updated>
    <published>2021-02-17T04:07:12Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-02-21T21:21:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/02/16/lattice-borromean-rings</id>
    <link href="https://11011110.github.io/blog/2021/02/16/lattice-borromean-rings.html" rel="alternate" type="text/html"/>
    <title>Lattice Borromean rings</title>
    <summary>A lot of topology is finding ways to prove things that are really obvious but where explaining why they’re obvious can be difficult. So I want to do this for a discrete analogue of ropelength, the length of the shortest lattice representation, for the Borromean rings. You can find several pretty lattice (and non-lattice) representations of the Borromean rings in a paper by Verhoeff &amp; Verhoeff, “Three families of mitered Borromean ring sculptures” [Bridges, 2015]; the one in the middle of their figure 2, thinned down to use only lattice edges and not thick solid components, is the one I have in mind. It is formed by three \(2\times 4\) rectangles, shown below next to Jessen’s icosahedron which has the same vertex coordinates. (You can do the same thing with a regular icosahedron but then you get non-lattice golden rectangles.)</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A lot of topology is finding ways to prove things that are really obvious but where explaining why they’re obvious can be difficult. So I want to do this for a discrete analogue of <a href="https://en.wikipedia.org/wiki/Ropelength">ropelength</a>, the length of the shortest lattice representation, for the <a href="https://en.wikipedia.org/wiki/Borromean_rings">Borromean rings</a>. You can find several pretty lattice (and non-lattice) representations of the Borromean rings in a paper by Verhoeff &amp; Verhoeff, “<a href="https://archive.bridgesmathart.org/2015/bridges2015-53.pdf">Three families of mitered Borromean ring sculptures</a>” [<em>Bridges</em>, 2015]; the one in the middle of their figure 2, thinned down to use only lattice edges and not thick solid components, is the one I have in mind. It is formed by three \(2\times 4\) rectangles, shown below next to <a href="https://en.wikipedia.org/wiki/Jessen%27s_icosahedron">Jessen’s icosahedron</a> which has the same vertex coordinates. (You can do the same thing with a regular icosahedron but then you get non-lattice golden rectangles.)</p>

<p style="text-align: center;"><img alt="Lattice Borromean rings and Jessen's icosahedron" src="https://11011110.github.io/blog/assets/2021/Borromean-Jessen.svg"/></p>

<p>Each of the three rectangles has perimeter \(12\), so the total length of the whole link is \(36\). Why should this be the minimum possible? One could plausibly run a brute force search over all small-enough realizations, but this would be tedious and some effort would be needed to prune the search enough to get it to run at all. Instead, I found an argument based on the lengths of the individual components of the link, allowing me to analyze them (mostly) separately.</p>

<p>Each component is unknotted, so it can be the boundary of a disk in space. Importantly, for the Borromean rings, every disk spanned by one of the components must be crossed at least twice by other components. If we could find a disk spanned by one component that was not crossed at all by other components, then we could shrink the first component topologically within its disk down to a size so small that it could easily be pulled apart from the other two components, something that is not possible with the Borromean rings. And if we could find a disk that was only crossed once by another component, then the <a href="https://en.wikipedia.org/wiki/Linking_number">linking number</a> of the two components would be one, something that doesn’t happen for the Borromean rings.</p>

<p>If you travel in some consistent direction around a cycle in a 3d lattice, every step in one direction along a coordinate axis must be cancelled by a step in the opposite direction elsewhere along the ring. So if a lattice cycle has length \(\ell\), there must be \(\ell/2\) pairs of opposite steps, partitioned somehow among the three dimensions. If the bounding box of the cycle has size \(a\times b\times c\), then we must have \(a+b+c\le\ell/2\), and we can classify the possible shapes of lattice cycles of length \(\ell\) by the possible shapes of their bounding boxes. This gives us the following cases:</p>

<ul>
  <li>
    <p>A lattice cycle of length \(\ell=4\) can only be a square, with bounding box dimensions \(1\times 1\times 0\) (the zero means that it lies in a single plane in 3d, not that it doesn’t exist at all). The square itself is a disk not crossed by any other lattice path, unusable as a component of the Borromean rings.</p>
  </li>
  <li>
    <p>A lattice cycle of length \(\ell=6\) can be a rectangle with bounding box \(2\times 1\times 0\), or fully 3-dimensional with bounding box \(1\times 1\times 1\). There are two fully 3-dimensional cases, one that avoids two opposite vertices of the bounding box and one that avoids two adjacent vertices. The rectangle can be its own spanning disk, and in the 3-dimensional cases we can use a spanning disk connecting the center of the bounding cube by a line segment to each point along the ring. Neither of these types of disks is crossed by any other lattice path.</p>

    <p style="text-align: center;"><img alt="Spanning disks for three grid 6-cycles" src="https://11011110.github.io/blog/assets/2021/grid-6-cycles.svg"/></p>
  </li>
  <li>
    <p>A lattice cycle of length \(\ell=8\) can be a rectangle with bounding box \(3\times 1\times 0\), a square with bounding box \(2\times 2\times 0\) or fully 3-dimensional with bounding box \(2\times 1\times 1\). It can also double back on itself and cover all vertices of a cube, with bounding box \(1\times 1\times 1\). All cases except the \(2\times 2\times 0\) square can be handled as in the length \(6\) cases; for instance, for the \(2\times 1\times 1\) bounding box we form a disk at the center of the box, connected by a line segment to all points of the ring. These cases cannot be crossed by any other lattice path. The \(2\times 2\times 0\) square can be crossed by a lattice path, through its center point, but only by one path. We can see from this that the shortest lattice representation of the <a href="https://en.wikipedia.org/wiki/Hopf_link">Hopf link</a> (two linked circles) is the obvious one formed from two length-\(8\) squares. However, these squares are still too small to be used in the Borromean rings.</p>
  </li>
  <li>
    <p>A lattice cycle of length \(\ell=10\) can be a rectangle with bounding box \(4\times 1\times 0\) or \(3\times 2\times 0\), or fully 3-dimensional with bounding box \(3\times 1\times 1\) or \(2\times 2\times 1\). The \(4\times 1\times 0\) rectangle and the center-point spanning disk of the \(3\times 1\times 1\) box cannot be crossed by any other lattice path, and the center-point spanning disk of the \(2\times 2\times 1\) can be crossed only by one, through its center edge. Using even-smaller bounding boxes doesn’t help.</p>
  </li>
</ul>

<p>That leaves only one problematic case, the \(3\times 2\times 0\) rectangle, of perimeter \(10\), which is shorter than the rectangles of the optimal representation but can nevertheless be crossed by two other lattice paths. In fact, this rectangle can be used as a component in a representation of the Borromean rings. It is even possible to use two of them! (I’ll leave this as an exercise.) So we need some other argument to prove that, when we use one or two of these short rectangles, we have to make up for it elsewhere by making something else extra-long.</p>

<p>If a \(3\times 2\times 0\) rectangle is a component of the Borromean rings, it must be twice by one of the other components, because if its two crossings were from different components it would have nonzero linking number with both of them, different from what happens in the Borromean rings. And the crossings must happen at the two interior lattice points of the rectangle, through paths that (to avoid each other and the boundary of the rectangle) must pass straight across the rectangle, at least for one unit on each side. The component that crosses the rectangle in this way consists of two loops connecting the pairs of ends of these two straight paths; any other connection pattern would lead to linking number \(2\), not zero. We can think of these two loops as being separate cycles, shortcut by the lattice edges between the endpoints of the two straight paths. And any disk that spans either of these two loops must itself be crossed by another component of the Borromean rings, because if one of the loops had an uncrossed spanning disk then we could wrap a spanning disk for the rectangle around it (like a glove around a hand) and create an uncrossed spanning disk for the rectangle as well.</p>

<p style="text-align: center;"><img alt="Spanning disk wrapping around a loop like a glove around a hand, adapted from https://commons.wikimedia.org/wiki/File:Disposable_nitrile_glove_with_transparent_background.png" src="https://11011110.github.io/blog/assets/2021/glove.png"/></p>

<p>By the analysis above, in order to be crossed by something else, both of the two shortcut loops of the component that crosses the \(3\times 2\times 0\) rectangle must have length at least \(8\). Adding in the two straight paths (and removing the two shortcut edges) shows that the component itself must have length at least \(18\). And if we have one component of length \(18\) and two more of length \(10\), we get total length at least \(38\), more than the length of the minimal representation. Since all representations that use components of length less than \(12\) are too long, the representation in which all three component lengths are exactly \(12\) must be the optimal one, QED.</p>

<p>Researching all this led me to an interesting paper by Dai, Ernst, Por, and Ziegler,  “<a href="https://doi.org/10.1142/S0218216519500858">The ropelengths of knots are almost linear in terms of their crossing numbers</a>” [<em>J. Knot Theory and its Ramifications</em>, 2019]. Ropelength is the minimum length of a 3d representation that can be thickened to a radius-1 tube without self-intersections. (Some sources use diameter in place of radius; this changes the numeric values by a factor of two but does not change the optimizing representations.) Doubling the dimensions of a lattice representation gives you such a representation, and on the other hand one can find short lattice representations by following the thickened tubes of a ropelength representation, so ropelength and lattice length are within constant factors of each other. Dai et al. use this to show that knots that can be drawn in the plane with few crossings also have small ropelength. It doesn’t work to use the plane embedding directly, adding a third coordinate to handle the crossings, because some planar graphs (like the <a href="https://en.wikipedia.org/wiki/Nested_triangles_graph">nested triangles graph</a>) have nonlinear total edge length in any planar lattice drawing. Instead, Dai et al show how to crumple up a planar drawing of any degree-four planar graph into a 3d integer lattice embedding of the graph, with near-linear total edge length, so that the faces of the drawing can also be embedded as disks that are not crossed by each other or the graph edges. One can then modify the lifted drawing to turn the degree-four vertices into crossings in the lifted topologically-planar surface formed by these faces, giving a grid representation of the original knot with near-linear total length.</p>

<p>The ropelength of the Borromean rings has also been the subject of some study. Doubling the grid rectangles and rounding off their corners produces three <a href="https://en.wikipedia.org/wiki/Stadium_(geometry)">stadia</a> with total perimeter \(12\pi+24\approx 61.7\). The same argument as above shows that each curve must be at least long enough for all its spanning disks to be crossable by two disjoint radius-1 tubes. Intuitively the smallest curve that can surround two tubes is a smaller stadium corresponding to the \(2\times 3\) rectangle, with length \(4\pi+4\). If so, this would give a lower bound of \(12\pi+12\approx 49.7\) for the total ropelength of the Borromean rings. The conjectured-optimal configuration, <a href="https://archive.bridgesmathart.org/2008/bridges2008-63.html">used for the logo of the International Mathematical Union</a>, uses three copies of a complicated two-lobed planar curve in roughly the same positions as the three rectangles or stadia; it is described carefully by Cantarella, Fu, Kusner, Sullivan, and Wrinkle, “<a href="http://dx.doi.org/10.2140/gt.2006.10.2055">Criticality for the Gehring link problem</a>” [<em>Geometry &amp; Topology</em> 2006] (section 10), and has length \(\approx 58.006\). The intuition that the \(2\times 3\) stadium is the shortest curve that can surround two others also appears to be stated as proven in this paper, in section 7.1. But they state that the best lower bound for the Borromean ropelength is \(12\pi\) so maybe the \(12\pi+12\) argument above is new?</p>

<p><strong>Update, February 17:</strong> In email, John Sullivan pointed me to a paper by Uberti, Janse van Rensburg, Orlandinit, Tesi, and Whittington, “<a href="https://doi.org/10.1007/978-1-4612-1712-1_9">Minimal links in the cubic lattice</a>” [<em>Topology and Geometry in Polymer Science</em>, 1998; see table 2, p. 97], which does the tedious computer search and comes up with the same result, that the shortest length for a lattice representation of the Borromean rings is 36. (I had searched for papers on lattice representations of the Borromean rings but didn’t find this one, probably failing because it identifies the Borromean rings only by the <a href="https://en.wikipedia.org/wiki/Alexander%E2%80%93Briggs_notation">Alexander–Briggs notation</a> \(6_2^3\), which is hard to search for.) John also tells me that the proof of ropelength-minimality of the \(2\times 3\) stadium is only for links in which it is linked with two other components, different enough from the situation here in which every spanning disk is crossed twice that the same proof doesn’t apply. So the question of whether this stadium really is the ropelength minimizer for components satisfying this crossed-twice condition seems to fall into the category of obvious topological facts that are difficult to prove, rather than being already known.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105743724683948185">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-02-16T16:11:00Z</updated>
    <published>2021-02-16T16:11:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-02-20T07:30:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/016</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/016" rel="alternate" type="text/html"/>
    <title>TR21-016 |  Unambiguous DNFs from Hex | 

	Shalev Ben-David, 

	Mika Göös, 

	Siddhartha Jain, 

	Robin Kothari</title>
    <summary>We exhibit an unambiguous $k$-DNF formula that requires CNF width $\tilde{\Omega}(k^{1.5})$. Our construction is inspired by the board game Hex and it is vastly simpler than previous ones, which achieved at best an exponent of $1.22$. Our result is known to imply several other improved separations in query and communication complexity (e.g., clique vs. independent set problem) and graph theory (Alon--Saks--Seymour problem).</summary>
    <updated>2021-02-16T12:53:50Z</updated>
    <published>2021-02-16T12:53:50Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T21:20:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/015</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/015" rel="alternate" type="text/html"/>
    <title>TR21-015 |  Hitting Sets for Orbits of Circuit Classes and Polynomial Families | 

	Chandan Saha, 

	Bhargav Thankey</title>
    <summary>The orbit of an $n$-variate polynomial $f(\mathbf{x})$ over a field $\mathbb{F}$ is the set $\mathrm{orb}(f) := \{f(A\mathbf{x}+\mathbf{b}) : A \in \mathrm{GL}(n,\mathbb{F}) \ \mathrm{and} \ \mathbf{b} \in \mathbb{F}^n\}$. This paper studies explicit hitting sets for the orbits of polynomials computable by certain well-studied circuit classes. This version of the hitting set problem is interesting as $\mathrm{orb}(f)$ is a natural subset of the set of affine projections of $f$. Affine projections of polynomials computable by seemingly weak circuit classes can be quite powerful. For example, the polynomial $\mathrm{IMM}_{3,d}$ -- the $(1,1)$-th entry of a product of $d$ generic $3 \times 3$ matrices -- is computable by a constant-width read-once oblivious algebraic branching program (ROABP), yet every polynomial computable by a size-$s$ general arithmetic formula is an affine projection of $\mathrm{IMM}_{3,\mathrm{poly}(s)}$. To our knowledge, no efficient hitting set construction was known for even $\mathrm{orb}(\mathrm{IMM}_{3, d})$ before this work. 

In this work, we give efficient constructions of hitting sets for the orbits of several interesting circuit classes and polynomial families. In particular, we give quasi-polynomial time hitting sets for the orbits of:

1. Low-individual-degree polynomials computable by commutative ROABP. This implies quasi-polynomial time hitting sets for the orbits of multilinear sparse polynomials and the orbits of the elementary symmetric polynomials.

2. Multilinear polynomials computable by constant-width ROABP. This implies a quasi-polynomial time hitting set for the orbit of $\mathrm{IMM}_{3,d}$.

3. Polynomials computable by constant-depth, constant-occur formulas with low-individual-degree sparse polynomials at the leaves. This implies quasi-polynomial time hitting sets for the orbits of multilinear depth-4 circuits with constant top fan-in, and also poly-time hitting sets for the orbits of the power symmetric polynomials and the sum-product polynomials. 

4. Polynomials computable by occur-once formulas with low-individual-degree sparse polynomials at the leaves. 

We say a polynomial has low individual degree if the degree of every variable in the polynomial is at most $\mathrm{poly}(\log n)$, where $n$ is the number of variables.

The first two results are obtained by building upon the rank concentration by translation technique of [Agrawal-Saha-Saxena, STOC'13]; the second result also uses the merge-and-reduce idea from [Forbes-Shpilka, APPROX'13], [Forbes-Saptharishi-Shpilka, STOC'14]. The proof of the third result applies the algebraic independence based technique of [Agrawal-Saha-Saptharishi-Saxena, STOC'12], [Beecken-Mittmann-Saxena, ICALP'11] to reduce to the case of constructing hitting sets for orbits of sparse polynomials. A similar reduction using the Shpilka-Volkovich (SV) generator based argument in [Shpilka-Volkovich, STOC'08, APPROX-RANDOM'09] yields the fourth result. The SV generator plays an important role in all the four results.</summary>
    <updated>2021-02-16T08:54:53Z</updated>
    <published>2021-02-16T08:54:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-21T10:37:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5330</id>
    <link href="https://www.scottaaronson.com/blog/?p=5330" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5330#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5330" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">On standing up sans backbone</title>
    <summary xml:lang="en-US">Note: To get myself into the spirit of writing this post, tonight I watched the 2019 movie Mr. Jones, about the true story of the coverup of Stalin’s 1932-3 mass famine by New York Times journalist Walter Duranty. Recommended! In my last post, I wrote that despite all my problems with Cade Metz’s New York […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><blockquote class="wp-block-quote"><p><strong>Note:</strong> To get myself into the spirit of writing this post, tonight I watched the 2019 movie <a href="https://www.amazon.com/Mr-Jones-James-Norton/dp/B089XVJB9S/ref=sr_1_1?dchild=1&amp;keywords=Mr.+Jones+%282019%29&amp;qid=1613446265&amp;s=instant-video&amp;sr=1-1">Mr. Jones</a>, about the true story of the coverup of Stalin’s 1932-3 mass famine by <em>New York Times</em> journalist <a href="https://en.wikipedia.org/wiki/Walter_Duranty">Walter Duranty</a>.  Recommended!</p></blockquote>



<p>In my <a href="https://www.scottaaronson.com/blog/?p=5310">last post</a>, I wrote that despite all my problems with Cade Metz’s <em>New York Times</em> hit piece on Scott Alexander, I’d continue talking to journalists—even Metz himself, I added, assuming he’d still talk to me after my public disparagement of his work.  Over the past few days, though, the many counterarguments in my comments section and elsewhere gradually caused me to change my mind.  I now feel like to work with Metz again, even just on some quantum computing piece, would be to reward—and to be seen as rewarding—journalistic practices that are making the world worse, and that this consideration overrides even my extreme commitment to openness.</p>



<p>At the least, before I could talk to Metz again, I’d need a better understanding of how the hit piece happened.  What was the role of the editors?  How did the original hook—namely, the rationalist community’s early rightness about covid-19—disappear entirely from the article?  How did the piece manage to evince so little <em>curiosity</em> about such an unusual subculture and such a widely-admired writer?  How did it fail so completely to engage with the rationalists’ <em>ideas</em>, instead jumping immediately to “six degrees of Peter Thiel” and other reductive games?  How did an angry SneerClubber, David Gerard, end up (according to <a href="https://twitter.com/davidgerard/status/1360735880466604040">his own boast</a>) basically dictating the NYT piece’s content?</p>



<p>It’s always ripping-off-a-bandage painful to admit when trust in another person was wildly misplaced—for then who<em> else</em> can we not trust?  But sometimes that’s the truth of it.</p>



<p>I continue to believe passionately in the centrality of good journalism to a free society.  I’ll continue to talk to journalists often, about quantum computing or whatever else.  I also recognize that the NYT is a large, heterogeneous institution (I myself <a href="https://www.nytimes.com/2011/12/06/science/scott-aaronson-quantum-computing-promises-new-insights.html">published</a> in it <a href="https://www.nytimes.com/2019/10/30/opinion/google-quantum-computer-sycamore.html">twice</a>); it’s not hard to imagine that many of its own staff take issue with the SSC piece.</p>



<p>But let’s be clear about the stakes here.  In the discussion of my last post, I <a href="https://www.scottaaronson.com/blog/?p=5310#comment-1878641">described</a> the NYT as “still the main vessel of consensus reality in <s>human civilization</s>” [alright, alright, American civilization!].  What’s really at issue, beyond the treatment of a single blogger, is whether the NYT can continue serving that central role in a world reshaped by social media, resurgent fascism, and entitled wokery.</p>



<p>Sure, we all know that the NYT has been disastrously wrong before: it ridiculed Goddard’s dream of spaceflight, denied the Holodomor, relegated the Holocaust to the back pages while it was happening, published the fabricated justifications for the Iraq War.  But the NYT and a few other publications were still the blockchain of reality, the engine of the consensus of all that is, the last bulwark against the conspiracists and the anti-vaxxers and the empowered fabulists and the horned insurrectionists storming the Capitol, because there was no ability to coordinate around any serious alternative.  I’m <em>still</em> skeptical that there’s a serious alternative, but I now look more positively than I did just a few days ago on attempts to create one.</p>



<p>To all those who called me naïve or a coward for having cooperated with the NYT: believe me, I’m well aware that I wasn’t born with much backbone.  (I am, after all, that guy on the Internet who famously once planned on a life of celibate asceticism, or more likely suicide, rather than asking women out and thereby risking eternal condemnation as a misogynistic sexual harasser by the normal, the popular, the socially adept, the … <em>humanities grads</em> and the <em>journalists</em>.)  But whenever I need a pick-me-up, I tell myself that rather than being ashamed about my lack of a backbone, I can take pride in having occasionally managed to stand even without one.</p></div>
    </content>
    <updated>2021-02-16T05:33:59Z</updated>
    <published>2021-02-16T05:33:59Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Self-Help"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Obviously I'm Not Defending Aaronson"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Self-Referential"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-02-19T21:03:56Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/02/15/linkage</id>
    <link href="https://11011110.github.io/blog/2021/02/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Let’s not dumb down the history of computer science (\(\mathbb{M}\), via). A 2014 plea from Knuth to historians of computer science to stop ignoring the technical parts of the history, reprinted this month in CACM.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://cacm.acm.org/opinion/articles/250078-lets-not-dumb-down-the-history-of-computer-science/fulltext">Let’s not dumb down the history of computer science</a> (<a href="https://mathstodon.xyz/@11011110/105663355639241434">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/190214/Lets-Not-Dumb-Down-the-History-of-Computer-Science">via</a>). A 2014 plea from Knuth to historians of computer science to stop ignoring the technical parts of the history, reprinted this month in CACM.</p>
  </li>
  <li>
    <p><a href="https://www.newscientist.com/article/2266041-tom-gaulds-runaway-lobster-telephone-problem/">Studies in ethical surrealism: the runaway lobster telephone problem</a> (<a href="https://mathstodon.xyz/@11011110/105664424099324982">\(\mathbb{M}\)</a>). I was pleased to learn that <a href="https://en.wikipedia.org/wiki/Lobster_Telephone">the lobster telephone depicted in this cartoon is a real objet d’art</a>.</p>
  </li>
  <li>
    <p><a href="https://www.archim.org.uk/eureka/archive/">Archive of back issues of Eureka</a> (<a href="https://mathstodon.xyz/@11011110/105671433589477405">\(\mathbb{M}\)</a>, <a href="https://aperiodical.com/2021/02/aperiodical-news-roundup-january-2021/">via</a>), the recreational mathematics journal of the Cambridge Archimedeans, now online for open access. On Wikipedia, the popular articles from Eureka appear to be Dyson’s work on ranks of partitions, in #8, Haselgrove &amp; Haselgrove on polyominoes, in #23, Penrose on pentaplexity, in #39, and Leinster on his eponymous groups, in #55.</p>
  </li>
  <li>
    <p><a href="https://philpapers.org/rec/BOBFPT">In a new book chapter, Susanne Bobzien claims that famous philosopher of logic Gottlob Frege plagiarized extensively from the Stoic logicians</a> (<a href="https://mathstodon.xyz/@11011110/105680228594244342">\(\mathbb{M}\)</a>, <a href="https://dailynous.com/2021/02/03/frege-plagiarize-stoics/">via</a>, <a href="https://www.metafilter.com/190330/Frege-plagiarized-the-Stoics">via2</a>, <a href="https://handlingideas.blog/2021/02/05/the-stoic-foundations-of-analytic-philosophy-on-susanne-bobziens-groundbreaking-discovery-in-frege-and-prantl/">see also</a>).</p>
  </li>
  <li>
    <p>How did I not know about the <a href="https://civs.cs.cornell.edu/">Condorcet Internet Voting Service</a> before (<a href="https://mathstodon.xyz/@11011110/105683306227631389">\(\mathbb{M}\)</a>)? Set up public or private polls and collate the results with your favorite Condorcet rank aggregation method (at least, if your favorite is one of the five they implement, which it probably is). Their public polls are kind of insipid, though, and in comments David Bremner brings up their past history of enabling online abusers.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@RefurioAnachro/105684468712016832">Perkel’s graph and the 57-cell</a>, multi-post sequence on an abstract 4-polytope and associated distance-regular graph, by Refurio Anachro.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html">The Victoria Delfino Problems</a> (<a href="https://mathstodon.xyz/@11011110/105694384937282737">\(\mathbb{M}\)</a>). Bill Gasarch blogs about mathematics problems named after non-mathematicians, in this case a Los Angeles based real estate agent.</p>
  </li>
  <li>
    <p>The speech recognition system Zoom and/or my university are using to auto-caption my recorded lectures (whatever it is) really doesn’t like the word “bipartite”, heavily used in my lecture on matching (<a href="https://mathstodon.xyz/@11011110/105700305493373685">\(\mathbb{M}\)</a>). It came out “bipartisan”, “invite part tight”, “by party”, “by protect”, “by apartheid”, “by part aight”, and “by partnership”. Also “spanning forest” is now “Hispanic forest”, but mysteriously it got “spanning tree” right.</p>
  </li>
  <li>
    <p><a href="https://retractionwatch.com/2021/02/09/20-ways-to-spot-the-work-of-paper-mills/">20 ways to spot the work of paper mills</a> (<a href="https://mathstodon.xyz/@11011110/105702699189961130">\(\mathbb{M}\)</a>). However one, using a non-institutional email address, is not “a bad global habit”, but deliberate. I have no thought of moving but do not want my entire professional life tied by email to my employer. My UCI address keeps student emails private but I tend to use gmail for off-campus concerns such as publishers. And not all scholars have institutions who can provide emails. If they refuse my email, I refuse to publish with them.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.01543">Ben Green presents super-polynomial lower bounds for off-diagonal van der Waerden numbers \(W(3,k)\)</a> (<a href="https://mathstodon.xyz/@11011110/105713896982428000">\(\mathbb{M}\)</a>, <a href="https://gilkalai.wordpress.com/2021/02/08/to-cheer-you-up-in-difficult-times-20-ben-green-presents-super-polynomial-lower-bounds-for-off-diagonal-van-der-waerden-numbers-w3k/">via</a>). \(W(3,k)\) is the smallest \(N\) such that a 2-coloring of \([N]\) has a 3-term arithmetic progression of one color or a \(k\)-term progression of the other. It was previously known to be subexponential and thought to be only quadratic.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/382940/440">The compound of an 11-simplex in an 11-hypercube (as a subset of its vertices) has the Mathieu group M11 as its symmetries</a> (<a href="https://mathstodon.xyz/@11011110/105717258433892969">\(\mathbb{M}\)</a>, <a href="https://cp4space.hatsya.com/2021/02/08/a-curious-construction-of-the-mathieu-group-m11/">via</a>). The via link goes on to describe how to find two dual 11-simplices in the same hypercube from the perfect ternary Golay code, much like the two simplices in a 3-cube that form the stella octangula.</p>
  </li>
  <li>
    <p><a href="https://distill.pub/selforg/2021/textures/">Self-organizing textures</a> (<a href="https://mathstodon.xyz/@11011110/105719504474157451">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26112959">via</a>). A small input image + “neural cellular automata” magic leads to organic-looking image textures.</p>
  </li>
  <li>
    <p><a href="https://cse.buffalo.edu/socg21/accepted.html">Accepted papers for the Symposium on Computational Geometry</a> (SoCG; <a href="https://mathstodon.xyz/@11011110/105727235598338446">\(\mathbb{M}\)</a>). Decisions are out for the Symposium on Theory of Computing (STOC) but I haven’t seen a public list yet. Upcoming submission deadlines include the <a href="https://projects.cs.dal.ca/wads2021/">Algorithms and Data Structures Symposium</a> (WADS, Feb. 20), <a href="https://wg2021.mimuw.edu.pl/">Graph-Theoretic Concepts in Computer Science</a> (WG, Mar. 3), and the new <a href="https://www.siam.org/conferences/cm/conference/acda21">SIAM Conference on Applied and Computational Discrete Algorithms</a> (ACDA21, Mar. 1).</p>
  </li>
  <li>
    <p><a href="http://jdh.hamkins.org/ode-to-hippasus/">A new contribution of Hypatia to mathematics</a> (<a href="https://mathstodon.xyz/@11011110/105732195870260348">\(\mathbb{M}\)</a>). Not the ancient Hypatia, but Hypatia Hamkins, and her parents, philosopher Barbara Gail Montero and logician Joel David Hamkins; the contribution is a verse proof of the irrationality of \(\sqrt{2}\).</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/channel/UC8bRNi3tJX-tfR_RMtyWR7w">Computational Geometry YouTube channel</a> (<a href="https://mathstodon.xyz/@11011110/105739350560629546">\(\mathbb{M}\)</a>). This has been set up by Sariel Har-Peled and Sándor Fekete, and is recording talks from the New York Geometry Seminar. So far there are eleven, of roughly an hour length each.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-02-15T21:45:00Z</updated>
    <published>2021-02-15T21:45:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-02-20T07:30:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18111</id>
    <link href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/" rel="alternate" type="text/html"/>
    <title>Pigenhole Principle</title>
    <summary>Mathematics is based on the application of simple ideas over and over: From tiny nuts do big trees grow. Jorgen Veisdal is an assistant professor at the Norwegian University of Science and Technology. He is also the editor in chief at Cantor’s Paradise, which is a publication of math and science essays. Today I thought […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Mathematics is based on the application of simple ideas over and over: From tiny nuts do big trees grow. </em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/jv/" rel="attachment wp-att-18113"><img alt="" class="alignright wp-image-18113" src="https://rjlipton.files.wordpress.com/2021/02/jv.png?w=150" width="150"/></a></p>
<p>
Jorgen Veisdal is an assistant professor at the Norwegian University of Science and Technology. He is also the editor in chief at <a href="https://medium.com/cantors-paradise">Cantor’s Paradise</a>, which is a publication of math and science essays. </p>
<p>
Today I thought we would discuss a <a href="https://medium.com/cantors-paradise/the-pigeonhole-principle-e4c637940619">post</a> of his on the famous Pigenhole Princeiple (PP).</p>
<p>
Recall the PP states that if <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n}"/> items are put into <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{m}"/> boxes, with <img alt="{n &gt; m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3E+m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{n &gt; m}"/>, then at least one box must contain more than one item.</p>
<p>
The paradox in my opinion is that this idea has any power at all. I wonder if I could explain why it was stated as an explicit <a href="https://en.wikipedia.org/wiki/Pigeonhole_principle">principle</a> by the famous Peter Dirichlet under the name Schubfachprinzip (“drawer principle” or “shelf principle”) in 1834. </p>
<p>
Parts of mathematics not only use PP, but could not live without it. Other parts of mathematics—I believe—are almost untouched by it. Am I right about this? Number theory and combinatorics especially Ramsey theory could not survive without it. What happens in your favorite area? Is there some area of math that is almost untouched by PP?</p>
<p>
</p><table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/pigeons/" rel="attachment wp-att-18114"><img alt="" class="aligncenter  wp-image-18114" src="https://rjlipton.files.wordpress.com/2021/02/pigeons.png?w=400" width="400"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
</p><p/><h2> An Example of PP </h2><p/>
<p>
The main issue is why is PP so indispensable to some areas of math. But I though it might be fun to give a sample type of proof that uses PP.</p>
<p>
Prove that however one selects 55 integers 	</p>
<p align="center"><img alt="\displaystyle  1 \le x_{1} &lt; x_{2} &lt; x_{3} &lt; \dots &lt; x_{55} \le 100," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+%5Cle+x_%7B1%7D+%3C+x_%7B2%7D+%3C+x_%7B3%7D+%3C+%5Cdots+%3C+x_%7B55%7D+%5Cle+100%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  1 \le x_{1} &lt; x_{2} &lt; x_{3} &lt; \dots &lt; x_{55} \le 100,"/></p>
<p>there will be some two that differ by 9, some two that differ by 10, a pair that differ by 12, and a pair that differ by 13. Surprisingly, there need not be a pair of numbers that differ by 11. </p>
<p/><h2> Proof </h2><p/>
<p>Let <img alt="{f(y,x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28y%2Cx%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f(y,x)}"/> be the number of collisions when placing <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{y}"/> into <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x}"/>. Claim:	</p>
<p align="center"><img alt="\displaystyle  f(x+d,x) \ge f(x+1,x), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%2Bd%2Cx%29+%5Cge+f%28x%2B1%2Cx%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  f(x+d,x) \ge f(x+1,x), "/></p>
<p>for <img alt="{x \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x \ge 1}"/> and <img alt="{d \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{d \ge 1}"/> and 	</p>
<p align="center"><img alt="\displaystyle  f(x+1,x) \ge f(x,x-1), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%2B1%2Cx%29+%5Cge+f%28x%2Cx-1%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  f(x+1,x) \ge f(x,x-1), "/></p>
<p>for <img alt="{x \ge 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x \ge 2}"/>. </p>
<p>
Note the first is really simple. Consider the first <img alt="{x+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x+1}"/> pigeons. They are placed into <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x}"/> places and the inequality follows. The second is about the same. Consider the first <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x}"/> pigeons. There are two cases. They are all placed in <img alt="{x-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x-1}"/> places. Then we are done. So there must have been some placed into the last place. But if two are there then we are also done. So <img alt="{x-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x-1}"/> are placed into <img alt="{x-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x-1}"/>. But where does the last one go? In either case we are done.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>Are there areas that almost never use the PP? I would like to hear about areas that just do not use PP. </p>
<p/><p><br/>
[some word fixes]</p></font></font></div>
    </content>
    <updated>2021-02-15T17:28:11Z</updated>
    <published>2021-02-15T17:28:11Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="pigen hole"/>
    <category term="principle"/>
    <category term="Ramsey"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-02-21T21:21:05Z</updated>
    </source>
  </entry>
</feed>
