<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-05-02T23:37:19Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3204953698007493895</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3204953698007493895/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/the-next-chapter.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3204953698007493895" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3204953698007493895" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/the-next-chapter.html" rel="alternate" type="text/html"/>
    <title>The Next Chapter</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="separator" style="clear: both; text-align: center;">
<a href="https://4.bp.blogspot.com/-R3cQu_-p5K8/XMhzxr-ZDrI/AAAAAAABnYQ/4tXefV5yICMHnD_U4g8QmFFftzHzeOWTQCLcBGAs/s1600/COS_stacked_blk_red.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="95" src="https://4.bp.blogspot.com/-R3cQu_-p5K8/XMhzxr-ZDrI/AAAAAAABnYQ/4tXefV5yICMHnD_U4g8QmFFftzHzeOWTQCLcBGAs/s320/COS_stacked_blk_red.jpg" width="320"/></a></div>
<div>
<br/></div>
I've <a href="https://news.iit.edu/stories/2019/05/lance-fortnow-designated-new-college-science-dean">accepted a position</a> as Dean of the <a href="https://science.iit.edu/">College of Science</a> at the Illinois Institute of Technology in Chicago starting in August. It's an exciting opportunity to really build up the sciences and computing in the city that I have spent the bulk of my academic career and grew to love.<br/>
<div>
<br/></div>
<div>
I had a fantastic time at Georgia Tech over the last seven years working with an incredible faculty, staff and students in the School of Computer Science. This is a special place and I enjoyed watching the school, the institute and the City of Atlanta grow and evolve.<br/>
<br/>
After I <a href="https://twitter.com/fortnow/status/1123644799825907712">tweeted</a> the news yesterday, Bill Cook reminded me that<br/>
<blockquote class="tr_bq">
Illinois Tech was the long-time home of Karl Menger, the first person to pose the problem of determining the complexity of the TSP. Now you can settle it!</blockquote>
I wouldn't bet on my settling the complexity of traveling salesman even if I didn't have a college to run. But it goes to remind us that wherever you go in life, P and NP will be right there waiting for you. </div></div>
    </content>
    <updated>2019-05-02T12:20:00Z</updated>
    <published>2019-05-02T12:20:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-02T14:13:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00369</id>
    <link href="http://arxiv.org/abs/1905.00369" rel="alternate" type="text/html"/>
    <title>Fast hashing with Strong Concentration Bounds</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aamand:Anders.html">Anders Aamand</a>, Jakob B. T. Knudsen, Mathias B. T. Knudsen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rasmussen:Peter_M=_R=.html">Peter M. R. Rasmussen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thorup:Mikkel.html">Mikkel Thorup</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00369">PDF</a><br/><b>Abstract: </b>Previous work on tabulation hashing of P\v{a}tra\c{s}cu and Thorup from
STOC'11 on simple tabulation and from SODA'13 on twisted tabulation offered
Chernoff-style concentration bounds on hash based sums, but under some quite
severe restrictions on the expected values of these sums. More precisely, the
basic idea in tabulation hashing is to view a key as consisting of $c=O(1)$
characters, e.g., a 64-bit key as $c=8$ characters of 8-bits. The character
domain $\Sigma$ should be small enough that character tables of size $|\Sigma|$
fit in fast cache. The schemes then use $O(1)$ tables of this size, so the
space of tabulation hashing is $O(|\Sigma|)$. However the above concentration
bounds only apply if the expected sums are $\ll |\Sigma|$.
</p>
<p>To see the problem, consider the very simple case where we use tabulation
hashing to throw $n$ balls into $m$ bins and apply Chernoff bounds to the
number of balls that land in a given bin. We are fine if $n=m$, for then the
expected value is $1$. However, if $m=2$ bins as when tossing $n$ unbiased
coins, then the expectancy $n/2$ is $\gg |\Sigma|$ for large data sets, e.g.,
data sets that don't fit in fast cache.
</p>
<p>To handle expectations that go beyond the limits of our small space, we need
a much more advanced analysis of simple tabulation, plus a new tabulation
technique that we call tabulation-permutation hashing which is at most twice as
slow as simple tabulation. No other hashing scheme of comparable speed offers
similar Chernoff-style concentration bounds.
</p></div>
    </summary>
    <updated>2019-05-02T23:26:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00350</id>
    <link href="http://arxiv.org/abs/1905.00350" rel="alternate" type="text/html"/>
    <title>Coordinatizing Data With Lens Spaces and Persistent Cohomology</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Luis Polanco, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perea:Jose_A=.html">Jose A. Perea</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00350">PDF</a><br/><b>Abstract: </b>We introduce here a framework to construct coordinates in \emph{finite} Lens
spaces for data with nontrivial 1-dimensional $\mathbb{Z}_q$ persistent
cohomology, $q\geq 3$. Said coordinates are defined on an open neighborhood of
the data, yet constructed with only a small subset of landmarks. We also
introduce a dimensionality reduction scheme in $S^{2n-1}/\mathbb{Z}_q$
(Lens-PCA: $\mathsf{LPCA}$), and demonstrate the efficacy of the pipeline
$PH^1(\;\cdot\; ; \mathbb{Z}_q)$ class $\Rightarrow$ $S^{2n-1}/\mathbb{Z}_q$
coordinates $\Rightarrow$ $\mathsf{LPCA}$, for nonlinear (topological)
dimensionality reduction.
</p></div>
    </summary>
    <updated>2019-05-02T23:34:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00340</id>
    <link href="http://arxiv.org/abs/1905.00340" rel="alternate" type="text/html"/>
    <title>Independent Set Reconfiguration Parameterized by Modular-Width</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Belmonte:R=eacute=my.html">Rémy Belmonte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanaka:Tesshu.html">Tesshu Hanaka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lampis:Michael.html">Michael Lampis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ono:Hirotaka.html">Hirotaka Ono</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Otachi:Yota.html">Yota Otachi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00340">PDF</a><br/><b>Abstract: </b>Independent Set Reconfiguration is one of the most well-studied problems in
the setting of combinatorial reconfiguration. It is known that the problem is
PSPACE-complete even for graphs of bounded bandwidth. This fact rules out the
tractability of parameterizations by most well-studied structural parameters as
most of them generalize bandwidth. In this paper, we study the parameterization
by modular-width, which is not comparable with bandwidth. We show that the
problem parameterized by modular-width is fixed-parameter tractable under all
previously studied rules TAR, TJ, and TS. The result under TAR resolves an open
problem posed by Bonsma [WG 2014, JGT 2016].
</p></div>
    </summary>
    <updated>2019-05-02T23:28:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00333</id>
    <link href="http://arxiv.org/abs/1905.00333" rel="alternate" type="text/html"/>
    <title>A convex cover for closed unit curves has area at least 0.0975</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grechuk:Bogdan.html">Bogdan Grechuk</a>, Sittichoke Som-Am <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00333">PDF</a><br/><b>Abstract: </b>We combine geometric methods with numerical box search algorithm to show that
the minimal area of a convex set on the plane which can cover every closed
plane curve of unit length is at least 0.0975. This improves the best previous
lower bound of 0.096694. In fact, we show that the minimal area of convex hull
of circle, equilateral triangle, and rectangle of perimeter $1$ is between
0.0975 and 0.09763.
</p></div>
    </summary>
    <updated>2019-05-02T23:34:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00305</id>
    <link href="http://arxiv.org/abs/1905.00305" rel="alternate" type="text/html"/>
    <title>Parameterized Complexity of Conflict-free Graph Coloring</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodlaender:Hans_L=.html">Hans L. Bodlaender</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolay:Sudeshna.html">Sudeshna Kolay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pieterse:Astrid.html">Astrid Pieterse</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00305">PDF</a><br/><b>Abstract: </b>Given a graph G, a q-open neighborhood conflict-free coloring or
q-ONCF-coloring is a vertex coloring $c:V(G) \rightarrow \{1,2,\ldots,q\}$ such
that for each vertex $v \in V(G)$ there is a vertex in $N(v)$ that is uniquely
colored from the rest of the vertices in $N(v)$. When we replace $N(v)$ by the
closed neighborhood $N[v]$, then we call such a coloring a q-closed
neighborhood conflict-free coloring or simply q-CNCF-coloring. In this paper,
we study the NP-hard decision questions of whether for a constant q an input
graph has a q-ONCF-coloring or a q-CNCF-coloring. We will study these two
problems in the parameterized setting.
</p>
<p>First of all, we study running time bounds on FPT-algorithms for these
problems, when parameterized by treewidth. We improve the existing upper
bounds, and also provide lower bounds on the running time under ETH and SETH.
</p>
<p>Secondly, we study the kernelization complexity of both problems, using
vertex cover as the parameter. We show that both $(q \geq 2)$-ONCF-coloring and
$(q \geq 3)$-CNCF-coloring cannot have polynomial kernels when parameterized by
the size of a vertex cover unless $NP \in coNP/poly$. However, we obtain a
polynomial kernel for 2-CNCF-coloring parameterized by vertex cover.
</p>
<p>We conclude with some combinatorial results. Denote $\chi_{ON}(G)$ and
$\chi_{CN}(G)$ to be the minimum number of colors required to ONCF-color and
CNCF-color G, respectively. Upper bounds on $\chi_{CN}(G)$ with respect to
structural parameters like minimum vertex cover size, minimum feedback vertex
set size and treewidth are known. To the best of our knowledge only an upper
bound on $\chi_{ON}(G)$ with respect to minimum vertex cover size was known. We
provide tight bounds for $\chi_{ON}(G)$ with respect to minimum vertex cover
size. Also, we provide the first upper bounds on $\chi_{ON}(G)$ with respect to
minimum feedback vertex set size and treewidth.
</p></div>
    </summary>
    <updated>2019-05-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00164</id>
    <link href="http://arxiv.org/abs/1905.00164" rel="alternate" type="text/html"/>
    <title>On a conditional inequality in Kolmogorov complexity and its applications in communication complexity</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Andrei Romashchenko, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zimand:Marius.html">Marius Zimand</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00164">PDF</a><br/><b>Abstract: </b>Romashchenko and Zimand~\cite{rom-zim:c:mutualinfo} have shown that if we
partition the set of pairs $(x,y)$ of $n$-bit strings into combinatorial
rectangles, then $I(x:y) \geq I(x:y \mid t(x,y)) - O(\log n)$, where $I$
denotes mutual information in the Kolmogorov complexity sense, and $t(x,y)$ is
the rectangle containing $(x,y)$. We observe that this inequality can be
extended to coverings with rectangles which may overlap. The new inequality
essentially states that in case of a covering with combinatorial rectangles,
</p>
<p>$I(x:y) \geq I(x:y \mid t(x,y)) - \log \rho - O(\log n)$, where $t(x,y)$ is
any rectangle containing $(x,y)$ and $\rho$ is the thickness of the covering,
which is the maximum number of rectangles that overlap. We discuss applications
to communication complexity of protocols that are nondeterministic, or
randomized, or Arthur-Merlin, and also to the information complexity of
interactive protocols.
</p></div>
    </summary>
    <updated>2019-05-02T23:24:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00163</id>
    <link href="http://arxiv.org/abs/1905.00163" rel="alternate" type="text/html"/>
    <title>Separate Chaining Meets Compact Hashing</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=ouml=ppl:Dominik.html">Dominik Köppl</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00163">PDF</a><br/><b>Abstract: </b>While separate chaining is a common strategy for resolving collisions in a
hash table taught in most textbooks, compact hashing is a less common technique
for saving space when hashing integers whose domain is relatively small with
respect to the problem size. It is widely believed that hash tables waste a
considerable amount of memory, as they either leave allocated space untouched
(open addressing) or store additional pointers (separate chaining). For the
former, Cleary introduced the compact hashing technique that stores only a part
of a key to save space. However, as can be seen by the line of research
focusing on compact hash tables with open addressing, there is additional
information, called displacement, required for restoring a key. There are
several representations of this displacement information with different space
and time trade-offs. In this article, we introduce a separate chaining hash
table that applies the compact hashing technique without the need for the
displacement information. Practical evaluations reveal that insertions in this
hash table are faster or use less space than all previously known compact hash
tables on modern computer architectures when storing sufficiently large
satellite data.
</p></div>
    </summary>
    <updated>2019-05-02T23:31:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00157</id>
    <link href="http://arxiv.org/abs/1905.00157" rel="alternate" type="text/html"/>
    <title>Improved Algorithms for the Bichromatic Two-Center Problem for Pairs of Points</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Haitao.html">Haitao Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xue:Jie.html">Jie Xue</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00157">PDF</a><br/><b>Abstract: </b>We consider a bichromatic two-center problem for pairs of points. Given a set
$S$ of $n$ pairs of points in the plane, for every pair, we want to assign a
red color to one point and a blue color to the other, in such a way that the
value $\max\{r_1,r_2\}$ is minimized, where $r_1$ (resp., $r_2$) is the radius
of the smallest enclosing disk of all red (resp., blue) points. Previously, an
exact algorithm of $O(n^3\log^2 n)$ time and a $(1+\varepsilon)$-approximate
algorithm of $O(n + (1/\varepsilon)^6 \log^2 (1/\varepsilon))$ time were known.
In this paper, we propose a new exact algorithm of $O(n^2\log^2 n)$ time and a
new $(1+\varepsilon)$-approximate algorithm of $O(n + (1/\varepsilon)^3 \log^2
(1/\varepsilon))$ time.
</p></div>
    </summary>
    <updated>2019-05-02T23:34:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00148</id>
    <link href="http://arxiv.org/abs/1905.00148" rel="alternate" type="text/html"/>
    <title>Inventory Routing Problem with Facility Location</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiao:Yang.html">Yang Jiao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ravi:R=.html">R. Ravi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00148">PDF</a><br/><b>Abstract: </b>We study problems that integrate depot location decisions along with the
inventory routing problem of serving clients from these depots over time
balancing the costs of routing vehicles from the depots with the holding costs
of demand delivered before they are due. Since the inventory routing problem is
already complex, we study the version that assumes that the daily vehicle
routes are direct connections from the depot thus forming stars as solutions,
and call this problem the Star Inventory Routing Problem with Facility Location
(SIRPFL). As a stepping stone to solving SIRPFL, we first study the Inventory
Access Problem (IAP), which is the single depot, single client special case of
IRP. The Uncapacitated IAP is known to have a polynomial time dynamic program.
We provide an NP-hardness reduction for Capacitated IAP where each demand
cannot be split among different trips. We give a $3$-approximation for the case
when demands can be split and a $6$-approximation for the unsplittable case.
For Uncapacitated SIRPFL, we provide a $12$-approximation by rounding an LP
relaxation. Combining the ideas from Capacitated IAP and Uncapacitated SIRPFL,
we obtain a $24$-approximation for Capacitated Splittable SIRPFL and a
$48$-approximation for the most general version, the Capacitated Unsplittable
SIRPFL.
</p></div>
    </summary>
    <updated>2019-05-02T23:31:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00118</id>
    <link href="http://arxiv.org/abs/1905.00118" rel="alternate" type="text/html"/>
    <title>A Detailed Analysis of Quicksort Algorithms with Experimental Mathematics</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Yukun Yao <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00118">PDF</a><br/><b>Abstract: </b>We study several variants of single-pivot and multi-pivot Quicksort
algorithms and consider them as discrete probability problems. With
experimental mathematics, explicit expressions for expectations, variances and
even higher moments of their numbers of comparisons and swaps can be obtained.
For some variants, Monte Carlo experiments are performed, the numerical results
are demonstrated and the scaled limiting distribution is also discussed.
</p></div>
    </summary>
    <updated>2019-05-02T23:31:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00091</id>
    <link href="http://arxiv.org/abs/1905.00091" rel="alternate" type="text/html"/>
    <title>Derandomization from Algebraic Hardness: Treading the Borders</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Mrinal.html">Mrinal Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saptharishi:Ramprasad.html">Ramprasad Saptharishi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomon:Noam.html">Noam Solomon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00091">PDF</a><br/><b>Abstract: </b>A hitting-set generator (HSG) is a polynomial map $Gen:\mathbb{F}^k \to
\mathbb{F}^n$ such that for all $n$-variate polynomials $Q$ of small enough
circuit size and degree, if $Q$ is non-zero, then $Q\circ Gen$ is non-zero. In
this paper, we give a new construction of such a HSG assuming that we have an
explicit polynomial of sufficient hardness in the sense of approximative or
border complexity. Formally, we prove the following result over any
characteristic zero field $\mathbb{F}$:
</p>
<p>Suppose $P(z_1,\ldots, z_k)$ is an explicit $k$-variate degree $d$ polynomial
that is not in the border of circuits of size $s$. Then, there is an explicit
hitting-set generator $Gen_P:\mathbb{F}^{2k} \rightarrow \mathbb{F}^n$ such
that every non-zero $n$-variate degree $D$ polynomial $Q(x_1, x_2, \ldots,
x_n)$ in the border of size $s'$ circuits satisfies $Q \neq 0 \Rightarrow Q
\circ \mathsf{Gen}_P \neq 0$, provided $n^{10k}d^2 D s'&lt; s$.
</p>
<p>This is the first HSG in the algebraic setting that yields a complete
derandomization of polynomial identity testing (PIT) for general circuits from
a suitable algebraic hardness assumption.
</p>
<p>As a direct consequence, we prove the following bootstrapping result for PIT:
</p>
<p>Let $\delta &gt; 0$ be any constant and $k$ be a large enough constant. Suppose,
for every $s \geq k$, there is an explicit hitting set of size $s^{k-\delta}$
for all degree $s$ polynomials in the border of $k$-variate size $s$ algebraic
circuits. Then, there is an explicit hitting set of size $poly(s)$ for the
border $s$-variate algebraic circuits of size $s$ and degree $s$.
</p>
<p>Unlike the prior constructions of such maps [NW94, KI04, AGS18, KST19], our
construction is purely algebraic and does not rely on the notion of
combinatorial designs.
</p></div>
    </summary>
    <updated>2019-05-02T23:20:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00079</id>
    <link href="http://arxiv.org/abs/1905.00079" rel="alternate" type="text/html"/>
    <title>FastContext: an efficient and scalable implementation of the ConText algorithm</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shi:Jianlin.html">Jianlin Shi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hurdle:John_F=.html">John F. Hurdle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00079">PDF</a><br/><b>Abstract: </b>Objective: To develop and evaluate FastContext, an efficient, scalable
implementation of the ConText algorithm suitable for very large-scale clinical
natural language processing. Background: The ConText algorithm performs with
state-of-art accuracy in detecting the experiencer, negation status, and
temporality of concept mentions in clinical narratives. However, the speed
limitation of its current implementations hinders its use in big data
processing. Methods: We developed FastContext through hashing the ConText's
rules, then compared its speed and accuracy with JavaConText and
GeneralConText, two widely used Java implementations. Results: FastContext ran
two orders of magnitude faster and was less decelerated by rule increase than
the other two implementations used in this study for comparison. Additionally,
FastContext consistently gained accuracy improvement as the rules increased
(the desired outcome of adding new rules), while the other two implementations
did not. Conclusions: FastContext is an efficient, scalable implementation of
the popular ConText algorithm, suitable for natural language applications on
very large clinical corpora.
</p></div>
    </summary>
    <updated>2019-05-02T23:26:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00073</id>
    <link href="http://arxiv.org/abs/1905.00073" rel="alternate" type="text/html"/>
    <title>Permutation Code Equivalence is not Harder than Graph Isomorphism when Hulls are Trivial</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bardet:Magali.html">Magali Bardet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Otmani:Ayoub.html">Ayoub Otmani</a>, Mohamed Saeed-Taha <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00073">PDF</a><br/><b>Abstract: </b>The paper deals with the problem of deciding if two finite-dimensional linear
subspaces over an arbitrary field are identical up to a permutation of the
coordinates. This problem is referred to as the permutation code equivalence.
We show that given access to a subroutine that decides if two weighted
undirected graphs are isomorphic, one may deterministically decide the
permutation code equivalence provided that the underlying vector spaces
intersect trivially with their orthogonal complement with respect to an
arbitrary inner product. Such a class of vector spaces is usually called linear
codes with trivial hulls. The reduction is efficient because it essentially
boils down to computing the inverse of a square matrix of order the length of
the involved codes. Experimental results obtained with randomly drawn binary
codes having trivial hulls show that permutation code equivalence can be
decided in a few minutes for lengths up to 50,000.
</p></div>
    </summary>
    <updated>2019-05-02T23:30:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00066</id>
    <link href="http://arxiv.org/abs/1905.00066" rel="alternate" type="text/html"/>
    <title>Online Bin Covering with Advice</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boyar:Joan.html">Joan Boyar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Favrholdt:Lene_M=.html">Lene M. Favrholdt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamali:Shahin.html">Shahin Kamali</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Larsen:Kim_S=.html">Kim S. Larsen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00066">PDF</a><br/><b>Abstract: </b>The bin covering problem asks for covering a maximum number of bins with an
online sequence of $n$ items of different sizes in the range $(0,1]$; a bin is
said to be covered if it receives items of total size at least 1. We study this
problem in the advice setting and provide tight bounds for the size of advice
required to achieve optimal solutions. Moreover, we show that any algorithm
with advice of size $o(\log \log n)$ has a competitive ratio of at most 0.5. In
other words, advice of size $o(\log \log n)$ is useless for improving the
competitive ratio of 0.5, attainable by an online algorithm without advice.
This result highlights a difference between the bin covering and the bin
packing problems in the advice model: for the bin packing problem, there are
several algorithms with advice of constant size that outperform online
algorithms without advice. Furthermore, we show that advice of size $O(\log
\log n)$ is sufficient to achieve a competitive ratio that is arbitrarily close
to $0.53\bar{3}$ and hence strictly better than the best ratio $0.5$ attainable
by purely online algorithms. The technicalities involved in introducing and
analyzing this algorithm are quite different from the existing results for the
bin packing problem and confirm the different nature of these two problems.
Finally, we show that a linear number of bits of advice is necessary to achieve
any competitive ratio better than 15/16 for the online bin covering problem.
</p></div>
    </summary>
    <updated>2019-05-02T23:24:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00044</id>
    <link href="http://arxiv.org/abs/1905.00044" rel="alternate" type="text/html"/>
    <title>Simpler and Better Algorithms for Minimum-Norm Load Balancing</title>
    <feedworld_mtime>1556755200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakrabarty:Deeparnab.html">Deeparnab Chakrabarty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Swamy:Chaitanya.html">Chaitanya Swamy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00044">PDF</a><br/><b>Abstract: </b>Recently, Chakrabarty and Swamy (STOC 2019) introduced the {\em minimum-norm
load-balancing} problem on unrelated machines, wherein we are given a set $J$
of jobs that need to be scheduled on a set of $m$ unrelated machines, and a
monotone, symmetric norm; We seek an assignment $\sg:J\mapsto[m]$ that
minimizes the norm of the resulting load vector $\lvec_\sg\in\R_+^m$, where
$\lvec_\sg(i)$ is the load on machine $i$ under the assignment $\sg$. Besides
capturing all $\ell_p$ norms, symmetric norms also capture other norms of
interest including top-$\ell$ norms, and ordered norms. Chakrabarty and Swamy
(STOC 2019) give a $(38+\ve)$-approximation algorithm for this problem via a
general framework they develop for minimum-norm optimization that proceeds by
first carefully reducing this problem (in a series of steps) to a problem
called \minmax ordered load balancing, and then devising a so-called
deterministic oblivious LP-rounding algorithm for ordered load balancing.
</p>
<p>We give a direct, and simple $4$-approximation algorithm for the minimum-norm
load balancing based on rounding a (near-optimal) solution to a novel
convex-programming relaxation for the problem. Whereas the natural convex
program encoding minimum-norm load balancing problem has a large non-constant
integrality gap, we show that this issue can be remedied by including a key
constraint that bounds the "norm of the job-cost vector." Our techniques also
yield a (essentially) $4$-approximation for: (a) {\em multi-norm load
balancing}, wherein we are given multiple monotone symmetric norms, and we seek
an assignment respecting a given budget for each norm; (b) the best {\em
simultaneous approximation factor} achievable for all symmetric norms for a
given instance.
</p></div>
    </summary>
    <updated>2019-05-02T23:30:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=12881</id>
    <link href="https://gilkalai.wordpress.com/2019/05/01/the-last-paper-of-catherine-renyi-and-alfred-renyi-counting-k-trees/" rel="alternate" type="text/html"/>
    <title>The last paper of Catherine Rényi and Alfréd Rényi: Counting k-Trees</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A k-tree is a graph obtained as follows: A clique with k vertices is a k-tree. A k-tree with n+1 vertices is obtained from a k-tree with n-vertices by adding a new vertex and connecting it to all vertices of a … <a href="https://gilkalai.wordpress.com/2019/05/01/the-last-paper-of-catherine-renyi-and-alfred-renyi-counting-k-trees/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A <em>k</em>-tree is a graph obtained as follows: A clique with <em>k</em> vertices is a <em>k</em>-tree. A <em>k</em>-tree with <em>n+1</em> vertices is obtained from a <em>k</em>-tree with n-vertices by adding a new vertex and connecting it to all vertices of a <em> k</em>-clique. There is a <a href="https://www.sciencedirect.com/science/article/pii/S0021980069801201">beautiful formula</a> by Beineke and Pippert (1969) for the number of <em>k</em>-trees with <em>n</em> labelled vertices. Their number is</p>
<p style="text-align: center;"><img alt="{{n} \choose {k}}(k(n-k)+1)^{n-k-2}." class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7Bk%7D%7D%28k%28n-k%29%2B1%29%5E%7Bn-k-2%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{n} \choose {k}}(k(n-k)+1)^{n-k-2}."/></p>
<p>If we count <strong>rooted</strong> <em>k</em>-trees where the root is a <em>k</em>-clique the formula becomes somewhat simpler.</p>
<p style="text-align: center;"><img alt="(k(n-k)+1)^{n-k-1}." class="latex" src="https://s0.wp.com/latex.php?latex=%28k%28n-k%29%2B1%29%5E%7Bn-k-1%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(k(n-k)+1)^{n-k-1}."/></p>
<p>In 1972, when I was a teenage undergraduate student I was very interested in various extensions of Cayley’s formula for counting labeled trees. I thought about the question of finding a <a href="https://en.wikipedia.org/wiki/Pr%C3%BCfer_sequence">Prüfer code</a> for<em> k</em>-trees and  how to extend the results by Beineke and  Pippert when  for every clique of size <img alt="k-1" class="latex" src="https://s0.wp.com/latex.php?latex=k-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k-1"/> in the <em>k</em>-tree we specify its “degree”, namely, the number of <em>k</em>-cliques containing it. (I will come back to the mathematics at the end of the post.) I thank Miki Simonovits for the photos and description and very helpful comments.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/lakeloui.jpg"><img alt="" class="alignnone size-full wp-image-17134" height="434" src="https://gilkalai.files.wordpress.com/2019/03/lakeloui.jpg?w=640&amp;h=434" width="640"/></a></p>
<p><strong><span style="color: #ff0000;">Above, Kató Renyi, Paul Turan, Vera Sós, and Paul Erdős ; below Kató, Vera, and Lea Schönheim. Pictures: Jochanan (Janos) Schönheim.</span></strong></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/lakelouise69.jpg"><img alt="" class="alignnone size-full wp-image-17137" height="447" src="https://gilkalai.files.wordpress.com/2019/03/lakelouise69.jpg?w=640&amp;h=447" width="640"/></a></p>
<p> </p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2015/04/renyi-turan-erdos.jpg"><img alt="renyi-turan-erdos" class="alignnone size-full wp-image-12883" src="https://gilkalai.files.wordpress.com/2015/04/renyi-turan-erdos.jpg?w=640"/></a></p>
<p><strong><span style="color: #ff0000;">From right, Rényi, Tur<b>á</b>n and Erdős and Grätzer. </span></strong></p>
<p> </p>
<p>While I was working on enumeration of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-trees I came across  a paper by Catherine Rényi and Alfréd Rényi that did everything I intended to do and quite a bit more.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/rr5.png"><img alt="" class="alignnone size-large wp-image-17340" height="260" src="https://gilkalai.files.wordpress.com/2019/04/rr5.png?w=640&amp;h=260" width="640"/></a></p>
<p>What caught my eye was a heartbreaking footnote: when the paper was completed Catherine Rényi was no longer alive.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/rr2r.png"><img alt="" class="alignnone size-large wp-image-17341" height="172" src="https://gilkalai.files.wordpress.com/2019/04/rr2r.png?w=640&amp;h=172" width="640"/></a></p>
<p>The proceedings where the paper appeared were of a conference in combinatorics in Hungary in 1969. This was the first international conference in combinatorics that took place in Hungary.  The list of speakers consists of the best combinatorialists in the world and many young people including Laci Lovasz, Laci Babai, Endre Szemeredi, and many more who since then have become world-class  scientists.</p>
<p>Years later Vera Sós told me the story of Alfréd Rényi’s lecture at this conference, the first international conference in combinatorics that took place in Hungary:  “Kató died on August 23, on the day of arrival of the conference on “Combinatorial Theory and its Applications” (Balatonfured, August 24-29). Alfréd Renyi gave his talk (with the same title as the paper) on August 27 and his talk was longer than initially scheduled.  They proved the results in the paper just the week before the conference. The paper appeared in the proceedings  of the conference.”</p>
<p>Alfréd Renyi was one of the organizers of the conference and also served as one of the editors of the proceedings of the conference, which appeared in 1970. A few months after the conference, on February 1, 1970 Alfréd Rényi  died of a violent illness. The proceedings are dedicated to the memory of Catherine Rényi and Alfréd Rényi.</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/scan-19.jpg"><img alt="" class="alignnone size-full wp-image-17135" height="444" src="https://gilkalai.files.wordpress.com/2019/03/scan-19.jpg?w=640&amp;h=444" width="640"/></a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/scan-4.jpg"><img alt="" class="alignnone size-full wp-image-17136" height="445" src="https://gilkalai.files.wordpress.com/2019/03/scan-4.jpg?w=640&amp;h=445" width="640"/></a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/erdos-renyi.png"><img alt="" class="alignnone size-full wp-image-17138" height="426" src="https://gilkalai.files.wordpress.com/2019/03/erdos-renyi.png?w=640&amp;h=426" width="640"/></a></p>
<p>Two pictures showing Alfréd and Catherine Rényi and a picture of Alfred Rényi and Paul Erdős.</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/srrhe.png"><img alt="" class="alignnone size-full wp-image-17315" height="498" src="https://gilkalai.files.wordpress.com/2019/04/srrhe.png?w=640&amp;h=498" width="640"/></a></p>
<p>Repeating a picture from last-week <a href="https://gilkalai.wordpress.com/2019/04/21/the-random-matrix-and-more/">post</a>. From left: Sándor Szalai,  Catherine Rényi, Alfréd Rényi, András Hajnal and Paul Erdős (Matrahaza)</p>
<p>Going back to my story. I was 17 at the time and naturally I wondered if counting trees and similar things is what I want to do in my life. Shortly afterwards I went to the army. Without belittling the excitement of the army I quickly reached the conclusion that I prefer to count trees and to do similar things. My first result as a PhD student was another high dimensional extension of Cayley’s formula (mentioned in <a href="https://gilkalai.wordpress.com/2008/06/10/hellys-theorem-hypertrees-and-strange-enumeration-i/">this post</a> and a few subsequent posts).  The question of how to generalize both formulas for <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-trees and for my hypertrees is still an open problem. We know the objects we want to count,  we know what the outcome should be, and we know that we can cheat and use weighted counting, but still I don’t know how to make it work.</p>
<p>Some more comments on k-trees:</p>
<ol>
<li>Regarding the degree sequences for k-trees. You cannot specify the actual (k-2)-faces because those (in fact just the graph) determines the k-tree completely. So you need to count rooted k-trees and to specify the (k-2)-faces in terms of how they “grew” from the root.</li>
<li>The case that all degrees are 1 and 2 that correspond to paths for ordinary trees and to triangulating polygons with diagonals for 2-trees are precisely the stacked (k-1)-dimensional polytopes. This is a special case of the Renyi &amp; Renyi formula that was also <a href="https://link.springer.com/article/10.1007%2FBF02330563?LI=true"> found, with a different proof,</a> by Beineke and Pippert.</li>
<li>It is  unlikely that there would be a matrix-tree formula for k-trees since telling  if a graph contains a 2-tree  is known to be NP complete. See <a href="https://mathoverflow.net/questions/281848/spanning-k-trees">this MO question</a>. Maybe some matrix-tree formulas are available when we start with special classes of graphs.</li>
<li>Regarding the general objects – those are simplicial complexes that are Cohen-Macaulay and their dual (blocker) is also Cohen-Macaulay.</li>
</ol>
<p>This post is just about a single paper of Catherine Rényi and Alfréd Rényi mainly through my eyes from 45 years ago. Catherine Rényi’s  main interest originally was  Number theory, she was a student of Turàn, and soon she became  interested in the theory of Complex Analytic Functions. Alfréd Rényi was a student of Frigyes Riesz and he is known for many contributions in number theory, graph theory and combinatorics and primarily in probability theory.  Alfréd Rényi wrote several papers about enumeration of trees, and this joint paper was Catherine Rényi ‘s first paper on this topic.</p></div>
    </content>
    <updated>2019-05-01T14:19:17Z</updated>
    <published>2019-05-01T14:19:17Z</published>
    <category term="Combinatorics"/>
    <category term="People"/>
    <category term="Alfr&#xE9;d R&#xE9;nyi"/>
    <category term="Catherine R&#xE9;nyi"/>
    <category term="k-trees"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-05-02T23:35:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/065</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/065" rel="alternate" type="text/html"/>
    <title>TR19-065 |  Derandomization from Algebraic Hardness: Treading the Borders | 

	Mrinal Kumar, 

	Ramprasad Saptharishi, 

	Noam Solomon</title>
    <summary>A hitting-set generator (HSG) is a polynomial map $Gen:\mathbb{F}^k \to \mathbb{F}^n$ such that for all $n$-variate polynomials $Q$ of small enough circuit size and degree, if $Q$ is non-zero, then $Q\circ Gen$ is non-zero. In this paper, we give a new construction of  such a HSG assuming that we have an explicit polynomial of sufficient hardness in the sense of approximative or border complexity.  Formally, we prove the following result over any characteristic zero field $\mathbb{F}$:

Suppose $P(z_1,\ldots, z_k)$ is an explicit $k$-variate degree $d$ polynomial that is not in the border of circuits of size $s$. Then, there is an explicit hitting-set generator $Gen_P:\mathbb{F}^{2k} \rightarrow \mathbb{F}^n$ such that  every non-zero $n$-variate degree $D$ polynomial $Q(x_1, x_2, \ldots, x_n)$ in the border of size $s'$ circuits satisfies $Q \neq 0 \Rightarrow Q \circ Gen_P \neq 0$, provided $n^{10k}d D s' \leq s$. 

This is the first HSG in the algebraic setting that yields a complete derandomization of polynomial identity testing (PIT) for general circuits from a suitable algebraic hardness assumption.

As a direct consequence, we show that even a slightly non-trivial explicit construction of hitting sets for polynomials in the border of constant-variate circuits implies a deterministic polynomial time algorithm for PIT. More precisely, we prove the following theorem:

Let $\delta &gt; 0$ be any constant and $k$ be a large enough constant. Suppose, for every $s \geq k$, there is an explicit hitting set of size $s^{k-\delta}$ for all degree $s$ polynomials in the border of $k$-variate size $s$ algebraic circuits. Then, there is an explicit hitting set of size $poly(s)$ for the border $s$-variate algebraic circuits of size $s$ and degree $s$. 

Unlike the prior constructions of such maps [NW94, KI04, AGS18, KST19], our construction is purely algebraic and does not rely on the notion of combinatorial designs.</summary>
    <updated>2019-05-01T05:54:23Z</updated>
    <published>2019-05-01T05:54:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-02T23:35:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/04/30/linkage</id>
    <link href="https://11011110.github.io/blog/2019/04/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Good article, terrible headline (). Bill Gasarch rants about several recent instances of clickbaity, inaccurate, and overhyped media coverage of theoretical computer science topics. I suspect the answer to his question “is it just our field?” is no.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2019/04/good-article-terrible-headline.html">Good article, terrible headline</a> (<a href="https://mathstodon.xyz/@11011110/101938798669973189"/>). Bill Gasarch rants about several recent instances of clickbaity, inaccurate, and overhyped media coverage of theoretical computer science topics. I suspect the answer to his question “is it just our field?” is no.</p>
  </li>
  <li>
    <p><a href="https://www.vox.com/science-and-health/2019/4/16/18311194/black-hole-katie-bouman-trolls">Vox on the sexist backlash against astronomer Katie Bouman</a> (<a href="https://mathstodon.xyz/@11011110/101942756338391262"/>, <a href="https://www.cnn.com/2019/04/12/us/andrew-chael-katie-bouman-black-hole-image-trnd/index.html">see also</a>), of black hole image fame, after she was cast by the media in the “lone genius” role typically reserved for men and untypical of how science actually happens.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/2019/04/mathematical-sign-language-interview-with-dr-jess-boland/">Mathematical sign language</a> (<a href="https://mathstodon.xyz/@11011110/101950143529837988"/>). Hearing-impaired eletrical engineering researcher Jess Boland discovered that weren’t enough technical terms in British Sign Language to cover the mathematics she uses in her work, so she’s been creating new ones as well as promoting the ones BSL already had. Katie Steckles interviews her for <em>The Aperiodical</em>.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1804.05452">Regular polygon surfaces</a> (<a href="https://mathstodon.xyz/@11011110/101955536664219652"/>). Ian Alevy answers <a href="http://cs.smith.edu/~jorourke/TOPP/P72.html#Problem.72">Problem 72 of The Open Problems Project</a>: every topological sphere made of regular pentagons can be constructed by gluing regular dodecahedra together. You can also <a href="https://momath.org/mathmonday/the-paragons-system/">glue dodecahedra to get higher-genus surfaces</a>, but Alevy’s theorem doesn’t apply, so we don’t know whether all higher-genus regular-pentagon surfaces are formed that way.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2019/04/12/czech-president-blocks-professorships-academic-critics">Czech president Miloš Zeman “has repeatedly used presidential powers to block the professorships of political opponents”</a> (<a href="https://mathstodon.xyz/@11011110/101965701030220573"/>). Charles University is now suing to allow their promotions to go through.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1904.08845">Planar point sets determine many pairwise crossing segments</a> (<a href="https://mathstodon.xyz/@11011110/101968467896290245"/>). János Pach, Natan Rubin, and Gábor Tardos make significant progress on  whether every<br/>
 points in the plane have a large matching where all edges cross each other. A 1994 paper by Paul Erdős and half a dozen others only managed to prove this for “large” meaning . The new paper proves a much stronger bound,  (Ryan Williams’ favorite function).</p>
  </li>
  <li>
    <p><a href="https://randomascii.wordpress.com/2019/04/21/on2-in-createprocess/">Why asymptotics matters</a> (<a href="https://mathstodon.xyz/@11011110/101970781407484011"/>, <a href="https://news.ycombinator.com/item?id=19716673">via</a>): because if you don’t pay attention to it you get problems like this slow quadratic-time process creation bug in Windows 10.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@jsiehler/101982200745112808">Snap cube puzzle</a>. The cubes have one peg and five holes; how many ways can you snap them into a connected  block with no pegs showing? See link in discussion thread for spoilers.</p>
  </li>
  <li>
    <p>There’s lots of reasons to be unenthusiastic about newly-official-presidential-candidate Biden involving multiple instances of poor treatment of African-Americans and women, but here’s another more techy reason: <a href="https://www.pastemagazine.com/articles/2019/04/biden-to-attend-fundraiser-hosted-by-comcast-blue.html">his first major fundraiser as a candidate closely involves anti-net-neutrality lobbyists from Comcast</a> (<a href="https://mathstodon.xyz/@11011110/101987719804605064"/>).</p>
  </li>
  <li>
    <p><a href="https://adsabs.github.io/blog/transition-reminder">SAO/NASA Astrophysics Data System updates its user interface</a> (<a href="https://mathstodon.xyz/@11011110/101996739440443058"/>). <a href="http://adsabs.harvard.edu/">The ADS</a> is a useful database of papers in astronomy and related fields. From comments on their post, the new UI is very slow. It is <a href="http://adsabs.github.io/help/faq/">unusable without JavaScript</a>. And it <a href="https://en.wikipedia.org/wiki/Special:Diff/892128592">“sends the users’ personal identifying information to at least 5 third-party companies”</a>. This is progress?</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/329910/440">I ask for a reference for an easy fact about divisibility representations of partial orders</a> (<a href="https://mathstodon.xyz/@11011110/102002516978139958"/>). The MathOverflow community isn’t very helpful, preferring instead to simultaneously complain that it’s too trivial and explain why it’s true to me as if I didn’t already say in my question that I thought it was trivial.`</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@henryseg/101975738950740643">Cannon-Thurston maps for veering triangulations</a>, whatever those are. Henry Segerman posts some pretty pictures from his joint work with David Bachman and Saul Schleimer.</p>
  </li>
  <li>
    <p><a href="https://www.mathunion.org/fileadmin/CWM/Initiatives/CWMNewsletter1.pdf">Newsletter of the IMU Committee for Women</a> (<a href="https://aperiodical.com/2019/04/imu-committee-for-women-in-mathematics-now-has-a-newsletter/">via</a>). Includes an interview with Marie-Francoise Roy and the announcement of the book <em>World Women in Mathematics 2018</em>.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">Network coding yields lower bounds</a> (<a href="https://mathstodon.xyz/@11011110/102018096543192991"/>). Lipton and Regan report on <a href="https://arxiv.org/abs/1902.10935">a new paper by Afshani, Freksen, Kamma, and Larsen</a> on lower bounds for multiplication. If algorithmically opening and recombining network messages never improves fractional flow, then  circuit size for multiplication is optimal. But the same lower bound holds for simpler bit-shifting operations, so it’s not clear how it could extend from circuits to bignum algorithms.</p>
  </li>
</ul></div>
    </content>
    <updated>2019-04-30T23:15:00Z</updated>
    <published>2019-04-30T23:15:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-05-01T06:43:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/064</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/064" rel="alternate" type="text/html"/>
    <title>TR19-064 |  Randomness and Intractability in Kolmogorov Complexity | 

	Igor Carboni Oliveira</title>
    <summary>We introduce randomized time-bounded Kolmogorov complexity (rKt), a natural extension of Levin's notion of Kolmogorov complexity from 1984. A string w of low rKt complexity can be decompressed from a short representation via a time-bounded algorithm that outputs w with high probability. 

This complexity measure gives rise to a decision problem over strings: MrKtP (The Minimum rKt Problem). We explore ideas from pseudorandomness to prove that MrKtP and its variants cannot be solved in randomized quasi-polynomial time. This exhibits a natural string compression problem that is provably intractable, even for randomized computations. Our techniques also imply that there is no n^{1-eps}-approximate algorithm for MrKtP running in randomized quasi-polynomial time. 

Complementing this lower bound, we observe connections between rKt, the power of randomness in computing, and circuit complexity. In particular, we present the first hardness magnification theorem for a natural problem that is unconditionally hard against a strong model of computation.</summary>
    <updated>2019-04-30T18:41:08Z</updated>
    <published>2019-04-30T18:41:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-02T23:35:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15814</id>
    <link href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/" rel="alternate" type="text/html"/>
    <title>Network Coding Yields Lower Bounds</title>
    <summary>Practice leads theory Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen have a beautiful new paper titled “Lower Bounds for Multiplication via Network Coding”. Today we will talk about how practical computing played a role in this theory research. The authors (AFKL) state this: In this work, we prove that if a central conjecture […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Practice leads theory</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<a href="https://rjlipton.files.wordpress.com/2019/04/akkl.jpg"><img alt="" class="alignright wp-image-15816" height="162" src="https://rjlipton.files.wordpress.com/2019/04/akkl.jpg?w=123&amp;h=162" width="123"/></a><p/><p>
Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen have a beautiful new <a href="https://arxiv.org/abs/1902.10935">paper</a> titled “Lower Bounds for Multiplication via Network Coding”. </p><p>
Today we will talk about how practical computing played a role in this theory research.</p><p>
The authors (AFKL) state this:</p><blockquote><p><b> </b> <em> In this work, we prove that if a central conjecture in the area of network coding is true, then any constant degree boolean circuit for multiplication must have size <img alt="{\Omega(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Omega(n \log n)}"/>, thus <s>almost</s> completely settling the complexity of multiplication circuits. </em>
</p></blockquote><p/><p>
We added the strikeout because of the <img alt="{O(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n \log n)}"/> upper bound that we discussed recently <a href="https://rjlipton.wordpress.com/2019/03/29/integer-multiplication-in-nlogn-time/">here</a>.</p><p>
AFKL have conditionally solved a long standing open problem: “How hard is it to multiply two <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-bit numbers?” Their proof shows that a conjecture from practice implies a circuit lower bound. This is rare: using a conjecture from practice, to solve a complexity open problem. We have used conjectures from many parts of mathematics, and from some parts of physics, to make progress, but drawing on experience with practical networking is strikingly fresh. </p><p>
</p><p/><h2> Integer Multiplication </h2><p/><p/><p>
The authors AFKL explain the history of the multiplication problem. We knew some of the story, but not all the delicious details.</p><blockquote><p><b> </b> <em> In 1960, Andrey Kolmogorov conjectured that the thousands of years old <img alt="{O(n^{2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B2%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^{2})}"/>-time algorithm is optimal and he arranged a seminar at Moscow State University with the goal of proving this conjecture. However only a week into the seminar, the student Anatoly Karatsuba came up with an <img alt="{O(n^{\log_{2}3}) \approx O(n^{1.585})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B%5Clog_%7B2%7D3%7D%29+%5Capprox+O%28n%5E%7B1.585%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^{\log_{2}3}) \approx O(n^{1.585})}"/> time algorithm. The algorithm was presented at the next seminar meeting and the seminar was terminated. </em>
</p></blockquote><p/><p>
Ken and I wish we could have Kolmogorov’s luck, in one of our seminars. Partly because it would advance knowledge; partly because it would let us out of teaching. Sweet.</p><p>
The main result of AKKL is:</p><blockquote><p><b>Theorem 1</b> <em><a name="NC2mult"/> Assuming the Network Conjecture, every general boolean circuit that computes the product of two <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>-bit integers has size order at least <img alt="{n\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n\log n}"/>. </em>
</p></blockquote><p/><p>
This says that the boolean complexity of multiplication is super-linear. No restriction of a bounded depth, no restriction on the operations allowed, no restrictions at all. Given our non-existent lower bounds this is remarkable. If it was unconditional, it would be a terrific result. But it still is a strong one. </p><p>
We will next explain what the Network Coding Conjecture (NCC) is. </p><p>
</p><p/><h2> Network Coding </h2><p/><p/><p>
One of the basic papers was authored by Rudolf Ahlswede, Ning Cai, Shuo-Yen Li, and Raymond Yeung <a href="http://www.inf.fu-berlin.de/lehre/WS11/Wireless/papers/CodAhlswede00.pdf">here</a>. The paper has close to ten thousand citations, which would be amazing for a theory paper.</p><p>
In basic networks each node can receive and send messages to and from other nodes. They can only move messages around—they are not allowed to peer into a message. The concept of <i>network coding</i> is to allow nodes also to decode and encode messages. Nodes can peer into messages and create new ones. The goal, of course, is to decrease the time required to transmit information through the network.</p><p>
The following example combines figures from a 2004 <a href="http://www.eecg.utoronto.ca/~bli/papers/allerton04.pdf">paper</a> by Zongpeng Li and Baochun Li which formulated the NCC. At left is a situation where two senders, <img alt="{S_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_1}"/> with an <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-bit message <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{S_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_2}"/> with an <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-bit message <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/>, wish to transmit to respective receivers <img alt="{T_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_1}"/> and <img alt="{T_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_2}"/>. The network’s links are one-way as shown, with two intermediate nodes <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>, and each link can carry <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits at any one time.</p><p/><p><br/>
<a href="https://rjlipton.files.wordpress.com/2019/04/flowfigures.png"><img alt="" class="aligncenter wp-image-15815" height="115" src="https://rjlipton.files.wordpress.com/2019/04/flowfigures.png?w=500&amp;h=115" width="500"/></a></p><p/><p><br/>
If <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> are black-boxes that must be kept entire, there is no way to solve this in three time steps. But if the nodes can read messages and do lightweight computations, then the middle diagram gives a viable solution. Node <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> reads <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> and on-the-fly transmits their bitwise exclusive-or to node <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>. Node <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> relays this to each receiver, who has also received the other party’s message directly. The receivers can each do a final exclusive-or to recover the messages intended for them. </p><p>
The ability to look inside messages seems powerful, and there are networks where it helps even more dramatically. Incidentally, as <a href="https://en.wikipedia.org/wiki/Linear_network_coding">noted</a> by Wikipedia, the exclusive-or trick was anticipated in a 1978 <a href="https://ieeexplore.ieee.org/document/1455117">paper</a> showing how the two senders can exchange their messages <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> by relaying them to a satellite which transmits <img alt="{a \oplus b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%5Coplus+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a \oplus b}"/> back to each.</p><p>
</p><p/><h2> The Conjecture </h2><p/><p/><p>
However, there is another solution if the links are bi-directional and messages can be broken in half. Sender <img alt="{S_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_1}"/> simply routes half of <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> one way around the network and the other half the other way. Sender <img alt="{S_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_2}"/> does similarly. This is shown at far right. Each link never has more than <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits of total load and the three-step elapsed time is the same. Moreover, the link from <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> to <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is not needed. This is just an undirected network commodity flow with fractional units.</p><p>
In fact, <i>no</i> example is known in an undirected network where encoding beats fractional routing. That is, the known network encoding rate is just the flow rate of the network. The network coding (NCC) conjecture is informally:</p><blockquote><p><b> </b> <em> <i>The coding rate is never better than the flow rate in undirected graphs</i>. </em>
</p></blockquote><p/><p>
The paper by Li and Li gave formal details and several equivalent statements. Quoting them:</p><blockquote><p><b> </b> <em> For undirected networks with integral routing, there still exist configurations that are feasible with network coding but infeasible with routing only. For undirected networks with fractional routing, we show that the potential of network coding to help increase throughput in a capacitied network is equivalent to the potential of network coding to increase bandwidth efficiency in an uncapacitied network. We conjecture that these benefits are non-existent. </em>
</p></blockquote><p>
</p><p/><h2> Good and Bad News </h2><p/><p/><p>
What has become of the NCC in the fifteen years since? Here’s how Ken and I see it:</p><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>The Good News</i>: The NCC helps solve long-standing open problems. Since this conjecture is widely believed this is impressive. Besides integer multiplication, NCC has been used to prove other lower bounds. For example, Larsen working with Alireza Farhadi, Mohammad Hajiaghayi, and Elaine Shi used it to <a href="https://arxiv.org/abs/1811.01313">prove</a> lower bounds on sorting with external memory. </p><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>The Bad News</i>: The NCC helps solve long-standing open problems. This suggests that this conjecture could be deep and hard to resolve. The boolean complexity of integer multiplication is a long standing open question. Since the NCC leads to a non-linear lower bound, perhaps proving this conjecture could be hopeless.</p><p>
I have mixed feelings about these lower bound results. They are impressive and shed light on hard open problems. But I wonder if the NCC could be wrong. There is a long <a href="https://rjlipton.wordpress.com/2010/06/19/guessing-the-truth/">history</a> in complexity theory where guesses of the form: </p><blockquote><p><b> </b> <em> <i>The obvious algorithm is optimal</i> </em>
</p></blockquote><p>have failed. The situation strikes us as resembling that of the (Strong) Exponential Time Hypothesis, in ways we <a href="https://rjlipton.wordpress.com/2015/06/01/puzzling-evidence/">discussed</a> four years ago. </p><p>
</p><p/><h2> How the New Paper Works </h2><p/><p/><p>
The authors AKKL did not know that an <img alt="{O(n\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n\log n)}"/> upper bound had been proved for integer multiplication when they posted their paper. They did, however, prove a stronger version of Theorem (<a href="https://rjlipton.wordpress.com/feed/#NC2mult">1</a>) for a problem with a known <img alt="{n\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n\log n}"/> upper bound. This is to create circuits with <img alt="{n + \log_2 n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%2B+%5Clog_2+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n + \log_2 n}"/> input gates and <img alt="{2n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2n}"/> output gates that given <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and a binary number <img alt="{\ell \leq n = |x|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%5Cleq+n+%3D+%7Cx%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell \leq n = |x|}"/> output the string <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> whose bits <img alt="{n-\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn-%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n-\ell}"/> through <img alt="{2n-\ell-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n-%5Cell-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2n-\ell-1}"/> equal <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, with other bits <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. A conditional lower bound on this <i>shift</i> task implies the same for multiplication, since the shift is the same as multiplication by <img alt="{2^\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^\ell}"/>. </p><blockquote><p><b>Theorem 2</b> <em><a name="NC2shift"/> Assuming the NCC, circuits for the shift task need size order <img alt="{n\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n\log n}"/>. </em>
</p></blockquote><p/><p>
The proof is disarmingly elementary: The input <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> gives <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> “senders” and each value of <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell}"/> creates a different set of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> “receivers.” With a circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> fixed, they show one can fix a shift <img alt="{\ell_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_0}"/> so that the average distance from sender to receiver in an undirected multi-commodity flow is <img alt="{\Omega(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(\log n)}"/>, giving <img alt="{\Omega(n\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(n\log n)}"/> total flow. If <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> achieves smaller size, then it represents a counterexample to the NCC. </p><p>
Pretty neat—this is half a page in the paper. The paper proves more intricate results relating to conjectures by Les Valiant about Boolean circuits of bounded fan-in and <img alt="{O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\log n)}"/> depth that compute permutations and their reduction to depth-3 circuits of unbounded fan-in. This also may extend to the sorting/shifting problem Ken wrote about long ago in a guest <a href="https://blog.computationalcomplexity.org/2007/07/concrete-open-problem.html">post</a> for Lance Fortnow and Bill Gasarch’s blog.</p><p>
</p><p/><h2> Open Problems </h2><p/><p/><p>
Is the NCC true? Can it be proved for some interesting classes of graphs? I believe it is known for tiny size graphs of at most six nodes. What about, for example, planar graphs?</p><p>
[inserted “conditionally” before “solved” in intro]</p><table class="image alignright">










































</table></font></font></div>
    </content>
    <updated>2019-04-30T13:27:11Z</updated>
    <published>2019-04-30T13:27:11Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="trick"/>
    <category term="Casper Freksen"/>
    <category term="circuits"/>
    <category term="communication complexity"/>
    <category term="conjecture"/>
    <category term="integer multiplication"/>
    <category term="Kasper Larsen"/>
    <category term="Lior Kamma"/>
    <category term="lower bounds"/>
    <category term="network coding"/>
    <category term="Peyman Afshani"/>
    <category term="reduction"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-02T23:35:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://gradientscience.org/policy_gradients_pt3</id>
    <link href="http://gradientscience.org/policy_gradients_pt3" rel="alternate" type="text/html"/>
    <title>A Closer Look at Deep Policy Gradients (Part 3&amp;#58; Landscapes and Trust Regions)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post is the last of a three part series about our recent paper: “<a href="https://arxiv.org/abs/1811.02553">Are Deep
Policy Gradient Algorithms Truly Policy Gradient
Algorithms?</a>” Today, we will analyze agents’
reward landscapes as well as try to understand to what extent, and by what mechanisms,
our agents enforce so-called <i>trust regions</i>.</p>

<p>First, a quick recap (it’s been a while!):</p>
<ul>
  <li>
    <p>In our <a href="https://gradientscience.org/policy_gradients_pt1">first post</a>, we outlined the
RL framework and introduced policy gradient algorithms. We saw that
auxiliary optimizations hidden in the implementation details of RL algorithms
drastically impact performance. These findings highlighted the need for a more
fine-grained analysis of how algorithms really operate.</p>
  </li>
  <li>
    <p>In our <a href="https://gradientscience.org/policy_gradients_pt2">second post</a>, we zoomed in on
three algorithms: trust region policy optimization (TRPO), proximal policy
optimization (PPO), and an algorithm we called “PPO-M.” PPO-M is the core PPO
algorithm exactly as described in the <a href="https://arxiv.org/abs/1707.06347">original
paper</a>, without any of the auxiliary
optimizations. Using these methods as a test-bed, we studied two core
primitives of the policy gradient framework: gradient estimation and value
prediction.</p>
  </li>
</ul>

<p>Our discussion today begins where we left off in our second post. Recall that
last time we studied the variance of the gradient estimates our algorithms use
to maximize rewards. We found (among other things) that, despite high variance, algorithm steps were
still (very slightly) correlated with the actual, “true” gradient of the reward.
However, how good was this true gradient to begin with? After all, a fundamental
assumption of the whole policy gradient framework is that our gradient steps actually point in a direction (in policy
parameter space) that increases the reward. Is this indeed so in practice?</p>

<h2 id="optimization-landscapes">Optimization Landscapes</h2>
<p>Recall from our <a href="https://gradientscience.org/policy_gradients_pt1">first post</a> that
policy gradient methods treat reward maximization as a zeroth-order optimization
problem. That is, they maximize the objective by applying first order methods
with finite sample gradient estimates of the form:</p>



<p>Here,  represents the cumulative reward of a trajectory , where
 is sampled from the distribution of trajectories induced by the current
policy . We let  represent an easily computable
function of  that is an unbiased estimator of the gradient of the reward
(seen on the left hand side)—for more details see <a href="https://gradientscience.org/policy_gradients_pt1/#rl-with-policy-gradients">our previous post</a>.
Finally, we denote by  the number of trajectories used to estimate the
gradient.</p>

<p>An important point is, however, that instead of following the gradients of the cumulative reward (as
suggested by the above equation), the algorithms we analyze actually use a
<i>surrogate reward</i> at each step. The surrogate reward is a function of the
collected trajectories that is meant to locally approximate the true reward,
while providing other benefits such as easier computation and a smoother
optimization landscape. Consequently, at each step these algorithms maximize the
surrogate rewards  instead of following the gradient of the
true reward .</p>

<p>A natural question to ask is: <i>do steps maximizing the surrogate reward
consistently increase policy returns?</i> To answer this, we will use
<i>landscape plots</i> as a tool for visualizing the landscape of returns
around a given policy :</p>

<p><img alt="Labeled diagram of optimization landscape plot" src="https://gradientscience.org/images/rl/labeled_landscape.jpg"/></p>

<p>Here, for each point  in the plot,  and  specify a policy
 parameterized by</p>



<p>where  is the step computed by the studied algorithm. (Note that
we include the random Gaussian  direction to visualize how
“important” the step direction is compared to a random baseline). The  axis
corresponds to the return attained by the policy , which we
denote by .</p>

<p>Now, when we make this plot for a random  (corresponding to a randomly
initialized policy network), everything looks as expected:</p>

<p><img alt="Landscape for randomly initialized network" src="https://gradientscience.org/images/rl/step_0_landscape.jpg"/></p>

<p>That is, the return increases significantly more quickly along the step direction than in
the random direction. However, repeating these landscape experiments at later
iterations, we find a more surprising picture: going in the step direction
actually <i>decreases</i> the average cumulative reward obtained by the resulting
agent!</p>

<p><img alt="Landscape for trained networks" src="https://gradientscience.org/images/rl/landscape_step_150_300_450.jpg"/></p>

<p>So what exactly is going on here? The steps computed by these algorithms are
estimates of the gradient of the reward, so it is unexpected that the reward
plateaus (or in some cases decreases!) along this direction.</p>

<p>We find that the
answer lies in a <i>misalignment</i> of the true reward and the surrogate. Specifically, while
the steps taken do correspond to an improvement in the surrogate problem, they
do <i>not</i> always correspond to a similar improvement in the true return.
Here are the same graphs as above shown again, this time with the corresponding
optimization landscapes of the <i>surrogate loss</i>
<sup><a href="https://gradientscience.org/feed.xml#footnote1">1</a></sup>:</p>
<div class="footnote">
<sup><a id="footnote1">1</a></sup>All of the landscapes we plot in this post are for PPO; we have (similar)
results for TRPO in <a href="https://arxiv.org/abs/1811.02553">our paper</a>.
</div>

<p><img alt="Surrogate vs real landscape" src="https://gradientscience.org/images/rl/surrogate_vs_real_landscape.jpg"/></p>

<!-- The surrogate reward landscape misalignment is further complicated by the fact -->
<!-- that all of the rewards plotted in the landscapes we’ve seen so far are computed with -->
<!-- many, many more samples than an algorithm would ever collect at a single step in -->
<!-- practice.  -->

<p>To make matters worse, we find that in the low sample regime that policy
gradient methods actually operate in, it is hard to even <i>discern</i> directions of
improvement in the true reward landscape. (In all the plots above,
we use orders of magnitude more samples than an agent would ever see in practice
at a single step.) In the plot below, we visualize reward landscapes while
varying the number of samples used to estimate the expected return of a policy
:</p>

<p><img alt="Surrogate vs real landscape" src="https://gradientscience.org/images/rl/landscape_conc.jpg"/></p>

<p>In contrast to the smooth landscape we see on the right and in the plots above,
the reward landscape actually accessible to the model is jagged and poorly behaved.
This landscape makes it thus near-impossible for an agent to distinguish between good
and bad points in its relevant sample regime, even when the true underlying
landscape is fairly well-behaved!</p>

<p>Overall, our investigation into the optimization landscape of policy gradient algorithms
reveals that (a) the surrogate reward function is often misaligned with the
underlying true return of the policy, and (b) in the relevant sample regime, it
is hard to distinguish between “good” steps and “bad” steps, even when looking at the true reward
landscape. As always, however, <i>none of this stops the agents from training
and continually improving reward in the average sense</i>. This raises some
key questions about the landscape of policy optimization:</p>

<ul>
  <li>Given that the function we actually optimize is so often misaligned with the
underlying rewards, how is it that agents continually improve?</li>
  <li>Can we explain or link the local behaviour we observe in the landscape with a
more global view of policy optimization?</li>
  <li>How do we ensure that the reward landscape is navigable? And, more generally,
what is the best way to navigate it?</li>
</ul>

<h2 id="trust-regions">Trust Regions</h2>
<p>Let us now turn our attention to another important notion in the popular policy gradient algorithms: that of the <i>trust region</i>. 
Recall that a convenient way to think about our training process is to view it as a series of policy parameter iterates:</p>



<p>An important aspect of this process is ensuring that the steps we take don’t
lead us outside of the region (of parameter space) where the samples we
collected are informative. Intuitively, if we collect samples at a given set of
policy parameters , there is no reason to expect that these samples
should tell us about the performance of a new set of parameters that is far away
from .</p>

<p>Thus, in order to ensure that gradient steps are predictive, classical
algorithms like the 
<a href="http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf">conservative policy update</a> 
employ update schemes that constrain the probability distributions induced by 
successive policy parameters.  The <a href="https://arxiv.org/abs/1502.05477">TRPO paper</a> 
in particular showed 
that one can 
guarantee monotonic policy improvement with each step by solving a surrogate problem of the following form:</p>



<p>The second, “penalty” term in the above objective, referred to as the <i>trust region</i> penalty, is a
critical component of modern policy gradient algorithms. TRPO, one of the
algorithms we study, proposes a relaxation of \eqref{eq:klpen} that
instead imposes a hard constraint on the
<i>mean</i> KL divergence<sup><a href="https://gradientscience.org/feed.xml#footnote2">2</a></sup>
 (estimated using the empirical samples we obtain):</p>



<div class="footnote">
<sup><a id="footnote2">2</a></sup>It's worth noting that 
<a href="https://arxiv.org/abs/1705.10528">a
recent paper</a> showed that under some conditions, mean KL is actually
sufficient.
</div>

<p>In other words, we try to ensure that the <i>average</i> distance between 
conditional probability distributions is small. 
Finally, PPO approximates the mean KL bound of TRPO by attempting to
constrain a <i>ratio</i> between successive conditional probability
distributions, instead of the KL divergence. The exact mechanism for
enforcing this is shown in the 
box below. Intuitively, however, what PPO does is just throw away
(i.e. get no gradient signal from) the
rewards incurred from any state-action pair such that:</p>



<p>where  is a user-chosen hyperparameter.</p>

<section class="container">
<div>
<div class="checkboxdiv">
<input id="ac-1" name="accordion-1" type="checkbox"/>
<label for="ac-1"><span class="fas fa-chevron-right" id="titlespan"/> <strong>The PPO update step</strong> (Click to expand)</label>
<article class="small">
<i>Note that the following is only for interested readers and is unessential 
for reading the rest of the blog post.</i> <br/> <br/>

The exact update used by PPO is as follows, where $\widehat{A}_\pi$ is
the <a href="https://arxiv.org/abs/1506.02438">generalized advantage 
estimate</a>:
$$
\begin{array}{c}{\max _{\theta} \mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \pi}\left[\min \left(\operatorname{clip}\left(\rho_{t}, 1-\varepsilon, 1+\varepsilon\right) \widehat{A}_{\pi}\left(s_{t}, a_{t}\right), \rho_{t} \widehat{A}_{\pi}\left(s_{t}, a_{t}\right)\right)\right]} \\ {\text{where }\ \ \rho_{t}=\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi\left(a_{t} | s_{t}\right)}}\end{array}
$$
As described in the main text, this intuitively corresponds to throwing
away (i.e. getting no gradient signal) from state-action pairs where the
ratio of conditional probabilities between successive policies is too high.
</article>
</div>
</div>
</section>
<p><br/></p>

<p>To recap, there is a theoretically motivated algorithm \eqref{eq:klpen} which
constrains maximum KL. This motivates TRPO’s bound on
mean KL in \eqref{eq:trpotrust}, which in turn motivates the 
ratio-based bound of PPO (shown in the box). This chain of approximations might 
lead us to ask: <i>how well do these algorithms actually maintain trust regions</i>?</p>

<p>We first plot the mean KL divergence between successive policies for each
algorithm:
<img src="https://gradientscience.org/images/rl/meankl_trust.jpg" style="width: 50%;"/>
<br/></p>

<p>TRPO seems to constrain this very well
<a href="https://gradientscience.org/feed.xml#footnote3"><sup>3</sup></a>
! On the other hand, our two
varieties of the PPO algorithm paint a drastically different picture. Recall that we decided to
separately study two versions of PPO: PPO (based on a state-of-the-art implementation), 
and PPO-M, which we defined to be the core PPO algorithm without auxiliary optimizations. 
PPO <i>with</i> optimizations does quite well at maintaining a KL trust region, 
but PPO-M does not. This is unexpected: PPO’s main mechanism for maintaining the
trust region (the ratio clipping) is present in both methods—the
differences are only in auxiliary optimizations such as Adam learning rate annealing or
orthogonal initialization. As such, it is unclear exactly which mechanisms 
in PPO are responsible for maintaining the mean KL constraint.</p>

<div class="footnote">
<a id="footnote3"><sup>3</sup></a>
Note that this is somewhat unsurprising, since TRPO constrains this
directly in its optimization.
</div>

<p>In fact, we find that PPO’s inability to maintain a KL-based trust region 
is not entirely due to the looseness of its relaxation; it turns out that 
PPO does not even successfully enforce its <i>own</i> ratio-based trust region.
Below, we plot the maximum ratio \eqref{eq:ratio} between successive
policies for the three algorithms in question:</p>

<p><img src="https://gradientscience.org/images/rl/maxratio_trust.jpg" style="width: 50%;"/>
<br/></p>

<p>In the above, the dotted line represents , corresponding to the 
bound in \eqref{eq:ratio}—it looks like the max ratio is not kept at all! And once again, 
simply adding the auxiliary, code-level optimizations to PPO-M
yields <i>better</i> trust region
enforcement, despite the main clipping mechanism staying the same.
Indeed, it turns out that the
way that PPO enforces the ratio trust region does not actually keep the ratios
from becoming too large or too small. In fact, in our paper we show that there
are <i>infinite</i> optima of the optimization problem PPO solves to find each
step and only <i>one</i> of them enforces the intended trust region bound.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>Deep reinforcement learning algorithms are rooted in a well-grounded framework
of classical RL, and have shown great promise in practice. However, as we’ve
found in our three-part investigation, this framework often falls a little short
of explaining the behavior of these algorithms in practice.</p>

<p>Beyond just being disconcerting, this disconnect impedes our understanding of
why these algorithms succeed (or fail). It also poses a major barrier to
addressing key challenges facing deep RL, such as widespread brittleness and
poor reproducibility (as has been observed by our 
<a href="https://gradientscience.org/policy_gradients_pt1">study in part one</a> and
many others, e.g., [<a href="https://arxiv.org/abs/1709.06560">1</a>,
<a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">2</a>,
<a href="http://amid.fish/reproducing-deep-rl">3</a>]).</p>

<p>To close this gap, we need to either develop methods that adhere more closely to
theory, or build theory that can capture what makes existing policy gradient
methods successful. In both cases, the first step is to precisely pinpoint where
theory and practice diverge. Even more broadly, our findings suggest that
developing a deep RL toolkit that is truly robust and reliable will require
moving beyond the current benchmark-driven evaluation model, to a more
fine-grained understanding of deep RL algorithms.</p></div>
    </summary>
    <updated>2019-04-30T00:00:00Z</updated>
    <published>2019-04-30T00:00:00Z</published>
    <source>
      <id>http://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="http://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="http://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2019-05-02T23:35:57Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6025884855508893027</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6025884855508893027/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6025884855508893027" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6025884855508893027" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html" rel="alternate" type="text/html"/>
    <title>x3   + y3 + z3 = 33  has a solution in Z. And its big!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Consider the following problem:<br/>
<br/>
Given k, a natural number, determine if there exists x,y,z INTEGERS such that x<sup>3</sup>+y<sup>3</sup>+z<sup>3</sup>=k.<br/>
<br/>
It is not obvious that this problem is decidable (I think it is but have not been able to find an exact statement to that affect; however, if it was not solvable, I would know that, hence it is solvable. If you know a ref give it in the comments.)<br/>
<br/>
<br/>
If k≡ 4,5 mod 9  then mod arguments easily show there is no solution. <a href="https://arxiv.org/pdf/1604.07746.pdf">Huisman</a> showed that if k≤ 1000, k≡1,2,3,6,7,8 mod 9 and max(|x|,|y|,|z|) ≤ 10<sup>15</sup> and k is NOT one of<br/>
<br/>
33, 42, 114, 165, 390, 579, 627, 633, 732, 795, 906, 921, 975<br/>
<br/>
then there was a solution. For those on the list it was unknown.<br/>
<br/>
Recently <a href="https://people.maths.bris.ac.uk/~maarb/papers/cubesv1.pdf">Booker</a> (not Cory Booker, the candidate for prez, but Andrew Booker who I assume is a math-computer science person and is not running for prez) showed that<br/>
<br/>
x<sup>3</sup> + y<sup>3</sup> + z<sup>3</sup> =33<br/>
<br/>
DOES have a solution in INTEGERS. It is<br/>
<br/>
x= 8,866,128,975,287,528<br/>
<br/>
y=-8,778,405,442,862,239<br/>
<br/>
z=-2,736,111,468,807,040<br/>
<br/>
<br/>
does that make us more likely or less likely to think that<br/>
<br/>
x<sup>3</sup> + y<sup>3</sup> + z<sup>3</sup> =42<br/>
<br/>
has a solution? How about =114, etc, the others on the list?<br/>
<br/>
Rather than say what I think is true (I have no idea) here is what I HOPE is true: that the resolution of these problems leads to some mathematics of interest.<br/>
<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-04-29T02:26:00Z</updated>
    <published>2019-04-29T02:26:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-02T14:13:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/063</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/063" rel="alternate" type="text/html"/>
    <title>TR19-063 |  Efficient Black-Box Identity Testing for Free Group Algebra | 

	Abhranil Chatterjee, 

	Vikraman Arvind, 

	Partha Mukhopadhyay, 

	Rajit Datta</title>
    <summary>Hrubeš and Wigderson [HW14] initiated the study of
  noncommutative arithmetic circuits with division computing a
  noncommutative rational function in the free skew field, and
  raised the question of rational identity testing. It is now known
  that the problem can be solved in deterministic polynomial time in
  the white-box model for noncommutative formulas with
  inverses, and in randomized polynomial time in the black-box
  model [GGOW16, IQS18, DM18], where the running time is
  polynomial in the size of the formula. 

  The complexity of identity testing of noncommutative rational
  functions remains open in general (when the formula size
  is not polynomially bounded). We solve the problem for a natural
  special case. We consider polynomial expressions in the free group
  algebra $\mathbb{F}\langle X, X^{-1}\rangle$ where $X=\{x_1, x_2, \ldots, x_n\}$, a
  subclass of rational expressions of inversion height one. Our main
  results are the following.

1. Given a degree $d$ expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ as a black-box, we obtain a randomized $\text{poly}(n,d)$ algorithm to check
  whether $f$ is an identically zero expression or not. We obtain this
  by generalizing the Amitsur-Levitzki theorem [AL50] to
  $\mathbb{F}\langle X, X^{-1}\rangle$. This also yields a deterministic identity testing algorithm (and even
  an expression reconstruction algorithm) that is polynomial time in
  the sparsity of the input expression.

2. Given an expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ of degree at most
  $D$, and sparsity $s$, as black-box, we can check whether $f$ is
  identically zero or not in randomized $\text{poly}(n,\log s, \log D)$
  time.</summary>
    <updated>2019-04-28T15:46:25Z</updated>
    <published>2019-04-28T15:46:25Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-02T23:35:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/</id>
    <link href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/" rel="alternate" type="text/html"/>
    <title>New York Area Theory Day (Spring 2019)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">May 10, 2019 Columbia University http://www.cs.columbia.edu/theory/s19-tday.html The New York Area Theory Day, co-organized by Columbia, IBM, and NYU, is a semi-annual conference aiming to bring together researchers in the New York Metropolitan area. It usually features a few hour long talks on recent advances in theoretical computer science. The speakers this time are Sepehr Assadi, … <a class="more-link" href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/">Continue reading <span class="screen-reader-text">New York Area Theory Day (Spring 2019)</span></a></div>
    </summary>
    <updated>2019-04-26T22:05:01Z</updated>
    <published>2019-04-26T22:05:01Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-05-02T23:36:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4238</id>
    <link href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 2: Constructing Pseudorandom Sets</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Today we will see how to use the analysis of the multiplicative weights algorithm in order to construct pseudorandom sets. The method will yield constructions that are optimal in terms of the size of the pseudorandom set, but not very … <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 Today we will see how to use the analysis of the multiplicative weights algorithm in order to construct pseudorandom sets. </p>
<p>
The method will yield constructions that are optimal in terms of the size of the pseudorandom set, but not very efficient, although there is at least one case (getting an “almost pairwise independent” pseudorandom generator) in which the method does something that I am not sure how to replicate with other techniques. </p>
<p>
Mostly, the point of this post is to illustrate a concept that will reoccur in more interesting contexts: that we can use an online optimization algorithm in order to construct a combinatorial object satisfying certain desired properties. The idea is to run a game between a “builder” against an “inspector,” in which the inspector runs the online optimization algorithm with the goal of finding a violated property in what the builder is building, and the builder plays the role of the adversary selecting the cost functions, with the advantage that it gets to build a piece of the construction after seeing what property the “inspector” is looking for. By the regret analysis of the online optimization problem, if the builder did well at each round against the inspector, then it will do well also against the “offline optimum” that looks for a violated property after seeing the whole construction. For example, the construction of graph sparsifiers by Allen-Zhu, Liao and Orecchia can be cast in this framework.</p>
<p>
(In some other applications, it will be the “builder” that runs the algorithm and the “inspector” who plays the role of the adversary. This will be the case of the Frieze-Kannan weak regularity lemma and of the Impagliazzo hard-core lemma. In those cases we capitalize on the fact that we know that there is a very good offline optimum, and we keep going for as long as the adversary is able to find violated properties in what the builder is constructing. After a sufficiently large number of rounds, the regret experienced by the algorithm would exceed the general regret bound, so the process must terminate in a small number of rounds. I have been told that this is just the “dual view” of what I described in the previous paragraph.)</p>
<p>
But, back the pseudorandom sets: if <img alt="{{\cal C} = \{ C_1,\ldots,C_N \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+C%7D+%3D+%5C%7B+C_1%2C%5Cldots%2CC_N+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal C} = \{ C_1,\ldots,C_N \}}"/> is a collection of boolean functions <img alt="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_i+%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}"/>, for example the functions computed by circuits of a certain type and a certain size, then a multiset <img alt="{S\subseteq \{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S\subseteq \{ 0,1 \}^n}"/> is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-pseudorandom for <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/> if, for every <img alt="{C_i \in \cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_i+%5Cin+%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_i \in \cal C}"/>, we have </p>
<p align="center"><img alt="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC_i%28s%29+%3D+1+%5D+%7C+%5Cleq+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon "/></p>
<p> That is, sampling uniformly from <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, which we can do with <img alt="{\log_2 |S|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log_2 |S|}"/> random bits, is as good as sampling uniformly from <img alt="{\{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n}"/>, which requires <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits, as far as the functions in <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/> are concerned.</p>
<p>
It is easy to use Chernoff bounds and union bounds to argue that there is such a set of size <img alt="{O(N/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N/\epsilon^2)}"/>, so that we can sample from it using only <img alt="{\log N + 2\log \frac 1 \epsilon + O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+N+%2B+2%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log N + 2\log \frac 1 \epsilon + O(1)}"/> random bits.</p>
<p>
We will prove this result (while also providing an “algorithm” for the construction) using multiplicative weights.</p>
<p>
<span id="more-4238"/></p>
<p>
First of all, possibly by changing <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> to <img alt="{2N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2N}"/>, we may assume that for every function <img alt="{C \in {\cal C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%5Cin+%7B%5Ccal+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C \in {\cal C}}"/> the function <img alt="{1-C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-C}"/> is also in <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/>. This simplifies things a bit because then the pseudorandom condition is equivalent to just</p>
<p/><p align="center"><img alt="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+C%5Cin+%7B%5Ccal+C%7D+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC%28s%29+%3D+1+%5D+%5Cgeq+-+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon "/></p>
<p>
We will make up an “experts” setup in which there is an expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> for each function <img alt="{C_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_i}"/>. Thus, the algorithm, at each step, comes up with a probability distribution <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> over the functions, which we can think of as a “probabilistic function.” At time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, the adversary chooses a string <img alt="{s_t \in \{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t+%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t \in \{ 0,1 \}^n}"/> and defines the cost function </p>
<p align="center"><img alt="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+%5Csum_%7Bi%3D1%7D%5EN+x%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) "/></p>
<p> where the adversary chooses an <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> such that <img alt="{f_t(x_t) \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x_t) \geq 0}"/>. At this point, the reader should try, without reading ahead, to establish: </p>
<ol>
<li> That such a choice of <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> is always possible;
</li><li> That the cost function is of the form <img alt="{f_t(x) = \langle \ell_t , x\rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Clangle+%5Cell_t+%2C+x%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x) = \langle \ell_t , x\rangle}"/>, where the loss vector <img alt="{\ell_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t}"/> satisfies <img alt="{|\ell_t (i) | \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\ell_t (i) | \leq 1}"/>, so that the regret after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps is <img alt="{\leq 2 \sqrt{T \ln N}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+2+%5Csqrt%7BT+%5Cln+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\leq 2 \sqrt{T \ln N}}"/>;
</li><li> That the sequence <img alt="{s_1,\ldots,s_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_1%2C%5Cldots%2Cs_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_1,\ldots,s_T}"/> of choices by the adversary determines a <img alt="{2\sqrt {\frac {\ln N}{T}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Csqrt+%7B%5Cfrac+%7B%5Cln+N%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\sqrt {\frac {\ln N}{T}}}"/>-pseudorandom multiset for <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/>, and, in particular, we get an <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-pseudorandom multiset of cardinality <img alt="{4 \frac {\ln N}{\epsilon^2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4+%5Cfrac+%7B%5Cln+N%7D%7B%5Cepsilon%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{4 \frac {\ln N}{\epsilon^2}}"/>
</li></ol>
<p> For the first point, note that for a random <img alt="{s \sim \{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s \sim \{ 0,1 \}^n}"/> we have </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7Bs%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s%29+%5Cright%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 "/></p>
<p> so there is an <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> such that </p>
<p align="center"><img alt="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 "/></p>
<p> For the second point we just have to inspect the definition, and for the last point we have, by construction </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 "/></p>
<p> so the regret bound is </p>
<p align="center"><img alt="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cgeq+-+%7B%5Crm+Regret%7D_T+%5Cgeq+-+2+%5Csqrt%7BT%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} "/></p>
<p> which, after dividing by <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>, is </p>
<p align="center"><img alt="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i+%3A+%5C+%5C+%5C+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} "/></p>
<p align="center"><img alt="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i%3A+%5C+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+%5CPr_%7Bs%5Cin+%5C%7B+s_1%2C%5Cldots%2Cs_T+%5C%7D+%7D+%5BC_i+%28s%29+%3D+1+%5D+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} "/></p>
<p> Consider now the application of constructing a small-support distribution over <img alt="{\{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n}"/> that is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-almost-pairwise-independent, meaning that if <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> is a random string sampled according to this distribution, then, for every <img alt="{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i,j}"/>, the marginal <img alt="{(s_i,s_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28s_i%2Cs_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(s_i,s_j)}"/> is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-close to the uniform distribution over <img alt="{\{ 0,1 \}^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^2}"/> in total variation distance. This is the same thing as asking for a small-support distribution that is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-pseudorandom for all functions <img alt="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}"/> that depend on only two input variables. There are only <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> such functions, so the above construction gives us a pseudorandom distribution that is uniform over a set of size <img alt="{O(\epsilon^{-2} \ln n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+%5Cln+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} \ln n)}"/>, meaning that the distribution can be sampled using <img alt="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n+%2B+2+%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}"/> random bits. Furthermore the algorithm can be implemented to run in time <img alt="{n^{O(1)} / \epsilon^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BO%281%29%7D+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{O(1)} / \epsilon^2}"/>. The only tricky step is how to find the string <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> at each step. For a string <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>, the loss <img alt="{f (x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf+%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f (x_t)}"/> obtained by choosing <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> as the “reference string” is a polynomial of degree 2 in the bits of <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>, so we can find a no-worse-than-average <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> using the method of conditional expectations. I am not sure if there is a more standard way of doing this construction, perhaps one in which the bit <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> of the <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>-th string in the sample space can be generated in time <img alt="{(\log n)^{O(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\log n)^{O(1)}}"/>. The standard approach is to combine a small-bias generator with a linear family of pairwise independent hash functions, but even using Ta-Shma’s construction of small-bias generators we would not get the correct dependency on <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>. This framework can “derandomize Chernoff bounds” in other settings as well, such as randomized rounding of packing and covering integer linear programs, and it is basically the same thing as the method of “pessimistic estimators” described in the Motwani-Raghavan book on randomized algorithms. </p>
<p/></div>
    </content>
    <updated>2019-04-26T01:00:06Z</updated>
    <published>2019-04-26T01:00:06Z</published>
    <category term="theory"/>
    <category term="multiplicative weights"/>
    <category term="online optimization"/>
    <category term="Pseudorandomness"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-02T23:20:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=351</id>
    <link href="https://tcsplus.wordpress.com/2019/04/25/tcs-talk-wednesday-may-1st-chris-peikert-university-of-michigan/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 1st — Chris Peikert, University of Michigan</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Chris Peikert from University of Michigan will speak about “Noninteractive Zero Knowledge for NP from Learning With Errors” (abstract below). Please make sure you reserve a spot for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong><a href="http://web.eecs.umich.edu/~cpeikert/">Chris Peikert</a></strong> from University of Michigan will speak about “<em>Noninteractive Zero Knowledge for NP from Learning With Errors</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We finally close the long-standing problem of constructing a noninteractive zero-knowledge (NIZK) proof system for any NP language with security based on the Learning With Errors (LWE) problem, and thereby on worst-case lattice problems. Our proof system instantiates a framework developed in a series of recent works for soundly applying the Fiat—Shamir transform using a hash function family that is <em>correlation intractable</em> for a suitable class of relations. Previously, such hash families were based either on “exotic” assumptions (e.g., indistinguishability obfuscation or optimal hardness of ad-hoc LWE variants) or, more recently, on the existence of circularly secure fully homomorphic encryption. However, none of these assumptions are known to be implied by LWE or worst-case hardness.</p>
<p>Our main technical contribution is a hash family that is correlation intractable for arbitrary size-<img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" title="S"/> circuits, for any polynomially bounded <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" title="S"/>, based on LWE (with small polynomial approximation factors). Our construction can be instantiated in two possible “modes,” yielding a NIZK that is either computationally sound and statistically zero knowledge in the common random string model, or vice-versa in the common reference string model.</p>
<p>(This is joint work with Sina Shiehian. Paper: <a href="https://eprint.iacr.org/2019/158" rel="noopener" target="_blank">https://eprint.iacr.org/2019/158</a>)</p></blockquote></div>
    </content>
    <updated>2019-04-25T20:01:09Z</updated>
    <published>2019-04-25T20:01:09Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-05-02T23:36:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1084</id>
    <link href="http://corner.mimuw.edu.pl/?p=1084" rel="alternate" type="text/html"/>
    <title>Call for Participation: HALG 2019 (Highlights of Algorithms)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">------------------------------------------------------------------- 4rd Highlights of Algorithms conference (HALG 2019) Copenhagen, June 14-16, 2019 http://highlightsofalgorithms.org/ The Highlights of Algorithms conference is a forum for presenting the highlights of recent developments in algorithms and for discussing potential further advances in this area. The … <a href="http://corner.mimuw.edu.pl/?p=1084">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>-------------------------------------------------------------------<br/>
4rd Highlights of Algorithms conference (HALG 2019)<br/>
Copenhagen, June 14-16, 2019<br/>
<a href="http://highlightsofalgorithms.org/" rel="noopener noreferrer" target="_blank">http://highlightsofalgorithms.org/</a></p>
<p>The Highlights of Algorithms conference is a forum for presenting the<br/>
highlights of recent developments in algorithms and for discussing<br/>
potential further advances in this area. The conference will provide a<br/>
broad picture of the latest research in algorithms through a series of<br/>
invited talks, as well as the possibility for all researchers and<br/>
students to present their recent results through a series of short<br/>
talks and poster presentations. Attending the Highlights of Algorithms<br/>
conference will also be an opportunity for networking and meeting<br/>
leading researchers in algorithms.<br/>
-------------------------------------------------------------------</p>
<p>PROGRAM</p>
<p>The conference will begin on Friday, June 14, at 9:00 and end on<br/>
Sunday, June 16, at 18:00. A detailed schedule and a list of all<br/>
accepted short contributions can be found at:<br/>
<a href="http://2018.highlightsofalgorithms.org/programme" rel="noopener noreferrer" target="_blank">2018.highlightsofalgorithms.org/programme</a>.</p>
<p>-------------------------------------------------------------------</p>
<p>REGISTRATION</p>
<p>Please register on our webpage<br/>
     <a href="http://highlightsofalgorithms.org/registration" rel="noopener noreferrer" target="_blank">http://highlightsofalgorithms.org/registration</a><br/>
We have done our best to keep registration fees at a minimum:</p>
<p>Early registration (by April 29, 2019)<br/>
- academic rate (incl. postdocs): 160€<br/>
- student rate: 115€</p>
<p>Regular registration will be 50€ more expensive.</p>
<p>The organizers strongly recommend that you book your hotel as soon as possible.</p>
<p>-------------------------------------------------------------------</p>
<p>CONFERENCE VENUE</p>
<p>The conference will take place at the H.C. Ørsted Institute of the<br/>
University of Copenhagen.<br/>
The address is: Universitetsparken 5, DK-2100 Copenhagen.</p>
<p>-------------------------------------------------------------------</p>
<p>INVITED SPEAKERS</p>
<p>Survey speakers:<br/>
Monika Henzinger (University of Vienna)<br/>
Thomas Vidick (California Institute of Technology)<br/>
Laszlo Vegh (London School of Economics)<br/>
James Lee (University of Washington)<br/>
Timothy Chan (University of Illinois at Urbana-Champaign)<br/>
Sergei Vassilvitskii (Google, New York)</p>
<p>Invited talks:<br/>
Martin Grohe (RWTH Aachen University)<br/>
Josh Alman (MIT)<br/>
Nima Anari (Stanford University)<br/>
Michal Koucký (Charles University)<br/>
Naveen Garg (IIT Delhi)<br/>
Vera Traub (University of Bonn)<br/>
Rico Zenklusen (ETH Zurich)<br/>
Shayan Oveis Gharan (University of Washington)<br/>
Greg Bodwin (MIT)<br/>
Cliff Stein (Columbia University)<br/>
Sungjin Im (University of California at Merced)<br/>
C. Seshadhriy (University of California, Santa Cruz)<br/>
Shay Moran (Technion)<br/>
Bundit Laekhanukit (Shanghai University of Finance and Economics)<br/>
Sebastien Bubeck (Microsoft Research, Redmond)<br/>
Sushant Sachdeva (University of Toronto)<br/>
Kunal Talwar (Google Brain)<br/>
Moses Charikar (Stanford University)<br/>
Shuichi Hirahara (University of Tokyo)</p>
<p>------------------------------------------------------------------</p></div>
    </content>
    <updated>2019-04-25T12:36:21Z</updated>
    <published>2019-04-25T12:36:21Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2019-05-02T23:36:13Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8078353386966881451</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8078353386966881451/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8078353386966881451" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8078353386966881451" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html" rel="alternate" type="text/html"/>
    <title>Geo-Centric Complexity</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">An interesting discussion during Dagstuhl last month about the US-centric view of theory. Bad enough that all talks and papers in an international venue are in English but we also have<br/>
<ul>
<li><a href="https://en.wiktionary.org/wiki/Manhattan_distance">Manhattan Distance</a>. How are foreigners supposed to know about the structure of streets in New York? What's wrong with grid distance?</li>
<li><a href="https://en.wikipedia.org/wiki/Las_Vegas_algorithm">Las Vegas Algorithms</a>. I found this one a little unfair, after all Monte Carlo algorithms came first. Still today might not Macau algorithms make sense?</li>
<li><a href="https://en.wikipedia.org/wiki/Arthur%E2%80%93Merlin_protocol">Arthur-Merlin Games</a>. A British reference by a Hungarian living in the US (László Babai who also coined Las Vegas algorithms). Still the Chinese might not know the fables. Glad the Europeans don't remember the <a href="https://blog.computationalcomplexity.org/2017/04/alice-and-bob-and-pat-and-vanna.html">Pat and Vanna</a> terminology I used in my first STOC talk. </li>
<li>Alice and Bob. The famous pair of cryptographers but how generically American can you get. Why not Amare and Bhati?</li>
</ul>
<div>
I have two minds here. We shouldn't alienate or confuse those who didn't grow up in an Anglo-American culture. On the other hand, I hate to have to try and make all terminology culturally neutral, you'd just end up with technical and ugly names, like P and NP.</div></div>
    </content>
    <updated>2019-04-25T12:12:00Z</updated>
    <published>2019-04-25T12:12:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-02T14:13:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17389</id>
    <link href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/" rel="alternate" type="text/html"/>
    <title>Are Natural Mathematical Problems Bad Problems?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">One unique aspect of the conference “Visions in Mathematics Towards 2000” (see the previous post) was that there were several discussion sessions where speakers and other participants presented some thoughts about mathematics (or some specific areas), discussed and argued.  In … <a href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One unique aspect of the conference “Visions in Mathematics Towards 2000” (see the <a href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/">previous post</a>) was that there were several discussion sessions where speakers and other participants presented some thoughts about mathematics (or some specific areas), discussed and argued.  In the lectures themselves you could also see a large amount of audience participation and discussions which was very nice.</p>
<p>Let me draw your attention to  one question raised and discussed in one of the discussion sessions.</p>
<h3><a href="https://youtu.be/Fme_r-nE4CI?t=1400">3.4 Discussion on Geometry with introduction by M. Gromov</a></h3>
<p/>
<p>Now, lets skip a lot of interesting staff and move <a href="https://youtu.be/Fme_r-nE4CI?t=1400">to minute 23:20</a> where Noga Alon asked Misha Gromov to elaborate a statement from his <a href="https://youtu.be/gd6EB2Zk6OE">opening lecture of the conference</a> that  the densest packing problem in <img alt="R^3" class="latex" src="https://s0.wp.com/latex.php?latex=R%5E3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R^3"/> is not interesting.  In what follows Misha Gromov passionately argued that natural problems are bad problems (or are even stupid questions), and a lovely discussion emerged (in 25:00 Yuval Neeman commented about cosmology in response to Connes’s earlier remarks but then around 27:00 Vitali asked Misha to name some bad problems in geometry and the discussion resumed.) Misha made several lovely provocative further comments: he rejected the claim that this is a matter of taste, and argued that people make conjectures when they absolutely have no right to do so.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png"><img alt="" class="alignnone size-full wp-image-17390" height="361" src="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png?w=640&amp;h=361" width="640"/></a></p>
<p><strong><span style="color: #ff0000;"> Misha argues passionately that natural problems are stupid problems</span></strong></p>
<p>Actually one problem that Misha mentioned in his lecture as interesting (see also Gromov’s proceedings paper <a href="https://www.ihes.fr/~gromov/wp-content/uploads/2018/08/SpacesandQuestions.pdf">Spaces and questions),</a> and that was raised both by him and by me is to prove an exponential upper bound for the number of simplicial 3-spheres with n facets. I remember that we talked about it in the conference and Misha was certain that the problem could be solved for shellable spheres while I was confident that the case of shellable spheres would be as hard as the general case.  He was right! This goes back to works of physicists Durhuus and Jonsson see this paper <a href="https://arxiv.org/abs/0902.0436">On locally constructible spheres and balls</a> by Bruno Benedetti and  Günter M. Ziegler.</p>
<h5>(Disclaimer: I asked quite a few questions that were both unnatural and stupid and made several conjectures when I had no right to do so.)</h5>
<p><span id="more-17389"/></p>
<p>encore</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/v1.png"><img alt="" class="alignnone size-medium wp-image-17401" height="188" src="https://gilkalai.files.wordpress.com/2019/04/v1.png?w=300&amp;h=188" width="300"/></a>  <a href="https://gilkalai.files.wordpress.com/2019/04/v2.png"><img alt="" class="alignnone size-medium wp-image-17402" height="210" src="https://gilkalai.files.wordpress.com/2019/04/v2.png?w=300&amp;h=210" width="300"/>  </a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/n1.png"><img alt="" class="alignnone size-medium wp-image-17403" height="227" src="https://gilkalai.files.wordpress.com/2019/04/n1.png?w=300&amp;h=227" width="300"/></a> <a href="https://gilkalai.files.wordpress.com/2019/04/n2.png"><img alt="" class="alignnone size-medium wp-image-17404" height="300" src="https://gilkalai.files.wordpress.com/2019/04/n2.png?w=214&amp;h=300" width="214"/></a></p>
<p><span style="color: #ff0000;">Vitali Milman attacked the solution of the 4CT as “bad”and Segei Novikov disagreed and referred to the proof as “great”.  </span></p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2019-04-25T09:42:26Z</updated>
    <published>2019-04-25T09:42:26Z</published>
    <category term="Combinatorics"/>
    <category term="Conferences"/>
    <category term="Open discussion"/>
    <category term="What is Mathematics"/>
    <category term="Misha Gromov"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-05-02T23:35:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4236</id>
    <link href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 1: Multiplicative Weights</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The multiplicative weights or hedge algorithm is the most well known and most frequently rediscovered algorithm in online optimization. The problem it solves is usually described in the following language: we want to design an algorithm that makes the best … <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 The <em>multiplicative weights</em> or <em>hedge</em> algorithm is the most well known and most frequently rediscovered algorithm in online optimization. </p>
<p>
The problem it solves is usually described in the following language: we want to design an algorithm that makes the best possible use of the advice coming from <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> self-described experts. At each time step <img alt="{t=1,2,\ldots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C2%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1,2,\ldots}"/>, the algorithm has to decide with what probability to follow the advice of each of the experts, that is, the algorithm has to come up with a probability distribution <img alt="{x_t = (x_t(1),\ldots,x_t(n))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%3D+%28x_t%281%29%2C%5Cldots%2Cx_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t = (x_t(1),\ldots,x_t(n))}"/> where <img alt="{x_t (i) \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t (i) \geq 0}"/> and <img alt="{\sum_{i=1}^n x_t(i)=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{i=1}^n x_t(i)=1}"/>. After the algorithm makes this choice, it is revealed that following the advice of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> leads to loss <img alt="{\ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t (i)}"/>, so that the expected loss of the algorithm at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is <img alt="{\sum_{i=1}^n x_t(i) \ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{i=1}^n x_t(i) \ell_t (i)}"/>. A loss can be negative, in which case its absolute value can be interpreted as a profit.</p>
<p>
After <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps, the algorithm “regrets” that it did not just always follow the advice of the expert that, with hindsight, was the best one, so that the regret of the algorithm after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps is </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t%28i%29+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bi%3D1%2C%5Cldots%2Cn%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) "/></p>
<p>
This corresponds to the instantiation of the framework we described in the previous post to the special case in which the set of feasible solutions <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is the set <img alt="{\Delta \subseteq {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Delta \subseteq {\mathbb R}^n}"/> of probability distributions over the sample space <img alt="{\{ 1,\ldots,n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 1,\ldots,n\}}"/> and in which the loss functions <img alt="{f_t (x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t (x)}"/> are linear functions of the form <img alt="{f_t (x) = \sum_i x(i) \ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+x%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t (x) = \sum_i x(i) \ell_t (i)}"/>. In order to bound the regret, we also have to bound the “magnitude” of the loss functions, so in the following we will assume that for all <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> and all <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> we have <img alt="{| \ell_t (i) | \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{| \ell_t (i) | \leq 1}"/>, and otherwise we can scale everything by a known upper bound on <img alt="{\max_{t,i} |\ell_t |}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bt%2Ci%7D+%7C%5Cell_t+%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\max_{t,i} |\ell_t |}"/>.</p>
<p>
We now describe the algorithm.</p>
<p>
The algorithm maintains at each step <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> a vector of <em>weights</em> <img alt="{w_t = (w_t(1),\ldots,w_t(n))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_t+%3D+%28w_t%281%29%2C%5Cldots%2Cw_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_t = (w_t(1),\ldots,w_t(n))}"/> which is initialized as <img alt="{w_1 := (1,\ldots,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_1+%3A%3D+%281%2C%5Cldots%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_1 := (1,\ldots,1)}"/>. The algorithm performs the following operations at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>: </p>
<ul>
<li> <img alt="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_t+%28i%29+%3A%3D+w_%7Bt-1%7D+%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_%7Bt-1%7D+%28i%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}"/>
</li><li> <img alt="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%3A%3D+%5Cdisplaystyle+%5Cfrac+%7Bw_t+%28i%29+%7D%7B%5Csum_%7Bj%3D1%7D%5En+w_t%28j%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}"/>
</li></ul>
<p>
That is, the weight of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is <img alt="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-%5Cepsilon+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}"/>, and the probability <img alt="{x_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t(i)}"/> of following the advice of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is proportional to the weight. The parameter <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon&gt;0}"/> is hardwired into the algorithm and we will optimize it later. Note that the algorithm gives higher weight to experts that produced small losses (or negative losses of large absolute value) in the past, and thus puts higher probability on such experts.</p>
<p>
We will prove the following bound.</p>
<blockquote><p><b>Theorem 1</b> <em> Assuming that for all <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> and <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> we have <img alt="{| \ell_t(i) | \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{| \ell_t(i) | \leq 1}"/>, for every <img alt="{0 &lt; \epsilon &lt; 1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%3C+%5Cepsilon+%3C+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 &lt; \epsilon &lt; 1/2}"/>, after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps the multiplicative weight algorithm experiences a regret that is always bounded as </em></p><em>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell%5E2+_t+%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+%5Cleq+%5Cepsilon+T+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} "/></p>
<p> In particular, if <img alt="{T &gt; 4 \ln n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3E+4+%5Cln+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T &gt; 4 \ln n}"/>, by setting <img alt="{\epsilon = \sqrt{\frac{\ln n}{T}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+%5Csqrt%7B%5Cfrac%7B%5Cln+n%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon = \sqrt{\frac{\ln n}{T}}}"/> we achieve a regret bound </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
<span id="more-4236"/></p>
<p>
We will start by giving a short proof of the above theorem. </p>
<p>
For each time step <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, define the quantity</p>
<p/><p align="center"><img alt="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_t+%3A%3D+%5Csum_%7Bi%3D1%7D%5En+w_t%28i%29+%5C+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . "/></p>
<p> We want to prove that, roughly speaking, the only way for an adversary to make the algorithm incur a large loss is to produce a sequence of loss functions such that <em>even the best expert incurs a large loss</em>. The proof will work by showing that if the algorithm incurs a large loss after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps, then <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small, and that if <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small, then even the best expert incurs a large loss.</p>
<p>
Let us define </p>
<p align="center"><img alt="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L%5E%2A+%3D+%5Cmin_%7Bi+%3D+1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) "/></p>
<p> to be the loss of the best expert. Then we have</p>
<blockquote><p><b>Lemma 2 (If <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small, then <img alt="{L^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L^*}"/> is large)</b> <em> </em></p><em>
<p align="center"><img alt="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cgeq+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
<em>Proof:</em>  Let <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> be an index such that <img alt="{L^* = \sum_{t=1}^T \ell_t (j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A+%3D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L^* = \sum_{t=1}^T \ell_t (j)}"/>. Then we have </p>
<p align="center"><img alt="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%7D+%5Cgeq+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29%7D+%3D+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} "/></p>
<p> <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<blockquote><p><b>Lemma 3 (If the loss of the algorithm is large then <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small)</b> <em> </em></p><em>
<p align="center"><img alt="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cleq+n+%5Cprod_%7Bt%3D1%7D%5En+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t+%2C+%5Cell%5E2_t+%5Crangle%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) "/></p>
</em><p><em> where <img alt="{\ell_t^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t^2}"/> is the vector whose <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>-th coordinate is <img alt="{\left( \ell_t (i)\right)^2 }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%28+%5Cell_t+%28i%29%5Cright%29%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left( \ell_t (i)\right)^2 }"/> </em></p></blockquote>
<p/><p>
<em>Proof:</em>  Since we know that <img alt="{W_1 = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_1+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_1 = n}"/>, it is enough to prove that, for every <img alt="{t=1,\ldots, T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C%5Cldots%2C+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1,\ldots, T}"/>, we have <a name="eq.lemma.two"/></p><a name="eq.lemma.two">
<p align="center"><img alt="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7Bt%2B1%7D+%5Cleq+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t%2C+%5Cell_t%5E2+%5Crangle+%29+%5Ccdot+W_t++%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)"/></p>
</a><p><a name="eq.lemma.two"/> And we see that </p>
<p align="center"><img alt="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7BW_%7Bt%2B1%7D%7D%7BW_t%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_%7Bt%2B1%7D%28i%29%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} "/></p>
<p align="center"><img alt="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t+%28i%29+%7D+%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} "/></p>
<p align="center"><img alt="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } "/></p>
<p align="center"><img alt="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+%28+1+-+%5Cepsilon+%5Cell_t+%28i%29+%2B+%5Cepsilon%5E2+%5Cell_t%5E2%28i%29+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) "/></p>
<p align="center"><img alt="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+1+-+%5Cepsilon+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle "/></p>
<p> where we used the definitions of our quantities and the fact that <img alt="{e^{-z} \leq 1-z+z^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-z%7D+%5Cleq+1-z%2Bz%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{-z} \leq 1-z+z^2}"/> for <img alt="{|z| \leq 1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|z| \leq 1/2}"/>. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
Using the fact that <img alt="{1-z \leq e^{-z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-z+%5Cleq+e%5E%7B-z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-z \leq e^{-z}}"/> for all <img alt="{|z| \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|z| \leq 1}"/>, the above lemmas can be restated as </p>
<p align="center"><img alt="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cleq+%5Cln+n+-+%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET+%5Cepsilon+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+%2B+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+x_t%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cgeq+-+%5Cepsilon+L%5E%2A+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* "/></p>
<p> which together imply </p>
<p align="center"><img alt="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+-+L%5E%2A+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell%5E2_t+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle "/></p>
<p> as desired.</p>
<p>
Personally, I find all of the above very unsatisfactory, because both the algorithm and the analysis, but especially the analysis, seem to come out of nowhere. In fact, I never felt that I actually understood this analysis until I saw it presented as a special case of the <em>Follow The Regularized Leader</em> framework that we will discuss in a future post. (We will actually prove a slightly weaker bound, but with a much more satisfying proof.)</p>
<p>
Here is, however, a story of how a statistical physicist might have invented the algorithm and might have come up with the analysis. Let’s call the loss caused by expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> after <img alt="{t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t-1}"/> steps the <em>energy</em> of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>: </p>
<p align="center"><img alt="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_t%28i%29+%3D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) "/></p>
<p> Note that we have defined it in such a way that the algorithm knows <img alt="{E_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_t(i)}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. Our offline optimum is the energy of the lowest energy expert at time <img alt="{T+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T+1}"/>, that, is, the energy of the <em>ground state</em> at time <img alt="{T+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T+1}"/>. When we have a collection of numbers <img alt="{E_t(1),\ldots, E_t(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_t%281%29%2C%5Cldots%2C+E_t%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_t(1),\ldots, E_t(n)}"/>, a nice lower bound to their minimum is </p>
<p align="center"><img alt="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_i+E_t%28i%29+%5Cgeq+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } "/></p>
<p> which is true for every <img alt="{\epsilon &gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt;0}"/>. The right-hand side above is the <em>free energy</em> at temperature <img alt="{\frac 1 \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac 1 \epsilon}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. This seems like the kind of expression that we could use to bound the offline optimum, so let’s give it a name </p>
<p align="center"><img alt="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_t+%3A%3D+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } "/></p>
<p> In terms of coming up with an algorithm, all that we have got to work with at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> are the losses of the experts at times <img alt="{1,\ldots,t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2C%5Cldots%2Ct-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1,\ldots,t-1}"/>. If the adversary chooses to make one of the experts consistently much better than the others, it is clear that, in order to get any reasonable regret bound, the algorithm will have to put much of the probability mass in most of the steps on that expert. This suggests that the <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> should put higher probability on experts that have done well in the first <img alt="{t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t-1}"/> steps, that is <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> should put higher probability on “lower-energy” experts. When we have a system in which, at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, state <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> has energy <img alt="{E_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_t(i)}"/>, a standard distribution that puts higher probability on lower energy states is the <em>Gibbs distribution</em> at temperature <img alt="{1/\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/\epsilon}"/>, defined as </p>
<p align="center"><img alt="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7B+e%5E%7B-%5Cepsilon+E_t+%28i%29%7D+%7D%7B%5Csum_j+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } "/></p>
<p> where the denominator above is also called the <em>partition function</em> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> </p>
<p align="center"><img alt="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Z_t+%3A%3D+%5Csum_%7Bj%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } "/></p>
<p> So far we have “rediscovered” our multiplicative weights algorithm, and the quantity <img alt="{W_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_t}"/> that we had in our analysis gets interpreted as the partition function <img alt="{Z_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z_t}"/>. The fact that <img alt="{\Phi_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi_{T+1}}"/> bounds the offline optimum suggests that we should use <img alt="{\Phi_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi_t}"/> as a potential function, and aim for an analysis involving a telescoping sum. Indeed some manipulations (the same as in the short proof above, but which are now more mechanical) give that the loss of the algorithm at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is </p>
<p align="center"><img alt="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7Bt%2B1%7D+-+%5CPhi_%7Bt%7D+%2B+%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle "/></p>
<p> which telescopes to give </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7BT%2B1%7D+-+%5CPhi_1+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle "/></p>
<p> Recalling that </p>
<p align="center"><img alt="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_1+%3D+-+%5Cfrac+1+%7B%5Cepsilon%7D+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_%7BT%2B1%7D+%5Cleq+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) "/></p>
<p> we have again </p>
<p align="center"><img alt="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+%5Cright%29+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle "/></p>
<p> As mentioned above, we will give a better story when we get to the <em>Follow The Regularized Leader</em> framework. In the next post, we will discuss complexity-theory consequences of the result we just proved. </p></div>
    </content>
    <updated>2019-04-25T06:44:54Z</updated>
    <published>2019-04-25T06:44:54Z</published>
    <category term="theory"/>
    <category term="multiplicative weights"/>
    <category term="online optimization"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-02T23:20:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15798</id>
    <link href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/" rel="alternate" type="text/html"/>
    <title>Why Check A Proof?</title>
    <summary>Why check another’s proof? [Russell and Whitehead ] Bertrand Russell and Alfred Whitehead were not primarily trying to mechanize mathematics in writing their famous book. They wanted to assure precision and certainty in proofs while minimizing the axioms and rules they rest on. They cared more about checking proofs than generating theorems. By the way: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Why check another’s proof?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/russellwhitehead1900s/" rel="attachment wp-att-15809"><img alt="" class="alignright size-medium wp-image-15809" height="203" src="https://rjlipton.files.wordpress.com/2019/04/russellwhitehead1900s.png?w=300&amp;h=203" width="300"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[Russell and Whitehead ]</font></td>
</tr>
</tbody>
</table>
<p>
Bertrand Russell and Alfred Whitehead were not primarily trying to mechanize mathematics in writing their famous book. They wanted to assure precision and certainty in proofs while minimizing the axioms and rules they rest on. They cared more about checking proofs than generating theorems. By the way: They are listed in the order Whitehead and Russell on the book. See <a href="https://thonyc.wordpress.com/2016/05/19/bertrand-russell-did-not-write-principia-mathematica/">this</a> for a discussion about the importance of the order.</p>
<p><a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/unknown-120/" rel="attachment wp-att-15804"><img alt="" class="aligncenter size-full wp-image-15804" src="https://rjlipton.files.wordpress.com/2019/04/unknown-1.jpeg?w=600"/></a></p>
<p>
Today Ken and I thought we would add a few more thoughts on why proofs get checked.<br/>
<span id="more-15798"/></p>
<p>
We discussed those who <em>claim</em> proofs in our previous <a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/">post</a>. Once a proof is claimed, it needs people to check it. This is not as fraught as the <a href="https://en.wikipedia.org/wiki/Replication_crisis">replication crisis</a> in other sciences where “proof” is a statement of statistical significance whose most intensive check needs repeating the experiment. </p>
<p>
If you do a Google search on “why check proofs” you get lots of hits on using automated proof checkers. Coming on eleven decades after the publication of Russell and Whitehead’s three-volume <a href="https://en.wikipedia.org/wiki/Principia_Mathematica">opus</a> <em>Principia Mathematica</em>, these are still in their formative years. We <a href="https://rjlipton.wordpress.com/2013/07/14/surely-you-are-joking/">covered</a> a major system of this kind some years ago. </p>
<p>
We are personally more interested in what motivates us <em>humans</em> to check proofs. We believe that there are various factors that make it less or more likely to find a good human checker. So today we will try to list some of them. </p>
<p>
</p><p/><h2> Why Check A Proof? </h2><p/>
<p/><p>
One of the questions that was raised by some commenters to our recent post is: <i>Why should I check your proof?</i></p>
<p>
This is a critical question. If their is no reason to check your proof, then your result will not get checked. It is almost a tautology. We like this question and thought we could suggest several ways to increase the likelihood that one will check another person’s proof. </p>
<p>
So lets assume that Alice is claiming some new theorem <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> and we ponder whether Bob will spend time checking it.</p>
<p>
</p><p/><h3> Bob has to </h3><p/>
<p>This happens when Bob is required to check her proof. This can happen if Bob is a referee of her paper. It could also be when Bob is hired to do this task. It usually is a weak reason for making someone do the checking. In real life we think that it is unlikely to be a strong motivator.</p>
<p>
</p><p/><h3> Bob wants to </h3><p/>
<p>This happens when Bob feels that he will benefit from checking. The main type of situation here is: Alice’s theorem <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> uses some new method or trick. If Bob believes that this method can be used in his work, in his research, in his future papers, then he is strongly motivated.</p>
<p>
We are all very self-centered in our research. If we think we could in the future use your method we are likely to spent time and energy on your proof. Thus if Bob is convinced that Alice has a some new ideas, he is much more likely to spent the time checking her theorem. This means that Alice should—if possible–explain that her proof of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> uses something new. Proofs that are “just technical inductions” are very unlikely to get Bob to read them. In many areas some authors have stated things like: <i>The proof is a careful induction…</i> This is not a good idea. </p>
<p>
</p><p/><h3> Bob needs to </h3><p/>
<p>This happens when Bob has some “skin” in the game. A classic situation is when Bob has an earlier result that is affected by Alice’s new theorem. If <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is stronger than Bob’s previous result, then he is motivated to check her theorem. Or if <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> shows that his earlier theorem is false, this is a very strong motivation. Or perhaps Alice has proven a lemma that enables Bob to push something through.</p>
<p>
</p><p/><h2> Skin in the Game </h2><p/>
<p/><p>
Often we have situations where you do have skin in the game. An old <a href="https://rjlipton.wordpress.com/2009/09/27/surprises-in-mathematics-and-theory/">example</a> that comes to mind is from group theory. The problem is a natural question about a class of groups: Let <img alt="{B(m,n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%28m%2Cn%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B(m,n)}"/> be the class of groups that are generated by <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> elements and all elements in the group satisfy, <img alt="{x^{n} = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Bn%7D+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^{n} = 1}"/>. Sergei Adian and Pyotr Novikov proved that <img alt="{B(m, n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%28m%2C+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B(m, n)}"/> is infinite for <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> odd, <img alt="{{n \ge 4381}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Bn+%5Cge+4381%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{n \ge 4381}}"/> by a long complex combinatorial proof in 1968. This is a famous result. </p>
<p>
Shortly after another group theorist, John Britton, claimed an alternative proof in 1970. Unfortunately, Adian later discovered that Britton’s proof was wrong. I do not have first-hand information, but I was told that Adian was motivated by wanting to have <i>the</i> proof. He worked hard until he discovered an unrepairable bug in Britton’s 300-page monograph. The proof was unsalvageable.</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/yau/" rel="attachment wp-att-15802"><img alt="" class="aligncenter size-medium wp-image-15802" height="300" src="https://rjlipton.files.wordpress.com/2019/04/yau.jpg?w=199&amp;h=300" width="199"/></a></p>
<p>
A much newer example is from a recent book by Shing-Tung Yau, <a href="https://yalebooks.yale.edu/book/9780300235906/shape-life">The Shape of a Life</a>. He is a famous geometry expert and has made many important contributions to many areas of mathematics. We will probably discuss his book in detail in the future, but for today it has a neat example of “skin in the game”. He writes about an enumeration problem of counting how many curves lie on a certain manifold—a century old problem. One group used a clever trick to get the number 	</p>
<p align="center"><img alt="\displaystyle  317,206,375. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++317%2C206%2C375.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  317,206,375. "/></p>
<p>However another group discovered via a different method that the count was 	</p>
<p align="center"><img alt="\displaystyle  2,682,549,425. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%2C682%2C549%2C425.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  2,682,549,425. "/></p>
<p>Somewhat a different count—not even close. Clearly, both sets of authors were heavily motivated to check their work. And within a month the larger count was found to be wrong and the first was correct.</p>
<p>
</p><p/><h2> A<del datetime="2019-04-24T13:55:30-04:00">n</del> <del datetime="2019-04-24T13:55:11-04:00">Un</del>resolved Claim </h2><p/>
<p/><p>
This is from the wonderful P vs NP <a href="https://www.win.tue.nl/~gwoegi/P-versus-NP.htm">pages</a> of Gerhard Woeginger. It was pointed out to us by the commenter <i>gentzen</i>. Quoting Woeginger’s page, including its use of “showed”:</p>
<blockquote><p><b> </b> <em> In February 2016, Mathias Hauptmann showed that P is not equal to NP. Hauptmann starts from the assumption that P equals <img alt="{\Sigma_{2}^{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Sigma_{2}^{p}}"/>, proves a new variant of the Union Theorem of McCreight and Meyer for <img alt="{\Sigma_{2}^{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Sigma_{2}^{p}}"/>, and eventually derives a contradiction. This implies P not equal to NP. </em>
</p></blockquote>
<p/><p>
Woeginger gives a link to Hauptmann’s <a href="http://arxiv.org/abs/1602.04781">paper</a>, “On Alternation and the Union Theorem,” and thanks two people who communicated this to him. </p>
<p>
The union <a href="https://people.csail.mit.edu/meyer/meyer-mccreight.pdf">theorem</a> of Albert Meyer and Edward McCreight is the classic theorem that shows how to encode many complexity classes into one. Hauptmann’s idea is not unreasonable. He makes an assumption that P=NP and tries to use it to improve the union theorem. This is a nice idea: Make a strong assumption and then try to improve a deep result. The hope is that this will lead to a contradiction. His abstract ends by saying, “Hence the assumption <img alt="{P = \Sigma_{2}^{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP+%3D+%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P = \Sigma_{2}^{p}}"/> cannot hold.” We do not know if this paper has received a thorough reading. <b>Update:</b> We have learned that a pair of experts reviewed the argument and found that part of it implied a contradiction to the deterministic time hierarchy theorem, while another part relativizes in a way that would yield a false statement under certain oracles.</p>
<p>
</p><p/><h2> A Resolved Claim </h2><p/>
<p/><p>
Hauptmann is a colleague of Norbert Blum at the University of Bonn. Two years ago, Blum claimed to prove P <img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/> NP by making technical improvements on a well-known circuit-based attack from the 1980s and 1990s. He has had a long track record of expertise and reliability in this area and his <a href="http://arxiv.org/trackback/1708.03486">paper</a> was read right away. </p>
<p>
The reading was helped by his paper being well-organized, straightforward, and relatively short—the crucial segment was under ten pages. The news broke while we were preparing a post on the August 2017 total solar eclipse in the US. In the 24–48 hours it took us to modify our <a href="https://rjlipton.wordpress.com/2017/08/17/on-the-edge-of-eclipses-and-pnp/">post</a>, we were already able to draw on several accounts by first-responder readers and check those accounts ourselves against the paper. </p>
<p>
The error was triangulated in an interesting way. It was first observed that if Blum’s attack could succeed by the means and premises stated, then it would extend to prove something else that is known not to be true. Once this was ascertained, a closer reading was able to zero in on the exact technical point of error. Blum soon acknowledged this and that the breach was unfixable. The attempt still combines circuit theory and graph theory in ways a student can benefit from learning about, and this furnished its own incentive to read it.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We appreciate the comments on the previous post and hope this adds some additional insights.</p>
<p>
[added update about Hauptmann’s paper]</p></font></font></div>
    </content>
    <updated>2019-04-24T14:32:25Z</updated>
    <published>2019-04-24T14:32:25Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="check proofs"/>
    <category term="conjecture"/>
    <category term="motivation"/>
    <category term="P&#x2260;NP"/>
    <category term="skin in game"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-02T23:35:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/04/23/euler-characteristics-nonmanifold</id>
    <link href="https://11011110.github.io/blog/2019/04/23/euler-characteristics-nonmanifold.html" rel="alternate" type="text/html"/>
    <title>Euler characteristics of non-manifold polycubes</title>
    <summary>From a block of cubes, remove two non-adjacent and non-opposite cubes. The resulting polycube has a boundary that is not a manifold: between the two removed cubes, there is an edge shared by four squares, but a two-dimensional manifold can only have two faces per edge. Nevertheless, we can compute its Euler characteristic as the number of vertices () minus the number of edges () plus the number of square faces (). , the same number we would expect for the Euler characteristic of a topological sphere! What does it mean?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>From a  block of cubes, remove two non-adjacent and non-opposite cubes. The resulting polycube has a boundary that is not a <a href="https://en.wikipedia.org/wiki/Manifold">manifold</a>: between the two removed cubes, there is an edge shared by four squares, but a two-dimensional manifold can only have two faces per edge. Nevertheless, we can compute its Euler characteristic as the number of vertices () minus the number of edges () plus the number of square faces (). , the same number we would expect for the Euler characteristic of a topological sphere! What does it mean?</p>

<p style="text-align: center;"><img alt="Removing two non-adjacent and non-opposite cubes from a 2x2 block of cubes" src="https://11011110.github.io/blog/assets/2019/nonmanifold-polycube.svg"/></p>

<p>Any finite union of cubes of the integer lattice (not even necessarily connected) has as its boundary a set of vertices, edges, and squares, with each edge incident to an even number of squares. We can define the Euler characteristic to be the number of vertices minus edges plus squares, in the usual way. But we can also compute it in a different, more intrinsic and topological, way. For any  in the range , define the “shrunken interior” of the polycube to be the set of points of the interior farther than  from the boundary, and define the “shrunken exterior” in the same way. Then the shrunken interior and shrunken exterior both have (possibly disconnected) 2-manifolds as boundaries. We can define their Euler characteristics in the standard way from any cell decomposition of these boundaries (it doesn’t matter which cell decomposition we choose). Then the Euler characteristic of the polycube is the average of the Euler characteristics of the shrunken interior and shrunken exterior!</p>

<p>In the case of the mutilated  block, the shrunken interior and shrunken exterior are both topological balls (ignoring the puncture at infinity as it doesn’t have a boundary), so the average of their Euler characteristics is the Euler characteristic of a sphere, as we calculated.</p>

<p>There’s probably a simpler and more conceptual way of doing it, but here’s an explanation for why the Euler characteristic of the polycube boundary is the average of the Euler characteristics of the interior and exterior. Form a cell complex on the boundary of the interior and exterior, together, in the following way: expand each square of the polycube boundary to a cuboid with thickness 0.1, expand each edge into a cylinder with diameter 0.2 (big enough to enclose all the intersections of two expanded squares), and expand each vertex into a sphere with diameter 0.3 (big enough to enclose all the intersections of two cylinders but small enough that no two of these spheres touch). Remove the union of these expanded shapes from the space, and consider what’s left. It has the same topology as the union of the shrunken interior and exterior, and its boundary is now naturally divided up into cells: offset squares patches on the sides of each expanded square face, cylindrical patches on each expanded edge, and spherical patches on each expanded vertex, with curves where two patches meet.</p>

<p>Let’s calculate the Euler characteristic of this cell complex. Each square of the polycube leads to two offset square patches, so the  squares contribute   to the Euler characteristic. Each edge  of the polycube might be adjacent to two or four squares; call this number . Then the cylinder around  includes  surface patches between pairs of squares and  curves connecting them to the square patches. The patches count  and the curves count  for a total contribution to the Euler characteristic of .</p>

<p>Finally, each vertex of the polycube becomes a sphere, subdivided by the patches and curves of the complex. These spheres also contain all the vertices of the complex. The Euler characteristic of a subdivided sphere would be , but the vertex spheres have some parts of their subdivision removed. Where each edge cylinder or expanded square comes into the sphere, a patch of surfaces is removed, and the curves between these removed patches are also removed. An edge  with degree  contributes to the removal of  curves and  patches (one for itself and  for each adjacent square). So if there are  vertices in the polycube, the  contribution from the Euler characteristics of the subdivided spheres is modified by subtracting  for each incident edge. The total modification at both endpoints of each edge is . The  that we calculated here is cancelled by the  on the cylinder for , and we are left with a total modification of  where  is the number of polycube edges.</p>

<p>Putting all the pieces of this calculation together, the complex we have constructed on the union of the shrunken interior and exterior has Euler characteristic . Therefore, the Euler characteristic of the polycube boundary itself, , equals the average of the characteristics of the interior and exterior. The same reasoning shows more generally that whenever you have a finite cell complex embedded into , dividing space up into chambers, the Euler characteristic of the complex equals half the sum of Euler characteristics of the manifolds bounding shrunken chambers.</p>

<p>Although Euler characteristics of 2-manifolds embedded without boundary in  are always even, this averaging method can produce non-manifold surfaces with odd Euler characteristic. For instance, consider mutilating the  block in a different way, by removing two opposite cubes. The interior and exterior of the resulting polycube are both connected, but the interior is a solid torus and the exterior is a ball. So the Euler characteristic of the polycube should be the average of the torus and sphere, . And if we actually calculate it we get .</p>

<p>As this example shows, it’s possible for a polycube to have different topologies of surface on the interior and exterior, and it’s also possible to have different numbers of surfaces: for instance, two cubes attached vertex-to-vertex produce two interior surfaces but only one exterior. For cell complexes in , there appears to be no restriction on which combinations of surfaces are possible. But for cell complexes in other spaces (other 3-manifolds than Euclidean space) it may be possible to embed 2-manifolds with odd Euler characteristic. When this happens, the number of odd chambers of a cell complex must always be even. For, the parity of the sum of the Euler characteristics of the chambers must be even, in order to be able to divide by two and get an integer as the Euler characteristic of the cell complex.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/101978143052398446">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-04-23T16:37:00Z</updated>
    <published>2019-04-23T16:37:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-05-01T06:43:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17325</id>
    <link href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/" rel="alternate" type="text/html"/>
    <title>An Invitation to a Conference: Visions in Mathematics towards 2000</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Let me invite you to a conference. The conference took place in 1999 but only recently the 57 videos of the lectures and the discussion sessions are publicly available. (I thank Vitali Milman for telling me about it.) One novel … <a href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Let me invite you to a conference. The conference took place in 1999 but only recently <a href="https://www.youtube.com/playlist?list=PLP0YToNcfAwLBd8yibTtjv3aHfcbT4GBA">the 57 videos of the lectures and the discussion sessions are publicly available.</a> (I thank Vitali Milman for telling me about it.) One novel idea of Vitali Milman was to hold discussion sessions and they were quite interesting. (But, I am biased, I like discussions.) I will invite you to one of the heated discussions in the next post. There were very many nice talks and very many nice visions. And it is fun to watch the videos and judge the ideas in the perspective of time.</p>
<p>The proceedings appeared as GAFA special volumes, but alas the articles are not electronically available even to GAFA’s subscribers. Let me encourage both Birkhauser and the contributors to make them more available. My talk was: <a href="https://youtu.be/Wjg1_QwjUos">An invitation to Tverberg’s theorem</a>, and my own contribution to the Proceedings <a href="http://www.ma.huji.ac.il/~kalai/VIS.pdf">Combinatorics with a Geometric Flavor </a>is probably the widest scope survey article I ever wrote.  At the end of each section I added a brief philosophical thought about mathematics and those are collected in the post “<a href="https://gilkalai.wordpress.com/2008/10/12/about-mathematics/">about mathematics</a>“.</p>
<p>Two more things: The conference (and few others organized by Vitali) was  in Tel Aviv with a few days at the dead see and this worked very nicely.  Vitali also organized in the mid 90s another very successful geometry conference unofficially celebrating Gromov’s 50th birthday with, among others, a very nice lecture by Gregory Perelman. If videos will become available I will be delighted to invite you to that conference as well. Update from Vitali: It was also the week of Jeff Cheeger’s 50th birthday which was also celebrated. Grisha Perelman gave an absolutely excellent talk  talk on works of Cheeger.  Lectures were not videotaped.</p>
<p/>
<p><span style="color: #ff0000;">Avi Wigderson’s lecture</span></p>
<p><span style="color: #ff0000;">Are so called “natural questions” good for mathematics. Specifically is Kepler’s questions about the densest packing of unit balls in 3-space interesting? Watch a discussion of Misha Gromov, Noga Alon, Laci Lovasz and others. (next post)</span></p>
<h2/>
<h2><a href="https://gilkalai.files.wordpress.com/2019/04/vis99-p1.png"><img alt="" class="alignnone size-full wp-image-17362" height="363" src="https://gilkalai.files.wordpress.com/2019/04/vis99-p1.png?w=640&amp;h=363" width="640"/></a></h2>
<p><span style="color: #ff0000;">We were all so much younger!  (And in that old millennium,  we were also all men <img alt="&#x1F626;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f626.png" style="height: 1em;"/> )</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/misha5.png"><img alt="" class="alignnone size-full wp-image-17366" height="614" src="https://gilkalai.files.wordpress.com/2019/04/misha5.png?w=640&amp;h=614" width="640"/></a></p>
<p><span style="color: #ff0000;">Misha Gromov argues passionately that natural problems are bad problems (see next post)</span></p>
<p>Pictures of most participants ad two slides are below</p>
<p><span id="more-17325"/><br/>
<a href="https://gilkalai.files.wordpress.com/2019/04/vim1.png"><img alt="VIM1.png" class="alignnone size-full wp-image-17372" src="https://gilkalai.files.wordpress.com/2019/04/vim1.png?w=640" style="font-size: 12px;"/></a><br/>
<a href="https://gilkalai.files.wordpress.com/2019/04/vim2.png"><img alt="VIM2.png" class="alignnone size-full wp-image-17373" src="https://gilkalai.files.wordpress.com/2019/04/vim2.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim3.png"><img alt="VIM3.png" class="alignnone size-full wp-image-17374" src="https://gilkalai.files.wordpress.com/2019/04/vim3.png?w=640"/></a></p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/vis.png"><img alt="" class="alignnone size-full wp-image-17387" height="373" src="https://gilkalai.files.wordpress.com/2019/04/vis.png?w=640&amp;h=373" width="640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim5.png"><img alt="VIM5.png" class="alignnone size-full wp-image-17376" src="https://gilkalai.files.wordpress.com/2019/04/vim5.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim6.png"><img alt="VIM6.png" class="alignnone size-full wp-image-17377" src="https://gilkalai.files.wordpress.com/2019/04/vim6.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim7.png"><img alt="VIM7.png" class="alignnone size-full wp-image-17378" src="https://gilkalai.files.wordpress.com/2019/04/vim7.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim4.png"><img alt="VIM4.png" class="alignnone size-full wp-image-17375" src="https://gilkalai.files.wordpress.com/2019/04/vim4.png?w=640"/></a></p></div>
    </content>
    <updated>2019-04-23T15:40:47Z</updated>
    <published>2019-04-23T15:40:47Z</published>
    <category term="Combinatorics"/>
    <category term="Conferences"/>
    <category term="What is Mathematics"/>
    <category term="Vitali Milman"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-05-02T23:35:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8139226761048070665</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8139226761048070665/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/quiz-show-scandalsadmissions.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8139226761048070665" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8139226761048070665" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/quiz-show-scandalsadmissions.html" rel="alternate" type="text/html"/>
    <title>Quiz Show Scandals/Admissions Scandal/Stormy Daniels/Beer names:being  a lawyer would drive me nuts!!!!!!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">0) Charles van Doren (see <a href="https://en.wikipedia.org/wiki/Charles_Van_Doren">here</a>) passed away recently. For those who don't know he he was (prob most of you) he was one of the contestants involved in RIGGED quiz shows in the 1950's.  While there was a Grand Jury Hearing about Quiz Shows being rigged, nobody went to jail since TV was new and it was not clear if rigging quiz shows was illegal. Laws were then passed to make them it illegal.<br/>
<br/>
So why are today's so-called reality shows legal? I ask non-rhetorically.<br/>
<br/>
(The person he beat in a rigged game show- Herb Stempel (see <a href="https://en.wikipedia.org/wiki/Herb_Stempel">here</a>) is still alive.)<br/>
<br/>
1) The college admissions scandal. I won't restate the details and how awful it is since you can get that elsewhere and I doubt I can add much to it.  One thing I've heard in the discussions about it is a question that is often posted rhetorically but I want to pose for real:<br/>
<br/>
There are people whose parents give X dollars to a school and they get admitted even though they are not qualified. Why is that legal?<br/>
<br/>
I ask that question without an ax to grind and without anger. Why is out-right bribery of this sort legal?<br/>
<br/>
Possibilities:<br/>
<br/>
a) Its transparent. So being honest about bribery makes it okay?<br/>
<br/>
b) My question said `even though they are not qualified' - what if they explicitly or implicitly said `having parents give money to our school is one of our qualifications'<br/>
<br/>
c) The money they give is used to fund scholarships for students who can't afford to go. This is an argument for why its not immoral, not why its not illegal.<br/>
<br/>
But here is my question: Really, what is the legal issue here? It still seems like bribery.<br/>
<br/>
2) Big Oil gives money to congressman Smith, who then votes against a carbon tax. This seems like outright bribery<br/>
<br/>
Caveat:<br/>
<br/>
a) If Congressman Smith is normally a anti-regulation then he could say correctly that he was given the money because they agree with his general philosophy, so it's  not bribery.<br/>
<br/>
b) If Congressman smith is normally pro-environment and has no problem with voting for taxes then perhaps it is bribery.<br/>
<br/>
3) John Edwards a while back and Donald Trump now are claiming (not quite) that the money used to pay off their mistress to be quiet is NOT a campaign contribution, but was to keep the affair from his wife. (I don't think Donald Trump has admitted the affair so its harder to know what his defense is). But lets take a less controversial example of `what is a campaign contribution'<br/>
<br/>
I throw a party for my wife's 50th birthday and I invite Beto O'Rourke and many voters and some Dem party big-wigs to the party. The party costs me $50,000.  While I claim it's for my wife's bday it really is for Beto to make connections to voters and others. So is that a campaign contribution?<br/>
<br/>
4) The creators of HUGE ASS BEER are suing GIANT ASS BEER for trademark infringement. I am not making this up- see <a href="https://thetakeout.com/huge-giant-ass-beer-lawsuit-new-orleans-1832989913">here</a><br/>
<br/>
---------------------------------------------------------<br/>
<br/>
All of these cases involve ill defined questions (e.g., `what is a bribe'). And the people arguing either side are not unbiased. The cases also illustrate why I prefer mathematics: nice clean questions that (for the most part) have answers. We may have our biases as to which way they go, but if it went the other way we would not sue in a court of law.</div>
    </content>
    <updated>2019-04-23T03:24:00Z</updated>
    <published>2019-04-23T03:24:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-02T14:13:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4233</id>
    <link href="https://lucatrevisan.wordpress.com/2019/04/22/the-more-things-change-the-more-they-stay-the-same/" rel="alternate" type="text/html"/>
    <title>The more things change the more they stay the same</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">From a 1981 (!!) New York Times Article titled “Changing San Francisco is foreseen as a haven for wealthy and childless”: A major reason for the exodus of the middle class from San Francisco, demographers say, is the high cost … <a href="https://lucatrevisan.wordpress.com/2019/04/22/the-more-things-change-the-more-they-stay-the-same/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>From a 1981 (!!) <a href="https://www.nytimes.com/1981/06/09/us/changing-san-francisco-is-foreseen-as-a-haven-for-wealthy-and-childless.html">New York Times Article</a> titled “Changing San Francisco is foreseen as a haven for wealthy and childless”:</p>
<blockquote><p>
A major reason for the exodus of the middle class from San Francisco, demographers say, is the high cost of housing, the highest in the mainland United States. Last month, the median cost of a dwelling in the San Francisco Standard Metropolitan Statistical Area was $129,000, according to the Federal Home Loan Bank Board in Washington, D.C. The comparable figure for New York, Newark and Jersey City was $90,400, and for Los Angeles, the second most expensive city, $118,400.</p>
<p>”This city dwarfs anything I’ve ever seen in terms of housing prices,” said Mr. Witte. Among factors contributing to high housing cost, according to Mr. Witte and others, is its relative scarcity, since the number of housing units has not grown significantly in a decade; the influx of Asians, whose first priority is usually to buy a home; the high incidence of adults with good incomes and no children, particularly homosexuals who pool their incomes to buy homes, and the desirability of San Francisco as a place to live.
</p></blockquote>
<p>$129,000 in 1981 dollars is $360,748 in 2019 dollars.</p></div>
    </content>
    <updated>2019-04-23T01:52:32Z</updated>
    <published>2019-04-23T01:52:32Z</published>
    <category term="history"/>
    <category term="San Francisco"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-02T23:20:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4230</id>
    <link href="https://lucatrevisan.wordpress.com/2019/04/22/online-optimization-post-0-definitions/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 0: Definitions</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Online convex optimization deals with the following setup: we want to design an algorithm that, at each discrete time step , comes up with a solution , where is a certain convex set of feasible solution. After the algorithm has … <a href="https://lucatrevisan.wordpress.com/2019/04/22/online-optimization-post-0-definitions/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 Online convex optimization deals with the following setup: we want to design an algorithm that, at each discrete time step <img alt="{t=1,2,\ldots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C2%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1,2,\ldots}"/>, comes up with a solution <img alt="{x_t \in K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%5Cin+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t \in K}"/>, where <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is a certain convex set of feasible solution. After the algorithm has selected its solution <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/>, a convex cost function <img alt="{f_t : K \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t : K \rightarrow {\mathbb R}}"/>, coming from a known restricted set of admissible cost functions <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/>, is revealed, and the algorithm pays the loss <img alt="{f_t (x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t (x_t)}"/>. </p>
<p>
Again, the algorithm has to come up with a solution <em>without knowing what cost functions it is supposed to be optimizing</em>. Furthermore, we will think of the sequence of cost functions <img alt="{f_1,f_2, \ldots,f_t,\ldots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2Cf_2%2C+%5Cldots%2Cf_t%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,f_2, \ldots,f_t,\ldots}"/> not as being fixed in advanced and unknown to the algorithm, but as being dynamically generated by an adversary, after seeing the solutions provided by the algorithm. (This resilience to adaptive adversaries will be important in most of the applications.)</p>
<p>
The <em>offline optimum</em> after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps is the total cost that the best possible fixed solution would have incurred when evaluated against the cost functions seen by the algorithm, that is, it is a solution to </p>
<p align="center"><img alt="\displaystyle  \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) "/></p>
<p>
The <em>regret</em> after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps is the difference between the loss suffered by the algorithm and the offline optimum, that is, </p>
<p/><p align="center"><img alt="\displaystyle  {\rm Regret}_T = \sum_{t=1}^T f_t (x_t) - \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3D+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x_t%29+-+%5Cmin_%7Bx%5Cin+K%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+f_t+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T = \sum_{t=1}^T f_t (x_t) - \min_{x\in K} \ \ \sum_{t=1}^T f_t (x) "/></p>
<p>
The remarkable results that we will review give algorithms that achieve regret</p>
<p/><p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq O_{K, {\cal F}} (\sqrt T) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+O_%7BK%2C+%7B%5Ccal+F%7D%7D+%28%5Csqrt+T%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq O_{K, {\cal F}} (\sqrt T) "/></p>
<p> that is, for fixed <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> and <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/>, the regret-per-time-step goes to zero with the number of steps, as <img alt="{O\left( \frac 1 {\sqrt T} \right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%5Cleft%28+%5Cfrac+1+%7B%5Csqrt+T%7D+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O\left( \frac 1 {\sqrt T} \right)}"/>. It is intuitive that our bounds will have to depend on how big is the “diameter” of <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> and how large is the “magnitude” and “smoothness” of the functions <img alt="{f\in {\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\in {\cal F}}"/>, but depending on how we choose to formalize these quantities we will be led to define different algorithms. </p>
<p/></div>
    </content>
    <updated>2019-04-22T21:35:44Z</updated>
    <published>2019-04-22T21:35:44Z</published>
    <category term="theory"/>
    <category term="online optimization"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-02T23:20:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15778</id>
    <link href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/" rel="alternate" type="text/html"/>
    <title>P=NP Proofs</title>
    <summary>Advice to claimers The Claimers The Claimers are a gang on the hit AMC television series The Walking Dead. They are the main antagonists in the second half of the zombie-apocalypse show’s Season 4. According to Wikipedia’s description, they “live by the philosophy of ‘claiming’.” Today Ken and I discuss issues about ‘claiming’ and give […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Advice to claimers</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/claimers3/" rel="attachment wp-att-15790"><img alt="" class="alignright size-medium wp-image-15790" height="191" src="https://rjlipton.files.wordpress.com/2019/04/claimers3.png?w=300&amp;h=191" width="300"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://the-walking-dead-tvseries.fandom.com/wiki/The_Claimers">The Claimers</a></font></td>
</tr>
</tbody>
</table>
<p>
The Claimers are a gang on the hit AMC television <a href="https://en.wikipedia.org/wiki/The_Walking_Dead_(TV_series)">series</a> <em>The Walking Dead</em>. They are the main antagonists in the second half of the zombie-apocalypse show’s Season 4. According to Wikipedia’s <a href="https://en.wikipedia.org/wiki/The_Walking_Dead_(season_4)#The_Claimers">description</a>, they “live by the philosophy of ‘claiming’.”</p>
<p>
Today Ken and I discuss issues about ‘claiming’ and give advice on how to present your claims—or not.</p>
<p>
Yes, this post is about our own “claimers” in complexity theory. It is especially about those who claim to have a solution to P=NP. We will not give any names today. You know who you are.</p>
<p>
The TV Claimers meet a grisly end. We will not say any more about it. We want to be nice. But we would like to not keep seeing the same level of zombie claims raised again and again.</p>
<p>
<b>Please: Do not stop reading.</b> Yes we know that it is likely that no claimer really has such a proof. However, our suggestions apply to all of us when we have a non-trivial result. Especially a result that has been open, even if the result is not a major open problem. So please keep reading today.</p>
<p>
</p><p/><h2> So You Can Prove P=NP </h2><p/>
<p/><p>
This is a list of ideas for anyone who claims to have solved P=NP or some similar hard open problem in mathematics. There are already lots of suggestions online about what you should do, so this is just a list of additional thoughts. We hope they are helpful.</p>
<p>
</p><p/><h3> You are being pretty arrogant </h3><p/>
<p/><p>
In order to succeed in mathematics research one has to be a bit arrogant. It is quite difficult to prove new things without some swagger. However, proving or resolving P=NP requires a very non-humble attitude. I think many claimers have not thought how arrogant they are being. The P=NP problem is a huge open problem. Thousands and thousands of researchers have spent years thinking about it. Why do you, the claimer, think you see the light and we remain in the dark?</p>
<p>
It might be useful for the claimers to ponder: <i>Why did I succeed where all others have failed?</i> It might be useful to be a bit humble and at least think what did they see that we all missed? If they can say something like:</p>
<blockquote><p><b> </b> <em> The reason I succeeded in finding an algorithm for P=NP is that I noticed that <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\dots}"/> No one else seems to see that this insight is very powerful. It is very useful since it implies <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\dots}"/> </em>
</p></blockquote>
<p>
</p><p/><h3> Working alone </h3><p/>
<p/><p>
I think the vast majority of claimers of P=NP or other big results have almost always worked alone. This is okay, but the average number of authors these days of a theory paper is pretty large. So any paper that is sole authored, perhaps, leads the community think it is unusual—and is wrong. Another point is that being part of a team may help control the arrogance. It can also be invaluable in detecting errors. </p>
<p>
</p><p/><h3> Show them the money </h3><p/>
<p/><p>
There is an advantage in “proving” P=NP over other major problems. There is the Clay prize of a million dollars. I wonder if claimers could use the prize money in some interesting way. How about saying: If you read the my proof and repair it or make it more readable and it is correct, then you get something. A certain dollar amount. Or a percentage of the prize. Or—you get the idea.</p>
<p>
</p><p/><h3> The role of code </h3><p/>
<p/><p>
Many claimers have also supplied working code for their algorithm. That is they also supply a program that claims to solve some NP-complete problem. I have several thoughts about this. In some cases it seems that it could be possible to have code that works for small size problems, but not in the general case. This seems to be possible for the claims by some that they can solve the Traveling Salesman Problem, for example. Their algorithm could be correct for small instances.</p>
<p>
Mathematics is filled with surprises like: This effect works for all values of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> less than some bound. If the claimers give a program, our expectation based on experience is that it may work for small cases but will probably fail in general. </p>
<p>
The last point is that working code could actually be valuable. If the code can be used to solve SAT problems how about using your program to enter a SAT contest and win it. A win, or even a good showing, would help tremendously in convincing people to read the paper. Or use the code to break some known cryptosystem. That would also convince people that they need to read your paper.</p>
<p>
</p><p/><h2> Writing Your Paper </h2><p/>
<p/><p>
Okay we all dream about solving a major open problem. Or even a minor one. Here we give an outline of how to write up such a paper. </p>
<p/><h3> How to write up the proof </h3><p/>
<p>
I would suggest that you not have any statements about why P=NP is an important problem. None. No history of the problem. No literature survey is needed. None. You goal is to get an expert to read and believe the proof. They will just skip over the above. Also please no statements of how your algorithm that solves P=NP is going to change the world. Just give us the proof.</p>
<p>
</p><p/><h3> How to get them to read the proof </h3><p/>
<p/><p>
This is really hard. Hard. I have read a number of claimers’ papers. I try to be helpful. However, many of us do not have the time to look at such papers. Years ago, before Fermat’s Last Theorem was solved, a famous mathematician once made up a post-card that looked like this:</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/postcard/" rel="attachment wp-att-15784"><img alt="" class="aligncenter size-medium wp-image-15784" height="221" src="https://rjlipton.files.wordpress.com/2019/04/postcard.png?w=300&amp;h=221" width="300"/></a></p>
<p>
I think that we all have a mental version of this card. There are definitely ways to help induce someone to read a paper and its proof. Look at some recent top theory papers. Even the authors of these papers, often well known authors, work hard to motivate potential readers. The authors often do several things:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>They often sketch the proof.</em> By leaving out details they may help get a reader interested. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>They often explain the new trick—or tricks.</em> The goal here is to explain some new insight that is used in the proof. We are very self-oriented: If I see that your new trick could be useful in my research that is a huge motivator for me to understand the proof. People are very excited about a strong result, but they are even more excited about a new trick. Explain what is new in your proof. If there is nothing new, no new trick or method, then hmmmm<img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/></p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>They often first prove a weaker result.</em> That is, they show that their method can already make progress. If you could prove that the zeta function has all its nontrivial zeros on the critical line, that would be apocalyptic—the famous Riemann Hypothesis, of course. But if you could merely prove that there is no zero in some new region, then that would still be <em>wonderful</em>. And also probably more believable. If you could prove that there is no zero <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> with its real part <img alt="{0.99999}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.99999%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.99999}"/> that would be huge. If the proof of this is simpler, then use it to get readers excited about your full result. </p>
<p>
An observation related to the last point is: </p>
<blockquote><p><b> </b> <em> <i>Why do all claims of progress on P=NP give a polynomial time bound?</i> </em>
</p></blockquote>
<p>How about just getting a better bound of say <img alt="{2^{n/10}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%2F10%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{n/10}}"/> for the Traveling Salesman Problem? Or a better bound for factoring? Or a better bound for your favorite problem?</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>I hope these points help. One last pointer is to double-check dependencies. If your proof relies on a result by someone else, make sure the result really gives what you need. Terms may be defined differently from what you expect, or you may really need a feature of the proof rather than the mere statement. A mis-attributed result can become “undead.”</p></font></font></div>
    </content>
    <updated>2019-04-22T03:25:39Z</updated>
    <published>2019-04-22T03:25:39Z</published>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="claimed proofs"/>
    <category term="claims"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-02T23:35:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/062</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/062" rel="alternate" type="text/html"/>
    <title>TR19-062 |  Quantum Lower Bounds for Approximate Counting via Laurent Polynomials | 

	Scott Aaronson, 

	Robin Kothari, 

	William Kretschmer, 

	Justin Thaler</title>
    <summary>This paper proves new limitations on the power of quantum computers to solve approximate counting---that is, multiplicatively estimating the size of a nonempty set $S\subseteq [N]$.

Given only a membership oracle for $S$, it is well known that approximate counting takes $\Theta(\sqrt{N/|S|})$ quantum queries. But what if a quantum algorithm is also given "QSamples"---i.e., copies of the state $|S\rangle = \sum_{i\in S}|i\rangle$---or even the ability to apply reflections about $|S\rangle$? Our first main result is that, even then, the algorithm needs either $\Theta(\sqrt{N/|S|})$ queries or else $\Theta(\min\{|S|^{1/3},\sqrt{N/|S|}\})$ reflections or samples. We also give matching upper bounds.

We prove the lower bound using a novel generalization of the polynomial method of Beals et al. to Laurent polynomials, which can have negative exponents. We lower-bound Laurent polynomial degree using two methods: a new "explosion argument" and a new formulation of the dual polynomials method.

Our second main result rules out the possibility of a black-box Quantum Merlin-Arthur (or QMA) protocol for proving that a set is large. We show that, even if Arthur can make $T$ quantum queries to the set $S$, and also receives an $m$-qubit quantum witness from Merlin in support of $S$ being large, we have $Tm=\Omega(\min\{|S|,\sqrt{N/|S|}\})$. This resolves the open problem of giving an oracle separation between SBP and QMA.

Note that QMA is "stronger" than the queries+QSamples model in that Merlin's witness can be anything, rather than just the specific state $|S\rangle$, but also "weaker" in that Merlin's witness cannot be trusted. Intriguingly, Laurent polynomials also play a crucial role in our QMA lower bound, but in a completely different manner than in the queries+QSamples lower bound. This suggests that the "Laurent polynomial method" might be broadly useful in complexity theory.</summary>
    <updated>2019-04-21T13:57:29Z</updated>
    <published>2019-04-21T13:57:29Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-02T23:35:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/061</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/061" rel="alternate" type="text/html"/>
    <title>TR19-061 |  A Quantum Query Complexity Trichotomy for Regular Languages | 

	Daniel Grier, 

	Luke Schaeffer, 

	Scott Aaronson</title>
    <summary>We present a trichotomy theorem for the quantum query complexity of regular languages. Every regular language has quantum query complexity $\Theta(1)$, $\tilde{\Theta}(\sqrt n)$, or $\Theta(n)$. The extreme uniformity of regular languages prevents them from taking any other asymptotic complexity. This is in contrast to even the context-free languages, which we show can have query complexity $\Theta(n^c)$ for all computable $c \in [1/2,1]$. Our result implies an equivalent trichotomy for the approximate degree of regular languages, and a dichotomy---either $\Theta(1)$ or $\Theta(n)$---for sensitivity, block sensitivity, certificate complexity, deterministic query complexity, and randomized query complexity.

The heart of the classification theorem is an explicit quantum algorithm which decides membership in any star-free language in $\tilde{O}(\sqrt n)$ time. This well-studied family of the regular languages admits many interesting characterizations, for instance, as those languages expressible as sentences in first-order logic over the natural numbers with the less-than relation. Therefore, not only do the star-free languages capture functions such as OR, they can also express functions such as ``there exist a pair of 2's such that everything between them is a 0."  

Thus, we view the algorithm for star-free languages as a nontrivial generalization of Grover's algorithm which extends the quantum quadratic speedup to a much wider range of string-processing algorithms than was previously known.  We show a variety of applications---new quantum algorithms for dynamic constant-depth Boolean formulas, balanced parentheses nested constantly many levels deep, binary addition, a restricted word break problem, and path-discovery in narrow grids---all obtained as immediate consequences of our classification theorem.</summary>
    <updated>2019-04-21T13:56:47Z</updated>
    <published>2019-04-21T13:56:47Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-02T23:35:24Z</updated>
    </source>
  </entry>
</feed>
