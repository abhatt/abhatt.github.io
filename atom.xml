<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-05-20T03:22:24Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08406</id>
    <link href="http://arxiv.org/abs/2105.08406" rel="alternate" type="text/html"/>
    <title>A SAT attack on higher dimensional Erd\H{o}s--Szekeres numbers</title>
    <feedworld_mtime>1621468800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scheucher:Manfred.html">Manfred Scheucher</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08406">PDF</a><br/><b>Abstract: </b>A famous result by Erd\H{o}s and Szekeres (1935) asserts that, for every $k,d
\in \mathbb{N}$, there is a smallest integer $n = g^{(d)}(k)$, such that every
set of at least $n$ points in $\mathbb{R}^d$ in general position contains a
$k$-gon, i.e., a subset of $k$ points which is in convex position. We present a
SAT model for higher dimensional point sets which is based on chirotopes, and
use modern SAT solvers to investigate Erd\H{o}s--Szekeres numbers in dimensions
$d=3,4,5$. We show $g^{(3)}(7) \le 13$, $g^{(4)}(8) \le 13$, and $g^{(5)}(9)
\le 13$, which are the first improvements for decades. For the setting of
$k$-holes (i.e., $k$-gons with no other points in the convex hull), where
$h^{(d)}(k)$ denotes the minimum number $n$ such that every set of at least $n$
points in $\mathbb{R}^d$ in general position contains a $k$-hole, we show
$h^{(3)}(7) \le 14$, $h^{(4)}(8) \le 13$, and $h^{(5)}(9) \le 13$. Moreover,
all obtained bounds are sharp in the setting of chirotopes and we conjecture
them to be sharp also in the original setting of point sets.
</p></div>
    </summary>
    <updated>2021-05-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1883</id>
    <link href="https://theorydish.blog/2021/05/19/entropy-estimation-via-two-chains-streamlining-the-proof-of-the-sunflower-lemma/" rel="alternate" type="text/html"/>
    <title>Entropy Estimation via Two Chains: Streamlining the Proof of the Sunflower Lemma</title>
    <summary>The sunflower lemma describes an interesting combinatorial property of set families: any large family of small sets must contain a large sunflower—a sub-family consisting of sets with a shared core and disjoint petals  (See the figure below for an example and a non-example of sunflowers.) The application of the lemma in theoretical computer science dates back to the influential paper by Razborov in 1985 that established the first super-polynomial monotone circuit lower bound for a function in NP, and since then the lemma has been applied broadly to other problems in theoretical computer science (see STOC version of Alweiss-Lovett-Wu-Zhang for a discussion of applications of the lemma in computer science). Left: 3 sets forming a sunflower. Right: 3 sets NOT forming a sunflower. The lemma was first proved by Erdős-Rado in 1960, who gave the quantitative bound that among distinct sets each of size at most one can find sets forming a sunflower. Erdos-Rado conjectured that the in the bound could be significantly improved to For nearly 60 years since the Erdős-Rado upper bound, all known upper bounds had had the dependence on even for despite much research effort. In 2019, a breakthrough work by Alweiss-Lovett-Wu-Zhang improved the bound significantly to [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://en.wikipedia.org/wiki/Sunflower_(mathematics)">sunflower lemma</a> describes an interesting combinatorial property of set families: any large family of small sets must contain a large <em>sunflower</em>—a sub-family consisting of sets <img alt="A_1,\ldots,A_r" class="latex" src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with a shared <em>core</em> <img alt="S:=A_1\cap\cdots\cap A_r" class="latex" src="https://s0.wp.com/latex.php?latex=S%3A%3DA_1%5Ccap%5Ccdots%5Ccap+A_r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and disjoint <em>petals</em> <img alt="A_1\backslash S,\ldots,A_r\backslash S." class="latex" src="https://s0.wp.com/latex.php?latex=A_1%5Cbackslash+S%2C%5Cldots%2CA_r%5Cbackslash+S.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (See the <a href="https://theorydish.blog/feed/#fig-sunflower">figure</a> below for an example and a non-example of sunflowers.) The application of the lemma in theoretical computer science dates back to the influential <a href="http://people.cs.uchicago.edu/~razborov/files/clique.pdf">paper</a> by Razborov in 1985 that established the first super-polynomial monotone circuit lower bound for a function in NP, and since then the lemma has been applied broadly to other problems in theoretical computer science (see <a href="https://dl.acm.org/doi/abs/10.1145/3357713.3384234">STOC version</a> of Alweiss-Lovett-Wu-Zhang for a discussion of applications of the lemma in computer science).</p>



<div class="wp-block-image" id="fig-sunflower"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-2195" height="207" src="https://theorydish.files.wordpress.com/2021/04/sunflower_example-1.png" width="474"/>Left: 3 sets forming a sunflower. Right: 3 sets NOT forming a sunflower.</figure></div>



<p>The lemma was first proved by <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=111692">Erdős-Rado</a> in 1960, who gave the quantitative bound that among <img alt="(r-1)^kk!+1" class="latex" src="https://s0.wp.com/latex.php?latex=%28r-1%29%5Ekk%21%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> distinct sets each of size at most <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> one can find <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> sets forming a sunflower. Erdos-Rado conjectured that the <img alt="k!=k^{k(1 - o(1))} = k^{\Omega(k)}" class="latex" src="https://s0.wp.com/latex.php?latex=k%21%3Dk%5E%7Bk%281+-+o%281%29%29%7D+%3D+k%5E%7B%5COmega%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the bound could be significantly improved to <img alt="O(1)^k." class="latex" src="https://s0.wp.com/latex.php?latex=O%281%29%5Ek.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> For nearly 60 years since the Erdős-Rado upper bound, all known upper bounds had had the <img alt="k^{\Omega(k)}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7B%5COmega%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> dependence on <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> even for <img alt="r = 3" class="latex" src="https://s0.wp.com/latex.php?latex=r+%3D+3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> despite much research effort. In 2019, a breakthrough work by <a href="https://arxiv.org/abs/1908.08483">Alweiss-Lovett-Wu-Zhang</a> improved the bound significantly to <img alt="(Cr^3\log k\log\log_2 k)^k" class="latex" src="https://s0.wp.com/latex.php?latex=%28Cr%5E3%5Clog+k%5Clog%5Clog_2+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> being at least <img alt="3" class="latex" src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> being an absolute constant. The breakthrough led to many new results including improved monotone circuit lower bounds by <a href="https://arxiv.org/abs/2012.03883">Cavalar-Kumar-Rossman</a>.</p>



<p>Since the breakthrough of Alweiss-Lovett-Wu-Zhang, researchers have been refining the bound and simplifying the proof. The current best bound is <img alt="(Cr\log k)^k" class="latex" src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="k \ge 2" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cge+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> proved by Bell-Chueluecha-Warnke via a minor but powerful twist in the proof of an earlier <img alt="(Cr\log (rk))^k" class="latex" src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+%28rk%29%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <a href="https://arxiv.org/abs/1909.04774">bound</a> by Anup Rao:</p>



<p id="thm-1"><strong>Theorem 1</strong> (Sunflower Lemma <a href="https://arxiv.org/abs/2009.09327">[BCW’20]</a>).<em> There exists a constant <img alt="C&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=C%3E0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that the following holds for all positive integers <img alt="k\ge 2" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Cge+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="r." class="latex" src="https://s0.wp.com/latex.php?latex=r.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Let <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a family of at least <img alt="(Cr\log k)^k" class="latex" src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> distinct sets each of size at most <img alt="k." class="latex" src="https://s0.wp.com/latex.php?latex=k.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Then <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> contains <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> sets that form a sunflower.</em></p>



<p>The aim of this blog post is to present a streamlined proof of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a>. The proof is largely based on a <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a> by Terence Tao where he presented an elegant proof of Rao’s result using <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a>. However, Tao’s proof included a trick of <em>passing to a conditional copy twice</em>, which Tao described as <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/\#comment-579138">“somewhat magical”</a>. <strong>We show here that the trick is not necessary for the proof, and avoiding the trick gives a simpler proof with a slightly better constant in the bound.</strong></p>



<p>We start by defining <img alt="R" class="latex" src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-spread families, a notion key to all recent proofs of the sunflower lemma. We use <img alt="[N]" class="latex" src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as a shorthand for <img alt="\{1,\ldots,N\}." class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C%5Cldots%2CN%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> We use boldface symbols to denote random variables, and non-boldface ones to denote deterministic quantities.</p>



<p><strong>Definition 1</strong> (Spread family). <em>Let <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a family of finite sets <img alt="A_1,\ldots,A_N" class="latex" src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that are not necessarily distinct. For <img alt="R &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=R+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we say <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="R" class="latex" src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-spread if for all <img alt="S\subseteq \cup_{n=1}^NA_n," class="latex" src="https://s0.wp.com/latex.php?latex=S%5Csubseteq+%5Ccup_%7Bn%3D1%7D%5ENA_n%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p class="has-text-align-center"><em><img alt="\begin{aligned} \Pr[S\subseteq A_{\mathbf n}] \le R^{-|S|},\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5CPr%5BS%5Csubseteq+A_%7B%5Cmathbf+n%7D%5D+%5Cle+R%5E%7B-%7CS%7C%7D%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p><em>where <img alt="\mathbf n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is chosen uniformly at random from <img alt="[N]." class="latex" src="https://s0.wp.com/latex.php?latex=%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p>The main technical part of recent improvements in the sunflower lemma happens in the proof of the following refinement lemma. We use the base-<img alt="2" class="latex" src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> logarithm throughout.</p>



<p id="lm-2"><strong>Lemma 2</strong> (Refinement). <em>Let <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a finite set. <em>For <img alt="R &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=R+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em>, let <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be an <img alt="R" class="latex" src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-spread family of sets <img alt="A_1,\ldots,A_N\subseteq X." class="latex" src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N%5Csubseteq+X.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em> <em>Let <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a size-<img alt="\delta|X|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> subset of <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> chosen uniformly at random for <img alt="\delta\in(1/R,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%5Cin%281%2FR%2C1%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> assuming <img alt="\delta|X|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer. Let <img alt="\mathbf n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a uniform random number in <img alt="[N]" class="latex" src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> independent of <img alt="\mathbf W." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> There is a random number <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in <img alt="[N]" class="latex" src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (being not independent from <img alt="(\mathbf n,\mathbf W)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+n%2C%5Cmathbf+W%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in general) such that <img alt="A_{\mathbf n'}\subseteq A_{\mathbf n}\cup\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7B%5Cmathbf+n%27%7D%5Csubseteq+A_%7B%5Cmathbf+n%7D%5Ccup%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely and </em></p>



<p class="has-text-align-center"><em><img alt="\begin{aligned}\mathbb E|A_{\mathbf n'}\backslash \mathbf W|\le\frac{4}{\log(R\delta)}\mathbb E|A_{\mathbf n}|.\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmathbb+E%7CA_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%7C%5Cle%5Cfrac%7B4%7D%7B%5Clog%28R%5Cdelta%29%7D%5Cmathbb+E%7CA_%7B%5Cmathbf+n%7D%7C.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p>The proof of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> using Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> can be found in Rao’s <a href="https://arxiv.org/abs/1909.04774">paper</a> and Tao’s <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a>, so we omit it here and focus on proving Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a>. Rao and Tao both proved a slightly weaker version of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> but this weakness can be overcome using a minor twist observed by <a href="https://arxiv.org/abs/2009.09327">Bell-Chueluecha-Warnke</a>. They also used slightly different forms of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> but the differences are non-essential.</p>



<p>It is easy to see that in Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> one can convert <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to a deterministic function of <img alt="\mathbf n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> without violating any property of the original <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> guaranteed by the lemma, because after conditioning on <img alt="\mathbf n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> one can fix any additional randomness in <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> so that <img alt="|A_{\mathbf n'}\backslash \mathbf W|" class="latex" src="https://s0.wp.com/latex.php?latex=%7CA_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is minimized. This more deterministic version of <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can be somewhat more convenient for proving Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> and it was used in Rao’s proof, but allowing additional randomness in <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> enabled Tao to construct <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> explicitly and obtain a simpler proof of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a>.</p>



<p>We follow Tao’s idea of proving Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> using Shannon entropy, but we present the proof in a more streamlined fashion with a slightly sharper constant in the bound (Tao proved a version of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> where the constant 4 was replaced by 5). Specifically, we present the proof in a way resembling a basic technique in combinatorics called <em><a href="https://en.wikipedia.org/wiki/Double_counting_(proof_technique)">counting in two ways</a></em>: one can show that two quantities are equal by showing that they both count the number of elements in the same set. Here, we estimate the entropy of the same collection of random variables in two different ways, and prove Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> by comparing the two estimates. The way we obtain the two estimates relies crucially on the chain rule of conditional entropy:</p>



<p class="has-text-align-center" id="eq-1"><img alt="\begin{aligned} \mathbb H(\mathbf a_1,\ldots,\mathbf a_m) = \sum_{i=1}^m\mathbb H(\mathbf a_i|\mathbf a_1,\ldots,\mathbf a_{i-1}). &amp;&amp;  (1) \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+H%28%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m%29+%3D+%5Csum_%7Bi%3D1%7D%5Em%5Cmathbb+H%28%5Cmathbf+a_i%7C%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_%7Bi-1%7D%29.+%26%26++%281%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Equation <a href="https://theorydish.blog/feed/#eq-1">(1)</a> holds for arbitrary random variables <img alt="\mathbf a_1,\ldots,\mathbf a_m" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> taking values in a discret set. We say that equation <a href="https://theorydish.blog/feed/#eq-1">(1)</a> computes the entropy of <img alt="(\mathbf a_1,\ldots,\mathbf a_m)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> via the chain </p>



<p class="has-text-align-center"><img alt="\begin{aligned}\mathbf a_1\rightarrow \cdots \rightarrow \mathbf a_m.\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmathbf+a_1%5Crightarrow+%5Ccdots+%5Crightarrow+%5Cmathbf+a_m.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>To prove Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a>, we obtain two entropy estimates for the same collection of random variables by applying <a href="https://theorydish.blog/feed/#eq-1">(1)</a> to two different chains.</p>



<p>We need the following useful lemmas about Shannon entropy. We omit their proofs here as they can be found in Tao’s <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a>, where many basic properties of Shannon entropy are also discussed. (We also highly recommend Tao’s other blog posts about <a href="https://terrytao.wordpress.com/2017/03/01/special-cases-of-shannon-entropy/">Shannon entropy</a> and the <a href="https://terrytao.wordpress.com/2009/08/05/mosers-entropy-compression-argument/">entropy compression argument</a>.)</p>



<p id="lm-3"><strong>Lemma 3</strong> (Subsets of small sets have small conditional entropy). <em>Let <img alt="\mathbf A,\mathbf B" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%2C%5Cmathbf+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be finite random sets. Assume <img alt="\mathbf A\subseteq \mathbf B" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%5Csubseteq+%5Cmathbf+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely. Then <img alt="\mathbb H(\mathbf A|\mathbf B)\le \mathbb E|\mathbf B|." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+A%7C%5Cmathbf+B%29%5Cle+%5Cmathbb+E%7C%5Cmathbf+B%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p id="lm-4"><strong>Lemma 4</strong> (Information-theoretic interpretation of spread). <em>Let <img alt="A_1,\ldots,A_N" class="latex" src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be an <img alt="R" class="latex" src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-spread family of finite sets. Let <img alt="\mathbf n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be chosen uniformly at random from <img alt="[N]." class="latex" src="https://s0.wp.com/latex.php?latex=%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Let <img alt="\mathbf S" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+S&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a random set satisfying <img alt="\mathbf S\subseteq A_{\mathbf n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+S%5Csubseteq+A_%7B%5Cmathbf+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely. Then</em></p>



<p class="has-text-align-center"><em><img alt="\begin{aligned} \mathbb H(\mathbf n|\mathbf S) \le \mathbb H(\mathbf n) - (\log R) \mathbb E|\mathbf S|.\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+H%28%5Cmathbf+n%7C%5Cmathbf+S%29+%5Cle+%5Cmathbb+H%28%5Cmathbf+n%29+-+%28%5Clog+R%29+%5Cmathbb+E%7C%5Cmathbf+S%7C.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p id="lm-5"><strong>Lemma 5</strong> (Information-theoretic properties of uniformly random subsets of fixed size). <em>Let <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a finite set. Let <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a size-<img alt="\delta |X|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> subset of <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> chosen uniformly at random for <img alt="\delta\in (0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%5Cin+%280%2C1%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> assuming <img alt="\delta |X|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer. Let <img alt="\mathbf A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a random subset of <img alt="X." class="latex" src="https://s0.wp.com/latex.php?latex=X.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> </em>The following inequalities hold:</p>



<ol><li><em>(absorption) <img alt="\mathbb H(\mathbf A\cup \mathbf W) \le \mathbb H(\mathbf W) + 1 + (1 + \log(1/\delta))\mathbb E|\mathbf A|;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+A%5Ccup+%5Cmathbf+W%29+%5Cle+%5Cmathbb+H%28%5Cmathbf+W%29+%2B+1+%2B+%281+%2B+%5Clog%281%2F%5Cdelta%29%29%5Cmathbb+E%7C%5Cmathbf+A%7C%3B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></li><li><em>(spread) if <img alt="\mathbf A\subseteq \mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%5Csubseteq+%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely, then <img alt="\mathbb H(\mathbf W|\mathbf A)\le \mathbb H(\mathbf W) - \log(1/\delta)\mathbb E|\mathbf A|." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+W%7C%5Cmathbf+A%29%5Cle+%5Cmathbb+H%28%5Cmathbf+W%29+-+%5Clog%281%2F%5Cdelta%29%5Cmathbb+E%7C%5Cmathbf+A%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></li></ol>



<p><em>Proof of Lemma </em><a href="https://theorydish.blog/feed/#lm-2">2</a>. If there exists <img alt="n\in[N]" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Cin%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that <img alt="A_n" class="latex" src="https://s0.wp.com/latex.php?latex=A_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is empty, we can simply choose <img alt="\mathbf n'=n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27%3Dn&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> deterministically. We assume henceforth that <img alt="A_n\neq \emptyset" class="latex" src="https://s0.wp.com/latex.php?latex=A_n%5Cneq+%5Cemptyset&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for all <img alt="n\in[N]." class="latex" src="https://s0.wp.com/latex.php?latex=n%5Cin%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Following Tao’s proof, we construct <img alt="{\mathbf n}'" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> by creating a conditionally independent copy <img alt="({\mathbf n}',{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of <img alt="({\mathbf n},{\mathbf W})" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> given <img alt="{A_{{\mathbf n}'}}\cup {\mathbf W}' = {A_{{\mathbf n}}}\cup{\mathbf W}." class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%27+%3D+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> In other words, <img alt="({\mathbf n}',{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="({\mathbf n},{\mathbf W})" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> have the same conditional distribution given <img alt="{A_{{\mathbf n}}}\cup{\mathbf W}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and they are also conditionally independent given <img alt="{A_{{\mathbf n}}}\cup{\mathbf W}." class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> This construction guarantees that <img alt="{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}}}\cup{\mathbf W}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely, which implies that <img alt="A_{\mathbf n'}\backslash \mathbf W\subseteq A_{\mathbf n}\cap A_{\mathbf n'}" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%5Csubseteq+A_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely.</p>



<p>It remains to prove that the <img alt="{\mathbf n}'" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> constructed as above satisfies <img alt="{\mathbb E}|A_{\mathbf n}\cap A_{\mathbf n'}|\le \frac 4{\log(R\delta)}{\mathbb E}|{A_{{\mathbf n}}}|." class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+E%7D%7CA_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D%7C%5Cle+%5Cfrac+4%7B%5Clog%28R%5Cdelta%29%7D%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> We achieve this by estimating <img alt="{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> using the chain rule <a href="https://theorydish.blog/feed/#eq-1">(1)</a> via two different chains, one for a lower bound and the other for an upper bound. The lower bound is obtained via the following chain:</p>



<p class="has-text-align-center"><img alt="\begin{aligned}{\mathbf n},{\mathbf W}\rightarrow {\mathbf n}',{\mathbf W}'. \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%5Crightarrow+%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27.+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Namely, we apply <a href="https://theorydish.blog/feed/#eq-1">(1)</a> in the following way:</p>



<p class="has-text-align-center" id="eq-2"><img alt="\begin{aligned}{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') = {\mathbb H}({\mathbf n},{\mathbf W}) + {\mathbb H}({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W}). &amp;&amp; (2)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29.+%26%26+%282%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>By the independence of <img alt="{\mathbf n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{\mathbf W}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,</p>



<p class="has-text-align-center" id="eq-3"><img alt="\begin{aligned}{\mathbb H}({\mathbf n},{\mathbf W}) = {\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}). &amp;&amp; (3)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29.+%26%26+%283%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>By the conditional independence of <img alt="({\mathbf n}',{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="({\mathbf n},{\mathbf W})" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> given <img alt="{A_{{\mathbf n}}}\cup{\mathbf W}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and their identical conditional distribution,</p>



<p class="has-text-align-center" id="eq-4"><img alt="\begin{aligned} &amp; \mathbb H({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W}) \\ = {} &amp; {\mathbb H}({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W},{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n}',{\mathbf W}'|{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n},{\mathbf W}|{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n},{\mathbf W}) - {\mathbb H}({A_{{\mathbf n}}}\cup{\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) - {\mathbb H}({A_{{\mathbf n}}}\cup{\mathbf W})\\ \ge {} &amp; {\mathbb H}({\mathbf n}) - (2 + \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}|,&amp; (4) \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Cmathbb+H%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+-+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5C%5C+%5Cge+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+-+%282+%2B+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C%2C%26+%284%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>where the last inequality is by Lemma <a href="https://theorydish.blog/feed/#lm-5">5</a> Item 1. Plugging <a href="https://theorydish.blog/feed/#eq-3">(3)</a>, <a href="https://theorydish.blog/feed/#eq-4">(4)</a> into <a href="https://theorydish.blog/feed/#eq-2">(2)</a>, we get the following lower bound:</p>



<p class="has-text-align-center" id="eq-5"><img alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') \\ \ge {} &amp; 2{\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) - (2 + \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}|. &amp; (5)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%5C%5C+%5Cge+%7B%7D+%26+2%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%282+%2B+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26+%285%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Now we establish an upper bound for <img alt="{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> via a different chain:</p>



<p class="has-text-align-center"><img alt="\begin{aligned}{\mathbf n}\rightarrow {A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\rightarrow {\mathbf n}'\rightarrow {\mathbf W}\rightarrow {\mathbf W}'.\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbf+n%7D%5Crightarrow+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Crightarrow+%7B%5Cmathbf+n%7D%27%5Crightarrow+%7B%5Cmathbf+W%7D%5Crightarrow+%7B%5Cmathbf+W%7D%27.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Namely, we apply <a href="https://theorydish.blog/feed/#eq-1">(1)</a> in the following manner:</p>



<p class="has-text-align-center" id="eq-6"><img alt="\begin{aligned} &amp; {\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')\\ = {} &amp; {\mathbb H}({\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W},{\mathbf W}')\\= {} &amp; {\mathbb H}({\mathbf n})\\&amp; + {\mathbb H}({A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|{\mathbf n})\\&amp; + {\mathbb H}({\mathbf n}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\&amp; + {\mathbb H}({\mathbf W}|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}')\\&amp; + {\mathbb H}({\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W}).&amp;(6)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+W%7D%27%29%5C%5C%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%7B%5Cmathbf+n%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29.%26%286%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-3">3</a> and <img alt="{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}}}," class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p class="has-text-align-center" id="eq-7"><img alt="\begin{aligned}{\mathbb H}({A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|{\mathbf n})\le{\mathbb E}|{A_{{\mathbf n}}}|. &amp;&amp; (7) \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%7B%5Cmathbf+n%7D%29%5Cle%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26%26+%287%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-4">4</a> and <img alt="{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}'}}," class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p class="has-text-align-center" id="eq-8"><img alt="\begin{aligned} &amp; {\mathbb H}({\mathbf n}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\ \le {} &amp; {\mathbb H}({\mathbf n}'|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\ \le {} &amp; {\mathbb H}({\mathbf n}') - (\log R){\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|. &amp; (8) \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%29+-+%28%5Clog+R%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26+%288%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-5">5</a> Item 2 and <img alt="{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}\subseteq {\mathbf W}," class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Csubseteq+%7B%5Cmathbf+W%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p class="has-text-align-center" id="eq-9"><img alt="\begin{aligned}&amp;{\mathbb H}({\mathbf W}|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}') \\  \le {} &amp; {\mathbb H}({\mathbf W}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}) \\ \le {} &amp; {\mathbb H}({\mathbf W}) - (\log (1/\delta)){\mathbb E}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}|. &amp; (9)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%29+%5C%5C++%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%29+%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%28%5Clog+%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26+%289%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Since <img alt="{\mathbf W}' = ({A_{{\mathbf n}}}\cup{\mathbf W})\backslash({A_{{\mathbf n}'}}\backslash{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+W%7D%27+%3D+%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5Cbackslash%28%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{A_{{\mathbf n}'}}\backslash{\mathbf W}'\subseteq {A_{{\mathbf n}'}}," class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> by Lemma <a href="https://theorydish.blog/feed/#lm-3">3</a>,</p>



<p class="has-text-align-center" id="eq-10"><img alt="\begin{aligned}&amp; {\mathbb H}({\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W}) \\ \le {} &amp; {\mathbb H}({A_{{\mathbf n}'}}\backslash{\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W})\\ \le {} &amp; {\mathbb E}|{A_{{\mathbf n}'}}|. &amp;(10)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29+%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26%2810%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Plugging <a href="https://theorydish.blog/feed/#eq-7">(7)</a>, <a href="https://theorydish.blog/feed/#eq-8">(8)</a>, <a href="https://theorydish.blog/feed/#eq-9">(9)</a>, <a href="https://theorydish.blog/feed/#eq-10">(10)</a> into <a href="https://theorydish.blog/feed/#eq-6">(6)</a> and simplifying using</p>



<p class="has-text-align-center"><img alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n}') = {\mathbb H}({\mathbf n}),\quad {\mathbb E}|{A_{{\mathbf n}'}}| = {\mathbb E}|{A_{{\mathbf n}}}|,\\&amp;{\mathbb E}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}| = {\mathbb E}|{A_{{\mathbf n}'}}| - {\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|,\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29%2C%5Cquad+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C+%3D+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C%2C%5C%5C%26%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C+%3D+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C+-+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>we get the following upper bound:</p>



<p class="has-text-align-center" id="eq-11"><img alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') \\ \le {} &amp; 2{\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) + (2 - \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}| \\ &amp; - (\log (R\delta)){\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|. &amp; (11)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%5C%5C+%5Cle+%7B%7D+%26+2%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+%2B+%282+-+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C+%5C%5C+%26+-+%28%5Clog+%28R%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26+%2811%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Comparing <a href="https://theorydish.blog/feed/#eq-5">(5)</a> and <a href="https://theorydish.blog/feed/#eq-11">(11)</a> proves the desired inequality <img alt="{\mathbb E}|A_{\mathbf n}\cap A_{\mathbf n'}|\le \frac 4{\log(R\delta)}{\mathbb E}|{A_{{\mathbf n}}}|.\square" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+E%7D%7CA_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D%7C%5Cle+%5Cfrac+4%7B%5Clog%28R%5Cdelta%29%7D%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.%5Csquare&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Moses Charikar, Omer Reingold, and Li-Yang Tan for valuable feedback and inspiring discussions.</p></div>
    </content>
    <updated>2021-05-19T23:04:03Z</updated>
    <published>2021-05-19T23:04:03Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Lunjia Hu</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-05-20T03:21:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/19/tenure-track-assistant-professorship-in-algorithms-and-complexity-theory-at-lund-university-apply-by-june-8-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/19/tenure-track-assistant-professorship-in-algorithms-and-complexity-theory-at-lund-university-apply-by-june-8-2021/" rel="alternate" type="text/html"/>
    <title>Tenure-track assistant professorship in algorithms and complexity theory at Lund University (apply by June 8, 2021)</title>
    <summary>The CS department at Lund University invites applications for a tenure-track assistant professorship in algorithms and complexity theory. The application deadline is June 8, 2021. See https://lu.varbi.com/en/what:job/jobID:388991/ for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to jakob.nordstrom@cs.lth.se. Website: https://lu.varbi.com/en/what:job/jobID:388991/ Email: jakob.nordstrom@cs.lth.se</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The CS department at Lund University invites applications for a tenure-track assistant professorship in algorithms and complexity theory. The application deadline is June 8, 2021. See <a href="https://lu.varbi.com/en/what:job/jobID:388991/">https://lu.varbi.com/en/what:job/jobID:388991/</a> for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to jakob.nordstrom@cs.lth.se.</p>
<p>Website: <a href="https://lu.varbi.com/en/what:job/jobID:388991/">https://lu.varbi.com/en/what:job/jobID:388991/</a><br/>
Email: jakob.nordstrom@cs.lth.se</p></div>
    </content>
    <updated>2021-05-19T22:03:56Z</updated>
    <published>2021-05-19T22:03:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-05-20T03:21:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8124</id>
    <link href="https://windowsontheory.org/2021/05/19/workshop-on-local-algorithms-guest-post-by-ronitt-rubinfeld/" rel="alternate" type="text/html"/>
    <title>Workshop on Local Algorithms (Guest post by Ronitt Rubinfeld)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The fifth WOLA (Workshop on Local Algorithms) will be virtual, and take place June 14-15. Registration is free, but required: please fill this form by June 10th to attend. Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been studied in a number of … <a class="more-link" href="https://windowsontheory.org/2021/05/19/workshop-on-local-algorithms-guest-post-by-ronitt-rubinfeld/">Continue reading <span class="screen-reader-text">Workshop on Local Algorithms (Guest post by Ronitt Rubinfeld)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The fifth <a href="http://www.local-algorithms.com/" rel="noreferrer noopener" target="_blank">WOLA</a> (Workshop on Local Algorithms) will be virtual, and take place June 14-15. <strong>Registration is free</strong>, but required: please fill <a href="https://docs.google.com/forms/d/e/1FAIpQLSdot9yskpr-9DPJ2UK6iuYdzlX25w6tPy6H5UKHEVIR-yTLHg/viewform" rel="noreferrer noopener" target="_blank">this form</a> by June 10th to attend.</p>



<p><em>Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been studied in a number of areas in theoretical computer science and mathematics. Some of the related areas include sublinear-time algorithms, distributed algorithms, streaming algorithms, (massively) parallel algorithms, inference in large networks, and graphical models. These communities have similar goals but a variety of approaches, techniques, and methods. This workshop is aimed at fostering dialogue and cross-pollination of ideas between the various communities.</em></p>



<p>This year, the workshop will include:</p>



<ul><li><strong>A poster session</strong>: Please submit <a href="http://www.local-algorithms.com/?page=call-for-posters" rel="noreferrer noopener" target="_blank">your poster proposal</a> (title and abstract) at by<strong> May 26th</strong>. Everyone is invited to contribute. This session will take place on gather.town.</li><li><strong>Invited long talks</strong>: the tentative schedule is <a href="http://www.local-algorithms.com/?page=schedule" rel="noreferrer noopener" target="_blank">available</a>, and features talks by James Aspnes, Jelani Nelson, Elaine Shi, Christian Sohler, Uri Stemmer, and Mary Wootters.</li><li><strong>Junior-Senior social meetings</strong></li><li><strong>An AMA (Ask Me Anything) session</strong>, moderated by Merav Parter</li><li><strong>A Slack channel</strong></li><li><strong>An Open Problems session</strong></li></ul>



<p>The Program Committee of WOLA 2021 is comprised of:</p>



<ul><li>Venkatesan Guruswami (CMU)</li><li>Elchanan Mossel (MIT)</li><li>Merav Parter (Weizmann Institute of Science)</li><li>Sofya Raskhodnikova <strong>(chair) </strong>(Boston University)</li><li>Gregory Valiant (Stanford)</li></ul>



<p>and the organizing committee:</p>



<ul><li>Sebastian Brandt (ETH)</li><li>Yannic Maus (Technion)</li><li>Slobodan Mitrović (MIT)</li></ul>



<p>For more detail, see <a href="http://www.local-algorithms.com/?" rel="noreferrer noopener" target="_blank">the website</a>;</p></div>
    </content>
    <updated>2021-05-19T17:56:50Z</updated>
    <published>2021-05-19T17:56:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-05-20T03:21:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://nisheethvishnoi.wordpress.com/?p=107</id>
    <link href="https://nisheethvishnoi.wordpress.com/2021/05/19/submit-your-papers-to-the-62nd-focs/" rel="alternate" type="text/html"/>
    <title>Submit your papers to the 62nd FOCS!</title>
    <summary>The submission server for the 62nd FOCS is open! Please read the Call for Papers carefully before you submit. Below are some important points: The title/abstract registration deadline is 5 pm EDT on May 31st, 2021. Since this information will be used in paper assignments, significant changes to the title/abstract will not be allowed after […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://easychair.org/conferences/?conf=focs2021">submission server</a> for the 62nd FOCS is open! </p>



<p>Please read the <a href="https://focs2021.cs.colorado.edu/cfp/">Call for Papers</a> carefully before you submit. </p>



<p>Below are some important points:</p>



<ul><li>The title/abstract registration deadline is <strong>5 pm EDT</strong> on <strong>May 31st, 2021</strong>. Since this information will be used in paper assignments, significant changes to the title/abstract will not be allowed after this deadline.</li><li>The full paper submission deadline is <strong>5 pm EDT</strong> on <strong>June 3, 2021</strong>.</li><li>The conference is expected to take place <strong>physically</strong> in Boulder, Colorado <strong>Feb 7-10, 2022</strong>.</li><li>Papers that broaden the reach of the theory of computing, make foundational connections to other areas, or raise important problems and demonstrate that they can benefit from theoretical investigation and analysis are especially encouraged.</li><li>Reviewers will be asked to evaluate submissions <strong>both on conceptual and technical merits</strong>. So, authors are encouraged to emphasize both conceptual and technical novelty in the first few pages of the paper. </li><li>FOCS is an IEEE conference and as such, we will review papers in alignment with the IEEE ethics <a href="https://www.ieee.org/about/corporate/governance/p7-8.html">guidelines</a>. Authors are encouraged to reflect on these guidelines in shaping their work, dissemination, and submission.</li></ul>



<p/>



<p>The PC is eagerly looking forward to your submissions!</p>



<p> </p></div>
    </content>
    <updated>2021-05-19T14:23:56Z</updated>
    <published>2021-05-19T14:23:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>nisheethvishnoi</name>
    </author>
    <source>
      <id>https://nisheethvishnoi.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://nisheethvishnoi.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://nisheethvishnoi.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Algorithms, Nature, and Society</title>
      <updated>2021-05-20T03:22:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18714</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/05/19/hilbert-tenth-on-rationals/" rel="alternate" type="text/html"/>
    <title>Hilbert Tenth On Rationals</title>
    <summary>We must know. We will know—David Hilbert Mihai Prunescu is at the Institute of Mathematics of the Romanian Academy. He works in logic and complexity theory. We recently discussed his work on Hilbert’s Tenth. Today we thought we would do a follow up discussion of a recent paper of his on the famous Hilbert’s Tenth […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>We must know. We will know—David Hilbert</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/19/hilbert-tenth-on-rationals/mp/" rel="attachment wp-att-18716"><img alt="" class="alignright wp-image-18716" height="150" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/mp-150x150.jpeg?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
Mihai Prunescu is at the Institute of Mathematics of the Romanian Academy. He works in logic and complexity theory. We recently discussed his <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/">work</a> on Hilbert’s Tenth.</p>
<p>
Today we thought we would do a follow up discussion of a recent paper of his on the famous Hilbert’s Tenth problem. </p>
<p>
“Hilbert’s Tenth” is actually a suite of problems: is it decidable whether a finite set of equations of a certain kind have a common solution over a certain domain?  When the domain is the integers and the equations are polynomials set to zero we have the original form, which was shown to be undecidable by Yuriy Matiyasevich, completing work of Julia Robinson, Martin Davis, and Hilary Putnam.  But over the reals, the problem for polynomial equations is decidable, and Hilbert himself had shown this over the complex numbers.  The rational numbers are the key unsolved case, which I posted about <a href="https://rjlipton.wpcomstaging.com/2010/08/07/hilberts-tenth-over-the-rationals/">eleven</a> and <a href="https://rjlipton.wpcomstaging.com/2011/07/19/hilberts-10-5th-problem/">ten</a> years ago and <a href="https://rjlipton.wpcomstaging.com/2019/06/19/diophantine-equations/">again</a> more <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/">recently</a>.</p>
<p>
The last post, two months ago, was about attempts to get leverage on the rationals by extending the equations to allow exponentiation.  Mihai added several helpful comments to that post, and with his blessing, this is trying to restart the discussion as busy pandemic-impacted university terms for Ken and some others we know draw to a close.</p>
<p>
</p><h2> Subtleties With Exponents </h2><p/>
<p>
He has helped us understand a few subtleties in correspondence since then.  They concern both that rational numbers and exponentials are involved.  The focal point confounds our expectation that the square <img alt="{r^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of any nonzero rational number <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> should be positive.  Here are two examples of issues:</p>
<ul>
<li> If <img alt="{r = x^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+x%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and we put <img alt="{r^2 = x^{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%5E2+%3D+x%5E%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then <img alt="{x = -1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> gives a negative value on the right-hand side.
</li><li>If <img alt="{r = x^{1/4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+x%5E%7B1%2F4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and we put <img alt="{r^2 = x^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%5E2+%3D+x%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then the negative square root of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a possible value.
</li></ul>
<p>
If we have two equations <img alt="{E_1(\vec{x})=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_1%28%5Cvec%7Bx%7D%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{E_2(\vec{x})=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_2%28%5Cvec%7Bx%7D%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and we want to say that <img alt="{\vec{x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bx%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> solves at least one of them, we can introduce the single equation</p>
<p align="center"><img alt="\displaystyle  E_1(\vec{x})\cdot E_2(\vec{x}) = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_1%28%5Cvec%7Bx%7D%29%5Ccdot+E_2%28%5Cvec%7Bx%7D%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
This is not affected by the subtleties.  But now suppose we want to do AND instead of OR.  The natural idea is to make the equation</p>
<p align="center"><img alt="\displaystyle  E_1(\vec{x})^2 + E_2(\vec{x})^2 = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_1%28%5Cvec%7Bx%7D%29%5E2+%2B+E_2%28%5Cvec%7Bx%7D%29%5E2+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
However, suppose we have the equation <img alt="{x^{1/4} = y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F4%7D+%3D+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.  This is the same as <img alt="{x^{1/4} - y = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F4%7D+-+y+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.  If we square it, we get </p>
<p align="center"><img alt="\displaystyle  x^{1/2} + y^2 - 2x^{1/4}y = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7B1%2F2%7D+%2B+y%5E2+-+2x%5E%7B1%2F4%7Dy+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
The abstract worry is that this could allow solutions where <img alt="{x^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is chosen <i>negative</i>, though not intended as the square of <img alt="{x^{1/4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (whether positive or negative).  Mihai gave us a simple example of two exponential equations where unintended solutions occur after squaring and adding them.  Consider</p>
<p align="center"><img alt="\displaystyle  u^v = 0 \wedge w^x = 0. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u%5Ev+%3D+0+%5Cwedge+w%5Ex+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
versus</p>
<p align="center"><img alt="\displaystyle  u^{2v} + w^{2x} = 0. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u%5E%7B2v%7D+%2B+w%5E%7B2x%7D+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
The former set is solved only by <img alt="{u = w = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+w+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with the exponents nonzero.  The latter, however, allows solutions illustrating both issues above:</p>
<ul>
<li> <img alt="{u = v = 1,\; w = -1,\; x = 1/2;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+v+%3D+1%2C%5C%3B+w+%3D+-1%2C%5C%3B+x+%3D+1%2F2%3B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>
</li><li> <img alt="{u = v = w = 1,\; x = 1/4;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+v+%3D+w+%3D+1%2C%5C%3B+x+%3D+1%2F4%3B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>
</li></ul>
<p>
where in the latter, the negative square root of <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is taken.  What this means is that with rationals and/or exponentials we must watch our manipulations more carefully.  </p>
<p>
</p><h2> The Theorem </h2><p/>
<p>
The following <a href="https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/abs/exponential-diophantine-problem-for-mathbb-q/48F3FBED93B3138F34D39A52DB560BC0">theorem</a> is essentially due to Mihai: </p>
<blockquote><p><b>Theorem 1</b> <em> Let <img alt="{P(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> be an integer polynomial. Then it is undecidable to determine whether there are <img alt="{a_1,\dots,a_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_1%2C%5Cdots%2Ca_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> in <img alt="{{\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{b_1,\dots,b_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb_1%2C%5Cdots%2Cb_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> also in <img alt="{{\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> so that </em></p><em>
<ol>
<li>
<img alt="{P(a_1,\dots,a_n)=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%28a_1%2C%5Cdots%2Ca_n%29%3D0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <p/>
</li><li>
For each <img alt="{k=1,\dots,n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%3D1%2C%5Cdots%2Cn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> 	<p/>
<p align="center"><img alt="\displaystyle  b_k = 2^{a_k}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++b_k+%3D+2%5E%7Ba_k%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
</li></ol>
</em><p><em/>
</p></blockquote>
<p>Here <img alt="{{\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as usual is the rationals.  The full question whether the above theorem can be proved without any exponential functions <img alt="{2^x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> remains open, after much attention by many researchers. </p>
<p>
</p><p/><h2> Some History </h2><p/>
<p/><p>
The reason we say “essentially” due to Prunescu must be explained. He recently published an article showing that <i>The Exponential Diophantine Problem For <img alt="{{\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></i> is undecidable: It is in the <i>Journal of Symbolic Logic</i> (JSL), Volume 85, Issue 2. </p>
<p>
His clever proof showed that the solutions <img alt="{(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> over the rationals of 	</p>
<p align="center"><img alt="\displaystyle  x^y = y^x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5Ey+%3D+y%5Ex+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>lie in a one-dimensional space. This space is parameterized by the integers and so it can be used to define the integers. This immediately proves the undecidability by reduction to the classic Hilbert’s Tenth over the integers.</p>
<p>
Ken and I then wrote a <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/">post</a> on our blog GLL explaining his JSL result. Mihai kindly added a <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/#comment-116365">comment</a> on the post that basically stated the above theorem with <img alt="{2^y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </p>
<p>
I was initially puzzled since the above theorem seems to be stronger than his JSL theorem. His published result uses <img alt="{x^y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> a binary exponential function and the new theorem needs only the unary function <img alt="{2^y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Now it must be said that the cases could be incomparable—because the extra freedom of allowing any rational base <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in <img alt="{x^y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> could promote a different kind of analysis that makes the existence of such solutions decidable.  The equivalence logic coming back from undecidable cases over the integers might not apply in the above reduction.  But the undecidability when the base is fixed to be <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> still strikes me as capturing the general import.</p>
<p>
What’s more, the theorem fixing <img alt="{2^x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as the base has a much simpler proof.  It requires only a lemma that extends the famous Euclid Theorem: The value <img alt="{\sqrt{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is not rational.</p>
<p/><h2> Proof of the Lemma </h2><p/>
<p>
The proof rests on the famous result of Matiyasevich on Hilbert’s Tenth and the following lemma: </p>
<blockquote><p><b>Lemma 2</b> <em> Suppose that <img alt="{x \ge 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cge+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{x \in {\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="{2^x \in {\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ex+%5Cin+%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer. </em>
</p></blockquote>
<p/><p>
<em>Proof:</em>  Clearly we can assume that <img alt="{x&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We will prove the lemma in two steps. </p>
<ol>
<li>
The value <img alt="{2^x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer. <p/>
</li><li>
The value <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer.
</li></ol>
<p>
Let <img alt="{x = r/s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+r%2Fs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="{r, s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%2C+s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> integers that are co-prime and <img alt="{r \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{s \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </p>
<p>
<i>Step (1):</i> Now let 	</p>
<p align="center"><img alt="\displaystyle  2^x = \frac{u}{v} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5Ex+%3D+%5Cfrac%7Bu%7D%7Bv%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>where <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are co-prime integers. We can assume that <img alt="{|v| \ge 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cv%7C+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>; otherwise, step (1) is true. Now 	</p>
<p align="center"><img alt="\displaystyle  2^r = \frac{u^s}{v^s}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5Er+%3D+%5Cfrac%7Bu%5Es%7D%7Bv%5Es%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>But <img alt="{u^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{v^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are co-prime since <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are co-prime. But this implies that <img alt="{2^r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is not an integer, since <img alt="{|v^s| \ge 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cv%5Es%7C+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Note this uses that <img alt="{r \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{s \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This contradiction proves step (1).</p>
<p>
<i>Step (2):</i> By step (1) we thus have that 	</p>
<p align="center"><img alt="\displaystyle  2^{r/s} = y " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7Br%2Fs%7D+%3D+y+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>for some integer <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then 	</p>
<p align="center"><img alt="\displaystyle  2^r = y^s. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5Er+%3D+y%5Es.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Thus <img alt="{y^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer and so by unique factorization it follows that <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> must be an integer power of <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Let <img alt="{y = 2^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By+%3D+2%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then 	</p>
<p align="center"><img alt="\displaystyle  2^{r} = 2^{sk}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7Br%7D+%3D+2%5E%7Bsk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>This implies that <img alt="{r=sk}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%3Dsk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and so that <img alt="{x = r/s=k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+r%2Fs%3Dk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer. This proves step (2). <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p/><h2> Open Problems </h2><p/>
<p>
We have shared the topic also with Joël Ouaknine and thank him for his comments as well. Addressing our readers, do you have any further comments? What do you think about the rationals? </p>
<p>
Ken related an idea while corresponding with Mihai: Suppose we introduce a special squaring function <img alt="{Sq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BSq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with the properties that</p>
<ul>
<li>
<img alt="{Sq(r)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BSq%28r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is always positive for nonzero <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<p/>
</li><li>
In particular, <img alt="{Sq(x^{1/4})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BSq%28x%5E%7B1%2F4%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> gives only the positive square root of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<p/>
</li><li>
Terms with <img alt="{Sq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BSq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> may not appear in exponents, nor be bases of them.  They may only be added and multiplied.
</li></ul>
<p>
The motivation is that now</p>
<p align="center"><img alt="\displaystyle  Sq(E_1(\vec{x}) + Sq(E_2(\vec{x})) = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Sq%28E_1%28%5Cvec%7Bx%7D%29+%2B+Sq%28E_2%28%5Cvec%7Bx%7D%29%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
does give the AND of the two equations.  What then becomes of the above undecidability proofs, and does the AND property have other applications?</p></font></font></div>
    </content>
    <updated>2021-05-19T12:37:11Z</updated>
    <published>2021-05-19T12:37:11Z</published>
    <category term="History"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="Diophantine"/>
    <category term="H10"/>
    <category term="Hilbert Tenth"/>
    <category term="Mihai Prunescu"/>
    <category term="rationals"/>
    <category term="undecidability"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-05-20T03:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1517</id>
    <link href="https://ptreview.sublinear.info/?p=1517" rel="alternate" type="text/html"/>
    <title>Announcing WOLA’21 (Workshop on Local Algorithms)</title>
    <summary>The fifth WOLA (Workshop on Local Algorithms) will be virtual, and take place June 14-15. Registration is free, but required: please fill this form by June 10th to attend. Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The fifth <a href="http://www.local-algorithms.com">WOLA</a> (Workshop on Local Algorithms) will be virtual, and take place June 14-15. <strong>Registration is free</strong>, but required: please fill <a href="https://docs.google.com/forms/d/e/1FAIpQLSdot9yskpr-9DPJ2UK6iuYdzlX25w6tPy6H5UKHEVIR-yTLHg/viewform">this form</a> by June 10th to attend.</p>



<p><em>Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been studied in a number of areas in theoretical computer science and mathematics. Some of the related areas include sublinear-time algorithms, distributed algorithms, streaming algorithms, (massively) parallel algorithms, inference in large networks, and graphical models. These communities have similar goals but a variety of approaches, techniques, and methods. This workshop is aimed at fostering dialogue and cross-pollination of ideas between the various communities.</em></p>



<p>This year, the workshop will include:</p>



<ul><li><strong>A poster session</strong>: Please submit <a href="http://www.local-algorithms.com/?page=call-for-posters">your poster proposal</a> (title and abstract) at by<strong> May 26th</strong>. Everyone is invited to contribute. This session will take place on gather.town.</li><li><strong>Invited long talks</strong>: the tentative schedule is <a href="http://www.local-algorithms.com/?page=schedule">available</a>, and features talks by James Aspnes, Jelani Nelson, Elaine Shi, Christian Sohler, Uri Stemmer, and Mary Wootters.</li><li><strong>Junior-Senior social meetings</strong></li><li><strong>An AMA (Ask Me Anything) session</strong>, moderated by Merav Parter </li><li><strong>A Slack channel</strong></li><li><strong>An Open Problems session</strong></li></ul>



<p>The Program Committee of WOLA 2021 is comprised of:</p>



<ul><li>Venkatesan Guruswami (CMU)</li><li>Elchanan Mossel (MIT)</li><li>Merav Parter (Weizmann Institute of Science)</li><li>Sofya Raskhodnikova <strong>(chair) </strong>(Boston University)</li><li>Gregory Valiant (Stanford)</li></ul>



<p>and the organizing committee:</p>



<ul><li>Sebastian Brandt (ETH)</li><li>Yannic Maus (Technion)</li><li>Slobodan Mitrović (MIT)</li></ul>



<p>For more detail, see <a href="http://www.local-algorithms.com/?">the website</a>; to stay up to date with the latest announcements concerning WOLA, <a href="https://lists.csail.mit.edu/mailman/listinfo/wola">join our mailing list</a>!</p></div>
    </content>
    <updated>2021-05-19T00:29:53Z</updated>
    <published>2021-05-19T00:29:53Z</published>
    <category term="Announcement"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-05-20T00:06:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08706</id>
    <link href="http://arxiv.org/abs/2105.08706" rel="alternate" type="text/html"/>
    <title>Durable Queues: The Second Amendment</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sela:Gal.html">Gal Sela</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Petrank:Erez.html">Erez Petrank</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08706">PDF</a><br/><b>Abstract: </b>We consider durable data structures for non-volatile main memory, such as the
new Intel Optane memory architecture. Substantial recent work has concentrated
on making concurrent data structures durable with low overhead, by adding a
minimal number of blocking persist operations (i.e., flushes and fences). In
this work we show that focusing on minimizing the number of persist
instructions is important, but not enough. We show that access to flushed
content is of high cost due to cache invalidation in current architectures.
Given this finding, we present a design of the queue data structure that
properly takes care of minimizing blocking persist operations as well as
minimizing access to flushed content. The proposed design outperforms
state-of-the-art durable queues.
</p>
<p>We start by providing a durable version of the Michael Scott queue (MSQ). We
amend MSQ by adding a minimal number of persist instructions, fewer than in
available durable queues, and meeting the theoretical lower bound on the number
of blocking persist operations. We then proceed with a second amendment to this
design, that eliminates accesses to flushed data. Evaluation shows that the
second amendment yields substantial performance improvement, outperforming the
state of the art and demonstrating the importance of reduced accesses to
flushed content. The presented queues are durably linearizable and lock-free.
Finally, we discuss the theoretical optimal number of accesses to flushed
content.
</p></div>
    </summary>
    <updated>2021-05-19T22:52:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08675</id>
    <link href="http://arxiv.org/abs/2105.08675" rel="alternate" type="text/html"/>
    <title>The Computational Complexity of ReLU Network Training Parameterized by Data Dimensionality</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Froese:Vincent.html">Vincent Froese</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hertrich:Christoph.html">Christoph Hertrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niedermeier:Rolf.html">Rolf Niedermeier</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08675">PDF</a><br/><b>Abstract: </b>Understanding the computational complexity of training simple neural networks
with rectified linear units (ReLUs) has recently been a subject of intensive
research. Closing gaps and complementing results from the literature, we
present several results on the parameterized complexity of training two-layer
ReLU networks with respect to various loss functions. After a brief discussion
of other parameters, we focus on analyzing the influence of the dimension $d$
of the training data on the computational complexity. We provide running time
lower bounds in terms of W[1]-hardness for parameter $d$ and prove that known
brute-force strategies are essentially optimal (assuming the Exponential Time
Hypothesis). In comparison with previous work, our results hold for a broad(er)
range of loss functions, including $\ell^p$-loss for all $p\in[0,\infty]$. In
particular, we extend a known polynomial-time algorithm for constant $d$ and
convex loss functions to a more general class of loss functions, matching our
running time lower bounds also in these cases.
</p></div>
    </summary>
    <updated>2021-05-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08577</id>
    <link href="http://arxiv.org/abs/2105.08577" rel="alternate" type="text/html"/>
    <title>Approximation Algorithms for Demand Strip Packing</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=aacute=lvez:Waldo.html">Waldo Gálvez</a>, Fabrizio Grandoni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ameli:Afrouz_Jabal.html">Afrouz Jabal Ameli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khodamoradi:Kamyar.html">Kamyar Khodamoradi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08577">PDF</a><br/><b>Abstract: </b>In the Demand Strip Packing problem (DSP), we are given a time interval and a
collection of tasks, each characterized by a processing time and a demand for a
given resource (such as electricity, computational power, etc.). A feasible
solution consists of a schedule of the tasks within the mentioned time
interval. Our goal is to minimize the peak resource consumption, i.e. the
maximum total demand of tasks executed at any point in time.
</p>
<p>It is known that DSP is NP-hard to approximate below a factor 3/2, and
standard techniques for related problems imply a (polynomial-time)
2-approximation. Our main result is a (5/3+eps)-approximation algorithm for any
constant eps&gt;0. We also achieve best-possible approximation factors for some
relevant special cases.
</p></div>
    </summary>
    <updated>2021-05-19T23:01:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08549</id>
    <link href="http://arxiv.org/abs/2105.08549" rel="alternate" type="text/html"/>
    <title>A cubic vertex-kernel for Trivially Perfect Editing</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Maël Dumas, Anthony Perez, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Todinca:Ioan.html">Ioan Todinca</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08549">PDF</a><br/><b>Abstract: </b>We consider the Trivially Perfect Editing problem, where one is given an
undirected graph $G = (V,E)$ and a parameter $k \in \mathbb{N}$ and seeks to
edit (add or delete) at most $k$ edges from $G$ to obtain a trivially perfect
graph. The related Trivially Perfect Completion and Trivially Perfect Deletion
problems are obtained by only allowing edge additions or edge deletions,
respectively. Trivially perfect graphs are both chordal and cographs, and have
applications related to the tree-depth width parameter and to social network
analysis. All variants of the problem are known to be NP-Complete and to admit
so-called polynomial kernels. More precisely, the existence of an $O(k^3)$
vertex-kernel for Trivially Perfect Completion was announced by Guo (ISAAC
2007) but without a stand-alone proof. More recently, Drange and Pilipczuk
(Algorithmica 2018) provided $O(k^7)$ vertex-kernels for these problems and
left open the existence of cubic vertex-kernels. In this work, we answer
positively to this question for all three variants of the problem.
</p></div>
    </summary>
    <updated>2021-05-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08540</id>
    <link href="http://arxiv.org/abs/2105.08540" rel="alternate" type="text/html"/>
    <title>Kemeny Consensus Complexity</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fitzsimmons:Zack.html">Zack Fitzsimmons</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hemaspaandra:Edith.html">Edith Hemaspaandra</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08540">PDF</a><br/><b>Abstract: </b>The computational study of election problems generally focuses on questions
related to the winner or set of winners of an election. But social preference
functions such as Kemeny rule output a full ranking of the candidates (a
consensus). We study the complexity of consensus-related questions, with a
particular focus on Kemeny and its qualitative version Slater. The simplest of
these questions is the problem of determining whether a ranking is a consensus,
and we show that this problem is coNP-complete. We also study the natural
question of the complexity of manipulative actions that have a specific
consensus as a goal. Though determining whether a ranking is a Kemeny consensus
is hard, the optimal action for manipulators is to simply vote their desired
consensus. We provide evidence that this simplicity is caused by the
combination of election system (Kemeny), manipulative action (manipulation),
and manipulative goal (consensus). In the process we provide the first
completeness results at the second level of the polynomial hierarchy for
electoral manipulation and for optimal solution recognition.
</p></div>
    </summary>
    <updated>2021-05-19T22:44:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08490</id>
    <link href="http://arxiv.org/abs/2105.08490" rel="alternate" type="text/html"/>
    <title>GSF-locality is not sufficient for proximity-oblivious testing</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adler:Isolde.html">Isolde Adler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=ouml=hler:Noleen.html">Noleen Köhler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Pan.html">Pan Peng</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08490">PDF</a><br/><b>Abstract: </b>In Property Testing, proximity-oblivious testers (POTs) form a class of
particularly simple testing algorithms, where a basic test is performed a
number of times that may depend on the proximity parameter, but the basic test
itself is independent of the proximity parameter. In their seminal work,
Goldreich and Ron [STOC 2009; SICOMP 2011] show that the graph properties that
allow constant-query proximity-oblivious testing in the bounded-degree model
are precisely the properties that can be expressed as a generalised subgraph
freeness (GSF) property that satisfies the non-propagation condition. It is
left open whether the non-propagation condition is necessary. Indeed, calling
properties expressible as a generalised subgraph freeness property GSF-local
properties, they ask whether all GSF-local properties are non-propagating. We
give a negative answer by exhibiting a property of graphs that is GSF-local and
propagating. Hence in particular, our property does not admit a POT, despite
being GSF-local. We prove our result by exploiting a recent work of the authors
which constructed a first-order (FO) property that is not testable [SODA 2021],
and a new connection between FO properties and GSF-local properties via
neighbourhood profiles.
</p></div>
    </summary>
    <updated>2021-05-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08378</id>
    <link href="http://arxiv.org/abs/2105.08378" rel="alternate" type="text/html"/>
    <title>On the Complexity of Robust Bilevel Optimization With Uncertain Follower's Objective</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Buchheim:Christoph.html">Christoph Buchheim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henke:Dorothee.html">Dorothee Henke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hommelsheim:Felix.html">Felix Hommelsheim</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08378">PDF</a><br/><b>Abstract: </b>We investigate the complexity of bilevel combinatorial optimization with
uncertainty in the follower's objective, in a robust optimization approach. In
contrast to single-level optimization, we show that the introduction of
interval uncertainty renders the bilevel problem significantly harder both in
the optimistic and the pessimistic setting. More precisely, the robust
counterpart of the uncertain bilevel problem can be $\Sigma^{\text P}_2$-hard
in this case, even when the certain bilevel problem is NP-equivalent and the
follower's problem is tractable. On the contrary, in the discrete uncertainty
case, the robust bilevel problem is at most one level harder than the
follower's problem. Moreover, we show that replacing an uncertainty set by its
convex hull may increase the complexity of the robust counterpart in our
setting, which again differs from the single-level case.
</p></div>
    </summary>
    <updated>2021-05-19T22:58:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08356</id>
    <link href="http://arxiv.org/abs/2105.08356" rel="alternate" type="text/html"/>
    <title>On Symmetry versus Asynchronism: at the Edge of Universality in Automata Networks</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wilson:Mart=iacute=n_R=iacute=os.html">Martín Ríos Wilson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Theyssier:Guillaume.html">Guillaume Theyssier</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08356">PDF</a><br/><b>Abstract: </b>An automata network (AN) is a finite graph where each node holds a state from
a finite alphabet and is equipped with a local map defining the evolution of
the state of the node depending on its neighbors. The global dynamics of the
network is then induced by an update scheme describing which nodes are updated
at each time step. We study how update schemes can compensate the limitations
coming from symmetric local interactions. Our approach is based on intrinsic
simulations and universality and we study both dynamical and computational
complexity. By considering several families of concrete symmetric AN under
several different update schemes, we explore the edge of universality in this
two-dimensional landscape. On the way, we develop a proof technique based on an
operation of glueing of networks, which allows to produce complex orbits in
large networks from compatible pseudo-orbits in small networks.
</p></div>
    </summary>
    <updated>2021-05-19T22:44:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08339</id>
    <link href="http://arxiv.org/abs/2105.08339" rel="alternate" type="text/html"/>
    <title>DRIVE: One-bit Distributed Mean Estimation</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vargaftik:Shay.html">Shay Vargaftik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Basat:Ran_Ben.html">Ran Ben Basat</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Portnoy:Amit.html">Amit Portnoy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mendelson:Gal.html">Gal Mendelson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Ben=Itzhak:Yaniv.html">Yaniv Ben-Itzhak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitzenmacher:Michael.html">Michael Mitzenmacher</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08339">PDF</a><br/><b>Abstract: </b>We consider the problem where $n$ clients transmit $d$-dimensional
real-valued vectors using only $d(1+o(1))$ bits each, in a manner that allows a
receiver to approximately reconstruct their mean. Such compression problems
arise in federated and distributed learning, as well as in other domains. We
provide novel mathematical results and derive corresponding new algorithms that
outperform previous compression algorithms in accuracy and computational
efficiency. We evaluate our methods on a collection of distributed and
federated learning tasks, using a variety of datasets, and show a consistent
improvement over the state of the art.
</p></div>
    </summary>
    <updated>2021-05-19T22:52:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08335</id>
    <link href="http://arxiv.org/abs/2105.08335" rel="alternate" type="text/html"/>
    <title>Interference-free Walks in Time: Temporally Disjoint Paths</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klobas:Nina.html">Nina Klobas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mertzios:George_B=.html">George B. Mertzios</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Molter:Hendrik.html">Hendrik Molter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niedermeier:Rolf.html">Rolf Niedermeier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zschoche:Philipp.html">Philipp Zschoche</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08335">PDF</a><br/><b>Abstract: </b>We investigate the computational complexity of finding temporally disjoint
paths or walks in temporal graphs. There, the edge set changes over discrete
time steps and a temporal path (resp. walk) uses edges that appear at
monotonically increasing time steps. Two paths (or walks) are temporally
disjoint if they never use the same vertex at the same time; otherwise, they
interfere. This reflects applications in robotics, traffic routing, or finding
safe pathways in dynamically changing networks. On the one extreme, we show
that on general graphs the problem is computationally hard. The "walk version"
is W[1]-hard when parameterized by the number of routes. However, it is
polynomial-time solvable for any constant number of walks. The "path version"
remains NP-hard even if we want to find only two temporally disjoint paths. On
the other extreme, restricting the input temporal graph to have a path as
underlying graph, quite counterintuitively, we find NP-hardness in general but
also identify natural tractable cases.
</p></div>
    </summary>
    <updated>2021-05-19T23:01:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08309</id>
    <link href="http://arxiv.org/abs/2105.08309" rel="alternate" type="text/html"/>
    <title>Time and Query Optimal Quantum Algorithms Based on Decision Trees</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beigi:Salman.html">Salman Beigi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Taghavi:Leila.html">Leila Taghavi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tajdini:Artin.html">Artin Tajdini</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08309">PDF</a><br/><b>Abstract: </b>It has recently been shown that starting with a classical query algorithm
(decision tree) and a guessing algorithm that tries to predict the query
answers, we can design a quantum algorithm with query complexity $O(\sqrt{GT})$
where $T$ is the query complexity of the classical algorithm (depth of the
decision tree) and $G$ is the maximum number of wrong answers by the guessing
algorithm [<a href="http://export.arxiv.org/abs/1410.0932">arXiv:1410.0932</a>, <a href="http://export.arxiv.org/abs/1905.13095">arXiv:1905.13095</a>]. In this paper we show that,
given some constraints on the classical algorithms, this quantum algorithm can
be implemented in time $\tilde O(\sqrt{GT})$. Our algorithm is based on
non-binary span programs and their efficient implementation. We conclude that
various graph theoretic problems including bipartiteness, cycle detection and
topological sort can be solved in time $O(n^{3/2}\log n)$ and with $O(n^{3/2})$
quantum queries. Moreover, finding a maximal matching can be solved with
$O(n^{3/2})$ quantum queries in time $O(n^{3/2}\log n)$, and maximum bipartite
matching can be solved in time $O(n^2\log n)$.
</p></div>
    </summary>
    <updated>2021-05-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08305</id>
    <link href="http://arxiv.org/abs/2105.08305" rel="alternate" type="text/html"/>
    <title>Snipperclips: Cutting Tools into Desired Polygons using Themselves</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abel:Zachary.html">Zachary Abel</a>, Hugo Akitaya, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chiu:Man=Kwun.html">Man-Kwun Chiu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Martin_L=.html">Martin L. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hesterberg:Adam.html">Adam Hesterberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Korman:Matias.html">Matias Korman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lynch:Jayson.html">Jayson Lynch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Renssen:Andr=eacute=_van.html">André van Renssen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roeloffzen:Marcel.html">Marcel Roeloffzen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08305">PDF</a><br/><b>Abstract: </b>We study Snipperclips, a computer puzzle game whose objective is to create a
target shape with two tools. The tools start as constant-complexity shapes, and
each tool can snip (i.e., subtract its current shape from) the other tool. We
study the computational problem of, given a target shape represented by a
polygonal domain of $n$ vertices, is it possible to create it as one of the
tools' shape via a sequence of snip operations? If so, how many snip operations
are required? We consider several variants of the problem (such as allowing the
tools to be disconnected and/or using an undo operation) and bound the number
of operations needed for each of the variants.
</p></div>
    </summary>
    <updated>2021-05-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08292</id>
    <link href="http://arxiv.org/abs/2105.08292" rel="alternate" type="text/html"/>
    <title>99% Revenue with Constant Enhanced Competition</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cai:Linda.html">Linda Cai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saxena:Raghuvansh_R=.html">Raghuvansh R. Saxena</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08292">PDF</a><br/><b>Abstract: </b>The enhanced competition paradigm is an attempt at bridging the gap between
simple and optimal auctions. In this line of work, given an auction setting
with $m$ items and $n$ bidders, the goal is to find the smallest $n' \geq n$
such that selling the items to $n'$ bidders through a simple auction generates
(almost) the same revenue as the optimal auction.
</p>
<p>Recently, Feldman, Friedler, and Rubinstein [EC, 2018] showed that an
arbitrarily large constant fraction of the optimal revenue from selling $m$
items to a single bidder can be obtained via simple auctions with a constant
number of bidders. However, their techniques break down even for two bidders,
and can only show a bound of $n' = n \cdot O(\log \frac{m}{n})$.
</p>
<p>Our main result is that $n' = O(n)$ bidders suffice for all values of $m$ and
$n$. That is, we show that, for all $m$ and $n$, an arbitrarily large constant
fraction of the optimal revenue from selling $m$ items to $n$ bidders can be
obtained via simple auctions with $O(n)$ bidders. Moreover, when the items are
regular, we can achieve the same result through auctions that are
prior-independent, {\em i.e.}, they do not depend on the distribution from
which the bidders' valuations are sampled.
</p></div>
    </summary>
    <updated>2021-05-19T23:00:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08285</id>
    <link href="http://arxiv.org/abs/2105.08285" rel="alternate" type="text/html"/>
    <title>Sublinear Least-Squares Value Iteration via Locality Sensitive Hashing</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shrivastava:Anshumali.html">Anshumali Shrivastava</a>, Zhao Song, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Zhaozhuo.html">Zhaozhuo Xu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08285">PDF</a><br/><b>Abstract: </b>We present the first provable Least-Squares Value Iteration (LSVI) algorithms
that have runtime complexity sublinear in the number of actions. We formulate
the value function estimation procedure in value iteration as an approximate
maximum inner product search problem and propose a locality sensitive hashing
(LSH) [Indyk and Motwani STOC'98, Andoni and Razenshteyn STOC'15, Andoni,
Laarhoven, Razenshteyn and Waingarten SODA'17] type data structure to solve
this problem with sublinear time complexity. Moreover, we build the connections
between the theory of approximate maximum inner product search and the regret
analysis of reinforcement learning. We prove that, with our choice of
approximation factor, our Sublinear LSVI algorithms maintain the same regret as
the original LSVI algorithms while reducing the runtime complexity to sublinear
in the number of actions. To the best of our knowledge, this is the first work
that combines LSH with reinforcement learning resulting in provable
improvements. We hope that our novel way of combining data-structures and
iterative algorithm will open the door for further study into cost reduction in
optimization.
</p></div>
    </summary>
    <updated>2021-05-19T22:59:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08215</id>
    <link href="http://arxiv.org/abs/2105.08215" rel="alternate" type="text/html"/>
    <title>Vertex Ordering Problems in Directed Graph Streams</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakrabarti:Amit.html">Amit Chakrabarti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Prantar.html">Prantar Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McGregor:Andrew.html">Andrew McGregor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vorotnikova:Sofya.html">Sofya Vorotnikova</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08215">PDF</a><br/><b>Abstract: </b>We consider directed graph algorithms in a streaming setting, focusing on
problems concerning orderings of the vertices. This includes such fundamental
problems as topological sorting and acyclicity testing. We also study the
related problems of finding a minimum feedback arc set (edges whose removal
yields an acyclic graph), and finding a sink vertex. We are interested in both
adversarially-ordered and randomly-ordered streams. For arbitrary input graphs
with edges ordered adversarially, we show that most of these problems have high
space complexity, precluding sublinear-space solutions. Some lower bounds also
apply when the stream is randomly ordered: e.g., in our most technical result
we show that testing acyclicity in the $p$-pass random-order model requires
roughly $n^{1+1/p}$ space. For other problems, random ordering can make a
dramatic difference: e.g., it is possible to find a sink in an acyclic
tournament in the one-pass random-order model using polylog$(n)$ space whereas
under adversarial ordering roughly $n^{1/p}$ space is necessary and sufficient
given $\Theta(p)$ passes. We also design sublinear algorithms for the feedback
arc set problem in tournament graphs; for random graphs; and for randomly
ordered streams. In some cases, we give lower bounds establishing that our
algorithms are essentially space-optimal. Together, our results complement the
much maturer body of work on algorithms for undirected graph streams.
</p></div>
    </summary>
    <updated>2021-05-19T22:57:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08124</id>
    <link href="http://arxiv.org/abs/2105.08124" rel="alternate" type="text/html"/>
    <title>Planar Drawings with Few Slopes of Halin Graphs and Nested Pseudotrees</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chaplick:Steven.html">Steven Chaplick</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lozzo:Giordano_Da.html">Giordano Da Lozzo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giacomo:Emilio_Di.html">Emilio Di Giacomo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liotta:Giuseppe.html">Giuseppe Liotta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Montecchiani:Fabrizio.html">Fabrizio Montecchiani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08124">PDF</a><br/><b>Abstract: </b>The $\textit{planar slope number}$ $psn(G)$ of a planar graph $G$ is the
minimum number of edge slopes in a planar straight-line drawing of $G$. It is
known that $psn(G) \in O(c^\Delta)$ for every planar graph $G$ of degree
$\Delta$. This upper bound has been improved to $O(\Delta^5)$ if $G$ has
treewidth three, and to $O(\Delta)$ if $G$ has treewidth two. In this paper we
prove $psn(G) \in \Theta(\Delta)$ when $G$ is a Halin graph, and thus has
treewidth three. Furthermore, we present the first polynomial upper bound on
the planar slope number for a family of graphs having treewidth four. Namely we
show that $O(\Delta^2)$ slopes suffice for nested pseudotrees.
</p></div>
    </summary>
    <updated>2021-05-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08116</id>
    <link href="http://arxiv.org/abs/2105.08116" rel="alternate" type="text/html"/>
    <title>A Neat Linked Queue with the Rear Sentinel</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xie:Xie.html">Xie Xie</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08116">PDF</a><br/><b>Abstract: </b>We introduce a very simple queue implementation with the singly linked list.
With the help of the rear sentinel instead of the traditional header node, we
avoid additional check steps in the pop operation. The essence of this
representation is the half-opened pointer interval, which can guarantee the
uniform treatment even in the empty queue case. We also present the variants of
linked queue, circularly linked queue and lazy circularly linked queue, which
can also be used to implement stack and improve the time performance in some
special cases.
</p></div>
    </summary>
    <updated>2021-05-19T22:59:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08115</id>
    <link href="http://arxiv.org/abs/2105.08115" rel="alternate" type="text/html"/>
    <title>Topology Optimization for Large-Scale Additive Manufacturing: Generating designs tailored to the deposition nozzle size</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fern=aacute=ndez:Eduardo.html">Eduardo Fernández</a>, Can Ayas, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Langelaar:Matthijs.html">Matthijs Langelaar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Duysinx:Pierre.html">Pierre Duysinx</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08115">PDF</a><br/><b>Abstract: </b>Additive Manufacturing (AM) processes intended for large scale components
deposit large volumes of material to shorten process duration. This reduces the
resolution of the AM process, which is typically defined by the size of the
deposition nozzle. If the resolution limitation is not considered when
designing for Large-Scale Additive Manufacturing (LSAM), difficulties can arise
in the manufacturing process, which may require the adaptation of the
deposition parameters. This work incorporates the nozzle size constraint into
Topology Optimization (TO) in order to generate optimized designs suitable to
the process resolution. This article proposes and compares two methods, which
are based on existing TO techniques that enable control of minimum and maximum
member size, and of minimum cavity size. The first method requires the minimum
and maximum member size to be equal to the deposition nozzle size, thus design
features of uniform width are obtained in the optimized design. The second
method defines the size of the solid members sufficiently small for the
resulting structure to resemble a structural skeleton, which can be interpreted
as the deposition path. Through filtering and projection techniques, the thin
structures are thickened according to the chosen nozzle size. Thus, a topology
tailored to the size of the deposition nozzle is obtained along with a
deposition proposal. The methods are demonstrated and assessed using 2D and 3D
benchmark problems.
</p></div>
    </summary>
    <updated>2021-05-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08113</id>
    <link href="http://arxiv.org/abs/2105.08113" rel="alternate" type="text/html"/>
    <title>A Coupled Alpha Complex</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reani:Yohai.html">Yohai Reani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bobrowski:Omer.html">Omer Bobrowski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08113">PDF</a><br/><b>Abstract: </b>The alpha complex is a subset of the Delaunay triangulation and is often used
in computational geometry and topology. One of the main drawbacks of using the
alpha complex is that it is non-monotone, in the sense that if ${\cal
X}\subset{\cal X}'$ it is not necessarily (and generically not) the case that
the corresponding alpha complexes satisfy ${\cal A}_r({\cal X})\subset{\cal
A}_r({\cal X}')$. The lack of monotonicity may introduce significant
computational costs when using the alpha complex, and in some cases even render
it unusable. In this work we present a new construction based on the alpha
complex, that is homotopy equivalent to the alpha complex while maintaining
monotonicity. We provide the formal definitions and algorithms required to
construct this complex, and to compute its homology. In addition, we analyze
the size of this complex in order to argue that it is not significantly more
costly to use than the standard alpha complex.
</p></div>
    </summary>
    <updated>2021-05-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.08098</id>
    <link href="http://arxiv.org/abs/2105.08098" rel="alternate" type="text/html"/>
    <title>A Scalable Concurrent Algorithm for Dynamic Connectivity</title>
    <feedworld_mtime>1621382400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fedorov:Alexander.html">Alexander Fedorov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koval:Nikita.html">Nikita Koval</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alistarh:Dan.html">Dan Alistarh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.08098">PDF</a><br/><b>Abstract: </b>Dynamic Connectivity is a fundamental algorithmic graph problem, motivated by
a wide range of applications to social and communication networks and used as a
building block in various other algorithms, such as the bi-connectivity and the
dynamic minimal spanning tree problems. In brief, we wish to maintain the
connected components of the graph under dynamic edge insertions and deletions.
In the sequential case, the problem has been well-studied from both theoretical
and practical perspectives. However, much less is known about efficient
concurrent solutions to this problem. This is the gap we address in this paper.
</p>
<p>We start from one of the classic data structures used to solve this problem,
the Euler Tour Tree. Our first contribution is a non-blocking single-writer
implementation of it. We leverage this data structure to obtain the first truly
concurrent generalization of dynamic connectivity, which preserves the time
complexity of its sequential counterpart, but is also scalable in practice. To
achieve this, we rely on three main techniques. The first is to ensure that
connectivity queries, which usually dominate real-world workloads, are
non-blocking. The second non-trivial technique expands the above idea by making
all queries that do not change the connectivity structure non-blocking. The
third ingredient is applying fine-grained locking for updating the connected
components, which allows operations on disjoint components to occur in
parallel.
</p>
<p>We evaluate the resulting algorithm on various workloads, executing on both
real and synthetic graphs. The results show the efficiency of each of the
proposed optimizations; the most efficient variant improves the performance of
a coarse-grained based implementation on realistic scenarios up to 6x on
average and up to 30x when connectivity queries dominate.
</p></div>
    </summary>
    <updated>2021-05-19T22:57:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-6555947.post-8966314939306062304</id>
    <link href="http://blog.geomblog.org/feeds/8966314939306062304/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.geomblog.org/2021/05/transitions.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/6555947/posts/default/8966314939306062304" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/6555947/posts/default/8966314939306062304" rel="self" type="application/atom+xml"/>
    <link href="http://feedproxy.google.com/~r/TheGeomblog/~3/aZaRzVm3CmE/transitions.html" rel="alternate" type="text/html"/>
    <title>Transitions</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I've been at the U of Utah and Salt Lake City for 14 years (14.5 really). It was my first academic job and the longest time I've spent anywhere (throughout my whole life). So it's a little hard to accept that I'm moving to my next adventure. </p><p>It's a two-part adventure, because why make one move when you can make two. </p><p>Firstly, as of today, I'm going to working with Alondra Nelson at the White House Office of Science and Technology Policy, advising on matters relating to fairness and bias in tech systems. This is a scary and exciting new position, and I hope to help to nudge things along just a bit further in the direction of tech that can help more than it harms, especially for those who've been left behind in our rush to an algorithmically controlled future. </p><p>Secondly, I'm moving to Brown University to join the CS department there as well as their Data Science Initiative. Together with Seny Kamara and others, I'm going to start a new center on Computing for the People, to help think through what it means to do computer science that truly responds to the needs of people, instead of hiding behind a neutrality that merely gives more power to those already in power. </p><p>Lots of changes, and because of the pandemic, all this will happen in slow machine, but it's a whirlwind of emotions (and new clothes - apparently tech conference T-shirts don't work in formal settings - WHO KNEW!!!). </p><p><br/></p><img alt="" height="1" src="http://feeds.feedburner.com/~r/TheGeomblog/~4/aZaRzVm3CmE" width="1"/></div>
    </content>
    <updated>2021-05-17T14:33:00Z</updated>
    <published>2021-05-17T14:33:00Z</published><feedburner:origlink xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">http://blog.geomblog.org/2021/05/transitions.html</feedburner:origlink>
    <author>
      <name>Suresh Venkatasubramanian</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/15898357513326041822</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-6555947</id>
      <category term="research"/>
      <category term="community"/>
      <category term="miscellaneous"/>
      <category term="soda"/>
      <category term="conferences"/>
      <category term="data-mining"/>
      <category term="socg"/>
      <category term="blogosphere"/>
      <category term="publishing"/>
      <category term="clustering"/>
      <category term="teaching"/>
      <category term="jobs"/>
      <category term="funding"/>
      <category term="humor"/>
      <category term="awards"/>
      <category term="outreach"/>
      <category term="stoc"/>
      <category term="cs.CG"/>
      <category term="focs"/>
      <category term="nsf"/>
      <category term="reviewing"/>
      <category term="socg-2010"/>
      <category term="fairness"/>
      <category term="academy"/>
      <category term="latex"/>
      <category term="stoc2017"/>
      <category term="theoryfest"/>
      <category term="workshops"/>
      <category term="acm"/>
      <category term="conf-blogs"/>
      <category term="writing"/>
      <category term="cs.DS"/>
      <category term="cs.LG"/>
      <category term="geometry"/>
      <category term="p-vs-nc"/>
      <category term="advising"/>
      <category term="sabbatical"/>
      <category term="simons foundation"/>
      <category term="announcement"/>
      <category term="big-data"/>
      <category term="deadline"/>
      <category term="jeff phillips"/>
      <category term="streaming"/>
      <category term="books"/>
      <category term="large-data"/>
      <category term="p-vs-np"/>
      <category term="cra"/>
      <category term="cstheory"/>
      <category term="focs2010"/>
      <category term="icdm"/>
      <category term="math.PR"/>
      <category term="memorial"/>
      <category term="personal"/>
      <category term="posters"/>
      <category term="potd"/>
      <category term="rajeev motwani"/>
      <category term="shonan"/>
      <category term="socg2012"/>
      <category term="software"/>
      <category term="stoc2012"/>
      <category term="GIA"/>
      <category term="SDM"/>
      <category term="alenex"/>
      <category term="alenex2011"/>
      <category term="arxiv"/>
      <category term="career"/>
      <category term="complexity"/>
      <category term="cs.CC"/>
      <category term="deolalikar"/>
      <category term="distributions"/>
      <category term="madalgo"/>
      <category term="nips"/>
      <category term="sdm2011"/>
      <category term="shape"/>
      <category term="talks"/>
      <category term="technology"/>
      <category term="theory.SE"/>
      <category term="travel"/>
      <category term="video"/>
      <category term="8f-cg"/>
      <category term="DBR"/>
      <category term="ICS"/>
      <category term="LISPI"/>
      <category term="acceptances"/>
      <category term="bibtex"/>
      <category term="bregman"/>
      <category term="cfp"/>
      <category term="clustering-book"/>
      <category term="column"/>
      <category term="combinatorial geometry"/>
      <category term="current-distance"/>
      <category term="ecml-pkdd"/>
      <category term="empirical"/>
      <category term="esa"/>
      <category term="fat*"/>
      <category term="focs2012"/>
      <category term="focs2014"/>
      <category term="fwcg"/>
      <category term="game theory"/>
      <category term="godel"/>
      <category term="graphs"/>
      <category term="implementation"/>
      <category term="journals"/>
      <category term="kernels"/>
      <category term="misc"/>
      <category term="models"/>
      <category term="obituary"/>
      <category term="productivity"/>
      <category term="programming"/>
      <category term="society"/>
      <category term="soda2011"/>
      <category term="topology"/>
      <category term="turing"/>
      <category term="tv"/>
      <category term="women-in-theory"/>
      <category term=".02"/>
      <category term="IMA"/>
      <category term="MOOC"/>
      <category term="PPAD"/>
      <category term="accountability"/>
      <category term="active-learning"/>
      <category term="aggregator"/>
      <category term="algorithms"/>
      <category term="ams"/>
      <category term="analco"/>
      <category term="barriers"/>
      <category term="beamer"/>
      <category term="blogging"/>
      <category term="candes"/>
      <category term="civil rights"/>
      <category term="classification"/>
      <category term="coding-theory"/>
      <category term="coffee"/>
      <category term="conjecture"/>
      <category term="cosmos"/>
      <category term="counting"/>
      <category term="cricket"/>
      <category term="cs.DC"/>
      <category term="dagstuhl"/>
      <category term="databuse"/>
      <category term="dimacs"/>
      <category term="dimensionality-reduction"/>
      <category term="distributed-learning"/>
      <category term="double-blind review"/>
      <category term="duality"/>
      <category term="eda"/>
      <category term="embarrassing"/>
      <category term="ethics"/>
      <category term="expanders"/>
      <category term="experiments"/>
      <category term="fake-news"/>
      <category term="fatml"/>
      <category term="fellowships"/>
      <category term="focs2013"/>
      <category term="fonts"/>
      <category term="gct"/>
      <category term="ggplot"/>
      <category term="gpu"/>
      <category term="graph minors"/>
      <category term="gt.game-theory"/>
      <category term="guest-post"/>
      <category term="guitar"/>
      <category term="hangouts"/>
      <category term="hirsch"/>
      <category term="history"/>
      <category term="ipe"/>
      <category term="ita"/>
      <category term="jmm"/>
      <category term="k-12"/>
      <category term="knuth"/>
      <category term="machine-learning"/>
      <category term="massive"/>
      <category term="math.ST"/>
      <category term="media"/>
      <category term="memes"/>
      <category term="metoo"/>
      <category term="metrics"/>
      <category term="morris"/>
      <category term="movies"/>
      <category term="multicore"/>
      <category term="music"/>
      <category term="narrative"/>
      <category term="networks"/>
      <category term="nih"/>
      <category term="parallelism"/>
      <category term="partha niyogi"/>
      <category term="polymath"/>
      <category term="polymath research"/>
      <category term="polytopes"/>
      <category term="postdocs"/>
      <category term="privacy"/>
      <category term="quant-ph"/>
      <category term="quantum"/>
      <category term="randomness"/>
      <category term="review"/>
      <category term="sampling"/>
      <category term="seminars"/>
      <category term="social-networking"/>
      <category term="soda2014"/>
      <category term="students"/>
      <category term="sublinear"/>
      <category term="submissions"/>
      <category term="summer-school"/>
      <category term="superbowl"/>
      <category term="surveys"/>
      <category term="svn"/>
      <category term="television"/>
      <category term="traffic"/>
      <category term="twitter"/>
      <category term="utah"/>
      <category term="wads"/>
      <category term="white elephant"/>
      <category term="xkcd"/>
      <author>
        <name>Suresh Venkatasubramanian</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/15898357513326041822</uri>
      </author>
      <link href="http://blog.geomblog.org/" rel="alternate" type="text/html"/>
      <link href="http://www.blogger.com/feeds/6555947/posts/default?alt=atom&amp;start-index=26&amp;max-results=25&amp;redirect=false" rel="next" type="application/atom+xml"/>
      <link href="http://feeds.feedburner.com/TheGeomblog" rel="self" type="application/atom+xml"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <subtitle>Ruminations on computational geometry, algorithms, theoretical computer science and life</subtitle>
      <title>The Geomblog</title>
      <updated>2021-05-18T05:33:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7172198197646487541</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7172198197646487541/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/why-do-countries-and-companies-invest.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7172198197646487541" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7172198197646487541" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/why-do-countries-and-companies-invest.html" rel="alternate" type="text/html"/>
    <title>Why do countries and companies invest their own money (or is it?) in Quantum Computing (Non-Rhetorical)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> There have been some recent blog posts by Scott (see <a href="https://www.scottaaronson.com/blog/?p=5387">here</a>) and Lance (see <a href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html">here</a>)  about the hype for SHORT TERM APPLICATIONS of Quantum Computing, which they both object to. </p><p>I have a question that has been touched on but I want to get it more out there.</p><p>PREAMBLE TO QUESTION:  The following scenarios, while distasteful, do make sense: </p><p>a) A researcher on their grants exaggerates or even out-right lies about the applications of their work. </p><p>b) A journalist in their articles exaggerates or even out-right lies about the applications of the science they are covering.</p><p>c) A company exaggerates or even out-right lies about the applications of their project to a venture capitalist or other kind of investor. </p><p>QUESTION: </p><p>Why does a company or country invest THEIR OWN MONEY into Quantum Computing which is unlikely to have a short term profit or benefit? Presumably they hire honest scientists to tell them the limits of the applications in the short term. </p><p>ANSWERS I HAVE GOTTEN FROM ASKING THIS</p><p>1) QC might be able to do something cool and profitable, like factoring, or simulating physics quantum experiments or something else, in the short term. Quantum Crypto is already happening, and that's a close cousin of Quantum Computing. </p><p>2) QC might be able to do something cool and profitable (like in (1)) in the long term, and both companies and countries think they will be around for a long time. (For a list of America's 10 oldest companies see <a href="https://www.businessnewsdaily.com/8122-oldest-companies-in-america.html">here</a>.) </p><p>3) The company or country is in this for the long term, not for a practical project, but because they realize that doing GOOD SCIENCE is of a general benefit (this might make more sense for a country than a company). And funding Quantum Computing is great for science. </p><p>4) Bait and Switch: The company claims they are doing Quantum to attract very smart people to work with them, and then have those smart people do something else.</p><p>5) (this is a variant of 1) While Quantum Computing may not have any short term applications, there will be classical applications INSPIRED by it (this has already happened, see <a href="https://www.scottaaronson.com/blog/?p=3880">here</a>).</p><p>6) Some of these companies make money by applying for GRANTS to do QC, so its NOT their money. In fact, they are using QC to  GET money.</p><p>7) For a country its not the leaders money, its that Taxpayer's money- though that still leaves the question of why spend Taxpayer money on this and not on something else.</p><p>8) For a company its not their money- its Venture Capitalists  and others (though for a big company like Google I would think it IS their money). </p><p>9) The scientists advising the company or country are giving them bad (or at least self-serving) advice so that those scientists can profit- and do good science. So this is a variant of (3) but without the company or country knowing it. </p><p>10) In some countries and companies group-think sets in, so if the leader (who perhaps is not a scientist) thinks intuitively that QC is good, the scientists who work for them, who know better, choose to not speak up, or else they would lose their jobs...or worse. </p><p>11) For countries this could be like going to the moon: Country A wants to beat Country B to the moon for bragging rights. Scientists get to do good research even if they don't care about bragging rights. </p><p>12) (similar to 11 but for a company) If a company does great work on QC then it is good publicity for that company. </p><p>13) Some variant of the <a href="https://en.wikipedia.org/wiki/Greater_fool_theory">greater fool theory</a>. If so, will there be a bubble? A bail-out? </p><p><br/></p></div>
    </content>
    <updated>2021-05-16T17:14:00Z</updated>
    <published>2021-05-16T17:14:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-20T02:09:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5490</id>
    <link href="https://www.scottaaronson.com/blog/?p=5490" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5490#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5490" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">What I told my kids</title>
    <summary xml:lang="en-US">You’ll hear that it’s not as simple as the Israelis are good guys and Palestinians are bad guys, or vice versa. And that’s true. But it’s also not so complicated that there are no clearly identifiable good guys or bad guys. It’s just that they cut across the sides. The good guys are anyone, on […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>You’ll hear that it’s not as simple as the Israelis are good guys and Palestinians are bad guys, or vice versa.  And that’s true.</p>



<p>But it’s also not so complicated that there are no clearly identifiable good guys or bad guys.  It’s just that they cut across the sides.</p>



<p>The good guys are anyone, on either side, whose ideal end state is two countries, Israel and Palestine, living side by side in peace.</p>



<p>The bad guys are anyone, on either side, whose ideal end state is the other side being, if not outright exterminated, then expelled from its current main population centers (ones where it’s been for several generations or more) and forcibly resettled someplace far away.</p>



<p>(And those whose ideal end state is everyone living together with no border — possibly as part of the general abolition of nation-states?   They’re not bad guys; they can plead insanity.  [<strong>Update:</strong> <a href="https://www.scottaaronson.com/blog/?p=5490#comment-1889918">See here</a> for clarifications!])</p>



<p>Hamas are bad guys.  They fire rockets indiscriminately at population centers, hoping to kill as many civilians as they can.  (Unfortunately for them and fortunately for Israel, they’re not great at that, and also they’re aiming at a target that’s world-historically good at defending itself.)</p>



<p>The IDF, whatever else you say about it, sends evacuation warnings to civilians before it strikes the missile centers that are embedded where they live.  Even if Hamas could aim its missiles, the idea of it extending the same courtesy to Israeli civilians is black comedy.</p>



<p>Netanyahu is not as bad as Hamas, because he has the power to kill millions of Palestinians and yet kills only hundreds … whereas if Hamas had the power to kill all Jews, it told the world in its charter that it would immediately do so, and it’s acted consistently with its word.</p>



<p>(An aside: I’m convinced that Hamas has the most top-heavy management structure of any organization in the world.  Every day, Israel takes out another dozen of its <em>most senior, highest-level</em> commanders, apparently leaving hundreds more.  How many senior commanders do they have?  Do they have even a single junior commander?)</p>



<p>Anyway, not being as bad as Hamas is an extremely low bar, and Netanyahu is a thoroughly bad guy.  He’s corrupt and power-mad.  Like Trump, he winks at his side’s monstrous extremists without taking moral responsibility for them.  And if it were ever possible to believe that he wanted two countries as the ideal end state, it hasn’t been possible to believe that for at least a decade.</p>



<p>Netanyahu and Hamas are allies, not enemies.  Both now blatantly, obviously rely on the other to stay in power, to demonstrate their worldview and thereby beat their internal adversaries.</p>



<p>Whenever you see anyone opine about this conflict, on Facebook or Twitter or in an op-ed or anywhere else, keep your focus relentlessly on the question of what that person <em>wants</em>, of what they’d do if they had unlimited power.  If they’re a Zionist who talks about how “there’s no such place as Palestine,” how it’s a newly invented political construct: OK then, does that mean they’d relocate the 5 million self-described Palestinians to Jordan?  Or where?  If, on the other side, someone keeps talking about the “Zionist occupation,” always leaving it strategically unspecified whether they mean just the West Bank and parts of East Jerusalem or also Tel Aviv and Haifa, if they talk about the Nakba (catastrophe) of Israel’s creation in 1947 … OK then, what’s to be done with the 7 million Jews now living there?  Should they go back to the European countries that murdered their families, or the Arab countries that expelled them?  Should the US take them all?  Out with it!</p>



<p>Don’t let them dodge the question.  Don’t let them change the subject to something they’d much rather talk about, like the details of the other side’s latest outrage.  Those details always seem so important, and yet everyone’s stance on every specific outrage is like 80% predictable if you know their desired end state.  So just keep asking directly about their desired end state.</p>



<p>If, like me, you favor two countries living in peace, then you need never fear anyone asking you the same thing.  You can then shout your desired end state from the rooftops, leaving unsettled only the admittedly-difficult “engineering problem” of how to get there.  Crucially, whatever their disagreements or rivalries, everyone trying to solve the same engineering problem is in a certain sense part of the same team.  At least, there’s rarely any reason to kill someone trying to solve the same problem that you are.</p>



<p>“What is this person’s ideal end state?”  Just keep asking that and there’s a limit to how wrong you can ever be about this.  You can still make factual mistakes, but it’s then almost impossible to make a moral mistake.</p></div>
    </content>
    <updated>2021-05-16T02:26:59Z</updated>
    <published>2021-05-16T02:26:59Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-05-19T22:14:09Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/05/15/linkage</id>
    <link href="https://11011110.github.io/blog/2021/05/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Statement of concern from the American Statistical Association over the Greek government’s persecution of former chief statistician Andreas Georgiou (\(\mathbb{M}\)) for (according to the ASA) producing accurate and truthful statistical reports on the Greek economy that cast disrepute on the unverifiable claims of earlier governments.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.amstat.org/asa/News/Greek-Statistician-Found-Liable-for-Slander-Continues-to-Face-Persecution.aspx">Statement of concern from the American Statistical Association over the Greek government’s persecution of former chief statistician Andreas Georgiou</a> (<a href="https://mathstodon.xyz/@11011110/106164322094769198">\(\mathbb{M}\)</a>) for (according to the ASA) producing accurate and truthful statistical reports on the Greek economy that cast disrepute on the unverifiable claims of earlier governments.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/arxivabs">That other microblogging site has a bot specifically devoted to replacing links to pdf versions of arxiv preprints by links to the abstracts of the same preprint</a> (<a href="https://mathstodon.xyz/@11011110/106169351037107488">\(\mathbb{M}\)</a>). Is there something like that for mastodon? If not there should be.</p>
  </li>
  <li>
    <p>For a centrally symmetric star-shaped set in the plane, each line through the center cuts its perimeter into two equal-length curves. But these are not the only shapes with this property: 18th-century Jesuit polymath <a href="https://en.wikipedia.org/wiki/Roger_Joseph_Boscovich">Roger Boscovich</a> observed that a heart-like shape formed by three semicircles has the same property (<a href="https://mathstodon.xyz/@11011110/106175759357772585">\(\mathbb{M}\)</a>, <a href="https://doi.org/10.2307/2974900">via</a>).</p>

    <p style="text-align: center;"><img alt="Boscovich's cardioid, with its perimeter bisected by a line through its cusp" src="https://11011110.github.io/blog/assets/2021/boscovich.svg"/></p>
  </li>
  <li>
    <p><a href="https://picturethismaths.wordpress.com/2020/10/31/three-correlations-and-a-samosa/">Three correlations and a samosa</a> (<a href="https://mathstodon.xyz/@11011110/106180193506681489">\(\mathbb{M}\)</a>, <a href="https://picturethismaths.wordpress.com/2021/02/03/three-correlations-and-other-statistical-models/">see also</a>, <a href="https://picturethismaths.wordpress.com/2021/03/18/three-correlations-and-an-algebraic-classification/">see also</a>). \(3\times 3\) symmetric matrices with unit diagonals form a three-dimensional linear space, in which the samosa is a curvy 3d convex set representing the positive definite matrices. Taking sections of it allows you to infer the possible correlations between two variables, given each of their correlations with a third.</p>
  </li>
  <li>
    <p>I couldn’t resist picking up a copy of <em>The Architecture of Trees</em> (<a href="https://mathstodon.xyz/@11011110/106184861337043968">\(\mathbb{M}\)</a>), a coffee-table book centered on pen-and-ink illustrations of the summer and winter forms of over 200 types of tree, on a recent visit to the Mendocino Coast Botanical Gardens (beautiful this time of year with many flowers in bloom). Some reviews: <a href="https://www.startribune.com/new-book-is-tree-tome-like-few-others-part-science-part-art-marvel/507788702/">1</a>, <a href="https://www.thedailybeast.com/the-architecture-of-trees-travel-with-the-book-that-captures-the-beauty-of-trees">2</a>, <a href="http://spacing.ca/national/2020/08/04/book-review-the-architecture-of-trees/ https://dirt.asla.org/2019/07/10/the-architecture-of-trees-2/">3</a>.</p>
  </li>
  <li>
    <p><a href="http://www.xl-muse.com/html/en/index.php?ac=article&amp;at=read&amp;did=239">Dujiangyan Zhongshuge</a> (<a href="https://mathstodon.xyz/@11011110/106192492039422969">\(\mathbb{M}\)</a>, <a href="https://www.thisiscolossal.com/2021/05/x-living-dujiangyan-zhongshuge/">via</a>), bookstore in Chengdu with mirrored floors and ceilings creating the feeling of an infinite Escher palace of books.</p>
  </li>
  <li>
    <p><a href="http://keenan.is/illustrating/">Illustrating geometry</a> (<a href="https://mathstodon.xyz/@11011110/106204129125568328">\(\mathbb{M}\)</a>). An apparently-defunct blog from 2016–2017 with several interesting posts about technical illustrations in mathematics.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/391885/440">Which \(n\times n\times n\) grids have Hamiltonian cycles that turn at every step?</a> (<a href="https://mathstodon.xyz/@11011110/106218352172585271">\(\mathbb{M}\)</a>) After I linked to this, a later answer pointed to the recent book <em>Bicycle or Unicycle？A Collection of Intriguing Mathematical Puzzles</em>, by Stan Wagon and Daniel Velleman, which has solutions for all even \(n\) on pp. 89–96. A simple parity argument shows that it’s impossible on odd grids, but the same book conjectures that these have Hamiltonian paths except in the case \(n=3\).</p>
  </li>
  <li>
    <p><a href="https://wg2021.mimuw.edu.pl/accepted-papers/">Accepted papers to the International Workshop
on Graph-Theoretic Concepts in Computer Science</a> (<a href="https://mathstodon.xyz/@11011110/106220947095567586">\(\mathbb{M}\)</a>). My paper “<a href="https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable.html">The Graphs of Stably Matchable Pairs</a>” is one of them. The conference will be online June 23–25. Unlike many conferences, WG prepares the proceedings after the conference, to allow authors to incorporate feedback from presentations. Details of how to participate don’t seem to be online but I’m sure they’ll be made available soon.</p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2021/05/13/why-you-shouldnt-be-too-pessimistic/">Why you shouldn’t be too pessimistic</a> (<a href="https://mathstodon.xyz/@11011110/106230884405427138">\(\mathbb{M}\)</a>). Igor Pak on the nature of mathematical conjectures.</p>
  </li>
  <li>
    <p><a href="https://www.sciencemag.org/careers/2021/05/two-surnames-no-hyphen-claiming-my-identity-latin-american-scientist">Two surnames, no hyphen: Claiming my identity as a Latin American scientist</a> (<a href="https://mathstodon.xyz/@11011110/106241178695539953">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/05/15">via</a>). Johana Goyes Vallejos in <em>Science</em>, on respect for diversity in naming styles in academia.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-05-15T17:31:00Z</updated>
    <published>2021-05-15T17:31:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-16T00:57:09Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/05/14/congratulations-dr-havvaei</id>
    <link href="https://11011110.github.io/blog/2021/05/14/congratulations-dr-havvaei.html" rel="alternate" type="text/html"/>
    <title>Congratulations, Dr. Havvaei!</title>
    <summary>It’s getting towards the end of the academic year, that time when students think about finishing up their studies, and today we had another successful Ph.D. defense of one of those students. This time it was one of mine, Elham Havvaei (pronounced like the state of Hawai’i, glottal stop at the apostrophe and all, but with a v instead of w, and better known by her nickname Haleh).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>It’s getting towards the end of the academic year, that time when students think about finishing up their studies, and today we had another successful Ph.D. defense of one of those students. This time it was one of mine, <a href="https://www.ics.uci.edu/~ehavvaei/">Elham Havvaei</a> (pronounced like the state of Hawai’i, glottal stop at the apostrophe and all, but with a v instead of w, and better known by her nickname Haleh).</p>

<p>Haleh is Iranian, but came to UC Irvine via Florida in 2016, after working for a master’s degree with Narsingh Deo at the University of Central Florida. I’ve written here already about the research she’s done with me: <a href="https://11011110.github.io/blog/2018/10/07/recognizing-sparse-leaf.html">reconstructing unknown trees from graphs connecting close-together leaves</a> (from IPEC 2018 and Algorithmica 2020), <a href="https://11011110.github.io/blog/2019/01/29/simplifying-task-milestone.html">simplifying graphs whose vertices and edges represent the milestones, tasks, and ordering constraints of a project</a> (SWAT 2020), and <a href="https://11011110.github.io/blog/2021/01/27/which-induced-subgraph.html">classifying problems of finding large subgraphs with one property in graphs with another property</a> (not yet published).</p>

<p>Her next step will be taking a position at Twitter in San Francisco, as a data scientist.</p>

<p>Congratulations, Haleh!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106236739159044402">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-05-14T18:48:00Z</updated>
    <published>2021-05-14T18:48:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-16T00:57:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/070</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/070" rel="alternate" type="text/html"/>
    <title>TR21-070 |  SOS lower bound for exact planted clique | 

	Shuo Pang</title>
    <summary>We prove a SOS degree lower bound for the planted clique problem on Erd{\"o}s-R\'enyi random graphs $G(n,1/2)$. The bound we get is degree $d=\Omega(\epsilon^2\log n/\log\log n)$ for clique size $\omega=n^{1/2-\epsilon}$, which is almost tight. This improves the result of \cite{barak2019nearly} on the ``soft'' version of the problem, where the family of equality-axioms generated by $x_1+...+x_n=\omega$ was relaxed to one inequality $x_1+...+x_n\geq\omega$.

As a technical by-product, we also ``naturalize'' previous techniques developed for the soft problem. This includes a new way of defining the pseudo-expectation and a more robust method to solve the coarse diagonalization of the moment matrix.</summary>
    <updated>2021-05-13T17:19:11Z</updated>
    <published>2021-05-13T17:19:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-20T03:20:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18720</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/" rel="alternate" type="text/html"/>
    <title>Matrix—The Meeting</title>
    <summary>That’s how it is with people. Nobody cares how it works as long as it works—Councillor Hamann Santosh Vempala and Nikhil Srivastava announced the first in hopefully a series of online meetings about matrix algorithms. Not about the Matrix—the—movie. Santosh and Nikhil said: we expect to have an attendance of people. Wrong. It was over […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>That’s how it is with people. Nobody cares how it works as long as it works—Councillor Hamann</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/vempalasrivastava/" rel="attachment wp-att-18739"><img alt="" class="alignright wp-image-18739" height="128" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/VempalaSrivastava.png?resize=192%2C128&amp;ssl=1" width="192"/></a></p>
<p>
Santosh Vempala and Nikhil Srivastava announced the first in hopefully a series of online meetings about matrix algorithms. Not about the <i>Matrix</i>—the—<a href="https://rjlipton.wpcomstaging.com/feed/">movie</a>. Santosh and Nikhil said: we expect to have an attendance of <img alt="{20-60}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B20-60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> people. Wrong. It was over 200 today.</p>
<p>
Today I thought we would talk about the Zoom meeting and future ones being planned. </p>
<p>
Zoom feels closer to the world of the <i>Matrix</i> movies. If you haven’t seen them, all you need to know is the premise of humanity being diverted in a virtual reality.  How do we know the little figures in those boxes are real people?  More concretely, it seems obvious to Ken and me that simulated human online agents will arrive much earlier than person-like robots.  </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/the-matrix-architects-room/" rel="attachment wp-att-18742"><img alt="" class="alignright wp-image-18742" height="216" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/the-matrix-architects-room.jpg?resize=384%2C216&amp;ssl=1" width="384"/></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2"><i>Matrix Reloaded</i> virtual <a href="https://virtualbackgrounds.site/background/the-matrix-architects-room">background</a><br/>
</font>
</td>
</tr>
</tbody></table>
<p>
In particular, how much does it take to automate giving a lecture online?  Ken has spent much time this term upgrading his lecture notes in two courses to broadcast quality.  Delivering them remotely trades against the spontaneity of drawing pictures on a whiteboard or document camera and developing proofs and algorithms step-by-step.  It should be easier to develop an AI capable of reacting to questions put in Zoom chat than with in-class situations, where “reading the room” is also important for modulating the speed and manner of presentation.  </p>
<p>
</p><h2> The Meetings </h2><p/>
<p>
Daniel Kressner, Mike Mahoney, Cleve Moler, Alex Townsend, and Joel Tropp were also organizers of this smeeting on matrix computation. This Wednesday was the first in a series of online meetings. The speakers for today were Peter Bürgisser, Nick Higham, and Cameron Musco, and the panelists were Jim Demmel, Ilse Ipsen, and Richard Peng.</p>
<p>
The blurb for the meetings is:</p>
<blockquote><p><b> </b> <em> We are organizing an online seminar series on “Complexity of Matrix Computations”, whose goal is to bridge the gap between how numerical linear algebra and theoretical computer science researchers view and study the fundamental computational problems of linear algebra. This gap includes foundational issues such as: what is the computational model? What does it mean to solve a problem? On which criteria do we compare algorithms? We also hope to discuss which techniques in theoretical computer science might be useful in numerical linear algebra and vice versa. </em>
</p></blockquote>
<p>
I love seeing the words “fundamental” and “foundational”, and one question resonated even more.</p>
<p>
</p><p/><h2> The Question </h2><p/>
<p/><p>
What does it mean to solve a problem? In this case what does it mean to solve a linear equation? This is the question that was discussed the most—especially at the end of the meeting. </p>
<p>
I have always thought there is an answer to this. The answer is based on asking what the client wants. Imagine Alice is asked by Bob to <tt>solve</tt> a linear system 	</p>
<p align="center"><img alt="\displaystyle  Ax = b " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Ax+%3D+b+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Alice could go off and return the <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that solves the system. Or she could say there is no such <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Or she could say there are many such <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>‘s. Which is the correct answer? </p>
<p>
I believe the right answer is: Alice should ask Bob:</p>
<blockquote><p><b> </b> <em> Bob, what will you do with the answer to this? </em>
</p></blockquote>
<p>
Bob could say, for example: </p>
<ol>
<li>
I plan to compute the inner product of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> I have. <p/>
</li><li>
I plan to see what the norm of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is. <p/>
</li><li>
Or, I plan to see what <img alt="{x_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is. <p/>
</li><li>
Or, I could be just happy to know that there is some <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. <p/>
</li><li>
Or, and so on.
</li></ol>
<p>
Thus, I believe, the answer only makes sense if Alice knows what will be done next with the “solution”. What do you think?</p>
<p>
</p><p/><h2> One View </h2><p/>
<p/><p>
What does it mean to solve the equation <img alt="{Ax=b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%3Db%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, for an invertible matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? What do precision, accuracy, conditioning, and complexity mean in this context?</p>
<p>
Jim Demmel’s view is captured in his notes that he was kind enough to download to the site <a href="https://app.slack.com/client/T021927P7ST/C021PQXNPEE">SLACK</a>. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What do you think about this series of meetings? Did you attend them initially? Will you look in next time so we can break 200 attendees?</p>
<p>Santosh says: To join the seminar, please send an email<br/>
<a href="mailto: cmc-l-request@cornell.edu">Join Zoom</a><br/>
after adding the subject “join”. Information about how to connect to the Zoom conference call will be circulated via email to all registered attendees prior to each seminar.</p></font></font></div>
    </content>
    <updated>2021-05-13T12:17:45Z</updated>
    <published>2021-05-13T12:17:45Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="matrix"/>
    <category term="online"/>
    <category term="open problems"/>
    <category term="practice"/>
    <category term="Theory"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-05-20T03:21:06Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1929264998273205739</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1929264998273205739/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1929264998273205739" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1929264998273205739" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html" rel="alternate" type="text/html"/>
    <title>Cryptocurrency, Blockchains and NFTs</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I first wrote about bitcoin in this blog <a href="https://blog.computationalcomplexity.org/2011/11/making-money-computationally-hard-way.html">ten years ago</a> after I gave a lecture in a cryptography class I taught at Northwestern. Two years later I had a <a href="https://blog.computationalcomplexity.org/2013/12/bitcoins-revisited.html">follow-up post</a>, noting the price moved from $3 to $1000 with a market cap of about $11 Billion. My brother who thought they were a scam back then has since become a cryptocurrency convert. The bitcoin market cap is now over a trillion dollars and other cryptocurrencies are not far behind. No longer can we view cryptocurrencies as simply a neat exercise in applied cryptography now that it has serious value.</p><p>The main uses of cryptocurrencies are for speculation or illegal activities, such as drug sales, ransoms, money laundering and tax evasion. Sure you can buy a Tesla with bitcoins but that's more of a gimmick. Cryptocurrency spending is simply too slow, expensive and volatile right now to replace other methods of electronic payment. </p><p>Non-fungible tokens (NFTs) truly puzzle me. They are just a digital certificate of authentication. What could you do with them you couldn't do with docusign? Collectibles of publicly available digital goods is a fad already fading.</p><p>I'm not a fan of a fiat currency governed by strict rules not under governmental control. Bad things could happen. However thinking of cryptocurrencies and the blockchain technology that underlies them have brought up real needs for our digital world.</p><p/><ul style="text-align: left;"><li>An easy way to pay online without significant fees, expenses or energy consumption.</li><li>An easy and cheap way to transfer money between different countries.</li><li>A distributed database to allow tracking of supply chains, credentials and financial transactions for example. I see less a need to make these databases decentralized.</li><li>A need, for some, to have a digital replacement for the anonymity of cash.</li><li>People need something to believe in once they have given up believing in religion and a functioning democracy. </li></ul><div>Don't change your investing habits based on anything I write in this post. Speculation and illegal activities are powerful forces. Or it could all collapse. Make your bets, or don't.</div><div><br/></div><div><b>Note</b>: Since I wrote this post yesterday, Elon Musk <a href="https://twitter.com/elonmusk/status/1392602041025843203">tweeted</a> that Tesla will no longer accept bitcoins, and the bitcoin market cap has dropped below a trillion.</div><p/></div>
    </content>
    <updated>2021-05-13T11:55:00Z</updated>
    <published>2021-05-13T11:55:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-20T02:09:36Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry</id>
    <link href="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry.html" rel="alternate" type="text/html"/>
    <title>The constructive solid geometry of piecewise-linear functions</title>
    <summary>My latest preprint, “A stronger lower bound on parametric minimum spanning trees” (arXiv:2105.05371, to appear at WADS) gives examples of graphs, with edge weights that are linear functions of a parameter \(\lambda\), such that different choices of \(\lambda\) lead to \(\Omega(m\log n)\) different minimum spanning trees, improving a bound of \(\Omega\bigl(m\alpha(n)\bigr)\) from one of my earlier papers. But it was almost about a different problem in discrete geometry rather than graph theory, and it almost didn’t happen at all. I thought I had a bound for another related problem until the proof fell apart, irreparably. I was in the process of throwing away my mostly-written draft when I found a different proof, allowing me to rescue the paper.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest preprint, “A stronger lower bound on parametric minimum spanning trees” (<a href="https://arxiv.org/abs/2105.05371">arXiv:2105.05371</a>, to appear at WADS) gives examples of graphs, with edge weights that are linear functions of a parameter \(\lambda\), such that different choices of \(\lambda\) lead to \(\Omega(m\log n)\) different minimum spanning trees, improving a bound of \(\Omega\bigl(m\alpha(n)\bigr)\) from <a href="https://doi.org/10.1007/PL00009396">one of my earlier papers</a>. But it was almost about a different problem in discrete geometry rather than graph theory, and it almost didn’t happen at all. I thought I had a bound for another related problem until the proof fell apart, irreparably. I was in the process of throwing away my mostly-written draft when I found a different proof, allowing me to rescue the paper.</p>

<p>Here’s the problem I thought I was solving when I started writing the paper: Suppose you want to <a href="https://en.wikipedia.org/wiki/Constructive_solid_geometry">construct a complicated shape using unions and intersections of simpler shapes</a>. For the version of the problem I was considering, the shapes belong to the Euclidean plane, and the simple shapes that you start with are the half-planes above a line. When you take unions or intersections of these shapes, the more complicated shapes that you get are sets of points above a piecewise linear \(x\)-monotone curve. Another way to understand the same setup is that you’re starting with linear functions and building more-complicated piecewise linear functions by taking pointwise maxima or minima. And what I wanted to know was: If you have a formula expressing a shape using unions and intersections of \(n\) upper halfplanes, or equivalently expressing a piecewise-linear function using maxima and minima of \(n\) linear functions, how complicated can the result be? I thought I had a proof that one could construct shapes with \(\Omega(n\log n)\) vertices, or piecewise-linear functions with \(\Omega(n\log n)\) breakpoints, and when it broke I thought I didn’t have a paper any more.</p>

<p style="text-align: center;"><img alt="Recursive construction of a piecewise linear function by maxima and minima of simpler functions" src="https://11011110.github.io/blog/assets/2021/minmax.svg"/></p>

<p>The figure above illustrates what I thought was the recursive construction. The base case, in the upper left, is a linear function (a piecewise-linear function with one piece, generated by a max-min formula with one term). In middle left we have a second-level function, the pointwise maximum of two of these linear functions, with two pieces. On the bottom left we have a third-level function, the pointwise minimum of two second-level functions, with six pieces. And the large image on the right shows a fourth-level function, the pointwise maximum of two third-level functions, with 16 pieces. At each level of recursion, you replace each line by two perturbed copies, getting a breakpoint where they cross. When you take a maximum, each breakpoint that looked like a local maximum expands to three breakpoints, while each breakpoint that looked like a local minimum stays as just a single breakpoint; the case of taking a minimum is symmetric. Setting up and solving a recurrence for the numbers of breakpoints of each type gives \(\Omega(n\log n)\).</p>

<p>The problem was that I couldn’t control the resulting piecewise-linear functions well enough to ensure that I could expand all of the local maxima into triple breakpoints and produce new breakpoints for each pair of crossing lines. These two issues are related, because you get a tripled breakpoint only for pairs of pairs of lines that have a certain above-below relation, and the breakpoint of a pair of crossing lines changes their above-below relation. It works for each step in the figure, but that’s because these cases are still too small for the problems to show up. So the analysis above breaks down.</p>

<p>As well as unions and intersections of shapes, or minima and maxima of functions, there’s another graph-theoretical interpretation of the same problem, and that’s where the rewritten paper comes in. The piecewise linear functions that you get from recursive unions and intersections correspond to parametric solutions to the <em>bottleneck shortest path problem</em>: find a path that connects two fixed vertices of a graph, whose heaviest edge is as light as possible, and let \(\beta\) (the bottleneck) be the weight of this edge. How does \(\beta\) vary as a function of \(\lambda\)? For <a href="https://en.wikipedia.org/wiki/Series%E2%80%93parallel_graph">series-parallel graphs</a>, the two vertices should be the two terminals, series composition of graphs gives you the maximum of their bottleneck functions, and parallel composition of graphs gives you the minimum of their bottleneck functions. So for these graphs, the parametric bottleneck shortest path problem is the same one that I didn’t solve.</p>

<p>However, the bottleneck shortest path problem is solved by the minimum spanning tree, in the sense that the path between two vertices in a minimum spanning tree is always a bottleneck shortest path (although there may be other equally good paths). Some of the breakpoints of the bottleneck function, the ones that look locally like a minimum of two linear functions, come from combinatorial changes in the parametric minimum spanning tree, and (by negating everything and swapping min for max if necessary) we can ensure that at least half of the changes in the worst-case bottleneck function come from spanning tree changes in this way. Therefore, lower bounds on the bottleneck problem extend to minimum spanning trees, and upper bounds on minimum spanning trees extend to the bottleneck problem. In fact, my previous \(\Omega\bigl(m\alpha(n)\bigr)\) bound on the spanning tree problem came from a \(\Omega\bigl(n\alpha(n)\bigr)\) bound on two-level piecewise linear functions (minima of maxima of linear functions), and a previous \(O(mn^{1/3})\) <a href="https://doi.org/10.1007/PL00009354">upper bound of Tamal Dey on the spanning tree problem</a> implies an \(O(n^{4/3})\) upper bound on the bottleneck problem.</p>

<p>So when my lower bound for the bottleneck problem fell apart, I instead started thinking about trying to find a similar recursive lower bound for spanning trees instead of bottleneck paths, and was more successful. It works more easily because I don’t have to control the piecewise linear functions so carefully in order to keep their crossings and breakpoints intact; instead, I can just take three copies of the lower level of the construction, flatten them by linear transformations so they are each close to a line, with their breakpoints in disjoint intervals of the \(\lambda\)-axis, and combine them as if they were linear. It wouldn’t work for the bottleneck problem because you would only get a constant number of new breakpoints where one of the recursive copies crosses over to the other, but for the spanning tree problem you’re combining trees rather than functions so you get more breakpoints in these regions.</p>

<p>The figure below gives an example of the construction, a series-parallel graph with six vertices (upper right) and linear edge weight functions (upper left) that produces 12 parametric minimum spanning trees (bottom). The red, blue, and green parts show the three copies of the recursive construction that are combined to form this example. For a more detailed explanation see the preprint.
The preprint also includes a packing argument that transforms the resulting \(\Omega(n\log n)\) bound for \(n\)-vertex series-parallel graphs into an \(\Omega(m\log n)\) bound for graphs whose number \(m\) of edges can be significantly larger than \(n\), but I think that’s more just a technicality.</p>

<p style="text-align: center;"><img alt="Six-vertex series-parallel graph with 12 parametric minimum spanning trees" src="https://11011110.github.io/blog/assets/2021/parametric-mst.svg"/></p>

<p>It would be interesting either to find a different construction proving that the halfspace / piecewise linear function / bottleneck path problem can have complexity \(\Omega(n\log n)\), matching this new result, or to prove an upper bound separating this problem from the lower bound on the parametric minimum spanning tree problem, but that will have to wait for another day.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106225329805495195">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-05-12T17:21:00Z</updated>
    <published>2021-05-12T17:21:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-16T00:57:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/069</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/069" rel="alternate" type="text/html"/>
    <title>TR21-069 |  PPSZ is better than you think | 

	Dominik Scheder</title>
    <summary>PPSZ, for long time the fastest known algorithm for k-SAT, works by going through the variables of the input formula in random order; each variable is then set randomly to 0 or 1, unless the correct value can be inferred by an efficiently implementable rule (like small-width resolution; or being implied by a small set of clauses).
We show that PPSZ performs exponentially better than previously known, for all k &gt;= 3. For Unique-3-SAT we bound its running time by O(1.306973n), which is somewhat better than the algorithm of Hansen, Kaplan, Zamir, and Zwick.
All improvements are achieved without changing the original PPSZ. The core idea is to pretend that PPSZ does not process the variables in uniformly random order, but according to a carefully designed distribution. We write "pretend" since this can be done without any actual change to the algorithm.</summary>
    <updated>2021-05-12T08:47:32Z</updated>
    <published>2021-05-12T08:47:32Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-20T03:20:56Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/glm_saga/</id>
    <link href="https://gradientscience.org/glm_saga/" rel="alternate" type="text/html"/>
    <title>Debuggable Deep Networks: Sparse Linear Models (Part 1)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><!-- <script src="//code.jquery.com/jquery-1.10.2.js"></script> -->


<!-- <script src="https://code.jquery.com/jquery-1.12.4.js"></script> -->
<!-- <script src=https://code.jquery.com/ui/1.12.1/jquery-ui.min.js></script> -->










<p><a class="bbutton" href="https://arxiv.org/abs/2105.04857" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/DebuggableDeepNetworks" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<!-- <a class="bbutton" href="/breeds_class_hierarchy">
<i class="fa fa-tree"></i>
&nbsp;&nbsp; Hierarchies
</a> -->
<br/></p>

<p><i>This two-part series overviews our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on constructing deep networks that perform well while, at the same time, being easier to debug. Part 1 (below) describes our toolkit for building such networks and how it can be leveraged in the context of typical language and vision tasks. This toolkit applies the classical primitive of sparse linear classification on top of feature representations derived from deep networks, and includes a custom solver for fitting such sparse linear models at scale. <a href="https://gradientscience.org/debugging">Part 2</a> outlines a suite of human-in-the-loop experiments that we designed to evaluate the debuggability of such networks. These evaluations demonstrate, in particular, that simply inspecting the sparse final decision layer of these networks can facilitate detection of unintended model behaviours—e.g., spurious correlations and input patterns that cause misclassifications. </i></p>

<p>As ML models are being increasingly deployed in the real world, a question that jumps to the forefront is: how do we know these models are doing “the right thing”? In particular, how can we be sure that models aren’t relying on brittle or undesirable correlations extracted from the data, which undermines their robustness and reliability?</p>

<p>It turns out that, as things stand today, we often can’t. In fact, numerous recent studies have pointed out that seemingly accurate ML models base their predictions on data patterns that are unintuitive or unexpected, leading to a variety of  downstream failures. For instance, in a <a href="https://gradientscience.org/adv/">previous post</a> we discussed how adversarial examples arise because models make decisions based on imperceptible features in the data. There are many other examples of this—e.g., image pathology detection models relying on <a href="https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf">pen marks made by radiologists</a>; and toxic comment classification systems being disproportionately sensitive to <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-group related keywords</a>.</p>

<p>These examples highlight a growing need for model debugging tools: techniques which can facilitate the <i>semi-automatic</i> discovery of such failure modes. In fact, a closely related problem of interpretability—i.e., the task of precisely characterizing how and why models make their decisions, is already a major focus of the ML community.</p>

<h2 id="how-to-debug-your-deep-network">How to debug your deep network?</h2>

<p>A natural approach to model debugging is to inspect the model directly. While this may be feasible in certain settings (e.g., for small linear classifiers or decision trees), it quickly becomes  infeasible as we move towards large, complex models such as deep networks. To work around such scale issues, current approaches (spearheaded in the context of interpretability) attempt to understand  model behavior in a somewhat localized or decomposed manner. In particular, there exist two prominent families of deep network interpretability methods—one that attempts to explain what individual neurons do [<a href="https://arxiv.org/abs/1506.06579">Yosinski et al. 2015</a>, <a href="https://arxiv.org/abs/1704.05796">Bau et al. 2018</a>] and the other one aiming to discern how the model makes decisions for specific inputs [<a href="https://arxiv.org/abs/1312.6034">Simonyan et al. 2013</a>, <a href="https://arxiv.org/abs/1602.04938">Ribeiro et al. 2016</a>]. The challenge however is that, as shown in recent studies [<a href="https://arxiv.org/abs/1810.03292">Adebayo et al., 2018</a>, <a href="https://arxiv.org/abs/2011.05429">Adebayo et al., 2020</a>, <a href="https://arxiv.org/abs/2010.12016">Leavitt &amp; Morcos, 2020</a>], such localized interpretations can be hard to aggregate, are easily fooled, and overall, may not give a clear picture of the model’s reasoning process.</p>

<p>Our work thus takes an alternative approach. First, instead of trying to directly obtain a complete characterization of how and why a deep network makes its decision (which is the goal in  interpretability research), we focus on the more actionable problem of debugging unintended model behaviors. Second, instead of attempting to grapple with the challenge of analyzing these networks in a purely “post hoc” manner, we <i>train</i> them to make them inherently more debuggable.</p>

<p>The specific way we accomplish this goal is motivated by a natural view of a deep network as a composition of a <i>feature extractor</i> and a <i>linear decision layer</i> (see the figure below). From this viewpoint, we can break down the problem of inspecting and understanding a deep network into two subproblems: (1) interpreting the deep features (also known in the literature as neurons—that we will refer to as features henceforth) and (2) understanding how these features are aggregated in the (final) linear decision layer to make predictions.</p>

<p><img alt="Overview" src="https://gradientscience.org/assets/glm_saga/figures/intro.png"/></p>
<div class="footnote">
    <b> Overview of our approach to construct deep networks that are more debuggable:</b> We train a sparse decision layer on (pre-trained) deep feature embeddings and then view the network’s decision process as a linear combination of these features.
</div>

<p>Let us now discuss both of these subproblems in more detail.</p>

<h3 id="task-1-interpreting-deep-features">Task 1: Interpreting (deep) features</h3>

<p>Given the architectural complexity of deep networks, precisely characterizing the role of even a single neuron (in any layer) is challenging. However, research in ML interpretability has brought us a number of heuristics geared towards identifying the input patterns that cause specific neurons (or features) to activate. Thus, for the first task, we leverage some of these existing feature interpretation techniques—specifically, feature visualization, in case of vision models <a href="https://arxiv.org/abs/1904.08939">[Nguyen et al. 2019]</a> and LIME, in case of vision/language models <a href="https://arxiv.org/abs/1602.04938">[Ribeiro et al. 2016]</a>. While these methods have certain limitations, they turn out to be surprisingly effective for model debugging within our framework. Also, note that our approach is fairly modular, and we can substitute these methods with any other/better variants.</p>

<div class="footnote">
    Although LIME was originally used to interpret the predicted outputs of a network, in our work we adapt it to interpret individual neurons instead (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for more details). 
</div>

<p><img alt="Examples of feature visualization" src="https://gradientscience.org/assets/glm_saga/figures/fv_examples_both.png"/></p>
<div class="footnote">
    <b>Examples of feature visualizations for ImageNet classifiers:</b> Feature visualizations for standard vision models (<i>top</i>) are often hard to parse despite significant research on this front. This may be a side effect of these models relying on human-unintelligible features to make their predictions (discussed in a <a href="https://gradientscience.org/adv/">previous post</a>). On the other hand, robust vision models (<i>bottom</i>) tend to have more human-aligned features <a href="https://arxiv.org/abs/1906.00945">[Engstrom et al. 2019]</a>.
</div>

<p><img alt="Examples of word cloud visualization" src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_sst_6.png"/></p>
<div class="footnote">
    <b>Feature interpretation for language models</b>: Examples of a word cloud visualization for the positive and negative activation of a single neuron for a text sentiment classifier. We generate these by aggregating LIME explanations for features, with the whole process described in our <a href="https://arxiv.org/abs/2105.04857">paper</a>. 
</div>

<h3 id="task-2-examining-the-decision-layer">Task 2: Examining the decision layer</h3>

<p>At first glance, the task of making sense of the decision layer of a deep network appears trivial. Indeed, this layer is linear and interpreting a linear model is a routine task in statistical analysis.  However, this intuition is deceptive—the decision layers of modern deep networks often contain upwards of thousands of (deep) features and millions of parameters—making human inspection intractable.</p>

<p><img alt="Feature visualization dump" src="https://gradientscience.org/assets/glm_saga/figures/small_fv_dump.png"/></p>
<div class="footnote">
    <b>Scale of typical decision layers</b>: Feature visualizations for one quarter (512 out of 2048) of all the features of a robust ImageNet classifier. A typical dense decision layer will rely on a weighted sum of <i>all</i> of these features to produce a single prediction.
</div>

<p>So what can we do about this?</p>

<p>Recall that the major roadblock here is the size of the decision layer. What if we just constrained ourselves only to the “important” weights/features within this layer though? Would that allow us to understand the model?</p>

<p>To test this, we focus our attention on the features that are assigned large weights (in terms of magnitude) by the decision layer.  (Note that all the features are standardized to have zero mean and unit variance to make such a weight comparison more meaningful.)</p>

<p>In the figure below, we evaluate the performance of the decision layer when it is restricted to using: (a) only the “important features” or (b) all features <i>but</i> the important ones. The expectation here is that if the important features are to suffice for model debugging, they should at the very least be enough to let the model match its original performance.</p>

<div>
    <div class="ablation_dense">
        <canvas height="200" id="ablation_dense_chart" width="400"/>
    </div>
</div>
<div class="footnote">
     <b>Feature importance in dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. 
</div>

<p>As we can see, this is not the case for typical deep networks. Indeed, for all but one task, the top-k features (k is 10 for vision and 5 for language task) are far from sufficient to recover model performance. Further, there seems to be a great deal of redundancy in the standard decision layer—the model can perform quite well even without using any of the seemingly important features. Clearly, inspecting only the highest-weighted features does not seem to be sufficient from a debugging standpoint.</p>

<h4 id="our-solution-retraining-with-sparsity">Our solution: retraining with sparsity</h4>

<p>To make inspecting the decision layer more tractable for humans and also deal with feature redundancy, we replace that layer entirely. Specifically, rather than finding better heuristics for identifying salient features within the standard (dense) decision layer, we <i>retrain</i> it (on top of the existing feature representations) to be sparse.</p>

<p>To this end, we leverage a classic primitive from statistics: <i>sparse linear classifiers</i>. Concretely, we use the <a href="https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf">elastic net</a> approach to train regularized linear decision layers on top of the fixed (pre-trained) feature representation.</p>

<p>The elastic net is a popular approach for fitting linear models in statistics, that combines the benefits of both L1 and L2 regularization.  Elastic net solvers yield not one but a series of sparse linear models—each with different sparsity/accuracy—based on the strength of regularization. We can then let our application-specific accuracy vs sparsity needs guide our choice of a specific sparse decision layer from this series.</p>

<p>However, when employing this approach to modern deep networks, we hit an obstacle—existing solvers for training regularized linear models simply cannot scale to the number of datapoints and input features that we would typically have in deep learning. To overcome this problem, we develop a custom, efficient solver for fitting regularized generalized linear models at scale. This solver leverages recent advances in <a href="https://arxiv.org/abs/1902.00071">variance reduced gradient methods</a> and combines them with <a href="https://web.stanford.edu/~hastie/Papers/glmnet.pdf">path-algorithms</a> from statistics to get fast and stable convergence at ImageNet scales. We won’t go into much detail here, but we point the curious reader to our <a href="https://arxiv.org/abs/2105.04857">paper</a> and our <a href="https://github.com/madrylab/glm_saga">standalone PyTorch package</a> (which might be of independent interest) for more information.</p>

<p>To summarize—the elastic net gives us a sparse decision layer that, in turn, enables us  to debug the resulting network by applying the existing feature interpretation methods to a now-significantly-reduced number of features (i.e., only the ones used by the sparse decision layer).</p>

<h2 id="what-do-we-gain-from-sparity">What do we gain from sparity?</h2>

<p>Now that we have our methodology in place, we can apply it to standard ML tasks and measure the impact of enforcing sparsity of the final decision layer. Specifically, we discuss the results of applying it to ResNet-50 classifiers trained on ImageNet and Places-10 (a 10-class subset of Places365), as well as BERT models trained on the Stanford Sentiment Treebank and Wikipedia toxic comment classification tasks.</p>

<h3 id="sparsity-at-the-last-layer-is-almost-free">Sparsity at the last layer is (almost) free</h3>

<p>Needless to say, the usefulness of our method hinges on the degree of sparsity in the decision layer that we can achieve without losing much accuracy. So how far can we turn the sparsity dial? The answer turns out to be: <i>a lot</i>! For instance, the final decision layer of an ImageNet classifier with 2048 features can be reduced by two orders of
magnitude, i.e., to use only 20 features per class, at the cost of only 2% test 
accuracy loss.</p>

<p>In the following demonstration, one can move the slider to the right to increase the density of the final decision layer of a standard ImageNet classifier. And, indeed, with only 2% of weights being non-zero, the model can already essentially match the performance (74%) of a fully dense layer.</p>

<div>
    <div id="reg_acc">
        <img id="reg" src="https://gradientscience.org/feed.xml"/>
        <div id="reg_slider"/>
        <div class="quarterblock"> </div>
        <div class="quarterblock" style="text-align: center;">Accuracy: <span id="reg_accuracy"/>%</div>
        <div class="quarterblock" style="text-align: center;">Non-zero: <span id="reg_sparsity"/>%</div>
        <div class="quarterblock"> </div>
    </div>
</div>
<div class="footnote">
    <b>Sparsity-accuracy trade-off:</b> A visualization of the sparsity of an ImageNet decision layer and its corresponding accuracy as a function of the regularization strength. Move the slider all the way to the right to get the fully dense layer (no regularization, 74% accuracy), or all the way to the left to get the fully sparse layer (maximum regularization, 5% accuracy). 
</div>

<h3 id="a-closer-look-at-sparse-decision-layers">A closer look at sparse decision layers</h3>

<p>Our key motivation for constructing sparse decision layers was that it enables us to manually examine the (reduced set of) features that a network uses. As we saw above, our modified decision layers rely on substantially fewer features per class—which already significantly aids their inspection by a human. But what if we go one step further and look only at the “important” features of our sparse decision layer, as we tried to do with the dense decision layer earlier?</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc ablation_button" id="ablation_dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc ablation_button" id="ablation_sparse">Sparse</div>
        </div>
    </div>
    <div class="ablation">
        <canvas height="200" id="ablation_chart" width="400"/>
    </div>
</div>
<div class="footnote">
    <b>Feature importance in sparse and dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. Try toggling between the two to see the effects of sparsity. 
</div>

<p>As we can see below, for models with a sparse decision layer, the top 5-10 important features are necessary and almost sufficient for capturing the model’s performance. That is, (i) accuracy drops to near chance levels (1/number of classes) if the model does not leverage these features and (ii) using these features alone, the model can nearly recover its original performance. This indicates that the sparsity constraint not only reduces the number of features used by the model, but also makes it easier to rank features based on their importance.</p>

<h3 id="sparse-decision-layers-an-interactive-demonstration">Sparse decision layers: an interactive demonstration</h3>

<p>In the following interactive demonstration, you can explore a subset of the decision layer of a (robust) ResNet-50 on ImageNet with either a sparse or dense decision layer:</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc glm_button" id="dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc glm_button" id="sparse">Sparse</div>
        </div>
    </div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc class_button" id="576">Gondola</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="415">Bakery</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="292">Tiger</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="537">Dogsled</div>
        </div>
    </div>
    <div class="" id="linear">
        <div class="block sc" id="glm_class_name">Tiger</div>
        
            
            
            
            
            
            
            
            
            
            
        
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
    </div>
    <div id="zoom">
        <img id="feature_big" src="https://gradientscience.org/feed.xml"/>
        
    </div>
</div>
<div class="footnote">
    <b>An interactive demo of the sparse decision layer:</b> Select a dense or sparse model and a corresponding ImageNet class to visualize the features and weights for the corresponding decision layer. The opacity of each features corresponds to the magnitude of its weight in the decision layer, and you can click on a feature to see a larger version of it. 
</div>

<p>Finally, one should note that the features used by sparse decision layers seem somewhat more human-aligned than the ones used by the standard (dense) decision layers. This observation coupled with our previous ablations studies indicate that sparse decision layers could offer a path towards more debuggable deep networks. But, is this really the case? In our <a href="https://gradientscience.org/debugging">next post</a>, we will evaluate whether models obtained via our methodology are indeed easier for humans to understand, and whether they truly aid the diagnosis of unexpected model behaviors.</p></div>
    </summary>
    <updated>2021-05-12T00:00:00Z</updated>
    <published>2021-05-12T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2021-05-20T00:05:36Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/debugging/</id>
    <link href="https://gradientscience.org/debugging/" rel="alternate" type="text/html"/>
    <title>Debuggable Deep Networks: Usage and Evaluation (Part 2)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><!-- <script src="//code.jquery.com/jquery-1.10.2.js"></script> -->


<!-- <script src="https://code.jquery.com/jquery-1.12.4.js"></script> -->
<!-- <script src=https://code.jquery.com/ui/1.12.1/jquery-ui.min.js></script> -->










<!-- fancybox -->




<p><a class="bbutton" href="https://arxiv.org/abs/2105.04857" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/DebuggableDeepNetworks" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<!-- <a class="bbutton" href="/breeds_class_hierarchy">
<i class="fa fa-tree"></i>
&nbsp;&nbsp; Hierarchies
</a> -->
<br/></p>

<p><i>This is the second part of the overview of our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on training more debuggable deep networks. In our <a href="https://gradientscience.org/glm_saga">previous post</a>, we outlined our toolkit for constructing such networks, which involved training (very) sparse linear classifiers on (pre-trained) deep feature embeddings and viewing the network’s decision process as a linear combination of these features. In this post, we will delve deeper into evaluating to what extent these networks are amenable to debugging. Specifically, we want to get a sense of whether humans are able to intuit their behavior and pinpoint their failure modes.</i></p>

<h2 id="do-our-sparse-decision-layers-truly-aid-human-understanding">Do our sparse decision layers truly aid human understanding?</h2>

<p>Although our toolkit enables us to greatly simplify the network’s decision layer (by reducing the number of its weights and thus the features it relies on), it is not immediately obvious whether this will make debugging such models significantly easier.  To properly examine  this, we need to factor humans into the equation. One way to do that is to leverage the notion of <a href="https://arxiv.org/abs/1606.03490">simulatibility</a> used in the context of ML interpretability. According to this notion, an interpretability method is “good” if it can enable a human to reproduce the model’s decision. In our setup, this translates into evaluating how sparsity of the final decision layer influences humans’ ability to predict the model’s classification decision (irrespective of whether this decision is “correct” or not).</p>

<h4 id="the-simulatibility-study">The “simulatibility” study</h4>

<p>One approach to assess simulatibility  would be to ask annotators to guess what the model will label an input (e.g., an image) as, given an interpretation corresponding to that input. However, for non-expert annotators, this might be challenging due to the large number of (often fine-grained) classes that a typical dataset contains. Additionally, human cognitive biases may also muddle the evaluation—e.g., it may be hard for annotators to decouple “what they think the model should label the input as” from “what the interpretation suggests the model actually does” (and we are interested in the latter).</p>

<p>To alleviate these difficulties, we resort instead to the following task setup (conducted using an ImageNet-trained ResNet):</p>

<ol>
  <li>We pick a target class at random, and show annotators visualizations of five randomly-selected features used by the sparse decision layer to detect objects of this class, along with their relative weights.</li>
  <li>We present the annotators with three images from the validation set with varying (but still non-trivial) probabilities of being classified by the model as the target class. (Note that each of these images can potentially belong to different, non-target classes.)</li>
  <li>Finally, we ask annotators to pick which one among these three images they believe to best match the target class.</li>
</ol>

<div class="footnote">
    As mentioned in <a href="https://gradientscience.org/glm_saga">part 1</a>, feature visualizations for standard vision models are often hard to parse, so we use <a href="https://arxiv.org/abs/1906.00945">adversarially-trained models</a> for this study. 
</div>

<p>Here is a sample task (click to enlarge):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png"><img height="350" src="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png"/></a></p>

<p>Overall, our intention is to gauge whether humans can intuit which image (out of three) is most prototypical for the target class <i>according to the model</i>. Note that we do not show annotators any information about the target class—such as its name or description—other than illustrations of some of the features that the model uses to identify it.  As discussed previously, this is intentional: we want annotators to select the image that <i>visually</i> matches the features used by the model, instead of using their prior knowledge to associate images with the target label itself.  For instance, if the annotators know that the target label was “car”, they might end up choosing the image that most closely resembles their idea of a car—independent of (or even in contradiction to) how the model actually detects cars. In fact, the “most activating image” in our setup may not even belong to the target class.</p>

<p>Now, how well do humans do on this task?</p>

<p>We find that (MTurk) annotators are pretty good at simulating the behavior of our modified networks—they correctly guess the top activating image (out of three) 63% of the time! In contrast, they essentially fail, with only a 35% success rate (i.e., near chance), when this task is performed using models with standard, i.e., dense, decision layers. This suggests that even with a very simple setup—showing non-experts some of the features the sparse decision layer uses to recognize a target class—humans are actually able to emulate the behavior of our modified networks.</p>

<h2 id="debuggability-via-sparsity">Debuggability via Sparsity</h2>

<p>So far, we identified a number of advantages of employing sparse decision layers, such as having fewer components to analyze, selected features being more influential, and better human simulatibility. But what unintended model behaviors can we (semi-automatically) identify by just probing such decision layers?</p>

<h3 id="uncovering-spurious-correlations-and-biases">Uncovering (spurious) correlations and biases</h3>

<p>Let’s start with trying to uncover model biases. After all, it is by now evident that deep networks rely on undesired correlations extracted from the training data (e.g. <a href="https://gradientscience.org/background">backgrounds</a>, <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-related keywords</a>). But can we pinpoint this behavior without resorting to a targeted examination?</p>

<h4 id="bias-in-toxic-comment-classification">Bias in toxic comment classification</h4>

<p>In 2019, Jigsaw hosted a <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">competition on Kaggle</a> around creating  toxic comment detection systems. This effort was prompted by that fact that the systems available at the time were found to have incorrectly learned to associate the names of frequently attacked identities (e.g., nationality, religion or sexual identity) with toxicity, and so the goal of the competition was to construct a
“debiased” system. Can we understand to what extent this effort succeeded?</p>

<p>To answer this question we leverage our methodology and fit a sparse decision layer to the debiased model released by the contest organizers, and then inspect the utilized deep features. An example result is shown below:</p>

<p><img alt="Wordcloud visualization of feature used in unbiased BERT" src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_jigsaw-alt-toxic_6_redacted.png"/></p>
<div class="footnote">
    <b>Interpreting the deep features of a debiased sentiment classifier:</b>
    A word cloud visualization (with some of the words redacted) for a deep feature of the debiased model (with a sparse decision layer). The negative activation of this feature turns out to be influenced by Christianity-related words. 
</div>

<p>Looking at this visualization, we can observe that the debiased model no longer positively associates identity terms with toxicity (refer our <a href="https://arxiv.org/abs/2105.04857">paper</a> for a similar visualization corresponding to the original biased model). This seems to be a success—after all, the goal of the competition was to correct the over-sensitivity of prior models to identity-group keywords. However, upon closer inspection, one will note that the model has actually learned a strong, <i>negative</i> association between these keywords and comment toxicity. For example, one can take a word such as “christianity” and append it to toxic sentences to trick the model into thinking that these are non-toxic 74% of the time. One can try it by selecting words to add to the sentence below:</p>

<div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc toxic_button" id="toxic_" value="">None</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" id="toxic_christianity" value="christianity">+christianity</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" id="toxic_African" value="African">+African</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" id="toxic_Catholic" value="Catholic">+Catholic</div>
        </div>
    </div>
    <div id="toxic_confidence">
        <b>Sentence:</b> Jeez Ed, you seem like a ******* ****** ********* <span id="toxic_add"/>
        <canvas height="200" id="toxic" width="400"/>
    </div>
</div>
<div class="footnote">
    <b>Bias detection in language models: </b> using sparse decision layers we find that the debiased model is still oversensitive to keywords corresponding to frequently attacked identity group, although in the opposite sense from the previous model.
</div>

<p>So, what we see is that rather than being debiased, newer toxic comment detection systems remain disproportionately sensitive to identity terms—it is just the nature of the sensitivity that changed.</p>

<h4 id="spurious-correlations-in-imagenet">Spurious correlations in ImageNet</h4>

<p>In the NLP setting, we can directly measure correlations between the model’s predictions and input data patterns by toggling specific words or phrases in the input corpus. However, it is not obvious how to replicate such analysis in the vision setting. After all, we don’t have automated tools to decompose images into a set of human understandable patterns akin to words or phrases (e.g., “dog ears” or “wheels”).</p>

<p>We thus leverage instead a human-in-the-loop approach that uses (sparse) decision layer inspection as a primitive. Specifically, we enlist annotators on MTurk to identify and describe data patterns that activate individual features that the sparse decision layer uses (for a given class). This in turn allows us to pinpoint the correlations the model has learned between the input data and that class.</p>

<p>Concretely, to identify the data patterns that are positively correlated with a particular (deep) feature, we present to MTurk annotators a set of images that strongly activate it. The expectation here is that if a set of images activate a given  feature, these images should share a common input pattern and the annotators will be able to identify it.</p>

<div class="footnote">
Note that we show annotators images from multiple (two) classes that strongly activate a single feature. This is because images from any single class may have many input patterns in common—only some of which actually activate a specific feature. 
</div>

<p>We then ask annotators: (a) whether they see a common pattern in the images, and, if so, (b) to provide a free text description of that pattern. If the annotators identify a common input pattern, we also ask them if the identified pattern belongs to the class object (“spurious”) or its surroundings (“non-spurious”) for each of the two classes.</p>

<div class="footnote">
In general, we recognize that precisely defining spurious correlations might be challenging and context-dependent. Our definition of spurious correlations was chosen to be objective and easy for annotators to assess.
</div>

<p>Here is an example of the annotation task (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png"><img height="350" src="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png"/></a></p>

<p>Here are a few examples of (spurious) correlations identified by annotators:</p>

<div class="widget">
<span class="widgetheading" id="spurious">Select a class pair:</span>
<div class="choices_one_diff" id="sp"/>
<div class="choices_one_half" id="spuriousimages"> </div>
<div class="choices_one_quarter" id="wcimage"> </div>
<!-- <div class="choices_info">
    <div class="choices_info_text" id="spuriousinfo"> </div>
  </div> -->
</div>
<div style="clear: both;"/>
<div class="footnote">
<b>Detecting input-class correlations in vision models: </b> Select a class pair on the top to see the annotator-provided description for the deep feature that is activated by images of these classes (<i>left</i>). The free-text description provided by the annotators is visualized as a wordcloud (<i>right</i>), along with their selections for whether this input pattern is part of the class object ("non-spurious") or its surroundings ("spurious").
</div>

<p>Note that, one can, in principle, use the same human-in-the-loop methodology to identify input correlations extracted by standard deep networks (with dense decision layers). However, since these models rely on a large number of (deep) features to detect objects of every class, this process can quickly become intractable (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for details).</p>

<p>The above studies demonstrate that for typical vision and NLP tasks, sparsity in the decision layer makes it easier to look deeper into the model and understand what patterns it has extracted from its training corpus.</p>

<h3 id="creating-effective-counterfactuals">Creating effective counterfactuals</h3>

<p>Our second approach for characterizing model failure modes uses the lens of counterfactuals. We specifically focus on counterfactuals that are (minor) variations of given inputs that prompt the model to make a different prediction. Counterfactuals can be very helpful from a debugging standpoint—they can confirm that specific input patterns are not just correlated with the model prediction but actually causally influence them. Additionally, such counterfactuals can be used to provide recourse to users—e.g., to let them realize what attributes (e.g., credit rating) they should change to get the desired outcome (e.g., granting a loan). We will now discuss how to leverage the correlations identified in the previous section to construct counterfactuals for models with sparse decision layers.</p>

<h4 id="language-counterfactuals-in-sentiment-classification">Language counterfactuals in sentiment classification</h4>

<p>In sentiment classification, the task is to label a given sentence as having either positive or negative sentiment. Here, we consider counterfactuals via word substitution, effectively asking “what word could I have used instead to change the sentiment predicted by the model for a given sentence?”</p>

<p>To this end, we consider the words that are positively and negatively correlated with features used by the sparse decision layer as candidates for word substitution. For example, the word “astounding” activates a feature that a BERT model uses to detect positive sentiment, whereas the word “condescending” is negatively correlated with the activation of this feature. By substituting such a positively-correlated word with its negatively-correlated counterpart, we can effectively “flip” the corresponding feature. A demonstration of this process is shown below:</p>

<div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th class="reg_header" colspan="3">Positive activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">impressed</td><td class="positive_cell">brings</td><td class="positive_cell">marvel</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">exhilirated</td><td class="positive_cell main_cell rbutton cf_button">astounding</td><td class="positive_cell">completes</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">hilariously</td><td class="positive_cell">successfully</td><td class="positive_cell">yes</td>
            </tr>
        </tbody></table>
    </div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th class="reg_header" colspan="3">Negative activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">idiots</td><td class="negative_cell rbutton cf_button">inconsistent</td><td class="negative_cell rbutton cf_button">maddening</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">cheat</td><td class="negative_cell rbutton cf_button">condescending</td><td class="negative_cell rbutton cf_button">failure</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">dahmer</td><td class="negative_cell rbutton cf_button">pointless</td><td class="negative_cell rbutton cf_button">unseemly</td>
            </tr>
        </tbody></table>
    </div>
    <div id="sst_counterfactual">
        <b>Sentence:</b> The acting, costumes, music, cinematography and sound are all <i>[<span id="word_counterfactual">astounding</span>]</i> given the proudction's austere locales.
        <canvas height="200" id="sst_canvas" width="400"/>
    </div>
</div>
<div class="footnote">
<b>Language counterfactuals:</b> A wordcloud visualization for a deep feature (used by the sparse decision layer) that positively activates for the  sentence shown above. By replacing the specific word that activated this feature (in this case "astounding"), with any word that  deactivates it (<i>select on the right</i>), we can effectively flip the sentiment predicted by the model. In this way, we can construct counterfactuals for our modified deep networks via one-word substitutions.
</div>

<p>It turns out that these one-word modifications are indeed already quite successful (i.e., they cause a change in the model’s prediction 73% of the time). The obtained sentence pairs—which can be viewed as counterfactuals for one another—allow us to gain insight into data patterns that cause the model to predict a certain outcome. Finally, we find that for standard models finding effective counterfactuals that flip the model’s prediction is harder—the one-word modifications described above can  only change the model’s decision in 52% of cases.</p>

<h4 id="imagenet-counterfactuals">ImageNet counterfactuals</h4>
<p>For ImageNet-trained models, we can directly use the patterns <a href="https://gradientscience.org/feed.xml#spurious-correlations-in-imagenet">previously</a> identified by the annotators to generate counterfactual images that change its prediction. To this end, we manually modify images to add or subtract these patterns and observe the effect of this operation on the model’s decision.</p>

<p>For example, annotators identify a background feature “chainlink fence” to be spuriously
correlated with “ballplayers”. Using this information, we can then take images
of people playing basketball or tennis (correctly labeled as “basketball” or
“racket” by the model) and manually insert a “chainlink fence” into the
background, which successfully changes the model’s prediction to “ballplayer”.</p>

<p><img alt="ImageNet counterfactuals" src="https://gradientscience.org/assets/glm_saga/figures/counterfactuals.png"/></p>
<div class="footnote">
<b>Counterfactuals for ImageNet classifiers:</b> By adding specific spurious patterns to correctly-classified images (<i>top</i>), we can fool the model into predicting the desired class (<i>bottom</i>). 
</div>

<p>Thus, the counterfactuals that our methodology produced indeed allow us to identify data patterns that are causally linked to the model’s decision making process.</p>

<h3 id="identifying-reasons-for-misclassification">Identifying reasons for misclassification</h3>
<p>Finally, we turn our attention to debugging model errors. After all, when our models are wrong, it would be helpful to know why this was the case.</p>

<p>In the ImageNet setting, we find that many (over 30%) of the misclassifications of the 
sparse-decision-layer models can be attributed to a single “problematic”
feature. That is, manually removing this feature results in a correct prediction. One can thus view the feature interpretation for this problematic feature as a justification for the model’s error.</p>

<p><img alt="Problematic features" src="https://gradientscience.org/assets/glm_saga/figures/problematic.png"/></p>
<div class="footnote">
<b>A closer look at ImageNet misclassifications:</b> Examples of erroneously classified ImageNet images (<i>top</i>), along with the feature visualization for the "problematic feature" from the incorrect class (<i>bottom</i>). We find that manually setting the activation of this problematic feature to zero is sufficient to fix the model's mistake in each of these cases.
</div>

<p>Ideally, given such a justification, we would like humans to be able to identify the part of the image corresponding to the problematic feature that caused the model to make a mistake. How can we evaluate whether this is the case?
Namely, can we obtain an unbiased assessment of whether the data patterns that activate the problematic feature are noticeably present in the misclassified image?</p>

<p>To answer this question, we conduct a study on MTurk wherein we present annotators with an image, along with feature visualizations for: (i) the most activated feature from the true class and (ii) the problematic feature that is activated for the erroneous class. We do not explicitly tell the annotators what classes these features correspond to. We then ask annotators to select the patterns (feature visualizations) that match the image, and to determine which pattern is a better match if they select both.</p>

<p>Here is an example of a task we present to the annotators (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png"><img height="350" src="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png"/></a></p>

<div class="footnote">
As a control, we also rerun this experiment while replacing the problematic feature with a randomly-chosen feature. This serves as a baseline to compare annotator selection for the features from the true/incorrect classes. 
</div>

<p>It turns out that not only do annotators frequently (70% of the time) identify the top feature from the wrongly-predicted class as present in the image, but also that this feature is actually a better match than the top feature for the ground truth class (60% of the time). In contrast, annotators select the control (randomly-chosen) deep feature to be a match for the image only 16% of the time. One can explore some examples here:</p>

<div class="widget">
<span class="widgetheading" id="misclass">Inspect misclassified images:</span>
<div class="choices_one_full" id="mis"/>
  <div class="blocktxt" id="mislabels" style="float: none;"> </div>
  <div id="misimages" style="clear: both;"> </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<b>Misclassifications validated by MTurk annotators: </b> Select an image on the top to see its true and predicted labels, along with the most highly activated deep feature (of those used by the sparse decision layer) for both these classes. In all cases, annotators select the top feature from the (incorrect) predicted class to be present in the image, and to be a better match than the top feature from the true class.
</div>

<p>This experiment validates (devoid of confirmation biases from the class label) that humans can identify the data patterns that trigger the error-inducing problematic deep features. Note that once these patterns have been identified, one can examine them to better understand the root cause (e.g., issues with the training data) for model errors.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Over the course of this two-part series, we have shown that a natural approach of fitting sparse linear models over deep feature representations can already be surprisingly effective in creating more debuggable deep networks. In particular, we saw that models constructed using this methodology are more concise and amenable to human understanding—making it easier to detect and analyze unintended behaviors such as biases and misclassification. Going forward, this methodology of modifying the network architecture to make it inherently easier to probe can offer an attractive alternative to the existing paradigm of purely post-hoc debugging. Additionally, our analysis introduces a suite of human-in-the-loop techniques for model debugging at scale and thus can help guide further work in this field.</p>







<span class="choices_info_text"/><br/><span class="choices_info_text" style="color: red;"><b/></span><br/><span class="choices_info_text" style="color: green;"><b/></span><br/><hr/><h3 style="text-align: center;"><h3><div class="sp_txt" style="text-align: center; font-weight: 300; margin: 0.75em auto;"/><div class="wc_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;wc_&quot; + pair + &quot;.png"/><hr style="margin: 0.75em auto;"/><div class="sp_txt" style="text-align: center; font-weight: 200;"><span/></div><hr style="margin: 0.3em auto;"/><h3 style="text-align: center;"><h3><div class="sp_txt" style="text-align: center;"><span style="font-weight: 200;"/></div><br/><span/></h3></h3></div><div class="sp_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;sample_&quot; + pair + &quot;_&quot; + i + &quot;.png"/></div><div class="mis_txt blocktxt thirdblock"><span class="widgetheading"/><span class="choices_info_text"/><br/><span class="choices_info_text"/><div class="mis_txt blocktxt thirdblock"><br/><span class="widgetheading"/></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + origSrc + &quot;"/></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + base +                     &quot;dst_&quot; + pair + &quot;_&quot; + i + &quot;.png"/></div></div></h3></h3></div>
    </summary>
    <updated>2021-05-12T00:00:00Z</updated>
    <published>2021-05-12T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2021-05-20T00:05:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8117</id>
    <link href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/" rel="alternate" type="text/html"/>
    <title>STOC Test of time award</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A reminder: the deadline to submit nominations for the STOC Test of Time Award is May 24. You can nominate papers for the 10 year award – STOC 2007-2011 20 year award – STOC 1997-2001 30 year award – STOC 1987-1991 The award website ( https://sigact.org/prizes/stoc_tot.html ) helpfully contains links to the papers published in … <a class="more-link" href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/">Continue reading <span class="screen-reader-text">STOC Test of time award</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A reminder: the deadline to submit nominations for the <a href="https://sigact.org/prizes/stoc_tot.html">STOC Test of Time Award</a> is <strong>May 24</strong>.  You can nominate papers for the </p>



<ul><li>10 year award – STOC 2007-2011</li><li>20 year award – STOC 1997-2001</li><li>30 year award – STOC 1987-1991<br/><br/>The award website ( <a href="https://sigact.org/prizes/stoc_tot.html">https://sigact.org/prizes/stoc_tot.html </a>) helpfully contains links to the papers published in all these conferences. <br/><br/>Please nominate the papers you think have most influenced our field!</li></ul>



<p/></div>
    </content>
    <updated>2021-05-11T18:28:47Z</updated>
    <published>2021-05-11T18:28:47Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-05-20T03:21:24Z</updated>
    </source>
  </entry>
</feed>
