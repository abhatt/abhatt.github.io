<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-07-18T21:39:39Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/104</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/104" rel="alternate" type="text/html"/>
    <title>TR21-104 |  Does QRAT simulate IR-calc? QRAT simulation algorithm for $\forall$Exp+Res cannot be lifted to IR-calc | 

	Sravanthi Chede, 

	Anil Shukla</title>
    <summary>We show that the QRAT simulation algorithm of $\forall$Exp+Res from [B. Kiesl and M. Seidl, 2019] cannot be lifted to IR-calc.</summary>
    <updated>2021-07-18T16:15:18Z</updated>
    <published>2021-07-18T16:15:18Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-18T21:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/103</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/103" rel="alternate" type="text/html"/>
    <title>TR21-103 |  Elliptic Curve Fast Fourier Transform (ECFFT) Part I: Fast Polynomial Algorithms over all Finite Fields | 

	Eli Ben-Sasson, 

	Dan Carmon, 

	Swastik Kopparty, 

	David Levit</title>
    <summary>Over finite fields $F_q$ containing a root of unity of smooth order $n$ (smoothness means $n$ is the product of small primes), the Fast Fourier Transform (FFT) leads to the fastest known algebraic algorithms for many basic polynomial operations, such as multiplication, division, interpolation and multi-point evaluation. These operations can be computed by constant fan-in arithmetic circuits over $F_q$ of quasi-linear size; specifically, $O(n \log n)$ for multiplication and division, and $O(n \log^2 n)$ for interpolation and evaluation.

However, the same operations over fields with no smooth order root of unity suffer from an asymptotic slowdown, typically due to the need to introduce “synthetic” roots of unity to enable the FFT. The classical algorithm of Schönhage and Strassen incurred a multiplicative slowdown factor of $\log \log n$ on top of the smooth case. Recent remarkable results of Harvey, van der Hoeven and Lecerf dramatically reduced this multiplicative overhead to $\exp(\log^* (n))$.

We introduce a new approach to fast algorithms for polynomial operations over all large finite fields. The key idea is to replace the group of roots of unity with a set of points $L \subset F_q$ suitably related to a well-chosen elliptic curve group over $F_q$ (the set L itself is not a group). The key advantage of this approach is that elliptic curve groups can be of any size in the Hasse–Weil interval $[q + 1 \pm 2\sqrt{q}]$ and thus can have subgroups of large, smooth order, which an FFT-like divide and conquer algorithm can exploit. Compare this with multiplicative subgroups over $F_q$ whose order must divide $q-1$. By analogy, our method extends the standard, multiplicative FFT in a similar way to how Lenstra’s elliptic curve method extended Pollard’s $p-1$ algorithm for factoring integers.

For polynomials represented by their evaluation over subsets of $L$, we show that  multiplication, division, degree-computation, interpolation, evaluation and Reed–Solomon encoding (also known as low-degree extension) with fixed evaluation points can all be computed with arithmetic circuits of size similar to what is achievable with the classical FFTs when the field size $q$ is special. For several problems, this yields the asymptotically smallest known arithmetic circuits even in the standard monomial representation of polynomials.

The efficiency of the classical FFT follows from using the 2-to-1 squaring map to reduce the evaluation set of roots of unity of order $2^k$ to similar groups of size $2^{k?i}$, $i &gt; 0$. Our algorithms operate similarly, using isogenies of elliptic curves with kernel size 2 as 2-to-1 maps to reduce $L$ of size $2^k$ to sets of size $2^{k?i}$ that are, like $L$, suitably related to elliptic curves, albeit different ones.</summary>
    <updated>2021-07-18T14:07:56Z</updated>
    <published>2021-07-18T14:07:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-18T21:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/</id>
    <link href="https://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/" rel="alternate" type="text/html"/>
    <title>5th SIAM Symposium on Simplicity in Algorithms</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">January 10-11, 2022 Alexandria, Virginia, U.S. https://www.siam.org/conferences/cm/conference/sosa22 Submission deadline: August 9, 2021 Symposium on Simplicity in Algorithms is a conference in theoretical computer science dedicated to advancing algorithms research by promoting simplicity and elegance in the design and analysis of algorithms. The benefits of simplicity are manifold: simpler algorithms manifest a better understanding of the … <a class="more-link" href="https://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/">Continue reading <span class="screen-reader-text">5th SIAM Symposium on Simplicity in Algorithms</span></a></div>
    </summary>
    <updated>2021-07-18T13:15:17Z</updated>
    <published>2021-07-18T13:15:17Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-07-18T21:39:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07480</id>
    <link href="http://arxiv.org/abs/2107.07480" rel="alternate" type="text/html"/>
    <title>Newton-LESS: Sparsification without Trade-offs for the Sketched Newton Update</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Michał Dereziński, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lacotte:Jonathan.html">Jonathan Lacotte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pilanci:Mert.html">Mert Pilanci</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahoney:Michael_W=.html">Michael W. Mahoney</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07480">PDF</a><br/><b>Abstract: </b>In second-order optimization, a potential bottleneck can be computing the
Hessian matrix of the optimized function at every iteration. Randomized
sketching has emerged as a powerful technique for constructing estimates of the
Hessian which can be used to perform approximate Newton steps. This involves
multiplication by a random sketching matrix, which introduces a trade-off
between the computational cost of sketching and the convergence rate of the
optimization algorithm. A theoretically desirable but practically much too
expensive choice is to use a dense Gaussian sketching matrix, which produces
unbiased estimates of the exact Newton step and which offers strong
problem-independent convergence guarantees. We show that the Gaussian sketching
matrix can be drastically sparsified, significantly reducing the computational
cost of sketching, without substantially affecting its convergence properties.
This approach, called Newton-LESS, is based on a recently introduced sketching
technique: LEverage Score Sparsified (LESS) embeddings. We prove that
Newton-LESS enjoys nearly the same problem-independent local convergence rate
as Gaussian embeddings, not just up to constant factors but even down to lower
order terms, for a large class of optimization tasks. In particular, this leads
to a new state-of-the-art convergence result for an iterative least squares
solver. Finally, we extend LESS embeddings to include uniformly sparsified
random sign matrices which can be implemented efficiently and which perform
well in numerical experiments.
</p></div>
    </summary>
    <updated>2021-07-17T22:44:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07403</id>
    <link href="http://arxiv.org/abs/2107.07403" rel="alternate" type="text/html"/>
    <title>Local Search for Weighted Tree Augmentation and Steiner Tree</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Traub:Vera.html">Vera Traub</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zenklusen:Rico.html">Rico Zenklusen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07403">PDF</a><br/><b>Abstract: </b>We present a technique that allows for improving on some relative greedy
procedures by well-chosen (non-oblivious) local search algorithms. Relative
greedy procedures are a particular type of greedy algorithm that start with a
simple, though weak, solution, and iteratively replace parts of this starting
solution by stronger components. Some well-known applications of relative
greedy algorithms include approximation algorithms for Steiner Tree and, more
recently, for connectivity augmentation problems.
</p>
<p>The main application of our technique leads to a
$(1.5+\epsilon)$-approximation for Weighted Tree Augmentation, improving on a
recent relative greedy based method with approximation factor $1+\ln 2 +
\epsilon\approx 1.69$. Furthermore, we show how our local search technique can
be applied to Steiner Tree, leading to an alternative way to obtain the
currently best known approximation factor of $\ln 4 + \epsilon$. Contrary to
prior methods, our approach is purely combinatorial without the need to solve
an LP. Nevertheless, the solution value can still be bounded in terms of the
well-known hypergraphic LP, leading to an alternative, and arguably simpler,
technique to bound its integrality gap by $\ln 4$.
</p></div>
    </summary>
    <updated>2021-07-17T22:56:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07387</id>
    <link href="http://arxiv.org/abs/2107.07387" rel="alternate" type="text/html"/>
    <title>Scheme-theoretic Approach to Computational Complexity II. The Separation of P and NP over $\mathbb{C}$, $\mathbb{R}$, and $\mathbb{Z}$</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ali Çivril <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07387">PDF</a><br/><b>Abstract: </b>We show that the problem of determining the feasibility of quadratic systems
over $\mathbb{C}$, $\mathbb{R}$, and $\mathbb{Z}$ requires exponential time.
This separates P and NP over these fields/rings in the BCSS model of
computation.
</p></div>
    </summary>
    <updated>2021-07-17T22:42:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07386</id>
    <link href="http://arxiv.org/abs/2107.07386" rel="alternate" type="text/html"/>
    <title>Scheme-theoretic Approach to Computational Complexity I. The Separation of P and NP</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ali Çivril <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07386">PDF</a><br/><b>Abstract: </b>We lay the foundations of a new theory for algorithms and computational
complexity by parameterizing the instances of a computational problem as a
moduli scheme. Considering the geometry of the scheme associated to 3-SAT, we
separate P and NP.
</p></div>
    </summary>
    <updated>2021-07-17T22:42:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07383</id>
    <link href="http://arxiv.org/abs/2107.07383" rel="alternate" type="text/html"/>
    <title>Lossy Kernelization of Same-Size Clustering</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bandyapadhyay:Sayan.html">Sayan Bandyapadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovach:Petr_A=.html">Petr A. Golovach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Purohit:Nidhi.html">Nidhi Purohit</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Simonov:Kirill.html">Kirill Simonov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07383">PDF</a><br/><b>Abstract: </b>In this work, we study the $k$-median clustering problem with an additional
equal-size constraint on the clusters, from the perspective of parameterized
preprocessing. Our main result is the first lossy ($2$-approximate) polynomial
kernel for this problem, parameterized by the cost of clustering. We complement
this result by establishing lower bounds for the problem that eliminate the
existences of an (exact) kernel of polynomial size and a PTAS.
</p></div>
    </summary>
    <updated>2021-07-17T22:57:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07379</id>
    <link href="http://arxiv.org/abs/2107.07379" rel="alternate" type="text/html"/>
    <title>Spectral Processing and Optimization of Static and Dynamic 3D Geometries</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Arvanitis:Gerasimos.html">Gerasimos Arvanitis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07379">PDF</a><br/><b>Abstract: </b>Geometry processing of 3D objects is of primary interest in many areas of
computer vision and graphics, including robot navigation, 3D object
recognition, classification, feature extraction, etc. The recent introduction
of cheap range sensors has created a great interest in many new areas, driving
the need for developing efficient algorithms for 3D object processing.
Previously, in order to capture a 3D object, expensive specialized sensors were
used, such as lasers or dedicated range images, but now this limitation has
changed. The current approaches of 3D object processing require a significant
amount of manual intervention and they are still time-consuming making them
unavailable for use in real-time applications. The aim of this thesis is to
present algorithms, mainly inspired by the spectral analysis, subspace
tracking, etc, that can be used and facilitate many areas of low-level 3D
geometry processing (i.e., reconstruction, outliers removal, denoising,
compression), pattern recognition tasks (i.e., significant features extraction)
and high-level applications (i.e., registration and identification of 3D
objects in partially scanned and cluttered scenes), taking into consideration
different types of 3D models (i.e., static and dynamic point clouds, static and
dynamic 3D meshes).
</p></div>
    </summary>
    <updated>2021-07-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07359</id>
    <link href="http://arxiv.org/abs/2107.07359" rel="alternate" type="text/html"/>
    <title>Efficient M\"obius Transformations and their applications to Dempster-Shafer Theory: Clarification and implementation</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chaveroche:Maxime.html">Maxime Chaveroche</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davoine:Franck.html">Franck Davoine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cherfaoui:V=eacute=ronique.html">Véronique Cherfaoui</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07359">PDF</a><br/><b>Abstract: </b>Dempster-Shafer Theory (DST) generalizes Bayesian probability theory,
offering useful additional information, but suffers from a high computational
burden. A lot of work has been done to reduce the complexity of computations
used in information fusion with Dempster's rule. The main approaches exploit
either the structure of Boolean lattices or the information contained in belief
sources. Each has its merits depending on the situation. In this paper, we
propose sequences of graphs for the computation of the zeta and M\"obius
transformations that optimally exploit both the structure of distributive
semilattices and the information contained in belief sources. We call them the
Efficient M\"obius Transformations (EMT). We show that the complexity of the
EMT is always inferior to the complexity of algorithms that consider the whole
lattice, such as the Fast M\"obius Transform (FMT) for all DST transformations.
We then explain how to use them to fuse two belief sources. More generally, our
EMTs apply to any function in any finite distributive lattice, focusing on a
meet-closed or join-closed subset. This article extends our work published at
the international conference on Scalable Uncertainty Management (SUM). It
clarifies it, brings some minor corrections and provides implementation details
such as data structures and algorithms applied to DST.
</p></div>
    </summary>
    <updated>2021-07-17T22:39:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07358</id>
    <link href="http://arxiv.org/abs/2107.07358" rel="alternate" type="text/html"/>
    <title>A Refined Approximation for Euclidean k-Means</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Fabrizio Grandoni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ostrovsky:Rafail.html">Rafail Ostrovsky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rabani:Yuval.html">Yuval Rabani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schulman:Leonard_J=.html">Leonard J. Schulman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Venkat:Rakesh.html">Rakesh Venkat</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07358">PDF</a><br/><b>Abstract: </b>In the Euclidean $k$-Means problem we are given a collection of $n$ points
$D$ in an Euclidean space and a positive integer $k$. Our goal is to identify a
collection of $k$ points in the same space (centers) so as to minimize the sum
of the squared Euclidean distances between each point in $D$ and the closest
center. This problem is known to be APX-hard and the current best approximation
ratio is a primal-dual $6.357$ approximation based on a standard LP for the
problem [Ahmadian et al. FOCS'17, SICOMP'20].
</p>
<p>In this note we show how a minor modification of Ahmadian et al.'s analysis
leads to a slightly improved $6.12903$ approximation. As a related result, we
also show that the mentioned LP has integrality gap at least
$\frac{16+\sqrt{5}}{15}&gt;1.2157$.
</p></div>
    </summary>
    <updated>2021-07-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07347</id>
    <link href="http://arxiv.org/abs/2107.07347" rel="alternate" type="text/html"/>
    <title>Sparse Fourier Transform by traversing Cooley-Tukey FFT computation graphs</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bringmann:Karl.html">Karl Bringmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kapralov:Michael.html">Michael Kapralov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makarov:Mikhail.html">Mikhail Makarov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakos:Vasileios.html">Vasileios Nakos</a>, Amir Yagudin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zandieh:Amir.html">Amir Zandieh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07347">PDF</a><br/><b>Abstract: </b>Computing the dominant Fourier coefficients of a vector is a common task in
many fields, such as signal processing, learning theory, and computational
complexity. In the Sparse Fast Fourier Transform (Sparse FFT) problem, one is
given oracle access to a $d$-dimensional vector $x$ of size $N$, and is asked
to compute the best $k$-term approximation of its Discrete Fourier Transform,
quickly and using few samples of the input vector $x$. While the sample
complexity of this problem is quite well understood, all previous approaches
either suffer from an exponential dependence of runtime on the dimension $d$ or
can only tolerate a trivial amount of noise. This is in sharp contrast with the
classical FFT algorithm of Cooley and Tukey, which is stable and completely
insensitive to the dimension of the input vector: its runtime is $O(N\log N)$
in any dimension $d$.
</p>
<p>In this work, we introduce a new high-dimensional Sparse FFT toolkit and use
it to obtain new algorithms, both on the exact, as well as in the case of
bounded $\ell_2$ noise. This toolkit includes i) a new strategy for exploring a
pruned FFT computation tree that reduces the cost of filtering, ii) new
structural properties of adaptive aliasing filters recently introduced by
Kapralov, Velingker and Zandieh'SODA'19, and iii) a novel lazy estimation
argument, suited to reducing the cost of estimation in FFT tree-traversal
approaches. Our robust algorithm can be viewed as a highly optimized sparse,
stable extension of the Cooley-Tukey FFT algorithm.
</p>
<p>Finally, we explain the barriers we have faced by proving a conditional
quadratic lower bound on the running time of the well-studied non-equispaced
Fourier transform problem. This resolves a natural and frequently asked
question in computational Fourier transforms. Lastly, we provide a preliminary
experimental evaluation comparing the runtime of our algorithm to FFTW and SFFT
2.0.
</p></div>
    </summary>
    <updated>2021-07-17T22:42:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07183</id>
    <link href="http://arxiv.org/abs/2107.07183" rel="alternate" type="text/html"/>
    <title>Streaming Submodular Maximization with Matroid and Matching Constraints</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldman:Moran.html">Moran Feldman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Norouzi=Fard:Ashkan.html">Ashkan Norouzi-Fard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svensson:Ola.html">Ola Svensson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zenklusen:Rico.html">Rico Zenklusen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07183">PDF</a><br/><b>Abstract: </b>Recent progress in (semi-)streaming algorithms for monotone submodular
function maximization has led to tight results for a simple cardinality
constraint. However, current techniques fail to give a similar understanding
for natural generalizations such as matroid and matching constraints. This
paper aims at closing this gap. For a single matroid of rank $k$ (i.e., any
solution has cardinality at most $k$), our main results are:
</p>
<p>$\bullet$ A single-pass streaming algorithm that uses $\widetilde{O}(k)$
memory and achieves an approximation guarantee of 0.3178.
</p>
<p>$\bullet$ A multi-pass streaming algorithm that uses $\widetilde{O}(k)$
memory and achieves an approximation guarantee of $(1-1/e - \varepsilon)$ by
taking constant number of passes over the stream.
</p>
<p>This improves on the previously best approximation guarantees of 1/4 and 1/2
for single-pass and multi-pass streaming algorithms, respectively. In fact, our
multi-pass streaming algorithm is tight in that any algorithm with a better
guarantee than 1/2 must make several passes through the stream and any
algorithm that beats our guarantee $1-1/e$ must make linearly many passes.
</p>
<p>For the problem of maximizing a monotone submodular function subject to a
bipartite matching constraint (which is a special case of matroid
intersection), we show that it is not possible to obtain better than
0.3715-approximation in a single pass, which improves over a recent
inapproximability of 0.522 for this problem. Furthermore, given a plausible
assumption, our inapproximability result improves to $1/3 \approx 0.333$.
</p></div>
    </summary>
    <updated>2021-07-17T22:47:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07141</id>
    <link href="http://arxiv.org/abs/2107.07141" rel="alternate" type="text/html"/>
    <title>An Efficient Semi-Streaming PTAS for Tournament Feedback ArcSet with Few Passes</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Baweja:Anubhav.html">Anubhav Baweja</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jia:Justin.html">Justin Jia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07141">PDF</a><br/><b>Abstract: </b>We present the first semi-streaming PTAS for the minimum feedback arc set
problem on directed tournaments in a small number of passes. Namely, we obtain
a $(1 + \varepsilon)$-approximation in polynomial time $O \left( \text{poly}(n)
2^{\text{poly}(1/\varepsilon)} \right)$, with $p$ passes in $n^{1+1/p} \cdot
\text{poly}\left(\frac{\log n}{\varepsilon}\right)$ space. The only previous
algorithm with this pass/space trade-off gave a $3$-approximation (SODA, 2020),
and other polynomial-time algorithms which achieved a
$(1+\varepsilon)$-approximation did so with quadratic memory or with a linear
number of passes. We also present a new time/space trade-off for $1$-pass
algorithms that solve the tournament feedback arc set problem. This problem has
several applications in machine learning such as creating linear classifiers
and doing Bayesian inference. We also provide several additional algorithms and
lower bounds for related streaming problems on directed graphs, which is a
mostly unexplored territory.
</p></div>
    </summary>
    <updated>2021-07-17T22:52:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07103</id>
    <link href="http://arxiv.org/abs/2107.07103" rel="alternate" type="text/html"/>
    <title>Multilinear extension of $k$-submodular functions</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Baoxiang.html">Baoxiang Wang</a>, Huanjian Zhou <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07103">PDF</a><br/><b>Abstract: </b>A $k$-submodular function is a function that given $k$ disjoint subsets
outputs a value that is submodular in every orthant. In this paper, we provide
a new framework for $k$-submodular maximization problems, by relaxing the
optimization to the continuous space with the multilinear extension of
$k$-submodular functions and a variant of pipage rounding that recovers the
discrete solution. The multilinear extension introduces new techniques to
analyze and optimize $k$-submodular functions.
</p>
<p>When the function is monotone, we propose almost $\frac{1}{2}$-approximation
algorithms for unconstrained maximization and maximization under total size and
knapsack constraints. For unconstrained monotone and non-monotone maximization,
we propose an algorithm that is almost as good as any combinatorial algorithm
based on Iwata, Tanigawa, and Yoshida's meta-framework
($\frac{k}{2k-1}$-approximation for the monotone case and
$\frac{k^2+1}{2k^2+1}$-approximation for the non-monotone case).
</p></div>
    </summary>
    <updated>2021-07-17T22:42:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.07038</id>
    <link href="http://arxiv.org/abs/2107.07038" rel="alternate" type="text/html"/>
    <title>Conditional Teaching Size</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Manuel Garcia-Piqueras, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hern=aacute=ndez=Orallo:Jos=eacute=.html">José Hernández-Orallo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.07038">PDF</a><br/><b>Abstract: </b>Recent research in machine teaching has explored the instruction of any
concept expressed in a universal language. In this compositional context, new
experimental results have shown that there exist data teaching sets
surprisingly shorter than the concept description itself. However, there exists
a bound for those remarkable experimental findings through teaching size and
concept complexity that we further explore here. As concepts are rarely taught
in isolation we investigate the best configuration of concepts to teach a given
set of concepts, where those that have been acquired first can be reused for
the description of new ones. This new notion of conditional teaching size
uncovers new insights, such as the interposition phenomenon: certain prior
knowledge generates simpler compatible concepts that increase the teaching size
of the concept that we want to teach. This does not happen for conditional
Kolmogorov complexity. Furthermore, we provide an algorithm that constructs
optimal curricula based on interposition avoidance. This paper presents a
series of theoretical results, including their proofs, and some directions for
future work. New research possibilities in curriculum teaching in compositional
scenarios are now wide open to exploration.
</p></div>
    </summary>
    <updated>2021-07-17T22:37:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06980</id>
    <link href="http://arxiv.org/abs/2107.06980" rel="alternate" type="text/html"/>
    <title>Online Allocation and Display Ads Optimization with Surplus Supply</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abolhassani:Melika.html">Melika Abolhassani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esfandiari:Hossein.html">Hossein Esfandiari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nazari:Yasamin.html">Yasamin Nazari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sivan:Balasubramanian.html">Balasubramanian Sivan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Teng:Yifeng.html">Yifeng Teng</a>, Creighton Thomas <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06980">PDF</a><br/><b>Abstract: </b>In this work, we study a scenario where a publisher seeks to maximize its
total revenue across two sales channels: guaranteed contracts that promise to
deliver a certain number of impressions to the advertisers, and spot demands
through an Ad Exchange. On the one hand, if a guaranteed contract is not fully
delivered, it incurs a penalty for the publisher. On the other hand, the
publisher might be able to sell an impression at a high price in the Ad
Exchange. How does a publisher maximize its total revenue as a sum of the
revenue from the Ad Exchange and the loss from the under-delivery penalty? We
study this problem parameterized by \emph{supply factor $f$}: a notion we
introduce that, intuitively, captures the number of times a publisher can
satisfy all its guaranteed contracts given its inventory supply. In this work
we present a fast simple deterministic algorithm with the optimal competitive
ratio. The algorithm and the optimal competitive ratio are a function of the
supply factor, penalty, and the distribution of the bids in the Ad Exchange.
</p>
<p>Beyond the yield optimization problem, classic online allocation problems
such as online bipartite matching of [Karp-Vazirani-Vazirani '90] and its
vertex-weighted variant of [Aggarwal et al. '11] can be studied in the presence
of the additional supply guaranteed by the supply factor. We show that a supply
factor of $f$ improves the approximation factors from $1-1/e$ to $f-fe^{-1/f}$.
Our approximation factor is tight and approaches $1$ as $f \to \infty$.
</p></div>
    </summary>
    <updated>2021-07-17T22:51:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06951</id>
    <link href="http://arxiv.org/abs/2107.06951" rel="alternate" type="text/html"/>
    <title>Levenshtein Graphs: Resolvability, Automorphisms &amp; Determining Sets</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Perrin E. Ruth, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lladser:Manuel_E=.html">Manuel E. Lladser</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06951">PDF</a><br/><b>Abstract: </b>We introduce the notion of Levenshtein graphs, an analog to Hamming graphs
but using the edit distance instead of the Hamming distance; in particular,
Levenshtein graphs allow for underlying strings (nodes) of different lengths.
We characterize various properties of these graphs, including a necessary and
sufficient condition for their geodesic distance to be identical to the edit
distance, their automorphism group and determining number, and an upper bound
on their metric dimension. Regarding the latter, we construct a resolving set
composed of two-run strings and an algorithm that computes the edit distance
between a string of length $k$ and any single-run or two-run string in $O(k)$
operations.
</p></div>
    </summary>
    <updated>2021-07-17T22:51:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.06889</id>
    <link href="http://arxiv.org/abs/2107.06889" rel="alternate" type="text/html"/>
    <title>Counting list homomorphisms from graphs of bounded treewidth: tight complexity bounds</title>
    <feedworld_mtime>1626480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Focke:Jacob.html">Jacob Focke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marx:D=aacute=niel.html">Dániel Marx</a>, Paweł Rzążewski <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.06889">PDF</a><br/><b>Abstract: </b>The goal of this work is to give precise bounds on the counting complexity of
a family of generalized coloring problems (list homomorphisms) on
bounded-treewidth graphs. Given graphs $G$, $H$, and lists $L(v)\subseteq V(H)$
for every $v\in V(G)$, a {\em list homomorphism} is a function $f:V(G)\to V(H)$
that preserves the edges (i.e., $uv\in E(G)$ implies $f(u)f(v)\in E(H)$) and
respects the lists (i.e., $f(v)\in L(v))$. Standard techniques show that if $G$
is given with a tree decomposition of width $t$, then the number of list
homomorphisms can be counted in time $|V(H)|^t\cdot n^{\mathcal{O}(1)}$. Our
main result is determining, for every fixed graph $H$, how much the base
$|V(H)|$ in the running time can be improved. For a connected graph $H$ we
define $\operatorname{irr}(H)$ the following way: if $H$ has a loop or is
nonbipartite, then $\operatorname{irr}(H)$ is the maximum size of a set
$S\subseteq V(H)$ where any two vertices have different neighborhoods; if $H$
is bipartite, then $\operatorname{irr}(H)$ is the maximum size of such a set
that is fully in one of the bipartition classes. For disconnected $H$, we
define $\operatorname{irr}(H)$ as the maximum of $\operatorname{irr}(C)$ over
every connected component $C$ of $H$. We show that, for every fixed graph $H$,
the number of list homomorphisms from $(G,L)$ to $H$
</p>
<p>* can be counted in time $\operatorname{irr}(H)^t\cdot n^{\mathcal{O}(1)}$ if
a tree decomposition of $G$ having width at most $t$ is given in the input, and
</p>
<p>* cannot be counted in time $(\operatorname{irr}(H)-\epsilon)^t\cdot
n^{\mathcal{O}(1)}$ for any $\epsilon&gt;0$, even if a tree decomposition of $G$
having width at most $t$ is given in the input, unless the #SETH fails.
</p>
<p>Thereby we give a precise and complete complexity classification featuring
matching upper and lower bounds for all target graphs with or without loops.
</p></div>
    </summary>
    <updated>2021-07-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/07/16/assistant-associate-full-professorship-in-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-september-12-2021/</id>
    <link href="https://cstheory-jobs.org/2021/07/16/assistant-associate-full-professorship-in-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-september-12-2021/" rel="alternate" type="text/html"/>
    <title>Assistant/Associate/Full Professorship in Machine Learning at Department of Computer Science, Faculty of Science, University of Copenhagen (apply by September 12, 2021)</title>
    <summary>The researcher will join a rapidly growing department, with strong research sections in the areas of Algorithms and Complexity, Machine Learning, Natural Language Processing, Human-Centered Computing, Software Engineering and Data Management, Programming Languages, and Image Analysis. Website: https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=154462&amp;DepartmentId=18971&amp;MediaId=4642 Email: simonsen@di.ku.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The researcher will join a rapidly growing department, with strong research sections in the areas of Algorithms and Complexity, Machine Learning, Natural Language Processing, Human-Centered Computing, Software Engineering and Data Management, Programming Languages, and Image Analysis.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=154462&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=154462&amp;DepartmentId=18971&amp;MediaId=4642</a><br/>
Email: simonsen@di.ku.dk</p></div>
    </content>
    <updated>2021-07-16T08:41:28Z</updated>
    <published>2021-07-16T08:41:28Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-07-18T21:37:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/07/15/linkage-many-wikipedia</id>
    <link href="https://11011110.github.io/blog/2021/07/15/linkage-many-wikipedia.html" rel="alternate" type="text/html"/>
    <title>Linkage with many Wikipedia Good Articles</title>
    <summary>There are two reasons for the large number of Good Articles in this set. First, I had previously been trying to keep my nominations and reviews in balance, but there were too few nominations to review on topics of interest to me, and the inability to find things to review was preventing me from nominating other articles when they were ready. So I started nominating more often. And second, the Wikipedia Good Articles editors are having a drive this month to clean out old nominations, as they tend to do a couple of times per year.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>There are two reasons for the large number of Good Articles in this set. First, I had previously been trying to keep my nominations and reviews in balance, but there were too few nominations to review on topics of interest to me, and the inability to find things to review was preventing me from nominating other articles when they were ready. So I started nominating more often. And second, the Wikipedia Good Articles editors are having a drive this month to clean out old nominations, as they tend to do a couple of times per year.</p>

<ul>
  <li>
    <p><a href="https://www.thisiscolossal.com/2021/07/steve-lindsay-fractal-vise/">Morphing fractal engraving vice jaws</a> (<a href="https://mathstodon.xyz/@11011110/106506737270461558">\(\mathbb{M}\)</a>). Circular arcs nested within circular arcs rotate to conform to whatever shape is being gripped.</p>
  </li>
  <li>
    <p>Christian Lawson-Perfect sets up a new wiki, <a href="https://whystartat.xyz">Why start at \(x,y,z\)?</a> (<a href="https://mathstodon.xyz/@christianp/106500170446647463">\(\mathbb{M}\)</a>). Its aim is to collect ambiguous, inconsistent, or just plain unpleasant conventions in mathematical notation. My contribution: <a href="https://whystartat.xyz/wiki/Big_O_notation">big O notation</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Pick%27s_theorem">Pick’s theorem </a> (<a href="https://mathstodon.xyz/@11011110/106523554824974108">\(\mathbb{M}\)</a>). The area of a grid polygon equals its number of interior grid points, plus half the boundary points, minus one. Good Article #1.</p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2021/07/03/the-problem-with-combinatorics-textbooks/">The problem with combinatorics textbooks</a> (<a href="https://mathstodon.xyz/@11011110/106526891677914307">\(\mathbb{M}\)</a>). Igor Pak on the difficulty of teaching combinatorics in a comprehensive way in a single term. Instead, he suggests teaching courses on narrower subtopics: “the more specific you make the combinatorics course the more interesting it is to the students”.</p>
  </li>
  <li>
    <p>I recently returned from a relaxing early-long-weekend mini-vacation to Avila Beach (<a href="https://mathstodon.xyz/@11011110/106532239057852428">\(\mathbb{M}\)</a>), with seafood sunset beach dinners (the coast faces south so the sun sets over land), wine tasting (near the setting of Sideways), and sulphur springs hot tub soaks. The photo below is a garden in a field of rusted pylons in the flood basin of San Luis Obispo Creek. I liked its contrast of natural growth and regular artificial forms.</p>

    <p style="text-align: center;"><img alt="Secret garden, Sycamore Springs Resort, Avila Beach" src="https://www.ics.uci.edu/~eppstein/pix/sycsprings/SecretGarden-m.jpg" style="border-style: solid; border-color: black;"/></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Vi%C3%A8te%27s_formula">Viète’s formula</a> (<a href="https://mathstodon.xyz/@11011110/106535094271822431">\(\mathbb{M}\)</a>), an infinite product of nested roots evaluating to \(2/\pi\), “the first formula of European mathematics to represent an infinite process”. Good Article #2.</p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2021/06/25/science/puzzles-fonts-math-demaine.html">The <em>New York Times</em> on Erik and Marty Demaine’s mathematical typefaces</a> (<a href="https://mathstodon.xyz/@11011110/106545874983076062">\(\mathbb{M}\)</a>, <a href="https://archive.ph/oJ8xG">also</a>, <a href="http://stormbear.com/carnival-of-mathematics-195-july-2021/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Euclid%E2%80%93Euler_theorem">Euclid–Euler theorem</a> (<a href="https://mathstodon.xyz/@11011110/106554476459806645">\(\mathbb{M}\)</a>). A 2-millenium-long collab in which Euclid proved that all Mersenne primes produce even perfect numbers, and Euler proved that all even perfect numbers come from Mersenne primes. But let’s not forget <a href="https://en.wikipedia.org/wiki/Ibn_al-Haytham">Ibn al-Haytham</a> (Alhazen), halfway between them in time, who conjectured Euler’s result but couldn’t prove it. Good Article #3.</p>
  </li>
  <li>
    <p><a href="https://mirtitles.org/2021/07/07/convex-figures-and-polyhedra-lyusternik/">Lyusternik’s book <em>Convex Figures and Polyhedra</em></a> (<a href="https://mathstodon.xyz/@jarban/106551202604578180">\(\mathbb{M}\)</a>), one of the Mir translations from Russian to English, <a href="https://archive.org/details/lyusternik-convex-figures-and-polyhedra">available without restrictions on archive.org</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Bucket_queue">Bucket queue</a> (<a href="https://mathstodon.xyz/@11011110/106570282017522543">\(\mathbb{M}\)</a>). This priority queue is a bit out of fashion, but good for small integer priorities or for shortest paths when the ratio of longest to shortest edge is small. Good Article #4, despite a reviewer who had <a href="https://en.wikipedia.org/wiki/WP:CHEESE">somehow become convinced that deletion from doubly linked lists is nonconstant</a>. The issue is off-topic but real: updating objects in data structures often needs the objects to track their location in the structure; for instance, changing priorities in a binary heap requires each object to know its index. Most introductory material on these topics and even some standard library implementations (like Python’s heapq) fail to address this complication.</p>
  </li>
  <li>
    <p><a href="https://www.cs.ru.nl/~freek/100/">Formalizing 100 theorems</a> (<a href="https://mathstodon.xyz/@11011110/106579903443188306">\(\mathbb{M}\)</a>). Freek Wiedijk uses a rather arbitrary collection of 100 favorite theorems from some 1999 web page as a benchmark set for the progress of automatic proof assistants. I’m sad that Pick’s theorem has seen so little love.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.06490">Greedy spanners in Euclidean spaces admit sublinear separators</a> (<a href="https://mathstodon.xyz/@11011110/106583477388316730">\(\mathbb{M}\)</a>). My <a href="https://11011110.github.io/blog/2020/02/17/spanners-have-sparse.html">SoCG’21 result with Hadi Khodabandeh that 2d greedy spanners have separators of size \(O(\sqrt{n})\)</a> used crossing-based methods heavily dependent on planarity. This new preprint by Hung Le and Cuong Than uses different ideas to find separators of size \(O(n^{1-1/d})\), optimal in any dimension \(d\). Their work also extends from Euclidean spaces to doubling spaces.</p>
  </li>
  <li>
    <p><a href="https://oscarcunningham.com/670/unique-distancing-problem/">Unique distancing</a> (<a href="https://mathstodon.xyz/@11011110/106587574216525670">\(\mathbb{M}\)</a>). How many points can you place in an \(n\times n\) grid so that all pairwise distances are distinct? The linked post concerns whether \(n\) points are possible (no for all but finitely many cases because there are too many pairs and too few sums of squares) but it also looks interesting to maximize the number of points.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-07-15T18:02:00Z</updated>
    <published>2021-07-15T18:02:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-07-16T01:38:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18952</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/" rel="alternate" type="text/html"/>
    <title>Socially Reproduced Experiments</title>
    <summary>We must avoid becoming one Cropped from USA Today source José Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseball’s most dramatic cheating accusation of 2019. Today, at baseball’s All-Star break, we review this and other social […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>We must avoid becoming one</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/altuvehr/" rel="attachment wp-att-18954"><img alt="" class="alignright size-full wp-image-18954" height="151" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AltuveHR.jpg?resize=151%2C151&amp;ssl=1" width="151"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from USA Today <a href="https://www.usatoday.com/story/sports/mlb/2021/07/11/astros-jose-altuve-walk-off-hr-shirt-ripped-off-yankees/7933018002/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
José Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseball’s most dramatic cheating accusation of 2019.</p>
<p>
Today, at baseball’s All-Star break, we review this and other social experiments that have quite a bit more data.</p>
<p>
Altuve won the 2019 American League Championship series with a pinch-hit homer in the bottom of the ninth against the Yankees’ closer, Aroldis Chapman. As he approached home plate, he was <a href="https://www.youtube.com/watch?v=MTNBnk1dz6g">seen</a> telling his teammates waiting to mob him at the plate not to rip off his jersey in celebration. He subsequently <a href="https://youtu.be/-ryvOya4PoE?t=247">scooted</a> into the corridor behind the dugout, then re-emerged into the on-field celebration. This fed accusations that he had been wired with a buzzer to know what kind of pitch was coming from Chapman, in line with <a href="https://en.wikipedia.org/wiki/Houston_Astros_sign_stealing_scandal">sign-stealing</a> by other means from the Astros’ 2017 championship season and 2018 that was proven and punished by Major League Baseball. </p>
<p>
Almost the same scenario was reproduced Sunday: Houston down 7-5 against the Yankees with two out and two on base in the ninth, Altuve up against the Yankees’ closer (Chad Green recently supplanting Chapman). Altuve <a href="https://www.youtube.com/watch?v=OHrc6OdYOpo">socked</a> a homer to the same part of the ballpark to complete a shocking six-run comeback. Immediately upon touching home, he had his shirt ripped off to reveal nothing but the top half of his birthday suit. This was the most direct way possible to witness that he could have hit the other homer without illegal information.</p>
<p>
</p><p/><h2> Examples and Non-Examples </h2><p/>
<p/><p>
I have dealt with chess-cheating cases in which electronic buzzing has been specifically alleged, including the two <a href="https://en.wikipedia.org/wiki/Borislav_Ivanov#Retirement_from_competitive_chess_and_brief_return_to_chess-related_activities">most</a> prominent <a href="https://www.nytimes.com/2013/08/18/crosswords/chess/a-master-is-disqualified-over-suspicions-of-cheating.html">cases</a> of 2013. I will not take this post further in this direction, however, but rather to pose this question:</p>
<blockquote><p><b> </b> <em> What is considered a “social proof” of an assertion—especially when there are elements of scientific control and reproduction? </em>
</p></blockquote>
<p/><p>
A simple example is a police lineup. This tries to control for whether a witness has previously seen the accused by including the accused among usually four or five similarly represented people. Picking the right person is considered to prove the previous encounter. Statistically, however, this is a <a href="https://en.wikipedia.org/wiki/P-value"><img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-value</a> of only 0.20 or 0.167, which are not considered significant at even the weakest level of “statistical proof.” Allowing <a href="https://www.apa.org/monitor/julaug04/lineups">null</a> lineups does not change the statistics much.</p>
<p>
Baseball gives a non-example that surprises me. One of the bad performances that cost Chapman his closer role was losing an 8-4 lead against the Los Angeles Angels on June 30. As a fantasy-baseball player, I’ve regularly observed poor pitching (by the closers on my “fantasy team”) when the lead is too large to earn credit for a coveted <a href="https://en.wikipedia.org/wiki/Save_(baseball)">save</a>. Does the data reproduce a phenomenon of closers bearing down less when way ahead, with no “save” to gain? A <a href="https://www.beyondtheboxscore.com/2014/1/27/5344580/the-closer-mentality-part-1-closers-in-non-save-situations">study</a> after the 2013 season, which cleverly represented performances by the same <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scores I use in chess, found none. “Meltdowns” like Chapman’s are offset by cases where closers pitched better. The <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scores in the study are all in the range -1.25 to +1.50 anyway, which count as statistically random.</p>
<p>
This study used a reasonably large data set, one that is well-defined and admits controlling factors such as normalizing for game circumstances and the quality of the opposing hitters. At least it is more than the two instances of Altuve. In-between would be an attempt to determine whether certain national soccer teams are consistently worse at penalty-kick tiebreaks. England’s and Italy’s teams brought their long tortured histories together in the tiebreak of Sunday’s European Cup final. The Italians missed two of five kicks, a score that often spells doom, but the English missed three.</p>
<p>
</p><p/><h2> Larger Scale </h2><p/>
<p/><p>
Dick and I are really interested in “experiments” that have spilled into society, with minimal controls but large data. One sphere of this is cybersecurity. </p>
<p>
It seems to us that only in the past decade have security experts begun formalizing their research as experimental science with repeatability and reproducibility as explicit criteria. The NSA devoted a special 2012 <a href="https://www.nsa.gov/Portals/70/documents/resources/everyone/digital-media-center/publications/the-next-wave/TNW-19-2.pdf">issue</a> of their <em>Next Wave</em> series to what they titled as “Developing a blueprint for a science of cybersecurity.” Among the contents are:</p>
<ul>
<li>
An introductory essay by Carl Landwehr titled, “Cybersecurity: From engineering to science.” <p/>
</li><li>
A linchpin <a href="https://www.cs.dartmouth.edu/~ccpalmer/teaching/cs55/Resources/Papers/TNW_19_2_BlueprintScienceCybersecurity_Schneider.pdf">paper</a> by Fred Schneider titled, “Blueprint for a science of cybersecurity.” <p/>
</li><li>
A <a href="https://www.cs.cmu.edu/~maxion/pubs/Maxion12.pdf">paper</a> by Roy Maxion titled, “Making experiments dependable,” which came from a 2011 Springer LNCS <a href="https://link.springer.com/book/10.1007/978-3-642-24541-1">Festschrift</a>.
</li></ul>
<p>
Maxion’s main example is <a href="https://en.wikipedia.org/wiki/Keystroke_dynamics">keystroke biometrics</a>. This covers inferences made from typing style on a computer keyboard or mouse or similar handheld input device. This can be used to verify identity or screen for malfeasant activities. Online chess playing platforms collect data of this nature—okay we could not resist adding chess example. </p>
<p>
Another area is experiments designed to simulate attacks and test defenses against them. Schneider’s paper begins with a contrast between <em>predictive</em> modeling versus <em>reactive</em> handling of them. About the latter, he draws an analogy with health care:</p>
<blockquote><p><b> </b> <em> “Some health problems are best handled in a reactive manner. We know what to do when somebody breaks a finger, and each year we create a new influenza vaccine in anticipation of the flu season to come. But only after making significant investments in basic medical sciences are we starting to understand the mechanisms by which cancers grow, and a cure seems to require that kind of deep understanding.” </em>
</p></blockquote>
<p/><p>
He goes on to outline the kind of scientific foundation that could hopefully underlie a ‘cure’ for intrusion and malware and the like. </p>
<p>
What we have seen happen especially in the past months, however—in both health and security—is uncontrolled experiments with society as the domain. Large-scale ransomware attacks are becoming as frequent as hurricanes and heat waves. And of course, the pandemic. These share with Altuve the property of being one-off instances, but have large data on the receiving end.</p>
<p>
</p><p/><h2> Summer Pandemic Update </h2><p/>
<p/><p>
The following chart updates our June 20 <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">post</a> on the state of the pandemic and its projection for the summer—for Florida and the United Kingdom in particular:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/flukcases071321/" rel="attachment wp-att-18955"><img alt="" class="aligncenter wp-image-18955" height="440" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/FLUKCases071321.png?resize=450%2C440&amp;ssl=1" width="450"/></a></p>
<p>
The vertical line shows about where the charts were on June 20. The past few days are the first where we can point to a significant rise in Florida, though Missouri had a similar rise last week and it is showing up in some other states.  The charts are taken from the <em>Worldometer</em> coronavirus <a href="https://www.worldometers.info/coronavirus/">pages</a>. </p>
<p>
The UK rise looks ghastly. It was a subtext of our previous post to worry that allowing the large dense soccer crowds at London’s Wembley Stadium for the semis and final—and anything similar in baseball—would stoke the rise in our respective countries even more. However, the rate of hospitalizations in the UK has remained largely flat. This Fortune <a href="https://fortune.com/2021/07/08/kids-vulnerable-covid-delta-variant-vaccinated-europe/">article</a> last week is one of several attesting that the new cases are mostly in children or in vaccinated people with enough immunity to contain the “breakthrough” positive. The UK is going ahead with large-scale re-openings later this month, with the portion of those 18 and older who have had one dose approaching 90% and those fully vaccinated coming past 70%. The latter number in relation to the whole population is about 52%.</p>
<p>
The US looks like becoming an experiment in how the local vaccination rate affects the numbers. The rates of those fully vaccinated by state are currently eerily <a href="https://www.cnn.com/2021/07/08/politics/electoral-map-vaccine-map-covid-19/index.html">similar</a> to Joe Biden’s vote percentage in the state. One aspect of scientific reproducibility is the size of the simplest classifier of the results. For a presidential vote to have simpler explaining power than any factors of biology or other life circumstances would make a strange experiment indeed.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Dick and I tried to come up with other examples—from computer security in particular—to sustain what has been occupying our thoughts about standards of proof for policy. We would welcome some examples from you, our readers. </p>
<p>
And of course, we have been concerned about the present course of the pandemic amid re-openings since the referenced post last month. In the meantime, if it is your taste, please enjoy the All-Star Game, which Altuve is, ironically, <a href="https://calltothepen.com/2021/07/11/houston-astros-officially-skipping-star-game/">skipping</a>.</p>
<p/><p><br/>
[some word fixes and changes]</p></font></font></div>
    </content>
    <updated>2021-07-13T19:47:02Z</updated>
    <published>2021-07-13T19:47:02Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="baseball"/>
    <category term="coronavirus"/>
    <category term="Jose Altuve"/>
    <category term="pandemic"/>
    <category term="reproducibility"/>
    <category term="science"/>
    <category term="security"/>
    <category term="social process"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-07-18T21:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/102</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/102" rel="alternate" type="text/html"/>
    <title>TR21-102 |  Tight bounds on the Fourier growth of bounded functions on the hypercube | 

	Siddharth Iyer, 

	Anup Rao, 

	Victor Reis, 

	Thomas Rothvoss, 

	Amir Yehudayoff</title>
    <summary>We give tight bounds on the degree $\ell$  homogenous parts $f_\ell$ of a bounded function $f$ on the cube. We show that if $f: \{\pm 1\}^n \rightarrow [-1,1]$ has degree $d$, then $\| f_\ell \|_\infty$ is bounded by $d^\ell/\ell!$, and $\| \hat{f}_\ell \|_1$ is bounded by $d^\ell e^{{\ell+1 \choose 2}} n^{\frac{\ell-1}{2}}$. We describe applications to pseudorandomness and learning theory. We use similar methods to generalize the classical Pisier's inequality from convex analysis. Our analysis  involves properties of real-rooted polynomials that may be useful elsewhere.</summary>
    <updated>2021-07-13T17:55:40Z</updated>
    <published>2021-07-13T17:55:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-18T21:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/101</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/101" rel="alternate" type="text/html"/>
    <title>TR21-101 |  A Parallel Repetition Theorem for the GHZ Game: A Simpler Proof | 

	Uma Girish, 

	Justin Holmgren, 

	Kunal Mittal, 

	Ran Raz, 

	Wei Zhan</title>
    <summary>We give a new proof of the fact that the parallel repetition of the (3-player) GHZ game reduces the value of the game to zero polynomially quickly. That is, we show that the value of the $n$-fold GHZ game is at most $n^{-\Omega(1)}$. This was first established by Holmgren and Raz [HR20]. We present a new proof of this theorem that we believe to be simpler and more direct. Unlike most previous works on parallel repetition, our proof makes no use of information theory, and relies on the use of Fourier analysis.

The GHZ game [GHZ89] has played a foundational role in the understanding of quantum information theory, due in part to the fact that quantum strategies can win the GHZ game with probability $1$. It is possible that improved parallel repetition bounds may find applications in this setting.

Recently, Dinur, Harsha, Venkat, and Yuen [DHVY17] highlighted the GHZ game as a simple three-player game, which is in some sense maximally far from the class of multi-player games whose behavior under parallel repetition is well understood. Dinur et al. conjectured that parallel repetition decreases the value of the GHZ game exponentially quickly, and speculated that progress on proving this would shed light on parallel repetition for general multi-player (multi-prover) games.</summary>
    <updated>2021-07-13T14:48:27Z</updated>
    <published>2021-07-13T14:48:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-18T21:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://sarielhp.org/blog/?p=9420</id>
    <link href="https://sarielhp.org/blog/?p=9420" rel="alternate" type="text/html"/>
    <title>FSTTCS 2021 deadline is this Monday…</title>
    <summary>The link is here. Due to the pandemic it is going to be virtual. Quoting Bob Dylan, this blog is not dead, it is just asleep.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The link is <a href="https://www.fsttcs.org.in/2021/" rel="noreferrer noopener" target="_blank">here</a>. Due to the pandemic it is going to be virtual.</p>



<p>Quoting Bob Dylan,  this blog is not dead, it is just asleep.</p></div>
    </content>
    <updated>2021-07-13T03:38:25Z</updated>
    <published>2021-07-13T03:38:25Z</published>
    <category term="Research"/>
    <category term="cs theory"/>
    <author>
      <name>Sariel</name>
    </author>
    <source>
      <id>https://sarielhp.org/blog</id>
      <link href="https://sarielhp.org/blog/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://sarielhp.org/blog" rel="alternate" type="text/html"/>
      <subtitle>Sariel's blog</subtitle>
      <title>Vanity of Vanities, all is Vanity</title>
      <updated>2021-07-18T21:37:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4532</id>
    <link href="https://lucatrevisan.wordpress.com/2021/07/12/what-a-difference-a-few-months-can-make/" rel="alternate" type="text/html"/>
    <title>What a difference a few months can make</title>
    <summary>Piazza Duomo, in Milan, on December 26, 2020 Piazza Duomo, in Milan, on July 11, 2021</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Piazza Duomo, in Milan, on December 26, 2020</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure>



<p>Piazza Duomo, in Milan, on July 11, 2021</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure></div>
    </content>
    <updated>2021-07-12T17:04:51Z</updated>
    <published>2021-07-12T17:04:51Z</published>
    <category term="Milan"/>
    <category term="things that are excellent"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2021-07-18T21:37:09Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/exponential-mechanism-bounded-range/</id>
    <link href="https://differentialprivacy.org/exponential-mechanism-bounded-range/" rel="alternate" type="text/html"/>
    <title>A Better Privacy Analysis of the Exponential Mechanism</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A basic and frequent task in data analysis is <em>selection</em> – given a set of options \(\mathcal{Y}\), output the (approximately) best one, where “best” is defined by some loss function \(\ell : \mathcal{Y} \times \mathcal{X}^n \to \mathbb{R}\) and a dataset \(x \in \mathcal{X}^n\). That is, we want to output some \(y \in \mathcal{Y}\) that approximately minimizes \(\ell(y,x)\). Naturally, we are interested in <em>private selection</em> – i.e., the output should be differentially private in terms of the dataset \(x\).
This post discusses algorithms for private selection – in particular, we give an improved privacy analysis of the popular exponential mechanism.</p>

<h2 id="the-exponential-mechanism">The Exponential Mechanism</h2>

<p>The most well-known algorithm for private selection is the <a href="https://en.wikipedia.org/wiki/Exponential_mechanism_(differential_privacy)"><em>exponential mechanism</em></a> <a href="https://doi.org/10.1109/FOCS.2007.66" title="Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007."><strong>[MT07]</strong></a>. The exponential mechanism \(M : \mathcal{X}^n \to \mathcal{Y} \) is a randomized algorithm given by \[\forall x \in \mathcal{X}^n ~ \forall y \in \mathcal{Y} ~~~~~ \mathbb{P}[M(x) = y] = \frac{\exp(-\frac{\varepsilon}{2\Delta} \ell(y,x))}{\sum_{y’ \in \mathcal{Y}} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x)) }, \tag{1}\] where \(\Delta\) is the sensitivity of the loss function \(\ell\) given by \[\Delta = \sup_{x,x’ \in \mathcal{X}^n : d(x,x’) \le 1} \max_{y\in\mathcal{Y}} |\ell(y,x) - \ell(y,x’)|,\tag{2}\] where the supremum is taken over all datasets \(x\) and \(x’\) differing on the data of a single individual (which we denote by \(d(x,x’)\le 1\)).</p>

<p>In terms of utility, we can easily show that <a href="https://arxiv.org/abs/1511.02513" title="Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, Jonathan Ullman. Algorithmic Stability for Adaptive Data Analysis. STOC 2016."><strong>[BNSSSU16]</strong></a> \[\mathbb{E}[\ell(M(x),x)] \le \min_{y \in \mathcal{Y}} \ell(y,x) + \frac{2\Delta}{\varepsilon} \log |\mathcal{Y}|\] for all \(x \in \mathcal{X}^n\) (and we can also give high probability bounds).</p>

<p>It is easy to show that the exponential mechanism satisfies \(\varepsilon\)-differential privacy.
But there is more to this story! We’re going to look at a more refined privacy analysis.</p>

<h2 id="bounded-range">Bounded Range</h2>

<p>The privacy guarantee of the exponential mechanism is more precisely characterized by <em>bounded range</em>. This was observed and defined by David Durfee and Ryan Rogers <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a> and further analyzed later <a href="https://arxiv.org/abs/1909.13830" title="Jinshuo Dong, David Durfee, Ryan Rogers. Optimal Differential Privacy Composition for Exponential Mechanisms. ICML 2020."><strong>[DDR20]</strong></a>.</p>

<blockquote>
  <p><strong>Definition 1 (Bounded Range).</strong><sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1" rel="footnote">1</a></sup> 
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\eta\)-bounded range if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, there exists some \(t \in \mathbb{R}\) such that \[\forall y \in \mathcal{Y} ~~~~~ \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]}\right) \in [t, t+\eta].\] Here \(t\) may depend on the pair of input datasets \(x,x’\), but not on the output \(y\).</p>
</blockquote>

<p>To interpret this definition, we <a href="https://differentialprivacy.org/flavoursofdelta/">recall the definition of the privacy loss random variable</a>: Define \(f : \mathcal{Y} \to \mathbb{R}\) by \[f(y) = \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]}\right).\] Then the privacy loss random variable \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) is given by \(Z = f(M(x))\).</p>

<p>Pure \(\varepsilon\)-differential privacy is equivalent to demanding that the privacy loss is bounded by \(\varepsilon\) – i.e., \(\mathbb{P}[|Z|\le\varepsilon]=1\). Approximate \((\varepsilon,\delta)\)-differential privacy is, roughly, equivalent to demanding that \(\mathbb{P}[Z\le\varepsilon]\ge1-\delta\).<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2" rel="footnote">2</a></sup></p>

<p>Now \(\eta\)-bounded range is simply demanding that the privacy loss \(Z\) is supported on some interval of length \(\eta\). This interval \([t,t+\eta]\) may depend on the pair \(x,x’\).</p>

<p>Bounded range and pure differential privacy are equivalent up to a factor of 2 in the parameters:</p>

<blockquote>
  <p><strong>Lemma 2 (Bounded Range versus Pure Differential Privacy).</strong></p>
  <ul>
    <li>\(\varepsilon\)-differential privacy implies \(\eta\)-bounded range with \(\eta \le 2\varepsilon\).</li>
    <li>\(\eta\)-bounded range implies \(\varepsilon\)-differential privacy with \(\varepsilon \le \eta\).</li>
  </ul>
</blockquote>

<p><em>Proof.</em> The first part of the equivalence follows from the fact that pure \(\varepsilon\)-differential privacy implies the privacy loss is supported on the interval \([-\varepsilon,\varepsilon]\). Thus, if we set \(t=-\varepsilon\) and \(\eta=2\varepsilon\), then \([t,t+\eta] = [-\varepsilon,\varepsilon]\).
The second part follows from the fact that the support of the privacy loss \([t,t+\eta]\) must straddle \(0\). That is, the privacy loss cannot be always positive nor always negative, so \(0 \in [t,t+\eta]\) and, hence, \([t,t+\eta] \subseteq [-\eta,\eta]\). Otherwise \(\forall y ~ f(y)&gt;0\) or \(\forall y ~ f(y)&lt;0\)  would imply \(\forall y ~ \mathbb{P}[M(x)=y]&gt;\mathbb{P}[M(x’)=y]\) or \(\forall y ~ \mathbb{P}[M(x)=y]&lt;\mathbb{P}[M(x’)=y]\), contradicting the fact that \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x)=y] = 1\) and \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x’)=y] = 1\). ∎</p>

<p>OK, back to the exponential mechanism:</p>

<blockquote>
  <p><strong>Lemma 3 (The Exponential Mechanism is Bounded Range).</strong>
The exponential mechanism (given in Equation 1 above) satisfies \(\varepsilon\)-bounded range .<sup id="fnref:3"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:3" rel="footnote">3</a></sup></p>
</blockquote>

<p><em>Proof.</em>
We have \[e^{f(y)} = \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x))}{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x’))} \cdot \frac{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x’))}{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x))}.\]
Setting \(t = \log\left(\frac{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x’))}{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x))}\right) - \frac{\varepsilon}{2}\), we have \[ f(y) = \frac{\varepsilon}{2\Delta} (\ell(y,x’)-\ell(y,x)+\Delta) + t.\]
By the definition of sensitivity (given in Equation 2), we have \( 0 \le \ell(y,x’)-\ell(y,x)+\Delta \le 2\Delta\), whence \(t \le f(y) \le t + \varepsilon\). ∎</p>

<p>Bounded range is not really a useful privacy definition on its own. Thus we’re going to relate it to a relaxed version of differential privacy next.</p>

<h2 id="concentrated-differential-privacy">Concentrated Differential Privacy</h2>

<p>Concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a> and its variants <a href="https://arxiv.org/abs/1603.01887" title="Cynthia Dwork, Guy N. Rothblum. Concentrated Differential Privacy. 2016."><strong>[DR16]</strong></a> <a href="https://arxiv.org/abs/1702.07476" title="Ilya Mironov. R&#xE9;nyi Differential Privacy. CCS 2017."><strong>[M17]</strong></a> are relaxations of pure differential privacy with many nice properties. In particular, it composes very cleanly.</p>

<blockquote>
  <p><strong>Definition 4 (Concentrated Differential Privacy).</strong>
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, 
\[\forall \lambda &gt; 0 ~~~~~ \mathbb{E}[\exp( \lambda Z)] \le \exp(\lambda(\lambda+1)\rho),\tag{3}\]
where \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) is the privacy loss random variable.<sup id="fnref:4"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:4" rel="footnote">4</a></sup></p>
</blockquote>

<p>Intuitively, concentrated differential privacy requires that the privacy loss is subgaussian. Specifically, the bound on the moment generating function of \(\rho\)-concentrated differential privacy is tight if the privacy loss \(Z\) follows the distribution \(\mathcal{N}(\rho,2\rho)\). Indeed, the privacy loss random variable of the Gaussian mechanism has such a distribution.<sup id="fnref:5"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:5" rel="footnote">5</a></sup></p>

<p>OK, back to the exponential mechanism:
We know that \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>.
This, of course, applies to the exponential mechaism. A cool fact – that we want to draw more attention to – is that we can do better! 
Specifically, \(\eta\)-bounded range implies \(\frac18 \eta^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/2004.07223" title="Mark Cesar, Ryan Rogers. Bounding, Concentrating, and Truncating: Unifying Privacy Loss Composition for Data Analytics. ALT 2021."><strong>[CR21]</strong></a>.
What follows is a proof of this fact following that of Mark Cesar and Ryan Rogers, but with some simplification.</p>

<blockquote>
  <p><strong>Theorem 5 (Bounded Range implies Concentrated Differential Privacy).</strong>
If \(M\) is \(\eta\)-bounded range, then it is \(\frac18\eta^2\)-concentrated differentially private.</p>
</blockquote>

<p><em>Proof.</em>
Fix datasets \(x,x’ \in \mathcal{X}^n\) differing on a single individual’s data.
Let \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) be the privacy loss random variable of the mechanism \(M\) on this pair of datasets.
By the definition of bounded range (Definition 1), there exists some \(t \in \mathbb{R}\) such that \(Z \in [t, t+\eta]\) with probability 1.
Now we employ <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_lemma">Hoeffding’s Lemma</a> <a href="https://doi.org/10.1080%2F01621459.1963.10500830" title="Wassily Hoeffding. Probability inequalities for sums of bounded random variables. JASA 1963."><strong>[H63]</strong></a>:</p>
<blockquote>
  <p><strong>Lemma 6 (Hoeffding’s Lemma).</strong>
Let \(X\) be a random variable supported on the interval \([a,b]\). Then, for all \(\lambda \in \mathbb{R}\), we have \[\mathbb{E}[\exp(\lambda X)] \le \exp \left( \mathbb{E}[X] \cdot \lambda + \frac{(b-a)^2}{8} \cdot \lambda^2 \right).\]</p>
</blockquote>

<p>Applying the lemma to the privacy loss gives \[\forall \lambda \in \mathbb{R} ~~~~~  \mathbb{E}[\exp(\lambda Z)] \le \exp \left( \mathbb{E}[Z] \cdot \lambda + \frac{\eta^2}{8} \cdot \lambda^2 \right).\]
The only remaining thing we need is to show is that \(\mathbb{E}[Z] \le \frac18 \eta^2\).<sup id="fnref:6"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:6" rel="footnote">6</a></sup></p>

<p>If we set \(\lambda = -1 \), then we get \( \mathbb{E}[\exp( - Z)] \le \exp \left( -\mathbb{E}[Z] + \frac{\eta^2}{8} \right)\), which rearranges to \(\mathbb{E}[Z] \le \frac18 \eta^2 - \log \mathbb{E}[\exp( - Z)]\). 
Now we have \[ \mathbb{E}[\exp( - Z)] \!=\! \sum_y \mathbb{P}[M(x)\!=\!y] \exp(-f(y)) \!=\! \sum_y \mathbb{P}[M(x)\!=\!y]  \!\cdot\! \frac{\mathbb{P}[M(x’)\!=\!y]}{\mathbb{P}[M(x)\!=\!y]} \!=\! 1.\]
∎</p>

<p>This brings us to the TL;DR of this post:</p>

<blockquote>
  <p><strong>Corollary 7.</strong> The exponential mechanism (given by Equation 1) is \(\frac18 \varepsilon^2\)-concentrated differentially private.</p>
</blockquote>

<p>This is great news. The standard analysis only gives \(\frac12 \varepsilon^2\)-concentrated differential privacy. Constants matter when applying differential privacy, and we save a factor of 4 in the concentrated differential privacy analysis of the exponential mechanism for free with this improved analysis.</p>

<p>Combining Lemma 2 with Theorem 5 also gives a simpler proof of the conversion from pure differential privacy to concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>:</p>

<blockquote>
  <p><strong>Corollary 8.</strong> \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy.</p>
</blockquote>

<h2 id="beyond-the-exponential-mechanism">Beyond the Exponential Mechanism</h2>

<p>The exponential mechanism is not the only algorithm for private selection. A closely-related algorithm is <em>report noisy max/min</em>:<sup id="fnref:7"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:7" rel="footnote">7</a></sup> Draw independent noise \(\xi_y\) from some distribution for each \(y \in \mathcal{Y}\) then output \[M(x) = \underset{y \in \mathcal{Y}}{\mathrm{argmin}} ~ \ell(y,x) - \xi_y.\]</p>

<p>If the noise distribution is an appropriate <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution</a>, then report noisy max is exactly the exponential mechanism. (This equivalence is known as the “Gumbel max trick.”)</p>

<p>We can also use the Laplace distribution or the exponential distribution. Report noisy max with the exponential distribution is equivalent to the <em>permute and flip</em> algorithm <a href="https://arxiv.org/abs/2010.12603" title="Ryan McKenna, Daniel Sheldon. Permute-and-Flip: A new mechanism for differentially private selection . NeurIPS 2020."><strong>[MS20]</strong></a> <a href="https://arxiv.org/abs/2105.07260" title="Zeyu Ding, Daniel Kifer, Sayed M. Saghaian N. E., Thomas Steinke, Yuxin Wang, Yingtai Xiao, Danfeng Zhang. The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with Exponential Noise. 2021."><strong>[DKSSWXZ21]</strong></a>. However, these algorithms don’t enjoy the same improved bounded range and concentrated differential privacy guarantees as the exponential mechanism.</p>

<p>There are also other variants of the selection problem. For example, in some cases we can assume that only a few options have low loss and the rest of the options have high loss – i.e., there is a gap between the minimum loss and the second-lowest loss (or, more generally, the \(k\)-th lowest loss). In this case there are algorithms that attain better accuracy than the exponential mechanism under relaxed privacy definitions <a href="https://arxiv.org/abs/1409.2177" title="Kamalika Chaudhuri, Daniel Hsu, Shuang Song. The Large Margin Mechanism for Differentially Private Maximization. NIPS 2014."><strong>[CHS14]</strong></a> <a href="https://dl.acm.org/doi/10.1145/3188745.3188946" title=" Mark Bun, Cynthia Dwork, Guy N. Rothblum, Thomas Steinke. Composable and versatile privacy via truncated CDP. STOC 2018."><strong>[BDRS18]</strong></a> <a href="https://arxiv.org/abs/1905.13229" title="Mark Bun, Gautam Kamath, Thomas Steinke, Zhiwei Steven Wu. Private Hypothesis Selection. NeurIPS 2019."><strong>[BKSW19]</strong></a>.</p>

<p>There are a lot of interesting aspects of private selection, including questions for further research! We hope to have further posts about some of these topics.</p>

<hr/>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For simplicity, we restrict our discussion here to finite sets of outputs, although the definitions, algorithms, and results can be extended to infinite sets. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>To be more precise, \((\varepsilon,\delta)\)-differential privacy is equivalent to demanding that \(\mathbb{E}[\max\{0,1-\exp(\varepsilon-Z)\}]\le\delta\) <a href="https://arxiv.org/abs/2004.00010" title="Cl&#xE9;ment L. Canonne, Gautam Kamath, Thomas Steinke. The Discrete Gaussian for Differential Privacy. NeurIPS 2020."><strong>[CKS20]</strong></a>. (To be completely precise, we must appropriately deal with the \(Z=\infty\) case, which we ignore in this discussion for simplicity.) <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">↩</a></p>
    </li>
    <li id="fn:3">
      <p>This proof actually gives <a href="https://dongjs.github.io/2020/02/10/ExpMech.html">a slightly stronger result</a>: We can replace the sensitivity \(\Delta\) (defined in Equation 2) by half the range \[\hat\Delta = \frac12 \sup_{x,x’ \in \mathcal{X}^n : d(x,x’) \le 1} \left( \max_{\overline{y}\in\mathcal{Y}} \ell(\overline{y},x) - \ell(\overline{y},x’) - \min_{\underline{y}\in\mathcal{Y}} \ell(\underline{y},x) - \ell(\underline{y},x’) \right).\] We always have \(\hat\Delta \le \Delta\) but it is possible that \(\hat\Delta &lt; \Delta\) and the privacy analysis of the exponential mechanism still works if we replace \(\Delta\) by \(\hat\Delta\). <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:3">↩</a></p>
    </li>
    <li id="fn:4">
      <p>Equivalently, a randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, \[\forall \lambda &gt; 0 ~~~~~ \mathrm{D}_{\lambda+1}(M(x)\|M(x’)) \le \lambda(\lambda+1)\rho,\] where \(\mathrm{D}_{\lambda+1}(M(x)\|M(x’)))\) is the order \(\lambda+1\) Rényi divergence of \(M(x)\) from \(M(x’)\). <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:4">↩</a></p>
    </li>
    <li id="fn:5">
      <p>To be precise, if \(M(x) = q(x) + \mathcal{N}(0,\sigma^2I)\), then \(M : \mathcal{X}^n \to \mathbb{R}^d\) satisfies \(\frac{\Delta_2^2}{2\sigma^2}\)-concentrated differential privacy, where \(\Delta_2 = \sup_{x,x’\in\mathcal{X}^n : d(x,x’)\le1} \|q(x)-q(x’)\|_2\) is the 2-norm sensitivity of \(q:\mathcal{X}^n \to \mathbb{R}^d\). Furthermore, the privacy loss of the Gaussian mechanism is itself a Gaussian and it makes the inequality defining concentrated differential privacy (Equation 3) an equality for all \(\lambda\) <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:5">↩</a></p>
    </li>
    <li id="fn:6">
      <p>Note that the expectation of the privacy loss is simply the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>: \(\mathbb{E}[Z] = \mathrm{D}_1( M(x) \| M(x’) )\). <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:6">↩</a></p>
    </li>
    <li id="fn:7">
      <p>We have presented selection here in terms of minimization, but most of the literature is in terms of maximization. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:7">↩</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2021-07-12T17:00:00Z</updated>
    <published>2021-07-12T17:00:00Z</published>
    <author>
      <name>Thomas Steinke</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-07-17T23:31:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/100</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/100" rel="alternate" type="text/html"/>
    <title>TR21-100 |  Karchmer-Wigderson Games for Hazard-free Computation | 

	Christian Ikenmeyer, 

	Balagopal Komarath, 

	Nitin Saurabh</title>
    <summary>We present a Karchmer-Wigderson game to study the complexity of hazard-free formulas. This new game is both a generalization of the monotone Karchmer-Wigderson game and an analog of the classical Boolean Karchmer-Wigderson game. Therefore, it acts as a bridge between the existing monotone and general games.

Using this game, we prove hazard-free formula size and depth lower bounds that are provably stronger than those possible by the standard technique of transferring results from monotone complexity in a black-box fashion.
For the multiplexer function
we give (1) a hazard-free formula of optimal size and (2) an improved low-depth hazard-free formula of almost optimal size and (3) a hazard-free formula with alternation depth $2$ that has optimal depth.
We then use our optimal constructions to obtain an improved universal worst-case hazard-free formula size upper bound.
We see our results as a significant step towards establishing hazard-free computation as an independent missing link between Boolean complexity and monotone complexity.</summary>
    <updated>2021-07-12T07:35:35Z</updated>
    <published>2021-07-12T07:35:35Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-18T21:37:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6138760775046652444</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6138760775046652444/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6138760775046652444" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6138760775046652444" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html" rel="alternate" type="text/html"/>
    <title>Would you take this bet (Part 2) ?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Recall from my last post (<a href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html">here</a>)</p><p><br/></p><p>I offer you the following bet: </p><p>I will flip a coin.</p><p>If  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>If  HEADS you get 2 dollars and we end there.</p><p>If  TAILS I flip again</p><p><br/></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>The expected value is infinity.</p><p><br/></p><p>Would you pay $1000 to play this game?</p><p>Everyone who responded said NO. Most gave reasons similar to what I have below. </p><p>This is called The St Petersburg Paradox. Not sure it's a paradox, but it is odd. The concrete question of <i>would you pay $1000 to play</i> might be a paradox since most people would say NO even though the expected value is infinity.  See <a href="https://en.wikipedia.org/wiki/St._Petersburg_paradox">here</a> for more background.</p><p>Shapley (see <a href="https://www.sciencedirect.com/science/article/abs/pii/0022053177901429">here</a>) gives a good reason why you would not  pay $1000 to play the game, and also how much you should pay to play the game (spoiler alert: not much). I will summarize his argument and then add to it. </p><p><br/></p><p>1) Shapley's argument: Lets say the game goes for 40 rounds. Then you are owed 2^{40} dollars. </p><p>The amount of money in the world is, according to <a href="https://bibloteka.com/how-much-money-is-there-in-the-world/#:~:text=Short%20Answer%3A%20Money%20in%20circulation%20in%20the%20world,the%20medium%20of%20trade%20for%20goods%20and%20services.">this article</a> around 1.2 quadrillion dollars  which is roughly 2^{40} dollars. </p><p>So the expected value calculation has to be capped at (say) 40 rounds. This means you expect to get 20 dollars! So pay 19 to play. </p><p><br/></p><p>2) My angle which is very similar: at what point is more money not going to change your life at all? For me it is way less than 2^{40} dollars. Hence I would not pay 1000. Or even 20. </p><p><i>Exercise</i>: If you think the game will go at most R rounds and you only wand D dollars, how much should you pay to play? You can also juggle more parameters - the bias of the coin, how much they pay out when you win. </p><p>Does Shapley's discussions  <i>resolve</i> the paradox? It depends on what you consider paradoxical. If the paradox is that people would NOT pay 1000 even though the expected value is infinity, then Shapley  resolves the paradox  by contrasting the real world to the math world. </p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-07-12T00:53:00Z</updated>
    <published>2021-07-12T00:53:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-18T07:49:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/099</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/099" rel="alternate" type="text/html"/>
    <title>TR21-099 |  Improved Product-Based High-Dimensional Expanders | 

	Louis Golowich</title>
    <summary>High-dimensional expanders generalize the notion of expander graphs to higher-dimensional simplicial complexes. In contrast to expander graphs, only a handful of high-dimensional expander constructions have been proposed, and no elementary combinatorial construction with near-optimal expansion is known. In this paper, we introduce an improved combinatorial high-dimensional expander construction, by modifying a previous construction of Liu, Mohanty, and Yang (ITCS 2020), which is based on a high-dimensional variant of a tensor product. Our construction achieves a spectral gap of $\Omega(\frac{1}{k^2})$ for random walks on the $k$-dimensional faces, which is only quadratically worse than the optimal bound of $\Theta(\frac{1}{k})$. Previous combinatorial constructions, including that of Liu, Mohanty, and Yang, only achieved a spectral gap that is exponentially small in $k$. We also present reasoning that suggests our construction is optimal among similar product-based constructions.</summary>
    <updated>2021-07-11T07:39:29Z</updated>
    <published>2021-07-11T07:39:29Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-18T21:37:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/098</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/098" rel="alternate" type="text/html"/>
    <title>TR21-098 |  On the Probabilistic Degree of an $n$-variate Boolean Function | 

	Srikanth Srinivasan, 

	S Venkitesh</title>
    <summary>Nisan and Szegedy (CC 1994) showed that any Boolean function $f:\{0,1\}^n\to\{0,1\}$ that depends on all its input variables, when represented as a real-valued multivariate polynomial $P(x_1,\ldots,x_n)$, has degree at least $\log n - O(\log \log n)$. This was improved to a tight $(\log n - O(1))$ bound by Chiarelli, Hatami and Saks (Combinatorica 2020). Similar statements are also known for other Boolean function complexity measures such as Sensitivity (Simon (FCT 1983)), Quantum query complexity, and Approximate degree (Ambainis and de Wolf (CC 2014)). 

In this paper, we address this question for \emph{Probabilistic degree}. The function $f$ has probabilistic degree at most $d$ if there is a random real-valued polynomial of degree at most $d$ that agrees with $f$ at each input with high probability. Our understanding of this complexity measure is significantly weaker than those above: for instance, we do not even know the probabilistic degree of the OR function, the best-known bounds put it between $(\log n)^{1/2-o(1)}$ and $O(\log n)$ (Beigel, Reingold, Spielman (STOC 1991); Tarui (TCS 1993); Harsha, Srinivasan (RSA 2019)).

Here we can give a near-optimal understanding of the probabilistic degree of $n$-variate functions $f$, \emph{modulo} our lack of understanding of the probabilistic degree of OR. We show that if the probabilistic degree of OR is $(\log n)^c$, then the minimum possible probabilistic degree of such an $f$ is at least $(\log n)^{c/(c+1)-o(1)}$, and we show this is tight up to $(\log n)^{o(1)}$ factors.</summary>
    <updated>2021-07-11T04:50:01Z</updated>
    <published>2021-07-11T04:50:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-18T21:37:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/07/10/angles-arc-triangles</id>
    <link href="https://11011110.github.io/blog/2021/07/10/angles-arc-triangles.html" rel="alternate" type="text/html"/>
    <title>Angles of arc-triangles</title>
    <summary>Piecewise-circular curves or, if you like, arc-polygons are a very old topic in mathematics. Archimedes and Pappus studied the arbelos, a curved triangle formed from three semicircles, and Hippocrates of Chios found that the lune of Hippocrates, a two-sided figure bounded by a semicircle and a quarter-circle, has the same area as an isosceles right triangle stretched between the same two points. The history of the Reuleaux triangle, bounded by three sixths of circles, stretches back well past Reuleaux to the shapes of of Gothic church windows and its use by Leonardo da Vinci for fortress floor plans and world map projections. But despite their long history and frequent use (for instance in the design of machined parts), there are some basic properties of arc-polygons that seem to have been unexplored so far.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Piecewise-circular curves or, if you like, arc-polygons are a very old topic in mathematics. Archimedes and Pappus studied the <a href="https://en.wikipedia.org/wiki/Arbelos">arbelos</a>, a curved triangle formed from three semicircles, and Hippocrates of Chios found that the <a href="https://en.wikipedia.org/wiki/Lune_of_Hippocrates">lune of Hippocrates</a>, a two-sided figure bounded by a semicircle and a quarter-circle, has the same area as an isosceles right triangle stretched between the same two points. The history of the <a href="https://en.wikipedia.org/wiki/Reuleaux_triangle">Reuleaux triangle</a>, bounded by three sixths of circles, stretches back well past Reuleaux to the shapes of of Gothic church windows and its use by Leonardo da Vinci for fortress floor plans and world map projections. But despite their long history and frequent use (for instance in the design of machined parts), there are some basic properties of arc-polygons that seem to have been unexplored so far.</p>

<p>I looked at one of these properties, <a href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html">the ability of arc-triangles to tile the plane</a>, in an earlier post. Another of these properties involves the feasible combinations of angles of these shapes. As is well known, in a straight-sided triangle in the plane, the three interior angles always sum to exactly \(\pi\), and any three positive angles summing to \(\pi\) are possible. Let \(T\) be the set of triples of angles \((\theta_1,\theta_2,\theta_3)\) from triangles, and reinterpret these triples as coordinates of points in Euclidean space. Then \(T\) is itself an equilateral triangle, with corners at the three points \((\pi,0,0)\), \((0,\pi,0)\), and \((0,0\pi)\). (More precisely, it’s the relative interior of this triangle.)</p>

<p>What about arc-triangles? Are their angles similarly constrained? What shape do their triples of angles make? First of all, their angles don’t have a fixed sum (except for the tilers, for which this sum is again \(\pi\)). The arbelos has three interior angles that are all zero, summing to zero. The Reuleux triangle has three angles of \(2\pi/3\), summing to \(2\pi\). <a href="https://11011110.github.io/blog/2021/05/15/linkage.html">Boscovitch’s cardioid</a>, below, uses three semicircles like the arbelos, but with one interior angle of \(2\pi\) and two others equal to \(\pi\), summing to \(4\pi\). The <a href="https://en.wikipedia.org/wiki/Trefoil">trefoil</a>, a common architectural motif, bulges outward from its three corners, forming interior angles that are much larger, up to \(2\pi\) each for a trefoil made from three \(5/6\)-circle arcs, for a total interior angle of \(6\pi\).</p>

<p style="text-align: center;"><img alt="Boscovich's cardioid" src="https://11011110.github.io/blog/assets/2021/boscovich.svg"/></p>

<p>Nevertheless, for a non-self-crossing arc-triangle, not all combinations of angles are possible. For instance, it’s not possible to have one angle that is zero and two that are \(2\pi\). My new preprint, “Angles of arc-polygons and lombardi drawings of cacti” (<a href="https://arxiv.org/abs/2107.03615">arXiv:2107.03615
</a>, with UCI students Daniel Frishberg and Martha Osegueda, to appear at CCCG) proves a precise characterization: beyond the obvious requirement that each angle \(\theta_i\) be in the range \(0\le\theta_i\le 2\pi\), we have only the additional inequalities</p>

\[-\pi &lt; \frac{\pi - \theta_i + \theta_{i+1} - \theta_{i+2}}{2} &lt; \pi\]

<p>where the index arithmetic is done modulo three. The formula in the middle of each of these inequalities is itself an angle, the angle of incidence between one of the circular arcs of the arc-triangle and the circle through its three corners. Where the straight triangles had an equilateral-triangle feasible region, the arc-triangles have a more complicated shape. The obvious constraints \(0\le\theta_i\le 2\pi\) would produce a cubical feasible region \([0,2\pi]^3\), but the additional inequalities above cut off six corners of the cube, leaving a feasible region looking like this:</p>

<p style="text-align: center;"><img alt="The feasible region for triples of angles of arc-triangles" src="https://11011110.github.io/blog/assets/2021/feasible-arc-triangles.svg"/></p>

<p>The motivating application for all of this is graph drawing, and more specifically Lombardi drawing, in which edges are circular arcs meeting at equal angles at each vertex. Using our new understanding of arc-polygons, we prove that every <a href="https://en.wikipedia.org/wiki/Cactus_graph">cactus graph</a> has a planar Lombardi drawing for its natural embedding (the one in which each cycle of the cactus forms a face) but might not for some other embeddings, including the one below.</p>

<p style="text-align: center;"><img alt="An embedded cactus that has no planar Lombardi drawing" src="https://11011110.github.io/blog/assets/2021/badhat.svg"/></p>

<p>But beyond graph drawing, I think that the long history and many applications of arc-polygons justifies more study of their general properties. For instance, what about arc-polygons with more than three sides? What can their angles be? Our paper has a partial answer, enough to answer the questions we asked in our Lombardi drawing application, but a complete characterization for arc-polygons of more than three sides is still open.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106557711895868173">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-07-10T11:06:00Z</updated>
    <published>2021-07-10T11:06:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-07-16T01:38:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/097</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/097" rel="alternate" type="text/html"/>
    <title>TR21-097 |  Number of Variables for Graph Identification and the Resolution of GI Formulas | 

	Jacobo Toran, 

	Florian Wörz</title>
    <summary>We show that the number of variables and the quantifier depth needed to distinguish a pair of graphs by first-order logic sentences exactly match the complexity measures of clause width and positive depth needed to refute the corresponding graph isomorphism formula in propositional narrow resolution. 

Using this connection, we obtain upper and lower bounds for refuting graph  isomorphism formulas in (normal) resolution. In particular, we show that if $k$ is the minimum number of variables needed to distinguish two graphs with $n$ vertices each, then there is an $n^{\mathrm{O}(k)}$ resolution refutation size upper bound for the corresponding isomorphism formula, as well as lower bounds of $2^{k-1}$ and $k$ for the tree-like resolution size and resolution clause space for this formula. We also show a resolution size lower bound of ${\exp} \big( \Omega(k^2/n) \big)$ for the case of colored graphs with constant color class size.

Applying these results, we prove the first exponential lower bound for graph isomorphism formulas in the proof system SRC-1, a system that extends resolution with a global symmetry rule, thereby answering an open question posed by Schweitzer and Seebach.</summary>
    <updated>2021-07-09T15:37:56Z</updated>
    <published>2021-07-09T15:37:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-18T21:37:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/07/09/spanners-unit-ball</id>
    <link href="https://11011110.github.io/blog/2021/07/09/spanners-unit-ball.html" rel="alternate" type="text/html"/>
    <title>Spanners for unit ball graphs in doubling spaces</title>
    <summary>My student Hadi Khodabandeh had a paper with me on spanners earlier this year at SoCG, in which we showed that the greedy spanner algorithm for points in the Euclidean plane produces graphs with few crossings and small separators. Now we have another spanner preprint: “Optimal spanners for unit ball graphs in doubling metrics”, arXiv:2106.15234.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My student Hadi Khodabandeh had a paper with me on spanners earlier this year at SoCG, in which we showed that <a href="https://11011110.github.io/blog/2020/02/17/spanners-have-sparse.html">the greedy spanner algorithm for points in the Euclidean plane produces graphs with few crossings and small separators</a>. Now we have another spanner preprint: <a href="https://arxiv.org/abs/2106.15234">“Optimal spanners for unit ball graphs in doubling metrics”, arXiv:2106.15234</a>.</p>

<p><a href="https://en.wikipedia.org/wiki/Doubling_space">Doubling metrics</a> are a generalization of Euclidean spaces. Like Euclidean spaces, they have a dimension, the <em>doubling dimension</em>, but it might not be an integer. Even the doubling dimension of the Euclidean plane itself is \(\log_2 7\approx 2.807355\); this means that every circular disk of radius \(r\) in the plane can be covered by seven closed disks of radius \(r/2\).</p>

<p style="text-align: center;"><img alt="Seven disks of radius $$r/2$$ cover a single disk of radius $$r$$" src="https://11011110.github.io/blog/assets/2021/doubling-dim.svg"/></p>

<p>Analogously, the <em>doubling constant</em> of a metric space (if it exists) is the smallest number \(c\) such that every closed metric ball (the set of points within some radius \(r\) of a fixed point) can be covered by \(c\) balls of half the radius. The <em>doubling dimension</em> is the binary logarithm of the doubling constant. A metric space is a <em>doubling metric</em> or <em>doubling space</em> if it has a doubling constant and doubling dimension. This is true for all Euclidean spaces: for instance, if you cover a ball with a grid of hypercubes, sized small enough that their long diagonal has length at most the radius of the ball, you obtain doubling constant at most \(\lceil 2\sqrt{d}\rceil^d\), although it seems difficult to compute the precise doubling constant in general.</p>

<p style="text-align: center;"><img alt="Nine disks of radius $$r/2$$ cover a square grid that covers a single disk of radius $$r$$" src="https://11011110.github.io/blog/assets/2021/grid-doubling.svg"/></p>

<p>The hyperbolic plane provides a natural example of a space that is not a doubling space: arbitrarily large-radius disks require arbitrarily many half-radius disks to cover them.</p>

<p>Many results in computational geometry can be generalized to doubling metrics, but not always, and sometimes with difficulty. That includes results on spanners, as we consider in our paper. A spanner of a weighted graph is a subgraph whose shortest path distances approximate the distances in the full graph, and often for spanners of metric spaces one uses the complete graph, weighted by the metric distance between each pair of points. But here, we are using a different graph, the unit ball graph. The unit ball graph, for points in a continuous space, has an edge whenever two unit balls centered at two of the points have a nonempty intersection, and it can be extended to discrete point sets by instead including an edge whenever two points are at distance at most 2 (or, as in our paper, distance at most 1; scaling doesn’t really change anything). The weights are the same as in the complete graph. If a spanner accurately approximates all edge weights, it approximates all paths.</p>

<p>The unit ball graph has fewer edges to approximate than the complete graph. But that actually makes it harder to approximate, because by the same token there are fewer edges that can be used in the spanner. Despite that, the greedy spanner algorithm still produces a spanner, but one of its key properties is lost when going from Euclidean to doubling spaces: Euclidean greedy spanners have bounded degree, but greedy spanners in doubling spaces do not. Instead, our paper provides different spanner algorithms that apply to unit ball graphs, approximate paths in these graphs arbitrarily well, have bounded degree, have total weight a constant times that of the minimum spanning tree, and can be constructed efficiently in a distributed model of computing. I think the details are too technical to go into here, so see the paper for more.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106551534535329957">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-07-09T09:06:00Z</updated>
    <published>2021-07-09T09:06:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-07-16T01:38:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/096</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/096" rel="alternate" type="text/html"/>
    <title>TR21-096 |  Keep That Card in Mind: Card Guessing with Limited Memory | 

	Boaz Menuhin, 

	Moni Naor</title>
    <summary>A card guessing game is played between two players, Guesser and Dealer. At the beginning of the game, the Dealer holds a deck of $n$ cards (labeled $1, ..., n$). For $n$ turns, the Dealer draws a card from the deck, the Guesser guesses which card was drawn, and then the card is discarded from the deck. The Guesser receives a point for each correctly guessed card. 

With perfect memory, a Guesser can keep track of all cards that were played so far and pick at random a card that has not appeared so far, yielding in expectation $\ln n$ correct guesses. With no memory, the best a Guesser can do will result in a single guess in expectation. 

We consider the case of a memory bounded Guesser that has $m &lt; n$ memory bits. We show that the performance of such a memory bounded Guesser depends much on the behavior of the Dealer. In more detail, we show that there is a gap between the static case, where the Dealer draws cards from a properly shuffled deck or a prearranged one, and the adaptive case, where the Dealer draws cards thoughtfully, in an adversarial manner. Specifically: 

1. We show a Guesser with $O(\log^2 n)$ memory bits that scores a near optimal result against any static Dealer. 

2. We show that no Guesser with $m$ bits of memory can score better than $O(\sqrt{m})$ correct guesses, thus, no Guesser can score better than $\min \{\sqrt{m}, \ln n\}$, i.e., the above Guesser is optimal. 

3. We show an efficient adaptive Dealer against which no Guesser with $m$ memory bits can make more than $\ln m + 2 \ln \log n + O(1)$ correct guesses in expectation. 

These results are (almost) tight, and we prove them using compression arguments that harness the guessing strategy for encoding.</summary>
    <updated>2021-07-08T21:32:56Z</updated>
    <published>2021-07-08T21:32:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-18T21:37:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2021/07/08/imp-reg-tf/</id>
    <link href="http://offconvex.github.io/2021/07/08/imp-reg-tf/" rel="alternate" type="text/html"/>
    <title>Implicit Regularization in Tensor Factorization&amp;#58; Can Tensor Rank Shed Light on Generalization in Deep Learning?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In effort to understand implicit regularization in deep learning, a lot of theoretical focus is being directed at matrix factorization, which can be seen as linear neural networks.
This post is based on our <a href="https://arxiv.org/pdf/2102.09972.pdf">recent paper</a> (to appear at ICML 2021), where we take a step towards practical deep learning, by investigating <em>tensor factorization</em> — a model equivalent to a certain type of non-linear neural networks.
It is well known that <a href="https://arxiv.org/pdf/0911.1393.pdf">most tensor problems are NP-hard</a>, and accordingly, the common sentiment is that working with tensors (in both theory and practice) entails extreme difficulties.
However, by adopting a dynamical systems view, we manage to avoid such difficulties, and establish an implicit regularization towards low <em>tensor rank</em>.
Our results suggest that tensor rank may shed light on generalization in deep learning.</p>

<h2 id="challenge-finding-a-right-measure-of-complexity">Challenge: finding a right measure of complexity</h2>

<p>Overparameterized neural networks are mysteriously able to generalize even when trained without any explicit regularization.
Per conventional wisdom, this generalization stems from an <em>implicit regularization</em> — a tendency of gradient-based optimization to fit training examples with predictors of minimal ‘‘complexity.’’
A major challenge in translating this intuition to provable guarantees is that we lack measures for predictor complexity that are quantitative (admit generalization bounds), and at the same time, capture the essence of natural data (images, audio, text etc.), in the sense that it can be fit with predictors of low complexity.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/imp_reg_tf_data_complexity.png" style="width: 500px; padding-bottom: 10px; padding-top: 5px;"/>
<br/>
<i><b>Figure 1:</b> 
To explain generalization in deep learning, a complexity <br/>
measure must allow the fit of natural data with low complexity. On the <br/>
other hand, when fitting data which does not admit generalization, <br/>
e.g. random data, the complexity should be high. 
</i>
</div>
<p><br/></p>

<h2 id="a-common-testbed-matrix-factorization">A common testbed: matrix factorization</h2>

<p>Without a clear complexity measure for practical neural networks, existing analyses usually focus on simple settings where a notion of complexity is obvious. 
A common example of such a setting is <em>matrix factorization</em> — matrix completion via linear neural networks. 
This model was discussed pretty extensively in previous posts (see <a href="http://www.offconvex.org/2019/06/03/trajectories/">one</a> by Sanjeev, <a href="http://www.offconvex.org/2019/07/10/trajectories-linear-nets/">one</a> by Nadav and Wei and <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">another one</a> by Nadav), but for completeness we present it again here.</p>

<p>In <em>matrix completion</em> we’re given a subset of entries from an unknown matrix $W^* \in \mathbb{R}^{d, d’}$, and our goal is to predict the unobserved entries. 
This can be viewed as a supervised learning problem with $2$-dimensional inputs, where the label of the input $( i , j )$ is $( W^* )_{i,j}$.
Under such a viewpoint, the observed entries are the training set, and the average reconstruction error over unobserved entries is the test error, quantifying generalization.
A predictor can then be thought of as a matrix, and a natural notion of complexity is its <em>rank</em>.
Indeed, in many real-world scenarios (a famous example is the <a href="https://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a>) one is interested in <a href="https://arxiv.org/pdf/1601.06422.pdf">recovering a low rank matrix from incomplete observations</a>.</p>

<p>A ‘‘deep learning approach’’ to matrix completion is matrix factorization, where the idea is to use a linear neural network (fully connected neural network with no non-linearity), and fit observations via gradient descent (GD). 
This amounts to optimizing the following objective:</p>

<div style="text-align: center;">
\[
 \min\nolimits_{W_1 , \ldots , W_L} ~ \sum\nolimits_{(i,j) \in observations} \big[ ( W_L \cdots W_1 )_{i , j} - (W^*)_{i,j} \big]^2 ~.
\]
</div>

<p>It is obviously possible to constrain the rank of the produced solution by limiting the shared dimensions of the weight matrices $\{ W_j \}_j$. 
However, from an implicit regularization standpoint, the most interesting case is where rank is unconstrained and the factorization can express any matrix. 
In this case there is no explicit regularization, and the kind of solution we get is determined implicitly by the parameterization and the optimization algorithm.</p>

<p>As it turns out, in practice, matrix factorization with near-zero initialization and small step size tends to accurately recover low rank matrices.
This phenomenon (first identified in <a href="https://papers.nips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf">Gunasekar et al. 2017</a>) manifests some kind of implicit regularization, whose mathematical characterization drew a lot of interest.
It was initially conjectured that matrix factorization implicitly minimizes nuclear norm (<a href="https://papers.nips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf">Gunasekar et al. 2017</a>), but recent evidence points to implicit rank minimization, stemming from incremental learning dynamics (see <a href="https://papers.nips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf">Arora et al. 2019</a>; <a href="https://papers.nips.cc/paper/2020/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf">Razin &amp; Cohen 2020</a>; <a href="https://openreview.net/pdf/e29b53584bc9017cb15b9394735cd51b56c32446.pdf">Li et al. 2021</a>). 
Today, it seems we have a relatively firm understanding of generalization in matrix factorization.
There is a complexity measure for predictors — matrix rank — by which implicit regularization strives to lower complexity, and the data itself is of low complexity (i.e. can be fit with low complexity). 
Jointly, these two conditions lead to generalization.</p>

<h2 id="beyond-matrix-factorization-tensor-factorization">Beyond matrix factorization: tensor factorization</h2>

<p>Matrix factorization is interesting on its own behalf, but as a theoretical surrogate for deep learning it is limited.
First, it corresponds to <em>linear</em> neural networks, and thus misses the crucial aspect of non-linearity. 
Second, viewing matrix completion as a prediction problem, it doesn’t capture tasks with more than two input variables.
As we now discuss, both of these limitations can be lifted if instead of matrices one considers tensors.</p>

<p>A tensor can be thought of as a multi-dimensional array.
The number of axes in a tensor is called its <em>order</em>.
In the task of <em>tensor completion</em>, a subset of entries from an unknown tensor $\mathcal{W}^* \in \mathbb{R}^{d_1, \ldots, d_N}$ are given, and the goal is to predict the unobserved entries. 
Analogously to how matrix completion can be viewed as a prediction problem over two input variables, order-$N$ tensor completion can be seen as a prediction problem over $N$ input variables (each corresponding to a different axis).
In fact, any multi-dimensional prediction task with discrete inputs and scalar output can be formulated as a tensor completion problem.
Consider for example the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>, and for simplicity assume that image pixels hold one of two values, i.e. are either black or white. 
The task of predicting labels for the $28$-by-$28$ binary images can be seen as an order-$784$ (one axis for each pixel) tensor completion problem, where all axes are of length $2$ (corresponding to the number of values a pixel can take). 
For further details on how general prediction tasks map to tensor completion problems see <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a>.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/pred_prob_to_tensor_comp.png" style="width: 550px; padding-bottom: 10px; padding-top: 5px;"/>
<br/>
<i><b>Figure 2:</b> 
Prediction tasks can be viewed as tensor completion problems. <br/>
For example, predicting labels for input images with $3$ pixels, each taking <br/>
one of $5$ grayscale values, corresponds to completing a $5 \times 5 \times 5$ tensor.
</i>
</div>
<p><br/>
Like matrices, tensors can be factorized. 
The most basic scheme for factorizing tensors, named CANDECOMP/PARAFAC (CP), parameterizes a tensor as a sum of outer products (for information on this scheme, as well as others, see the <a href="http://www.kolda.net/publication/TensorReview.pdf">excellent survey</a> of Kolda and Bader).
In <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a> and this post, we use the term <em>tensor factorization</em> to refer to solving tensor completion by fitting observations via GD over CP parameterization, i.e. over the following objective ($\otimes$ here stands for outer product):</p>

<div style="text-align: center;">
\[
\min\nolimits_{ \{ \mathbf{w}_r^n \}_{r , n} } \sum\nolimits_{ (i_1 , ... , i_N) \in observations } \big[ \big(  {\textstyle \sum}_{r = 1}^R \mathbf{w}_r^1 \otimes \cdots \otimes \mathbf{w}_r^N \big)_{i_1 , \ldots , i_N} - (\mathcal{W}^*)_{i_1 , \ldots , i_N} \big]^2 ~.
\]
</div>

<p>The concept of rank naturally extends from matrices to tensors.
The <em>tensor rank</em> of a given tensor $\mathcal{W}$ is defined to be the minimal number of components (i.e. of outer product summands) $R$ required for CP parameterization to express it.
Note that for order-$2$ tensors, i.e. for matrices, this exactly coincides with matrix rank.
We can explicitly constrain the tensor rank of solutions found by tensor factorization via limiting the number of components $R$. 
However, since our interest lies on implicit regularization, we consider the case where $R$ is large enough for any tensor to be expressed.</p>

<p>By now you might be wondering what does tensor factorization have to do with deep learning.
Apparently, as Nadav mentioned in an <a href="http://www.offconvex.org/2020/11/27/reg_dl_not_norm/">earlier post</a>, analogously to how matrix factorization is equivalent to matrix completion (two-dimensional prediction) via linear neural networks, tensor factorization is equivalent to tensor completion (multi-dimensional prediction) with a certain type of <em>non-linear</em> neural networks (for the exact details behind the latter equivalence see <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a>). 
It therefore represents a setting one step closer to practical neural networks.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/mf_lnn_tf_nonlinear.png" style="width: 900px; padding-bottom: 10px; padding-top: 5px;"/>
<br/>
<i><b>Figure 3:</b> 
While matrix factorization corresponds to a linear neural network, <br/>
tensor factorization corresponds to a certain non-linear neural network.
</i>
</div>
<p><br/>
As a final piece of the analogy between matrix and tensor factorizations, in a <a href="https://arxiv.org/pdf/2005.06398.pdf">previous paper</a> (described in an <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">earlier post</a>) Noam and Nadav demonstrated empirically that (similarly to the phenomenon discussed above for matrices) tensor factorization with near-zero initialization and small step size tends to accurately recover low rank tensors.
Our goal in the <a href="https://arxiv.org/pdf/2102.09972.pdf">current paper</a> was to mathematically explain this finding. 
To avoid the <a href="https://arxiv.org/pdf/0911.1393.pdf">notorious difficulty of tensor problems</a>, we chose to adopt a dynamical systems view, and analyze directly the trajectories induced by GD.</p>

<h2 id="dynamical-analysis-implicit-tensor-rank-minimization">Dynamical analysis: implicit tensor rank minimization</h2>

<p>So what can we say about the implicit regularization in tensor factorization? 
At the core of our analysis is the following dynamical characterization of component norms:</p>

<blockquote>
  <p><strong>Theorem:</strong>
Running gradient flow (GD with infinitesimal step size) over a tensor factorization with near-zero initialization leads component norms to evolve by:
[ \frac{d}{dt} || \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) || \propto \color{brown}{|| \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) ||^{2 - 2/N}} ~,
]
where $\mathbf{w}_r^1 (t), \ldots, \mathbf{w}_r^N (t)$ denote the weight vectors at time $t \geq 0$.</p>
</blockquote>

<p>According to the theorem above, component norms evolve at a rate proportional to their size exponentiated by $\color{brown}{2 - 2 / N}$ (recall that $N$ is the order of the tensor to complete).
Consequently, they are subject to a momentum-like effect, by which they move slower when small and faster when large. 
This suggests that when initialized near zero, components tend to remain close to the origin, and then, after passing a critical threshold, quickly grow until convergence. 
Intuitively, these dynamics induce an incremental process where components are learned one after the other, leading to solutions with a few large components and many small ones, i.e. to (approximately) low tensor rank solutions!</p>

<p>We empirically verified the incremental learning of components in many settings. 
Here is a representative example from one of our experiments (see <a href="https://arxiv.org/pdf/2102.09972.pdf">the paper</a> for more):</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/tf_dyn_exps.png" style="width: 800px; padding-bottom: 15px; padding-top: 10px;"/>
<br/>
<i><b>Figure 4:</b> 
Dynamics of component norms during GD over tensor factorization. <br/>
An incremental learning effect is enhanced as initialization scale decreases, <br/>
leading to accurate completion of a low rank tensor.
</i>
</div>
<p><br/>
Using our dynamical characterization of component norms, we were able to prove that with sufficiently small initialization, tensor factorization (approximately) follows a trajectory of rank one tensors for an arbitrary amount of time. 
This leads to:</p>

<blockquote>
  <p><strong>Theorem:</strong>
If tensor completion has a rank one solution, then under certain technical conditions, tensor factorization will reach it.</p>
</blockquote>

<p>It’s worth mentioning that, in a way, our results extend to tensor factorization the incremental rank learning dynamics known for matrix factorization (cf. <a href="https://papers.nips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf">Arora et al. 2019</a> and <a href="https://arxiv.org/pdf/2012.09839v1.pdf">Li et al. 2021</a>). 
As typical when transitioning from matrices to tensors, this extension entailed various challenges that necessitated use of different techniques.</p>

<h2 id="tensor-rank-as-measure-of-complexity">Tensor rank as measure of complexity</h2>

<p>Going back to the beginning of the post, recall that a major challenge towards understanding implicit regularization in deep learning is that we lack measures for predictor complexity that capture natural data. 
Now, let us recap what we have seen thus far:
$(1)$ tensor completion is equivalent to multi-dimensional prediction; 
$(2)$ tensor factorization corresponds to solving the prediction task with certain non-linear neural networks; 
and 
$(3)$ the implicit regularization of these non-linear networks, i.e. of tensor factorization, minimizes tensor rank.
Motivated by these findings, we ask the following:</p>

<blockquote>
  <p><strong>Question:</strong> 
Can tensor rank serve as a measure of predictor complexity?</p>
</blockquote>

<p>We empirically explored this prospect by evaluating the extent to which tensor rank captures natural data, i.e. to which natural data can be fit with predictors of low tensor rank.
As testbeds we used <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> and <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> datasets, comparing the resulting errors against those obtained when fitting two randomized variants: one generated via shuffling labels (‘‘rand label’’), and the other by replacing inputs with noise (‘‘rand image’’).</p>

<p>The following plot, displaying results for Fashion-MNIST (those for MNIST are similar), shows that with predictors of low tensor rank the original data is fit way more accurately than the randomized datasets. 
Specifically, even with tensor rank as low as one the original data is fit relatively well, while the error in fitting random data is close to trivial (variance of the label). 
This suggests that tensor rank as a measure of predictor complexity has potential to capture aspects of natural data! 
Note also that an accurate fit with low tensor rank coincides with low test error, which is not surprising given that low tensor rank predictors can be described with a small number of parameters.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/exp_complexity_fmnist.png" style="width: 600px; padding-bottom: 15px; padding-top: 10px;"/>
<br/>
<i><b>Figure 5:</b> 
Evaluation of tensor rank as a measure of complexity — standard datasets <br/>
can be fit accurately with predictors of low tensor rank (far beneath what is required by <br/>
random datasets), suggesting it may capture aspects of natural data. Plot shows mean <br/>
error of predictors with low tensor rank over Fashion-MNIST. Markers correspond <br/>
to separate runs differing in the explicit constraint on the tensor rank.
</i>
</div>

<h2 id="concluding-thoughts">Concluding thoughts</h2>

<p>Overall, <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a> shows that tensor rank captures both the implicit regularization of a certain type of non-linear neural networks, and aspects of natural data. 
In light of this, we believe tensor rank (or more advanced notions such as hierarchical tensor rank) might pave way to explaining both implicit regularization in more practical neural networks, and the properties of real-world data translating this implicit regularization to generalization.</p>

<p><a href="https://noamrazin.github.io/">Noam Razin</a>, <a href="https://asafmaman101.github.io/">Asaf Maman</a>, <a href="http://www.cohennadav.com/">Nadav Cohen</a></p></div>
    </summary>
    <updated>2021-07-08T09:00:00Z</updated>
    <published>2021-07-08T09:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2021-07-17T23:30:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/095</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/095" rel="alternate" type="text/html"/>
    <title>TR21-095 |  LEARN-Uniform Circuit Lower Bounds and Provability in Bounded Arithmetic | 

	Valentine Kabanets, 

	Igor Oliveira, 

	Marco Carmosino, 

	Antonina Kolokolova</title>
    <summary>We investigate randomized LEARN-uniformity, which captures the power of randomness and equivalence queries (EQ) in the construction of Boolean circuits for an explicit problem. This is an intermediate notion between P-uniformity and non-uniformity motivated by connections to learning, complexity, and logic.  Building on a number of techniques, we establish the first unconditional lower bounds against LEARN-uniform circuits:

-- For all $c\geq 1$, there is $L \in P$ that is not computable by circuits of size $n \cdot (\log n)^c$ generated in deterministic polynomial time with $o(\log n/\log \log n)$ equivalence queries to $L$. In other words, small circuits for $L$ cannot be efficiently learned using a bounded number of EQs.
-- For each $k\geq 1$, there is $L \in NP$ such that circuits for $L$ of size $O(n^k)$ cannot be learned in deterministic polynomial time with access to $n^{o(1)}$ EQs.
--  For each $k\geq 1$, there is a problem in promise-ZPP that is not in FZPP-uniform $SIZE[n^k]$.
-- Conditional and unconditional lower bounds against LEARN-uniform circuits in the general setting that combines randomized uniformity and access to EQs.

In all these lower bounds, the learning algorithm is allowed to run in arbitrary polynomial time, while the hard problem is computed in some fixed polynomial time.

We employ these results to investigate the (un)provability of non-uniform circuit upper bounds (e.g., Is NP contained in $SIZE[n^3]$?) in theories of bounded arithmetic. Some questions of this form have been addressed in recent papers of Krajicek-Oliveira (2017), Muller-Bydzovsky (2020), and Bydzovsky-Krajicek-Oliveira (2020) via a mixture of techniques from proof theory, complexity theory, and model theory. In contrast, by extracting computational information from proofs via a direct translation to LEARN-uniformity, we establish robust unprovability theorems that unify, simplify, and extend nearly all previous results. In addition, our lower bounds against randomized LEARN-uniformity yield unprovability results for theories augmented with the \emph{dual weak pigeonhole principle}, such as $APC^1$ (Jerabek, 2007), which is known to formalize a large fragment of modern complexity theory.

Finally, we make precise potential limitations of theories of bounded arithmetic such as PV (Cook, 1975) and Jerabek's theory $APC^1$, by showing unconditionally that these theories cannot prove statements like ``$NP\not\subseteq BPP \wedge NP\subset io$-P/poly'', i.e., that NP is uniformly ``hard'' but non-uniformly ``easy'' on infinitely many input lengths. In other words, if we live in such a complexity world, then this cannot be established feasibly.</summary>
    <updated>2021-07-07T21:27:49Z</updated>
    <published>2021-07-07T21:27:49Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-18T21:37:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/open-problem-optimal-query-release/</id>
    <link href="https://differentialprivacy.org/open-problem-optimal-query-release/" rel="alternate" type="text/html"/>
    <title>Open Problem - Optimal Query Release for Pure Differential Privacy</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Releasing large sets of statistical queries is a centerpiece of the theory of differential privacy.  Here, we are given a <em>dataset</em> \(x = (x_1,\dots,x_n) \in [T]^n\), and a set of <em>statistical queries</em> \(f_1,\dots,f_k\), where each query is defined by some bounded function \(f_j : [T] \to [-1,1]\), and (abusing notation) is defined as
\[
f_j(x) = \frac{1}{n} \sum_{i=1}^{n} f_j(x_i).
\]
We use \(f(x) = (f_1(x),\dots,f_k(x))\) to denote the vector consisting of the true answers to all these queries.
Our goal is to design an \((\varepsilon, \delta)\)-differentially private algorithm \(M\) that takes a dataset \(x\in [T]^n\) and outputs a random vector \(M(x)\in \mathbb{R}^k\) such that \(\| M(x) - f(x) \|\) is small in expectation for some norm \(\|\cdot\|\). Usually algorithms for this problem also give high probability bounds on the error, but we focus on expected error for simplicity.</p>

<p>This problem has been studied for both <em>pure differential privacy</em> (\(\delta = 0\)) and <em>appproximate differential privacy</em> (\(\delta &gt; 0\)), and for both \(\ell_\infty\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{\infty} ) \leq \alpha,
\]
and \(\ell_2\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{2} ) \leq \alpha k^{1/2},
\]
giving four variants of the problem.  By now we know tight worst-case upper and lower bounds for two of these variants, and nearly tight bounds (up to logarithmic factors) for a third. The tightest known upper bounds are given in the following table.</p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>Pure DP</td>
      <td>Approx DP</td>
    </tr>
    <tr>
      <td>\( \ell_2 \)<br/>error</td>
      <td>\( \alpha \lesssim \left(\frac{\log^2 k ~\cdot~ \log^{3/2}T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://arxiv.org/abs/1212.0297">NTZ13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://guyrothblum.files.wordpress.com/2014/11/drv10.pdf">DRV10</a>]</td>
    </tr>
    <tr>
      <td>\( \ell_\infty \)<br/>error</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log T}{\varepsilon n} \right)^{1/3} \)  <br/> [<a href="https://arxiv.org/abs/1109.2229">BLR13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://guyrothblum.files.wordpress.com/2014/11/hr10.pdf">HR10</a>, <a href="https://arxiv.org/abs/1107.3731">GRU12</a>]</td>
    </tr>
  </tbody>
</table>

<p>The bounds for approximate DP are known to be tight [<a href="https://arxiv.org/abs/1311.3158">BUV14</a>].  Our two open problems both involve improving the best known upper bounds for pure differential privacy.</p>

<blockquote>
  <p><b>Open Problem 1:</b> What is the best possible \(\ell_\infty\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the known upper bound in the table can be improved to
\[
\alpha = \left(\frac{\log k \cdot \log T}{\varepsilon n} \right)^{1/2},
\]
which is known to be the best possible [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1].</p>

<blockquote>
  <p><b>Open Problem 2:</b> What is the best possible \(\ell_2\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the upper bound can be improved to
\[
\alpha = \left(\frac{\log T}{\varepsilon n} \right)^{1/2}.
\]
The construction used in [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1] can be analyzed to show this bound would be tight. Note, in particular, that this conjecture implies that the tight upper bound has no dependence on the number of queries, similarly to the case of \(\ell_2\) error and approximate DP.</p></div>
    </summary>
    <updated>2021-07-07T17:45:00Z</updated>
    <published>2021-07-07T17:45:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-07-17T23:31:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8892467103290123546</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8892467103290123546/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8892467103290123546" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8892467103290123546" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html" rel="alternate" type="text/html"/>
    <title>Would you take this bet (Part 1) ?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I am going to present a well known paradox (I didn't know it until last week, but the source I read said it was well known) and ask your opinion in this post, and reveal my thoughts in my next post.</p><p>I don't want you to go to the web and find out about it, I want your natural thoughts. Of course I can't stop you, but note that I did not give the name of the paradox. </p><p>Here it is:</p><p>I offer you the following bet: </p><p>I will flip a coin.</p><p>If  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>If  HEADS you get 2 dollars and we end there.</p><p>If  TAILS I flip again</p><p><br/></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>etc. </p><p>1) Expected value: </p><p>Prob of getting 1 dollar is 1/2</p><p>Prob of getting 2 dollars is 1/2^2</p><p>Prob of getting 2^2 dollars is 1/2^3</p><p>etc</p><p>Hence the Expected Value is </p><p>1/2 + 1/2 + 1/2 + ... = INFINITY</p><p><br/></p><p>QUESTION: Would you pay $1000 to play the game?</p><p>Leave your answer in the comments and you may say whatever you want as well,</p><p>but I request you don't give the name of the paradox if you know it. </p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-07-07T00:58:00Z</updated>
    <published>2021-07-07T00:58:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-18T07:49:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=879</id>
    <link href="https://emanueleviola.wordpress.com/2021/07/06/windows-never-changes/" rel="alternate" type="text/html"/>
    <title>Windows never changes</title>
    <summary>For months, Windows 10 complained that it didn’t have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the “storage” function that supposedly tells you what’s taking space wasn’t even close to the truth. This became so bad that I was forced to remove some […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>For months, Windows 10 complained that it didn’t have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the “storage” function that supposedly tells you what’s taking space wasn’t even close to the truth. This became so bad that I was forced to remove some things I didn’t want to remove, often with a lot of effort, because space was so tight that Windows didn’t even have enough to run the uninstaller! In the end I became so desperate that I installed <em>TreeSize Free</em>. It quickly revealed that <em>crash plan </em> was taking up a huge amount of space. This revealed to be associated to the <em>Code42</em> program — a program that the system was listing as taking 200MB. Well, uninstalling Code42 freed SIXTY PERCENT of the hard disk space, 140GB.</p></div>
    </content>
    <updated>2021-07-06T16:42:32Z</updated>
    <published>2021-07-06T16:42:32Z</published>
    <category term="Uncategorized"/>
    <category term="tech"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2021-07-18T21:38:24Z</updated>
    </source>
  </entry>
</feed>
