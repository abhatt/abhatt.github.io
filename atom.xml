<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2018-12-19T17:22:13Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1812.07532</id>
    <link href="http://arxiv.org/abs/1812.07532" rel="alternate" type="text/html"/>
    <title>On zero-free regions for the anti-ferromagnetic Potts model on bounded-degree graphs</title>
    <feedworld_mtime>1545177600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bencs:Ferenc.html">Ferenc Bencs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davies:Ewan.html">Ewan Davies</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Patel:Viresh.html">Viresh Patel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Regts:Guus.html">Guus Regts</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1812.07532">PDF</a><br/><b>Abstract: </b>For a graph $G=(V,E)$, $k\in \mathbb{N}$, and a complex number $w$ the
partition function of the univariate Potts model is defined as \[ {\bf
Z}(G;k,w):=\sum_{\phi:V\to [k]}\prod_{\substack{uv\in E \\ \phi(u)=\phi(v)}}w.
\] In this paper we give zero-free regions for the partition function of the
anti-ferromagnetic Potts model on bounded degree graphs. In particular we show
that for any $\Delta\in \mathbb{N}$ and any $k\geq 3.02 \Delta+1$, there exists
an open set $U$ in the complex plain that contains the interval $[0,1)$ such
that ${\bf Z}(G;k,w)\neq 0$ for any $w\in U$ and any graph $G$ of maximum
degree at most $\Delta$. For small values of $\Delta$ we are able to give
better results.
</p>
<p>As an application of our results we obtain improved bounds on $k$ for the
existence of deterministic approximation algorithms for counting the number of
proper $k$-colourings of graphs of small maximum degree.
</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-19T02:25:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2018-12-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1812.07484</id>
    <link href="http://arxiv.org/abs/1812.07484" rel="alternate" type="text/html"/>
    <title>Efficient Autotuning of Hyperparameters in Approximate Nearest Neighbor Search</title>
    <feedworld_mtime>1545177600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/J=auml==auml=saari:Elias.html">Elias Jääsaari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hyv=ouml=nen:Ville.html">Ville Hyvönen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roos:Teemu.html">Teemu Roos</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1812.07484">PDF</a><br/><b>Abstract: </b>Approximate nearest neighbor algorithms are used to speed up nearest neighbor
search in a wide array of applications. However, current indexing methods
feature several hyperparameters that need to be tuned to reach an acceptable
accuracy--speed trade-off. A grid search in the parameter space is often
impractically slow due to a time-consuming index-building procedure. Therefore,
we propose an algorithm for automatically tuning the hyperparameters of
indexing methods based on randomized space-partitioning trees. In particular,
we present results using randomized k-d trees, random projection trees and
randomized PCA trees. The tuning algorithm adds minimal overhead to the
index-building process but is able to find the optimal hyperparameters
accurately. We demonstrate that the algorithm is significantly faster than
existing approaches, and that the indexing methods used are competitive with
the state-of-the-art methods in query time while being faster to build.
</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-19T02:26:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2018-12-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1812.07441</id>
    <link href="http://arxiv.org/abs/1812.07441" rel="alternate" type="text/html"/>
    <title>A Scalable Heuristic for Fastest-Path Computation on Very Large Road Maps</title>
    <feedworld_mtime>1545177600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Renjie.html">Renjie Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gotsman:Craig.html">Craig Gotsman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1812.07441">PDF</a><br/><b>Abstract: </b>Fastest-path queries between two points in a very large road map is an
increasingly important primitive in modern transportation and navigation
systems, thus very efficient computation of these paths is critical for system
performance and throughput. We present a method to compute an effective
heuristic for the fastest path travel time between two points on a road map,
which can be used to significantly accelerate the classical A* algorithm when
computing fastest paths. Our method is based on two hierarchical sets of
separators of the map represented by two binary trees. A preprocessing step
computes a short vector of values per road junction based on the separator
trees, which is then stored with the map and used to efficiently compute the
heuristic at the online query stage. We demonstrate experimentally that this
method scales well to any map size, providing a better quality heuristic, thus
more efficient A* search, for fastest path queries between points at all
distances - especially small and medium range - relative to other known
heuristics.
</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-19T02:23:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2018-12-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1812.07431</id>
    <link href="http://arxiv.org/abs/1812.07431" rel="alternate" type="text/html"/>
    <title>Mo-Net: Flavor the Moments in Learning to Classify Shapes</title>
    <feedworld_mtime>1545177600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Mor Joseph-Rivlin, Alon Zvirin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kimmel:Ron.html">Ron Kimmel</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1812.07431">PDF</a><br/><b>Abstract: </b>A fundamental question in learning to classify 3D shapes is how to treat the
data in a way that would allow us to construct efficient and accurate geometric
processing and analysis procedures. Here, we restrict ourselves to networks
that operate on point clouds. There were several attempts to treat point clouds
as non-structured data sets by which a neural network is trained to extract
discriminative properties. The idea of using 3D coordinates as class
identifiers motivated us to extend this line of thought to that of shape
classification by comparing attributes that could easily account for the shape
moments. Here, we propose to add polynomial functions of the coordinates
allowing the network to account for higher order moments of a given shape.
Experiments on two benchmarks show that the suggested network is able to
provide more accurate results and at the same token learn more efficiently in
terms of memory and computational complexity.
</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-19T02:20:12Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2018-12-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1812.07075</id>
    <link href="http://arxiv.org/abs/1812.07075" rel="alternate" type="text/html"/>
    <title>Information theoretical clustering is hard to approximate</title>
    <feedworld_mtime>1545177600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cicalese:Ferdinando.html">Ferdinando Cicalese</a>, Eduardo Laber <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1812.07075">PDF</a><br/><b>Abstract: </b>An impurity measures $I: \mathbb{R}^d \mapsto \mathbb{R}^+$ is a function
that assigns a $d$-dimensional vector ${\bf v}$ to a non-negative value $I({\bf
v})$ so that the more homogeneous ${\bf v}$, with respect to the values of its
coordinates, the larger its impurity. A well known example of impurity measures
is the Entropy impurity.
</p>
<p>We study the problem of clustering based on impurity measures. Let $V$ be a
collection of $n$ many $d$-dimensional vectors with non-negative components.
Given $V$ and an impurity measure $I$, the goal is to find a partition
${\mathcal P}$ of $V$ into $k$ groups $V_1,\ldots,V_k$ so as to minimize the
sum of the impurities of the groups in ${\cal P}$, i.e., $I({\cal P})=
\sum_{i=1}^{k} I\bigg(\sum_{ {\bf v} \in V_i} {\bf v} \bigg).$
</p>
<p>Impurity minimization has been widely used as quality assessment measure in
probability distribution clustering (KL-divergence) as well as in categorical
clustering. However, in contrast to the case of metric based clustering, the
current knowledge of impurity measure based clustering in terms of
approximation and inapproximability results is very limited. Here, we
contribute to change this scenario by proving that for the Entropy impurity
measure the problem does not admit a PTAS even when all vectors have the same
$\ell_1$ norm. This result solves a question that remained open in previous
work on this topic [Chaudhuri and McGregor COLT 08; Ackermann et. al. ECCC 11].
</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-19T02:25:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2018-12-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1812.07030</id>
    <link href="http://arxiv.org/abs/1812.07030" rel="alternate" type="text/html"/>
    <title>A Fast Combination of AES Encryption and LZ4 Compression Algorithms</title>
    <feedworld_mtime>1545177600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Saber Malekzadeh <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1812.07030">PDF</a><br/><b>Abstract: </b>From a long time ago, beside encryption of data and making it secure,
compression packing it was also important that could make transmission of data
faster. In the past years need for improvement of encryption and compression
for a fast and easy transmission is more necessary. In this paper, a new method
for combination of LZ4 combination and AES encryption algorithms for a fast and
easy packing, securing and compressing of data is presented. Choose of these
two algorithms was for some special features of them about aim of this paper.
This paper also is introducing a method for Parallelism of compression and
encryption in a special way for improvement of speed and security of data.
</p></div>
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-19T02:27:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2018-12-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1470</id>
    <link href="https://theorydish.blog/2018/12/18/2019-godel-prize/" rel="alternate" type="text/html"/>
    <title>2019 Gödel Prize</title>
    <summary>If I write a post and the blog aggregator is down, does it still make a sound?   The call for nomination for the 2019 Gödel Prize is out and the deadline is February 15th. For all awards, we sometimes have the tendency to think that worthy candidates have surely been nominated by others. Often it is not the case (and thus worthy candidates are often left behind). So if there is a paper or papers deserving nomination, please nominate! The call for nomination is below.   The Gödel Prize 2019 – Call for Nominations Deadline: February 15, 2019 The Gödel Prize for outstanding papers in the area of theoretical computer science is sponsored jointly by the European Association for Theoretical Computer Science (EATCS) and the Association for Computing Machinery, Special Interest Group on Algorithms and Computation Theory (ACM SIGACT). The award is presented annually, with the presentation taking place alternately at the International Colloquium on Automata, Languages, and Programming (ICALP) and the ACM Symposium on Theory of Computing (STOC). The 27th Gödel Prize will be awarded at 51st Annual ACM Symposium on the Theory of Computing to be held during June 23-26, 2019 in Phoenix, AZ. The Prize is [...]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>If I write a post and the blog aggregator is down, does it still make a sound?</p>
<hr/>
<p> </p>
<p>The call for nomination for the 2019 Gödel Prize is out and the deadline is February 15th. For all awards, we sometimes have the tendency to think that worthy candidates have surely been nominated by others. Often it is not the case (and thus worthy candidates are often left behind). So if there is a paper or papers deserving nomination, please nominate! The call for nomination is below.</p>
<hr/>
<p> </p>
<h1>The Gödel Prize 2019 – Call for Nominations</h1>
<p>Deadline: February 15, 2019</p>
<p>The Gödel Prize for outstanding papers in the area of theoretical computer science is sponsored jointly by the European Association for Theoretical Computer Science (EATCS) and the Association for Computing Machinery, Special Interest Group on Algorithms and Computation Theory (ACM SIGACT). The award is presented annually, with the presentation taking place alternately at the International Colloquium on Automata, Languages, and Programming (ICALP) and the ACM Symposium on Theory of Computing (STOC). The 27th Gödel Prize will be awarded at 51st Annual ACM Symposium on the Theory of Computing to be held during June 23-26, 2019 in Phoenix, AZ. The Prize is named in honor of Kurt Gödel in recognition of his major contributions to mathematical logic and of his interest, discovered in a letter he wrote to John von Neumann shortly before von Neumann’s death, in what has become the famous “P versus NP” question. The Prize includes an award of USD 5,000.</p>
<p><strong>Award Committee: </strong>The 2019 Award Committee consists of Anuj Dawar (Cambridge University), Robert Krauthgamer (Weizmann Institute), Joan Feigenbaum (Yale University), Giuseppe Persiano (Università di Salerno), Omer Reingold (Chair, Stanford University) and Daniel Spielman (Yale University).</p>
<p><strong>Eligibility:</strong> The 2019 Prize rules are given below and they supersede any different interpretation of the generic rule to be found on websites of both SIGACT and EATCS. Any research paper or series of papers by a single author or by a team of authors is deemed eligible if: – The main results were not published (in either preliminary or final form) in a journal or conference proceedings before January 1st, 2006. – The paper was published in a recognized refereed journal no later than December 31, 2018. The research work nominated for the award should be in the area of theoretical computer science. Nominations are encouraged from the broadest spectrum of the theoretical computer science community so as to ensure that potential award winning papers are not overlooked. The Award Committee shall have the ultimate authority to decide whether a particular paper is eligible for the Prize.</p>
<p><strong>Nominations:</strong></p>
<p>Nominations for the award should be submitted by email to the Award Committee Chair: <a href="mailto:reingold@stanford.edu">reingold@stanford.edu</a>. Please make sure that the Subject line of all nominations and related messages begin with “Goedel Prize 2019.” To be considered, nominations for the 2019 Prize must be received by February 15, 2019.</p>
<p>A nomination package should include:</p>
<p>1. A printable copy (or copies) of the journal paper(s) being nominated, together with a complete citation (or citations) thereof.</p>
<p>2. A statement of the date(s) and venue(s) of the first conference or workshop publication(s) of the nominated work(s) or a statement that no such publication has occurred.</p>
<p>3. A brief summary of the technical content of the paper(s) and a brief explanation of its significance.</p>
<p>4. A support letter or letters signed by at least two members of the scientific community.</p>
<p>Additional support letters may also be received and are generally useful. The nominated paper(s) may be in any language. However, if a nominated publication is not in English, the nomination package must include an extended summary written in English.</p>
<p>Those intending to submit a nomination should contact the Award Committee Chair by email well in advance. The Chair will answer questions about eligibility, encourage coordination among different nominators for the same paper(s), and also accept informal proposals of potential nominees or tentative offers to prepare formal nominations. The committee maintains a database of past nominations for eligible papers, but fresh nominations for the same papers (especially if they highlight new evidence of impact) are always welcome.</p>
<p><strong>Selection Process:</strong></p>
<p>The Award Committee is free to use any other sources of information in addition to the ones mentioned above. It may split the award among multiple papers, or declare no winner at all. All matters relating to the selection process left unspecified in this document are left to the discretion of the Award Committee.</p>
<p><strong>Recent Winners</strong></p>
<p>(all winners since 1993 are listed at <a href="http://www.sigact.org/Prizes/Godel/">http://www.sigact.org/Prizes/Godel/</a> and <a href="http://eatcs.org/index.php/goedel-prize">http://eatcs.org/index.php/goedel-prize</a>):</p>
<p><strong>2018:</strong> Oded Regev, On lattices, learning with errors, random linear codes, and cryptography, Journal of the ACM (JACM), Volume 56 Issue 6, 2009 (preliminary version in Symposium on Theory of Computing, STOC 2005).</p>
<p><strong>2017:</strong> Cynthia Dwork, Frank McSherry, Kobbi Nissim and Adam Smith, Calibrating Noise to Sensitivity in Private Data Analysis, Journal of Privacy and Confidentiality, Volume 7, Issue 3, 2016 (preliminary version in Theory of Cryptography, TCC 2006).</p>
<p><strong>2016:</strong> Stephen Brookes, A Semantics for Concurrent Separation Logic. Theoretical Computer Science 375(1-3): 227-270 (2007). Peter W. O’Hearn, Resources, Concurrency, and Local Reasoning. Theoretical Computer Science 375(1-3): 271-307 (2007).</p>
<p><strong>2015:</strong> Dan Spielman and Shang-Hua Teng, Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems, Proc. 36th ACM Symposium on Theory of Computing, pp. 81-90, 2004; Spectral sparsification of graphs, SIAM J. Computing 40:981-1025, 2011; A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning, SIAM J. Computing 42:1-26, 2013; Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems, SIAM J. Matrix Anal. Appl. 35:835-885, 2014.</p>
<p><strong>2014: </strong>Ronald Fagin, Amnon Lotem, and Moni Naor, Optimal Aggregation Algorithms for Middleware, Journal of Computer and System Sciences 66(4): 614–656, 2003.</p>
<p><strong>2013: </strong>Antoine Joux, A one round protocol for tripartite Diffie-Hellman, J. Cryptology 17(4): 263-276, 2004. Dan Boneh and Matthew K. Franklin, Identity-Based Encryption from the Weil pairing, SIAM J. Comput. 32(3): 586-615, 2003.</p></div>
    </content>
    <updated>2018-12-18T20:14:50Z</updated>
    <published>2018-12-18T20:14:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2018-12-19T17:21:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2018/12/18/assistant-professor-mathematics-mscs-department-at-university-of-illinois-at-chicago-uic-apply-by-january-14-2019/</id>
    <link href="https://cstheory-jobs.org/2018/12/18/assistant-professor-mathematics-mscs-department-at-university-of-illinois-at-chicago-uic-apply-by-january-14-2019/" rel="alternate" type="text/html"/>
    <title>Assistant Professor, Mathematics (MSCS) Department at University of Illinois at Chicago (UIC) (apply by January 14, 2019)</title>
    <summary>The department of Mathematics (MSCS) is looking for an Assistant Professor to join the Mathematical Computer Science group. Theoretical computer science is a target area, and all TCS subfields will be considered. (Note that at UIC, cs theory research is split between CS and Math.) Website: https://www.mathjobs.org/jobs/jobs/13215 Email: lreyzin@uic.edu
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The department of Mathematics (MSCS) is looking for an Assistant Professor to join the Mathematical Computer Science group. Theoretical computer science is a target area, and all TCS subfields will be considered. (Note that at UIC, cs theory research is split between CS and Math.)</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/jobs/13215">https://www.mathjobs.org/jobs/jobs/13215</a><br/>
Email: lreyzin@uic.edu</p></div>
    </content>
    <updated>2018-12-18T15:39:22Z</updated>
    <published>2018-12-18T15:39:22Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2018-12-19T17:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2018/12/17/faculty-associate-or-full-professor-at-university-of-california-san-diego-apply-by-january-17-2019/</id>
    <link href="https://cstheory-jobs.org/2018/12/17/faculty-associate-or-full-professor-at-university-of-california-san-diego-apply-by-january-17-2019/" rel="alternate" type="text/html"/>
    <title>faculty (associate or full professor) at University of California, San Diego (apply by January 17, 2019)</title>
    <summary>UC San Diego invites applications from outstanding candidates for multiple tenure-track faculty positions for primary appointment at the Halicioglu Data Science Institute with optional joint appointment in another academic department. In particular, one of the focus areas is theoretical foundations of data science. Website: https://apol-recruit.ucsd.edu/apply/JPF01994 Email: slovett@ucsd.edu
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>UC San Diego invites applications from outstanding candidates for multiple tenure-track faculty positions for primary appointment at the Halicioglu Data Science Institute with optional joint appointment in another academic department. In particular, one of the focus areas is theoretical foundations of data science.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/apply/JPF01994">https://apol-recruit.ucsd.edu/apply/JPF01994</a><br/>
Email: slovett@ucsd.edu</p></div>
    </content>
    <updated>2018-12-17T19:49:46Z</updated>
    <published>2018-12-17T19:49:46Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2018-12-19T17:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2018/12/17/faculty-assistant-professor-at-university-of-california-san-diego-apply-by-january-17-2019/</id>
    <link href="https://cstheory-jobs.org/2018/12/17/faculty-assistant-professor-at-university-of-california-san-diego-apply-by-january-17-2019/" rel="alternate" type="text/html"/>
    <title>faculty (assistant professor) at University of California, San Diego (apply by January 17, 2019)</title>
    <summary>UC San Diego invites applications from outstanding candidates for multiple tenure-track faculty positions for primary appointment at the Halicioglu Data Science Institute with optional joint appointment in another academic department. In particular, one of the focus areas is theoretical foundations of data science. Website: https://apol-recruit.ucsd.edu/apply/JPF01984 Email: slovett@ucsd.edu
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>UC San Diego invites applications from outstanding candidates for multiple tenure-track faculty positions for primary appointment at the Halicioglu Data Science Institute with optional joint appointment in another academic department. In particular, one of the focus areas is theoretical foundations of data science.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/apply/JPF01984">https://apol-recruit.ucsd.edu/apply/JPF01984</a><br/>
Email: slovett@ucsd.edu</p></div>
    </content>
    <updated>2018-12-17T19:49:36Z</updated>
    <published>2018-12-17T19:49:36Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2018-12-19T17:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4021</id>
    <link href="https://www.scottaaronson.com/blog/?p=4021" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4021#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4021" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Why are amplitudes complex?</title>
    <summary xml:lang="en-US">[By prior agreement, this post will be cross-posted on Microsoft’s Q# blog, even though it has nothing to do with the Q# programming language.  It does, however, contain many examples that might be fun to implement in Q#!] Why should Nature have been quantum-mechanical?  It’s totally unclear what would count as an answer to such […]
      <div class="commentbar">
        <p/>
        <span class="commentbutton" href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4021"/>
        <a href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4021">
          <img class="commenticon" src="/images/feed-icon.png"/> Subscribe to comments
        </a>  | 
        <a href="https://www.scottaaronson.com/blog/?p=4021#comments">
          <img class="commenticon" src="/images/post-icon.png"/> Post a comment
        </a>
      </div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>[By prior agreement, this post will be cross-posted on <a href="https://blogs.msdn.microsoft.com/visualstudio/2018/11/15/q-advent-calendar-2018/">Microsoft’s Q# blog</a>, even though it has nothing to do with the Q# programming language.  It does, however, contain many examples that might be fun to implement in Q#!]</p>



<p>Why should Nature have been quantum-mechanical?  It’s totally unclear what would count as an answer to such a question, and also totally clear that people will never stop asking it.</p>



<p>Short of an ultimate answer, we can at least try to explain why, if you want this or that <em>piece</em> of quantum mechanics, then the rest of the structure is inevitable: why quantum mechanics is an <a href="https://www.scottaaronson.com/papers/island.pdf">“island in theoryspace,”</a> as I put it in 2003.</p>



<p>In this post, I’d like to focus on a question that any “explanation” for QM at some point needs to address, in a non-question-begging way: <strong>why should amplitudes have been complex numbers?  </strong>When I was a grad student, it was his relentless focus on that question, and on others in its vicinity, that made me a lifelong fan of Chris Fuchs (see for example his <a href="https://arxiv.org/abs/quant-ph/0105039">samizdat</a>), despite my philosophical differences with him.</p>



<p>It’s not that complex numbers are a <em>bad</em> choice for the foundation of the deepest known description of the physical universe—far from it!  (They’re a field, they’re algebraically closed, they’ve got a norm, how much more could you want?)  It’s just that they seem like a <em>specific</em> choice, and not the only possible one.  There are also the real numbers, for starters, and in the other direction, the <a href="https://en.wikipedia.org/wiki/Quaternion">quaternions</a>.</p>



<p>Quantum mechanics over the reals or the quaternions still has constructive and destructive interference among amplitudes, and unitary transformations, and probabilities that are absolute squares of amplitudes.  Moreover, these variants turn out to lead to precisely the same power for quantum computers—namely, the class <a href="https://en.wikipedia.org/wiki/BQP">BQP</a>—as “standard” quantum mechanics, the one over the complex numbers.  So none of those are relevant differences.</p>



<p>Indeed, having just finished teaching an undergrad <a href="https://www.scottaaronson.com/blog/?p=3943">Intro to Quantum Information</a> course, I can attest that the complex nature of amplitudes is needed only rarely—shockingly rarely, one might say—in quantum computing and information.  Real amplitudes typically suffice.  <a href="https://en.wikipedia.org/wiki/Quantum_teleportation">Teleportation</a>, <a href="https://en.wikipedia.org/wiki/Superdense_coding">superdense coding</a>, the <a href="https://en.wikipedia.org/wiki/Bell%27s_theorem">Bell inequality</a>, <a href="https://en.wikipedia.org/wiki/Quantum_money">quantum money</a>, <a href="https://en.wikipedia.org/wiki/Quantum_key_distribution">quantum key distribution</a>, the <a href="https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm">Deutsch-Jozsa</a> and <a href="https://www.scottaaronson.com/qclec/18.pdf">Bernstein-Vazirani</a> and <a href="https://en.wikipedia.org/wiki/Simon%27s_problem">Simon</a> and <a href="https://en.wikipedia.org/wiki/Grover%27s_algorithm">Grover</a> algorithms, <a href="https://en.wikipedia.org/wiki/Quantum_error_correction">quantum error-correction</a>: all of those and more can be fully explained without using a single <em>i</em> that’s not a summation index.  (<a href="https://en.wikipedia.org/wiki/Shor%27s_algorithm">Shor’s factoring algorithm</a> is an exception; it’s much more natural with complex amplitudes.  But as the previous paragraph implied, their use is removable even there.)</p>



<p>It’s true that, if you look at even the simplest “real” examples of quantum systems—or as a software engineer might put it, at the application layers built on top of the quantum OS—then complex numbers are everywhere, in a way that seems impossible to remove.  The <a href="https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation">Schrödinger equation</a>, <a href="https://en.wikipedia.org/wiki/Stationary_state">energy eigenstates</a>, the <a href="https://en.wikipedia.org/wiki/Canonical_commutation_relation">position/momentum commutation relation</a>, the state space of a <a href="https://en.wikipedia.org/wiki/Spin-%C2%BD">spin-1/2 particle</a> in 3-dimensional space: none of these make much sense without complex numbers (though it can be fun to try).</p>



<p>But from a sufficiently Olympian remove, it feels circular to use any of this as a “reason” for why quantum mechanics should’ve involved complex amplitudes in the first place.  It’s like, once your OS provides a certain core functionality (in this case, complex numbers), it’d be surprising if the application layer <em>didn’t</em> exploit that functionality to the hilt—especially if we’re talking about fundamental physics, where we’d like to imagine that nothing is wasted or superfluous (hence Rabi’s famous question about the muon: “who ordered that?”).</p>



<p>But why should the quantum OS have provided complex-number functionality at all?  Is it possible to answer that question purely in terms of the OS’s internal logic (i.e., abstract quantum information), making minimal reference to how the OS will eventually get used?  Maybe not—but if so, then that itself would seem worthwhile to know.</p>



<p>If we stick to abstract quantum information language, then the most “obvious, elementary” argument for why amplitudes should be complex numbers is one that I spelled out in <em><a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing Since Democritus</a></em>, as well as my <a href="https://www.scottaaronson.com/papers/island.pdf">Is quantum mechanics an island in theoryspace?</a> paper.  Namely, it seems desirable to be able to implement a “fraction” of any unitary operation U: for example, some V such that V<sup>2</sup>=U, or V<sup>3</sup>=U.  With complex numbers, this is trivial: we can simply diagonalize U, or use the Hamiltonian picture (i.e., take e<sup>-iH/2</sup> where U=e<sup>-iH</sup>), both of which ultimately depend on the complex numbers being algebraically closed.  Over the reals, by contrast, a 2×2 orthogonal matrix like $$ U = \left(\begin{array}[c]{cc}1 &amp; 0\\0 &amp; -1\end{array}\right)$$</p>



<p>has no 2×2 orthogonal square root, as follows immediately from its determinant being -1.  If we want a square root of U (or rather, of something that acts like U on a subspace) while sticking to real numbers only, then we need to add another dimension, like so: $$ \left(\begin{array}[c]{ccc}1 &amp; 0 &amp; 0\\0 &amp; -1 &amp; 0\\0 &amp; 0&amp;-1\end{array}\right)=\left(\begin{array}[c]{ccc}1 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 1\\0 &amp; -1 &amp; 0\end{array}\right)  ^{2} $$</p>



<p>This is directly related to the fact that there’s no way for a Flatlander to “reflect herself” (i.e., switch her left and right sides while leaving everything else unchanged) by any continuous motion, unless she can lift off the plane and rotate herself through the third dimension.  Similarly, for <em>us</em> to reflect ourselves would require rotating through a fourth dimension.</p>



<p>One could reasonably ask: is that it?  Aren’t there any “deeper” reasons in quantum information for why amplitudes should be complex numbers?</p>



<p>Indeed, there are certain phenomena in quantum information that, slightly mysteriously, work out more elegantly if amplitudes are complex than if they’re real.  (By “mysteriously,” I mean not that these phenomena can’t be 100% verified by explicit calculations, but simply that I don’t know of any deep principle by which the results of those calculations could’ve been predicted in advance.)</p>



<p>One famous example of such a phenomenon is due to Bill Wootters: if you take a uniformly random pure state in d dimensions, and then you measure it in an orthonormal basis, what will the probability distribution (p<sub>1</sub>,…,p<sub>d</sub>) over the d possible measurement outcomes look like?  The answer, amazingly, is that you’ll get a <em>uniformly random probability distribution</em>: that is, a uniformly random point on the simplex defined by p<sub>i</sub>≥0 and p<sub>1</sub>+…+p<sub>d</sub>=1.  This fact, which I’ve used in several papers, is closely related to <a href="http://mathworld.wolfram.com/ArchimedesHat-BoxTheorem.html">Archimedes’ Hat-Box Theorem</a>, beloved by friend-of-the-blog Greg Kuperberg.  But here’s the kicker: it only works if amplitudes are complex numbers.  If amplitudes are real, then the resulting distribution over distributions will be too bunched up near the corners of the probability simplex; if they’re quaternions, it will be too bunched up near the middle.</p>



<p>There’s an even more famous example of such a Goldilocks coincidence—one that’s been elevated, over the past two decades, to exalted titles like “the Axiom of Local Tomography.”  Namely: suppose we have an unknown finite-dimensional mixed state ρ, shared by two players Alice and Bob.  For example, ρ might be an <a href="https://en.wikipedia.org/wiki/Bell_state">EPR pair</a>, or a correlated classical bit, or simply two qubits both in the state |0⟩.  We imagine that Alice and Bob share many identical copies of ρ, so that they can learn more and more about it by measuring this copy in this basis, that copy in that basis, and so on.</p>



<p>We then ask: can ρ be fully determined from the joint statistics of <em>product measurements</em>—that is, measurements that Alice and Bob can apply separately and locally to their respective subsystems, with no communication between them needed?  A good example here would be the set of measurements that arise in a <a href="https://en.wikipedia.org/wiki/Bell_test_experiments">Bell experiment</a>—measurements that, despite being local, certify that Alice and Bob must share an entangled state.</p>



<p>If we asked the analogous question for classical probability distributions, the answer is clearly “yes.”  That is, once you’ve specified the individual marginals, and you’ve <em>also</em> specified all the possible correlations among the players, you’ve fixed your distribution; there’s nothing further to specify.</p>



<p>For quantum mixed states, the answer again turns out to be yes, <em>but only because amplitudes are complex numbers!</em>  In quantum mechanics over the reals, you could have a 2-qubit state like $$ \rho=\frac{1}{4}\left(\begin{array}[c]{cccc}1 &amp; 0 &amp; 0 &amp; -1\\0 &amp; 1 &amp; 1 &amp; 0\\0 &amp; 1 &amp; 1 &amp; 0\\-1&amp; 0 &amp; 0 &amp; 1\end{array}\right) ,$$</p>



<p>which clearly isn’t the maximally mixed state, yet which is indistinguishable from the maximally mixed state by any local measurement that can be specified using real numbers only.  (Proof: exercise!)</p>



<p>In quantum mechanics over the quaternions, something even “worse” happens: namely, the tensor product of two <a href="https://en.wikipedia.org/wiki/Hermitian_matrix">Hermitian</a> matrices need not be Hermitian.  Alice’s measurement results might be described by the 2×2 quaternionic density matrix $$ \rho_{A}=\frac{1}{2}\left(\begin{array}[c]{cc}1 &amp; -i\\i &amp; 1\end{array}\right), $$</p>



<p>and Bob’s results might be described by the 2×2 quaternionic density matrix $$ \rho_{B}=\frac{1}{2}\left(\begin{array}[c]{cc}1 &amp; -j\\j &amp; 1\end{array}\right), $$</p>



<p>and yet there might not be (and in this case, isn’t) any 4×4 quaternionic density matrix corresponding to ρ<sub>A</sub>⊗ρ<sub>B</sub>, which would explain both results separately.</p>



<p>What’s going on here?  Why do the local measurement statistics <em>underdetermine</em> the global quantum state with real amplitudes, and <em>overdetermine</em> it with quaternionic amplitudes, being in one-to-one correspondence with it only when amplitudes are complex?</p>



<p>We can get some insight by looking at the number of independent real parameters needed to specify a d-dimensional Hermitian matrix.  Over the complex numbers, the number is exactly d<sup>2</sup>: we need 1 parameter for each of the d diagonal entries, and 2 (a real part and an imaginary part) for each of the d(d-1)/2 upper off-diagonal entries (the lower off-diagonal entries being determined by the upper ones).  Over the real numbers, by contrast, “Hermitian matrices” are just real symmetric matrices, so the number of independent real parameters is only d(d+1)/2.  And over the quaternions, the number is d+4[d(d-1)/2] = 2d(d-1).</p>



<p>Now, it turns out that the Goldilocks phenomenon that we saw above—with local measurement statistics determining a unique global quantum state when and only when amplitudes are complex numbers—ultimately boils down to the simple fact that $$ (d_A d_B)^2 = d_A^2 d_B^2, $$</p>



<p>but $$\frac{d_A d_B (d_A d_B + 1)}{2} &gt; \frac{d_A (d_A + 1)}{2} \cdot \frac{d_B (d_B + 1)}{2},$$</p>



<p>and conversely $$ 2 d_A d_B (d_A d_B – 1) &lt; 2 d_A (d_A – 1) \cdot 2 d_B (d_B – 1).$$</p>



<p>In other words, only with complex numbers does the number of real parameters needed to specify a “global” Hermitian operator, exactly match the product of the number of parameters needed to specify an operator on Alice’s subsystem, and the number of parameters needed to specify an operator on Bob’s.  With real numbers it overcounts, and with quaternions it undercounts.</p>



<p>A major research goal in quantum foundations, since at least the early 2000s, has been to “derive” the formalism of QM purely from “intuitive-sounding, information-theoretic” postulates—analogous to how, in 1905, some guy whose name I forget derived the otherwise strange-looking Lorentz transformations purely from the assumption that the laws of physics (including a fixed, finite value for the speed of light) take the same form in every inertial frame.  There have been some nontrivial successes of this program: most notably, the “axiomatic derivations” of QM due to <a href="https://arxiv.org/abs/quant-ph/0101012">Lucien Hardy</a> and (more recently) <a href="https://arxiv.org/abs/1011.6451">Chiribella et al.</a>  Starting from axioms that sound suitably general and nontechnical (if sometimes unmotivated and weird), these derivations perform the impressive magic trick of <em>deriving</em> the full mathematical structure of QM: complex amplitudes, unitary transformations, tensor products, the Born rule, everything.</p>



<p>However, in every such derivation that I know of, some axiom needs to get introduced to capture “local tomography”: i.e., the “principle” that composite systems must be uniquely determined by the statistics of local measurements.  And while this principle might sound vague and unobjectionable, to those in the business, it’s obvious what it’s going to be used for the second it’s introduced.  Namely, it’s going to be used to rule out quantum mechanics over the real numbers, which would otherwise be a model for the axioms, and thus to “explain” why amplitudes have to be complex.</p>



<p>I confess that I was always dissatisfied with this.  For I kept asking myself: would I have ever formulated the “Principle of Local Tomography” in the first place—or if someone else had proposed it, would I have ever accepted it as intuitive or natural—if I didn’t <em>already know</em> that QM over the complex numbers just happens to satisfy it?  And I could never honestly answer “yes.”  It always felt to me like a textbook example of drawing the target around where the arrow landed—i.e., of handpicking your axioms so that they yield a predetermined conclusion, which is then no more “explained” than it was at the beginning.</p>



<p>Two months ago, something changed for me: namely, I smacked into the “Principle of Local Tomography,” and its reliance on complex numbers, in my own research, when I hadn’t in any sense set out to look for it.  This still doesn’t convince me that the principle is any sort of <em>a-priori</em> necessity.  But it at least convinces me that it’s, you know, the sort of thing you can smack into when you’re not looking for it.</p>



<p>The aforementioned smacking occurred while I was writing up a small part of a huge paper with Guy Rothblum, about a new connection between so-called “gentle measurements” of quantum states (that is, measurements that don’t damage the states much), and the subfield of classical CS called <a href="https://en.wikipedia.org/wiki/Differential_Privacy">differential privacy</a>.  That connection is a story in itself; let me know if you’d like me to blog about it separately.  Our paper should be on the arXiv any day now; in the meantime, <a href="https://www.scottaaronson.com/talks/dpgentle-mit.ppt">here are some PowerPoint slides</a>.</p>



<p>Anyway, for the paper with Guy, it was of interest to know the following: suppose we have a two-outcome measurement E (let’s say, on n qubits), and suppose it accepts every product state with the same probability p.  Must E then accept every entangled state with probability p as well?  Or, a closely-related question: suppose we know E’s acceptance probabilities on every product state.  Is that enough to determine its acceptance probabilities on <em>all</em> n-qubit states?</p>



<p>I’m embarrassed to admit that I dithered around with these questions, finding complicated proofs for special cases, before I finally stumbled on the one-paragraph, obvious-in-retrospect “Proof from the Book” that slays them in complete generality.</p>



<p>Here it is: if E accepts every product state with probability p, then clearly it accepts every separable mixed state (i.e., every convex combination of product states) with the same probability p.  Now, a well-known result of <a href="https://arxiv.org/abs/quant-ph/9811018">Braunstein et al.</a>, from 1998, states that (surprisingly enough) the separable mixed states have <em>nonzero density</em> within the set of all mixed states, in any given finite dimension.  Also, the probability that E accepts ρ can be written as f(ρ)=Tr(Eρ), which is linear in the entries of ρ.  OK, but a linear function that’s determined on a subset of nonzero density is determined everywhere.  And in particular, if f is constant on that subset then it’s constant everywhere, QED.</p>



<p>But what does any of this have to do with why amplitudes are complex numbers?  Well, it turns out that the 1998 Braunstein et al. result, which was the linchpin of the above argument, only works in complex QM, not in real QM.  We can see its failure in real QM by simply counting parameters, similarly to what we did before.  An n-qubit density matrix requires 4<sup>n</sup> real parameters to specify (OK, 4<sup>n</sup>-1, if we demand that the trace is 1).  Even if we restrict to n-qubit density matrices with real entries only, we still need 2<sup>n</sup>(2<sup>n</sup>+1)/2 parameters.  By contrast, it’s not hard to show that an n-qubit real <em>separable</em> density matrix can be specified using only 3<sup>n</sup> real parameters—and indeed, that any such density matrix lies in a 3<sup>n</sup>-dimensional subspace of the full 2<sup>n</sup>(2<sup>n</sup>+1)/2-dimensional space of 2<sup>n</sup>×2<sup>n</sup> symmetric matrices.  (This is simply the subspace spanned by all possible tensor products of n <a href="https://en.wikipedia.org/wiki/Pauli_matrices">Pauli</a> I, X, and Z matrices—excluding the Y matrix, which is the one that involves imaginary numbers.)</p>



<p>But it’s not only the Braunstein et al. result that fails in real QM: the fact that I wanted for my paper with Guy fails as well.  As a counterexample, consider the 2-qubit measurement that accepts the state ρ with probability Tr(Eρ), where $$ E=\frac{1}{2}\left(\begin{array}[c]{cccc}1 &amp; 0 &amp; 0 &amp; -1\\0 &amp; 1 &amp; 1 &amp; 0\\0 &amp; 1 &amp; 1 &amp; 0\\-1 &amp; 0 &amp; 0 &amp; 1\end{array}\right).$$</p>



<p>I invite you to check that this measurement, which we specified using a real matrix, accepts every product state (a|0⟩+b|1⟩)(c|0⟩+d|1⟩), where a,b,c,d are real, with the same probability, namely 1/2—just like the “measurement” that simply returns a coin flip without even looking at the state at all.  And yet the measurement can clearly be nontrivial on entangled states: for example, it always rejects $$\frac{\left|00\right\rangle+\left|11\right\rangle}{\sqrt{2}},$$ and it always accepts $$ \frac{\left|00\right\rangle-\left|11\right\rangle}{\sqrt{2}}.$$</p>



<p>Is it a coincidence that we used exactly the same 4×4 matrix (up to scaling) to produce a counterexample to the real-QM version of Local Tomography, and <em>also</em> to the real-QM version of the property I wanted for the paper with Guy?  Is anything <em>ever</em> a coincidence in this sort of discussion?</p>



<p>I claim that, looked at the right way, Local Tomography and the property I wanted are the same property, their truth in complex QM is the same truth, and their falsehood in real QM is the same falsehood.  Why?  Simply because Tr(Eρ), the probability that the measurement E accepts the mixed state ρ, is a function of two Hermitian matrices E and ρ (both of which can be either “product” or “entangled”), and—crucially—is symmetric under the interchange of E and ρ.</p>



<p>Now it’s time for another confession.  We’ve identified an elegant property of quantum mechanics that’s true but only because amplitudes are complex numbers: namely, if you know the probability that your quantum circuit accepts every product state, then you also know the probability that it accepts an arbitrary state.  Yet, despite its elegance, this property turns out to be nearly useless for “real-world applications” in quantum information and computing.  The reason for the uselessness is that, for the property to kick in, you really do need to know the probabilities on product states almost <em>exactly</em>—meaning (say) to 1/exp(n) accuracy for an n-qubit state.</p>



<p>Once again a simple example illustrates the point.  Suppose n is even, and suppose our measurement simply projects the n-qubit state onto a tensor product of n/2 Bell pairs.  Clearly, this measurement accepts every n-qubit product state with exponentially small probability, even as it accepts the entangled state <br/>$$\left(\frac{\left|00\right\rangle+\left|11\right\rangle}{\sqrt{2}}\right)^{\otimes n/2}$$</p>



<p>with probability 1.  But this implies that noticing the nontriviality on entangled states, would require knowing the acceptance probabilities on product states to exponential accuracy.</p>



<p>In a sense, then, I come back full circle to my original puzzlement: why <em>should</em> Local Tomography, or (alternatively) the-determination-of-a-circuit’s-behavior-on-arbitrary-states-from-its-behavior-on-product-states, have been important principles for Nature’s laws to satisfy?  Especially given that, in practice, the exponential accuracy required makes it difficult or impossible to exploit these principles anyway?  How could we have known a-priori that these principles would be important—if indeed they <em>are</em> important, and are not just mathematical <a href="https://en.wikipedia.org/wiki/Spandrel_(biology)">spandrels</a>?</p>



<p>But, while I remain less than 100% satisfied about “why the complex numbers? why not just the reals?,” there’s <em>one</em> conclusion that my recent circling-back to these questions has made me fully confident about.  Namely: quantum mechanics over the quaternions is a <strong>flaming garbage fire</strong>, which would’ve been rejected at an extremely early stage of God and the angels’ deliberations about how to construct our universe.</p>



<p>In the literature, when the question of “why not quaternionic amplitudes?” is discussed at all, you’ll typically read things about how the parameter-counting doesn’t quite work out (just like it doesn’t for real QM), or how the tensor product of quaternionic Hermitian matrices need not be Hermitian.  In <a href="https://arxiv.org/abs/0911.1761">this paper by McKague</a>, you’ll read that the CHSH game is winnable with probability 1 in quaternionic QM, while in <a href="https://arxiv.org/abs/quant-ph/0307017">this paper by Fernandez and Schneeberger</a>, you’ll read that the non-commutativity of the quaternions introduces an order-dependence even for spacelike-separated operations.</p>



<p>But none of that does justice to the enormity of the problem.  To put it bluntly: unless something clever is done to fix it, quaternionic QM allows superluminal signaling.  This is easy to demonstrate: suppose Alice holds a qubit in the state |1⟩, while Bob holds a qubit in the state |+⟩ (yes, this will work even for unentangled states!)  Also, let $$U=\left(\begin{array}[c]{cc}1 &amp; 0\\0 &amp; j\end{array}\right)  ,~~~V=\left(\begin{array}[c]{cc}1 &amp; 0\\0&amp; i\end{array}\right).$$</p>



<p>We can calculate that, if Alice applies U to her qubit and then Bob applies V to his qubit, Bob will be left with the state $$ \frac{j \left|0\right\rangle + <br/>k \left|1\right\rangle}{\sqrt{2}}.$$</p>



<p>By contrast, if Alice decided to apply U only <em>after</em> Bob applied V, Bob would be left with the state <br/>$$ \frac{j \left|0\right\rangle – k \left|1\right\rangle}{\sqrt{2}}.$$</p>



<p>But Bob can distinguish these two states with certainty, for example by applying the unitary $$ \frac{1}{\sqrt{2}}\left(\begin{array}[c]{cc}j &amp; k\\k &amp; j\end{array}\right). $$</p>



<p>Therefore Alice communicated a bit to Bob.</p>



<p>I’m aware that there’s a whole literature on quaternionic QM, including for example a <a href="https://www.amazon.com/Quaternionic-Quantum-Mechanics-International-Monographs/dp/019506643X">book by Adler</a>.  Would anyone who knows that literature be kind enough to enlighten us on how it proposes to escape the signaling problem?  Regardless of the answer, though, it seems worth knowing that the “naïve” version of quaternionic QM—i.e., the version that gets invoked in quantum information discussions like the ones I mentioned above—is just immediately blasted to smithereens by the signaling problem, without the need for any subtle considerations like the ones that differentiate real from complex QM.</p>



<p><font color="red"><strong>Unrelated Update (Dec. 18):</strong></font> Probably many of you have already seen it, and/or already know what it covers, but the <a href="https://www.nytimes.com/2018/12/17/science/donald-knuth-computers-algorithms-programming.html">NYT profile of Donald Knuth</a> (entitled “The Yoda of Silicon Valley”) is enjoyable and nicely written.</p></div>
    </content>
    <updated>2018-12-17T16:01:17Z</updated>
    <published>2018-12-17T16:01:17Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2018-12-18T19:57:20Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6363</id>
    <link href="https://windowsontheory.org/2018/12/16/algorithmic-and-information-theoretic-decoding-thresholds-for-low-density-parity-check-code/" rel="alternate" type="text/html"/>
    <title>Algorithmic and Information Theoretic Decoding Thresholds for Low density Parity-Check Code</title>
    <summary>by Jeremy Dohmann, Vanessa Wong, Venkat Arun Abstract We will discuss error-correcting codes: specifically, low-density parity-check (LDPC) codes. We first describe their construction and information-theoretical decoding thresholds, .  Belief propagation (BP) (see Tom’s notes) can be used to decode these. We analyze BP to find the maximum error-rate upto which BP succeeds. After this point, […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3>by Jeremy Dohmann, Vanessa Wong, Venkat Arun</h3>



<h2>Abstract</h2>



<p>We will discuss error-correcting codes: specifically, low-density parity-check (LDPC) codes. We first describe their construction and information-theoretical decoding thresholds, <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/>. </p>



<p>Belief propagation (BP) (<a href="https://d1b10bmlvqabco.cloudfront.net/attach/jjsqup73w3r2s3/ihaz9phexeb6fb/jmch5yx8edqc/229R_Talk_Notes_2.pdf">see Tom’s notes</a>) can be used to decode these. We analyze BP to find the maximum error-rate upto which BP succeeds.</p>



<p>After this point, BP will reach a suboptimal fixed point with high probability. This is lower than the information-theoretic bound, illustrating a gap between algorithmic and information-theoretic thresholds for decoding.</p>



<p>Then we outline a proof of a theorem suggesting that any efficient algorithm, not just BP, will fail after the algorithmic threshold. This is because there is a phase transition at the algorithmic threshold, after which there exist an exponentially large number of suboptimal `metastable’ states near the optimal solution. Local search algorithms will tend to get stuck at these suboptimal points.</p>



<h3>Introduction to Error Correcting Codes</h3>



<h4>Motivation</h4>



<p>Alice wants to send a message to Bob, but their channel of communication is such that Bob receives a corrupted version of what Alice sent. Most practical communication devices are imperfect and introduce errors in the messages they are transmitting. For instance, if Alice sends 3V, Bob will really receive three volts plus some noise (we have chosen to ignore some inconvenient practical details here). In many cases, this noise is quite small, e.g. it could be less than 0.5V in 99.99% of cases. So, in principle, Alice could have had <em>very </em>reliable delivery by just choosing to always send 0V for a logical 0 and 5V for logical 1, using checksums to detect the occasional error. But this is wasteful. Alice could have squeezed more levels between 0 and 5V to get a higher bitrate. This causes errors, but Alice can introduce redundancy in the bits she is transmitting which can enable Bob to decode the correct message with high probability. Since it is much easier to control redundancy in encoding than in physical quantities, practical communication devices often choose choose to pack enough bits into their physical signal that errors are relatively quite likely, relying instead on redundancy in their encoding to recover from the errors. Redundancy is also used in storage, where we don’t have the option of detecting an error and retransmitting the message.</p>



<p>Some errors in communication are caused by thermal noise. These are unpredictable and unavoidable, but the errors they cause can be easily modeled; they cause bit-flips in random positions in the message. There are other sources of error. The clocks on the two devices may not be correctly synchronized,  causing systematic bit-flips in a somewhat predictable pattern. A sudden surge of current (e.g. because someone turned on the lights, and electricity sparked between the contacts) can corrupt a large contiguous segment of bits. Or the cable could simply be cut in which case no information gets through. These other kinds of errors are often harder to model (and easier to detect/mitigate), so we have to remain content with merely detecting them. Thus for the remainder of this blog post, we shall restrict ourselves to an error model where each bit is corrupted with some fixed (but potentially unknown) probability, independent of the other bits. For simplicity, we shall primarily consider the Binary Erasure Channel (BEC), where a bit either goes through successfully, or the receiver <em>knows </em>that there has been an error (though we will introduce some related channels along the way).</p>



<p>Claude Shannon found that given any channel, there is a bitrate below which it is possible to communicate reliably with vanishing error rate. Reliable communication cannot be achieved above this bitrate. Hence this threshold bitrate is called the channel capacity. He showed that random linear codes are an optimal encoding scheme that achieves channel capacity. We will only briefly discuss random linear codes, but essentially they work by choosing random vectors in the input space and mapping them randomly to vectors in the encoded space. Unfortunately we do not have efficient algorithms for decoding these codes (mostly due to the randomness in their construction), and it is conjectured that one doesn’t exist. Recently Low-Density Parity Check (LDPC) codes have gained in popularity. They are simple to construct, and can be efficiently decoded at error levels quite close to the theoretical limits.</p>



<p>With LDPC codes, there are three limits of interest for any given channel and design bitrate (M/N): 1) the error level upto which an algorithm can efficiently decode them, <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/>, 2) the error level level upto which they can be decoded by a computationally unbounded decoder, <img alt="\epsilon_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_c"/> and, 3) the error level beyond which <em>no </em>encoding scheme can achieve reliable communication, <img alt="\epsilon_s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_s&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_s"/>. Obviously, <img alt="\epsilon_d \le \epsilon_c \le \epsilon_s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%5Cle+%5Cepsilon_c+%5Cle+%5Cepsilon_s&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d \le \epsilon_c \le \epsilon_s"/>, and in general these inequalities can be strict. Our goal here is to study the gap between <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/> and <img alt="\epsilon_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_c"/>. We will sometimes refer to these three quantities as <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/>, <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/>, and <img alt="p_{shannon}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bshannon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{shannon}"/> when discussing channels besides the BEC. This is following the notation of Mezard and Montanari (2009).</p>



<p>More formally, information theory concerns reliable communication via an unreliable channel. To mitigate the errors in message transmission, error correcting codes introduce some type of systematic redundancy in the transmitted message. Encoding maps are applied to the information sequence to get the encoded message that is transmitted through the channel. The decoding map, on the other hand, is applied to the noisy channel bit (see Figure below). Each message encoded is comprised of <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> bits and <img alt="N&gt;M" class="latex" src="https://s0.wp.com/latex.php?latex=N%3EM&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N&gt;M"/> redundant sequences of bits in an error correcting code. <img alt="2^M" class="latex" src="https://s0.wp.com/latex.php?latex=2%5EM&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^M"/> possible codewords form a “codebook” <img alt="|\mathbb{C}|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cmathbb%7BC%7D%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\mathbb{C}|"/> in binary space <img alt="\{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{0,1\}"/>.</p>



<p>Claude Shannon’s code ensembles proved that it is easier to construct stochastic (characterized by good properties and high probability) models vs. deterministic code designs. Stochastic models were able to achieve optimal error correcting code performance in comparison to a more rigidly constructed model, proving that it was possible to communicate with a vanishing error probability as long as the rate of transmission <img alt="R=M/N" class="latex" src="https://s0.wp.com/latex.php?latex=R%3DM%2FN&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R=M/N"/> is smaller than the channel capacity, a measure of the maximum mutual information between channel input and output.</p>



<figure class="wp-block-image is-resized"><img alt="" class="wp-image-6366" height="261" src="https://windowsontheory.files.wordpress.com/2018/12/encodedecode.png?w=456&amp;h=261" width="456"/><strong>Fig 1</strong> Schematic of the communication model of information communication.</figure>



<p>Thus, in order to construct an optimal error correcting code, one must first define the subset of the space of encoding maps, endow the set with probability distributions, and subsequently define the associated decoding map for each of the encoding maps in the codes. We have included a section in the  A that gives a thorough discussion of random code ensembles which are known to achieve optimal decoding, whether via scoring decoding success by bit error rate or decoded word error rate. We will also show a perspective which uses principles from statistical physics to unify the two (often called finite-temperature decoding). From hereon out we will discuss LDPC and explore how the values of <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/>, <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> and <img alt="p_{shannon}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bshannon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{shannon}"/> for various channels reveal deep things about the structure of the decoding solution space.</p>



<h3>Low-density Parity Check Code</h3>



<p>LDPC codes are linear and theoretically excellent error correcting codes that communicate at a rate close to the Shannon capacity. The LDPC codebook is a linear subspace of  <img alt="\{0,1\}^N " class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5EN+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{0,1\}^N "/>. For an MxN sparse matrix  <img alt="\mathbb{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{H}"/>, the codebook is defined as the kernel: </p>



<p><img alt="\mathbb{C} = { \underline{x} \in \{0,1\}^N:\mathbb{H}\underline{x}=\underline{0}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D+%3D+%7B+%5Cunderline%7Bx%7D+%5Cin+%5C%7B0%2C1%5C%7D%5EN%3A%5Cmathbb%7BH%7D%5Cunderline%7Bx%7D%3D%5Cunderline%7B0%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{C} = { \underline{x} \in \{0,1\}^N:\mathbb{H}\underline{x}=\underline{0}}"/></p>



<p><br/>where all the multiplications and sums involved in <br/><img alt="\mathbb{H} \underline{x} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BH%7D+%5Cunderline%7Bx%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{H} \underline{x} "/> are computed modulo 2.<br/></p>



<p>Matrix  <img alt="\mathbb{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{H}"/>is called the <strong>parity check matrix</strong> and the size of the codebook is  <img alt="2^{N-rank(\mathbb{H})}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7BN-rank%28%5Cmathbb%7BH%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{N-rank(\mathbb{H})}"/>. Given this code, encoding is a linear operation when mapping an  <img alt="N x L" class="latex" src="https://s0.wp.com/latex.php?latex=N+x+L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N x L"/> binary generating matrix  <img alt="\mathbb{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BG%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{G}"/> (the codebook is the image of  <img alt="\mathbb{G}:\mathbb{C}={x=\mathbb{G}\underline{z}, \text{where } \underline{z} \in \{0,1\}^L} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BG%7D%3A%5Cmathbb%7BC%7D%3D%7Bx%3D%5Cmathbb%7BG%7D%5Cunderline%7Bz%7D%2C+%5Ctext%7Bwhere+%7D+%5Cunderline%7Bz%7D+%5Cin+%5C%7B0%2C1%5C%7D%5EL%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{G}:\mathbb{C}={x=\mathbb{G}\underline{z}, \text{where } \underline{z} \in \{0,1\}^L} "/>) such that  <img alt="\underline{z}\rightarrow \underline{x} =\mathbb{G}z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bz%7D%5Crightarrow+%5Cunderline%7Bx%7D+%3D%5Cmathbb%7BG%7Dz&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{z}\rightarrow \underline{x} =\mathbb{G}z"/>).</p>



<p>Every coding scheme has three essential properties that determine its utility: the geometry of its codebook and the way it sparsely distributes proper codewords within the encoding space <img alt="\{0,1\}^{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5E%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{0,1\}^{N}"/> (c.f. our mention of sphere packing as a geometric analogy for RLCs), the ease with which one can <em>construct a code</em> which sparsely distributes codes within the encoding space, and the existence of fast algorithms to perform effective decoding.</p>



<p>A coding scheme over a given channel (whether it be <a href="https://en.wikipedia.org/wiki/Binary_symmetric_channel">BSC</a>, <a href="https://en.wikipedia.org/wiki/Binary_erasure_channel">BEC</a>, <a href="https://en.wikipedia.org/wiki/Additive_white_Gaussian_noise">AWGN</a>, etc.) also has three parameters of interest, <img alt="p_{d} " class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d} "/> which is the error rate above which some chosen algorithm cannot perform error-free decoding, <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> above which even exhaustively enumerating over all <img alt="2^{M}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7BM%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{M}"/> codewords in the codebook and calculating the MAP probability does not successfully decode, <img alt="p_{shannon}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bshannon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{shannon}"/> which is the capacity of the channel, an error rate above which no decoding scheme could perform error-free decoding.</p>



<h4>LDPC codebook geometry and <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/></h4>



<p>On the subject of codebook geometry, it is well known that LDPC ensembles, in expectation, produce sparsely distributed codewords. This means that valid codewords (i.e. those that pass all parity checks) are far apart from one another in Hamming space and thus require a relatively large number of bits to be lost in order for one codeword to degenerate into another one. There is an important property called the distance enumerator which determines the expected number of codewords in a tight neighborhood of any given codeword. If for a given distance the expected number is exponentially small, then the coding scheme is robust up to error rates causing that degree of distortion. We discuss a proof of LDPC distance properties in Appendix B and simply state here that LDPCs are good at sparsely distributing valid codewords within the encoding space. The property <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/>, introduced above is intimately related to the geometry of the codebook and the properties of the noisy channel being decoded over. </p>



<p>The information theoretic threshold, <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> is the noise level above which MAP decoding no longer successfully performs decoding. <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> is important because it is the error value above which we could theoretically always perform (slow) decoding below <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> by enumerating all <img alt="2^{M}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7BM%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{M}"/> codewords in the codebook and calculating the one with the highest MAP probability. </p>



<p>Every LDPC ensemble has some different <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> for a given channel. WFor now we will simply remark that LDPC ensembles are effective because they can be chosen such that <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> closely approaches the <img alt="p_{shannon}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bshannon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{shannon}"/> limit for many channels. Even more importantly, we will show that it is <em>likely </em>that there is no fast algorithm for which <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/> = <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> for general LDPCs over a general noisy channel.</p>



<p>We will not derive the <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> for any LDPC ensembles over any channels (I recommend <a href="https://web.stanford.edu/~montanar/RESEARCH/book.html">chapter 15 of this book for details</a>), but we will, in section 3, present results derived by other researchers.</p>



<h4>Ease of construction</h4>



<p>On the subject of ease of construction and ease of decoding, there is a much simpler graphical representation of LDPC codes which can be used to demonstrate LDPC tractability. </p>



<p>LDPCs can be thought of as bipartite regular graphs, where there are N variable nodes which are connected to M parity check nodes according to randomly chosen edges based on the degree distribution of the LDPC. Though the appendix discusses general degree distributions we will discuss here only (d,k) regular bipartite graphs, in which all variables have d edges and all parity check nodes have k edges, and how to generate them under the configuration model. </p>



<p>The <a href="https://en.wikipedia.org/wiki/Configuration_model">configuration model</a> can be used to generate a bipartite graph with (d,k) degree distribution by initially assigning all variable nodes d half-edges, all parity check nodes k half-edges, and then randomly linking up half-edges between the two sets, deleting all nodes which end up being paired an even number of times, and collapsing all odd numbered multi-edges into a single edge. This system doesn’t work perfectly but for large N, the configuration model will generate a graph for which most nodes have the proper degree. Thus it is relatively easy to generate random graphs which represent LDPC codes of any desired uniform degree distribution. An example of this graphical representation is in figure 2</p>



<figure class="wp-block-image"><img alt="" class="wp-image-6382" src="https://windowsontheory.files.wordpress.com/2018/12/graph.png?w=600"/><strong>Fig 2 </strong>Factor graph of a (2,3) regular LDPC code with factor nodes as black squares and variable nodes as white circles, and notation for BP messages.</figure>



<h2>How the graphical model relates to fast decoding</h2>



<p>The graphical model of LDPCs is useful because it is both easy to construct and presents a natural way to perform fast decoding. In fact, the fast graph-based decoding algorithm, <a href="http://{https://en.wikipedia.org/wiki/Belief_propagation">Belief Propagation</a>, we use has a <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/> which likely represents the upper limit on fast decoding for LDPCs.</p>



<p>We have seen <a href="https://windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/">recently </a>that bipartite graphical models <br/>which represent a factorized probability distribution can be used to calculate marginal probabilities of individual variable nodes (what a mouthful!). </p>



<p>Basically, if the structure of the graph reflects some underlying probability distribution (e.g. the probability that noisy bit <img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_i"/> was originally sent as bit <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>) then we can use an iterative algorithm called Belief Propagation (see blog post above) to actually converge to the exact probability distribution for each <img alt="P(y_i~|~x_{i})" class="latex" src="https://s0.wp.com/latex.php?latex=P%28y_i%7E%7C%7Ex_%7Bi%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(y_i~|~x_{i})"/>.</p>



<p>This is important because when we perform decoding, we would like to estimate the marginal probability of each individual variable node (bit in our received vector), and simply set the variable to be the most likely value (this is known as bit-MAP decoding, discussed earlier). As mentioned above, under certain conditions the Belief Propagation algorithm correctly calculates those marginal probabilities for noise rates up to an algorithmic threshold <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/>. </p>



<p>The Belief Propagation algorithm is an iterative message passing algorithm in which messages are passed between variable nodes and parity check/factor nodes such that, if the messages converge to a fixed point, the messages encode the marginal probabilities of each variable node. Thus BP, if it succeeds can perform bit-MAP decoding and thus successfully decode. </p>



<p>We will show in the next section how the configuration model graphs map to a factorized probability distribution and mention the <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/> for BP. In the following section we will show an example of decoding over the binary erasure channel, then finally we will show motivation to suggest that the <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/> for BP over LDPCs represents a hard upper limit above which no fast decoding algorithms exist.</p>



<h3>Decoding Errors via Belief Propagation</h3>



<p>As mentioned above (<a href="https://windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/">again, please see Tom’s excellent blog post for details</a>), the belief propagation algorithm is a useful inference algorithm for stochastic models and sparse graphs derived from computational problems exhibiting thresholding behavior. As discussed, symbol/bit MAP decoding of error correcting codes can be regarded as a statistical inference problem. In this section, we will explore BP decoding to determine the threshold for reliable communication and according optimization for LDPC code ensembles in communication over a binary input output symmetric memoryless channel (BSC or BMS).</p>



<h4>Algorithm Overview</h4>



<p>Recall that the conditional distribution of the channel input <img alt="\underline{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{x}"/> given the output <img alt="\underline{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{y}"/> is given by and that we wish to find the <img alt="\underline{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{x}"/> that maximizes the below probability given <img alt="\underline{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{y}"/> </p>



<p><img alt="p(\underline{x}|\underline{y}) = \frac{1}{Z(y)}\prod_{i=1}^{N}Q(y_i|x_i) \prod_{a=1}^{M} \mathbb{I}(x_{i_{1^{a}}}&#xA0;\otimes ... ~x_{k(a)^a} = 0)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28%5Cunderline%7Bx%7D%7C%5Cunderline%7By%7D%29+%3D+%5Cfrac%7B1%7D%7BZ%28y%29%7D%5Cprod_%7Bi%3D1%7D%5E%7BN%7DQ%28y_i%7Cx_i%29+%5Cprod_%7Ba%3D1%7D%5E%7BM%7D+%5Cmathbb%7BI%7D%28x_%7Bi_%7B1%5E%7Ba%7D%7D%7D%C2%A0%5Cotimes+...+%7Ex_%7Bk%28a%29%5Ea%7D+%3D+0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p(\underline{x}|\underline{y}) = \frac{1}{Z(y)}\prod_{i=1}^{N}Q(y_i|x_i) \prod_{a=1}^{M} \mathbb{I}(x_{i_{1^{a}}}&#xA0;\otimes ... ~x_{k(a)^a} = 0)"/> (1)<br/></p>



<p style="text-align: left;"><br/>Where <img alt="Q(y_i~|~x_{i})" class="latex" src="https://s0.wp.com/latex.php?latex=Q%28y_i%7E%7C%7Ex_%7Bi%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q(y_i~|~x_{i})"/> is the conditional probability of <img alt="y_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=y_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_{i}"/> of observing noisy bit <img alt="y_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=y_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_{i}"/> given that <img alt="x_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_{i}"/> was sent. For the BSC we have <img alt="Q(y_{i} = 1 | x_{i} = 1) = Q(y_{i} = 0 | x_{i} = 0) = 1-p" class="latex" src="https://s0.wp.com/latex.php?latex=Q%28y_%7Bi%7D+%3D+1+%7C+x_%7Bi%7D+%3D+1%29+%3D+Q%28y_%7Bi%7D+%3D+0+%7C+x_%7Bi%7D+%3D+0%29+%3D+1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q(y_{i} = 1 | x_{i} = 1) = Q(y_{i} = 0 | x_{i} = 0) = 1-p"/> and <img alt="Q(y_{i} = 1 | x_{i} = 0) = Q(y_{i} = 0 | x_{i} = 1) = p" class="latex" src="https://s0.wp.com/latex.php?latex=Q%28y_%7Bi%7D+%3D+1+%7C+x_%7Bi%7D+%3D+0%29+%3D+Q%28y_%7Bi%7D+%3D+0+%7C+x_%7Bi%7D+%3D+1%29+%3D+p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q(y_{i} = 1 | x_{i} = 0) = Q(y_{i} = 0 | x_{i} = 1) = p"/>. </p>



<p>Furthermore </p>



<p><img alt="\mathbb{I} (x_{i_{1^{a}}}&#xA0;\otimes&#xA0; ... ~x_{k(a)^a} = 0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BI%7D+%28x_%7Bi_%7B1%5E%7Ba%7D%7D%7D%C2%A0%5Cotimes%C2%A0+...+%7Ex_%7Bk%28a%29%5Ea%7D+%3D+0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{I} (x_{i_{1^{a}}}&#xA0;\otimes&#xA0; ... ~x_{k(a)^a} = 0)"/></p>



<p> is an indicator variable which takes value <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1"/> if <img alt="\underline{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{x}"/> satisfies parity check a and 0 otherwise. In particular, the product of these indicators takes into account the fact that 0 probability should be assigned to hypotheses <img alt="\underline{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{x}"/> which aren’t in the code book (indicated by at least one parity check failing). </p>



<p>We would like to design a message passing scheme such that the incoming messages for a given variable node encode their marginal probabilities <img alt="p(x_i~|~y_i)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x_i%7E%7C%7Ey_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p(x_i~|~y_i)"/>.</p>



<p>Note, first and foremost that this probability can be <em>factorized </em>a la BP factor graphs such that there is a factor node for each parity check node <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a"/> which contributes probability </p>



<p><img alt="\mathbb{I} (x_{i_{1^{a}}}&#xA0;\otimes ... ~x_{k(a)^a} = 0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BI%7D+%28x_%7Bi_%7B1%5E%7Ba%7D%7D%7D%C2%A0%5Cotimes+...+%7Ex_%7Bk%28a%29%5Ea%7D+%3D+0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{I} (x_{i_{1^{a}}}&#xA0;\otimes ... ~x_{k(a)^a} = 0)"/> </p>



<p>and a factor node for each channel probability term <img alt="Q(y_i~|~x_{i})" class="latex" src="https://s0.wp.com/latex.php?latex=Q%28y_i%7E%7C%7Ex_%7Bi%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q(y_i~|~x_{i})"/>. Since each channel probability term is only connected to a single variable, its message never gets updated during BP and so we omit it from the factor graphs (e.g. note that figure 2 only has parity check nodes and variable nodes)</p>



<p>The message passing scheme ends up taking the form</p>



<p><img alt="v_{i\rightarrow a}^{(t+1)}(x_i) \propto Q(y_{i} | x_{i}) \prod_{b \in \partial i \setminus a} \hat{v}{b \rightarrow i}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=v_%7Bi%5Crightarrow+a%7D%5E%7B%28t%2B1%29%7D%28x_i%29+%5Cpropto+Q%28y_%7Bi%7D+%7C+x_%7Bi%7D%29+%5Cprod_%7Bb+%5Cin+%5Cpartial+i+%5Csetminus+a%7D+%5Chat%7Bv%7D%7Bb+%5Crightarrow+i%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v_{i\rightarrow a}^{(t+1)}(x_i) \propto Q(y_{i} | x_{i}) \prod_{b \in \partial i \setminus a} \hat{v}{b \rightarrow i}^{(t)}"/> (2)</p>



<p><img alt="\hat{v}_{a \rightarrow i}(x_{i}) \propto \sum_{{x_{j}}} \mathbb{I} (x_{i} \otimes x_{j_{1}} ... x_{j_{k-1}} = 0) \prod_{j \in \partial a \setminus i} v_{j\rightarrow a}^{(t)}(x_j)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bv%7D_%7Ba+%5Crightarrow+i%7D%28x_%7Bi%7D%29+%5Cpropto+%5Csum_%7B%7Bx_%7Bj%7D%7D%7D+%5Cmathbb%7BI%7D+%28x_%7Bi%7D+%5Cotimes+x_%7Bj_%7B1%7D%7D+...+x_%7Bj_%7Bk-1%7D%7D+%3D+0%29+%5Cprod_%7Bj+%5Cin+%5Cpartial+a+%5Csetminus+i%7D+v_%7Bj%5Crightarrow+a%7D%5E%7B%28t%29%7D%28x_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\hat{v}_{a \rightarrow i}(x_{i}) \propto \sum_{{x_{j}}} \mathbb{I} (x_{i} \otimes x_{j_{1}} ... x_{j_{k-1}} = 0) \prod_{j \in \partial a \setminus i} v_{j\rightarrow a}^{(t)}(x_j)"/> (3)</p>



<p>Where <img alt="\partial a" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpartial+a&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\partial a"/> denotes the neighborhood of factor node <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a"/> and the sum in (3) is over all possible configurations of the neighbors of <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a"/> not including <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>. </p>



<p>Messages are passed along the edges as distributions over binary valued variables described by the log-likelihoods</p>



<p><img alt="h_{i\rightarrow a} = \frac{1}{2}\log \frac{v_{i\rightarrow a(0)}}{v_{i\rightarrow a (1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bi%5Crightarrow+a%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%5Clog+%5Cfrac%7Bv_%7Bi%5Crightarrow+a%280%29%7D%7D%7Bv_%7Bi%5Crightarrow+a+%281%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{i\rightarrow a} = \frac{1}{2}\log \frac{v_{i\rightarrow a(0)}}{v_{i\rightarrow a (1)}}"/> (4)</p>



<p><img alt="u_{i\rightarrow a} = \frac{1}{2}\log \frac{\hat{v}{i\rightarrow a(0)}}{\hat{v}{i\rightarrow a (1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=u_%7Bi%5Crightarrow+a%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%5Clog+%5Cfrac%7B%5Chat%7Bv%7D%7Bi%5Crightarrow+a%280%29%7D%7D%7B%5Chat%7Bv%7D%7Bi%5Crightarrow+a+%281%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u_{i\rightarrow a} = \frac{1}{2}\log \frac{\hat{v}{i\rightarrow a(0)}}{\hat{v}{i\rightarrow a (1)}}"/> (5)</p>



<p><br/>We also introduce the a priori log likelihood for bit <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> given the received message <img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_i"/>:</p>



<p><img alt="B_{i} = \frac{1}{2} log \frac{Q(y_{i} | 0)}{Q(y_{i} | 1)}" class="latex" src="https://s0.wp.com/latex.php?latex=B_%7Bi%7D+%3D+%5Cfrac%7B1%7D%7B2%7D+log+%5Cfrac%7BQ%28y_%7Bi%7D+%7C+0%29%7D%7BQ%28y_%7Bi%7D+%7C+1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B_{i} = \frac{1}{2} log \frac{Q(y_{i} | 0)}{Q(y_{i} | 1)}"/></p>



<p>Once we parametrize the messages as log-likelihoods, it turns out we can rewrite our update rules in terms of the parametrized values h and u, making updates much simpler:</p>



<p><img alt="h_{i \rightarrow a}^{(t+1)} = B_i + \sum_{b \in \partial i \setminus a} u_{b \rightarrow i}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bi+%5Crightarrow+a%7D%5E%7B%28t%2B1%29%7D+%3D+B_i+%2B+%5Csum_%7Bb+%5Cin+%5Cpartial+i+%5Csetminus+a%7D+u_%7Bb+%5Crightarrow+i%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{i \rightarrow a}^{(t+1)} = B_i + \sum_{b \in \partial i \setminus a} u_{b \rightarrow i}^{(t)}"/> (6)</p>



<p><img alt="u_{b\rightarrow i}^{(t)} = atanh{ \prod_{j \in \partial a \setminus i} tanh(h_{j \rightarrow a}^{(t)})}" class="latex" src="https://s0.wp.com/latex.php?latex=u_%7Bb%5Crightarrow+i%7D%5E%7B%28t%29%7D+%3D+atanh%7B+%5Cprod_%7Bj+%5Cin+%5Cpartial+a+%5Csetminus+i%7D+tanh%28h_%7Bj+%5Crightarrow+a%7D%5E%7B%28t%29%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u_{b\rightarrow i}^{(t)} = atanh{ \prod_{j \in \partial a \setminus i} tanh(h_{j \rightarrow a}^{(t)})}"/> (7)<br/></p>



<p>Given a set of messages, we would perform decoding via the overall log likelihood <img alt="h_{i}^{(t+1)} = B_i + \sum_{b \in \partial i} u_{b \rightarrow i}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D%5E%7B%28t%2B1%29%7D+%3D+B_i+%2B+%5Csum_%7Bb+%5Cin+%5Cpartial+i%7D+u_%7Bb+%5Crightarrow+i%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{i}^{(t+1)} = B_i + \sum_{b \in \partial i} u_{b \rightarrow i}^{(t)}"/>. Where <img alt="x_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bi%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_{i}"/> gets decoded to 0 for <img alt="h_{i} &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{i} &gt; 0"/> and 1 for <img alt="h_{i} &lt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D+%3C+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{i} &lt; 0"/>.</p>



<p>Typically BP is run until it converges to a set of messages that decode to a word in the codebook, or until a max number of iterations have occurred. Other stopping criteria exist such as the messages between time step t and t+1 being all within some small <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/> of one another.</p>



<p>It is important to note some properties of BP:</p>



<ol><li><strong>BP always terminates in <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> steps if the factor graph is a tree of depth <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/></strong></li><li><strong>It is not known under what circumstances so called “loopy” BP will converge for non-tree graphs</strong></li></ol>



<p>Because factor graphs of LDPC codes are relatively sparse, they appear “locally tree-like”, a property which is believed to play a crucial role in BP convergence over the factorized probability distribution used in LDPC MAP decoding (eqn 1). As mentioned above BP manages to converge on many sorts of non tree-like graphs given that they have “nice” probability distributions. For example <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;ved=2ahUKEwic8ozUqoTfAhXLxlkKHWKGDfYQFjABegQICBAC&amp;url=https%3A%2F%2Fstatweb.stanford.edu%2F~souravc%2Ftalk_spin.pdf&amp;usg=AOvVaw0LixaqHUkY6G795LkiyEgX">the SK model</a> is known to converge even though the underlying factor graph is a complete graph! </p>



<p>It turns out that BP converges under some noise levels for LDPC decoding, and that the threshold at which it fails to converge, <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/>, represents a phase transition to a generically different regime in the solution space of the codebook. It’s been noted elsewhere that the BP threshold is often the threshold of fast solving for many cool problems; e.g. <a href="https://www.amazon.com/Nature-Computation-Cristopher-Moore/dp/0199233217">k-SAT</a>. This is because it is often thought to generically represent the “best” possible local (ergo “fast”) algorithm for those problems</p>



<p>In appendix C we will show some important properties of BP. The following tables summarize important results for several ensembles and channels. Note how close the information theoretic threshold for LDPCs is to the actual shannon limit <img alt="p_{shannon}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bshannon%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{shannon}"/> for the channels below.</p>



<p style="text-align: center;"><strong>Table 1</strong>: Thresholds for BSC<br/>Various thresholds for BP over LDPC codes in a Binary Symmetric Channel</p>



<table class="wp-block-table has-fixed-layout is-style-stripes"><tbody><tr><td>d</td><td>k</td><td><img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/></td><td><img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/></td><td>Shannon limit</td></tr><tr><td>3</td><td>4</td><td>.1669</td><td>.2101</td><td>.2145</td></tr><tr><td>3</td><td>5</td><td>.1138</td><td>.1384</td><td>.1461</td></tr><tr><td>3</td><td>6</td><td>.084</td><td>.101</td><td>.11</td></tr><tr><td>4</td><td>6</td><td>.1169</td><td>.1726</td><td>.174</td></tr></tbody></table>



<p style="text-align: center;">See Mezard and Montanari, 2009 Chapt 15. for this table </p>



<p style="text-align: center;"><strong>Table 2</strong>: Thresholds for BEC<br/>Various thresholds for BP over LDPC codes in a Binary Erasure Channel</p>



<table class="wp-block-table has-fixed-layout is-style-stripes"><tbody><tr><td>d</td><td>k</td><td><img alt="\epsilon_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_{d}"/></td><td><img alt="\epsilon_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_{c}"/></td><td>Shannon limit</td></tr><tr><td>3</td><td>4</td><td>.65</td><td>.746</td><td>.75</td></tr><tr><td>3</td><td>5</td><td>.52</td><td>.59</td><td>.6</td></tr><tr><td>3</td><td>6</td><td>.429</td><td>.4882</td><td>.5</td></tr><tr><td>4</td><td>6</td><td>.506</td><td>.66566</td><td>.6667</td></tr></tbody></table>



<p style="text-align: center;">

See Mezard and Montanari, 2009 Chapt 15. for this table

</p>



<p>We will now show exact behavior of the (3,6) LDPC ensemble over the binary erasure channel.</p>



<h3>Algorithmic Thresholds for Belief Propagation (BP)</h3>



<h4>Definitions and notation</h4>



<p><strong>Definition 1.</strong> In a <strong>Binary Erasure Channel (BEC)</strong>, when the transmitter sends a bit <img alt="\in {0, 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cin+%7B0%2C+1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\in {0, 1}"/>, the receiver receives the correct bit with probability <img alt="1 - \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1 - \epsilon"/> or an error symbol <img alt="*" class="latex" src="https://s0.wp.com/latex.php?latex=%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="*"/> with probability <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/>.</p>



<p>For BECs, the Shannon capacity—the maximum number of data bits that can be transmitted per encoded bit—is given by <img alt="1 - \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1 - \epsilon"/>.<br/></p>



<p><strong>Definition 2.</strong> An <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/>-bit <strong>Error Correcting Code (ECC)</strong> is defined by a codebook <img alt="\mathcal{C} \subset {0, 1}^N" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D+%5Csubset+%7B0%2C+1%7D%5EN&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{C} \subset {0, 1}^N"/>. The transmitter encodes information as an element of <img alt="\mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{C}"/>. The receiver receives a corrupted version <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> of the transmitted codeword. To decode, it picks an <img alt="x \in \mathcal{C}" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Cmathcal%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \mathcal{C}"/> that is most likely given <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> and the channel characteristics.<br/></p>



<p>For ease of discourse, we have refrained from defining ECC in full generality.</p>



<p><strong>Definition 3.</strong> An <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/>-bit <img alt="(\lambda, \rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C+%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\lambda, \rho)"/> Low Density Parity Check Code (LDPC) is an ECC with <img alt="\mathcal{C} = {x | Hx = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BC%7D+%3D+%7Bx+%7C+Hx+%3D+0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{C} = {x | Hx = 0}"/>. Here <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> is an <img alt="M \times N" class="latex" src="https://s0.wp.com/latex.php?latex=M+%5Ctimes+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M \times N"/> matrix and arithmetic is over <img alt="\mathbb{Z}_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{Z}_2"/>. <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> is a sparse parity-check matrix. <img alt="\lambda(x) = \sum_i\lambda_ix^{i-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%28x%29+%3D+%5Csum_i%5Clambda_ix%5E%7Bi-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda(x) = \sum_i\lambda_ix^{i-1}"/> and <img alt="\rho(x) = \sum_i\rho_ix^{i-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%28x%29+%3D+%5Csum_i%5Crho_ix%5E%7Bi-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho(x) = \sum_i\rho_ix^{i-1}"/> are finite polynomials that characterize <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/>; <img alt="\lambda_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda_i"/> is the fraction of columns with <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1"/>s and <img alt="\rho_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho_i"/> is the fraction of rows with <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1"/>s. Since these are fractions/probabilities, they must be normalized. Hence <img alt="\lambda(1) = \rho(1) = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%281%29+%3D+%5Crho%281%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda(1) = \rho(1) = 1"/>. <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> is a random matrix, and therefore has full rank with high probability.<br/></p>



<p>In an LDPC code, <img alt="|\mathcal{C}| = 2^{N - M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cmathcal%7BC%7D%7C+%3D+2%5E%7BN+-+M%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\mathcal{C}| = 2^{N - M}"/>. Hence every <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/> bits contain <img alt="N-M" class="latex" src="https://s0.wp.com/latex.php?latex=N-M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N-M"/> bits of information, making the rate <img alt="1 - M / N" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+M+%2F+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1 - M / N"/>. Over binary erasure channels (BECs), on receiving <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/>, the decoder must choose an <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> such that <img alt="x_i = y_i \forall i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i+%3D+y_i+%5Cforall+i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i = y_i \forall i"/> such that <img alt="y_i \neq *" class="latex" src="https://s0.wp.com/latex.php?latex=y_i+%5Cneq+%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_i \neq *"/>, and <img alt="Hx = 0" class="latex" src="https://s0.wp.com/latex.php?latex=Hx+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Hx = 0"/> (<img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, <img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_i"/> denote the <img alt="i^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=i%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i^{th}"/> bit of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> respectively). That is, the bits that were successfully transmitted should be preserved; other bits should be chosen to satisfy the parity check equations. If multiple correct choices of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> are possible, then <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> cannot be unambiguously decoded.</p>



<h4>BP/Peeling algorithm</h4>



<p>In general, decoding can be computationally hard. But there exists an error rate <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/>, a function of <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> and <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/>, below which belief propagation succeeds in decoding. Let <img alt="\epsilon_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_c"/> be the maximum error rate upto which successful decoding is possible (i.e. we can unambiguously determine the transmitted codeword) and <img alt="\epsilon_s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_s&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_s"/> be the Shannon limit, then <img alt="\epsilon_d \leq \epsilon_c \leq \epsilon_s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%5Cleq+%5Cepsilon_c+%5Cleq+%5Cepsilon_s&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d \leq \epsilon_c \leq \epsilon_s"/>. In general, these inequalities can be strict, illustrating the gap between what is information theoretically possible, and what is computationally feasible.</p>



<p>Belief propagation (BP) for decoding LDPC-codes is equivalent to a simple peeling algorithm. Let us first describe the factor-graph representation for decoding. This is denoted in figure 3. Variables on the left are the received symbol <img alt="\in {0, 1, *}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cin+%7B0%2C+1%2C+%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\in {0, 1, *}"/>. Factor nodes on the right denote the parity-check constraint (rows of <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/>). The XOR of variables connected to each factor node must be 0.</p>



<p>BP/the peeling algorithm works as follows. For simplicity of exposition, consider that the all zeros code-word has been transmitted. Since this is a linear code, there is no loss of generality.  At first, only the <img alt="1 - \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1 - \epsilon"/> variables that were successfully transmitted are fully determined. In the next round, the factor nodes that have exactly one undetermined variable can determine that variable using their parity-check constraint.</p>



<figure class="wp-block-image"><img alt="" class="wp-image-6433" src="https://windowsontheory.files.wordpress.com/2018/12/bp-fails.png?w=600"/><strong>Fig 3</strong> An example of a received code-word and corresponding parity-check constraints that is information-theoretically determined (only the all zeros codeword satisfies all constraints), but cannot be decoded by the belief propagation algorithm because no factor node has exactly one unknown variable. Here, only the V0 is correctly received. Constraints F1, F2, F3 and F4 imply that V1=V2=V3=V4=V5. If V0=0, then all the rest must be 0s to satisfy F0 (note, the only other valid codeword is all ones).</figure>



<h4>BP isn’t perfect</h4>



<p>This algorithm is not perfect. Figure 3 is an example of a received codeword which <em>can </em>be unambiguously decoded — only the all zeros codeword satisfies all the constraints— but the BP algorithm fails, because at any point, all factor nodes have more than one unknown variable. It seems that the only way to solve problems like that is to exhaustively understand the implications of the parity-check equations. If this examples seems contrived, that is because it is. Decoding becomes harder as the degree and number of constraints increases; we had to add a lot of constraints to make this example work. Fortunately, if the graph is sparse, BP succeeds. We prove this in the following theorem:</p>



<h4>Phase transitions for BP</h4>



<p><strong>Theorem 1.</strong> A <img alt="(\lambda, \rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C+%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\lambda, \rho)"/> LDPC code can be decoded by BP as <img alt="N \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N \rightarrow \infty"/> when the error rate is less than <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/>:</p>



<p><img alt="\epsilon_d = \mathrm{inf}_{z \in (0, 1)}\left[\frac{z}{\lambda(1 - \rho(1 - z))}\right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%3D+%5Cmathrm%7Binf%7D_%7Bz+%5Cin+%280%2C+1%29%7D%5Cleft%5B%5Cfrac%7Bz%7D%7B%5Clambda%281+-+%5Crho%281+-+z%29%29%7D%5Cright%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d = \mathrm{inf}_{z \in (0, 1)}\left[\frac{z}{\lambda(1 - \rho(1 - z))}\right]"/></p>



<p><em>Proof.</em> To prove this, let us analyze the density evolution. For BECs, this is particularly simple as we only need to keep track of the fraction of undetermined variables and factor nodes at timestep <img alt="t:~latex z_t" class="latex" src="https://s0.wp.com/latex.php?latex=t%3A%7Elatex+z_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t:~latex z_t"/> and <img alt="\hat{z}_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bz%7D_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\hat{z}_t"/> respectively. As <img alt="N \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N \rightarrow \infty"/>, these fractions are probabilities. A factor node is determined when all of its variables are determined (note: if all but one is determined, the last one can be immediately determined). The following recursion relations hold:</p>



<p><img alt="z_{t+1} = \epsilon\lambda(\hat{z}_t) \:\:\:\mathrm{and}\:\:\: \hat{z}_t = 1 - \rho(1 - z_t)" class="latex" src="https://s0.wp.com/latex.php?latex=z_%7Bt%2B1%7D+%3D+%5Cepsilon%5Clambda%28%5Chat%7Bz%7D_t%29+%5C%3A%5C%3A%5C%3A%5Cmathrm%7Band%7D%5C%3A%5C%3A%5C%3A+%5Chat%7Bz%7D_t+%3D+1+-+%5Crho%281+-+z_t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z_{t+1} = \epsilon\lambda(\hat{z}_t) \:\:\:\mathrm{and}\:\:\: \hat{z}_t = 1 - \rho(1 - z_t)"/> (8)</p>



<p>The first holds because a variable node is undetermined at timestep <img alt="t+1" class="latex" src="https://s0.wp.com/latex.php?latex=t%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t+1"/> if it was originally undetermined (which happens with probability <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/>) <em>and </em>if it isn’t determined in the last step, which happens with probability say <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>. Now,</p>



<p><img alt="p = \mathbf{P}(degree=2)\hat{z}_t + \mathbf{P}(degree=3)\hat{z}_t^2 + \cdots = \lambda(\hat{z}_t)" class="latex" src="https://s0.wp.com/latex.php?latex=p+%3D+%5Cmathbf%7BP%7D%28degree%3D2%29%5Chat%7Bz%7D_t+%2B+%5Cmathbf%7BP%7D%28degree%3D3%29%5Chat%7Bz%7D_t%5E2+%2B+%5Ccdots+%3D+%5Clambda%28%5Chat%7Bz%7D_t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p = \mathbf{P}(degree=2)\hat{z}_t + \mathbf{P}(degree=3)\hat{z}_t^2 + \cdots = \lambda(\hat{z}_t)"/></p>



<p>A similar reasoning holds for the second relation. <img alt="1 - z_t" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+z_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1 - z_t"/> is the probability that a given neighboring variable node is determined. <img alt="\rho(1 - z_t)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%281+-+z_t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho(1 - z_t)"/> is the probability that at-most one is undetermined, and hence this function node is determined. <img alt="1 - \rho(1 - z_t)" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+%5Crho%281+-+z_t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1 - \rho(1 - z_t)"/> is the probability that this function node is undetermined.</p>



<p>Composing the two relations in equation 8, we get the recursion:</p>



<p><img alt="z_{t+1} = F_{\epsilon}(z) = \epsilon \lambda(1 - \rho(1 - z_t))" class="latex" src="https://s0.wp.com/latex.php?latex=z_%7Bt%2B1%7D+%3D+F_%7B%5Cepsilon%7D%28z%29+%3D+%5Cepsilon+%5Clambda%281+-+%5Crho%281+-+z_t%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z_{t+1} = F_{\epsilon}(z) = \epsilon \lambda(1 - \rho(1 - z_t))"/></p>



<p>An example of <img alt="F(z)" class="latex" src="https://s0.wp.com/latex.php?latex=F%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(z)"/> is shown in Figure 4 for <img alt="\lambda(x) = x^2, \rho(x) = x^5" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%28x%29+%3D+x%5E2%2C+%5Crho%28x%29+%3D+x%5E5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda(x) = x^2, \rho(x) = x^5"/>. That is a (3,6) regular graph where variable nodes and function nodes have 3 and 6 neighbors respectively. On the left, <img alt="F(z)" class="latex" src="https://s0.wp.com/latex.php?latex=F%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(z)"/> is always below <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/>. Hence the recursion with <img alt="z_0" class="latex" src="https://s0.wp.com/latex.php?latex=z_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z_0"/> starting from the far right will converge to the fixed point <img alt="F(z) = z = 0" class="latex" src="https://s0.wp.com/latex.php?latex=F%28z%29+%3D+z+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(z) = z = 0"/>. But on the right, <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/> is large enough that <img alt="F(z)" class="latex" src="https://s0.wp.com/latex.php?latex=F%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(z)"/> intersects <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/> at a non-zero point. Hence the recursion will converge to the higher fixed point instead, without ever reaching the `correct’ fixed point. BP therefore gets stuck at a suboptimal solution, though information-theoretically a correct solution exists. This can be interpreted as a glassy state, where many deep local minima are present, and BP will converge to the wrong minimum.</p>



<p>The condition for BP to converge is <img alt="F_\epsilon(z) \le z \:\: \forall z \in (0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=F_%5Cepsilon%28z%29+%5Cle+z+%5C%3A%5C%3A+%5Cforall+z+%5Cin+%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F_\epsilon(z) \le z \:\: \forall z \in (0, 1)"/>. Hence the threshold error rate, <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/>, below which this condition holds is:</p>



<p><img alt="\epsilon_d = \mathrm{inf}_{z \in (0, 1)}\left[\frac{z}{\lambda(1 - \rho(1 - z))}\right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%3D+%5Cmathrm%7Binf%7D_%7Bz+%5Cin+%280%2C+1%29%7D%5Cleft%5B%5Cfrac%7Bz%7D%7B%5Clambda%281+-+%5Crho%281+-+z%29%29%7D%5Cright%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d = \mathrm{inf}_{z \in (0, 1)}\left[\frac{z}{\lambda(1 - \rho(1 - z))}\right]"/></p>



<p>For (3, 6) regular graphs, <img alt="\epsilon_d \approx 0.429" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%5Capprox+0.429&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d \approx 0.429"/> ∎</p>



<figure class="wp-block-image is-resized"><img alt="" class="wp-image-6442" height="293" src="https://windowsontheory.files.wordpress.com/2018/12/ldpc-3-6-above.png?w=460&amp;h=293" width="460"/></figure>



<figure class="wp-block-image is-resized"><img alt="" class="wp-image-6443" height="295" src="https://windowsontheory.files.wordpress.com/2018/12/ldpc-3-6-below.png?w=463&amp;h=295" width="463"/><br/><strong>Fig 4</strong> The recursion relation for a (3,6) regular graph, where <img alt="F_\epsilon(z)" class="latex" src="https://s0.wp.com/latex.php?latex=F_%5Cepsilon%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F_\epsilon(z)"/> is plotted against <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/>. The identity function <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z"/> is also shown (in blue). Here <img alt="\epsilon_d \approx0.429" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%5Capprox0.429&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d \approx0.429"/>. The graph above and below show the case the error rate is below and above <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/> respectively.</figure>



<p>Another interesting phase transition can be observed. As <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/> increases, for some values of <img alt="(\lambda, \rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C+%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\lambda, \rho)"/>, the first intersection of <img alt="F(z)" class="latex" src="https://s0.wp.com/latex.php?latex=F%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(z)"/> and <img alt="F(z)" class="latex" src="https://s0.wp.com/latex.php?latex=F%28z%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F(z)"/> happens at a non-zero point. For others, it starts of at <img alt="z=0" class="latex" src="https://s0.wp.com/latex.php?latex=z%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z=0"/> and goes up continuously. In the former case, the decoding error rate jumps discontinuously as <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon"/> increases from 0 to a non-zero values. For the latter, it increases continuously.</p>



<p>To see the gap between <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/> and what can be information theoretically, we look at what happens when the degrees of the LDPC code is increased while keeping the rate constant. Specifically consider the <img alt="(l, k)" class="latex" src="https://s0.wp.com/latex.php?latex=%28l%2C+k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(l, k)"/> regular graph (i.e. <img alt="\lambda(x) = x^{l-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda%28x%29+%3D+x%5E%7Bl-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda(x) = x^{l-1}"/> and <img alt="\rho(x) = x^{k-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%28x%29+%3D+x%5E%7Bk-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho(x) = x^{k-1}"/>) as <img alt="k \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k \rightarrow \infty"/> while <img alt="l / k = 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=l+%2F+k+%3D+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="l / k = 0.5"/> is fixed. Note that the rate of the code is <img alt="1 - l / k" class="latex" src="https://s0.wp.com/latex.php?latex=1+-+l+%2F+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1 - l / k"/>. This is shown in Figure 5. <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/> decreases toward 0. But as <img alt="k \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k \rightarrow \infty"/>, it should become information-theoretically easier to decode. In fact, as <img alt="k \rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k \rightarrow \infty"/>, the code approaches a random linear code, which is known to achieve Shannon capacity. Hence we can believe that the information-theoretically achievable decoding rate is non-decreasing. Thus there is a gap between what is information theoretically possible to decode, and what is computationally feasible using Belief Propagation.</p>



<figure class="wp-block-image"><img alt="" class="wp-image-6447" src="https://windowsontheory.files.wordpress.com/2018/12/ldpc-ed-lim-inf.png?w=600"/><strong>Fig 5</strong> <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/> decreases as <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> increases, while the rate <img alt="= 1 - l/k = 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=%3D+1+-+l%2Fk+%3D+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="= 1 - l/k = 0.5"/> is fixed. In fact <img alt="\mathrm{lim}_{k \rightarrow \infty}\epsilon_d = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Blim%7D_%7Bk+%5Crightarrow+%5Cinfty%7D%5Cepsilon_d+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathrm{lim}_{k \rightarrow \infty}\epsilon_d = 0"/>.</figure>



<p>Finally we would like to mention that it is possible to choose a sequence of polynomials <img alt="(\lambda, \rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clambda%2C+%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\lambda, \rho)"/> such that <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/> approaches the Shannon limit. While it is non-trivial to sample exactly from this distribution, good approximations exist and LDPC codes can achieve close to channel capacity over binary erasure channels.</p>



<h3>The solution space in <img alt="p_{d}\leq p \leq p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D%5Cleq+p+%5Cleq+p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}\leq p \leq p_{c}"/></h3>



<h4>The energy landscape of LDPC decoding</h4>



<p>We have already shown the exact location of the <img alt="p_{MAP} = p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7BMAP%7D+%3D+p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{MAP} = p_{c}"/> threshold above which decoding is not possible for the LDPC ensemble and have also investigated the point at which the BP algorithm fails, <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/>. </p>



<p>It should not be surprising to us that any given algorithm we attempt to throw at the problem fails at a certain point below <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/>. In fact there are many simple, random algorithms from the class of Markov-chain Monte Carlos which give fast run times but which fail at values far below even <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/>. The failing point of a particular algorithm, per se, is not necessarily very significant. We shouldn’t expect that any given algorithm, besides explicitly calculating the symbol MAP by traversing the entire codebook, would be able to achieve <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/>.</p>



<p>What is of interest to us here, is that <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/> marks a provable threshold in the solution space of LDPC decoding during which it is likely no locally-based methods, and therefore no <em>fast </em>algorithms can decode with nonzero probability. We will show later precisely the number and energy levels of these metastable states for the BEC. Proof of this transition for other channel types is outside the scope of this lecture.</p>



<p>In this section we will rephrase decoding as an energy minimization problem and use three techniques to explore the existence of metastable states and their effect on local search algorithms.</p>



<p>In particular we will first use a generic local search algorithm that attempts to approximately solve energy minimization expression of decoding.</p>



<p>We will next use a more sophisticated Markov chain Monte Carlo method called simulated annealing. Simulated annealing is useful because it offers a perspective that more closely models real physical processes and that has the property that its convergence behavior closely mimics the structure of the metastable configurations.</p>



<h4>Energy minimization problem</h4>



<p>To begin, we will reframe our problem in terms of constraint satisfaction.</p>



<p>The codewords of an LDPC code are solutions of a CSP. The variables are the bits of the word and the constraints are the parity check equations. Though this means our constraints are a system of linear equations, our problem here is made more complicated by the fact that we are searching for not just ANY solution to the system but for a particular solution, namely the transmitted codeword.</p>



<p>The received message <img alt="\underbar{\textit{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7By%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underbar{\textit{y}}"/> tells us where we should look for the solution. </p>



<p>Assume we are using the binary-input, memoryless, output-symmetric channel with transition probability <img alt="\mathbf{Q}(y | x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BQ%7D%28y+%7C+x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbf{Q}(y | x)"/>.</p>



<p>The probability that <img alt="\underbar{\textit{x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7Bx%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underbar{\textit{x}}"/> was the transmitted codeword, given <img alt="\underbar{\textit{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7By%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underbar{\textit{y}}"/> is <img alt="\mathbb{P}(\underbar{\textit{x}} | \underbar{\textit{y}}) = \mu_{y}(\underbar{\textit{x}})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D+%7C+%5Cunderbar%7B%5Ctextit%7By%7D%7D%29+%3D+%5Cmu_%7By%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{P}(\underbar{\textit{x}} | \underbar{\textit{y}}) = \mu_{y}(\underbar{\textit{x}})"/></p>



<p>Where</p>



<p><img alt="\mu_{y}(\underline{x}|\underline{y}) = \frac{1}{Z(y)}\prod_{i=1}^NQ(y_i|x_i)\prod_{a=1}^M\mathbb{I}(x_{i_1^a}\otimes ... \otimes x_{k(a)^a} = 0)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_%7By%7D%28%5Cunderline%7Bx%7D%7C%5Cunderline%7By%7D%29+%3D+%5Cfrac%7B1%7D%7BZ%28y%29%7D%5Cprod_%7Bi%3D1%7D%5ENQ%28y_i%7Cx_i%29%5Cprod_%7Ba%3D1%7D%5EM%5Cmathbb%7BI%7D%28x_%7Bi_1%5Ea%7D%5Cotimes+...+%5Cotimes+x_%7Bk%28a%29%5Ea%7D+%3D+0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_{y}(\underline{x}|\underline{y}) = \frac{1}{Z(y)}\prod_{i=1}^NQ(y_i|x_i)\prod_{a=1}^M\mathbb{I}(x_{i_1^a}\otimes ... \otimes x_{k(a)^a} = 0)"/> (10)</p>



<p>We can associate an optimization problem with this code. In particular, define <img alt="E(\underbar{\textit{x}})" class="latex" src="https://s0.wp.com/latex.php?latex=E%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(\underbar{\textit{x}})"/> to be twice the number of parity check equations violated by <img alt="\underbar{\textit{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7By%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underbar{\textit{y}}"/>.</p>



<p>We have already discussed how symbol MAP computes the marginals of the distribution <img alt="\mu_{y}(\underbar{\textit{x}})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_%7By%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_{y}(\underbar{\textit{x}})"/> and how word MAP finds its argmax.</p>



<p>We shall here discuss two related problems</p>



<ul><li>optimizing the energy function within a subset of the configuration space defined by the received word</li><li>sampling from a ’tilted’ Boltzmann distribution associated with the energy</li></ul>



<p>Define the log-likelihood of x being the input given the received y to be</p>



<p><img alt="L_{\underline{y}}(\underline{x}) = \sum_{i=1}^N Q(y_i|x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderline%7By%7D%7D%28%5Cunderline%7Bx%7D%29+%3D+%5Csum_%7Bi%3D1%7D%5EN+Q%28y_i%7Cx_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_{\underline{y}}(\underline{x}) = \sum_{i=1}^N Q(y_i|x_i)"/> (11)</p>



<p>If we assume WLOG that the all zero codeword was transmitted, by the law of large numbers, for large N the log-likelihood <img alt="L_{\underbar{\textit{y}}}(\underbar{\textit{x}})" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderbar%7B%5Ctextit%7By%7D%7D%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_{\underbar{\textit{y}}}(\underbar{\textit{x}})"/> of this codeword is close to <img alt="-Nh" class="latex" src="https://s0.wp.com/latex.php?latex=-Nh&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-Nh"/> where <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h"/> is the channel entropy. The probability of an order-N deviation away from this value is exponentially small.</p>



<p>This suggests that we should look for the transmitted codeword amongst those <img alt="\underbar{\textit{x}} \in \mathbb{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7Bx%7D%7D+%5Cin+%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underbar{\textit{x}} \in \mathbb{C}"/> such that <img alt="L_{\underbar{\textit{y}}}(\underbar{\textit{x}})" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderbar%7B%5Ctextit%7By%7D%7D%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_{\underbar{\textit{y}}}(\underbar{\textit{x}})"/> is close to h.</p>



<p>The constraint version of our decoding strategy – known as typical-pairs decoding – is thus, find <img alt="\underbar{\textit{x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7Bx%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underbar{\textit{x}}"/> such that <img alt="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderbar%7B%5Ctextit%7By%7D%7D%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3E+-N%28h%2B%5Cdelta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)"/>. This constraint will be referred to as the `distance constraint’ and we should consider the situation where if exactly one codeword satisfies the distance constraint, return it.</p>



<p>Since codewords are global energy minima (<img alt="E(\underbar{\textit{x}}) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=E%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(\underbar{\textit{x}}) = 0"/> for all <img alt="\underbar{\textit{x}} \in \mathbb{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderbar%7B%5Ctextit%7Bx%7D%7D+%5Cin+%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underbar{\textit{x}} \in \mathbb{C}"/>), we can phrase typical-pairs decoding as an optimization problem</p>



<p>Minimize <img alt="E(\underbar{\textit{x}})" class="latex" src="https://s0.wp.com/latex.php?latex=E%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(\underbar{\textit{x}})"/> subject to <img alt="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderbar%7B%5Ctextit%7By%7D%7D%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3E+-N%28h%2B%5Cdelta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)"/>.</p>



<p>This decoding succeeds iff the minimum is non-degenerate. This happens with high probability for <img alt="p &lt; p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p+%3C+p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p &lt; p_{c}"/> and with zero probability for <img alt="p &gt; p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p+%3E+p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p &gt; p_{c}"/>. In particular, there are exponentially many degenerate energy minima above <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/>.</p>



<p>Similar to what we have seen elsewhere in the course, there exists a generically intermediate regime <img alt="p_{d}\leq p \leq p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D%5Cleq+p+%5Cleq+p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}\leq p \leq p_{c}"/> in which the global energy minimum is still the correct codeword bu there is an exponentially large number of local energy minima obscuring it (see figure 6).</p>



<p>What is so special about BP is that the threshold at which these exponentially many metastable states proliferate is exactly the algorithmic threshold <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/> for BP which we proved earlier.</p>



<figure class="wp-block-image"><img alt="" class="wp-image-6456" src="https://windowsontheory.files.wordpress.com/2018/12/cartoon.png?w=600"/><strong>Fig 6</strong> A cartoon landscape for the Energy function defined above (number of violated checks). <em>Left</em>: The energy has a unique global minimum with <img alt="E(\underline{x}) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=E%28%5Cunderline%7Bx%7D%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(\underline{x}) = 0"/> (the transmitted codeword) and no local minima. <em>Center</em>: many deep local minima appear, although the global minimum is non degenerate. <em>Right</em>: more than one codeword is compatible with the likelihood constraint, the global minimum <img alt="E(\underline{x}) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=E%28%5Cunderline%7Bx%7D%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(\underline{x}) = 0"/> is degenerate <em>adapted from Mezard and Montanari, 2009 Chapt 21</em></figure>



<p>While finding solutions <img alt="E(\underbar{\textit{x}}) = 0" class="latex" src="https://s0.wp.com/latex.php?latex=E%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E(\underbar{\textit{x}}) = 0"/> amounts to Gaussian elimination, the constraint <img alt="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7B%5Cunderbar%7B%5Ctextit%7By%7D%7D%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3E+-N%28h%2B%5Cdelta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="L_{\underbar{\textit{y}}}(\underbar{\textit{x}}) &gt; -N(h+\delta)"/> is not a linear constraint. Thus one needs to use some sort of more advanced search procedure to find satisfying vectors <img alt="\underline{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{x}"/>.</p>



<p>We will show that if one resorts to local-search-based algorithms, the metastable states above <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/> block the algorithm. Furthermore, we suggest that the behavior of the local algorithms discussed below are typical of all local search algorithms (including BP) and that it is very likely the case that no fast algorithm exists capable of finding global energy minima without getting caught in the metastable states which proliferate above <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/>.</p>



<p>Below is the simplest of local search algorithms, <img alt="\Delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta"/>-local search.</p>



<figure class="wp-block-image is-resized"><img alt="" class="wp-image-6457" height="294" src="https://windowsontheory.files.wordpress.com/2018/12/deltasearch.png?w=548&amp;h=294" width="548"/><strong>Fig 7</strong><em> Excerpted from Mezard and Montanari, 2009 Chapt 21</em></figure>



<p>Delta search typefies local search algorithms. It walks semi-randomly through the landscape searching for low energy configurations. Its parameter is defined such that, when stuck in a metastable state it can climb out of it in polynomial time if the steepness of its energy barrier is <img alt="\leq \Delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+%5CDelta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\leq \Delta"/>. Thus its failure in the <img alt="p \geq p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cgeq+p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p \geq p_{d}"/> region suggests that there are no barriers of constant size and that barriers of order N are the norm.</p>



<h4>MCMC and the relaxation time of a random walk</h4>



<p>We can understand the geometry of the metastable states in greater detail by reframing our MAP problem as follows: </p>



<p><img alt="\mu_{y,\beta}(\underline{x}) = \frac{1}{Z(\beta)}exp{-\beta \cdot E(\underline{x})}\prod_{i=1}^N Q(y_i|x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_%7By%2C%5Cbeta%7D%28%5Cunderline%7Bx%7D%29+%3D+%5Cfrac%7B1%7D%7BZ%28%5Cbeta%29%7Dexp%7B-%5Cbeta+%5Ccdot+E%28%5Cunderline%7Bx%7D%29%7D%5Cprod_%7Bi%3D1%7D%5EN+Q%28y_i%7Cx_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_{y,\beta}(\underline{x}) = \frac{1}{Z(\beta)}exp{-\beta \cdot E(\underline{x})}\prod_{i=1}^N Q(y_i|x_i)"/> (12)<br/></p>



<p>This form is referred to as the `tilted’ Boltzmann because it is a Boltzmann distribution biased by the likelihood function.</p>



<p>In the low temperature limit this reduces to eqn 10 because it finds support only over words in the codebook.</p>



<p>This distribution more closely mimics physical systems. For nonzero temperature it allows support over vectors which are not actually in our codebook but still have low distance to our received message and have low energy – this allows us to probe the metastable states which trap our local algorithms. This is referred to as a code with `soft’ parity check constraints as our distribution permits decodings which fail some checks.</p>



<p>We will use the following algorithm excerpted from Mezard and Montanari Chapt 21:</p>



<figure class="wp-block-image"><img alt="" class="wp-image-6458" src="https://windowsontheory.files.wordpress.com/2018/12/anneal.png?w=600"/><strong>Fig 8</strong> <em>Excerpted from Mezard and Montanari, 2009 Chapt 21</em></figure>



<p>Where a Glauber update consists of scanning through the bits of the current proposed configuration and flipping the value of bit <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> with probability </p>



<p><img alt="w_{i}(\underbar{\textit{x}}) = \frac{\mu_{y,\beta}(\underline{x}^{(i)})}{\mu_{y,\beta}(\underline{x}^{(i)}) + \mu_{y,\beta}(\underline{x})}" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7Bi%7D%28%5Cunderbar%7B%5Ctextit%7Bx%7D%7D%29+%3D+%5Cfrac%7B%5Cmu_%7By%2C%5Cbeta%7D%28%5Cunderline%7Bx%7D%5E%7B%28i%29%7D%29%7D%7B%5Cmu_%7By%2C%5Cbeta%7D%28%5Cunderline%7Bx%7D%5E%7B%28i%29%7D%29+%2B+%5Cmu_%7By%2C%5Cbeta%7D%28%5Cunderline%7Bx%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_{i}(\underbar{\textit{x}}) = \frac{\mu_{y,\beta}(\underline{x}^{(i)})}{\mu_{y,\beta}(\underline{x}^{(i)}) + \mu_{y,\beta}(\underline{x})}"/> (13)<br/></p>



<p>Where <img alt="\underline{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{x}"/> is the current configuration and <img alt="\underline{x}^{(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D%5E%7B%28i%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{x}^{(i)}"/> is the configuration obtained by flipping <img alt="\underline{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{x}"/>‘s <img alt="i^{th}" class="latex" src="https://s0.wp.com/latex.php?latex=i%5E%7Bth%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i^{th}"/> bit</p>



<p>This method is a spin-off of traditional Markov chain Monte-Carlo algorithms with the variation that we lower the temperature according to an annealing schedule that initially assigns probability to all states proportional to the likelihood component of equation 12, allowing the chain to randomly sample the configuration space in the neighborhood of the received noisy word, until in the low temperature limit it becomes concentrated near to configurations which are proper codewords.</p>



<p>This method is useful to us because MCMCs are good models of how randomized methods of local searching for optimal configurations occurs in physical systems. Furthermore, the convergence of MCMCs and the time it takes them to converge tells us both the properties of the energy wells they terminate in and the barriers between minima in the energy landscape.</p>



<p>Let’s now show a property relating convergence times of MCMCs and energy barriers known as the Arrhenius law.</p>



<p>If we take the example of using a simple MCMC random walk with the update rule below over the following landscape</p>



<p><img alt="w(x\rightarrow x') = min \{e^{-\beta [E(x')-E(x)]},~1\}" class="latex" src="https://s0.wp.com/latex.php?latex=w%28x%5Crightarrow+x%27%29+%3D+min+%5C%7Be%5E%7B-%5Cbeta+%5BE%28x%27%29-E%28x%29%5D%7D%2C%7E1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w(x\rightarrow x') = min \{e^{-\beta [E(x')-E(x)]},~1\}"/></p>



<figure class="wp-block-image is-resized"><img alt="" class="wp-image-6462" height="291" src="https://windowsontheory.files.wordpress.com/2018/12/fig1311.png?w=540&amp;h=291" width="540"/><strong>Fig 9</strong> This represents a random walk along a line in which there are two ground states separated by an energy barrier of height <img alt="\Delta E" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta+E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta E"/>.  <em>Excerpted from Mezard and Montanari, 2009 Chapt 13</em></figure>



<p>We find that the expected number of time steps to cross from one well to another is governed by the Arrhenius law <img alt="\tau \approx exp{\beta \Delta E}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau+%5Capprox+exp%7B%5Cbeta+%5CDelta+E%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tau \approx exp{\beta \Delta E}"/>.</p>



<p>In general, if there exists a largest energy barrier between any two components of the configuration space (also known as the bottleneck) the time it takes to sample both components, also known as the relaxation time of the MCMC is <img alt="\tau_{exp} \geq O(e^{\beta \Delta E})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau_%7Bexp%7D+%5Cgeq+O%28e%5E%7B%5Cbeta+%5CDelta+E%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tau_{exp} \geq O(e^{\beta \Delta E})"/></p>



<p>With this in mind, we can apply our simulated annealing MCMC to LDPC decoding and examine the properties of the bottlenecks, or metastable states, in our configuration landscape.</p>



<h4>Exact values of the metastable energy states for the BEC</h4>



<p>It is a well-known consequence of the 1RSB cavity method that the number of metastable states of energy <img alt="Ne" class="latex" src="https://s0.wp.com/latex.php?latex=Ne&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Ne"/> grows like <img alt="exp(N\Sigma^{e}(e))" class="latex" src="https://s0.wp.com/latex.php?latex=exp%28N%5CSigma%5E%7Be%7D%28e%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="exp(N\Sigma^{e}(e))"/> where <img alt="\Sigma^{e}(e)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CSigma%5E%7Be%7D%28e%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Sigma^{e}(e)"/> is known as the energetic complexity function, a function whose form is implied by the 1RSB cavity equations. This computation can be carried out using a method called Survey Propagation which constructs a factor graph of the messages passed in the original BP factor graph and estimates the values of the marginals of the messages via another round of BP (hence the name 1-step RSB).</p>



<p>Neglecting the actual form of the calculations I will show the following approximate results for the BEC.</p>



<figure class="wp-block-image is-resized"><img alt="" class="wp-image-6463" height="254" src="https://windowsontheory.files.wordpress.com/2018/12/fig213.png?w=602&amp;h=254" width="602"/><strong>Fig 10</strong> Metastable states for random elements of the (3,6) regular ensemble used over the BEC (<img alt="\epsilon_d = .4294" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%3D+.4294&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d = .4294"/> and <img alt="\epsilon_c = .4882" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_c+%3D+.4882&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_c = .4882"/>. <em>Left</em>: the complexity function as a function of energy density above <img alt="\epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d"/>. <em>Right</em>: the maximum and minimum energy densities of metastable states as a function of the erasure probability. Note that at <img alt="\epsilon_d \leq .45  \leq \epsilon_c " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_d+%5Cleq+.45++%5Cleq+%5Cepsilon_c+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_d \leq .45  \leq \epsilon_c "/>latex  the curve is positive only for non zero metastable energy densities. This indicates exponentially many metastable states. At erasure rates above <img alt="\epsilon_c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_c"/> there are exponentially many degenerate ground states. <em>Excerpted from Mezard and Montanari, 2009 Chapt 21</em></figure>



<p>In the regime <img alt="p \geq p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cgeq+p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p \geq p_{d}"/> there exists a zero-energy word corresponding to the correct solution. On top of this, there exist non-trivial solutions to the 1RSB method yielding a complexity curve positive in the regime (<img alt="e_{c}, e_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=e_%7Bc%7D%2C+e_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e_{c}, e_{d}"/>). The positive complexity means that there are exponentially many such states and their finite energy means they violate a finite fraction of the parity checks, making their energy wells relatively deep.</p>



<p>As the error rate of the channel increases above <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> the minimum energy of the metastable state reaches zero continuously. This means at noise levels above <img alt="p_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{c}"/> there are an exponential number of zero-energy states corresponding to configurations which aren’t code words. These codewords are separated by energy barriers <img alt="O(N)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28N%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(N)"/> thus making the relaxation-time of local algorithms, by the Arrhenius law <img alt="exp(N)" class="latex" src="https://s0.wp.com/latex.php?latex=exp%28N%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="exp(N)"/> in this regime.</p>



<figure class="wp-block-image is-resized"><img alt="" class="wp-image-6464" height="229" src="https://windowsontheory.files.wordpress.com/2018/12/fig214.png?w=622&amp;h=229" width="622"/><strong>Fig 11</strong> Decoding random codes from the (3,6) ensemble over the BEC. In both cases <img alt="N = 10^4" class="latex" src="https://s0.wp.com/latex.php?latex=N+%3D+10%5E4&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N = 10^4"/>, and the annealing schedule consists of <img alt="t_max = 1000" class="latex" src="https://s0.wp.com/latex.php?latex=t_max+%3D+1000&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t_max = 1000"/> equidistant temperatures in <img alt="T = 1 / \beta \in [0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=T+%3D+1+%2F+%5Cbeta+%5Cin+%5B0%2C1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T = 1 / \beta \in [0,1]"/>. <em>Left</em>: indicates convergence to the correct ground state at <img alt="\epsilon = .4 &lt; \epsilon_d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+.4+%3C+%5Cepsilon_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon = .4 &lt; \epsilon_d"/>. <em>Right</em>: indicates convergence to approximately the energy density of the highest metastable states <img alt="e_d" class="latex" src="https://s0.wp.com/latex.php?latex=e_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e_d"/> (as calculated by the complexity function via 1RSB) for <img alt="\epsilon = .6 &gt; \epsilon_{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+.6+%3E+%5Cepsilon_%7Bc%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon = .6 &gt; \epsilon_{c}"/>. <em>Excerpted from Mezard and Montanari, 2009 Chapt 21</em><br/><br/></figure>



<p>Here you can see a rough sketch of convergence of the simulated annealing algorithm. As the temperature decreases in the <img alt="p \leq p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cleq+p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p \leq p_{d}"/> regime the algorithm converges to a 0 energy ground state. In the figure on the right we can see that simulated annealing converges to the horizontal line here which corresponds to the energy of the highest metastable state <img alt="e_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=e_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e_{d}"/> for the BEC at <img alt="p = .6" class="latex" src="https://s0.wp.com/latex.php?latex=p+%3D+.6&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p = .6"/>.</p>



<p>Thus we see our local search algorithms end up being attracted to the highest energy of the metastable state.</p>



<figure class="wp-block-image"><img alt="" class="wp-image-6465" src="https://windowsontheory.files.wordpress.com/2018/12/fig215.png?w=600"/><strong>Fig 12</strong> Decoding random codes as in figure 11. Here we plot the minimum energy density achieved through simulated annealing plotted as a function of the erasure probability of the BEC. The continuous line indicates the highest lying metastable states as calculated from the complexity function via 1RSB.<br/><em> Excerpted from Mezard and Montanari, 2009 Chapt 21</em></figure>



<p>Though there is not necessarily an exact correspondence between the residual energy at T=0 for simulated annealing and the highest metastable state <img alt="e_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=e_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e_{d}"/> we see that across all values of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> that at T=0, <img alt="e_{ann} \approx e_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=e_%7Bann%7D+%5Capprox+e_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e_{ann} \approx e_{d}"/> suggesting local search tends to get caught in the deepest metastable energy wells.</p>



<p>This discussion shows that the algorithmic threshold of BP, <img alt="p_{d}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d}"/> indicates the onset of a truly different regime within the energy landscape of the codebook. Metastable states of <img alt="O(N)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28N%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(N)"/> hight proliferate and become exponentially difficult to escape from via local search methods. Thus the failure of BP likely indicates a regime in which no fast algorithms can perform decoding, even though decoding is still theoretically possible when below <img alt="p_c" class="latex" src="https://s0.wp.com/latex.php?latex=p_c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_c"/>, e.g. via exhaustive search of the codebook.</p>



<h3>Appendix A: Random Code Ensembles</h3>



<p>In an RCE, encoding maps applied to the information sequence are chosen with uniform probability over a solution space. Two decoding schemes are often used and applied to the noise – word MAP and symbol MAP decoding. MAP, otherwise known as “maximum <em>a priori</em> probability” works by maximizing the probability distribution to output the most probable transmission. Word MAP decoding schemes output the codeword with the highest probability by minimizing the block error probability, which is otherwise known as the probability with respect to the channel distribution that the decoded word is different than the true transmitted word. Symbol MAP decoding, on the other hand, minimizes the fraction of incorrect bits averaged over the transmitted code word (bit error rate).</p>



<p>RCE code is defined by the codebook in Hamming space, or the set of all binary strings of length <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/>. In a Hamming space characterized by uniform probability, the number of codewords at a given Hamming distance are a function of the distance enumerator. Distance enumerators take as parameters different weights, given that probabilities of codewords are independent of each other. The distance enumerator shows that, for small enough fractional distance from the true message <img alt="x_0" class="latex" src="https://s0.wp.com/latex.php?latex=x_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_0"/>, the growth rate is negative and the average number of codewords at small distance from <img alt="x_0" class="latex" src="https://s0.wp.com/latex.php?latex=x_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_0"/> vanishes exponentially with N. The Gilbert-Varshamov distance, a lower bound threshold, shows that the the average number of codewords is exponentially large at points where the weight numerator is concentrated.</p>



<p>We look at the performance of RCE code in communication over the Binary Symmetric Channel (BSC), where it is assumed that there is a probability p that transmitted bits will be “flipped” (i.e. with probability p, 1 becomes 0 and 0 becomes 1). With BSCs, channel input and output are the same length N sequences of bits. At larger noise levels, there are an exponential number of codewords closer to <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> and decoding is unsuccessful. However, decoding via the symbol MAP decoding scheme shows that the <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>-th bit is decoded by maximizing the marginal probability and amasses contributions from all codewords in the set. Above a threshold, the bit error rate is the same as if the message was transmitted without encoding and decoding, but below this, the RCE seems to work quite well in transmission. </p>



<p>Finite temperature decoding has also been looked at as an interpolation between the two MAP decoding schemes. At low noise, a completely ordered phase can be observed as compared to a glassy phase at higher noise channels. Similar to the a statistical physics model, we can also note an entropy dominated paramagnetic phase at higher temperatures. </p>



<p>Each decoding scheme can be analogized to “sphere packing”, where each probability in the Hamming space distribution represents a sphere of radius r. Decoding schemes have partitions in the Hamming space, so these spheres must be disjoint. If not, intersecting spheres must be eliminated. The lower bound of the remaining spheres is then given by Gilbert-Varshamov bound, whereas the upper bound is dictated by the Hamming distance.</p>



<p>Another random code beside the RCE is the RLC, or random linear code. Encoding in an RLC forms a scheme similar to a linear map, of which all points are equiprobable. The code is specified by an <img alt="N x M" class="latex" src="https://s0.wp.com/latex.php?latex=N+x+M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N x M"/> binary matrix, otherwise known as the generating matrix, and it is projected to be error-free below the Shannon capacity. </p>



<p>There are several sources of randomness in codes. Codes are chosen randomly from an ensemble and the codeword to be transmitted is chosen with uniform probability from the code, according to the theorem of source-channel separation. The channel output is then distributed according to a probabilistic process accounting for channel noise and decoding is done by constructing another probability distribution over possible channel inputs and by estimating its signal bit marginal. The decision on the <em>i</em>-th bit is dependent on the distribution. Thus, complications may arise in distinguishing between the two levels of randomness: code, channel input, and noise (“quenched” disorder) versus Boltzmann probability distributions.</p>



<h3>Appendix B: Weight enumerators and code performance</h3>



<p>The geometric properties of the LDPC codebooks is given by studying the distance enumerator <img alt="N_{\underline{x}0}(d)" class="latex" src="https://s0.wp.com/latex.php?latex=N_%7B%5Cunderline%7Bx%7D0%7D%28d%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N_{\underline{x}0}(d)"/> to give the number of codewords at Hamming distance <em>d</em> from <img alt="\underline{x}_0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{x}_0"/>. This takes all-zeros codewords as the reference and uses the weight enumerator, <img alt="\mathbb{N}(w)=\mathbb{N}{\underline{x_0}}(d=w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BN%7D%28w%29%3D%5Cmathbb%7BN%7D%7B%5Cunderline%7Bx_0%7D%7D%28d%3Dw%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{N}(w)=\mathbb{N}{\underline{x_0}}(d=w)"/> as the denomination (number of codewords having weight equal to <em>w</em>). To estimate the expected weight enumerator <img alt="\mathcal{N}(w)=\mathbb{E}\mathbb{N}(w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%28w%29%3D%5Cmathbb%7BE%7D%5Cmathbb%7BN%7D%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(w)=\mathbb{E}\mathbb{N}(w)"/> for a random code in the <img alt="LDPC_N(\Lambda,P)" class="latex" src="https://s0.wp.com/latex.php?latex=LDPC_N%28%5CLambda%2CP%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="LDPC_N(\Lambda,P)"/> ensemble, we know that <img alt="\mathbb{N}(w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BN%7D%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{N}(w)"/> grows exponentially in block-length <em>N</em>, and that each codeword has a weight <img alt="w=Nw" class="latex" src="https://s0.wp.com/latex.php?latex=w%3DNw&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w=Nw"/> that grows linearly with N. The exponential growth rate <img alt="\phi(w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(w)"/> is defined by</p>



<p><img alt="\mathcal{N}(w=Nw) = e^{N \phi (w)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%28w%3DNw%29+%3D+e%5E%7BN+%5Cphi+%28w%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(w=Nw) = e^{N \phi (w)}"/> (14)<br/></p>



<p>denoting an ‘annealed average’, or a disordered system that could be dominated by rare instances in the ensemble. This gives an upper bound on the number of ‘colored factor graphs’ that have an even number of weighted incident edges divided by the total number of factor graphs in the ensemble. </p>



<p>On the other hand, for graphs of fixed degrees with N variable nodes of degree<em> l</em> and M function nodes of degree <em>k</em>, the total number of edges F is given by <img alt="F=Mk=Nl" class="latex" src="https://s0.wp.com/latex.php?latex=F%3DMk%3DNl&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F=Mk=Nl"/>. A valid colored graph would have <img alt="E=wl" class="latex" src="https://s0.wp.com/latex.php?latex=E%3Dwl&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E=wl"/> edges, with the number of variable nodes given in <img alt="{N\choose w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%5Cchoose+w%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{N\choose w}"/> ways, l assignments of weighted sockets to nodes, and l assignments of unweighted sockets to nodes outside the set. If we take <img alt="m_r" class="latex" src="https://s0.wp.com/latex.php?latex=m_r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m_r"/> to denote the number of function nodes with weighted sockets under the constraints of <img alt="\Sigma_{r=0}^km_r=M" class="latex" src="https://s0.wp.com/latex.php?latex=%5CSigma_%7Br%3D0%7D%5Ekm_r%3DM&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Sigma_{r=0}^km_r=M"/> and <img alt="\Sigma_{r=0}^krm_r=lw" class="latex" src="https://s0.wp.com/latex.php?latex=%5CSigma_%7Br%3D0%7D%5Ekrm_r%3Dlw&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Sigma_{r=0}^krm_r=lw"/>, we find the number of ways to color the function node sockets by</p>



<p><img alt="\mathbb{C}(k,M,w) = \sum_{m_{0},...m_{k}}^{even}{M\choose m_{0},...,m_{k}}\prod_{r}{k\choose r}^{m_{r}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D%28k%2CM%2Cw%29+%3D+%5Csum_%7Bm_%7B0%7D%2C...m_%7Bk%7D%7D%5E%7Beven%7D%7BM%5Cchoose+m_%7B0%7D%2C...%2Cm_%7Bk%7D%7D%5Cprod_%7Br%7D%7Bk%5Cchoose+r%7D%5E%7Bm_%7Br%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{C}(k,M,w) = \sum_{m_{0},...m_{k}}^{even}{M\choose m_{0},...,m_{k}}\prod_{r}{k\choose r}^{m_{r}}"/> (15)</p>



<p><img alt="\mathbb{I}\Big(\sum_{r=0}^km_r=M\Big)\mathbb{I}\Big(\sum_{r=0}^krm_r=lw\Big)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BI%7D%5CBig%28%5Csum_%7Br%3D0%7D%5Ekm_r%3DM%5CBig%29%5Cmathbb%7BI%7D%5CBig%28%5Csum_%7Br%3D0%7D%5Ekrm_r%3Dlw%5CBig%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{I}\Big(\sum_{r=0}^km_r=M\Big)\mathbb{I}\Big(\sum_{r=0}^krm_r=lw\Big)"/> (16)<br/></p>



<p>If we aim to join variable and check nodes so that colorings are matched, knowing that there are <img alt="(lw)!(F-lw)!" class="latex" src="https://s0.wp.com/latex.php?latex=%28lw%29%21%28F-lw%29%21&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(lw)!(F-lw)!"/> possible matchings in each ensemble element, this yields the following formula: </p>



<p><img alt="\mathcal{N}(w)=\frac{(lw)!(F-lw)!}{F!}{N\choose w}\mathbb{C}(k,M,w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%28w%29%3D%5Cfrac%7B%28lw%29%21%28F-lw%29%21%7D%7BF%21%7D%7BN%5Cchoose+w%7D%5Cmathbb%7BC%7D%28k%2CM%2Cw%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(w)=\frac{(lw)!(F-lw)!}{F!}{N\choose w}\mathbb{C}(k,M,w)"/> (17)<br/></p>



<p>At low noise limits, code performance depends on the existence of codewords at distances close to the transmitted codeword. Starting with degree 1 and knowing that the parametric representation for weights is given by </p>



<p><img alt="w = \sum_{l=1}^{l_{max}}\Lambda_l\frac{xy^l}{1+xy^l}" class="latex" src="https://s0.wp.com/latex.php?latex=w+%3D+%5Csum_%7Bl%3D1%7D%5E%7Bl_%7Bmax%7D%7D%5CLambda_l%5Cfrac%7Bxy%5El%7D%7B1%2Bxy%5El%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w = \sum_{l=1}^{l_{max}}\Lambda_l\frac{xy^l}{1+xy^l}"/> (18)</p>



<p>derive that </p>



<p><img alt="\phi(w) = -\frac{1}{2}w\log(w/\Lambda_1^2)+O(w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29+%3D+-%5Cfrac%7B1%7D%7B2%7Dw%5Clog%28w%2F%5CLambda_1%5E2%29%2BO%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(w) = -\frac{1}{2}w\log(w/\Lambda_1^2)+O(w)"/> (19)</p>



<p> when <img alt="x,y,z" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cy%2Cz&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x,y,z"/> scale to <img alt="\sqrt{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bw%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sqrt{w}"/>. This shows that the exponential growth rate <img alt="\phi(w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(w)"/> is strictly positive when <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w"/> is sufficiently small, and that the expected number of codewords within a small Hamming distance from a given codeword is exponential in N. If we take the logarithm of the expected weight enumerator and plot this versus the reduced weight <img alt="w=w/N" class="latex" src="https://s0.wp.com/latex.php?latex=w%3Dw%2FN&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w=w/N"/> for an irregular code with <img alt="l_{min}=1" class="latex" src="https://s0.wp.com/latex.php?latex=l_%7Bmin%7D%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="l_{min}=1"/>, we see that <img alt="\phi(w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(w)"/> is positive near the origin, but that its dervative diverges as <img alt="w\rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Crightarrow+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\rightarrow 0"/>. Since this means that each codeword is surrounded by a large number of very close other codewords, this makes the code a very bad ECC and thus, makes it hard to discriminate between codewords at Hamming distances <img alt="O(1)" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(1)"/> with noisy observations. Applying this same logic to <img alt="l" class="latex" src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="l"/> of min 2, we still observe that <img alt="\phi(w)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(w)"/> tends to 0 more quickly as <img alt="w\rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Crightarrow+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\rightarrow 0"/> in the present case. If we assume that this holds beyond the asymptotic regime, we get</p>



<p><img alt="\bar{\mathcal{N}}(w) = e^{Aw}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbar%7B%5Cmathcal%7BN%7D%7D%28w%29+%3D+e%5E%7BAw%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\bar{\mathcal{N}}(w) = e^{Aw}"/> (20)<br/></p>



<p>or that the number of codewords around a particular codeword is <img alt="o(N)" class="latex" src="https://s0.wp.com/latex.php?latex=o%28N%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="o(N)"/> until a Hamming distance <img alt="d_* \simeq \log N/A" class="latex" src="https://s0.wp.com/latex.php?latex=d_%2A+%5Csimeq+%5Clog+N%2FA&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d_* \simeq \log N/A"/>, otherwise known as the “effective minimum distance”. For <img alt="l_{min} \geq 3" class="latex" src="https://s0.wp.com/latex.php?latex=l_%7Bmin%7D+%5Cgeq+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="l_{min} \geq 3"/>, we find: </p>



<p><br/><img alt="\phi(w) \simeq \Big(\frac{l_{min}-2}{2}\Big)w\log\Big(\frac{w}{\Lambda_{l_min}}\Big)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28w%29+%5Csimeq+%5CBig%28%5Cfrac%7Bl_%7Bmin%7D-2%7D%7B2%7D%5CBig%29w%5Clog%5CBig%28%5Cfrac%7Bw%7D%7B%5CLambda_%7Bl_min%7D%7D%5CBig%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(w) \simeq \Big(\frac{l_{min}-2}{2}\Big)w\log\Big(\frac{w}{\Lambda_{l_min}}\Big)"/> (21)<br/></p>



<p> suggesting that LDPC codes with this property have good short distance behavior. Thus, any error that changes a fraction of the bits smaller than <img alt="w_{*}/2" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7B%2A%7D%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_{*}/2"/> can be corrected in the absence of codewords within an extensive distance <img alt="Nw_{*}" class="latex" src="https://s0.wp.com/latex.php?latex=Nw_%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Nw_{*}"/>. </p>



<p>Let us now focus on the capacity of LDPC codes to correct typical errors in a probabilistic channel. For binary symmetric channels that flip each transmitted bit independently with probability <img alt="p&lt;\frac{1}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=p%3C%5Cfrac%7B1%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p&lt;\frac{1}{2}"/>. If the all-zero codeword <img alt="\underline{x}^{(0)} =\underline{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bx%7D%5E%7B%280%29%7D+%3D%5Cunderline%7B0%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{x}^{(0)} =\underline{0}"/> has been transmitted as <img alt="\underline{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{y}"/>, whose components are iid random variables that take value 0 with probability <img alt="1-p" class="latex" src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1-p"/> and value 1 with probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>, then we use the MAP decoding strategy to minimize the block error rate and output the codeword closest to the channel output <img alt="\underline{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{y}"/>. The expectation value of the code ensemble <img alt="P_B = \mathbb{E}P_B(\mathbb{C})" class="latex" src="https://s0.wp.com/latex.php?latex=P_B+%3D+%5Cmathbb%7BE%7DP_B%28%5Cmathbb%7BC%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_B = \mathbb{E}P_B(\mathbb{C})"/> is an indicator of code ensemble performances. We will show that, as <img alt="N\rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\rightarrow \infty"/>, codes with <img alt="l_{min}\geq 3" class="latex" src="https://s0.wp.com/latex.php?latex=l_%7Bmin%7D%5Cgeq+3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="l_{min}\geq 3"/> will undergo a phase transition separating a low noise from a high noise phase. To derive a lower bound for the capacity of LDPC codes in a BSC channel, we take <img alt="\mathbb{N}=2^{NR}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BN%7D%3D2%5E%7BNR%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{N}=2^{NR}"/> as the size of the codebook <img alt="\mathbb{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{C}"/> and, by union bound:</p>



<p><img alt="P_{B}(\mathbb{C})= \mathbb{P}\Big\{\exists \alpha \neq 0 \text{s.t. } d(\underline{x}^{(\alpha)},\underline{y})\leq d(\underline{0},\underline{y})\Big\}" class="latex" src="https://s0.wp.com/latex.php?latex=P_%7BB%7D%28%5Cmathbb%7BC%7D%29%3D+%5Cmathbb%7BP%7D%5CBig%5C%7B%5Cexists+%5Calpha+%5Cneq+0+%5Ctext%7Bs.t.+%7D+d%28%5Cunderline%7Bx%7D%5E%7B%28%5Calpha%29%7D%2C%5Cunderline%7By%7D%29%5Cleq+d%28%5Cunderline%7B0%7D%2C%5Cunderline%7By%7D%29%5CBig%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{B}(\mathbb{C})= \mathbb{P}\Big\{\exists \alpha \neq 0 \text{s.t. } d(\underline{x}^{(\alpha)},\underline{y})\leq d(\underline{0},\underline{y})\Big\}"/> (22)</p>



<p><img alt="\leq \sum_{\alpha=1}^{\textit{N}-1}\mathbb{P}\Big\{d(\underline{x}^{(\alpha)},\underline{y}) \leq d(\underline{0},\underline{y})\Big\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+%5Csum_%7B%5Calpha%3D1%7D%5E%7B%5Ctextit%7BN%7D-1%7D%5Cmathbb%7BP%7D%5CBig%5C%7Bd%28%5Cunderline%7Bx%7D%5E%7B%28%5Calpha%29%7D%2C%5Cunderline%7By%7D%29+%5Cleq+d%28%5Cunderline%7B0%7D%2C%5Cunderline%7By%7D%29%5CBig%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\leq \sum_{\alpha=1}^{\textit{N}-1}\mathbb{P}\Big\{d(\underline{x}^{(\alpha)},\underline{y}) \leq d(\underline{0},\underline{y})\Big\}"/> (23)</p>



<p><img alt="\leq \sum_{w=1}^N \textit{N}(w)e^{-\gamma w}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+%5Csum_%7Bw%3D1%7D%5EN+%5Ctextit%7BN%7D%28w%29e%5E%7B-%5Cgamma+w%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\leq \sum_{w=1}^N \textit{N}(w)e^{-\gamma w}"/> (24)</p>



<p>This derivation proves that the block error probability depends on the weight enumerator and the <img alt="exp(-\gamma w)" class="latex" src="https://s0.wp.com/latex.php?latex=exp%28-%5Cgamma+w%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="exp(-\gamma w)"/>. This second term shows that an increase in the weight of the codeword corresponds to their contribution being scaled down by an exponential factor. This is because it is less likely that the received message <img alt="\underline{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{y}"/> will be closer to a codeword of large weight than to the all-zero codeword. A geometric construction of this phenomena implies that for large enough <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>, Shannon’s Theorem implies that <img alt="P_B" class="latex" src="https://s0.wp.com/latex.php?latex=P_B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_B"/> is bounded away from 0 for any non-vanishing rate <img alt="R &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=R+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R &gt; 0"/> so that at any p less than the ML threshold for which the <img alt="lim_{N\rightarrow \infty}P_B=0" class="latex" src="https://s0.wp.com/latex.php?latex=lim_%7BN%5Crightarrow+%5Cinfty%7DP_B%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="lim_{N\rightarrow \infty}P_B=0"/>, one can communicate with an arbitrarily small error probability. At a probability equal to the lower bound, the upper bound on <img alt="P_B" class="latex" src="https://s0.wp.com/latex.php?latex=P_B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_B"/> is dominated by codewords of weight <img alt="w \approx N\Tilde{w}" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Capprox+N%5CTilde%7Bw%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w \approx N\Tilde{w}"/>, suggesting that each time an error occurs, a finite fraction of the its are decoded incorrectly and that this fraction doesn’t change very much per transmission. The construction also illustrates that this fraction of incorrectly decoded bits jumps discontinuously from 0 to a finite value when <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> crosses the critical value <img alt="p_{ML}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7BML%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{ML}"/>, constituting a “gap.” This gap is close to a factor of 2. </p>



<h3>Appendix C: BP performance</h3>



<p>See figure 2 for an illustration of a factor graph illustrating this relationship. Again, recall that for LDPC code ensembles in large block-length limits, the degree distributions of variable nodes and check nodes are given by <img alt="\Lambda = {\Lambda_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5CLambda+%3D+%7B%5CLambda_t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Lambda = {\Lambda_t}"/> and <img alt="P = {P_k}" class="latex" src="https://s0.wp.com/latex.php?latex=P+%3D+%7BP_k%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P = {P_k}"/> respectively, where we assume that messages are initialized to <img alt="u_{a\rightarrow i}^{(0)} = 0" class="latex" src="https://s0.wp.com/latex.php?latex=u_%7Ba%5Crightarrow+i%7D%5E%7B%280%29%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u_{a\rightarrow i}^{(0)} = 0"/> for simplicity. This implies that the bit error probability is independent of the transmitted codeword and that therefore, we have the freedom to assume transmission of the all-zero codeword. In analyzing the recursion at the basis of the BP algorithm, we can show that decoding performance improves over time on the basis of symmetry and physical degradation. </p>



<h4>Symmetry</h4>



<p>Symmetry of channel log-likelihood and the variables appearing in density evolution are attributes of a desired BMS channel, suggesting that symmetry is preserved by BP operations in evolution. If we assume that the factor graph associated with an LDPC code is “tree-like”, we can apply BP decoding with a symmetric random initial condition and note that the messages <img alt="u_{a\rightarrow i}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=u_%7Ba%5Crightarrow+i%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u_{a\rightarrow i}^{(t)}"/> and <img alt="h_{i\rightarrow a}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bi%5Crightarrow+a%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{i\rightarrow a}^{(t)}"/> are symmetric variables at all <img alt="t\geq 0" class="latex" src="https://s0.wp.com/latex.php?latex=t%5Cgeq+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t\geq 0"/>. This observance of symmetry is analogous to the Nishimori condition in spin glasses and holds for the MAP log-likelihood of a bit as well.</p>



<h4>Physical degradation</h4>



<p>Let’s first define physical degradation with BMS channels. If we take two channels BMS(1) and BMS(2) denoted by transition matrices </p>



<p><img alt="\{Q_{1}(y|x)\}, \{Q_{2}(y|x)\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BQ_%7B1%7D%28y%7Cx%29%5C%7D%2C+%5C%7BQ_%7B2%7D%28y%7Cx%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{Q_{1}(y|x)\}, \{Q_{2}(y|x)\}"/> and output alphabets <img alt="\mathbb{Y}_1,\mathbb{Y}_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BY%7D_1%2C%5Cmathbb%7BY%7D_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{Y}_1,\mathbb{Y}_2"/>, then BMS(2) is physically degraded with respect to BMS(1) if there exists a third channel C with input alphabet <img alt="\mathbb{Y}_1,\mathbb{Y}_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BY%7D_1%2C%5Cmathbb%7BY%7D_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{Y}_1,\mathbb{Y}_2"/> such that BMS(2) is the concatenation of BMS(1) and C. If we represent transition matrix C as <img alt="\{R(y_2|y_1)\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BR%28y_2%7Cy_1%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{R(y_2|y_1)\}"/> we can represent the above physical degradation as </p>



<p><img alt="Q_2(y_2|x) = \sum{y_1 \in \textit{Y}_1}R(y_2|y_1)Q_1(y_1|x)" class="latex" src="https://s0.wp.com/latex.php?latex=Q_2%28y_2%7Cx%29+%3D+%5Csum%7By_1+%5Cin+%5Ctextit%7BY%7D_1%7DR%28y_2%7Cy_1%29Q_1%28y_1%7Cx%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_2(y_2|x) = \sum{y_1 \in \textit{Y}_1}R(y_2|y_1)Q_1(y_1|x)"/> (25)<br/></p>



<p> This is analogous to a Markov chain <img alt="X \rightarrow Y_1 \rightarrow Y_2" class="latex" src="https://s0.wp.com/latex.php?latex=X+%5Crightarrow+Y_1+%5Crightarrow+Y_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X \rightarrow Y_1 \rightarrow Y_2"/> following partial ordering. Channel reliability is then ordered by measures of conditional entropy and bit error rate. This extends to symmetric random variables, which are associated with BMS channels.</p>



<h4>Thresholds</h4>



<p>We then fix a particular LDPC code and look at BP messages as random variables due to randomness in the vector <img alt="\underline{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7By%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{y}"/> with regards to the proposition down below, showing that the bit error rate decreases monotonously with time:</p>



<p><strong>Proposition:</strong><em> If <img alt="B_{i,r}(F)" class="latex" src="https://s0.wp.com/latex.php?latex=B_%7Bi%2Cr%7D%28F%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B_{i,r}(F)"/> is a tree, then <img alt="h_i^{(0)}\preceq h_i^{(1)} \preceq ... \preceq h_i^{(t-1)} \preceq h_i^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=h_i%5E%7B%280%29%7D%5Cpreceq+h_i%5E%7B%281%29%7D+%5Cpreceq+...+%5Cpreceq+h_i%5E%7B%28t-1%29%7D+%5Cpreceq+h_i%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_i^{(0)}\preceq h_i^{(1)} \preceq ... \preceq h_i^{(t-1)} \preceq h_i^{(t)}"/> for any <img alt="t\leq r-1" class="latex" src="https://s0.wp.com/latex.php?latex=t%5Cleq+r-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t\leq r-1"/>. Analogously, if <img alt="B_{i\rightarrow a,r}(F)" class="latex" src="https://s0.wp.com/latex.php?latex=B_%7Bi%5Crightarrow+a%2Cr%7D%28F%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B_{i\rightarrow a,r}(F)"/>, then <img alt="h_{i\rightarrow a}^{(0)}\preceq h_{i\rightarrow a}^{(1)} \preceq ... \preceq h_{i\rightarrow a}^{(t-1)} \preceq h_{i\rightarrow a}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7Bi%5Crightarrow+a%7D%5E%7B%280%29%7D%5Cpreceq+h_%7Bi%5Crightarrow+a%7D%5E%7B%281%29%7D+%5Cpreceq+...+%5Cpreceq+h_%7Bi%5Crightarrow+a%7D%5E%7B%28t-1%29%7D+%5Cpreceq+h_%7Bi%5Crightarrow+a%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{i\rightarrow a}^{(0)}\preceq h_{i\rightarrow a}^{(1)} \preceq ... \preceq h_{i\rightarrow a}^{(t-1)} \preceq h_{i\rightarrow a}^{(t)}"/> for any <img alt="t\leq r-1" class="latex" src="https://s0.wp.com/latex.php?latex=t%5Cleq+r-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t\leq r-1"/>.</em> </p>



<p>Density evolution in this manner is a useful estimate of the number of distributions of density evolution variables <img alt="{h^{(t)},u^{(t)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%5E%7B%28t%29%7D%2Cu%5E%7B%28t%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{h^{(t)},u^{(t)}}"/> and <img alt="{h_{*}^{(t)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh_%7B%2A%7D%5E%7B%28t%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{h_{*}^{(t)}}"/>. By looking again at the bit error rate <img alt="P_{b}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=P_%7Bb%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{b}^{(t)}"/> and the conditional entropy <img alt="H^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{(t)}"/> as both monotonically decreasing functions of the number of iterations and conversely, monotonically increasing functions of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>, we can derive a finite limit <img alt="P_b^{BP} \equiv \lim_{t\rightarrow\infty}P_b^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=P_b%5E%7BBP%7D+%5Cequiv+%5Clim_%7Bt%5Crightarrow%5Cinfty%7DP_b%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_b^{BP} \equiv \lim_{t\rightarrow\infty}P_b^{(t)}"/>. The corresponding BP threshold can then be defined as</p>



<p><img alt="p_{d} \equiv \sup \Big\{ p \in [0,1/2] : P_b^{BP}(p)=0 \Big\}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bd%7D+%5Cequiv+%5Csup+%5CBig%5C%7B+p+%5Cin+%5B0%2C1%2F2%5D+%3A+P_b%5E%7BBP%7D%28p%29%3D0+%5CBig%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_{d} \equiv \sup \Big\{ p \in [0,1/2] : P_b^{BP}(p)=0 \Big\}"/></p>



<p>For <img alt="p \leq p_d" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cleq+p_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p \leq p_d"/>, however, increasing the number of iterations does not help as the bit error rate is asymptotically lower bounded by <img alt="P_{b}^{BP}(p)&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=P_%7Bb%7D%5E%7BBP%7D%28p%29%3E0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P_{b}^{BP}(p)&gt;0"/> for a fixed number of iterations. Good LDPC codes are thus designed with a large BP threshold with design rate <img alt="R_{des}=1-P'(1)/\Lambda '(1)" class="latex" src="https://s0.wp.com/latex.php?latex=R_%7Bdes%7D%3D1-P%27%281%29%2F%5CLambda+%27%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R_{des}=1-P'(1)/\Lambda '(1)"/> to maximize the threshold noise level for a given degree distribution pair. This ensemble will have a finite fraction of variable nodes of degree 2 and a large number of codewords with small weight, which ultimately prevent the block error probability from vanishing as <img alt="N\rightarrow \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Crightarrow+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\rightarrow \infty"/>.</p>



<h4>References</h4>



<p>[1] Marc Mezard and Andrea Montanari. <br/><em>Information, Physics, and Computation</em>. <br/>Oxford Graduate Texts, 2009.</p></div>
    </content>
    <updated>2018-12-17T01:16:55Z</updated>
    <published>2018-12-17T01:16:55Z</published>
    <category term="Uncategorized"/>
    <category term="Belief propagation"/>
    <category term="coding theory"/>
    <category term="cs229r"/>
    <category term="LDPC"/>
    <category term="statistical physics"/>
    <category term="thresholding behavior"/>
    <author>
      <name>Jeremy Dohmann</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-19T17:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15507</id>
    <link href="https://rjlipton.wordpress.com/2018/12/14/explaining-the-jaccard-metric/" rel="alternate" type="text/html"/>
    <title>Explaining The Jaccard Metric</title>
    <summary>Why is it a metric? Composite of source 1, source 2 Paul Jaccard was a botanist who worked at ETH in Zurich during much of the first half of the 20th century. He created, or discovered, the similarity notion that became the Jaccard metric. Very neat having a metric named after you. Today we discuss […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Why is it a metric?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2018/12/TwoJaccards.png"><img alt="" class="alignright wp-image-15508" height="140" src="https://rjlipton.files.wordpress.com/2018/12/TwoJaccards.png?w=150&amp;h=140" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://alchetron.com/Paul-Jaccard">source 1</a>, <a href="https://orepic.com/paul.jaccard">source 2</a></font></td>
</tr>
</tbody>
</table>
<p>
Paul Jaccard was a botanist who worked at ETH in Zurich during much of the first half of the 20th century. He created, or discovered, the similarity notion that became the <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard metric</a>. Very neat having a metric named after you. </p>
<p>
Today we discuss proofs and explanations that the metric is indeed a metric.</p>
<p>
The Jaccard <i>index</i> <img alt="{J(A,B)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%28A%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J(A,B)}"/> is the ratio of <img alt="{|A \cap B|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccap+B%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|A \cap B|}"/> to <img alt="{|A \cup B|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+B%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|A \cup B|}"/>.  The metric is	</p>
<p align="center"><img alt="\displaystyle \text{[*]} \qquad  J_{\delta}(A,B) = 1 - J(A,B) = 1 - \frac{|A \cap B|}{|A \cup B|} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctext%7B%5B%2A%5D%7D+%5Cqquad++J_%7B%5Cdelta%7D%28A%2CB%29+%3D+1+-+J%28A%2CB%29+%3D+1+-+%5Cfrac%7B%7CA+%5Ccap+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \text{[*]} \qquad  J_{\delta}(A,B) = 1 - J(A,B) = 1 - \frac{|A \cap B|}{|A \cup B|} "/></p>
<p>provided <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> are not both empty sets. If <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> are both empty then <img alt="{J(A,B)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ%28A%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J(A,B)}"/> by definition is <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> and so <img alt="{J_\delta(A,B)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ_%5Cdelta%28A%2CB%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J_\delta(A,B)}"/> is <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>.  Generally we can assume that all the sets are non-empty.</p>
<p>
The key question is to show that this satisfies the <b>triangle inequality</b>. That is, we must show that 	</p>
<p align="center"><img alt="\displaystyle  \text{[**]}\qquad J_\delta(A,C) \le J_\delta(A,B) + J_\delta(B,C). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctext%7B%5B%2A%2A%5D%7D%5Cqquad+J_%5Cdelta%28A%2CC%29+%5Cle+J_%5Cdelta%28A%2CB%29+%2B+J_%5Cdelta%28B%2CC%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \text{[**]}\qquad J_\delta(A,C) \le J_\delta(A,B) + J_\delta(B,C). "/></p>
<p>
Many proofs of this are known, and it has been remarked that some are fairly complicated. Some are short, but <a href="https://www.tandfonline.com/doi/abs/10.1080/07468342.2018.1526020">continued</a> <a href="https://arxiv.org/pdf/1612.02696.pdf">recent</a> <a href="https://www.researchgate.net/publication/326912768_Distance_Between_Sets_-_A_survey">interest</a> seems to say they haven’t satisfied as <em>explanations</em>. </p>
<p>
We think we can supply a quick explanation, if you are already familiar with the triangle inequality holding for Hamming distance on sets: </p>
<p align="center"><img alt="\displaystyle  |A \oplus C| \leq |A \oplus B| + |B \oplus C|, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA+%5Coplus+C%7C+%5Cleq+%7CA+%5Coplus+B%7C+%2B+%7CB+%5Coplus+C%7C%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |A \oplus C| \leq |A \oplus B| + |B \oplus C|, "/></p>
<p>
where <img alt="{A \oplus B = (A \cup B) \setminus (A \cap B)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Coplus+B+%3D+%28A+%5Ccup+B%29+%5Csetminus+%28A+%5Ccap+B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \oplus B = (A \cup B) \setminus (A \cap B)}"/> is symmetric difference. By rewriting [*] as <img alt="{J_\delta(A,B) = |A \oplus B|/|A \cup B|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ_%5Cdelta%28A%2CB%29+%3D+%7CA+%5Coplus+B%7C%2F%7CA+%5Ccup+B%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J_\delta(A,B) = |A \oplus B|/|A \cup B|}"/> we can see that [**] becomes similar but includes denominators. If [**] is <em>false</em> then </p>
<p align="center"><img alt="\displaystyle  \text{[***]}\qquad \frac{|A \oplus C|}{|A \cup C|} &gt; \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctext%7B%5B%2A%2A%2A%5D%7D%5Cqquad+%5Cfrac%7B%7CA+%5Coplus+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%3E+%5Cfrac%7B%7CA+%5Coplus+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+%5Cfrac%7B%7CB+%5Coplus+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \text{[***]}\qquad \frac{|A \oplus C|}{|A \cup C|} &gt; \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. "/></p>
<p>Now if we have <img alt="{B \subseteq A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B \subseteq A \cup C}"/> then replacing both right-hand denominators by <img alt="{|A \cup C|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|A \cup C|}"/> cannot make the right-hand side of [***] bigger. But then we have a common denominator, and we can see from Hamming distance that [**] must be true. </p>
<p>
So suppose <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> includes <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> elements not in <img alt="{A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \cup C}"/>. The left-hand side of [***] is at most <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, so each right-hand fraction must be of the form <img alt="{\frac{p}{q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bp%7D%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{p}{q}}"/> where $latex {p <q> \frac{p – b}{q – b}}&amp;fg=000000$, so removing those elements from <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> would also make the right-hand side of [***] smaller. That brings us back to the <img alt="{B \subseteq A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B \subseteq A \cup C}"/> case and the previous contradiction. So [**] must be true. That’s our proof and explanation in brief. </q></p>
<p>
</p><p/><h2> A Careful Take </h2><p/>
<p/><p>
We will do the above proof more slowly and carefully, to ensure it is really clear. A convention: to make the formulas a bit more readable we use <img alt="{AB}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{AB}"/> to denote the intersection of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>. So the Jaccard metric is now 	</p>
<p align="center"><img alt="\displaystyle  J_\delta(A,B) = 1 - \frac{|AB|}{|A \cup B|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++J_%5Cdelta%28A%2CB%29+%3D+1+-+%5Cfrac%7B%7CAB%7C%7D%7B%7CA+%5Ccup+B%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  J_\delta(A,B) = 1 - \frac{|AB|}{|A \cup B|}. "/></p>
<p>First we explain the main ideas:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> We will argue that the intermediate set <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> in the triangle inequality can be constrained. The set <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> can be a subset of <img alt="{A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \cup C}"/>. The intuition is that any extra elements in <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> can only make the triangle inequality weaker.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> We will replace the definition of the Jaccard metric by equivalent one. This new definition is much closer to a known metric.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> We will reduce the triangle inequality finally to a known triangle inequality.</p>
<p>
<em>Proof:</em> </p>
<p>
Let’s assume that <img alt="{A,B,C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B,C}"/> are the sets, and we wish to prove the triangle inequality: 	</p>
<p align="center"><img alt="\displaystyle  1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+-+%5Cfrac%7B%7CA+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cle+1-%5Cfrac%7B%7CA+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+1-%5Cfrac%7B%7CB+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. "/></p>
<p>
<b>Claim</b>: We can assume that <img alt="{B \subseteq A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B \subseteq A \cup C}"/>. Suppose there was an element <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> in <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> but not in <img alt="{A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \cup C}"/>. Then removing <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> from <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> would only tighten the triangle inequality. That is the LHS 	</p>
<p align="center"><img alt="\displaystyle  1 - \frac{|A C|}{|A \cup C|} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+-+%5Cfrac%7B%7CA+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1 - \frac{|A C|}{|A \cup C|} "/></p>
<p>stays the same and the RHS terms 	</p>
<p align="center"><img alt="\displaystyle  1-\frac{|A B|}{|A \cup B|} \text{ and } 1-\frac{|B C|}{|B \cup C|}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1-%5Cfrac%7B%7CA+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%5Ctext%7B+and+%7D+1-%5Cfrac%7B%7CB+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1-\frac{|A B|}{|A \cup B|} \text{ and } 1-\frac{|B C|}{|B \cup C|}, "/></p>
<p>can only decrease.</p>
<p>
<b>Claim</b>: We can re-write <img alt="{J_\delta(X,Y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BJ_%5Cdelta%28X%2CY%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{J_\delta(X,Y)}"/> as 	</p>
<p align="center"><img alt="\displaystyle  \frac{|X \oplus Y|}{|X \cup Y|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%7CX+%5Coplus+Y%7C%7D%7B%7CX+%5Ccup+Y%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{|X \oplus Y|}{|X \cup Y|}. "/></p>
<p>As noted above, <img alt="{X \oplus Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%5Coplus+Y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X \oplus Y}"/> is the set of elements that are in <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> or <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/> but not both. </p>
<p>
<b>Claim</b>: We note that after applying the last claim three times, [**] becomes: 	</p>
<p align="center"><img alt="\displaystyle  \frac{|A \oplus C|}{|A \cup C|} \le \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%7CA+%5Coplus+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cle+%5Cfrac%7B%7CA+%5Coplus+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+%5Cfrac%7B%7CB+%5Coplus+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{|A \oplus C|}{|A \cup C|} \le \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. "/></p>
<p>
<b>Claim</b>	: Since <img alt="{B \subseteq A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B \subseteq A \cup C}"/> we can multiply by <img alt="{|A \cup C|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|A \cup C|}"/> and get that the triangle inequality is implied by 	</p>
<p align="center"><img alt="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C|. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA+%5Coplus+C%7C+%5Cle+%7CA+%5Coplus+B%7C+%2B+%7CB+%5Coplus+C%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C|. "/></p>
<p>Note this uses that 	</p>
<p align="center"><img alt="\displaystyle  |A \oplus B| + |B \oplus C| \le |A \oplus B| \frac{|A \cup C|}{|A \cup B|} + |B \oplus C|\frac{|A \cup C|}{|B \cup C|}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA+%5Coplus+B%7C+%2B+%7CB+%5Coplus+C%7C+%5Cle+%7CA+%5Coplus+B%7C+%5Cfrac%7B%7CA+%5Ccup+C%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+%7CB+%5Coplus+C%7C%5Cfrac%7B%7CA+%5Ccup+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |A \oplus B| + |B \oplus C| \le |A \oplus B| \frac{|A \cup C|}{|A \cup B|} + |B \oplus C|\frac{|A \cup C|}{|B \cup C|}, "/></p>
<p>which follows from 	</p>
<p align="center"><img alt="\displaystyle  1 \le \frac{|A \cup C|}{|A \cup B|} \text{ and } 1 \le \frac{|A \cup C|}{|B \cup C|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+%5Cle+%5Cfrac%7B%7CA+%5Ccup+C%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%5Ctext%7B+and+%7D+1+%5Cle+%5Cfrac%7B%7CA+%5Ccup+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1 \le \frac{|A \cup C|}{|A \cup B|} \text{ and } 1 \le \frac{|A \cup C|}{|B \cup C|}. "/></p>
<p>
<b>Claim</b>	: But the last step is that 	</p>
<p align="center"><img alt="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA+%5Coplus+C%7C+%5Cle+%7CA+%5Coplus+B%7C+%2B+%7CB+%5Coplus+C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C| "/></p>
<p>is just the triangle inequality for the Hamming distance. It uses that 	</p>
<p align="center"><img alt="\displaystyle  (x \oplus y) \le (x \oplus z) + (z \oplus y) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x+%5Coplus+y%29+%5Cle+%28x+%5Coplus+z%29+%2B+%28z+%5Coplus+y%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (x \oplus y) \le (x \oplus z) + (z \oplus y) "/></p>
<p>holds for any single bits <img alt="{x,y,z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y,z}"/>.</p>
<p>
<img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
</p><p/><h2> How We Found The Proof </h2><p/>
<p/><p>
If you’re curious how we found the above, we were trying to check a different kind of proof.  We started with the statement of the triangle inequality: 	</p>
<p align="center"><img alt="\displaystyle  \text{[**]}\qquad 1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctext%7B%5B%2A%2A%5D%7D%5Cqquad+1+-+%5Cfrac%7B%7CA+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cle+1-%5Cfrac%7B%7CA+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+1-%5Cfrac%7B%7CB+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \text{[**]}\qquad 1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. "/></p>
<p>
This looks a bit scary, with its multiple ratios and addition and subtraction. But the following feature jumps out: </p>
<blockquote><p><b> </b> <em>The right side depends on <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B}"/> but the left side does not. </em>
</p></blockquote>
<p>This suggests the idea of asking what happens if we change <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> one element at a time? Can we “walk” it to an extreme point at which the truth of [**] is obvious? A promising start was that we could remove any elements from <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> that are not in <img alt="{A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \cup C}"/>, as shown above. So we can assume <img alt="{B \subseteq A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B \subseteq A \cup C}"/>. Can we move <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> toward <img alt="{A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \cup C}"/> or at least <img alt="{A \oplus C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Coplus+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \oplus C}"/> while preserving the implication of truth for [**]? </p>
<p>
Seen in this light, our above proof’s replacing the denominators by <img alt="{|A \cup C|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|A \cup C|}"/> is an “illegal move”—not a change to <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>—so less interesting. But we happened to notice it worked. Let’s follow the train of thought from where we got that <img alt="{B \subseteq A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B \subseteq A \cup C}"/> can be assumed. Okay, what the next move to make with <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>? Look again at the key expression: 	</p>
<p align="center"><img alt="\displaystyle  1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1+-+%5Cfrac%7B%7CA+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cle+1-%5Cfrac%7B%7CA+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+1-%5Cfrac%7B%7CB+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1 - \frac{|A C|}{|A \cup C|} \le 1-\frac{|A B|}{|A \cup B|} + 1-\frac{|B C|}{|B \cup C|}. "/></p>
<p>Can we simplify this in some way? The answer is yes. The structure of <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> minus an expression suggested that perhaps we could combine the <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> and the ratios. Indeed it is not too hard to note that 	</p>
<p align="center"><img alt="\displaystyle  1-\frac{|XY|}{|X \cup Y|} = \frac{|X \oplus Y|}{|X \cup Y|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1-%5Cfrac%7B%7CXY%7C%7D%7B%7CX+%5Ccup+Y%7C%7D+%3D+%5Cfrac%7B%7CX+%5Coplus+Y%7C%7D%7B%7CX+%5Ccup+Y%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1-\frac{|XY|}{|X \cup Y|} = \frac{|X \oplus Y|}{|X \cup Y|}. "/></p>
<p>Okay perhaps this is not obvious. It is not trivial, but it is a standard idea that <img alt="{1 - p/q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+-+p%2Fq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 - p/q}"/> looks like the complementation of the “probabilty” ratio <img alt="{p/q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%2Fq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p/q}"/>. Once you think of this the exact formula follows. Thus we can re-write the required triangle inequality now as 	</p>
<p align="center"><img alt="\displaystyle  \frac{|A \oplus C|}{|A \cup C|} \le \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B%7CA+%5Coplus+C%7C%7D%7B%7CA+%5Ccup+C%7C%7D+%5Cle+%5Cfrac%7B%7CA+%5Coplus+B%7C%7D%7B%7CA+%5Ccup+B%7C%7D+%2B+%5Cfrac%7B%7CB+%5Coplus+C%7C%7D%7B%7CB+%5Ccup+C%7C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{|A \oplus C|}{|A \cup C|} \le \frac{|A \oplus B|}{|A \cup B|} + \frac{|B \oplus C|}{|B \cup C|}. "/></p>
<p>We are almost done. The ratios are annoying, so can we get rid of them? We have assumed that <img alt="{B \subseteq A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%5Csubseteq+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B \subseteq A \cup C}"/>. So it seems like a good idea to assume—for the moment—that <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is actually equal to <img alt="{A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \cup C}"/>. But then it is easy to see that <img alt="{A \cup B = A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+B+%3D+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \cup B = A \cup C}"/> and <img alt="{B \cup C = A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%5Ccup+C+%3D+A+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B \cup C = A \cup C}"/>. So the above becomes after multiplying by <img alt="{|A \cup C|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CA+%5Ccup+C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|A \cup C|}"/>, 	</p>
<p align="center"><img alt="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C|. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA+%5Coplus+C%7C+%5Cle+%7CA+%5Coplus+B%7C+%2B+%7CB+%5Coplus+C%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |A \oplus C| \le |A \oplus B| + |B \oplus C|. "/></p>
<p>This looks really nice, no more ratios. Wait what is this expression? The <img alt="{\oplus}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Coplus%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\oplus}"/> is the exclusive-or function and it is not hard to note that this is the classic Hamming distance. So this inequality is a fact. Recall the Hamming distance records the number of differences between two bit-vectors, but sets are really just bit vectors.</p>
<p>
Are we there yet? Almost. We only need to argue that if <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is less than <img alt="{A \cup C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Ccup+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \cup C}"/> the inequality we need is actually stronger. So we are done. </p>
<p/><h2> Open Problems </h2><p/>
<p/><p>
Do you like our proof? Is it clear from the intro—or from the second section? Or are you still unsure why the Jaccard metric satisfies the triangle inequality? We may follow up with more about other proofs.</p>
<p>
We don’t think the photo used by this online Alchetron <a href="https://alchetron.com/Paul-Jaccard">bio</a> of Paul Jaccard is the botanist. It looks too modern, for one. No other photo seems extant. Google Images guesses Alchetron’s image to be Adrian Herzog, but we think its highest Jaccard index is to <a href="https://orepic.com/">Orepic</a> user <a href="https://orepic.com/paul.jaccard">paul.jaccard</a>, as shown at top. Can you solve the mystery of who? </p>
<p/><p><br/>
[Switched to standard J-sub-delta notation to clarify and fix issues, fixed HTML conversion glitch with p-q fractions]</p></font></font></div>
    </content>
    <updated>2018-12-15T01:27:23Z</updated>
    <published>2018-12-15T01:27:23Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Oldies"/>
    <category term="Proofs"/>
    <category term="explanations"/>
    <category term="Jaccard index"/>
    <category term="Paul Jaccard"/>
    <category term="similarity metric"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2018-12-19T17:21:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2018/12/14/faculty-at-duke-university-apply-by-december-15-2018/</id>
    <link href="https://cstheory-jobs.org/2018/12/14/faculty-at-duke-university-apply-by-december-15-2018/" rel="alternate" type="text/html"/>
    <title>Faculty at Duke University (apply by December 15, 2018)</title>
    <summary>The Dec 15 deadline is soft. Applications submitted in the next couple of weeks would also receive consideration. All ranks. All areas in theoretical computer science including algorithms, complexity, and cryptography. Website: https://www.cs.duke.edu/openings_faculty Email: jschmidt@cs.duke.edu
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Dec 15 deadline is soft. Applications submitted in the next couple of weeks would also receive consideration. All ranks. All areas in theoretical computer science including algorithms, complexity, and cryptography.</p>
<p>Website: <a href="https://www.cs.duke.edu/openings_faculty">https://www.cs.duke.edu/openings_faculty</a><br/>
Email: jschmidt@cs.duke.edu</p></div>
    </content>
    <updated>2018-12-14T17:13:10Z</updated>
    <published>2018-12-14T17:13:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2018-12-19T17:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2018/12/14/head-of-school-in-computer-science-and-engineering-at-unsw-sydney-australia-apply-by-january-18-2019/</id>
    <link href="https://cstheory-jobs.org/2018/12/14/head-of-school-in-computer-science-and-engineering-at-unsw-sydney-australia-apply-by-january-18-2019/" rel="alternate" type="text/html"/>
    <title>Head of School in Computer Science and Engineering at UNSW Sydney, Australia (apply by January 18, 2019)</title>
    <summary>UNSW Sydney is recruiting a Head of School for the School of Computer Science &amp; Engineering. This is a continuing position, normally at the level of professor. Website: http://external-careers.jobs.unsw.edu.au/cw/en/job/495688/head-of-school-cse Email: sergeg@cse.unsw.edu.au
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>UNSW Sydney is recruiting a Head of School for the School of Computer Science &amp; Engineering.<br/>
This is a continuing position, normally at the level of professor.</p>
<p>Website: <a href="http://external-careers.jobs.unsw.edu.au/cw/en/job/495688/head-of-school-cse">http://external-careers.jobs.unsw.edu.au/cw/en/job/495688/head-of-school-cse</a><br/>
Email: sergeg@cse.unsw.edu.au</p></div>
    </content>
    <updated>2018-12-14T06:29:06Z</updated>
    <published>2018-12-14T06:29:06Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2018-12-19T17:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6351</id>
    <link href="https://windowsontheory.org/2018/12/13/ising-perceptron-under-gaussian-disorder-and-k-nae-sat/" rel="alternate" type="text/html"/>
    <title>Ising Perceptron under Gaussian Disorder, and k-NAE-SAT</title>
    <summary>Blog Post By: Patrick Guo, Vinh-Kha Le, Shyam Narayanan, and David Stoner Methods in statistical physics are known to be extremely useful for understanding certain problems in theoretical computer science. Physical observations can motivate the underlying theoretical models, which in turn explain some of the physical phenomena. This post is based on Professor Nike Sun’s […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Blog Post By: Patrick Guo, Vinh-Kha Le, Shyam Narayanan, and David Stoner</strong></p>
<p>Methods in statistical physics are known to be extremely useful for understanding certain problems in theoretical computer science. Physical observations can motivate the underlying theoretical models, which in turn explain some of the physical phenomena.</p>
<p>This post is based on Professor Nike Sun’s guest lecture on the Ising Perceptron model and regular NAE-SAT for CS 229R: Physics and Computation. These are both examples of random constraint satisfaction problems, where we have <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> variables <img alt="x_1, ..., x_n \in \{-1, 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C+...%2C+x_n+%5Cin+%5C%7B-1%2C+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1, ..., x_n \in \{-1, 1\}"/> and certain relations, or constraints, between the <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i"/>, and we wish to approximate the number of solutions or visualize the geometry of the solutions. For both problems, the problem instance can be random: for example, the linear constraints in the Ising perceptron model are random, and the clauses in the NAE-SAT instance are chosen at random. As in the previous blog posts, to understand the geometry of solutions, statistical physicists think of sampling random solutions from <img alt="\{-1, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{-1, 1\}^n"/>, which introduces a second type of randomness.</p>
<p>This post is meant to be expository. For interested readers, we point to useful references at the end for more rigorous treatments of these topics.</p>
<p><strong>1. Perceptron Model</strong></p>
<p>The Ising Perceptron under Gaussian disorder asks how many points in <img alt="\{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}^n"/> survive <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> half-plane bisections (i.e. are satisfying assignments for <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> constraints), where the planes’ coefficients are drawn from standard Gaussians. Like many problems in statistical physics, there is likely a critical capacity of constraints where having more constraints yields no survivors with high probability, and having fewer constraints yields survivors with high probability. This lecture gives an overview of a proof for one side of this physics prediction, i.e. the existence of a lower bound critical capacity, where having fewer constraints yields survivors with positive probability. Briefly, we use the <em>Cavity method</em> to approximate the distribution of the number of satisfying assignments, then attempt to use the second moment method on that distribution to get our lower bound. A direct application fails, however, due to the variance of the Gaussian constraints. The solution is to carefully choose exactly what to condition on without destroying the model. Specifically, we iteratively compute values related to the Gaussian disorder, after which we are able remove enough variance for the second moment method to work and thus establish the lower bound for the Ising Perceptron’s critical capacity. The result holds subject to an analytical condition which is detailed in the paper (Condition 1.2 in [6]) and which remains to be rigorously verified.</p>
<p><strong>1.1. Problem</strong></p>
<p>We pick a random direction in <img alt="\mathbb{R}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{R}^n"/>, and delete all vertices in the hypercube <img alt="\{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}^n"/> which are in the half-space negatively correlated with that direction. We repeat this process of picking a random half space and deleting points <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> times, and see if any points in the hypercube survive. Formally, we define the perceptron model as follows:</p>
<p><strong>Definition 1:</strong> Let <img alt="G = (g_{\mu{}i})_{\mu\ge 1, i\ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=G+%3D+%28g_%7B%5Cmu%7B%7Di%7D%29_%7B%5Cmu%5Cge+1%2C+i%5Cge+1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G = (g_{\mu{}i})_{\mu\ge 1, i\ge 1}"/> array of i.i.d. standard Gaussians. Let <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> be the largest integer such that:</p>
<p><img alt="\left\{J\in \{\pm 1\}^N:\sum_{i=1}^N\frac{g_{\mu{}i}J_i}{\sqrt{N}}\ge 0 \hspace{0.1cm} \forall \mu\le M\right\}\neq \emptyset." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7BJ%5Cin+%5C%7B%5Cpm+1%5C%7D%5EN%3A%5Csum_%7Bi%3D1%7D%5EN%5Cfrac%7Bg_%7B%5Cmu%7B%7Di%7DJ_i%7D%7B%5Csqrt%7BN%7D%7D%5Cge+0+%5Chspace%7B0.1cm%7D+%5Cforall+%5Cmu%5Cle+M%5Cright%5C%7D%5Cneq+%5Cemptyset.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left\{J\in \{\pm 1\}^N:\sum_{i=1}^N\frac{g_{\mu{}i}J_i}{\sqrt{N}}\ge 0 \hspace{0.1cm} \forall \mu\le M\right\}\neq \emptyset."/></p>
<p>More compactly, <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> is the largest integer such that</p>
<p><img alt="\left\{J\in \{\pm 1\}^N:\frac{GJ}{\sqrt{N}}\ge 0\right\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%5C%7BJ%5Cin+%5C%7B%5Cpm+1%5C%7D%5EN%3A%5Cfrac%7BGJ%7D%7B%5Csqrt%7BN%7D%7D%5Cge+0%5Cright%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left\{J\in \{\pm 1\}^N:\frac{GJ}{\sqrt{N}}\ge 0\right\}"/></p>
<p>is nonempty, where the inequality is taken pointwise, <img alt="G \in \mathbb{R}^{M \times N}" class="latex" src="https://s0.wp.com/latex.php?latex=G+%5Cin+%5Cmathbb%7BR%7D%5E%7BM+%5Ctimes+N%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G \in \mathbb{R}^{M \times N}"/> is an array of i.i.d. std. Gaussians, and <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J"/> is treated as a vector in <img alt="\{\pm 1\}^N \subset \mathbb{R}^N" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D%5EN+%5Csubset+%5Cmathbb%7BR%7D%5EN&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}^N \subset \mathbb{R}^N"/>.</p>
<p><img alt="CS229 Pic 1" class="alignnone size-full wp-image-6353" src="https://windowsontheory.files.wordpress.com/2018/12/CS229-Pic-1.png?w=600"/></p>
<p>In the late 80’s, physicists conjectured that there is a critical capacity <img alt="\alpha_{*}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_{*}"/> such that <img alt="\frac{M}{N} \overset{P}{\to} \alpha_{*}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BM%7D%7BN%7D+%5Coverset%7BP%7D%7B%5Cto%7D+%5Calpha_%7B%2A%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{M}{N} \overset{P}{\to} \alpha_{*}"/> where <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> is a function of <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/>. The predicted critical capacity has been studied, for example in [7,9]. Our goal is to establish a lower bound on <img alt="\alpha_*" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_*"/> for the Ising Perceptron under Gaussian disorder. To do this, let <img alt="Z(G)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)"/> denote the random variable which measures how many choices of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> variables survive <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M"/> Gaussian constraints in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> (this is the partition function). We want to show <img alt="Z &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=Z+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z &gt; 0"/> with high probability when there are <img alt="M &lt; \alpha_{*}N" class="latex" src="https://s0.wp.com/latex.php?latex=M+%3C+%5Calpha_%7B%2A%7DN&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="M &lt; \alpha_{*}N"/> constraints. To this end, the second moment method seems promising:</p>
<p><strong>Second Moment Method: </strong>If <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> is a nonnegative random variable with finite variance, then</p>
<p><img alt="P(Z &gt; 0) \ge \frac{\left(\mathbb{E}[Z]\right)^2}{\mathbb{E}[Z^2]}" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z+%3E+0%29+%5Cge+%5Cfrac%7B%5Cleft%28%5Cmathbb%7BE%7D%5BZ%5D%5Cright%29%5E2%7D%7B%5Cmathbb%7BE%7D%5BZ%5E2%5D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z &gt; 0) \ge \frac{\left(\mathbb{E}[Z]\right)^2}{\mathbb{E}[Z^2]}"/></p>
<p>However, this method actually fails due to various sources of variance in the perceptron model. We will briefly sketch the fix as given in [6].</p>
<p>Before we can start, however, what does the distribution of <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> even look like? This is actually quite computationally intensive; we will use the <strong>cavity equations</strong>, a technique developed in [12,13], to approximate <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/>‘s distribution.</p>
<p><strong>1.2. Cavity Method</strong></p>
<p>The goal of the cavity method is to see how the solution space changes as we remove a row or a column of the matrix <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> with i.i.d. standard Gaussian entries, assuming that <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is fixed. Since our matrix <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is big and difficult to deal with, we try to see how the solution space changes as we add one row or one column at a time. Why is this valuable? We can think of the system of variables and constraints as an interaction between the rows (constraints) and columns (variables), so the number of solutions should behave proportionally to the product of the solutions attributed to each variable and each constraint. With this as motivation, define <img alt="G_{-\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{-\mu}"/> as the matrix obtained by removing row <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/> and <img alt="G^{-i}" class="latex" src="https://s0.wp.com/latex.php?latex=G%5E%7B-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G^{-i}"/> as the matrix obtained by removing column <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>. We can approximate</p>
<p><img alt="Z(G) \approx \prod\limits_{\mu = 1}^{M} \frac{Z(G)}{Z(G_{-\mu})} \cdot \prod\limits_{i = 1}^{N} \frac{Z(G)}{Z(G^{-i})}" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29+%5Capprox+%5Cprod%5Climits_%7B%5Cmu+%3D+1%7D%5E%7BM%7D+%5Cfrac%7BZ%28G%29%7D%7BZ%28G_%7B-%5Cmu%7D%29%7D+%5Ccdot+%5Cprod%5Climits_%7Bi+%3D+1%7D%5E%7BN%7D+%5Cfrac%7BZ%28G%29%7D%7BZ%28G%5E%7B-i%7D%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G) \approx \prod\limits_{\mu = 1}^{M} \frac{Z(G)}{Z(G_{-\mu})} \cdot \prod\limits_{i = 1}^{N} \frac{Z(G)}{Z(G^{-i})}"/></p>
<p>since we can think of the partition function as receiving a multiplicative factor from each addition of a row and each addition of a column. Thus, the cavity method seeks to compute <img alt="Z(G)/Z(G_{-\mu})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G_{-\mu})"/> and <img alt="Z(G)/Z(G^{-i})." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})."/></p>
<p><strong>1.2.1. Removing a constraint</strong></p>
<p>Our goal in computing <img alt="Z(G)/Z(G_{-\mu})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G_{-\mu})"/> is to understand how the solution space <img alt="SOL(G_{-\mu})" class="latex" src="https://s0.wp.com/latex.php?latex=SOL%28G_%7B-%5Cmu%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="SOL(G_{-\mu})"/> changes when we add in the constraint <img alt="\mu." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu."/> Recalling that <img alt="J \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=J+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J \in \{\pm 1\}^n"/> is our vector that is potentially in the solution space, if we define</p>
<p><img alt="\Delta_\mu := \sum_{i = 1}^{N}\frac{g_{\mu{}i}J_i}{\sqrt{N}}," class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%3A%3D+%5Csum_%7Bi+%3D+1%7D%5E%7BN%7D%5Cfrac%7Bg_%7B%5Cmu%7B%7Di%7DJ_i%7D%7B%5Csqrt%7BN%7D%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu := \sum_{i = 1}^{N}\frac{g_{\mu{}i}J_i}{\sqrt{N}},"/></p>
<p>then adding the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/> is equivalent to forcing <img alt="\Delta_\mu \ge 0." class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%5Cge+0.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu \ge 0."/> We will try to understand the distribution of <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> under the Gibbs measure <img alt="\nu_{-\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\nu_{-\mu}"/>, which is the uniform measure on the solution space without the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>. This way, we can determine the probability of <img alt="\Delta_\mu \ge 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu \ge 0"/> under the Gibbs measure, which will equal <img alt="Z(G)/Z(G_{-\mu})." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G_%7B-%5Cmu%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G_{-\mu})."/> To do so, we will use the <strong>Replica Symmetric Cavity Assumption</strong> (which we will abbreviate as RS cavity assumption). The RS cavity assumption lets us assume that the <img alt="J_i" class="latex" src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i"/>‘s for <img alt="1 \le i \le N" class="latex" src="https://s0.wp.com/latex.php?latex=1+%5Cle+i+%5Cle+N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1 \le i \le N"/> are independent under <img alt="\nu_{-\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnu_%7B-%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\nu_{-\mu}"/>. As the <img alt="J_i" class="latex" src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i"/>‘s are some discrete sample space that depend on our constraints, we do not actually have full independence, but the RS cavity assumption tells us there is very little dependence between the <img alt="J_i" class="latex" src="https://s0.wp.com/latex.php?latex=J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i"/>‘s, so we pretend they are independent.</p>
<p>Note that <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> should be approximately normally distributed, since it is the sum of many “independent” terms <img alt="g_{\mu i} J_i" class="latex" src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu+i%7D+J_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_{\mu i} J_i"/> by the RS Cavity assumption. Now, define <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/> as the expectation of <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> under the Gibbs measure without the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>, and <img alt="v_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=v_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v_\mu"/> as the variance of <img alt="\Delta_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Delta_\mu"/> under the Gibbs measure without the constraint <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu"/>. It will also turn out that the variance <img alt="v_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=v_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v_\mu"/> will concentrate around a constant <img alt="\sigma^2." class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma^2."/> Thus,</p>
<p><img alt="\frac{Z(G)}{Z(G_{-\mu})} \approx \nu_{-\mu}(\Delta_{\mu} \ge 0) = \overline{\Phi} (\frac{-h_\mu}{\sigma})." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BZ%28G%29%7D%7BZ%28G_%7B-%5Cmu%7D%29%7D+%5Capprox+%5Cnu_%7B-%5Cmu%7D%28%5CDelta_%7B%5Cmu%7D+%5Cge+0%29+%3D+%5Coverline%7B%5CPhi%7D+%28%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{Z(G)}{Z(G_{-\mu})} \approx \nu_{-\mu}(\Delta_{\mu} \ge 0) = \overline{\Phi} (\frac{-h_\mu}{\sigma})."/></p>
<p>Here, <img alt="\overline{\Phi}(\frac{-h_\mu}{\sigma})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Coverline%7B%5CPhi%7D%28%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\overline{\Phi}(\frac{-h_\mu}{\sigma})"/> equals the probability that a random <img alt="\mathcal{N}(h_\mu, \sigma^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%28h_%5Cmu%2C+%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(h_\mu, \sigma^2)"/> Gaussian distribution is positive, or equivalently, the probability that a standard Gaussian <img alt="\mathcal{N}(0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(0, 1)"/> distribution is greater than <img alt="\frac{-h_\mu}{\sigma}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B-h_%5Cmu%7D%7B%5Csigma%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{-h_\mu}{\sigma}."/></p>
<p><strong>1.2.2. Removing a spin</strong></p>
<p>To calculate the cavity equation for removing one column, we think of removing a column as deleting one spin from <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J"/>. Define <img alt="J^{-i}" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E%7B-i%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J^{-i}"/> as the vector resulting from removing spin <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> from <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J"/>. We want to calculate <img alt="Z(G)/Z(G^{-i})." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})."/> Note that it is possible for <img alt="J^{-i} \not\in SOL(G^{-i})" class="latex" src="https://s0.wp.com/latex.php?latex=J%5E%7B-i%7D+%5Cnot%5Cin+SOL%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J^{-i} \not\in SOL(G^{-i})"/> but <img alt="J \in SOL(G)" class="latex" src="https://s0.wp.com/latex.php?latex=J+%5Cin+SOL%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J \in SOL(G)"/> now, which complicates this calculation. It is possible to overcome this difficulty by passing to positive temperature, though this makes the calculations incredibly difficult. We do not worry about these issues here, and just briefly sketch how <img alt="Z(G)/Z(G^{-i})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})"/> is computed.</p>
<p>To compute <img alt="Z(G)/Z(G^{-i})" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29%2FZ%28G%5E%7B-i%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)/Z(G^{-i})"/>, we will split the numerator into a sum of two terms based on the sign of <img alt="J_i," class="latex" src="https://s0.wp.com/latex.php?latex=J_i%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i,"/> which will allow us to compute not only <img alt="Z(G)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G)"/> but also <img alt="\langle J_i \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+J_i+%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\langle J_i \rangle"/>, which represents the magnetization at spin <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>. A series of complicated calculations (see the lecture notes for the details) will give us</p>
<p><img alt="\frac{Z(G)}{Z(G^{-i})} = \frac{\exp(H_i)+\exp(-H_i)}{exp(c)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BZ%28G%29%7D%7BZ%28G%5E%7B-i%7D%29%7D+%3D+%5Cfrac%7B%5Cexp%28H_i%29%2B%5Cexp%28-H_i%29%7D%7Bexp%28c%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{Z(G)}{Z(G^{-i})} = \frac{\exp(H_i)+\exp(-H_i)}{exp(c)}"/></p>
<p>for some constant <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/>, where <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/> is a quantity that compares how much more correlated spin <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/> is to the constraints than all other spins. The <img alt="\exp(+H_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28%2BH_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(+H_i)"/> will come from the solutions for <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> with <img alt="J_i = 1" class="latex" src="https://s0.wp.com/latex.php?latex=J_i+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i = 1"/> and the <img alt="\exp(-H_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-H_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(-H_i)"/> will come from the solutions with <img alt="J_i = -1" class="latex" src="https://s0.wp.com/latex.php?latex=J_i+%3D+-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="J_i = -1"/>.</p>
<p>The above equations allow us to deduce that</p>
<p><img alt="Z(G) \approx \prod\limits_{\mu = 1}^{M} \overline{\Phi}\left(-\frac{h_\mu}{\sigma}\right) \cdot \prod\limits_{i = 1}^{N} \frac{\exp(H_i) + \exp(-H_i)}{\exp(c)}." class="latex" src="https://s0.wp.com/latex.php?latex=Z%28G%29+%5Capprox+%5Cprod%5Climits_%7B%5Cmu+%3D+1%7D%5E%7BM%7D+%5Coverline%7B%5CPhi%7D%5Cleft%28-%5Cfrac%7Bh_%5Cmu%7D%7B%5Csigma%7D%5Cright%29+%5Ccdot+%5Cprod%5Climits_%7Bi+%3D+1%7D%5E%7BN%7D+%5Cfrac%7B%5Cexp%28H_i%29+%2B+%5Cexp%28-H_i%29%7D%7B%5Cexp%28c%29%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(G) \approx \prod\limits_{\mu = 1}^{M} \overline{\Phi}\left(-\frac{h_\mu}{\sigma}\right) \cdot \prod\limits_{i = 1}^{N} \frac{\exp(H_i) + \exp(-H_i)}{\exp(c)}."/></p>
<p><strong>1.3. The Randomness of G</strong></p>
<p>In the previous section, we regarded <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> as fixed. We now use the these results but allow for <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> to be random again. Recall <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>‘s entries were i.i.d. Gaussians. We thus get that <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/>, as a linear combination of the <img alt="g_{\mu{}i}" class="latex" src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu%7B%7Di%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_{\mu{}i}"/>‘s, is a Gaussian. We thus can write <img alt="h_\mu \sim \mathcal{N}(0, q)." class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu+%5Csim+%5Cmathcal%7BN%7D%280%2C+q%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu \sim \mathcal{N}(0, q)."/> Similarly, <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/> is a linear combination of the <img alt="g_{\mu{}i}" class="latex" src="https://s0.wp.com/latex.php?latex=g_%7B%5Cmu%7B%7Di%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_{\mu{}i}"/>‘s and must be Gaussian. We thus write <img alt="H_i \sim \mathcal{N}(0, \psi)" class="latex" src="https://s0.wp.com/latex.php?latex=H_i+%5Csim+%5Cmathcal%7BN%7D%280%2C+%5Cpsi%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i \sim \mathcal{N}(0, \psi)"/>.</p>
<p>With some calculations detailed in the lecture notes and [12], we get <em>Gardner’s Formula</em>:</p>
<p><img alt="\frac{\ln Z}{N} \rightarrow \alpha \int \ln \overline{\Phi}\left(-\frac{\sqrt{q} z}{\sigma}\right) \varphi(z) dz + \int \ln \left(2 \cosh(\sqrt{\psi z})\right)\varphi(z) dz" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cln+Z%7D%7BN%7D+%5Crightarrow+%5Calpha+%5Cint+%5Cln+%5Coverline%7B%5CPhi%7D%5Cleft%28-%5Cfrac%7B%5Csqrt%7Bq%7D+z%7D%7B%5Csigma%7D%5Cright%29+%5Cvarphi%28z%29+dz+%2B+%5Cint+%5Cln+%5Cleft%282+%5Ccosh%28%5Csqrt%7B%5Cpsi+z%7D%29%5Cright%29%5Cvarphi%28z%29+dz&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{\ln Z}{N} \rightarrow \alpha \int \ln \overline{\Phi}\left(-\frac{\sqrt{q} z}{\sigma}\right) \varphi(z) dz + \int \ln \left(2 \cosh(\sqrt{\psi z})\right)\varphi(z) dz"/></p>
<p>where <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/> is our capacity, <img alt="\frac{M}{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BM%7D%7BN%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{M}{N}"/>. It turns out that <img alt="\sigma^2 = 1-q" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma%5E2+%3D+1-q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma^2 = 1-q"/> and <img alt="c = \psi(1-q)/2," class="latex" src="https://s0.wp.com/latex.php?latex=c+%3D+%5Cpsi%281-q%29%2F2%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c = \psi(1-q)/2,"/> so the above equation only depends on two parameters, <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and <img alt="\psi." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\psi."/> It will turn out that <img alt="q, \psi" class="latex" src="https://s0.wp.com/latex.php?latex=q%2C+%5Cpsi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q, \psi"/> have relations dependent on each other based on our definitions of <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/> and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>, and these will give us a fixed point equation for <img alt="(q, \psi)," class="latex" src="https://s0.wp.com/latex.php?latex=%28q%2C+%5Cpsi%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(q, \psi),"/> which has two solutions: one near <img alt="\alpha \approx 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \approx 1"/> and one near <img alt="\alpha \approx 0.83." class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%5Capprox+0.83.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha \approx 0.83."/> It is believed that the second point is correct, meaning that the critical capacity should equal <img alt="\alpha_* = 0.83." class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%2A+%3D+0.83.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_* = 0.83."/></p>
<p><strong>1.4. Second Moment Method</strong></p>
<p>Now that we have Gardner’s formula, solving for <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> gives us an approximation for its distribution as</p>
<p><img alt="Z \sim \exp\{N\mathcal{G}(\alpha) + \mathcal{N}(0, N\varepsilon^2)\}" class="latex" src="https://s0.wp.com/latex.php?latex=Z+%5Csim+%5Cexp%5C%7BN%5Cmathcal%7BG%7D%28%5Calpha%29+%2B+%5Cmathcal%7BN%7D%280%2C+N%5Cvarepsilon%5E2%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z \sim \exp\{N\mathcal{G}(\alpha) + \mathcal{N}(0, N\varepsilon^2)\}"/></p>
<p>where <img alt="\mathcal{G}(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BG%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{G}(\alpha)"/> is Gardner’s formula, and the Gaussian noise <img alt="\mathcal{N}(0,N\varepsilon^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BN%7D%280%2CN%5Cvarepsilon%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathcal{N}(0,N\varepsilon^2)"/> arises from the Gaussian distribution of the <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/>‘s and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>‘s. Again, we are interested in the probability that <img alt="Z &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=Z+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z &gt; 0"/>, but due to the approximate nature of the above equation, we cannot work with this distribution directly, and instead can try the second moment method. However, the exponentiated Gaussian makes <img alt="\mathbb{E}[Z^2] \gg \mathbb{E}[Z]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5E2%5D+%5Cgg+%5Cmathbb%7BE%7D%5BZ%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[Z^2] \gg \mathbb{E}[Z]^2"/> and the moment method just gives <img alt="P(Z&gt;0) \ge 0" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29+%5Cge+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0) \ge 0"/>, whereas we need <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/> with high probability.</p>
<p>The fault here lies with Gaussian noise due to the <img alt="h_\mu" class="latex" src="https://s0.wp.com/latex.php?latex=h_%5Cmu&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_\mu"/>‘s and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>‘s, so it is natural to consider what happens when we condition on them, getting rid of the noise. We denote</p>
<p><img alt="\underline{h} = (h_\mu)_{\mu = 1}^M \text{ and } \underline{H} = (H_i)_{i=1}^N." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D+%3D+%28h_%5Cmu%29_%7B%5Cmu+%3D+1%7D%5EM+%5Ctext%7B+and+%7D+%5Cunderline%7BH%7D+%3D+%28H_i%29_%7Bi%3D1%7D%5EN.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h} = (h_\mu)_{\mu = 1}^M \text{ and } \underline{H} = (H_i)_{i=1}^N."/></p>
<p>Then, we can compute that</p>
<p><img alt="\frac{1}{N}\log \mathbb{E}[Z | \underline{H},\underline{h}] \to \mathcal{G}(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7BN%7D%5Clog+%5Cmathbb%7BE%7D%5BZ+%7C+%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D+%5Cto+%5Cmathcal%7BG%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{N}\log \mathbb{E}[Z | \underline{H},\underline{h}] \to \mathcal{G}(\alpha)"/></p>
<p>in probability as <img alt="N\to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=N%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N\to \infty"/>. (This is not an exact equality becacuse of the approximations made in the derivation of Gardner’s formula.) Moreover, we will have <img alt="\mathbb{E}[Z^2|\underline{H},\underline{h}] \approx \mathbb{E}[Z|\underline{H},\underline{h}]^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BZ%5E2%7C%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D+%5Capprox+%5Cmathbb%7BE%7D%5BZ%7C%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D%5D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[Z^2|\underline{H},\underline{h}] \approx \mathbb{E}[Z|\underline{H},\underline{h}]^2"/>. The second moment method gives us the desired lower bound on <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/>. However, note that these <img alt="\underline{H},\underline{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%2C%5Cunderline%7Bh%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H},\underline{h}"/> must satisfy the equations that defined them. In vector form, we have</p>
<p><img alt="\frac{1}{\sqrt{N}}G\tanh \underline{H} = \underline{h} + b_*F(\underline{h})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7BN%7D%7DG%5Ctanh+%5Cunderline%7BH%7D+%3D+%5Cunderline%7Bh%7D+%2B+b_%2AF%28%5Cunderline%7Bh%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{\sqrt{N}}G\tanh \underline{H} = \underline{h} + b_*F(\underline{h})"/><br/>
<img alt="\frac{1}{\sqrt{N}}G^T\tanh F(\underline{h}) = \underline{H} + d_*\tanh\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7BN%7D%7DG%5ET%5Ctanh+F%28%5Cunderline%7Bh%7D%29+%3D+%5Cunderline%7BH%7D+%2B+d_%2A%5Ctanh%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{\sqrt{N}}G^T\tanh F(\underline{h}) = \underline{H} + d_*\tanh\underline{H}"/></p>
<p>where <img alt="b_*" class="latex" src="https://s0.wp.com/latex.php?latex=b_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="b_*"/>, <img alt="F" class="latex" src="https://s0.wp.com/latex.php?latex=F&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="F"/>, and <img alt="d_*" class="latex" src="https://s0.wp.com/latex.php?latex=d_%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d_*"/> are constants and functions that appear in the rigorous definitions of <img alt="h_{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=h_%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_{\mu}"/> and <img alt="H_i" class="latex" src="https://s0.wp.com/latex.php?latex=H_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_i"/>.<br/>
Here arises a problem, however. We cannot solve these equations easily, and furthermore when we condition on <img alt="\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}"/> and <img alt="\underline{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}"/>, the <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>‘s that satisfy the above are not necessarily representative of matrices of standard Gaussians. Hence, simply conditioning on knowing the values of <img alt="\underline{h},\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%2C%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h},\underline{H}"/> destroys the model and will not prove the lower bound on <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/> for Gaussian disorder.</p>
<p>To solve this problem, we introduce iteration: first, initialize <img alt="\underline{h}^{(0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%280%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}^{(0)}"/> and <img alt="\underline{H}^{(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%281%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}^{(1)}"/> (initialized to specific values detailed in [6]. Then, a simplified version of each time step’s update looks like</p>
<p><img alt="\underline{h}^{(t)} = \frac{G\tanh'{\underline{H}^{(t)}}}{\sqrt{N}} - b_*F(\underline{h}^{(t - 1)})," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%28t%29%7D+%3D+%5Cfrac%7BG%5Ctanh%27%7B%5Cunderline%7BH%7D%5E%7B%28t%29%7D%7D%7D%7B%5Csqrt%7BN%7D%7D+-+b_%2AF%28%5Cunderline%7Bh%7D%5E%7B%28t+-+1%29%7D%29%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}^{(t)} = \frac{G\tanh'{\underline{H}^{(t)}}}{\sqrt{N}} - b_*F(\underline{h}^{(t - 1)}),"/><br/>
<img alt="\underline{H}^{(t + 1)} = \frac{G^TF(\underline{h}^t)}{\sqrt{N}} - d_*\tanh{\underline{H}^{(t + 1)}}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%28t+%2B+1%29%7D+%3D+%5Cfrac%7BG%5ETF%28%5Cunderline%7Bh%7D%5Et%29%7D%7B%5Csqrt%7BN%7D%7D+-+d_%2A%5Ctanh%7B%5Cunderline%7BH%7D%5E%7B%28t+%2B+1%29%7D%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}^{(t + 1)} = \frac{G^TF(\underline{h}^t)}{\sqrt{N}} - d_*\tanh{\underline{H}^{(t + 1)}}."/></p>
<p>It has been proven (see [2,4]) that <img alt="\underline{h}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h}^{(t)}"/> and <img alt="\underline{H}^{(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7BH%7D%5E%7B%28t%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{H}^{(t)}"/> converge as <img alt="t\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=t%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t\to\infty"/>, and moreover the convergent values are distributed by what looks like a Gaussian at each time step. Since this sequence of <img alt="\underline{h},\underline{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderline%7Bh%7D%2C%5Cunderline%7BH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\underline{h},\underline{H}"/> converge (at a rate that is independent of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>) and are representative of Gaussian <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>, for a special kind of truncated partition function <img alt="\tilde Z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tilde Z"/>, conditioning on these iteration values allows the second moment method to work and gives</p>
<p><img alt="\mathbb{E}[\tilde Z | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]^2 \approx \mathbb{E}[\tilde Z^2 | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Ctilde+Z+%7C+%5Cunderline%7Bh%7D%5E%7B%280%29%7D%2C%5Cunderline%7BH%7D%5E%7B%280%29%7D%2C%5Cdots%2C%5Cunderline%7Bh%7D%5E%7B%28t%29%7D%2C%5Cunderline%7BH%7D%5E%7B%28t%29%7D%5D%5E2+%5Capprox+%5Cmathbb%7BE%7D%5B%5Ctilde+Z%5E2+%7C+%5Cunderline%7Bh%7D%5E%7B%280%29%7D%2C%5Cunderline%7BH%7D%5E%7B%280%29%7D%2C%5Cdots%2C%5Cunderline%7Bh%7D%5E%7B%28t%29%7D%2C%5Cunderline%7BH%7D%5E%7B%28t%29%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{E}[\tilde Z | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]^2 \approx \mathbb{E}[\tilde Z^2 | \underline{h}^{(0)},\underline{H}^{(0)},\dots,\underline{h}^{(t)},\underline{H}^{(t)}]"/></p>
<p>which is then enough to establish the lower bound <img alt="P(Z&gt;0)" class="latex" src="https://s0.wp.com/latex.php?latex=P%28Z%3E0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P(Z&gt;0)"/> for the non-truncated partition function <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> (see [6] for details, see also [3] for closely related computations).</p>
<p><strong>2. <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAE-SAT</strong></p>
<p>This lecture also gave a brief overview of results in <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT. In the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-SAT problem, we ask whether a boolean formula in <img alt="CNF" class="latex" src="https://s0.wp.com/latex.php?latex=CNF&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="CNF"/> form, with each clause containing exactly <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> literals, has a satisfying solution. For <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT, we instead require that the satisfying solution is not uniform on any clause; equivalently, each clause must contain at least one true and at least one false value. Finally, we will restrict our set of possible boolean formulae to those for which every variable is contained in exactly <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> clauses; we call this model <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-regular <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT. We briefly outline the critical capacities of clauses where the solution space changes. For more background on the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT probem and its variants, see [10].</p>
<p>As with the perceptron model, we are concerned with the expected number of solutions <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> of this system where the <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-regular formula is chosen uniformly at random. It turns out that for any fixed <img alt="\alpha=\frac{d}{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3D%5Cfrac%7Bd%7D%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha=\frac{d}{k}"/>, the expected proportion of satisfiable solutions as <img alt="n\to\infty" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Cto%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n\to\infty"/> converges to some <img alt="f(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\alpha)"/>. More on this, the model in general and the critical <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/> constants mentioned below can be found in [15]. Via an application of Markov’s inequality and Jensen’s Inequality for the convex function <img alt="x^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^n"/>, <img alt="f(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\alpha)"/> may be bounded above by <img alt="f^{RS}(\alpha)=(\mathbb{E} Z)^{\frac{1}{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7BRS%7D%28%5Calpha%29%3D%28%5Cmathbb%7BE%7D+Z%29%5E%7B%5Cfrac%7B1%7D%7Bn%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f^{RS}(\alpha)=(\mathbb{E} Z)^{\frac{1}{n}}"/>, shown graphically below.</p>
<p><img alt="CS 229 Pic 2" class="alignnone size-full wp-image-6352" src="https://windowsontheory.files.wordpress.com/2018/12/CS-229-Pic-2.png?w=600"/><br/>
The diagram also shows the nature of the expected assortment of the solutions of a random <img alt="d-" class="latex" src="https://s0.wp.com/latex.php?latex=d-&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d-"/>regular <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-NAESAT for ranges of values of <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha"/>. Namely, physics analysis suggests the following:</p>
<ul>
<li>For <img alt="0&lt;\alpha&lt;\alpha_d" class="latex" src="https://s0.wp.com/latex.php?latex=0%3C%5Calpha%3C%5Calpha_d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0&lt;\alpha&lt;\alpha_d"/>, w.h.p. the solutions are concentrated in a single large cluster.</li>
<li>For <img alt="\alpha_d&lt;\alpha&lt;\alpha_{\text{cond}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_d%3C%5Calpha%3C%5Calpha_%7B%5Ctext%7Bcond%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_d&lt;\alpha&lt;\alpha_{\text{cond}}"/>, w.h.p. the solutions are distributed among a large number of clusters.</li>
<li>For <img alt="\alpha_{\text{cond}}&lt;\alpha_{\text{sat}}&lt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha_%7B%5Ctext%7Bcond%7D%7D%3C%5Calpha_%7B%5Ctext%7Bsat%7D%7D%3C0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha_{\text{cond}}&lt;\alpha_{\text{sat}}&lt;0"/>, the function <img alt="f(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\alpha)"/> breaks away from <img alt="f^{RS}(\alpha)" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7BRS%7D%28%5Calpha%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f^{RS}(\alpha)"/>, and w.h.p. the solutions are concentrated in a small number of clusters.</li>
<li>For <img alt="\alpha&gt;\alpha_{\text{sat}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3E%5Calpha_%7B%5Ctext%7Bsat%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha&gt;\alpha_{\text{sat}}"/>, w.h.p. there are no satisfiable solutions.</li>
</ul>
<p>One final note for this model: the solutions to satisfiability problems tend to be more clumpy than in the perceptron model, so correlation decay methods won’t immediately work. See [15] for how this is handled.</p>
<p><strong>References</strong></p>
<ol>
<li>N. Bansal. Constructive algorithms for discrepancy minimization. In <em>Proc. FOCS</em> 2010, pages 3–10.</li>
<li>M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. <em>IEEE Trans. Inform. Theory</em>, 57(2):764-785, 2011.</li>
<li>Bolthausen, E., 2018. A Morita type proof of the replica-symmetric formula for SK. arXiv preprint arXiv:1809.07972.</li>
<li>E. Bolthausen. An iterative construction of solutions of the TAP equations for the Sherrington-Kirkpatrick model. <em>Commun. Math. Phys.</em>, 325(1):333-366, 2014.</li>
<li>T. Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. <em>IEEE Trans. Electron. Comput.</em> 14(3):326-334.</li>
<li>J. Ding and N. Sun. Capacity lower bound for the Ising perceptron. <a href="https://arxiv.org/pdf/1809.07742.pdf" rel="nofollow">https://arxiv.org/pdf/1809.07742.pdf</a></li>
<li>E. Gardner and B. Derrida. Optimal storage properties of neural network models. <em>J. Phys. A.</em>, 21(1): 271–284, 1988.</li>
<li>J. H. Kim and J. R. Roche. Covering cubes by random half cubes, with applications to binary neural networks. <em>J. Comput. Syst. Sci.</em>, 56(2):223–252, 1998</li>
<li>W. Krauth and M. Mézard. Storage capacity of memory networks with binary couplings. <em>J. Phy.</em> 50(20): 3057-3066, 1989.</li>
<li>F. Krzakala et al. “Gibbs states and the set of solutions of random constraint satisfaction problems.” <i>Proc. Natl. Acad. Sci.</i> 104.25 (2007): 10318-10323.</li>
<li>S. Lovett and R. Meka. Constructive discrepancy minimization by walking on the edges. <em>SIAM J. Comput.</em>, 44(5):1573-1582.</li>
<li>M. Mézard. The space of interactions in neural networks: Gardner’s computation with the cavity method. <em>J. Phys. A.</em>, 22(12):2181, 1989</li>
<li>M. Mézard and G. Parisi and M. Virasoro. SK Model: The Replica Solution without Replicas. <em>Europhys. Lett.</em>, 1(2): 77-82, 1986.</li>
<li>M. Shcherbina and B. Tirozzi. Rigorous solution of the Gardner problem. <em>Commun. Math. Phys.</em>, 234(3):383-422, 2003.</li>
<li>A. Sly and N. Sun and Y. Zhang. The Number of Solutions for Random Regular NAE-SAT. In <em>Proc. FOCS</em> 2016, pages 724-731.</li>
<li>J. Spencer. Six standard deviations suffice. <em>Trans. Amer. Math. Soc.</em> 289 (1985), 679-706</li>
<li>M. Talagrand. Intersecting random half cubes. <em>Random Struct. Algor.</em>, 15(3-4):436–449, 1999.</li>
</ol></div>
    </content>
    <updated>2018-12-14T04:44:40Z</updated>
    <published>2018-12-14T04:44:40Z</published>
    <category term="physics"/>
    <author>
      <name>degeneratetriangle</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-19T17:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2018/12/13/postdoc-at-national-university-of-singapore-nus-apply-by-march-15-2019/</id>
    <link href="https://cstheory-jobs.org/2018/12/13/postdoc-at-national-university-of-singapore-nus-apply-by-march-15-2019/" rel="alternate" type="text/html"/>
    <title>Postdoc at National University of Singapore (NUS) (apply by March 15, 2019)</title>
    <summary>One post-doctoral research fellow fellow position is available at the School of Computing, National University of Singapore, Singapore for the project: Beyond NP Revolution. Interested candidates should send their resume and statement about their research to Kuldeep Meel (meel@comp.nus.edu.sg). Website: https://www.comp.nus.edu.sg/~meel/theory-postdoc.html Email: meel@comp.nus.edu.sg
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One post-doctoral research fellow fellow position is available at the School of Computing, National University of Singapore, Singapore for the project: Beyond NP Revolution.</p>
<p>Interested candidates should send their resume and statement about their research to Kuldeep Meel (meel@comp.nus.edu.sg).</p>
<p>Website: <a href="https://www.comp.nus.edu.sg/~meel/theory-postdoc.html">https://www.comp.nus.edu.sg/~meel/theory-postdoc.html</a><br/>
Email: meel@comp.nus.edu.sg</p></div>
    </content>
    <updated>2018-12-13T07:51:34Z</updated>
    <published>2018-12-13T07:51:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2018-12-19T17:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2018/211</id>
    <link href="https://eccc.weizmann.ac.il/report/2018/211" rel="alternate" type="text/html"/>
    <title>TR18-211 |  Parametric Shortest Paths in Planar Graphs | 

	Kshitij Gajjar, 

	Jaikumar  Radhakrishnan</title>
    <summary>We construct a family of planar graphs $(G_n: n\geq 4)$, where $G_n$ has $n$ vertices including a source vertex $s$ and a sink vertex $t$, and edge weights that change linearly with a parameter $\lambda$ such that, as $\lambda$ increases, the cost of the shortest path from $s$ to $t$ has $n^{\Omega(\log n)}$ break points. This shows that lower bounds obtained earlier by Carstensen (1983) and Mulmuley &amp; Shah (2000) for general graphs also hold for planar graphs. A conjecture of Nikolova (2009) states that the number of break points in $n$-vertex planar graphs is bounded by a polynomial in $n$; our result refutes this conjecture.

Gusfield (1980) and Dean (2009) showed that the number of break points for an $n$-vertex graph is $n^{\log n + O(1)}$ assuming linear edge weights; we show that if the edge weights are allowed to vary as a polynomial of degree at most $d$, then the number of break points is $n^{\log n + O(\alpha(n)^d)}$, where $\alpha(n)$ is the slowly growing inverse Ackermann function. This upper bound arises from Davenport-Schinzel sequences.
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-12T15:10:30Z</updated>
    <published>2018-12-12T15:10:30Z</published>
    <source>
      <id>https://example.com/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://example.com/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2018-12-19T17:22:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=16618</id>
    <link href="https://gilkalai.wordpress.com/2018/12/12/nima-anari-kuikui-liu-shayan-oveis-gharan-and-cynthia-vinzant-solved-the-mihail-vazirani-conjecture/" rel="alternate" type="text/html"/>
    <title>Nima Anari, Kuikui Liu, Shayan Oveis Gharan, and Cynthia Vinzant Solved the Mihail-Vazirani Conjecture for Matroids!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">  Milena Mihail and Umesh Vazirani I thank Nati Linial, Dan Spielman and Karim Adiprasito for telling me about the news. The Mihail-Vazirani conjecture for matroids and the Feder-Mihail’s theorem Consider a collection of vectors. A basis is a subset … <a href="https://gilkalai.wordpress.com/2018/12/12/nima-anari-kuikui-liu-shayan-oveis-gharan-and-cynthia-vinzant-solved-the-mihail-vazirani-conjecture/">Continue reading <span class="meta-nav">→</span></a></div>
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> </p>
<p><strong><span style="color: #ff0000;"><img alt="Milena+Umesh" class="alignnone size-full wp-image-16635" src="https://gilkalai.files.wordpress.com/2018/12/milenaumesh.png?w=640"/><br/>
Milena Mihail and Umesh Vazirani</span></strong></p>
<p>I thank Nati Linial, Dan Spielman and Karim Adiprasito for telling me about the news.</p>
<h2>The Mihail-Vazirani conjecture for matroids and the Feder-Mihail’s theorem</h2>
<p>Consider a collection <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/> of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> vectors. A basis is a subset of <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/> of linearly independent vectors that span the subspace spanned by <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/>.  We can define a graph whose vertices are all bases and two bases are adjacent if their symmetric difference has two elements. Mihail and Vazirani conjectured that for every set <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> of vertices in this graph the number of edges between <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> to its complement <img alt="\bar Y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbar+Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\bar Y"/> is at least <img alt="\min (|Y| |,\bar Y|)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin+%28%7CY%7C+%7C%2C%5Cbar+Y%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\min (|Y| |,\bar Y|)"/>.</p>
<p>If <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/> consists of the elements of the standard basis in <img alt="\mathbb R^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb R^d"/> and their negatives then the graph we obtain is the graph of the discrete <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> dimensional discrete cube and the assertion of the Mihail-Vazirani conjecture is well known.</p>
<p>The Mihail-Vazirani conjecture was formulated (and have now been proved) for arbitrary matroids. Think about a matroid as a collection <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> of subsets (the independent sets of the matroid) of some ground set <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/> that closed under taking subsets (hence a simplicial complex) with the following property: For every <img alt="Y \subset X" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%5Csubset+X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y \subset X"/>, the induced simplicial complex on <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> denoted by <img alt="K(Y)" class="latex" src="https://s0.wp.com/latex.php?latex=K%28Y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K(Y)"/>  is pure. In other words, for every <img alt="Y \subset X" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%5Csubset+X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y \subset X"/>, all maximal independent subsets of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> have the same cardinality.</p>
<p>In a pioneering 1992 paper Tomás Feder and Milena Mihail proved the conjecture for balanced matroids.</p>
<h2>Mihail and Vazirani conjecture for 0-1 polytopes.</h2>
<p>An even more general conjecture by Mihail and Vazirani is still open. It asserts that for every 0-1 polytope, for every set <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> of vertices, the number of edges between <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> to its complement <img alt="\bar Y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbar+Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\bar Y"/> is at least <img alt="\min (|Y|, |\bar Y|)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin+%28%7CY%7C%2C+%7C%5Cbar+Y%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\min (|Y|, |\bar Y|)"/>.</p>
<h2>The new breakthrough</h2>
<p>Here is the link to the paper.</p>
<p class="title mathjax"><a href="https://arxiv.org/abs/1811.01816">Log-Concave Polynomials II: High-Dimensional Walks and an FPRAS for Counting Bases of a Matroid</a> by Nima Anari, Kuikui Liu, Shayan Oveis Gharan, and Cynthia Vinzant</p>
<p>Let me mention that there was simultaneously another paper by a subset of the authors on related questions, another paper by Brändén and Huh who independently proved the strong log-concavity of several of the polynomials that appear in the ALOV  paper, and there will be several forthcoming papers by these two groups.</p>
<p><strong>Here is the abstract:</strong> We design an FPRAS to count the number of bases of any matroid given by an independent set oracle, and to estimate the partition function of the random cluster model of any matroid in the regime where <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mn" id="MathJax-Span-3">0</span><span class="mo" id="MathJax-Span-4">&lt;</span><span class="mi" id="MathJax-Span-5">q</span><span class="mo" id="MathJax-Span-6">&lt;</span><span class="mn" id="MathJax-Span-7">1</span></span></span></span>. Consequently, we can sample random spanning forests in a graph and (approximately) compute the reliability polynomial of any matroid. We also prove the thirty year old conjecture of Mihail and Vazirani that the bases exchange graph of any matroid has expansion at least 1. One of our key observations is a close connection between pure simplicial complexes and multiaffine homogeneous polynomials. Specifically, if <span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-8"><span class="mrow" id="MathJax-Span-9"><span class="mi" id="MathJax-Span-10">X</span></span></span></span> is a pure simplicial complex with positive weights on its maximal faces, we can associate with <span class="MathJax" id="MathJax-Element-3-Frame"><span class="math" id="MathJax-Span-11"><span class="mrow" id="MathJax-Span-12"><span class="mi" id="MathJax-Span-13">X</span></span></span></span> a multiaffine homogeneous polynomial <img alt="p_X" class="latex" src="https://s0.wp.com/latex.php?latex=p_X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_X"/> such that the eigenvalues of the localized random walks on <span class="MathJax" id="MathJax-Element-5-Frame"><span class="math" id="MathJax-Span-21"><span class="mrow" id="MathJax-Span-22"><span class="mi" id="MathJax-Span-23">X</span></span></span></span> correspond to the eigenvalues of the Hessian of derivatives of <img alt="p_X" class="latex" src="https://s0.wp.com/latex.php?latex=p_X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_X"/>.</p>
<h2>Spectral negative dependence and Hodge-Riemann</h2>
<p>A key paragraph regarding the method is the following:</p>
<p>“Our approach has a close connection to the original plan of Feder and Mihail (1992) who used the negative correlation property of balanced matroids to show that the bases exchange walk mixes rapidly. Unfortunately, most interesting matroids do not satisfy negative correlation. But it was observed [AKH18; HW16; AOV18] that all matroids satisfy a spectral negative dependence property.” AKH18 (a typo, it should be AHK18) is the solution for the Rota-Heron conjecture by Adiprasito, Huh and Katz that relies on the Hodge-Riemann property for matroids that we mentioned <a href="https://gilkalai.wordpress.com/2015/08/14/updates-and-plans-iii/">in this post</a>. (I still feel in debt for a more detailed blog post about Adiprasito, Huh and Katz’ breakthrough!).”</p>
<p>I think that high dimensional expanders and random walks on them also plays a role in the new development.</p>
<p> </p></div>
    </content>
    <updated>2018-12-11T21:04:28Z</updated>
    <published>2018-12-11T21:04:28Z</published>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Updates"/>
    <category term="Cynthia Vinzant"/>
    <category term="Kuikui Liu"/>
    <category term="Milena Mihail"/>
    <category term="Nima Anari"/>
    <category term="Shayan Oveis Gharan"/>
    <category term="Tom&#xE1;s Feder"/>
    <category term="Vijay Vazirani"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2018-12-19T17:21:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4010</id>
    <link href="https://www.scottaaronson.com/blog/?p=4010" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4010#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4010" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The NP genie</title>
    <summary xml:lang="en-US">Hi from the Q2B conference! Every nerd has surely considered the scenario where an all-knowing genie—or an enlightened guru, or a superintelligent AI, or God—appears and offers to answer any question of your choice.  (Possibly subject to restrictions on the length or complexity of the question, to prevent glomming together every imaginable question.)  What do […]
      <div class="commentbar">
        <p/>
        <span class="commentbutton" href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4010"/>
        <a href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4010">
          <img class="commenticon" src="/images/feed-icon.png"/> Subscribe to comments
        </a>  | 
        <a href="https://www.scottaaronson.com/blog/?p=4010#comments">
          <img class="commenticon" src="/images/post-icon.png"/> Post a comment
        </a>
      </div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Hi from the <a href="https://q2b2018.qcware.com/">Q2B</a> conference!</p>
<p>Every nerd has surely considered the scenario where an all-knowing genie—or an enlightened guru, or a superintelligent AI, or God—appears and offers to answer any question of your choice.  (Possibly subject to restrictions on the length or complexity of the question, to prevent glomming together every imaginable question.)  What do you ask?</p>
<p>(Standard joke: “What question <em>should</em> I ask, oh wise master, and what is its answer?”  “The question you should ask me is the one you just asked, and its answer is the one I am giving.”)</p>
<p>The other day, it occurred to me that theoretical computer science offers a systematic way to generate interesting variations on the genie scenario, which have been contemplated less—variations where the genie is no longer omniscient, but “merely” more scient than any entity that humankind has ever seen.  One simple example, which I gather is often discussed in the AI-risk and rationality communities, is an oracle for the halting problem: what computer program can you write, such that knowing whether it halts would provide the most useful information to civilization?  Can you solve global warming with such an oracle?  Cure cancer?</p>
<p>But there are many other examples.  Here’s one: suppose what pops out of your lamp is a genie for <em>NP</em> questions.  Here I don’t mean NP in the technical sense (that would just be a pared-down version of the halting genie discussed above), but in the human sense.  The genie can only answer questions by pointing you to ordinary evidence that, once you know where to find it, makes the answer to the question clear to every competent person who examines the evidence, with no further need to trust the genie.  Or, of course, the genie could <em>fail</em> to provide such evidence, which itself provides the valuable information that there’s no such evidence out there.</p>
<p>More-or-less equivalently (because of binary search), the genie could do what my parents used to do when my brother and I searched the house for Hanukkah presents, and give us “hotter” or “colder” hints as we searched for the evidence ourselves.</p>
<p>To make things concrete, let’s assume that the NP genie will only provide answers of 1000 characters or fewer, in plain English text with no fancy encodings.  Here are the candidates for NP questions that I came up with after about 20 seconds of contemplation:</p>
<ul>
<li>Which pieces of physics beyond the Standard Model and general relativity can be experimentally confirmed with the technology of 2018? What are the experiments we need to do?</li>
<li>What’s the current location of the Ark of the Covenant, or its remains, if any still exist?  (Similar: where can we dig to find physical records, if any exist, pertaining to the Exodus from Egypt, or to Jesus of Nazareth?)</li>
<li>What’s a sketch of a resolution of P vs. NP, from which experts would stand a good chance of filling in the details?  (Similar for other any famous unsolved math problem.)</li>
<li>Where, if anywhere, can we point radio telescopes to get irrefutable evidence for the existence of extraterrestrial life?</li>
<li>What happened to Malaysia Flight 370, and where are the remains by which it could be verified?  (Similar for Amelia Earhart.)</li>
<li>Where, if anywhere, can we find intact DNA of non-avian dinosaurs?</li>
</ul>
<p>Which NP questions would <em>you</em> ask the genie?  And what other complexity-theoretic genies would be interesting to consider?  (I thought briefly about a <a href="https://en.wikipedia.org/wiki/Parity_P">⊕P</a> genie, but I’m guessing that the yearning to know whether the number of sand grains in the Sahara is even or odd is limited.)</p>
<hr/>
<p><font color="red"><b>Update:</b></font> I just read Lenny Susskind’s <a href="https://blog.ycombinator.com/leonard-susskind-on-richard-feynman-the-holographic-principle-and-unanswered-questions-in-physics/">Y Combinator interview</a>, and found it delightful—pure Lenny, and covering tons of ground that should interest anyone who reads this blog.



</p><p/></div>
    </content>
    <updated>2018-12-11T19:44:43Z</updated>
    <published>2018-12-11T19:44:43Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2018-12-18T19:57:20Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2018/12/10/faculty-at-university-of-california-at-riverside-apply-by-january-1-2019/</id>
    <link href="https://cstheory-jobs.org/2018/12/10/faculty-at-university-of-california-at-riverside-apply-by-january-1-2019/" rel="alternate" type="text/html"/>
    <title>Faculty at University of California at Riverside (apply by January 1, 2019)</title>
    <summary>The Department of Computer Science at the University of California, Riverside (UCR) invites applications for two tenure-track/tenured faculty positions beginning the 2019/20 academic year. Algorithmics is among the areas of interest. Other areas include machine learning, artificial intelligence, visualization, human-computer interaction, and virtual reality. Website: https://www1.cs.ucr.edu Email: marek@cs.ucr.edu
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of California, Riverside (UCR) invites applications for two tenure-track/tenured faculty positions beginning the 2019/20 academic year. Algorithmics is among the areas of interest. Other areas include machine learning, artificial intelligence, visualization, human-computer interaction, and virtual reality.</p>
<p>Website: <a href="https://www1.cs.ucr.edu">https://www1.cs.ucr.edu</a><br/>
Email: marek@cs.ucr.edu</p></div>
    </content>
    <updated>2018-12-10T22:09:58Z</updated>
    <published>2018-12-10T22:09:58Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2018-12-19T17:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15499</id>
    <link href="https://rjlipton.wordpress.com/2018/12/09/transposing/" rel="alternate" type="text/html"/>
    <title>Transposing</title>
    <summary>On the arithmetic complexity of the matrix transpose [ KKB ] Michael Kaminski, David Kirkpatrick, and Nader Bshouty are complexity theorists who together and separately have proved many wonderful theorems. They wrote an interesting paper recently—well not quite—in 1988 about the transpose operation. Today we want to discuss an alternative proof of the main result […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>On the arithmetic complexity of the matrix transpose</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2018/12/09/transposing/kkb/" rel="attachment wp-att-15501"><img alt="" class="alignright wp-image-15501" src="https://rjlipton.files.wordpress.com/2018/12/kkb.png?w=250" width="250"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ KKB ]</font></td>
</tr>
</tbody>
</table>
<p>
Michael Kaminski, David Kirkpatrick, and Nader Bshouty are complexity theorists who together and separately have proved many wonderful theorems. They wrote an interesting <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.3626&amp;rep=rep1&amp;type=pdf">paper</a> recently—well not quite—in 1988 about the transpose operation.</p>
<p>
Today we want to discuss an alternative proof of the main result of that paper.<br/>
<span id="more-15499"/></p>
<p>
</p><p/><h2> The Transpose </h2><p/>
<p/><p>
The operation that maps a <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \times n}"/> matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> to its transpose <img alt="{A^{T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7BT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^{T}}"/> is quite important in many aspects of linear algebra. Recall that the transpose is defined by 	</p>
<p align="center"><img alt="\displaystyle  A^{T}(i,j) = A(j,i) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A%5E%7BT%7D%28i%2Cj%29+%3D+A%28j%2Ci%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A^{T}(i,j) = A(j,i) "/></p>
<p>for all indices <img alt="{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i,j}"/>. A non-trivial issue arises already in proving this: </p>
<blockquote><p><b> </b> <em> <i>If <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> has an inverse then so does <img alt="{A^{T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7BT%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A^{T}}"/></i>. </em>
</p></blockquote>
<p>There are many proofs of this basic fact about the transpose, but it is not simple to prove it from first principles. For example look at William Wardlaw’s proof <a href="https://www.maa.org/sites/default/files/3004418139737.pdf.bannered.pdf">here</a>. Or <a href="http://www.doc.ic.ac.uk/~ae/papers/lecture04.pdf">here</a> for another.</p>
<p>
</p><p/><h2> A Theorem </h2><p/>
<p/><p>
The paper of Kaminski, Kirkpatrick, and Bshouty (KKB) came up the other day, while I visited the computer science department at University of Buffalo. Atri Rudra told about some of his recent work on various complexity issues around matrix computations. One was an interesting question about the transpose operation on matrices. The result is the following: </p>
<blockquote><p><b>Theorem 1</b> <em> If the arithmetic complexity of <img alt="{x \rightarrow Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Crightarrow+Ax%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x \rightarrow Ax}"/> is <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S}"/> then the arithmetic complexity of <img alt="{y \rightarrow A^{T}y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By+%5Crightarrow+A%5E%7BT%7Dy%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y \rightarrow A^{T}y}"/> is at most <img alt="{O(S + n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28S+%2B+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(S + n)}"/>. </em>
</p></blockquote>
<p>KKB had a nice proof of this theorem over forty years ago. Indeed they can get a precise expression for the complexity that is tighter than the above statement. Their proof is a careful examination of the structure of any arithmetic circuit that computes <img alt="{Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Ax}"/>. Essentially they show one can in a sense “run the computation backwards.” </p>
<p>
</p><p/><h2> Another Proof </h2><p/>
<p/><p>
Anyway in my discussion with Atri, the other day, he described another proof of this basic theorem. He said consider the function that maps <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> to <img alt="{Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Ax}"/> for a fixed matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>. Suppose this linear map has arithmetic complexity <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>: that is the minimum number of arithmetic operations that are needed to compute <img alt="{Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Ax}"/> is <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>. Note, <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> can vary greatly with the structure of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>. Even for nonsingular matrices the complexity can vary quite a bit: for a random matrix is likely to be order <img alt="{n^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{2}}"/>; for a Fourier transform matrix it is order <img alt="{n\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n\log n}"/>.</p>
<p>
He said it is known that the arithmetic complexity of <img alt="{Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Ax}"/> and <img alt="{A^{T}x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7BT%7Dx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^{T}x}"/> are about the same. But proving this, while not hard, requires some care. Atri told me about a quite neat argument that proves it. Further, the proof is “two-lines” as Atri said:</p>
<p>
<em>Proof:</em>  Consider the function <img alt="{f(x)=y^{T}Ax}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%3Dy%5E%7BT%7DAx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)=y^{T}Ax}"/> as a function of <img alt="{x=(x_{1},\dots,x_{n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%3D%28x_%7B1%7D%2C%5Cdots%2Cx_%7Bn%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x=(x_{1},\dots,x_{n})}"/>. The gradient <img alt="{ \nabla f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Cnabla+f%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ \nabla f(x)}"/> is equal to all the partial derivatives of <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/>. This means that it allows us to compute <img alt="{y^{T}A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5E%7BT%7DA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y^{T}A}"/>, since the function is linear in each variable <img alt="{x_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{i}}"/>. The famous <a href="https://core.ac.uk/download/pdf/82480031.pdf">Derivative</a> <a href="https://rjlipton.wordpress.com/2010/03/27/fast-matrix-products-and-other-amazing-results/">Lemma</a> of Walter Baur and Volker Strassen shows that a single arithmetical circuit of size order <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> plus the complexity of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> can compute <img alt="{\nabla f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+f%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\nabla f}"/>. It follows that we can compute all the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> partial derivatives in <img alt="{O(S+n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28S%2Bn%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(S+n)}"/> arithmetic steps. But for our function <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/> this is equal to <img alt="{y^{T}A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5E%7BT%7DA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y^{T}A}"/>. Transposing the resulting vector gives <img alt="{A^{T}y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%7BT%7Dy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^{T}y}"/> in the required number of steps. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I like this proof quite a bit. Can this argument be used to prove the basic fact about inverses of a matrix and its transpose? </p>
<p/></font></font></div>
    </content>
    <updated>2018-12-10T04:00:02Z</updated>
    <published>2018-12-10T04:00:02Z</published>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="arithmetic complexity"/>
    <category term="matrix transpose"/>
    <category term="Strassen"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2018-12-19T17:21:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2018/210</id>
    <link href="https://eccc.weizmann.ac.il/report/2018/210" rel="alternate" type="text/html"/>
    <title>TR18-210 |  On Closest Pair in Euclidean Metric: Monochromatic is as Hard as Bichromatic | 

	Karthik  C. S., 

	Pasin Manurangsi</title>
    <summary>Given a set of $n$ points in $\mathbb R^d$, the (monochromatic) Closest Pair problem asks to find a pair of distinct points in the set that are closest in the $\ell_p$-metric. Closest Pair is a fundamental problem in Computational Geometry and understanding its fine-grained complexity in the Euclidean metric when $d=\omega(\log n)$ was raised as an open question in recent works (Abboud-Rubinstein-Williams [FOCS'17], Williams [SODA'18], David-Karthik-Laekhanukit [SoCG'18]).

In this paper, we show that for every $p\in\mathbb R_{\ge 1}\cup\{0\}$, under the Strong Exponential Time Hypothesis (SETH), for every  $\varepsilon&gt;0$, the following holds:

$\bullet$  No algorithm running in time $O(n^{2-\varepsilon})$ can solve the Closest Pair problem in $d=(\log n)^{\Omega_{\varepsilon}(1)}$ dimensions in the $\ell_p$-metric.

$\bullet$  There exists $\delta = \delta(\varepsilon)&gt;0$ and $c = c(\varepsilon)\ge 1$ such that no algorithm running in time $O(n^{1.5-\varepsilon})$ can approximate Closest Pair problem to a factor of $(1+\delta)$ in $d\ge c\log n$ dimensions in the $\ell_p$-metric.


In particular, our first result is shown by establishing the computational equivalence of the bichromatic Closest Pair problem and the (monochromatic) Closest Pair problem (up to $n^{\varepsilon}$ factor in the running time) for $d=(\log n)^{\Omega_\varepsilon(1)}$ dimensions.

Additionally, under SETH, we rule out nearly-polynomial  factor approximation algorithms  running in subquadratic time for the (monochromatic) Maximum Inner Product problem where we are given a set of $n$ points in $n^{o(1)}$-dimensional Euclidean space and are required to find a pair of distinct points in the set that maximize the inner product. 


At the heart of all our proofs is the construction of a dense bipartite graph with low contact dimension, i.e., we construct a balanced bipartite graph on $n$ vertices with $n^{2-\varepsilon}$ edges whose vertices can be realized as points in a $(\log n)^{\Omega_\varepsilon(1)}$-dimensional Euclidean space  such that every pair of vertices which have an edge in the graph are at distance exactly 1 and every other pair of vertices are at distance greater than 1. This graph construction is inspired by the construction of locally dense codes introduced by Dumer-Miccancio-Sudan [IEEE Trans. Inf. Theory'03].
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-10T00:31:47Z</updated>
    <published>2018-12-10T00:31:47Z</published>
    <source>
      <id>https://example.com/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://example.com/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2018-12-19T17:22:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1051</id>
    <link href="https://ptreview.sublinear.info/?p=1051" rel="alternate" type="text/html"/>
    <title>News for November 2018</title>
    <summary>Alas! Sorry for the delay, but November was a good month with five seven papers. We missed a few papers from September, so please look at the updated post for September as well. (Update: Thanks to Omri Ben-Eliezer and an anonymous poster for pointing out two missed papers.) From Local to Robust Testing via Agreement […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Alas! Sorry for the delay, but November was a good month with <del datetime="2018-12-09T20:08:34+00:00">five</del> seven papers. We missed a few papers from September, so please look at the updated post for September as well. <em>(Update: Thanks to Omri Ben-Eliezer and an anonymous poster for pointing out two missed papers.)</em></p>
<p><strong>From Local to Robust Testing via Agreement Testing</strong>, by Irit Dinur, Tali Kaufman, and Noga Ron-Zewi (<a href="https://eccc.weizmann.ac.il/report/2018/198/">ECCC</a>). Consider a code, which is simply a subset of \(C \subseteq \mathbb{F}^n_q\). (Typically, we think of linear codes, which form a linear subspace.) The study of locally testable codes is a rich area. Consider an input \(x \in \mathbb{F}^n_q\). A local tester for code/property \(C\) queries, according to some distribution, a subset \(K\) of \(Q\) coordinates in \(x\), and rejects if the “view” \(x|_K\) is inconsistent with any codeword. The rejection probability should be proportional to the distance of \(x\) to \(C\), denoted \(dist(x,C)\). A <em>robust</em> tester demands the average distance of \(x|_K\) to the corresponding property \(C|_K\) must be proportional to \(dist(x,C)\). This is a much stronger demand than that of just local testing. The main result is local testability implies robust testability for the special class of lifted codes. The proof goes via <em>agreement testing</em>, where an algorithm gets access to some fragments of \(x\), and must decide if these fragments are consistent with a codeword.</p>
<p><strong>Testing local properties of arrays</strong>, by Omri Ben-Eliezer (<a href="https://eccc.weizmann.ac.il/report/2018/196/">ECCC</a>). Consider a function \(f:[n]^d \to \Sigma\), where \(\Sigma\) is finite. Now, suppose we define a property \(\mathcal{P}\) with respect to a set of forbidden \([k]^d\) (consecutive) subarrays. Such a property is called \(k\)-local. The inspiration for this work is a line of research on testing of image properties. Note that for \(k=2,3\), this notion subsumes a number of properties, such as monotonicity, \(c\)-Lipschitz, submodularity, convexity, etc. The main result is a one-sided, non-adaptive tester for all such properties. Ignoring \(\epsilon, k\) dependencies, the query complexity is \(O(\log n)\) for \(d=1\) and \(O(n^{d-1})\) for \(d &gt; 1\). Remarkably, there is no dependence on the alphabet size \(|\Sigma|\). Moreover, the bound above is optimal for one-sided, non-adaptive testers. The basic idea to pick blocks of various sizes, and query all points on the boundary. Then, the tester can determine (by sort of brute force search) if this is consistent with some function in \(\mathcal{P}\). The key is to prove that for a function that is far from \(\mathcal{P}\), there are many blocks that cannot be consistent with \(\mathcal{P}\).</p>
<p><strong>Erasures vs. Errors in Local Decoding and Property Testing</strong>, by Sofya Raskhodnikova, Noga Ron-Zewi and Nithin Varma  (<a href="https://eccc.weizmann.ac.il/report/2018/195/">ECCC</a>). The main theme of this paper is the notion of <em>erasure-resilient testing</em>. Consider property \(\mathcal{P}\). Suppose \(\alpha\)-fraction of the coordinates of input \(x\) is hidden. Thus, queries to these coordinates of \(x\) return nothing. Our aim is to accept if there is some “filling in” of the hidden coordinates that make \(x\) satisfy \(\mathcal{P}\), and reject if for all fillings, \(x\) remains from from \(\mathcal{P}\). If \(\alpha = 0\), this is standard property testing. Suppose there exists an \((\alpha, \alpha+\epsilon)\)-tolerant tester for \(\mathcal{P}\). (I’m fudging factors a bit here for readability.) We get an \(\epsilon\)-tester with \(\alpha\)-fraction of erasures, by simply filling \(x\) in arbitrarily. So this model is in between vanilla and tolerant testing. The main result is a separation. There is a property that is constant-query testable under erasures, but requires \(n^{\Omega(1)}\) queries to tolerant test (with the above parameters). One of the main technical results is a list decoder for the Hadamard code that can tolerate erasures of any \(\alpha \lt 1\).</p>
<p><strong>Domain Reduction for Monotonicity Testing: A o(d) Tester for Boolean Functions on Hypergrids</strong>, by Hadley Black, Deeparnab Chakrabarty, and C. Seshadhri  (<a href="https://eccc.weizmann.ac.il/report/2018/187/">ECCC</a>). Consider monotonicity testing of Boolean functions of the hypergrid, \(f:[n]^d \to \{0,1\}\). It is known that there are \(O(d)\) testers with complexity independent of \(n\), while for \(n=2\), there are \(o(d)\) testers. The main result is a tester with <em>both</em> these properties. The paper gives an \(O(d^{5/6})\) tester for such functions. The main technical ingredient is a domain reduction theorem. Consider restrictions of \(f\) to a uniform random \([poly(d\epsilon^{-1})]^d\) hypergrid. The paper proves that if \(f\) is \(\epsilon\)-far from monotone, then (with high probability) so is the restriction. Thus, for the purposes of monotonicity testing, one can simply assume that \(n = poly(d)\). This gives a black-box method to get testers independent of \(n\). </p>
<p><strong>On the Testability of Graph Partition Properties</strong>, by Yonatan Nakar and Dana Ron (<a href="https://eccc.weizmann.ac.il/report/2018/185/">ECCC</a>). Consider property testing on dense graphs. The seminar result of <a href="http://www.wisdom.weizmann.ac.il/~oded/PDF/ggr-jacm.pdf">Goldreich-Goldwasser-Ron</a> studies graph partitioning properties. Such a property is defined as follows. Fix constant \(k\). We wish to partition the vertices of graph \(G\) into \(V_1, V_2, \ldots, V_k\). For each \(i\), \(|V_i|/n \in [\rho^L_i, \rho^U_i]\). Furthermore, for each \(i, j\), the number of edges between \(V_i, V_j\) must be in \([\rho^L_{i,j}n^2, \rho^U_{i,j}n^2]\). (All the \(\rho\)s are parameters of the property.) This subsumes properties like \(k\)-colorability, and the classic result is a \(poly(1/\epsilon)\)-tester for these properties. Note that the edge conditions are absolute, with respect to \(n^2\), and not with respect to \(|V_i|\cdot |V_j|\). This paper allows for relative bounds of the form \([\rho^L_{i,j}|V_i|\cdot |V_j|, \rho^U_{i,j}|V_i|\cdot |V_j|]\). This is far more expressive, subsuming properties like having large cliques, or large complete bipartite graphs. One of the main results is a (two-sided) \(poly(1/\epsilon)\) tester for all such properties. Furthermore, there is a characterization of such properties that are one-sided testable in \(poly(1/\epsilon)\) queries. For such properties, it is proven that the density conditions (between \(V_i, V_j\)) must either be unconstrained or have \(\rho^L_{i,j} = \rho^U_{i,j}\) be either 1 or 0.</p>
<p><strong>Limits of Ordered Graphs and Images</strong>, by Omri Ben-Eliezer, Eldar Fischer, Amit Levi, and Yuichi Yoshida (<a href="https://arxiv.org/abs/1811.02023">arXiv</a>). The theory of graph limits is a deep topic, with connections to property testing, regularity lemmas, and analysis of real-world networks. Roughly speaking, consider a sequence \(\{G_n\}\) of graphs where the (relative) frequency of any fixed subgraph converges. Then, we can define a limit of the graphs, called a <em>graphon</em>, which is a measurable function \(W:[0,1]^2 \to [0,1]\). This paper discovers appropriate limits of ordered graphs. Note that images can be represented as ordered graphs. An ordered graph is a symmetric function \(G:[n]^2 \to \{0,1\}\) (with the trivial automorphism group). The graphon definitions and limits do not generalize here. There is a sequence of ordered graphs where the ordered subgraph frequencies converge, but one cannot associate any graphon as the limit. This paper discovered a new object called an <em>orderon</em>, which is a function \(W:([0,1]^2)^2 \to [0,1]\). Orderons can be shown to be the limits of sequences of ordered graphs.</p>
<p><strong>Two Party Distribution Testing: Communication and Security</strong>, by Alexandr Andoni, Tal Malkin, and Negev Shekel Nosatzki (<a href="https://arxiv.org/abs/1811.04065">arXiv</a>). This paper gives a new take on distribution testing. Suppose Alice and Bob have each have access to \(t\) samples from (unknown) distributions \(p\) and \(q\) respectively. Their aim is to distinguish \(p = q\) from \(\|p-q\|_{TV} &gt; \epsilon\). How much communication is required? In the standard setting, it is known that \(n^{2/3}\) (ignoring \(\epsilon\)) is necessary and sufficient. In the communication setting, the bound turns out to basically be \(\Theta((n/t)^2)\). Observe how for \(t=n^{2/3}\), the communication is \(n^{2/3}\) (saying that Alice is simply going to send all her samples to Bob). But for larger \(t\), the communication <em>decreases</em>. Since Alice can learn more about her distribution, she can use sketching techniques to reduce her communication. There is an analogous result for testing independence of two distributions. Furthermore, one can even require  these protocols to be <em>secure</em>, so Alice and Bob learn nothing about each others samples (other than the final answer).</p></div>
    </content>
    <updated>2018-12-08T06:21:54Z</updated>
    <published>2018-12-08T06:21:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2018-12-19T01:02:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2018/209</id>
    <link href="https://eccc.weizmann.ac.il/report/2018/209" rel="alternate" type="text/html"/>
    <title>TR18-209 |  AC0 unpredictability | 

	Emanuele Viola</title>
    <summary>We prove that for every distribution $D$ on $n$ bits with Shannon
entropy $\ge n-a$ at most $O(2^{d}a\log^{d+1}g)/\gamma{}^{5}$ of
the bits $D_{i}$ can be predicted with advantage $\gamma$ by an
AC$^{0}$ circuit of size $g$ and depth $d$ that is a function of
all the bits of $D$ except $D_{i}$. This answers a question by Meir
and Wigderson (2017) who proved a corresponding result for decision
trees.

We also show that there are distributions $D$ with entropy $\ge n-O(1)$
such that any subset of $O(n/\log n)$ bits of $D$ on can be distinguished
from uniform by a circuit of depth $2$ and size $\poly(n)$. This
separates the notions of predictability and distinguishability in
this context.
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-08T00:28:19Z</updated>
    <published>2018-12-08T00:28:19Z</published>
    <source>
      <id>https://example.com/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://example.com/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2018-12-19T17:22:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=6311</id>
    <link href="https://windowsontheory.org/2018/12/07/quantum-error-correction/" rel="alternate" type="text/html"/>
    <title>Peter Shor on Quantum Error Correction</title>
    <summary>[Guest post by Annie Wei who scribed Peter Shor’s lecture in our physics and computation seminar. See here for all the posts of this seminar. –Boaz] On October 19, we were lucky enough to have Professor Peter Shor give a guest lecture about quantum error correcting codes. In this blog post, I (Annie Wei) will […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Annie Wei who scribed Peter Shor’s lecture in our <a href="https://www.boazbarak.org/fall18seminar/">physics and computation seminar</a>. See <a href="https://windowsontheory.org/category/physics/">here</a> for all the posts of this seminar. –Boaz]</em></p>
<p>On October 19, we were lucky enough to have Professor Peter Shor give a guest lecture about quantum error correcting codes. In this blog post, I (Annie Wei) will present a summary of this guest lecture, which builds up quantum error correcting codes starting from classical coding theory. We will start by reviewing an example from classical error correction to motivate the similarities and differences when compared against the quantum case, before moving into quantum error correction and quantum channels. Note that we do assume a very basic familiarity with quantum mechanics, such as that which might be found <a href="https://people.eecs.berkeley.edu/~vazirani/f16quantum/lec1.pdf">here</a> or <a href="http://www.theory.caltech.edu/people/preskill/ph229/notes/chap2.pdf">here</a>.</p>
<p><strong>1. Motivation</strong><br/>
We are interested in quantum error correction, ultimately, because any real-world computing device needs to be able to tolerate noise. Theoretical work on quantum algorithms has shown that quantum computers have the potential to offer speedups for a variety of problems, but in practice we’d also like to be able to eventually build and operate real quantum computers. We need to be able to protect against any decoherence that occurs when a quantum computer interacts with the environment, and we need to be able to protect against the accumulation of small gate errors since quantum gates need to be unitary operators.</p>
<p>In error correction the idea is to protect against noise by encoding information in a way that is resistant to noise, usually by adding some redundancy to the message. The redundancy then ensures that enough information remains, even after noise corruption, so that decoding will allow us to recover our original message. This is what is done in classical error correction schemes.</p>
<p>Unfortunately, it’s not obvious that quantum error correction is possible. One obstacle is that errors are continuous, since a continuum of operations can be applied to a qubit, so a priori it might seem like identifying and correcting an error would require infinite resources. In a later section we show how this problem, that of identifying quantum errors, can be overcome. Another obstacle is the fact that, as we’ve stated, classical error correction works by adding redundancy to a message. This might seem impossible to perform in a quantum setting due to the No Cloning Theorem, which states the following:</p>
<p><strong>Theorem</strong> (<em>No Cloning Theorem</em>): Performing the mapping</p>
<p><img alt="|\psi\rangle|0\rangle\mapsto|\psi\rangle|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle%7C0%5Crangle%5Cmapsto%7C%5Cpsi%5Crangle%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle|0\rangle\mapsto|\psi\rangle|\psi\rangle"/></p>
<p>is not a permissible quantum operation.</p>
<p><strong>Proof</strong>: We will use unitarity, which says that a quantum operation specified by a unitary matrix <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> must satisfy</p>
<p><img alt="\langle\phi|U^{\dagger}U|\psi\rangle = \langle\phi|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cphi%7CU%5E%7B%5Cdagger%7DU%7C%5Cpsi%5Crangle+%3D+%5Clangle%5Cphi%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\langle\phi|U^{\dagger}U|\psi\rangle = \langle\phi|\psi\rangle"/>.</p>
<p>(This ensures that the normalization of the state <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/> is always preserved, i.e. that <img alt="|\langle\psi|\psi\rangle|^2=1" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Clangle%5Cpsi%7C%5Cpsi%5Crangle%7C%5E2%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\langle\psi|\psi\rangle|^2=1"/>, which is equivalent to the conservation of probability.)</p>
<p>Now suppose that we can perform the operation</p>
<p><img alt="U(|\psi\rangle|0\rangle)=|\psi\rangle|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=U%28%7C%5Cpsi%5Crangle%7C0%5Crangle%29%3D%7C%5Cpsi%5Crangle%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U(|\psi\rangle|0\rangle)=|\psi\rangle|\psi\rangle"/>.</p>
<p>Then, letting</p>
<p><img alt="(\langle\phi|\langle 0|)(|\psi\rangle|0\rangle)=\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clangle%5Cphi%7C%5Clangle+0%7C%29%28%7C%5Cpsi%5Crangle%7C0%5Crangle%29%3D%5Calpha&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\langle\phi|\langle 0|)(|\psi\rangle|0\rangle)=\alpha"/>,</p>
<p>we note that by unitarity</p>
<p><img alt="(\langle\phi|\langle 0|)(|\psi\rangle |0\rangle)=\alpha(\langle\phi|\langle 0|)U^{\dagger}U(|\psi\rangle|0\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clangle%5Cphi%7C%5Clangle+0%7C%29%28%7C%5Cpsi%5Crangle+%7C0%5Crangle%29%3D%5Calpha%28%5Clangle%5Cphi%7C%5Clangle+0%7C%29U%5E%7B%5Cdagger%7DU%28%7C%5Cpsi%5Crangle%7C0%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\langle\phi|\langle 0|)(|\psi\rangle |0\rangle)=\alpha(\langle\phi|\langle 0|)U^{\dagger}U(|\psi\rangle|0\rangle)"/>.</p>
<p>But</p>
<p><img alt="(\langle\phi|\langle 0|)U^{\dagger}U(|\psi\rangle|0\rangle)=(\langle\phi|\langle\phi|)(|\psi\rangle|\psi\rangle)=\alpha^2" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Clangle%5Cphi%7C%5Clangle+0%7C%29U%5E%7B%5Cdagger%7DU%28%7C%5Cpsi%5Crangle%7C0%5Crangle%29%3D%28%5Clangle%5Cphi%7C%5Clangle%5Cphi%7C%29%28%7C%5Cpsi%5Crangle%7C%5Cpsi%5Crangle%29%3D%5Calpha%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\langle\phi|\langle 0|)U^{\dagger}U(|\psi\rangle|0\rangle)=(\langle\phi|\langle\phi|)(|\psi\rangle|\psi\rangle)=\alpha^2"/>,</p>
<p>and in general <img alt="\alpha\neq\alpha^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%5Cneq%5Calpha%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha\neq\alpha^2"/>, so we have a contradiction.</p>
<p>How do we get around this apparent contradiction? To do so, note that the no-cloning theorem only prohibits the copying of non-orthogonal quantum states. With orthogonal quantum states, either <img alt="\alpha=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha=0"/> or <img alt="\alpha=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha=1"/>, so we don’t run into a contradiction. This also explains why it is possible to copy classical information, which we can think of as orthogonal quantum states.</p>
<p>So how do we actually protect quantum information from noise? In the next section we first review classical error correction, as ideas from the classical setting re-appear in the quantum setting, and then we move into quantum error correction.</p>
<p><strong>2. Review of Classical Error Correction</strong><br/>
First we start by reviewing classical error correction. In classical error correction we generally have a message that we encode, send through a noisy channel, and then decode, in the following schematic process:</p>
<p><img alt="fig1.png" class="alignnone size-full wp-image-6312" src="https://windowsontheory.files.wordpress.com/2018/12/fig1.png?w=600"/><br/>
In an effective error correction scheme, the decoding process should allow us to identify any errors that occurred when our message passed through the noisy channel, which then tells us how to correct the errors. The formalism that allows us to do so is the following: we first define a <img alt="r\times n" class="latex" src="https://s0.wp.com/latex.php?latex=r%5Ctimes+n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r\times n"/> encoding matrix <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> that takes a message <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> of length <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> and converts it to a codeword <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/> of length <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>, where the codewords make up the span of the rows of <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>. An example of such a matrix is</p>
<p><img alt="G=\left(\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\\1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=G%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bccccccc%7D0%260%260%261%261%261%261%5C%5C1%260%261%260%261%260%261%5C%5C0%261%261%260%260%261%261%5C%5C1%261%261%261%261%261%261%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G=\left(\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\\1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1\end{array}\right)"/>,</p>
<p>corresponding to the 7-bit Hamming codes, which encodes a 4-bit message as a 7-bit codeword. Note that this code has distance 3 since each of the rows in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> differ in at most 3 spots, which means that it can correct at most 1 error (the number of errors that can be corrected is given by half the code distance).</p>
<p>We also define the parity check matrix <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> to be the matrix that satisfies</p>
<p><img alt="GH^T=0" class="latex" src="https://s0.wp.com/latex.php?latex=GH%5ET%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="GH^T=0"/>.</p>
<p>For example, to define <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> corresponding to the <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> we defined for the 7-bit Hamming code, we could take</p>
<p><img alt="H=\left(\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=H%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bccccccc%7D0%260%260%261%261%261%261%5C%5C1%260%261%260%261%260%261%5C%5C0%261%261%260%260%261%261%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H=\left(\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\end{array}\right)"/>.</p>
<p>Then we may decode <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>, a 7-bit string, in the following manner. Say that</p>
<p><img alt="x=c+e" class="latex" src="https://s0.wp.com/latex.php?latex=x%3Dc%2Be&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x=c+e"/>,</p>
<p>where <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/> is a codeword and <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/> is the 1-bit error we wish to correct. Then</p>
<p><img alt="xH^T=(c+e)H^T=eH^T" class="latex" src="https://s0.wp.com/latex.php?latex=xH%5ET%3D%28c%2Be%29H%5ET%3DeH%5ET&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="xH^T=(c+e)H^T=eH^T"/>.</p>
<p>Here <img alt="eH^T" class="latex" src="https://s0.wp.com/latex.php?latex=eH%5ET&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="eH^T"/> uniquely identifies the error and is known as the <em>error syndrome</em>. Having it tells us how to correct the error. Thus our error correction scheme consists of the following steps:</p>
<ol>
<li>Encode a <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/>-bit message <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> by multiplying by <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> to obtain codeword <img alt="mG=c" class="latex" src="https://s0.wp.com/latex.php?latex=mG%3Dc&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="mG=c"/>.</li>
<li>Send the message through channel generating error <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/>, resulting in the string <img alt="x=c+e" class="latex" src="https://s0.wp.com/latex.php?latex=x%3Dc%2Be&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x=c+e"/>.</li>
<li>Multiply by <img alt="H^T" class="latex" src="https://s0.wp.com/latex.php?latex=H%5ET&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^T"/> to obtain the <em>error syndrome</em> <img alt="eH^T" class="latex" src="https://s0.wp.com/latex.php?latex=eH%5ET&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="eH^T"/>.</li>
<li>Correct error <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/> to obtain <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/>.</li>
</ol>
<p>Having concluded our quick review of classical error correction, we now look at the theory of quantum error correction.</p>
<p><strong>3. Quantum Error Correction</strong><br/>
In this section we introduce quantum error correction by directly constructing the 9-qubit code and the 7-qubit code. Then we introduce the more general formalism of CSS codes, which encompasses both the 9-qubit and 7-qubit codes, before introducing the stabilizer formalism, which tells us how we might construct a CSS code.</p>
<p><strong>3.1. Preliminaries</strong><br/>
First we introduce some tools that we will need in this section.</p>
<p><strong>3.1.1. Pauli Matrices</strong><br/>
The Pauli matrices are a set of 2-by-2 matrices that form an orthogonal basis for the 2-by-2 Hermitian matrices, where a Hermitian matrix <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> satisfies <img alt="H^{\dagger}=H" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cdagger%7D%3DH&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\dagger}=H"/>. Note that we can form larger Hilbert spaces by taking the tensor product of smaller Hilbert spaces, so in particular taking the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-fold tensor product of Pauli matrices gives us a basis for the <img alt="2^k" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^k"/>-by-<img alt="2^k" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^k"/> Hermitian matrices. Note also that generally, in quantum mechanics, we are interested in Hermitian matrices because they can be used to represent measurements, and because unitary matrices, which can be used to represent probability-preserving quantum operations, can be obtained by exponentiating Hermitian matrices (that is, every unitary matrix <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> can be written in the form <img alt="U=e^{iH}" class="latex" src="https://s0.wp.com/latex.php?latex=U%3De%5E%7BiH%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U=e^{iH}"/> for <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> a Hermitian matrix).</p>
<p>The Pauli matrices are given by</p>
<p><img alt="\sigma_x\equiv X\equiv\left(\begin{array}{cc}0&amp;1\\1&amp;0\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x%5Cequiv+X%5Cequiv%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D0%261%5C%5C1%260%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x\equiv X\equiv\left(\begin{array}{cc}0&amp;1\\1&amp;0\end{array}\right)"/></p>
<p><img alt="\sigma_y\equiv Y\equiv\left(\begin{array}{cc}0&amp;-i\\i&amp;0\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y%5Cequiv+Y%5Cequiv%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D0%26-i%5C%5Ci%260%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y\equiv Y\equiv\left(\begin{array}{cc}0&amp;-i\\i&amp;0\end{array}\right)"/></p>
<p><img alt="\sigma_z\equiv Z\equiv\left(\begin{array}{cc}1&amp;0\\0&amp;-1\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z%5Cequiv+Z%5Cequiv%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D1%260%5C%5C0%26-1%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z\equiv Z\equiv\left(\begin{array}{cc}1&amp;0\\0&amp;-1\end{array}\right)"/>.</p>
<p>By direction computation we can show that they satisfy the relations</p>
<p><img alt="X^2=Y^2=Z^2=I" class="latex" src="https://s0.wp.com/latex.php?latex=X%5E2%3DY%5E2%3DZ%5E2%3DI&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X^2=Y^2=Z^2=I"/></p>
<p><img alt="ZX=-XZ=iY" class="latex" src="https://s0.wp.com/latex.php?latex=ZX%3D-XZ%3DiY&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="ZX=-XZ=iY"/></p>
<p><img alt="YZ=-ZY=iX" class="latex" src="https://s0.wp.com/latex.php?latex=YZ%3D-ZY%3DiX&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="YZ=-ZY=iX"/></p>
<p><img alt="XY=-YX=iZ" class="latex" src="https://s0.wp.com/latex.php?latex=XY%3D-YX%3DiZ&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="XY=-YX=iZ"/>.</p>
<p><strong>3.1.2. Von Neumann Measurements</strong><br/>
We will also need the concept of a Von Neumann measurement. A Von Neumann measurement is given by a set of projection matrices <img alt="\{E_1, E_2, ..., E_k\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BE_1%2C+E_2%2C+...%2C+E_k%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{E_1, E_2, ..., E_k\}"/> satisfying</p>
<p><img alt="\sum_{i=1}^k E_i=I" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5Ek+E_i%3DI&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{i=1}^k E_i=I"/>.</p>
<p>That is, the projectors partition a Hilbert space <img alt="{\cal H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+H%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{\cal H}"/> into <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> subspaces. Then, given any state <img alt="|\psi\rangle\in{\cal H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle%5Cin%7B%5Ccal+H%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle\in{\cal H}"/>, when we perform a measurement using these projectors we obtain the measurement result corresponding to <img alt="E_i" class="latex" src="https://s0.wp.com/latex.php?latex=E_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_i"/>, with corresponding post-measurement state</p>
<p><img alt="\frac{E_i|\psi\rangle}{||E_i|\psi\rangle||}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BE_i%7C%5Cpsi%5Crangle%7D%7B%7C%7CE_i%7C%5Cpsi%5Crangle%7C%7C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{E_i|\psi\rangle}{||E_i|\psi\rangle||}"/>,</p>
<p>with probability <img alt="\langle\psi|E_i|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cpsi%7CE_i%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\langle\psi|E_i|\psi\rangle"/>.</p>
<p><strong>3.2. First Attempt at a Quantum Code</strong><br/>
Now we make a first attempt at coming up with a quantum code, noting that our efforts and adjustments will ultimately culminate in the 9-qubit code. Starting with the simplest possible idea, we take inspiration from the classical repetition code, which maps</p>
<p><img alt="0\mapsto 000" class="latex" src="https://s0.wp.com/latex.php?latex=0%5Cmapsto+000&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0\mapsto 000"/></p>
<p><img alt="1\mapsto 111" class="latex" src="https://s0.wp.com/latex.php?latex=1%5Cmapsto+111&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1\mapsto 111"/></p>
<p>and decodes by taking the majority of the 3 bits. We consider the quantum analog of this, which maps</p>
<p><img alt="|0\rangle\mapsto|000\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%5Cmapsto%7C000%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle\mapsto|000\rangle"/></p>
<p><img alt="|1\rangle\mapsto|111\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle%5Cmapsto%7C111%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle\mapsto|111\rangle"/>.</p>
<p>We will take our quantum errors to be the Pauli matrices <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/>, <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/>, and <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/>. Then the encoding process, where our message is a quantum state <img alt="\alpha|0\rangle+\beta|1\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle%2B%5Cbeta%7C1%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle+\beta|1\rangle"/>, looks like the following:</p>
<p><img alt="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|000\rangle+\beta|111\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle%2B%5Cbeta%7C1%5Crangle%5Cmapsto%5Calpha%7C000%5Crangle%2B%5Cbeta%7C111%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|000\rangle+\beta|111\rangle"/>.</p>
<p>We claim that this code can correct bit errors but not phase errors, which makes it equivalent to the original classical repetition code for error correction. To see this, note that applying an <img alt="X_1" class="latex" src="https://s0.wp.com/latex.php?latex=X_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_1"/> error results in the mapping</p>
<p><img alt="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|100\rangle+\beta|011\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle%2B%5Cbeta%7C1%5Crangle%5Cmapsto%5Calpha%7C100%5Crangle%2B%5Cbeta%7C011%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|100\rangle+\beta|011\rangle"/>.</p>
<p>This can be detected by the von Neumann measurement which projects onto the subspaces</p>
<p><img alt="\{|000\rangle,|111\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C000%5Crangle%2C%7C111%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|000\rangle,|111\rangle\}"/></p>
<p><img alt="\{|011\rangle,|100\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C011%5Crangle%2C%7C100%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|011\rangle,|100\rangle\}"/></p>
<p><img alt="\{|010\rangle,|101\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C010%5Crangle%2C%7C101%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|010\rangle,|101\rangle\}"/></p>
<p><img alt="\{|110\rangle,|001\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C110%5Crangle%2C%7C001%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|110\rangle,|001\rangle\}"/></p>
<p>We could then apply <img alt="\sigma_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x"/> to the first qubit to correct the error. To see that this doesn’t work for phase errors, note that applying a <img alt="Z_2" class="latex" src="https://s0.wp.com/latex.php?latex=Z_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_2"/> error results in the mapping</p>
<p><img alt="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|000\rangle-\beta|111\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle%2B%5Cbeta%7C1%5Crangle%5Cmapsto%5Calpha%7C000%5Crangle-%5Cbeta%7C111%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle+\beta|1\rangle\mapsto\alpha|000\rangle-\beta|111\rangle"/>.</p>
<p>This is a valid encoding of the state <img alt="\alpha|0\rangle-\beta|1\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle-%5Cbeta%7C1%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle-\beta|1\rangle"/>, so the error is undetectable.</p>
<p>What adjustments can we make so that we’re able to also correct <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> errors? For this we will introduce the Hadamard matrix, defined as</p>
<p><img alt="H=\frac{1}{\sqrt{2}}\left(\begin{array}{cc}1&amp;1\\1&amp;-1\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=H%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D1%261%5C%5C1%26-1%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H=\frac{1}{\sqrt{2}}\left(\begin{array}{cc}1&amp;1\\1&amp;-1\end{array}\right)"/></p>
<p>and satisfying</p>
<p><img alt="HX=ZH" class="latex" src="https://s0.wp.com/latex.php?latex=HX%3DZH&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="HX=ZH"/>.</p>
<p>Note in particular that, because <img alt="HX=ZH" class="latex" src="https://s0.wp.com/latex.php?latex=HX%3DZH&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="HX=ZH"/>, the Hadamard matrix turns bit errors into phase errors, and vice versa. This allows us to come up with a code that corrects phase errors by mapping</p>
<p><img alt="H|0\rangle\mapsto H^{\otimes 3}|000\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=H%7C0%5Crangle%5Cmapsto+H%5E%7B%5Cotimes+3%7D%7C000%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H|0\rangle\mapsto H^{\otimes 3}|000\rangle"/></p>
<p><img alt="H|1\rangle\mapsto H^{\otimes 3}|111\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=H%7C1%5Crangle%5Cmapsto+H%5E%7B%5Cotimes+3%7D%7C111%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H|1\rangle\mapsto H^{\otimes 3}|111\rangle"/></p>
<p>or equivalently,</p>
<p><img alt="|0\rangle\mapsto\frac{1}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%5Cmapsto%5Cfrac%7B1%7D%7B2%7D%28%7C000%5Crangle%2B%7C011%5Crangle%2B%7C101%5Crangle%2B%7C110%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle\mapsto\frac{1}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)"/></p>
<p><img alt="|1\rangle\mapsto\frac{1}{2}(|111\rangle+|100\rangle+|010\rangle+|001\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle%5Cmapsto%5Cfrac%7B1%7D%7B2%7D%28%7C111%5Crangle%2B%7C100%5Crangle%2B%7C010%5Crangle%2B%7C001%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle\mapsto\frac{1}{2}(|111\rangle+|100\rangle+|010\rangle+|001\rangle)"/></p>
<p>Now we can concatenate our bit flip code with our phase flip code to take care of both errors. This gives us the 9-qubit code, also known as the Shor code.</p>
<p><strong>3.3. 9-Qubit Code</strong><br/>
In the previous section, we went through the process of constructing the 9-qubit Shor code by considering how to correct both bit flip errors and phase flip errors. Explicitly, the 9-qubit Shor code is given by the following mapping:</p>
<p><img alt="|0\rangle\mapsto|0\rangle_L\equiv\frac{1}{2}(|000000000\rangle+|000111111\rangle+|111000111\rangle+|111111000\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%5Cmapsto%7C0%5Crangle_L%5Cequiv%5Cfrac%7B1%7D%7B2%7D%28%7C000000000%5Crangle%2B%7C000111111%5Crangle%2B%7C111000111%5Crangle%2B%7C111111000%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle\mapsto|0\rangle_L\equiv\frac{1}{2}(|000000000\rangle+|000111111\rangle+|111000111\rangle+|111111000\rangle)"/></p>
<p><img alt="|1\rangle\mapsto|1\rangle_L\equiv\frac{1}{2}(|111111111\rangle+|111000000\rangle+|000111000\rangle+|000000111\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle%5Cmapsto%7C1%5Crangle_L%5Cequiv%5Cfrac%7B1%7D%7B2%7D%28%7C111111111%5Crangle%2B%7C111000000%5Crangle%2B%7C000111000%5Crangle%2B%7C000000111%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle\mapsto|1\rangle_L\equiv\frac{1}{2}(|111111111\rangle+|111000000\rangle+|000111000\rangle+|000000111\rangle)"/>.</p>
<p>Here <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> and <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/> are known as <em>logical qubits</em>; note that our 9-qubit code essentially represents 1 logical qubit with 9 physical qubits.</p>
<p>Note that by construction this code can correct <img alt="\sigma_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x"/>, <img alt="\sigma_y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y"/>, and <img alt="\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z"/> errors on any one qubit (we’ve already shown by construction that it can correct <img alt="\sigma_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x"/> and <img alt="\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z"/> errors, and <img alt="\sigma_y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y"/> can be obtained as a product of the two). This is also equivalent to the statement that the states <img alt="\sigma_x^{(i)}|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x%5E%7B%28i%29%7D%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x^{(i)}|0\rangle_L"/>, <img alt="\sigma_y^{(i)}|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y%5E%7B%28i%29%7D%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y^{(i)}|0\rangle_L"/>, <img alt="\sigma_z^{(i)}|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z%5E%7B%28i%29%7D%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z^{(i)}|0\rangle_L"/>, <img alt="\sigma_x^{(i)}|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x%5E%7B%28i%29%7D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x^{(i)}|1\rangle_L"/>, <img alt="\sigma_y^{(i)}|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y%5E%7B%28i%29%7D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y^{(i)}|1\rangle_L"/>, and <img alt="\sigma_z^{(i)}|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z%5E%7B%28i%29%7D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z^{(i)}|1\rangle_L"/> are all orthogonal.</p>
<p>Now we have a 1-error quantum code. We claim that such a code can in fact correct any error operation, and that this is a property of all 1-error quantum codes:</p>
<p><strong>Theorem: </strong>Given any possible unitary, measurement, or quantum operation on a one-error quantum code, the code can correct it.</p>
<p><strong>Proof: </strong><img alt="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BI%2C+%5Csigma_x%2C+%5Csigma_y%2C+%5Csigma_z%5C%7D%5E%7B%5Cotimes+t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}"/> forms a basis for the <img alt="2\times 2" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ctimes+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2\times 2"/> matrices. For errors on <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> qubits, the code can correct these errors if it can individually correct errors <img alt="\sigma_{w_i}^{(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_%7Bw_i%7D%5E%7B%28i%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_{w_i}^{(i)}"/> for <img alt="w_i\in\{x,y,z\}" class="latex" src="https://s0.wp.com/latex.php?latex=w_i%5Cin%5C%7Bx%2Cy%2Cz%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_i\in\{x,y,z\}"/>, <img alt="i\in\{1,...,t\}" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cin%5C%7B1%2C...%2Ct%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i\in\{1,...,t\}"/>, since <img alt="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BI%2C+%5Csigma_x%2C+%5Csigma_y%2C+%5Csigma_z%5C%7D%5E%7B%5Cotimes+t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}"/> forms a basis for <img alt="\mathbb{C}^{2t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D%5E%7B2t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb{C}^{2t}"/>.</p>
<p><strong>Example: Phase Error</strong> Next we’ll do an example where we consider how we might correct an arbitrary phase error applied to the <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> state. Since quantum states are equivalent up to phases, the error operator is given by</p>
<p><img alt="\left(\begin{array}{cc}1&amp;0\\0&amp;e^{i\theta}\end{array}\right)\equiv\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D1%260%5C%5C0%26e%5E%7Bi%5Ctheta%7D%5Cend%7Barray%7D%5Cright%29%5Cequiv%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7De%5E%7B-i%5Ctheta%2F2%7D%260%5C%5C0%26e%5E%7Bi%5Ctheta%2F2%7D%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left(\begin{array}{cc}1&amp;0\\0&amp;e^{i\theta}\end{array}\right)\equiv\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)"/>.</p>
<p>Note that this can be rewritten in the <img alt="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BI%2C+%5Csigma_x%2C+%5Csigma_y%2C+%5Csigma_z%5C%7D%5E%7B%5Cotimes+t%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{I, \sigma_x, \sigma_y, \sigma_z\}^{\otimes t}"/> basis as</p>
<p><img alt="\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)=\cos\frac{\theta}{2}I-i\sin\frac{\theta}{2}\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7De%5E%7B-i%5Ctheta%2F2%7D%260%5C%5C0%26e%5E%7Bi%5Ctheta%2F2%7D%5Cend%7Barray%7D%5Cright%29%3D%5Ccos%5Cfrac%7B%5Ctheta%7D%7B2%7DI-i%5Csin%5Cfrac%7B%5Ctheta%7D%7B2%7D%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)=\cos\frac{\theta}{2}I-i\sin\frac{\theta}{2}\sigma_z"/>.</p>
<p>Now, applying this error to <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/>, we get</p>
<p><img alt="\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)\frac{1}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)=\frac{1}{2}\cos\frac{\theta}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)-\frac{i}{2}\sin\frac{\theta}{2}(|000\rangle-|011\rangle+|101\rangle-|110\rangle)=\cos\frac{\theta}{2}|0\rangle_L-i\sin\frac{\theta}{2}\sigma_z|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7De%5E%7B-i%5Ctheta%2F2%7D%260%5C%5C0%26e%5E%7Bi%5Ctheta%2F2%7D%5Cend%7Barray%7D%5Cright%29%5Cfrac%7B1%7D%7B2%7D%28%7C000%5Crangle%2B%7C011%5Crangle%2B%7C101%5Crangle%2B%7C110%5Crangle%29%3D%5Cfrac%7B1%7D%7B2%7D%5Ccos%5Cfrac%7B%5Ctheta%7D%7B2%7D%28%7C000%5Crangle%2B%7C011%5Crangle%2B%7C101%5Crangle%2B%7C110%5Crangle%29-%5Cfrac%7Bi%7D%7B2%7D%5Csin%5Cfrac%7B%5Ctheta%7D%7B2%7D%28%7C000%5Crangle-%7C011%5Crangle%2B%7C101%5Crangle-%7C110%5Crangle%29%3D%5Ccos%5Cfrac%7B%5Ctheta%7D%7B2%7D%7C0%5Crangle_L-i%5Csin%5Cfrac%7B%5Ctheta%7D%7B2%7D%5Csigma_z%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left(\begin{array}{cc}e^{-i\theta/2}&amp;0\\0&amp;e^{i\theta/2}\end{array}\right)\frac{1}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)=\frac{1}{2}\cos\frac{\theta}{2}(|000\rangle+|011\rangle+|101\rangle+|110\rangle)-\frac{i}{2}\sin\frac{\theta}{2}(|000\rangle-|011\rangle+|101\rangle-|110\rangle)=\cos\frac{\theta}{2}|0\rangle_L-i\sin\frac{\theta}{2}\sigma_z|0\rangle_L"/>.</p>
<p>After performing a projective measurement, we get state <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> with probability <img alt="\cos^2\frac{\theta}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ccos%5E2%5Cfrac%7B%5Ctheta%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\cos^2\frac{\theta}{2}"/>, in which case we do not need to perform any error correction, and we get <img alt="\sigma_z|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z|0\rangle_L"/> with probability <img alt="\sin^2\frac{\theta}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csin%5E2%5Cfrac%7B%5Ctheta%7D%7B2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sin^2\frac{\theta}{2}"/>, in which case we would know to correct the <img alt="\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z"/> error.</p>
<p><strong>3.4. 7-Qubit Code</strong><br/>
Now that we’ve constructed the 9-qubit code and shown that quantum error correction is possible, we might wonder whether it’s possible to do better. For example, we’d like a code that requires fewer qubits. We’ll construct a 7-qubit code that corrects 1 error, defining a mapping to <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> and <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/> by taking inspiration from a classical code, as we did for the 9-qubit case.</p>
<p>For this we will need to go back to the example we used to illustration classical error correction. Recall that in classical error correction, we have an encoding matrix <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> and a parity check matrix <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> satisfying <img alt="GH^T=0" class="latex" src="https://s0.wp.com/latex.php?latex=GH%5ET%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="GH^T=0"/>, with <img alt="\text{rank}(G)+\text{rank}(H)=n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Brank%7D%28G%29%2B%5Ctext%7Brank%7D%28H%29%3Dn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{rank}(G)+\text{rank}(H)=n"/>. We encode a message <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> to obtain codeword <img alt="mG=c" class="latex" src="https://s0.wp.com/latex.php?latex=mG%3Dc&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="mG=c"/>. After error <img alt="e" class="latex" src="https://s0.wp.com/latex.php?latex=e&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="e"/> is applied, this becomes <img alt="c+e" class="latex" src="https://s0.wp.com/latex.php?latex=c%2Be&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c+e"/>, from which we can extract the error syndrome <img alt="(c+e)H^T=eH^T" class="latex" src="https://s0.wp.com/latex.php?latex=%28c%2Be%29H%5ET%3DeH%5ET&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(c+e)H^T=eH^T"/>. We can then apply the appropriate correction to extract <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c"/> from <img alt="c+e" class="latex" src="https://s0.wp.com/latex.php?latex=c%2Be&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c+e"/>.</p>
<p>Now we will use the encoding matrix from our classical error correction example, and we will divide our codewords into two sets, <img alt="C_1" class="latex" src="https://s0.wp.com/latex.php?latex=C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1"/> and <img alt="C_1'" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%27&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1'"/>, given by</p>
<p><img alt="C_1=\left\{\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\end{array}\right." class="latex" src="https://s0.wp.com/latex.php?latex=C_1%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bccccccc%7D0%260%260%261%261%261%261%5C%5C1%260%261%260%261%260%261%5C%5C0%261%261%260%260%261%261%5Cend%7Barray%7D%5Cright.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1=\left\{\begin{array}{ccccccc}0&amp;0&amp;0&amp;1&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;0&amp;0&amp;1&amp;1\end{array}\right."/></p>
<p>and</p>
<p><img alt="C_1'=C_1+1111111" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%27%3DC_1%2B1111111&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1'=C_1+1111111"/>.</p>
<p>Similar to how we approached the 9-qubit case, we will start by defining our code as follows:</p>
<p><img alt="|0\rangle_L\equiv\frac{1}{\sqrt{8}}\sum_{v\in C_1}|v\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L%5Cequiv%5Cfrac%7B1%7D%7B%5Csqrt%7B8%7D%7D%5Csum_%7Bv%5Cin+C_1%7D%7Cv%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L\equiv\frac{1}{\sqrt{8}}\sum_{v\in C_1}|v\rangle"/></p>
<p><img alt="|1\rangle_L\equiv\frac{1}{\sqrt{8}}\sum_{w\in C_1'}|w\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L%5Cequiv%5Cfrac%7B1%7D%7B%5Csqrt%7B8%7D%7D%5Csum_%7Bw%5Cin+C_1%27%7D%7Cw%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L\equiv\frac{1}{\sqrt{8}}\sum_{w\in C_1'}|w\rangle"/>.</p>
<p>Note that this corrects bit flip errors by construction. How can we ensure that we are also able to correct phase errors? For this we again turn to the Hadamard matrix, which allows us to toggle between bit and phase errors. We claim that</p>
<p><img alt="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L+|1\rangle_L)" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7C0%5Crangle_L%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C0%5Crangle_L%2B%7C1%5Crangle_L%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L+|1\rangle_L)"/></p>
<p><img alt="H^{\otimes 7}|1\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L-|1\rangle_L)" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7C1%5Crangle_L%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C0%5Crangle_L-%7C1%5Crangle_L%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|1\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L-|1\rangle_L)"/>.</p>
<p><strong>Proof: </strong>We will show that</p>
<p><img alt="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L+|1\rangle_L)" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7C0%5Crangle_L%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C0%5Crangle_L%2B%7C1%5Crangle_L%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}(|0\rangle_L+|1\rangle_L)"/>,</p>
<p>noting that the argument for <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/> is similar. First we will need the fact that</p>
<p><img alt="H^{\otimes 7}|v\rangle=\frac{1}{2^{7/2}}\sum_{w\in\{0,1\}^7}(-1)^{w\cdot v}|w\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7Cv%5Crangle%3D%5Cfrac%7B1%7D%7B2%5E%7B7%2F2%7D%7D%5Csum_%7Bw%5Cin%5C%7B0%2C1%5C%7D%5E7%7D%28-1%29%5E%7Bw%5Ccdot+v%7D%7Cw%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|v\rangle=\frac{1}{2^{7/2}}\sum_{w\in\{0,1\}^7}(-1)^{w\cdot v}|w\rangle"/>.</p>
<p>To see that this fact is true, note that</p>
<p><img alt="H=\frac{1}{\sqrt{2}}(|0\rangle\langle 0|+|0\rangle\langle 1|+|1\rangle\langle 0|-|1\rangle\langle 1|)" class="latex" src="https://s0.wp.com/latex.php?latex=H%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C0%5Crangle%5Clangle+0%7C%2B%7C0%5Crangle%5Clangle+1%7C%2B%7C1%5Crangle%5Clangle+0%7C-%7C1%5Crangle%5Clangle+1%7C%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H=\frac{1}{\sqrt{2}}(|0\rangle\langle 0|+|0\rangle\langle 1|+|1\rangle\langle 0|-|1\rangle\langle 1|)"/></p>
<p>and that <img alt="w\cdot v" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Ccdot+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\cdot v"/> is equal to the number of bits in which <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w"/> and <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> are both 1. Now we can start by directly calculating</p>
<p><img alt="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{8}}\frac{1}{\sqrt{128}}\sum_{v\in C_1}\sum_{w\in\{0,1\}^7}(-1)^{v\cdot w}|w\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7C0%5Crangle_L%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B8%7D%7D%5Cfrac%7B1%7D%7B%5Csqrt%7B128%7D%7D%5Csum_%7Bv%5Cin+C_1%7D%5Csum_%7Bw%5Cin%5C%7B0%2C1%5C%7D%5E7%7D%28-1%29%5E%7Bv%5Ccdot+w%7D%7Cw%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{8}}\frac{1}{\sqrt{128}}\sum_{v\in C_1}\sum_{w\in\{0,1\}^7}(-1)^{v\cdot w}|w\rangle"/>.</p>
<p>Note that for <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> two codewords, assuming that <img alt="w\cdot y=1" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Ccdot+y%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\cdot y=1"/>, we must have that <img alt="x\cdot w=0" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Ccdot+w%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x\cdot w=0"/> iff <img alt="(x+y)\cdot w=1" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2By%29%5Ccdot+w%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(x+y)\cdot w=1"/>. Thus we can break the codespace up into an equal number of codewords <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> satisfying <img alt="x\cdot w=0" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Ccdot+w%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x\cdot w=0"/> and <img alt="x\cdot w=1" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Ccdot+w%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x\cdot w=1"/>. This means that we must have that the sum <img alt="\sum_{v\in C_1}\sum_{w\in\{0,1\}^7}(-1)^{w\cdot v}|w\rangle=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bv%5Cin+C_1%7D%5Csum_%7Bw%5Cin%5C%7B0%2C1%5C%7D%5E7%7D%28-1%29%5E%7Bw%5Ccdot+v%7D%7Cw%5Crangle%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{v\in C_1}\sum_{w\in\{0,1\}^7}(-1)^{w\cdot v}|w\rangle=0"/> unless we have <img alt="w\perp C_1" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Cperp+C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\perp C_1"/>. But those <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w"/> that satisfy <img alt="w\perp C_1" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Cperp+C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\perp C_1"/> are exactly all the codewords by definition, so we must have that</p>
<p><img alt="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}|0\rangle_L+\frac{1}{\sqrt{2}}|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=H%5E%7B%5Cotimes+7%7D%7C0%5Crangle_L%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%7C0%5Crangle_L%2B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H^{\otimes 7}|0\rangle_L=\frac{1}{\sqrt{2}}|0\rangle_L+\frac{1}{\sqrt{2}}|1\rangle_L"/></p>
<p>as the sum in <img alt="|0\rangle_L+|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L%2B%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L+|1\rangle_L"/> runs equally over all codewords.</p>
<p>Thus we have constructed a 7-qubit quantum code that corrects 1 error, and moreover we see that for both the 9-qubit and 7-qubit codes, both of which are 1-error quantum codes, the fact that they can correct 1-error comes directly from the fact that the original classical codes we used to construct them can themselves correct 1 error. This suggests that we should be able to come up with a more general procedure for constructing quantum codes from classical codes.</p>
<p><strong>3.5. CSS Codes</strong><br/>
<em>CSS (Calderbank-Shor-Steane) codes</em> generalize the process by which we constructed the 9-qubit and 7-qubit codes, and they give us a general framework for constructing quantum codes from classical codes. In a CSS code, we require groups <img alt="C_1" class="latex" src="https://s0.wp.com/latex.php?latex=C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1"/>, <img alt="C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2"/> satisfying</p>
<p><img alt="&#xA0;C_1\subseteq C_2" class="latex" src="https://s0.wp.com/latex.php?latex=%C2%A0C_1%5Csubseteq+C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&#xA0;C_1\subseteq C_2"/></p>
<p><img alt="C_2^{\perp}\subseteq C_1^{\perp}" class="latex" src="https://s0.wp.com/latex.php?latex=C_2%5E%7B%5Cperp%7D%5Csubseteq+C_1%5E%7B%5Cperp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2^{\perp}\subseteq C_1^{\perp}"/></p>
<p>Then we can define codewords to correspond to cosets of <img alt="C_1" class="latex" src="https://s0.wp.com/latex.php?latex=C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1"/> in <img alt="C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2"/>, so that the number of codewords is equal to <img alt="2^{\text{dim}(C_2)-\text{dim}(C_1)}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B%5Ctext%7Bdim%7D%28C_2%29-%5Ctext%7Bdim%7D%28C_1%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{\text{dim}(C_2)-\text{dim}(C_1)}"/>. Thus by this definition we can say that codewords <img alt="w_1, w_2\in C_2" class="latex" src="https://s0.wp.com/latex.php?latex=w_1%2C+w_2%5Cin+C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_1, w_2\in C_2"/> are in the same coset if <img alt="w_1-w_2\in C_1" class="latex" src="https://s0.wp.com/latex.php?latex=w_1-w_2%5Cin+C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w_1-w_2\in C_1"/>. Explicitly, the codeword for coset <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w"/> is given by the state</p>
<p><img alt="\frac{1}{|C_1|^{1/2}}\sum_{x\in C_1}|x+w\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%7CC_1%7C%5E%7B1%2F2%7D%7D%5Csum_%7Bx%5Cin+C_1%7D%7Cx%2Bw%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{|C_1|^{1/2}}\sum_{x\in C_1}|x+w\rangle"/>,</p>
<p>and under the Hadamard transformation applied to each qubit this state is in turn mapped to the state</p>
<p><img alt="\frac{1}{|C_1^{\perp}|^{1/2}}\sum_{x\in C_1^{\perp}}|x+w\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%7CC_1%5E%7B%5Cperp%7D%7C%5E%7B1%2F2%7D%7D%5Csum_%7Bx%5Cin+C_1%5E%7B%5Cperp%7D%7D%7Cx%2Bw%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{|C_1^{\perp}|^{1/2}}\sum_{x\in C_1^{\perp}}|x+w\rangle"/>.</p>
<p>That is to say, the Hadamard “dualizes” our original code, toggling bit errors to phase errors and vice versa. (This can be seen by direct calculation, as in the case of the 7-qubit code, where we used the fact that <img alt="\sum_{v\in C_1}(-1)^{v\cdot w}=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bv%5Cin+C_1%7D%28-1%29%5E%7Bv%5Ccdot+w%7D%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_{v\in C_1}(-1)^{v\cdot w}=0"/> for <img alt="w\not\in C_1^{\perp}" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Cnot%5Cin+C_1%5E%7B%5Cperp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w\not\in C_1^{\perp}"/>.)</p>
<p>Note also that this code can correct a number of bit errors equal to the minimum weight of <img alt="\{v\in C_2-C_1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bv%5Cin+C_2-C_1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{v\in C_2-C_1\}"/>.</p>
<p>With the CSS construction we have thus reduced the problem of finding a quantum error correcting code to the problem of finding appropriate <img alt="C_1" class="latex" src="https://s0.wp.com/latex.php?latex=C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1"/>, <img alt="C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2"/>. Note that the special case of <img alt="C_2^{\perp}=C_1=C" class="latex" src="https://s0.wp.com/latex.php?latex=C_2%5E%7B%5Cperp%7D%3DC_1%3DC&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2^{\perp}=C_1=C"/> corresponds to weakly self-dual codes, which are well studied classically. Doubly even, weakly self-dual codes additionally have the requirement that all codewords have Hamming weights that are multiples of 4; they satisfy the requirement</p>
<p><img alt="1^n\subseteq C^{\perp}\subseteq C\subseteq\mathbb{Z}_2^n" class="latex" src="https://s0.wp.com/latex.php?latex=1%5En%5Csubseteq+C%5E%7B%5Cperp%7D%5Csubseteq+C%5Csubseteq%5Cmathbb%7BZ%7D_2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1^n\subseteq C^{\perp}\subseteq C\subseteq\mathbb{Z}_2^n"/></p>
<p>and are also well studied classically.</p>
<p><strong>3.6. Gilbert-Varshamov Bound</strong><br/>
In the previous section we introduced CSS codes and demonstrated that the problem of constructing a quantum code could be reduced to the problem of finding two groups <img alt="C_1" class="latex" src="https://s0.wp.com/latex.php?latex=C_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1"/>, <img alt="C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2"/> satisfying</p>
<p><img alt="C_1\subseteq C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%5Csubseteq+C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1\subseteq C_2"/></p>
<p><img alt="C_2^{\perp}\subseteq C_1^{\perp}" class="latex" src="https://s0.wp.com/latex.php?latex=C_2%5E%7B%5Cperp%7D%5Csubseteq+C_1%5E%7B%5Cperp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2^{\perp}\subseteq C_1^{\perp}"/>.</p>
<p>The next natural question is to ask whether such groups can in fact be found.</p>
<p>The Gilbert-Varshamov bound answers this question in the affirmative, ensuring that there do exist good CSS codes (the bound applies to both quantum and classical codes). It can be stated in the following way:</p>
<p><strong>Theorem </strong>(<em>Gilbert-Varshamov Bound</em>): There exist CSS codes with rate <img alt="R=" class="latex" src="https://s0.wp.com/latex.php?latex=R%3D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R="/>(number of encoded bits)/(length of code) given by</p>
<p><img alt="R\geq 1-2H_2(d/n)" class="latex" src="https://s0.wp.com/latex.php?latex=R%5Cgeq+1-2H_2%28d%2Fn%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R\geq 1-2H_2(d/n)"/>,</p>
<p>where <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/> is the minimum distance of the code, <img alt="d/2" class="latex" src="https://s0.wp.com/latex.php?latex=d%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d/2"/> is the number of errors it can correct, and <img alt="H_2(x)" class="latex" src="https://s0.wp.com/latex.php?latex=H_2%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_2(x)"/> is the Shannon entropy, defined as</p>
<p><img alt="H_2(x)=-x\log_2x-(1-x)\log_2(1-x)" class="latex" src="https://s0.wp.com/latex.php?latex=H_2%28x%29%3D-x%5Clog_2x-%281-x%29%5Clog_2%281-x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H_2(x)=-x\log_2x-(1-x)\log_2(1-x)"/>.</p>
<p><strong>Proof:</strong> Note that we can always take a code, apply a random linear transformation to it, and get another code. Thus each vector is equally likely to appear in a random code. Given this fact, we can estimate the probability that a code of dimension <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> contains a word of weight <img alt="\leq d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\leq d"/> using the union bound:</p>
<p><img alt="P(" class="latex" src="https://s0.wp.com/latex.php?latex=P%28&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P("/>code of dimension <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> has word of weight <img alt="\leq d)\leq(" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+d%29%5Cleq%28&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\leq d)\leq("/>number of words<img alt=")\times P(" class="latex" src="https://s0.wp.com/latex.php?latex=%29%5Ctimes+P%28&amp;bg=ffffff&amp;fg=333333&amp;s=0" title=")\times P("/>word has weight <img alt="\leq d)=2^k\cdot\frac{\sum_{i=0}^d \binom{n}{i}}{2^n}\approx \frac{2^k\cdot 2^{nH(d/n)}}{2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+d%29%3D2%5Ek%5Ccdot%5Cfrac%7B%5Csum_%7Bi%3D0%7D%5Ed+%5Cbinom%7Bn%7D%7Bi%7D%7D%7B2%5En%7D%5Capprox+%5Cfrac%7B2%5Ek%5Ccdot+2%5E%7BnH%28d%2Fn%29%7D%7D%7B2%5En%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\leq d)=2^k\cdot\frac{\sum_{i=0}^d \binom{n}{i}}{2^n}\approx \frac{2^k\cdot 2^{nH(d/n)}}{2^n}"/></p>
<p>For this to be a valid probability we need to have</p>
<p><img alt="(k/n)+H(d/n)&lt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%28k%2Fn%29%2BH%28d%2Fn%29%3C+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(k/n)+H(d/n)&lt; 1"/>.</p>
<p>We can calculate rate by noting that for a CSS code, given by <img alt="C_1\subseteq C_2" class="latex" src="https://s0.wp.com/latex.php?latex=C_1%5Csubseteq+C_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_1\subseteq C_2"/>, <img alt="C_2^{\perp}\subseteq C_1^{\perp}" class="latex" src="https://s0.wp.com/latex.php?latex=C_2%5E%7B%5Cperp%7D%5Csubseteq+C_1%5E%7B%5Cperp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C_2^{\perp}\subseteq C_1^{\perp}"/>, with <img alt="\text{dim}(C_1)=n-k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bdim%7D%28C_1%29%3Dn-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{dim}(C_1)=n-k"/>, <img alt="\text{dim}(C_2)=k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bdim%7D%28C_2%29%3Dk&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{dim}(C_2)=k"/>, the expression for rate is given by</p>
<p><img alt="R=\frac{n-2k}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=R%3D%5Cfrac%7Bn-2k%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R=\frac{n-2k}{n}"/>.</p>
<p>Combining this with the bound we obtained by considering probabilities, we get that</p>
<p><img alt="R\geq 1-2H(d/n)" class="latex" src="https://s0.wp.com/latex.php?latex=R%5Cgeq+1-2H%28d%2Fn%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R\geq 1-2H(d/n)"/>.</p>
<p>Thus there exist good CSS codes.</p>
<p><strong>3.7. Stabilizer Codes</strong><br/>
Having discussed and constructed some examples of CSS codes, we will now discuss the stabilizer formalism. Note that this formalism allows us to construct codes without having to work directly with the states representing <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> and <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/>, as this can quickly get unwieldy. Instead, we will work with stabilizers, operators that leave these states invariant.</p>
<p>To see how working directly with states can get unwieldy, we can consider the 5-qubit code. We can define it the way we defined the 9-qubit and 7-qubit codes, by directly defining the basis vectors <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> and <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/>,</p>
<p><img alt="|0\rangle_L\equiv\frac{1}{4}(|00000\rangle-|01100\rangle+|00101\rangle+|01010\rangle-|01111\rangle+(" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L%5Cequiv%5Cfrac%7B1%7D%7B4%7D%28%7C00000%5Crangle-%7C01100%5Crangle%2B%7C00101%5Crangle%2B%7C01010%5Crangle-%7C01111%5Crangle%2B%28&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L\equiv\frac{1}{4}(|00000\rangle-|01100\rangle+|00101\rangle+|01010\rangle-|01111\rangle+("/>symmetric under cyclic permutations<img alt="))" class="latex" src="https://s0.wp.com/latex.php?latex=%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="))"/>,</p>
<p>with <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/> defined similarly. But we can also define this code more succinctly using the stabilizer formalism. To do so, we start by choosing a commutative subgroup of the Pauli group, with generators <img alt="g_i" class="latex" src="https://s0.wp.com/latex.php?latex=g_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_i"/> satisfying</p>
<p><img alt="g_i^2=I" class="latex" src="https://s0.wp.com/latex.php?latex=g_i%5E2%3DI&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_i^2=I"/></p>
<p><img alt="g_ig_j=g_jg_i" class="latex" src="https://s0.wp.com/latex.php?latex=g_ig_j%3Dg_jg_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_ig_j=g_jg_i"/></p>
<p>For example, for the 5-qubit code, the particular choice of generators we would need is given by</p>
<p><img alt="g_1\equiv IXZZX" class="latex" src="https://s0.wp.com/latex.php?latex=g_1%5Cequiv+IXZZX&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_1\equiv IXZZX"/></p>
<p><img alt="g_2\equiv XIXZZ" class="latex" src="https://s0.wp.com/latex.php?latex=g_2%5Cequiv+XIXZZ&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_2\equiv XIXZZ"/></p>
<p><img alt="g_3\equiv ZXIXZ" class="latex" src="https://s0.wp.com/latex.php?latex=g_3%5Cequiv+ZXIXZ&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_3\equiv ZXIXZ"/></p>
<p><img alt="g_4\equiv ZZXIX" class="latex" src="https://s0.wp.com/latex.php?latex=g_4%5Cequiv+ZZXIX&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_4\equiv ZZXIX"/>.</p>
<p>Now we consider states <img alt="\{|\psi\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|\psi\rangle\}"/> that are stabilized by the <img alt="\{g_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bg_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{g_i\}"/>. That is, they satisfy</p>
<p><img alt="g_i|\psi\rangle=|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=g_i%7C%5Cpsi%5Crangle%3D%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_i|\psi\rangle=|\psi\rangle"/>.</p>
<p>Note that the eigenvalues of <img alt="\sigma_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x"/>, <img alt="\sigma_y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_y"/>, and <img alt="\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_z"/> are <img alt="\pm 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpm+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\pm 1"/>, so in the case of the 5-qubit code, there exists a <img alt="2^5/2=16" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E5%2F2%3D16&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^5/2=16"/>-dimensional space of <img alt="\{|\psi\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|\psi\rangle\}"/> satisfying <img alt="g_1|\psi\rangle=|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=g_1%7C%5Cpsi%5Crangle%3D%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_1|\psi\rangle=|\psi\rangle"/>. Recalling that two commuting matrices are simultaneously diagonalizable, there exists a <img alt="16/2=8" class="latex" src="https://s0.wp.com/latex.php?latex=16%2F2%3D8&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="16/2=8"/>-dimensional space of <img alt="\{|\psi\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|\psi\rangle\}"/> satisfying <img alt="g_1|\psi\rangle=g_2|\psi\rangle=|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=g_1%7C%5Cpsi%5Crangle%3Dg_2%7C%5Cpsi%5Crangle%3D%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_1|\psi\rangle=g_2|\psi\rangle=|\psi\rangle"/>, and so on, where we cut the dimension of the subspace in half each time we add a generator. Finally, there exists a <img alt="2^5/2^4=2" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E5%2F2%5E4%3D2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^5/2^4=2"/>-dimensional space of <img alt="\{|\psi\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|\psi\rangle\}"/> satisfying <img alt="g_i|\psi\rangle=|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=g_i%7C%5Cpsi%5Crangle%3D%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g_i|\psi\rangle=|\psi\rangle"/> for all <img alt="i=1,...,4" class="latex" src="https://s0.wp.com/latex.php?latex=i%3D1%2C...%2C4&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i=1,...,4"/>. This 2-dimensional space is exactly the subspace spanned by <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> and <img alt="|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle_L"/>. Thus fixing the stabilizers is enough to give us our code.</p>
<p>Next we will consider all elements in the Pauli group that commute with all elements in our stabilizer group <img alt="G=\{g_1,...,g_4\}" class="latex" src="https://s0.wp.com/latex.php?latex=G%3D%5C%7Bg_1%2C...%2Cg_4%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G=\{g_1,...,g_4\}"/>. As we shall see, this will give us our logical operators, where a <em>logical operator</em> performs an operation on a logical qubit (for example, the logical <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/> operator, <img alt="X_L" class="latex" src="https://s0.wp.com/latex.php?latex=X_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_L"/>, would act on the logical qubit <img alt="|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle_L"/> by mapping <img alt="X_L|0\rangle_L=|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=X_L%7C0%5Crangle_L%3D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_L|0\rangle_L=|1\rangle_L"/>, and so on). In the 5-qubit case we end up with a 6-dimensional nonabelian group <img alt="\tilde{G}=\langle g_1,...,g_4, h_1, h_2\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BG%7D%3D%5Clangle+g_1%2C...%2Cg_4%2C+h_1%2C+h_2%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tilde{G}=\langle g_1,...,g_4, h_1, h_2\rangle"/> by adding the following two elements to those elements that are in <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/>:</p>
<p><img alt="h_1=XXXXX" class="latex" src="https://s0.wp.com/latex.php?latex=h_1%3DXXXXX&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_1=XXXXX"/></p>
<p><img alt="h_2=ZZZZZ" class="latex" src="https://s0.wp.com/latex.php?latex=h_2%3DZZZZZ&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h_2=ZZZZZ"/></p>
<p>These will be our logical operators</p>
<p><img alt="X_L\equiv h_1" class="latex" src="https://s0.wp.com/latex.php?latex=X_L%5Cequiv+h_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_L\equiv h_1"/></p>
<p><img alt="Z_L\equiv h_2" class="latex" src="https://s0.wp.com/latex.php?latex=Z_L%5Cequiv+h_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_L\equiv h_2"/></p>
<p>so that</p>
<p><img alt="X_L|0\rangle_L=|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=X_L%7C0%5Crangle_L%3D%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_L|0\rangle_L=|1\rangle_L"/></p>
<p><img alt="X_L|1\rangle_L=|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=X_L%7C1%5Crangle_L%3D%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_L|1\rangle_L=|0\rangle_L"/></p>
<p><img alt="Z_L|1\rangle_L=-|1\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=Z_L%7C1%5Crangle_L%3D-%7C1%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_L|1\rangle_L=-|1\rangle_L"/></p>
<p><img alt="Z_L|0\rangle_L=|0\rangle_L" class="latex" src="https://s0.wp.com/latex.php?latex=Z_L%7C0%5Crangle_L%3D%7C0%5Crangle_L&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_L|0\rangle_L=|0\rangle_L"/>.</p>
<p>Note that this code has distance 3 and corrects 1 error because 3 is the minimum Hamming weight in the group <img alt="\tilde{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BG%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tilde{G}"/>. (To see this, note that <img alt="XXXXX\cdot IXZZX=XIYYI" class="latex" src="https://s0.wp.com/latex.php?latex=XXXXX%5Ccdot+IXZZX%3DXIYYI&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="XXXXX\cdot IXZZX=XIYYI"/> has Hamming weight 3.)</p>
<p>Why is Hamming weight 2 not enough to correct one error? If we had, for example, <img alt="XZIII\in\tilde{G}" class="latex" src="https://s0.wp.com/latex.php?latex=XZIII%5Cin%5Ctilde%7BG%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="XZIII\in\tilde{G}"/>, then we would have</p>
<p><img alt="\sigma_x^{(1)}|\psi_1\rangle=\sigma_z^{(2)}|\psi_2\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_x%5E%7B%281%29%7D%7C%5Cpsi_1%5Crangle%3D%5Csigma_z%5E%7B%282%29%7D%7C%5Cpsi_2%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_x^{(1)}|\psi_1\rangle=\sigma_z^{(2)}|\psi_2\rangle"/></p>
<p>for <img alt="|\psi_1\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_1%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi_1\rangle"/>, <img alt="|\psi_2\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_2%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi_2\rangle"/> both in the code, which means that we wouldn’t be able to distinguish an <img alt="X_1" class="latex" src="https://s0.wp.com/latex.php?latex=X_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X_1"/> error from a <img alt="Z_2" class="latex" src="https://s0.wp.com/latex.php?latex=Z_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_2"/> error.</p>
<p>Note that, in general, when <img alt="x\in\tilde{G}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin%5Ctilde%7BG%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x\in\tilde{G}"/>, <img alt="x|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=x%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x|\psi\rangle"/> will be in the code, so elements of <img alt="\tilde{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BG%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\tilde{G}"/> map codewords to codewords. We can prove this fact by noting that</p>
<p><img alt="xg_i|\psi\rangle=x|\psi\rangle=g_ix|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=xg_i%7C%5Cpsi%5Crangle%3Dx%7C%5Cpsi%5Crangle%3Dg_ix%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="xg_i|\psi\rangle=x|\psi\rangle=g_ix|\psi\rangle"/>.</p>
<p>Note also that in the examples we’ve been dealing with so far, where we have a commuting subgroup of the Pauli group, our codes correspond to classical, additive, weakly self-dual codes over <img alt="GF(4)" class="latex" src="https://s0.wp.com/latex.php?latex=GF%284%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="GF(4)"/>. Here <img alt="GF(4)=\{0,1,\omega,\bar{\omega}\}" class="latex" src="https://s0.wp.com/latex.php?latex=GF%284%29%3D%5C%7B0%2C1%2C%5Comega%2C%5Cbar%7B%5Comega%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="GF(4)=\{0,1,\omega,\bar{\omega}\}"/> (with group elements <img alt="\{\omega, \bar{\omega},1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Comega%2C+%5Cbar%7B%5Comega%7D%2C1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\omega, \bar{\omega},1\}"/> corresponding to the third roots of unity) is the finite field on 4 elements, and multiplying Pauli matrices corresponds to group addition. Specifically,</p>
<p><img alt="X\equiv 1" class="latex" src="https://s0.wp.com/latex.php?latex=X%5Cequiv+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X\equiv 1"/></p>
<p><img alt="Y\equiv \omega" class="latex" src="https://s0.wp.com/latex.php?latex=Y%5Cequiv+%5Comega&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y\equiv \omega"/></p>
<p><img alt="Z\equiv \bar{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=Z%5Cequiv+%5Cbar%7B%5Comega%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z\equiv \bar{\omega}"/></p>
<p><img alt="I\equiv 0" class="latex" src="https://s0.wp.com/latex.php?latex=I%5Cequiv+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="I\equiv 0"/></p>
<p>satisfying</p>
<p><img alt="H\omega=\bar{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=H%5Comega%3D%5Cbar%7B%5Comega%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H\omega=\bar{\omega}"/></p>
<p><img alt="2X=2Y=2Z=0" class="latex" src="https://s0.wp.com/latex.php?latex=2X%3D2Y%3D2Z%3D0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2X=2Y=2Z=0"/>.</p>
<p>We have now concluded our discussion of quantum error-correcting codes. In the next section we will shift gears and look at quantum channels and channel capacities.</p>
<p><strong>4. Quantum Channels</strong><br/>
In this final section we will look at quantum channels and channel capacities.</p>
<p><strong>4.1. Definition and Examples</strong></p>
<p><strong>4.1.1. Definition</strong><br/>
We know that we want to define a quantum channel to take a quantum state as input. What should the output be? As a first attempt we might imagine having the output be a probability distribution <img alt="\{p_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bp_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{p_i\}"/> over states <img alt="\{|\psi_i\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C%5Cpsi_i%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|\psi_i\rangle\}"/>. It turns out that for a more succinct description, we can have both the input and output be a density matrix.</p>
<p>Recall that a density matrix takes the form</p>
<p><img alt="\rho=\sum_i p_i|\psi_i\rangle\langle\psi_i|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%3D%5Csum_i+p_i%7C%5Cpsi_i%5Crangle%5Clangle%5Cpsi_i%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho=\sum_i p_i|\psi_i\rangle\langle\psi_i|"/></p>
<p>representing a probability distribution over pure states <img alt="|\psi_i\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi_i%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi_i\rangle"/>. <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> must also be Hermitian, and it must satisfy <img alt="\text{Tr}(\rho)=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BTr%7D%28%5Crho%29%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{Tr}(\rho)=1"/> (equivalently, we must have <img alt="\sum_i p_i=1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_i+p_i%3D1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_i p_i=1"/>).</p>
<p>Now we may define a quantum channel as the map <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta"/> that takes</p>
<p><img alt="\eta:\rho\mapsto\sum_i E_i\rho E_i^{\dagger}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3A%5Crho%5Cmapsto%5Csum_i+E_i%5Crho+E_i%5E%7B%5Cdagger%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta:\rho\mapsto\sum_i E_i\rho E_i^{\dagger}"/>,</p>
<p>where</p>
<p><img alt="\sum_i E_i^{\dagger}E_i=I" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_i+E_i%5E%7B%5Cdagger%7DE_i%3DI&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sum_i E_i^{\dagger}E_i=I"/>.</p>
<p>To see that the output is in fact a density matrix, note that the output expression is clearly Hermitian and can be shown to have unit trace using the cyclical property of traces. Note also that the decomposition into <img alt="\{E_i\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BE_i%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{E_i\}"/> need not be unique.</p>
<p><strong>4.1.2. Example Quantum Channels</strong><br/>
Next we give a few examples of quantum channels. The dephasing channel is given by the map</p>
<p><img alt="\rho\mapsto(1-p)\rho+p\sigma_z\rho\sigma_z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%5Cmapsto%281-p%29%5Crho%2Bp%5Csigma_z%5Crho%5Csigma_z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho\mapsto(1-p)\rho+p\sigma_z\rho\sigma_z"/>.</p>
<p>It maps</p>
<p><img alt="\left(\begin{array}{cc}\alpha&amp;\beta\\\gamma&amp;\delta\end{array}\right)\mapsto \left(\begin{array}{cc}\alpha&amp;(1-2p)\beta\\(1-2p)\gamma&amp;\delta\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%5Calpha%26%5Cbeta%5C%5C%5Cgamma%26%5Cdelta%5Cend%7Barray%7D%5Cright%29%5Cmapsto+%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%5Calpha%26%281-2p%29%5Cbeta%5C%5C%281-2p%29%5Cgamma%26%5Cdelta%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left(\begin{array}{cc}\alpha&amp;\beta\\\gamma&amp;\delta\end{array}\right)\mapsto \left(\begin{array}{cc}\alpha&amp;(1-2p)\beta\\(1-2p)\gamma&amp;\delta\end{array}\right)"/></p>
<p><img alt="\left(\begin{array}{cc}\alpha&amp;\beta\\\gamma&amp;\delta\end{array}\right)\mapsto \left(\begin{array}{cc}\alpha&amp;(1-2p)\beta\\(1-2p)\gamma&amp;\delta\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%5Calpha%26%5Cbeta%5C%5C%5Cgamma%26%5Cdelta%5Cend%7Barray%7D%5Cright%29%5Cmapsto+%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%5Calpha%26%281-2p%29%5Cbeta%5C%5C%281-2p%29%5Cgamma%26%5Cdelta%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\left(\begin{array}{cc}\alpha&amp;\beta\\\gamma&amp;\delta\end{array}\right)\mapsto \left(\begin{array}{cc}\alpha&amp;(1-2p)\beta\\(1-2p)\gamma&amp;\delta\end{array}\right)"/>,</p>
<p>so it multiplies off-diagonal elements by a factor that is less than 1. Note that when <img alt="p=1/2" class="latex" src="https://s0.wp.com/latex.php?latex=p%3D1%2F2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p=1/2"/>, it maps</p>
<p><img alt="\alpha|0\rangle+\beta|1\rangle\mapsto|\alpha|^2|0\rangle\langle 0|+|\beta|^2|1\rangle\langle 1|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%7C0%5Crangle%2B%5Cbeta%7C1%5Crangle%5Cmapsto%7C%5Calpha%7C%5E2%7C0%5Crangle%5Clangle+0%7C%2B%7C%5Cbeta%7C%5E2%7C1%5Crangle%5Clangle+1%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\alpha|0\rangle+\beta|1\rangle\mapsto|\alpha|^2|0\rangle\langle 0|+|\beta|^2|1\rangle\langle 1|"/>,</p>
<p>which means that it turns superpositions into classical mixtures (hence the name “dephasing”).</p>
<p>Another example is the amplitude damping channel, which models an excited state decaying to a ground state. It is given by</p>
<p><img alt="E_1=\left(\begin{array}{cc}0&amp;\sqrt{p}\\0&amp;0\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=E_1%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D0%26%5Csqrt%7Bp%7D%5C%5C0%260%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_1=\left(\begin{array}{cc}0&amp;\sqrt{p}\\0&amp;0\end{array}\right)"/></p>
<p><img alt="E_2=\left(\begin{array}{cc}1&amp;0\\0&amp;\sqrt{1-p}\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=E_2%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D1%260%5C%5C0%26%5Csqrt%7B1-p%7D%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="E_2=\left(\begin{array}{cc}1&amp;0\\0&amp;\sqrt{1-p}\end{array}\right)"/></p>
<p>Here we let the vector <img alt="|0\rangle=(1, 0)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%3D%281%2C+0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle=(1, 0)"/> denote the ground state, and we let the vector <img alt="|1\rangle=(0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle%3D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle=(0, 1)"/> denote the excited state. Thus we can see that the channel maps the ground state to itself, <img alt="|0\rangle\mapsto|0\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle%5Cmapsto%7C0%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle\mapsto|0\rangle"/>, while the excited state <img alt="|1\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle"/> gets mapped to <img alt="|0\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C0%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|0\rangle"/> with probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> and stays at <img alt="|1\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C1%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|1\rangle"/> with probability <img alt="1-p" class="latex" src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1-p"/>.</p>
<p><strong>4.2 Quantum Channel Capacities</strong><br/>
Now we consider the capacity of quantum channels, where the capacity quantifies how much information can make it through the channel. We consider classical channels, classical information sent over quantum channels, and quantum information sent over quantum channels. First we start off with the example of the quantum erasure channel to demonstrate that quantum channels behave differently from classical channels, and then we give the actual expressions for the channel capacities before revisiting the example of the quantum erasure channel.</p>
<p><strong>4.2.1 Example: Quantum Erasure Channel</strong><br/>
First we start with the example of the quantum erasure channel, which given a state <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/> replaces it by an orthogonal state <img alt="|E\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7CE%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|E\rangle"/> with probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> and returns the same state <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/> with probability <img alt="1-p" class="latex" src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1-p"/>. We claim that the erasure channel can’t transmit quantum information when <img alt="p\geq 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p\geq 0.5"/>, behavior that is markedly different from that of classical information. That is to say, for <img alt="p\geq 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p\geq 0.5"/>, there is no way to encode quantum information to send it through the channel and then decode it so the receiver gets a state close to the state that was sent.</p>
<p>To see why this is the case, assume the contrary, that there do exist encoding and decoding protocols that send quantum information through quantum erasure channels with erasure rate <img alt="p\geq 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p\geq 0.5"/>. We will show that this violates the no-cloning theorem. Now, suppose that <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> does the following: For each qubit in the encoded state, she tosses a fair coin. If the coin lands heads, she send <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> the state <img alt="|E\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7CE%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|E\rangle"/> and sends <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> the channel input with probability <img alt="2p-1" class="latex" src="https://s0.wp.com/latex.php?latex=2p-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2p-1"/> and the erasure state <img alt="|E\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7CE%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|E\rangle"/> otherwise. If the coin lands tails, she sends <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> the state <img alt="|E\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7CE%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|E\rangle"/> and sends <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> the channel input with probability <img alt="2p-1" class="latex" src="https://s0.wp.com/latex.php?latex=2p-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2p-1"/> and the erasure state otherwise. This implements a <img alt="p\geq 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p\geq 0.5"/> channel to both receivers <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="B"/> and <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/>, which means that <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> can use this channel to transmit an encoding of <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/> to both receivers, which in turn means that both receivers will be able to decode <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/>. But this means that <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> has just used this channel to clone the quantum state <img alt="|\psi\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cpsi%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\psi\rangle"/>, resulting in a contradiction. Thus no quantum information can be transmitted through a channel with <img alt="p\geq 0.5" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cgeq+0.5&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p\geq 0.5"/>. Note, however, that we can send classical information over this channel, so the behavior of quantum and classical information is markedly different.</p>
<p>It turns out that the rate of quantum information sent over the erasure channel, as a function of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>, is given by the following graph:</p>
<p><img alt="fig4.png" class="alignnone size-full wp-image-6318" src="https://windowsontheory.files.wordpress.com/2018/12/fig4.png?w=600"/><br/>
while the rate of classical information sent over the erasure channel, as a function of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>, is given by the following graph:</p>
<p><img alt="fig5.png" class="alignnone size-full wp-image-6319" src="https://windowsontheory.files.wordpress.com/2018/12/fig5.png?w=600"/></p>
<p>Next we will formally state the definition of channel capacity, and then we will return to the quantum erasure channel example and derive the curve that plots rate against <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>.</p>
<p><strong>4.2.2. Definition of Channel Capacities</strong><br/>
Channel capacity is defined as the maximum rate at which information can be communicated over many independent uses of a channel from sender to receiver. Here we list the expressions for channel capacity for classical channels, classical information over a quantum channel, and quantum information over a quantum channel.</p>
<p><strong>Classical Channel Capacity</strong> For a classical channel this expression is just the maximum mutual information over all input-output pairs,</p>
<p><img alt="\max_X H(\eta(X))-H(\eta(X)|X)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_X+H%28%5Ceta%28X%29%29-H%28%5Ceta%28X%29%7CX%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\max_X H(\eta(X))-H(\eta(X)|X)"/>,</p>
<p>where <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/> is the input information and <img alt="\eta(X)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%28X%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta(X)"/> is the output information after having gone through the channel <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta"/>.</p>
<p><strong>Classical Information Over a Quantum Channel </strong>The capacity for classical information sent over a quantum channel is given by</p>
<p><img alt="\max_{\{p_i,\rho_i\}} H(\eta(\rho))-\sum_i p_iH(\eta(\rho_i))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%7B%5C%7Bp_i%2C%5Crho_i%5C%7D%7D+H%28%5Ceta%28%5Crho%29%29-%5Csum_i+p_iH%28%5Ceta%28%5Crho_i%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\max_{\{p_i,\rho_i\}} H(\eta(\rho))-\sum_i p_iH(\eta(\rho_i))"/></p>
<p>up to regularization, where <img alt="\rho=\sum_i p_i\rho_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%3D%5Csum_i+p_i%5Crho_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho=\sum_i p_i\rho_i"/> is the average input state, and <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta"/> is the channel.</p>
<p>Note that we would regularize this by using <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> copies of the state (that is to say, we want the output of <img alt="\eta^{\otimes n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%5E%7B%5Cotimes+n%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta^{\otimes n}"/>) and then dividing by <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>, to get an expression like the following for the regularized capacity of classical information over a quantum channel:</p>
<p><img alt="\lim_{n\rightarrow\infty}\max_{\{p_i,\rho_i\}} [H(\eta(\rho)^{\otimes n})-\sum_i p_i H(\eta(\rho_i)^{\otimes n})]/n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn%5Crightarrow%5Cinfty%7D%5Cmax_%7B%5C%7Bp_i%2C%5Crho_i%5C%7D%7D+%5BH%28%5Ceta%28%5Crho%29%5E%7B%5Cotimes+n%7D%29-%5Csum_i+p_i+H%28%5Ceta%28%5Crho_i%29%5E%7B%5Cotimes+n%7D%29%5D%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lim_{n\rightarrow\infty}\max_{\{p_i,\rho_i\}} [H(\eta(\rho)^{\otimes n})-\sum_i p_i H(\eta(\rho_i)^{\otimes n})]/n"/>.</p>
<p><strong>Quantum Information</strong> The capacity for quantum information is given by the expression</p>
<p><img alt="\max_\rho H(\eta(\rho))-H((\eta\otimes I)\Phi_\rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%5Crho+H%28%5Ceta%28%5Crho%29%29-H%28%28%5Ceta%5Cotimes+I%29%5CPhi_%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\max_\rho H(\eta(\rho))-H((\eta\otimes I)\Phi_\rho)"/>,</p>
<p>also up to regularization. Here <img alt="\eta(\rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%28%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta(\rho)"/> is the output when channel <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> acts on input state <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/>, while <img alt="\Phi_\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPhi_%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Phi_\rho"/> is the purification of <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> (that is, it is a pure state containing <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> that we can obtain by enlarging the Hilbert space). The regularized capacity for quantum information looks like the following:</p>
<p><img alt="&#xA0;\lim_{n\rightarrow\infty}\max_\rho [H(\eta(\rho)^{\otimes n})-H((\eta\otimes I)(\Phi_\rho)^{\otimes n})]/n" class="latex" src="https://s0.wp.com/latex.php?latex=%C2%A0%5Clim_%7Bn%5Crightarrow%5Cinfty%7D%5Cmax_%5Crho+%5BH%28%5Ceta%28%5Crho%29%5E%7B%5Cotimes+n%7D%29-H%28%28%5Ceta%5Cotimes+I%29%28%5CPhi_%5Crho%29%5E%7B%5Cotimes+n%7D%29%5D%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&#xA0;\lim_{n\rightarrow\infty}\max_\rho [H(\eta(\rho)^{\otimes n})-H((\eta\otimes I)(\Phi_\rho)^{\otimes n})]/n"/>.</p>
<p>Now that we have the exact expression that allows us to calculate the quantum channel capacity, we will revisit our example of the quantum erasure channel and reproduce the plot of channel rate vs erasure probability.</p>
<p><strong>4.2.3. Example Revisited: Quantum Erasure Channel</strong><br/>
Recall that, up to regularization, the capacity of a quantum channel is given by</p>
<p><img alt="\max_\rho H(\eta(\rho))-H((\eta\otimes I)\Phi_\rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%5Crho+H%28%5Ceta%28%5Crho%29%29-H%28%28%5Ceta%5Cotimes+I%29%5CPhi_%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\max_\rho H(\eta(\rho))-H((\eta\otimes I)\Phi_\rho)"/>.</p>
<p>We will directly calculate this expression for the example of the quantum erasure channel. Let the input <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> be given by the density matrix for the completely mixed state,</p>
<p><img alt="\rho=\left(\begin{array}{cc}\frac{1}{2}&amp;0\\0&amp;\frac{1}{2}\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%5Cfrac%7B1%7D%7B2%7D%260%5C%5C0%26%5Cfrac%7B1%7D%7B2%7D%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho=\left(\begin{array}{cc}\frac{1}{2}&amp;0\\0&amp;\frac{1}{2}\end{array}\right)"/>,</p>
<p>so that the purification of <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> is given by the state</p>
<p><img alt="\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%7C00%5Crangle%2B%7C11%5Crangle%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)"/>.</p>
<p>Recall that the erasure channel replaces our state with <img alt="|E\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%7CE%5Crangle&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|E\rangle"/> with probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/>, while with probability <img alt="1-p" class="latex" src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1-p"/> it leaves the input state unchanged. Then, in the basis <img alt="\{|0\rangle, |1\rangle, |E\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C0%5Crangle%2C+%7C1%5Crangle%2C+%7CE%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|0\rangle, |1\rangle, |E\rangle\}"/>, the matrix corresponding to <img alt="\eta(\rho)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%28%5Crho%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta(\rho)"/> is given by</p>
<p><img alt="\eta(\rho)=\left(\begin{array}{ccc}\frac{1-p}{2}&amp;0&amp;0\\0&amp;\frac{1-p}{2}&amp;0\\0&amp;0&amp;p\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%28%5Crho%29%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bccc%7D%5Cfrac%7B1-p%7D%7B2%7D%260%260%5C%5C0%26%5Cfrac%7B1-p%7D%7B2%7D%260%5C%5C0%260%26p%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\eta(\rho)=\left(\begin{array}{ccc}\frac{1-p}{2}&amp;0&amp;0\\0&amp;\frac{1-p}{2}&amp;0\\0&amp;0&amp;p\end{array}\right)"/></p>
<p>while in the basis <img alt="\{|00\rangle, |01\rangle, |10\rangle, |11\rangle, |0E\rangle, |1E\rangle\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7C00%5Crangle%2C+%7C01%5Crangle%2C+%7C10%5Crangle%2C+%7C11%5Crangle%2C+%7C0E%5Crangle%2C+%7C1E%5Crangle%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{|00\rangle, |01\rangle, |10\rangle, |11\rangle, |0E\rangle, |1E\rangle\}"/>, the matrix corresponding to <img alt="(\eta\otimes I)\Phi_\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Ceta%5Cotimes+I%29%5CPhi_%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\eta\otimes I)\Phi_\rho"/> is given by</p>
<p><img alt="(\eta\otimes I)\Phi_\rho=\left(\begin{array}{cccccc}\frac{1-p}{2}&amp;0&amp;0&amp;\frac{1-p}{2}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;0\\\frac{1-p}{2}&amp;0&amp;0&amp;\frac{1-p}{2}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;\frac{p}{2}&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;\frac{p}{2}\end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Ceta%5Cotimes+I%29%5CPhi_%5Crho%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcccccc%7D%5Cfrac%7B1-p%7D%7B2%7D%260%260%26%5Cfrac%7B1-p%7D%7B2%7D%260%260%5C%5C0%260%260%260%260%260%5C%5C0%260%260%260%260%260%5C%5C%5Cfrac%7B1-p%7D%7B2%7D%260%260%26%5Cfrac%7B1-p%7D%7B2%7D%260%260%5C%5C0%260%260%260%26%5Cfrac%7Bp%7D%7B2%7D%260%5C%5C0%260%260%260%260%26%5Cfrac%7Bp%7D%7B2%7D%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\eta\otimes I)\Phi_\rho=\left(\begin{array}{cccccc}\frac{1-p}{2}&amp;0&amp;0&amp;\frac{1-p}{2}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;0\\\frac{1-p}{2}&amp;0&amp;0&amp;\frac{1-p}{2}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;\frac{p}{2}&amp;0\\0&amp;0&amp;0&amp;0&amp;0&amp;\frac{p}{2}\end{array}\right)"/></p>
<p>We can directly calculate that</p>
<p><img alt="H(\eta(\rho))=H_2(p)+(1-p)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28%5Ceta%28%5Crho%29%29%3DH_2%28p%29%2B%281-p%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H(\eta(\rho))=H_2(p)+(1-p)"/></p>
<p><img alt="H((\eta\otimes I)\Phi_\rho)=H_2(p)+p" class="latex" src="https://s0.wp.com/latex.php?latex=H%28%28%5Ceta%5Cotimes+I%29%5CPhi_%5Crho%29%3DH_2%28p%29%2Bp&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H((\eta\otimes I)\Phi_\rho)=H_2(p)+p"/>.</p>
<p>Then, subtracting the two entropies, we can calculate the rate to be</p>
<p><img alt="R=1-2p" class="latex" src="https://s0.wp.com/latex.php?latex=R%3D1-2p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R=1-2p"/>,</p>
<p>which corresponds exactly to the line we saw on the diagram that plotted rate as a function of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> for the quantum erasure channel.</p>
<p><strong>References</strong></p>
<ol>
<li>Bennett, C. H., DiVencenzo, D. P., and Smolin, J. A. Capacities of quantum erasure channels. <em>Phys. Rev. Lett</em>., 78:3217-3220 (1997). quant-ph/9701015.</li>
<li>Bennett, C. H., DiVencenzo, D. P., Smolin, J. A., and Wootters, W. K. Mixed state entanglement and quantum error correction. <em>Phys. Rev. A,</em> 54:3824 (1996). quant-ph/9604024.</li>
<li>Calderbank, A. R. and Shor, P. W. Good quantum error-correcting codes exist. <em>Phys. Rev. A</em>, 54:1098 (1996). quant-ph/9512032.</li>
<li>Devetak, I. The Private Classical Capacity and Quantum Capacity of a Quantum Channel. <em>IEEE Trans. Inf. Theor</em>., 51:44-45 (2005). quant-ph/0304127</li>
<li>Devetak, I. and Winter, A. Classical data compression with quantum side information. <em>Phys. Rev. A</em>, 68(4):042301 (2003).</li>
<li>Gottesman, D. Class of quantum error-correcting codes saturating the quantum Hamming bound. <em>Phys. Rev. A</em>, 54:1862 (1996).</li>
<li>Laflamme, R., Miquel, C., Paz, J.-P., and Zurek, W. H. Perfect quantum error correction code. <em>Phys. Rev. Lett</em>., 77:198 (1996). quant-ph/9602019.</li>
<li>Lloyd, S. Capacity of the noisy quantum channel. <em>Phys. Rev. A</em>., 55:3 (1997). quant-ph/9604015.</li>
<li>Nielsen, M. A. and Chuang, I. L. <em>Quantum Computation and Quantum Information</em>., Cambridge University Press, New York (2011).</li>
<li>Shor, P. W. Scheme for reducing decoherence in quantum computer memory. <em>Phys. Rev. A</em>., 52:2493 (1995).</li>
<li>Shor, P. W. The quantum channel capacity and coherent information. <em>MSRI Workshop on Quantum Computation</em> (2002).</li>
<li>Steane, A. M. Error correcting codes in quantum theory. <em>Phys. Rev. Lett</em>., 77:793 (1996).</li>
<li>Steane, A. M. Multiple particle interference and quantum error correction. <em>Proc. R. Soc. London A</em>, 452:2551-76 (1996).</li>
</ol></div>
    </content>
    <updated>2018-12-07T15:12:52Z</updated>
    <published>2018-12-07T15:12:52Z</published>
    <category term="physics"/>
    <author>
      <name>anniewei</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2018-12-19T17:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2018/12/07/faculty-at-university-of-bristol-apply-by-january-13-2019/</id>
    <link href="https://cstheory-jobs.org/2018/12/07/faculty-at-university-of-bristol-apply-by-january-13-2019/" rel="alternate" type="text/html"/>
    <title>Faculty at University of Bristol (apply by January 13, 2019)</title>
    <summary>The Computer Science department of the University of Bristol invites applications for a full-time faculty member at the Lecturer/Senior Lecturer level (comparable to tenure-track Assistant/Associate Professorships). We are looking for exceptional candidates in the areas of algorithms and complexity. Website: https://goo.gl/ngvAw9 Email: raphael.clifford@bristol.ac.uk
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science department of the University of Bristol invites applications for a full-time faculty member at the Lecturer/Senior Lecturer level (comparable to tenure-track Assistant/Associate Professorships). We are looking for exceptional candidates in the areas of algorithms and complexity.</p>
<p>Website: <a href="https://goo.gl/ngvAw9">https://goo.gl/ngvAw9</a><br/>
Email: raphael.clifford@bristol.ac.uk</p></div>
    </content>
    <updated>2018-12-07T11:00:13Z</updated>
    <published>2018-12-07T11:00:13Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2018-12-19T17:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=332</id>
    <link href="https://tcsplus.wordpress.com/2018/12/06/tcs-talk-wednesday-december-12-julia-chuzhoy-ttic/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, December 12 — Julia Chuzhoy, TTIC</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, December 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Julia Chuzhoy from TTIC will speak about “Almost Polynomial Hardness of Node-Disjoint Paths in Grids” (abstract below). Please make sure you reserve a spot for your group to […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, December 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Julia Chuzhoy</strong> from TTIC will speak about “<em>Almost Polynomial Hardness of Node-Disjoint Paths in Grids</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: In the classical Node-Disjoint Paths (NDP) problem, we are given an n-vertex graph G, and a collection of pairs of its vertices, called demand pairs. The goal is to route as many of the demand pairs as possible, where to route a pair we need to select a path connecting it, so that all selected paths are disjoint in their vertices.</p>
<p>The best current algorithm for NDP achieves an <img alt="O(\sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%5Csqrt%7Bn%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(\sqrt{n})"/>-approximation, while, until recently, the best negative result was a roughly <img alt="\Omega(\sqrt{\log n})" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28%5Csqrt%7B%5Clog+n%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Omega(\sqrt{\log n})"/>-hardness of approximation. Recently, an improved <img alt="2^{\Omega(\sqrt{\log n})}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B%5COmega%28%5Csqrt%7B%5Clog+n%7D%29%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="2^{\Omega(\sqrt{\log n})}"/>-hardness of approximation for NDP was shown, even if the underlying graph is a subgraph of a grid graph, and all source vertices lie on the boundary of the grid. Unfortunately, this result does not extend to grid graphs.</p>
<p>The approximability of NDP in grids has remained a tantalizing open question, with the best upper bound of <img alt="\tilde{O}(n^{1/4})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BO%7D%28n%5E%7B1%2F4%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\tilde{O}(n^{1/4})"/>, and the best lower bound of APX-hardness. In this talk we come close to resolving this question, by showing an almost polynomial hardness of approximation for NDP in grid graphs.</p>
<p>Our hardness proof performs a reduction from the 3COL(5) problem to NDP, using a new graph partitioning problem as a proxy. Unlike the more standard approach of employing Karp reductions to prove hardness of approximation, our proof is a Cook-type reduction, where, given an input instance of 3COL(5), we produce a large number of instances of NDP, and apply an approximation algorithm for NDP to each of them. The construction of each new instance of NDP crucially depends on the solutions to the previous instances that were found by the approximation algorithm.</p>
<p>Joint work with David H.K. Kim and Rachit Nimavat.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2018-12-06T21:04:07Z</updated>
    <published>2018-12-06T21:04:07Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2018-12-19T17:22:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=602</id>
    <link href="https://emanueleviola.wordpress.com/2018/12/06/symmetry/" rel="alternate" type="text/html"/>
    <title>Symmetry</title>
    <summary>Yesterday at the Simon institute there was a fun talk about The edge of physics by its author, Anil Ananthaswamy. To divulgate the theory of computation isn’t as easy, since you can’t talk about mega experiments done at the South Pole or massive telescopes built on top of mountains.  (It would also be a lot […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: justify;">Yesterday at the Simon institute there was a fun talk about <em>T<a href="https://www.amazon.com/Edge-Physics-Journey-Extremes-Universe/dp/0547394527">he edge of physics</a></em> by its author, <span class="a-size-large a-text-bold">Anil Ananthaswamy. To divulgate the theory of computation isn’t as easy, since you can’t talk about mega experiments done at the South Pole or massive telescopes built on top of mountains.  (It would also be a lot easier if we could resolve P vs. NP.)  For some inspiration we can look at books related to mathematics.  Here I would like to recommend <em><a href="https://www.amazon.com/Symmetry-Journey-into-Patterns-Nature/dp/0060789417">Symmetry</a></em>, by <span class="author notFaded"><span class="a-size-small a-color-secondary">Marcus du Sautoy</span>.  I enjoyed very much reading and re-reading this book, much more than his previous book <em>The music of the primes</em>, which I don’t really recommend.  <em>Symmetry</em> is a gripping history of group theory.  The purpose isn’t so much explaining the math as making you excited about the historical developments of the theory and the people that worked and are working on it.</span></span></p></div>
    </content>
    <updated>2018-12-06T20:01:49Z</updated>
    <published>2018-12-06T20:01:49Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Emanuele</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>By Emanuele Viola</subtitle>
      <title>Thoughts</title>
      <updated>2018-12-19T17:20:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2018/208</id>
    <link href="https://eccc.weizmann.ac.il/report/2018/208" rel="alternate" type="text/html"/>
    <title>TR18-208 |  Placing Conditional Disclosure of Secrets in the Communication Complexity Universe | 

	Benny Applebaum, 

	Prashant Nalini Vasudevan</title>
    <summary>In the *Conditional Disclosure of Secrets* (CDS) problem (Gertner et al., J. Comput. Syst. Sci., 2000) Alice and Bob, who hold $n$-bit inputs $x$ and $y$ respectively, wish to release a common secret $z$ to Carol (who knows both $x$ and $y$) if and only if the input $(x,y)$ satisfies some predefined predicate $f$. Alice and Bob are allowed to send a single message to Carol which may depend on their inputs and some shared randomness, and the goal is to minimize the communication complexity while providing information-theoretic security.

Despite the growing interest in this model, very few lower-bounds are known. In this paper, we relate the CDS complexity of a predicate $f$ to its communication complexity under various communication games. For several basic predicates our results yield tight, or almost tight, lower-bounds of $\Omega(n)$ or $\Omega(n^{1-\epsilon})$, providing an exponential improvement over previous logarithmic lower-bounds.

We also define new communication complexity classes that correspond to different variants of the CDS model and study the relations between them and their complements. Notably, we show that allowing for imperfect correctness can significantly reduce communication -- a seemingly new phenomenon in the context of information-theoretic cryptography. Finally, our results show that proving explicit super-logarithmic lower-bounds for imperfect CDS protocols is a necessary step towards proving explicit lower-bounds against the class AM, or even $\text{AM}\cap \text{co-AM}$ -- a well known open problem in the theory of communication complexity. Thus imperfect CDS forms a new minimal class which is placed just beyond the boundaries of the ``civilized'' part of the communication complexity world for which explicit lower-bounds are known.
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-06T10:32:29Z</updated>
    <published>2018-12-06T10:32:29Z</published>
    <source>
      <id>https://example.com/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://example.com/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2018-12-19T17:22:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=16599</id>
    <link href="https://gilkalai.wordpress.com/2018/12/05/igor-pak-will-give-the-2018-erdos-lectures/" rel="alternate" type="text/html"/>
    <title>Igor Pak will give the 2018 Erdős Lectures</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">  Next week Igor Pak will give the 2018 Erdős Lectures (delayed from June) Here is the poster   Combinatorics — Erdos lecture: Igor Pak (UCLA) “Counting linear extensions” Monday December 10  11:00-13:00 Location:  IIAS Hall 130, Feldman building,  Givat Ram     … <a href="https://gilkalai.wordpress.com/2018/12/05/igor-pak-will-give-the-2018-erdos-lectures/">Continue reading <span class="meta-nav">→</span></a></div>
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2018/06/pak.jpg"><img alt="" class="alignnone size-full wp-image-16201" src="https://gilkalai.files.wordpress.com/2018/06/pak.jpg?w=640"/></a></p>
<p> </p>
<h3>Next week Igor Pak will give the 2018 Erdős Lectures (delayed from June)</h3>
<p>Here is the poster</p>
<p> </p>
<article class="node node-event node-teaser article event-start clearfix" id="node-56026">
<div class="event-content">
<header class="node-header">
<h2 class="node-title">Combinatorics — Erdos lecture: Igor Pak (UCLA) “Counting linear extensions”</h2>
</header>
<div class="node-content">
<div class="field field-name-field-date field-type-datetime field-label-hidden view-mode-teaser">
<div class="field-items">
<div class="field-item even">
<h3 class="event-start"><span class="event-year">Monday December 10  11:00-13:00</span></h3>
</div>
</div>
</div>
<section class="field field-name-field-event-location field-type-text field-label-inline clearfix view-mode-teaser">
<h3 class="field-label">Location:  <span class="event-year">IIAS Hall 130, Feldman building, </span> Givat Ram</h3>
<p> </p>
</section>
<div class="field field-name-body field-type-text-with-summary field-label-hidden view-mode-teaser">
<div class="field-items">
<p> </p>
<p class="field-item even">I will survey various known and recent results on counting the number of linear extensions of finite posets. I will emphasize the asymptotic and complexity aspects for special families, where the problem is especially elegant yet remains #P-complete.</p>
<div/>
<div/>
</div>
</div>
</div>
</div>
</article>
<article class="node node-event node-teaser article event-start clearfix" id="node-58788">
<div class="event-content">
<header class="node-header">
<h2 class="node-title">CS Theory — Erdős Lecture: Igor Pak (UCLA) “Counting Young tableaux”</h2>
</header>
<div class="node-content">
<section class="field field-name-field-lecturer field-type-text field-label-above view-mode-_custom_display">
<div class="field-items">
<div class="field-item even">
<h3 class="event-start"><span class="event-year">Wednesday, Dec. 12,  2018, <span class="date-display-single"><span class="date-display-start">10:30am</span> to <span class="date-display-end">12:00pm</span></span></span></h3>
</div>
</div>
</section>
<div class="field-items">
<div class="field-item even"/>
</div>
<section class="field field-name-field-event-location field-type-text field-label-inline clearfix view-mode-teaser">
<h3 class="field-label">Location: Rothberg (CS building) B-220</h3>
</section>
<div class="field field-name-body field-type-text-with-summary field-label-hidden view-mode-teaser">
<div class="field-items">
<div class="field-item even">
<p>The number of standard Young tableaux of skew shape is a mesmerizing special case of the number of linear extensions of posets, that is important for applications in representation theory and algebraic geometry.  In this case there is a determinant formula, but finding their asymptotics is a difficult challenge.  I will survey some of the many beautiful result on the subject, explain some surprising product formulas, connections to Selberg integral, lozenge tilings and certain particle systems.</p>
</div>
</div>
</div>
</div>
</div>
</article>
<article class="node node-event node-teaser article event-start clearfix" id="node-51625">
<div class="event-content">
<header class="node-header">
<h2 class="node-title">Colloquium: Erdos lecture – Igor Pak (UCLA) “Counting integer points in polytopes”</h2>
</header>
<div class="node-content">
<div class="field field-name-field-date field-type-datetime field-label-hidden view-mode-teaser">
<div class="field-items">
<div class="field-item even">
<h3 class="event-start"><span class="event-year">Thursday, </span><span class="event-start-day"> Dec 13, 2018 <span class="date-display-single"><span class="date-display-start">2:30pm</span> to <span class="date-display-end">3:30pm</span></span></span></h3>
</div>
</div>
</div>
<section class="field field-name-field-event-location field-type-text field-label-inline clearfix view-mode-teaser">
<h3 class="field-label">Location: Manchester Building (Hall 2), Hebrew University Jerusalem</h3>
</section>
<div class="field field-name-body field-type-text-with-summary field-label-hidden view-mode-teaser">
<div class="field-items">
<p class="field-item even">Given a convex polytope P, what is the number of integer points in P? This problem is of great interest in combinatorics and discrete geometry, with many important applications ranging from integer programming to statistics. From a computational point of view it is hopeless in any dimensions, as the knapsack problem is a special case. Perhaps surprisingly, in bounded dimension the problem becomes tractable. How far can one go? Can one count points in projections of P, finite intersections of such projections, etc.?</p>
</div>
</div>
</div>
</div>
</article></div>
    </content>
    <updated>2018-12-05T16:17:41Z</updated>
    <published>2018-12-05T16:17:41Z</published>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Updates"/>
    <category term="Erdos lecture"/>
    <category term="Igor Pak"/>
    <category term="Paul Erdos"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2018-12-19T17:21:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2018/207</id>
    <link href="https://eccc.weizmann.ac.il/report/2018/207" rel="alternate" type="text/html"/>
    <title>TR18-207 |  On the Probabilistic Degree of OR over the Reals | 

	Tulasimohan Molli, 

	Siddharth Bhandari, 

	Prahladh Harsha, 

	Srikanth Srinivasan</title>
    <summary>We study the probabilistic degree over reals of the OR function on $n$ variables. For an error parameter $\epsilon$ in (0,1/3), the $\epsilon$-error probabilistic degree of any Boolean function $f$ over reals is the smallest non-negative integer $d$ such that the following holds: there exists a distribution $D$ of polynomials entirely supported on polynomials of degree at most $d$ such that for all $z \in \{0,1\}^n$, we have $Pr_{P \sim D} [P(z) = f(z) ] \geq 1- \epsilon$.  It is known from the works of Tarui (Theoret. Comput. Sci. 1993) and Beigel, Reingold, and Spielman ( Proc. 6th CCC 1991), that the $\epsilon$-error probabilistic degree of the OR function is at most $O(\log n.\log 1/\epsilon)$. Our first observation is that this can be improved to $O{\log {{n}\choose{\leq \log 1/\epsilon}}}$, which is better for small values of $\epsilon$. 

In all known constructions of probabilistic polynomials for the OR function (including the above improvement), the polynomials $P$ in the support of the distribution $D$ have the following special structure:$P = 1 - (1-L_1).(1-L_2)...(1-L_t)$, where each $L_i(x_1,..., x_n)$ is a linear form in the variables $x_1,...,x_n$, i.e., the polynomial $1-P(x_1,...,x_n)$ is a product of affine forms. We show that the $\epsilon$-error probabilistic degree of OR when restricted to polynomials of the above form is $\Omega ( \log a/\log^2 a )$ where $a = \log {{n}\choose{\leq \log 1/\epsilon}}$. Thus matching the above upper bound (up to poly-logarithmic factors).
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-05T15:03:19Z</updated>
    <published>2018-12-05T15:03:19Z</published>
    <source>
      <id>https://example.com/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://example.com/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2018-12-19T17:22:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2018/12/05/postdoc-at-uw-madison-apply-by-january-7-2019/</id>
    <link href="https://cstheory-jobs.org/2018/12/05/postdoc-at-uw-madison-apply-by-january-7-2019/" rel="alternate" type="text/html"/>
    <title>Postdoc at UW Madison (apply by January 7, 2019)</title>
    <summary>Applications are invited for postdoctoral positions in theoretical computer science and machine learning in the Department of Computer Sciences at UW Madison, hosted by Ilias Diakonikolas. The position is expected to start in Fall of 2019 and is for a period of up to two years. To apply, please send your CV, research statement, and […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are invited for postdoctoral positions in theoretical computer science and machine learning in the Department of Computer Sciences at UW Madison, hosted by Ilias Diakonikolas. The position is expected to start in Fall of 2019 and is for a period of up to two years.<br/>
To apply, please send your CV, research statement, and three recommendation letters. Please apply by January 7th 2019.</p>
<p>Website: <a href="http://www.iliasdiakonikolas.org/postdoc.html">http://www.iliasdiakonikolas.org/postdoc.html</a><br/>
Email: ilias.diakonikolas@gmail.com</p></div>
    </content>
    <updated>2018-12-05T01:43:39Z</updated>
    <published>2018-12-05T01:43:39Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2018-12-19T17:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=598</id>
    <link href="https://emanueleviola.wordpress.com/2018/12/03/i-am-looking-for-students/" rel="alternate" type="text/html"/>
    <title>I am looking for students</title>
    <summary>If you are applying for a PhD in theoretical computer science, you may want to check out my homepage, to see what my team and I are up to, the theory group at Northeastern University, and where we stand in the rankings. (For a discussion of csrankings see this previous post.) If you think there […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>If you are applying for a PhD in theoretical computer science, you may want to check out <a href="http://www.ccs.neu.edu/home/viola/">my homepage</a>, to see what my team and I are up to, <a href="https://www2.ccs.neu.edu/theory/index.html">the theory group at Northeastern University</a>, and <a href="http://csrankings.org/">where we stand in the rankings</a>. (For a discussion of csrankings see this <a href="https://emanueleviola.wordpress.com/2016/07/21/csrankings/">previous post</a>.) If you think there is a match write my name on the application.</p></div>
    </content>
    <updated>2018-12-03T22:24:01Z</updated>
    <published>2018-12-03T22:24:01Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Emanuele</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>By Emanuele Viola</subtitle>
      <title>Thoughts</title>
      <updated>2018-12-19T17:20:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2018/206</id>
    <link href="https://eccc.weizmann.ac.il/report/2018/206" rel="alternate" type="text/html"/>
    <title>TR18-206 |  Equality Alone Does Not Simulate Randomness | 

	Arkadev Chattopadhyay, 

	Shachar Lovett, 

	Marc Vinyals</title>
    <summary>The canonical problem that gives an exponential separation between deterministic and randomized communication complexity in the classical two-party communication model is `Equality'. In this work, we show that even allowing access to an `Equality' oracle, deterministic protocols remain exponentially weaker than randomized ones. More precisely, we exhibit a total function on $n$ bits with randomized one-sided communication complexity $O(\log n)$, but such that every deterministic protocol with access to `Equality' oracle needs $\Omega(n/\log n)$ cost to compute it.
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-03T20:54:45Z</updated>
    <published>2018-12-03T20:54:45Z</published>
    <source>
      <id>https://example.com/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://example.com/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2018-12-19T17:22:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15474</id>
    <link href="https://rjlipton.wordpress.com/2018/12/02/a-tiebreak-win-and-the-problem-of-draws/" rel="alternate" type="text/html"/>
    <title>A Tiebreak Win and the Problem of Draws</title>
    <summary>Carlsen impressed in fast chess, but what of classical? Cropped from AFP/Getty source (Irish Times) Magnus Carlsen retained his title of World Chess Champion on Wednesday. He thumped the American challenger Fabiano 3-0 in a best-of-four tiebreak series of games played at Rapid time controls. Despite his having only one-fourth the standard thinking time, Carlsen’s […]
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Carlsen impressed in fast chess, but what of classical?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2018/12/carlsenwithcup.jpg"><img alt="" class="alignright wp-image-15475" height="185" src="https://rjlipton.files.wordpress.com/2018/12/carlsenwithcup.jpg?w=205&amp;h=185" width="205"/></a></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from AFP/Getty <a href="https://www.irishtimes.com/sport/other-sports/magnus-carlsen-global-star-of-chess-to-press-on-after-latest-title-1.3715138">source</a> (Irish Times) </font></td>
</tr>
</tbody>
</table>
<p>Magnus Carlsen retained his title of World Chess Champion on Wednesday. He thumped the American challenger Fabiano 3-0 in a best-of-four tiebreak series of games played at <a href="https://en.wikipedia.org/wiki/Fast_chess#Rapid_(FIDE)_or_quick_(USCF)">Rapid</a> time controls. Despite his having only one-fourth the standard thinking time, Carlsen’s quality in the tiebreakers was plausibly <em>higher</em> than in the twelve regulation games. </p>
<p>
Today Dick and I congratulate Carlsen on his victory and discuss implications for future top-level competitions in chess. <span id="more-15474"/></p>
<p>
Chess ratings via the <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo</a> system are based on results of games and the ratings of the opponents. My Intrinsic Performance Ratings (IPRs) are on the same scale but use only the quality of the player’s moves as judged by strong chess programs. The mapping from quality measures to Elo ratings comes from training data of millions of moves by players of all ratings. Per remarks in my previous <a href="https://rjlipton.wordpress.com/2018/11/25/far-from-a-turkey-shoot/">post</a>, I don’t claim the mapping captures all aspects of chess skill—but it does provide firm ground for judging players’ performances relative to their peers. My training sets go from beginner clear up to the Elo 2800+ level of Carlsen and Caruana.</p>
<p>
One use of IPR is to measure how thinking time affects quality. The title match tiebreaker gave 25 minutes plus an extra 10 seconds per move, the same as in the famous <a href="https://en.wikipedia.org/wiki/Amber_chess_tournament">Melody Amber</a> tournaments, whereas the <a href="https://en.wikipedia.org/wiki/World_Rapid_Chess_Championship">World Rapid</a> championships give 10 fewer minutes. My preliminary results show an average dropoff of 200–210 Elo in the Ambers and 280–290 Elo in World Rapids. Caruana’s quality of 2575 (with huge error bars from under 100 relevant moves in the three tiebreak games) was consistent with this, but here is what I measured for Carlsen:</p>
<blockquote><p><b> </b> <em> <b>2945 +- 190</b>. </em>
</p></blockquote>
<p/><p>
Since the error-bars are two-sigma, this was more than one standard deviation <em>higher</em> than Carlsen’s rating at <em>standard</em> time controls, and higher than his IPR for the twelve regulation games of the match. Clearly Caruana ran into a buzzsaw. </p>
<p>
Carlsen’s match two years ago also went to tiebreakers and I measured <a href="https://rjlipton.wordpress.com/2016/12/08/magnus-and-the-turkey-grinder/">no dropoff</a> there either: <b>2835 +- 250</b>. Carlsen won the World Rapid Championship in 2014 and 2015 and is the reigning World Blitz champion to boot. His prowess upheld his reasons for taking a draw rather than press his advantage in the twelfth regulation game, which I discussed <a href="https://rjlipton.wordpress.com/2018/11/25/far-from-a-turkey-shoot/">before</a>. Here we will consider what it says about standards and expectations for the chess title in general.</p>
<p>
</p><p/><h2> Two Issues </h2><p/>
<p/><p>
The first thing to note is that although many chess luminaries have voiced dissatisfaction with the 12 draws and tiebreaker and match rules that produced this outcome, neither of the two players is among them. This contrasts with great fights over rules by Bobby Fischer and Garry Kasparov in particular. Caruana has simply <a href="https://twitter.com/FabianoCaruana/status/1067848751052668928">offered</a> his congratulations and thanks. Carlsen in a post-match <a href="https://www.theguardian.com/sport/2018/nov/29/magnus-carlsen-my-last-world-championship-match-chess">interview</a> considered using Rapid and Blitz formats more not less. </p>
<p>
That the tiebreak games were so decisive makes the outcome seem <em>juste</em>. My near-namesake Kenneth Rogoff <a href="https://www.project-syndicate.org/commentary/human-chess-survives-artificial-intelligence-by-kenneth-rogoff-2018-11">opined</a> after Wednesday’s conclusion that nothing is amiss. He compared the twelve draws to a 0-0 soccer match that has tantalizing near-misses before its penalty-kick shootout. Most of the draws were hard fights where one or both sides had chances. All six with Caruana as White were Sicilian defenses, considered a fighting choice by Black. The more-drawish <a href="https://en.wikipedia.org/wiki/Ruy_Lopez#Berlin_Defence">Berlin</a> defense never happened, and of the two Petroffs chosen by Caruana as Black, one produced the fascinating game-6 <a href="https://rjlipton.wordpress.com/2018/11/25/far-from-a-turkey-shoot/">endgame</a> which Caruana could have won. Two games with the Queen’s Gambit Declined (QGD) were fairly tame, but in two others, Carlsen’s 1.c4 English Opening led to sharp play.</p>
<p>
There are at least two separate issues:</p>
<ol>
<li>
Is it right to mix formats? Suppose a 1500 meter championship footrace ended in a dead heat—as did the 1500 meter speed skating <a href="https://en.wikipedia.org/wiki/Speed_skating_at_the_1960_Winter_Olympics_-_Men's_1500_metres">final</a> of the 1960 Winter Olympics. Would we have the runners do a 400-meter tiebreaker? Or blitz out a series of 100-meter dashes? Consensus is no: the format is so different as to make many mile-pace skills irrelevant, let alone how it changes the race strategy. Is the situation in chess similar? <p/>
</li><li>
How terrible was having all 12 regulation games drawn?—for the general public and prospective sponsors, not just chess cognoscenti. What if all games had been Berlins and Petroffs and QGDs?
</li></ol>
<p>
Thoughtful considerations and proposals have been given <a href="https://gregshahade.wordpress.com/2018/12/01/fixing-chess/">here</a> by my fellow International Master Greg Shahade (whose father Michael I knew in the 1970s and whose sister Jennifer was a US commentator on the match), <a href="http://www.thechessmind.net/blog/2018/11/30/should-the-world-championship-be-changed.html">here</a> by master teacher Dennis Monokroussos on his blog <em>The Chess Mind</em>, and <a href="https://marginalrevolution.com/marginalrevolution/2018/11/save-future-chess.html">here</a> by the noted economist and blogger Tyler Cowen (who played on <a href="https://nezhmet.wordpress.com/2008/05/15/the-fabulous-70s-jersey-squad-takes-1976-us-amateur-team/">teams</a> with me in the 1970s). See also <a href="https://new.uschess.org/news/are-twelve-games-enough/">this</a> from 2016. Dick and I will not pretend we can solve all the issues in one post. But we can offer a few particular observations.</p>
<p>
</p><p/><h2> Single Format </h2><p/>
<p/><p>
To find an opinion against the inclusion of games under fast time controls, we need look no further than Shahade two years <a href="https://gregshahade.wordpress.com/2016/11/28/how-to-make-the-world-chess-championship-games-less-boring/">ago</a>, when the regular part of the match had 10 draws and one win apiece:</p>
<blockquote><p><b> </b> <em> Just as in the 20th century, the Champion should retain the title on a drawn match. There should be no rapid tiebreak. … I don’t agree with giving anyone the chance to become World Chess Championship by tying a Classical match and then winning some rapid games. At every moment in the match, someone will be behind on the scoreboard [and will] fight harder every game. </em>
</p></blockquote>
<p/><p>
Under this system, Carlsen would have retained his title on two straight drawn matches, as did Mikhail Botvinnik in his 1951 <a href="https://en.wikipedia.org/wiki/World_Chess_Championship_1951">match</a> against David Bronstein and 1954 <a href="https://en.wikipedia.org/wiki/World_Chess_Championship_1954">match</a> against Vasily Smyslov. Kasparov retained his title once that way against Anatoly Karpov by winning the last game to make a tie. Those matches, however, were 24 games each, and even at that length, Fischer <a href="https://www.mark-weeks.com/chess/7375$wix.htm">led</a> a chorus of many who felt the “draw odds” were too steep on grounds that the defending champion could play for draws.</p>
<p>
Fischer’s solution was to have draws not count. It was used for the 1978 <a href="https://en.wikipedia.org/wiki/World_Chess_Championship_1978">match</a> between Karpov and Viktor Korchnoi, which had a dramatic ending but took 32 games over three months. The format cratered in 1984–85 when Kasparov, after losing 4 of the first 9 games, shifted to grinding for draws. The <a href="https://en.wikipedia.org/wiki/World_Chess_Championship_1984">match</a> was annulled after 48 games with Karpov still shy of the six wins needed. </p>
<p>
Multi-month matches are out, not only because of the expense and time commitment for sponsors, but for the positive reasons that today’s champions have been attracted to play in a more diverse array of chess events than their predecessors. Cowen <a href="https://marginalrevolution.com/marginalrevolution/2018/11/save-future-chess.html">advocates</a> the old 24-game format (with fewer rest days) but with prescribed openings away from Berlins and Petroffs and QGDs and lines that players already know 30 moves deep.  </p>
<p>
Otherwise, the only way to mitigate the champion’s draw odds is to alter chess so that draws are less frequent. This can be done by branching the opening phase so greatly as to reduce the expected span of computer-aided opening preparation and by changing rules to provide more symmetry-breaking. </p>
<p>
Here, too, Fischer was a visionary. His <a href="https://en.wikipedia.org/wiki/Chess960">Fischer Random</a> format, also called Chess960 for its number of starting positions, continues to be used in some high-level events, but has issues in that these positions retain symmetry that in numerous cases gives White more advantage than in the classical setup. I stand by my proposal to <a href="https://www.chessvariants.com/diffsetup.dir/baselinef.html">combine</a> an older non-random, non-symmetrical form by Bronstein with Fischer’s rules and a few tweaks.</p>
<p>
If such rule changes are too drastic, and the classic 24-game format falls on grounds of length and inequity, then we must vary time allotments. Here there is much creative leeway.</p>
<p>
</p><p/><h2> Mixed Formats </h2><p/>
<p/><p>
Dick suggests one simple idea which has also been remarked in comments to the above-linked proposals and <a href="https://en.chessbase.com/post/world-championship-out-of-the-box">elsewhere</a>: Play a day of fast games <em>first</em>. The winner would effectively have draw odds for the regular match. That removes the inequity of giving those odds to the defending champion and creates Shahade’s situation where someone always needs to win. </p>
<p>
A variant on this that if a regular game finishes drawn within a certain short time, the players follow with one of the Rapid games. Then the Rapid standings would loom over the match. This prefigures ways to integrate the faster-paced games into the scoring system. This month’s <a href="https://www.londonchessclassic.com/press/lcc18_pr4.htm">London Classic</a> has become one of several elite tournaments and series to blend Classical, Rapid, and Blitz into one system.</p>
<p>
Shahade’s new ideas begin by estimating that a 25–30% time reduction would bring only a 30-odd point reduction in quality. Since I measured 200 points from a 75% time reduction, that seems about right. The time saved from today’s standard playing session of up to 7 hours allows the following schedule:</p>
<blockquote><p><b> </b> <em> </em></p><em>
<ul>
<li>
You play one game at 90 minutes plus a 20 second increment. The winner gets 10 points the loser gets 0. <p/>
</li><li>
If that game is drawn you reverse colors and play one game at 20 minutes plus a 10 second increment. The winner gets 7 points and the loser gets 3. <p/>
</li><li>
If that game is drawn you keep the same colors as the rapid game and play one game at 5 minutes plus a 3 second increment. The winner gets 6 points and the loser gets 4. If this game is drawn, both players get 5 points.
</li></ul>
</em><p><em/>
</p></blockquote>
<p/><p>
The points division makes a classical win 2.5 times more valuable than a win at Rapid pace, and the latter worth twice a win at Blitz. But he notes that all formats would count and the overall match score would be less often tied. That a Blitz game is a relative crapshoot would reduce playing to preserve a tie over the whole session, though the Black player in the classical would have incentive to stodge and enjoy White in the faster game or games. </p>
<p>
Shahade’s idea still gives every playing day the same structure. If we are willing to vary the structure in return for lowering the prescribed use of Rapid, here is my suggestion:</p>
<ul>
<li>
Play 2 games at the classic time, so each player gets one turn as White, one as Black. <p/>
</li><li>
If the score is tied, then play 2 games with 25–30% reduced time. <p/>
</li><li>
If the score is still tied, then play 2 games per day at 50–60% reduced time. <p/>
</li><li>
Whenever a 2-game set produced at least one win, the next set uses the longest time format. <p/>
</li><li>
A tie after the last 2-game set in regulation still brings a one-day fast-chess playoff.<p/>
</li><li>
The match follows prescribed playing days except when the last 2-game set needs what is otherwise designated the rest day before tiebreaks.
</li></ul>
<p>
This might set up a subtle feedback reward system for “not playing to draw”—insofar as I can attest that having more time feels nicer when playing. Except for the possible playoff, the Rapid pace would not intrude. The match length expands in the absence of decisive games.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>
<b>Update 12/4:</b> A <a href="https://www.arctic.com/secno/en/magnus-carlsens-blog/the-world-championship-match-in-london-no-4">review</a> of the match by Carlsen; incisive <a href="https://chessbase.in/news/Anand-on-Carlsen-Caruana-2018">analysis</a> by former world champion V. Anand.  And an extended <a href="https://www.3quarksdaily.com/3quarksdaily/2018/12/after-carlsens-victory-questions-for-chess-and-for-the-champion.html">discussion</a> of both the games and the match format by Steve Gardner in 3 Quarks Daily.</p>
<p>
Does the format of championship chess need fixing? Should we just have a 16 or 18-game classical match with fewer rest days where the champion retains the title in a tie? Note this was the rule in last night’s drawn heavyweight boxing <a href="https://bleacherreport.com/articles/2808438-tyson-fury-vs-deontay-wilder-ends-in-controversial-draw-wilder-retains-title">fight</a>. </p>
<p>
How should the two highlighted issues be weighed—also for possibly reducing draws in other kinds of tournaments?</p>
<p>
Shahade’s scoring scheme would also apply to round-robin tournaments, for which soccer-style scoring of 3 points for a win has sometimes been employed to encourage combat. It would also apply to “Swiss System” tournaments since it conserves the total of 10 points between the players—though demands on the tournament officials to oversee so many more rapid and blitz games could be prohibitive. </p>
<p/><p><br/>
[added link for a version of playing the fast-game series first, link for 2016 discussion, and a note about match length in my suggestion at the end, plus some word tweaks; added 12/4 update]</p></font></font></div>
    </content>
    <updated>2018-12-03T00:22:05Z</updated>
    <published>2018-12-03T00:22:05Z</published>
    <category term="All Posts"/>
    <category term="chess"/>
    <category term="detection"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Bobby Fischer"/>
    <category term="draws"/>
    <category term="Fabiano Caruana"/>
    <category term="Magnus Carlsen"/>
    <category term="rules"/>
    <category term="scoring systems"/>
    <category term="tiebreaks"/>
    <category term="world championship"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2018-12-19T17:21:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2018/205</id>
    <link href="https://eccc.weizmann.ac.il/report/2018/205" rel="alternate" type="text/html"/>
    <title>TR18-205 |  New Constructions with Quadratic Separation between Sensitivity and Block Sensitivity | 

	Siddhesh Chaubal, 

	Anna Gal</title>
    <summary>Nisan and Szegedy conjectured that block sensitivity is at most polynomial in sensitivity for any Boolean function. There is a huge gap between the best known upper bound on block sensitivity in terms of sensitivity - which is exponential, and the best known separating examples - which give only a quadratic separation between block sensitivity and sensitivity.

In this paper we give various new constructions of families of Boolean functions that exhibit quadratic separation between sensitivity and block sensitivity. Some of our constructions match the current largest separation between sensitivity and block sensitivity by Ambainis and Sun. Our constructions have several novel aspects. We use more general function compositions instead of just OR-composition, and give constructions based on algebraic operations. In addition, we give the first direct constructions of families of Boolean functions that have both 0-block sensitivity and 1-block sensitivity quadratically larger than sensitivity.
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-02T23:54:49Z</updated>
    <published>2018-12-02T23:54:49Z</published>
    <source>
      <id>https://example.com/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://example.com/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2018-12-19T17:22:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2018/204</id>
    <link href="https://eccc.weizmann.ac.il/report/2018/204" rel="alternate" type="text/html"/>
    <title>TR18-204 |  Exponential Separation between Quantum Communication and Logarithm of Approximate Rank | 

	Makrand Sinha, 

	Ronald de Wolf</title>
    <summary>Chattopadhyay, Mande and Sherif (ECCC 2018) recently exhibited a total 
Boolean function, the sink function, that has polynomial approximate rank and 
polynomial randomized communication complexity. This gives an exponential 
separation between randomized communication complexity and logarithm of the
approximate rank, refuting the log-approximate-rank conjecture. We show that 
even the quantum communication complexity of the sink function is polynomial, 
thus also refuting the quantum log-approximate-rank conjecture. 

Our lower bound is based on the fooling distribution method introduced by Rao 
and Sinha (ECCC 2015) for the classical case and extended by Anshu, Touchette, 
Yao and Yu (STOC 2017) for the quantum case. We also give a new proof of the 
classical lower bound using the fooling distribution method.
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-02T08:50:21Z</updated>
    <published>2018-12-02T08:50:21Z</published>
    <source>
      <id>https://example.com/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://example.com/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2018-12-19T17:22:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2018/203</id>
    <link href="https://eccc.weizmann.ac.il/report/2018/203" rel="alternate" type="text/html"/>
    <title>TR18-203 |  Non-Interactive Non-Malleability from Quantum Supremacy | 

	Yael Kalai, 

	Dakshita Khurana</title>
    <summary>We construct non-interactive non-malleable commitments with respect to replacement, without setup in the plain model, under well-studied assumptions.

First, we construct non-interactive non-malleable commitments with respect to commitment for $\epsilon \log \log n$ tags for a small constant $\epsilon&gt;0$, under the following assumptions:

- Sub-exponential hardness of factoring or discrete log.

- Quantum sub-exponential hardness of learning with errors (LWE).

Second, as our key technical contribution, we introduce a new tag amplification technique. We show how to convert any non-interactive non-malleable commitment with respect to commitment for $\epsilon \log \log n$ tags (for any constant $\epsilon&gt;0$) into a non-interactive non-malleable commitment with respect to replacement for $2^n$ tags. This part only assumes the existence of sub-exponentially secure non-interactive witness indistinguishable (NIWI) proofs, which can be based on sub-exponential security of the decisional linear assumption.

Interestingly, for the tag amplification technique, we crucially rely on the leakage lemma due to Gentry and Wichs (STOC 2011). For the construction of non-malleable commitments for $\epsilon \log \log n$ tags, we rely on quantum supremacy. This use of quantum supremacy in classical cryptography is novel, and we believe it will have future applications. We provide one such application to two-message witness indistinguishable (WI) arguments based on the polynomial hardness of factoring and quantum polynomial hardness of LWE.
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-01T18:29:39Z</updated>
    <published>2018-12-01T18:29:39Z</published>
    <source>
      <id>https://example.com/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://example.com/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2018-12-19T17:22:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2018/202</id>
    <link href="https://eccc.weizmann.ac.il/report/2018/202" rel="alternate" type="text/html"/>
    <title>TR18-202 |  A stochastic calculus approach to the oracle separation of BQP and PH | 

	Xinyu Wu</title>
    <summary>After presentations of the oracle separation of BQP and PH result by Raz and Tal [ECCC TR18-107], several people
(e.g. Ryan O’Donnell, James Lee, Avishay Tal) suggested that the proof may be simplified by
stochastic calculus. In this short note, we describe such a simplification.
      <div class="commentbar">
        <p/>
      </div>
    </summary>
    <updated>2018-12-01T12:51:33Z</updated>
    <published>2018-12-01T12:51:33Z</published>
    <source>
      <id>https://example.com/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://example.com/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2018-12-19T17:22:10Z</updated>
    </source>
  </entry>
</feed>
