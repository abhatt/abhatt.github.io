<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-05-27T17:25:00Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3247584017741087776</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3247584017741087776/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/separating-fact-from-fiction-with-56-of.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3247584017741087776" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3247584017741087776" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/separating-fact-from-fiction-with-56-of.html" rel="alternate" type="text/html"/>
    <title>separating fact from fiction with the 56% of Americans say Arabic Numerals should not be taught in school</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
On the excellent TV show Veep there was a subplot about a political candidate (who himself had failed algebra in HS) objecting to Algebra since it was invented by the Muslims. I don't recall the exact line, but he said something like `Math teachers are terrorists'<br/>
This was, of course, fiction.<br/>
<br/>
The same week I read that 56% of survey respondents say `<u><i>Arabic Numerals' shouldn't be taught in</i></u> <i><u>schools'</u></i> Obviously also a fiction. Perhaps a headline from <i>The Onion</i>.<br/>
<br/>
No. The story is true.<br/>
<br/>
See snopes entry on this: <a href="http://declaremathoperator%7B/TRAIN%7D%7BTRAIN%7D">here</a><br/>
<br/>
but also see many FALSE but FUNNY websites:<br/>
<br/>
Sarah Palin wants Arabic Numerals out of the schools: <a href="http://nationalreport.net/sarah-palin-wants-arabic-numerals-banned-americas-schools/">here</a> Funny but false.<br/>
<br/>
Jerry Brown is forcing students in California to learn Arabic Numerals as part of multi-culturism False by funny:  <a href="https://me.me/i/sharia-law-must-be-stopped-under-gov-brown-students-in-20990368">here</a><br/>
<br/>
A website urging us to use Roman Numerals (which Jesus used!) False but funny:  <a href="http://freedomnumerals.com/">here</a><br/>
<br/>
OKAY, what to make of the truth that really, really, 56% of Americans are against Arab Numerals<br/>
<br/>
1) Bigotry combined with ignorance.<br/>
<br/>
2) Some of the articles I read about this say its a problem with polls and people. There may be some of that, but still worries me.<br/>
<br/>
3) In Nazi Germany (WOW- Goodwin's law popped up rather early!) they stopped teaching relativity because Albert Einstein was Jewish (the story is more complicated than that, see <a href="https://www.scientificamerican.com/article/how-2-pro-nazi-nobelists-attacked-einstein-s-jewish-science-excerpt1/">her</a>e). That could of course never happen in America now (or could it, see <a href="https://www.tabletmag.com/jewish-news-and-politics/50097/time-warp">here</a> and <a href="https://www.conservapedia.com/index.php?title=Counterexamples_to_Relativity">here</a>).<br/>
<br/>
4) There is no danger that we will dump Arabic Numerals. I wonder if we will change there name to Freedom Numerals.<br/>
<br/>
5) Ignorance of science is a more immediate problem with the anti-vax people.<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-05-27T15:12:00Z</updated>
    <published>2019-05-27T15:12:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-27T15:12:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.10360</id>
    <link href="http://arxiv.org/abs/1905.10360" rel="alternate" type="text/html"/>
    <title>The advantages of multiple classes for reducing overfitting from test set reuse</title>
    <feedworld_mtime>1558915200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldman:Vitaly.html">Vitaly Feldman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Frostig:Roy.html">Roy Frostig</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hardt:Moritz.html">Moritz Hardt</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.10360">PDF</a><br/><b>Abstract: </b>Excessive reuse of holdout data can lead to overfitting. However, there is
little concrete evidence of significant overfitting due to holdout reuse in
popular multiclass benchmarks today. Known results show that, in the
worst-case, revealing the accuracy of $k$ adaptively chosen classifiers on a
data set of size $n$ allows to create a classifier with bias of
$\Theta(\sqrt{k/n})$ for any binary prediction problem. We show a new upper
bound of $\tilde O(\max\{\sqrt{k\log(n)/(mn)},k/n\})$ on the worst-case bias
that any attack can achieve in a prediction problem with $m$ classes. Moreover,
we present an efficient attack that achieve a bias of $\Omega(\sqrt{k/(m^2
n)})$ and improves on previous work for the binary setting ($m=2$). We also
present an inefficient attack that achieves a bias of $\tilde\Omega(k/n)$.
Complementing our theoretical work, we give new practical attacks to
stress-test multiclass benchmarks by aiming to create as large a bias as
possible with a given number of queries. Our experiments show that the
additional uncertainty of prediction with a large number of classes indeed
mitigates the effect of our best attacks.
</p>
<p>Our work extends developments in understanding overfitting due to adaptive
data analysis to multiclass prediction problems. It also bears out the
surprising fact that multiclass prediction problems are significantly more
robust to overfitting when reusing a test (or holdout) dataset. This offers an
explanation as to why popular multiclass prediction benchmarks, such as
ImageNet, may enjoy a longer lifespan than what intuition from literature on
binary classification suggests.
</p></div>
    </summary>
    <updated>2019-05-27T01:29:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.10337</id>
    <link href="http://arxiv.org/abs/1905.10337" rel="alternate" type="text/html"/>
    <title>What Can ResNet Learn Efficiently, Going Beyond Kernels?</title>
    <feedworld_mtime>1558915200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Allen=Zhu:Zeyuan.html">Zeyuan Allen-Zhu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yuanzhi.html">Yuanzhi Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.10337">PDF</a><br/><b>Abstract: </b>How can neural networks such as ResNet $\textit{efficiently}$ learn CIFAR-10
with test accuracy more than 96%, while other methods, especially kernel
methods, fall far behind? Can we more provide theoretical justifications for
this gap?
</p>
<p>There is an influential line of work relating neural networks to kernels in
the over-parameterized regime, proving that they can learn certain concept
class that is also learnable by kernels, with similar test error. Yet, can we
show neural networks provably learn some concept class $\textit{better}$ than
kernels?
</p>
<p>We answer this positively in the PAC learning language. We prove neural
networks can efficiently learn a notable class of functions, including those
defined by three-layer residual networks with smooth activations, without any
distributional assumption. At the same time, we prove there are simple
functions in this class that the test error obtained by neural networks can be
$\textit{much smaller}$ than $\textit{any}$ "generic" kernel method, including
neural tangent kernels, conjugate kernels, etc.
</p>
<p>The main intuition is that $\textit{multi-layer}$ neural networks can
implicitly perform hierarchal learning using different layers, which reduces
the sample complexity comparing to "one-shot" learning algorithms such as
kernel methods.
</p></div>
    </summary>
    <updated>2019-05-27T01:29:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.10284</id>
    <link href="http://arxiv.org/abs/1905.10284" rel="alternate" type="text/html"/>
    <title>Hardness of Distributed Optimization</title>
    <feedworld_mtime>1558915200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Nir Bachrach, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Censor=Hillel:Keren.html">Keren Censor-Hillel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dory:Michal.html">Michal Dory</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Efron:Yuval.html">Yuval Efron</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leitersdorf:Dean.html">Dean Leitersdorf</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paz:Ami.html">Ami Paz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.10284">PDF</a><br/><b>Abstract: </b>This paper studies lower bounds for fundamental optimization problems in the
CONGEST model. We show that solving problems exactly in this model can be a
hard task, by providing $\tilde{\Omega}(n^2)$ lower bounds for cornerstone
problems, such as minimum dominating set (MDS), Hamiltonian path, Steiner tree
and max-cut. These are almost tight, since all of these problems can be solved
optimally in $O(n^2)$ rounds. Moreover, we show that even in bounded-degree
graphs and even in simple graphs with maximum degree 5 and logarithmic
diameter, it holds that various tasks, such as finding a maximum independent
set (MaxIS) or a minimum vertex cover, are still difficult, requiring a
near-tight number of $\tilde{\Omega}(n)$ rounds.
</p>
<p>Furthermore, we show that in some cases even approximations are difficult, by
providing an $\tilde{\Omega}(n^2)$ lower bound for a
$(7/8+\epsilon)$-approximation for MaxIS, and a nearly-linear lower bound for
an $O(\log{n})$-approximation for the $k$-MDS problem for any constant $k \geq
2$, as well as for several variants of the Steiner tree problem.
</p>
<p>Our lower bounds are based on a rich variety of constructions that leverage
novel observations, and reductions among problems that are specialized for the
CONGEST model. However, for several additional approximation problems, as well
as for exact computation of some central problems in $P$, such as maximum
matching and max flow, we show that such constructions cannot be designed, by
which we exemplify some limitations of this framework.
</p></div>
    </summary>
    <updated>2019-05-27T01:30:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.10143</id>
    <link href="http://arxiv.org/abs/1905.10143" rel="alternate" type="text/html"/>
    <title>A Practical Framework for Solving Center-Based Clustering with Outliers</title>
    <feedworld_mtime>1558915200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Hu.html">Hu Ding</a>, Haikuo Yu <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.10143">PDF</a><br/><b>Abstract: </b>Clustering has many important applications in computer science, but
real-world datasets often contain outliers. Moreover, the existence of outliers
can make the clustering problems to be much more challenging. In this paper, we
propose a practical framework for solving the problems of
$k$-center/median/means clustering with outliers. The framework actually is
very simple, where we just need to take a small sample from input and run
existing approximation algorithm on the sample. However, our analysis is
fundamentally different from the previous sampling based ideas. In particular,
the size of the sample is independent of the input data size and
dimensionality. To explain the effectiveness of random sampling in theory, we
introduce a "significance" criterion and prove that the performance of our
framework depends on the significance degree of the given instance. Actually,
our result can be viewed as a new step along the direction of beyond worst-case
analysis in terms of clustering with outliers. The experiments suggest that our
framework can achieve comparable clustering result with existing methods, but
greatly reduce the running time.
</p></div>
    </summary>
    <updated>2019-05-27T01:31:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.10017</id>
    <link href="http://arxiv.org/abs/1905.10017" rel="alternate" type="text/html"/>
    <title>Computational cost for determining an approximate global minimum using the selection and crossover algorithm</title>
    <feedworld_mtime>1558915200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Isomura:Takuya.html">Takuya Isomura</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.10017">PDF</a><br/><b>Abstract: </b>This work examines the expected computational cost to determine an
approximate global minimum of a class of cost functions characterized by the
variance of coefficients. The cost function takes $N$-dimensional binary states
as arguments and has many local minima. Iterations in the order of $2^N$ are
required to determine an approximate global minimum using random search. This
work analytically and numerically demonstrates that the selection and crossover
algorithm with random initialization can reduce the required computational cost
(i.e., number of iterations) for identifying an approximate global minimum to
the order of $\lambda^N$ with $\lambda$ less than 2. The two best solutions,
referred to as parents, are selected from a pool of randomly sampled states.
Offspring generated by crossovers of the parents' states are distributed with a
mean cost lower than that of the original distribution that generated the
parents. It is revealed that in contrast to the mean, the variance of the cost
of the offspring is asymptotically the same as that of the original
distribution. Consequently, sampling from the offspring's distribution leads to
a higher chance of determining an approximate global minimum than sampling from
the original distribution, thereby accelerating the global search. This feature
is distinct from the distribution obtained by a mixture of a large population
of favorable states, which leads to a lower variance of offspring. These
findings demonstrate the advantage of the crossover between two favorable
states over a mixture of many favorable states for an efficient determination
of an approximate global minimum.
</p></div>
    </summary>
    <updated>2019-05-27T01:20:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09992</id>
    <link href="http://arxiv.org/abs/1905.09992" rel="alternate" type="text/html"/>
    <title>Fast Convergence of Belief Propagation to Global Optima: Beyond Correlation Decay</title>
    <feedworld_mtime>1558915200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koehler:Frederic.html">Frederic Koehler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09992">PDF</a><br/><b>Abstract: </b>Belief propagation is a fundamental message-passing algorithm for
probabilistic reasoning and inference in graphical models. While it is known to
be exact on trees, in most applications belief propagation is run on graphs
with cycles. Understanding the behavior of "loopy" belief propagation has been
a major challenge for researchers in machine learning, and positive convergence
results for BP are known under strong assumptions which imply the underlying
graphical model exhibits decay of correlations. We show that under a natural
initialization, BP converges quickly to the global optimum of the Bethe free
energy for Ising models on arbitrary graphs, as long as the Ising model is
\emph{ferromagnetic} (i.e. neighbors prefer to be aligned). This holds even
though such models can exhibit long range correlations and may have multiple
suboptimal BP fixed points. We also show an analogous result for iterating the
(naive) mean-field equations; perhaps surprisingly, both results are
dimension-free in the sense that a constant number of iterations already
provides a good estimate to the Bethe/mean-field free energy.
</p></div>
    </summary>
    <updated>2019-05-27T01:35:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09952</id>
    <link href="http://arxiv.org/abs/1905.09952" rel="alternate" type="text/html"/>
    <title>Accelerated Primal-Dual Coordinate Descent for Computational Optimal Transport</title>
    <feedworld_mtime>1558915200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guo:Wenshuo.html">Wenshuo Guo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Ho:Nhat.html">Nhat Ho</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jordan:Michael_I=.html">Michael I. Jordan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09952">PDF</a><br/><b>Abstract: </b>We propose and analyze a novel accelerated primal-dual coordinate descent
framework for computing the optimal transport (OT) distance between two
discrete probability distributions. First, we introduce the accelerated
primal-dual randomized coordinate descent (APDRCD) algorithm for computing OT.
Then we provide a complexity upper bound
$\widetilde{\mathcal{O}}(\frac{n^{5/2}}{\varepsilon})$ for the APDRCD method
for approximating OT distance, where $n$ stands for the number of atoms of
these probability measures and $\varepsilon &gt; 0$ is the desired accuracy. This
upper bound matches the best known complexities of adaptive primal-dual
accelerated gradient descent (APDAGD) and adaptive primal-dual accelerate
mirror descent (APDAMD) algorithms while it is better than those of Sinkhorn
and Greenkhorn algorithms, which are of the order
$\widetilde{\mathcal{O}}(\frac{n^{2}}{\varepsilon^2})$, in terms of the desired
accuracy $\varepsilon &gt; 0$. Furthermore, we propose a greedy version of APDRCD
algorithm that we refer to as the accelerated primal-dual greedy coordinate
descent (APDGCD) algorithm and demonstrate that it has a better practical
performance than the APDRCD algorithm. Extensive experimental studies
demonstrate the favorable performance of the APDRCD and APDGCD algorithms over
state-of-the-art primal-dual algorithms for OT in the literature.
</p></div>
    </summary>
    <updated>2019-05-27T01:21:05Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09898</id>
    <link href="http://arxiv.org/abs/1905.09898" rel="alternate" type="text/html"/>
    <title>Graph regret bounds for Thompson Sampling and UCB</title>
    <feedworld_mtime>1558915200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lykouris:Thodoris.html">Thodoris Lykouris</a>, Eva Tardos, Drishti Wali <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09898">PDF</a><br/><b>Abstract: </b>We study the stochastic multi-armed bandit problem with the graph-based
feedback structure introduced by Mannor and Shamir. We analyze the performance
of the two most prominent stochastic bandit algorithms, Thompson Sampling and
Upper Confidence Bound (UCB), in the graph-based feedback setting. We show that
these algorithms achieve regret guarantees that combine the graph structure and
the gaps between the means of the arm distributions. Surprisingly this holds
despite the fact that these algorithms do not explicitly use the graph
structure to select arms. Towards this result we introduce a "layering
technique" highlighting the commonalities in the two algorithms.
</p></div>
    </summary>
    <updated>2019-05-27T01:31:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1808.01949</id>
    <link href="http://arxiv.org/abs/1808.01949" rel="alternate" type="text/html"/>
    <title>OptStream: Releasing Time Series Privately</title>
    <feedworld_mtime>1558915200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fioretto:Ferdinando.html">Ferdinando Fioretto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hentenryck:Pascal_Van.html">Pascal Van Hentenryck</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1808.01949">PDF</a><br/><b>Abstract: </b>Many applications of machine learning and optimization operate on data
streams. While these datasets are fundamental to fuel decision-making
algorithms, often they contain sensitive information about individuals and
their usage poses significant privacy risks. Motivated by an application in
energy systems, this paper presents OPTSTREAM, a novel algorithm for releasing
differentially private data streams under the w-event model of privacy.
OPTSTREAM is a 4-step procedure consisting of sampling, perturbation,
reconstruction, and post-processing modules. First, the sampling module selects
a small set of points to access in each period of interest. Then, the
perturbation module adds noise to the sampled data points to guarantee privacy.
Next, the reconstruction module reassembles non-sampled data points from the
perturbed sample points. Finally, the post-processing module uses convex
optimization over the private output of the previous modules, as well as the
private answers of additional queries on the data stream, to improve accuracy
by redistributing the added noise. OPTSTREAM is evaluated on a test case
involving the release of a real data stream from the largest European
transmission operator. Experimental results show that OPTSTREAM may not only
improve the accuracy of state-of-the-art methods by at least one order of
magnitude but also supports accurate load forecasting on the private data.
</p></div>
    </summary>
    <updated>2019-05-27T01:31:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-27T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-955320605612447790</id>
    <link href="http://processalgebra.blogspot.com/feeds/955320605612447790/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=955320605612447790" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/955320605612447790" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/955320605612447790" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2019/05/an-interview-with-jamie-gabbay-and.html" rel="alternate" type="text/html"/>
    <title>An interview with Jamie Gabbay and Andrew Pitts, 2019 Alonzo Church Award recipients</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The 2019 Alonzo Church Award committee consisting of Thomas Eiter, Javier Esparza, Radha Jagadeesan, Catuscia Palamidessi, and Natarajan Shankar, have selected <a href="http://www.gabbay.org.uk/">Murdoch J. Gabbay </a>and <a href="https://www.cl.cam.ac.uk/~amp12/">Andrew M. Pitts </a>for the <a href="http://eatcs.org/index.php/component/content/article/1-news/2812-the-2019-alonzo-church-award">2019 Alonzo Church Award</a>, for introducing the theory of nominal representations, a powerful and elegant mathematical model for computing with data involving atomic names. In particular, the nomination for the Alonzo Church Award singled out the following two papers:<br/><ul><li>“<a href="https://www.cl.cam.ac.uk/~amp12/papers/newaas/newaas-jv.pdf">A new approach to abstract syntax with variable binding</a>” by Murdoch J. Gabbay and Andrew M. Pitts, Formal Aspects of Computing 13(3):341– 363, 2002; and</li><li>“<a href="https://www.cl.cam.ac.uk/~amp12/papers/nomlfo/nomlfo-draft.pdf">Nominal logic, a first order theory of names and binding</a>” by Andrew M. Pitts, Information and Computation 186(2):165–193, 2003.</li></ul>For the conference version of the first article, Andy and Jamie will also be receiving the Test-of-Time Award from LICS 1999.<br/><br/>The award recipients kindly agreed to answer some questions of mine via email. You can find the transcript of the interview below. My questions are labelled with <b>LA</b>, Andy's answers with <b>AP</b> and Jamie's with <b>JG</b>. I hope that you'll enjoy reading their insights and the story of their award-receiving work as much as I did myself. <br/><br/><div dir="ltr"><b>LA: </b>You are receiving the 2019 Alonzo Church Award  for  Outstanding Contributions to Logic and Computation as well as the  Test-of-Time Award from LICS 1999 for your invention of nominal  techniques to provide a semantic understanding of abstract syntax with  binding.   Could you briefly describe the history of the ideas that led  you to use the <a href="https://en.wikipedia.org/wiki/Permutation_model">permutation model of set theory with atoms</a> due to  Fraenkel and Mostowski to represent name abstraction and fresh name  generation? What were the  main inspirations and motivations for your work? In your opinion, how  did nominal techniques advance the state of the art at that time?</div><div dir="ltr"/><br/><div dir="ltr"><b>AP: </b>I have had a long-standing interest in the mathematical semantics of programming language features that restrict resources to a specific scope, or hide information from a program's environment; think local mutable state in languages like <a href="http://ocaml.org/">OCaml</a>, or channel-name restriction in the <a href="https://en.wikipedia.org/wiki/%CE%A0-calculus">pi-calculus</a>. When <a href="http://homepages.inf.ed.ac.uk/stark/">Ian Stark</a> was doing his PhD with me in the 90s we tried to understand a simple instance: the observable properties of higher-order functions combined with dynamically generated atomic names that can be tested for equality, but don't have any other attribute -- we called this the "nu-calculus". Ian gave a denotational semantics for the nu-calculus using Moggi's monad for modelling dynamic allocation. That monad is defined on the category of pullback-preserving functors from the category of injective functions between finite ordinals to the category of sets. This functor category was well-known to me from topos theory, where it is called <a href="https://ncatlab.org/nlab/show/Schanuel+topos">Schanuel's topos </a>and hosts the generic model of a geometric theory of an infinite decidable set.  A few years later, when Jamie joined me as a PhD student in 1998, I suggested we look at the Schanuel topos as a setting for initial algebra semantics of syntax involving binding operations, modulo alpha-equivalence. I think Jamie prefers set theory over category theory, so he pushed us to use another known equivalent presentation of the Schanuel topos, in terms of continuous actions of the group of permutations of the set N of natural numbers (topologized as a subspace of the product of countably many copies of N). In this form there is an obvious connection with the cumulative hierarchy of sets (with atoms) that are hereditarily finitely supported with respect to the action of permuting atoms. This universe of sets was devised by Fraenkel and Mostowski in the first part of the twentieth century to model ZFA set theory without axioms of choice.  Whether one emphasises set theory or category theory, the move to making permutations of names, rather than injections between sets of names, the primary concept was very fruitful. For example, it certainly makes higher-order constructions (functions and powersets) in the topos/set-theory easier to describe and use. We ended up with a generic construction for name-abstraction modulo <a href="https://en.wikipedia.org/wiki/Lambda_calculus#Alpha_equivalence">alpha-equivalence</a> compatible with classical higher-order logic or set theory, so long as one abstains from unrestricted use of choice. </div><div dir="ltr"/><div dir="ltr"><br/><b>JG:</b> At the time it wasn't an idea to consider names as elements of a distinctive datatype of names, with properties just like other datatypes such as the natural numbers Nat. If we want to add 1 to 1, we take 1:Nat and invoke the "plus" function, which is a specific thing associated to Nat; so why not abstract a in x by assuming a:Atm (where Atm is a distinct thing in our mathematical universe) and x:X and invoking a function "abstract", which is a thing associated to Atm?  We unfolded the implications of this idea in set theory and rediscovered FM sets.  I was inspired by the way I saw mathematics built up in ZF set theory as an undergraduate, starting from a simple basis and building up the cumulative hierarchy.  When I saw the chance to do this for a universe with names, I jumped at the chance.  It turns out FM sets are not required.  Nominal techniques can be built in ZFA set theory, which contains more functions and permits unrestricted choice. </div><div dir="ltr"><br/><b>LA: </b>Over  the last fifteen years, nominal techniques have become a fundamental  tool for modelling locality in computation, underlying research  presented in over a hundred papers, new programming languages and models  of computation. They have applications to the syntax and semantics of  programming languages, to logics for machine-assisted reasoning about  programming-language semantics and to the automatic verification of  specifications in process calculi. Variations on nominal sets are used  in automata theory over infinite alphabets, with applications to  querying XML and databases, and also feature in work on models of  Homotopy Type Theory. When did it dawn on you that you had succeeded in  finding a very good model for name abstraction and fresh name  generation, and one that would have a  lot of impact? Did you imagine that your model would generate such a  large amount of follow-up work, leading to a whole body of work on  nominal computation theory? <br/><br/><b>AP: </b>No, to begin with I was very focussed on getting better techniques for computing and reasoning about syntax with bound names. But that only represents a part of the current broad landscape of nominal techniques, the part that mainly depends on the mathematical notion of "finite support" (a way of expressing, via name-permutation, that an object only involves finitely many names). Independently of us, some people realised that a related notion of finiteness, "orbit-finiteness" (which expresses that an object is finite modulo symmetries) is crucial for many applications of nominal techniques. I am referring to the work of Montanari and Pistore on pi-calculus and <a href="https://core.ac.uk/download/pdf/82414059.pdf">HD automata </a>using named sets (yet another equivalent of the Schanuel topos) and the work on automata theory over infinite alphabets (and much else besides) using "sets with atoms" by the Warsaw group (Bojanczyk, Klin, Lasota, Torunczyk,...). The latter is particularly significant because it considers groups of symmetry for atoms other than the full permutation group (in which the only property of an atom preserved under symmetry is its identity). <br/><br/><b>JG:</b> Yes, I did.  Nobody could anticipate the specific applications but I knew we were on to something, which is why I stayed on to build the field after the PhD.  The amount of structure was just too striking.  This showed early: e.g. in the equivariance properties, and the commutation of nominal atoms-abstraction with function-spaces.  When I sent the proof of this property to Andrew, at first he didn't believe it!  I had a sense that there was something deep going on and I still do. <br/><br/><b>LA: </b>What is the result of  yours on nominal techniques you are most proud of? And what are your  favourite results amongst those achieved by others on nominal computation?<b/><br/><b><br/></b><b>AP:</b> Not so much a specific result, but rather a logical concept, the freshness quantifier (which we wrote using an upside down "N" -- N stands for "New"). In informal practice when reasoning about syntax involving binders, one often chooses <i>some</i> fresh name for the bound variable, but then has to revise that choice in view of later ones; but fortunately <i>any </i>fresh name does as well as some particular one. This distinctive "some/any" property occurs all over the place when computing and reasoning about languages with binders and the freshness quantifier formalises it, in terms of the freshness ("not in the support of") relation and conventional quantifiers.  For the second part of your question I would choose two things. One is the work by Jamie with Fernandez and Mackie on <a href="https://www.sciencedirect.com/science/article/pii/S0890540106001635">nominalrewriting systems</a>, which won the PPDP Most Influential Paper 10-year Award in 2014. The second is the characterisation of orbit-finite sets with atoms in terms of "set-builder expressions"---see Klin et al, "<a href="https://www.mimuw.edu.pl/~szymtor/papers/locfin.pdf">Locally Finite Constraint Satisfaction Problems</a>", Proc. LICS 2015); it's a nice application of the classical model theory of homogeneous structures with interesting applications for languages that compute with finite structures. <br/><br/><b>JG:</b> Thanks for asking.  Aside from the initial papers, my work on nominal rewriting with Fernandez has probably had most impact.  However, I am rather fond of the thread of research going from Nominal Algebra, through the axiomatisation of substitution and first-order logic and the characterisation of quantification as a limit in nominal sets, and on to Stone duality.  It's a mathematical foundation built from a nominal perspective of naming and quantification and I hope that as the state of the art in nominal techniques advances and broadens, it might prove useful.  Andrew's book has been helpful in marking out nominal techniques as a field.  I also agree with Andrew that orbit-finiteness and the applications of this idea to transition systems and automata, is important.  I like the automata work for another concrete reason: nominal techniques were discovered in the context of names and binding in syntax, which has bequeathed a misconception that nominal techniques are <i>only</i> about this.  The Warsaw school of nominal techniques gives an independent illustration of the other applications of these ideas. <br/><b><br/></b><b><b>LA: </b></b>Twenty years have passed since your LICS 1999 paper and the  literature on variations on nominal techniques now contains over a hundred papers. Do you expect any further  development related to the theory and application of nominal techniques in the  coming years? What advice would you give to a PhD student who is  interested in working on topics related to nominal computation today?</div><div dir="ltr"><br/><b>AP: </b> For the purpose of answering your question, let's agree to divide LICS topics into Programming Languages and Semantics (PLS) versus Logic and Algorithms (LAS). (So long as we don't think of it as a dichotomy!) Then it seems to me that applications of nominal techniques to LAS are currently in the ascendant and show no sign of slowing down. My own interests are with PLS and there is still work to be done there. In particular, I would like better support for using nominal techniques within the mainstream interactive theorem proving systems: we have the Nominal Package of Urban and Berghofer for classical higher-order logic within Isabelle (which lead to Urban and Tasson winning the CADE Skolem Award in 2015), but nothing analogous for systems based on dependent type theory, such as Agda, Coq and Lean. Recent work of Swan (arXiv:1702.01556) gives us a better understanding of how to develop nominal sets within constructive logic; but I have yet to see a dependent type theory that both corresponds to some form of constructive nominal logic under Curry-Howard and is sufficiently simple that it appeals to users of systems lke Coq who want to mechanise programming language meta-theory in a nameful style. Really, I would like the utility of the FreshML programming language that Jamie, Mark Shinwell and I proposed in 2003 (and which Mark implemented as a patch of OCaml) restricted to total functional programming in the style of Agda; but I don't quite know how to achieve that. <br/><br/><b>JG:</b> Yes.  We are far from understanding nominal techniques and the field has a lot of life and will continue to surprise.  I've always believed that.  A key sticking-point right now is implementations.  I wrote a paper about this recently, on equivariance and the foundations of nominal techniques.  One point in the paper is a sketch for a next-generation nominal theorem-prover (based on ZFA + equivariance).  I'd like to see this carried out, so if anybody reading this is interested then please be in touch.  I'd also like to see nominal techniques implemented as a package in a language like Haskell, ML, or even Python!  If we can get this stuff into the working programmer's toolbox, in a way that just works and does not require special configuration, then that would be helpful.  I suspect that nominal techniques as currently presented in the maths papers, might not fit into a programming language at the moment.  The theory is too strong and may need weakened first.  We need a subset of nominal techniques weak enough to squeeze into an existing language, yet expressive enough for interesting applications.  Some general advice, specifically for the PhD student.  If you have an idea which most people around you don't understand, consider this may be a gap in the collective imagination.  There can be peer pressure when faced by incomprehension to blame yourself, back down, and think about something else.  By all means do this, but only if you yourself judge it right to do so. <br/><br/></div><b>LA: </b>Is there any general research-related lesson you have learnt in the process of working on nominal techniques?<br/><br/><b>AP: </b>On the one hand, don't lose sight of what application your theory is supposed to be good for; but on the other hand, let beauty and simplicity be your guide.<br/><br/><b>JG:</b> Yes:<br/><ul><li>Proving stuff is 30% of the work; convincing people is 70%. </li><li>It's the basic ideas that are hard, not the complicated theorems.</li><li>Competence and imagination are orthogonal. </li><li>It's doesn't have to be complex to be clever. </li><li>Elegant + applicable is a potent combination. </li><li>Seek out good listeners.  Give up quickly on bad ones.  Try to be a good listener. </li><li>Other people have a lot to teach you, but it might not be the things you expected. </li><li>Writing papers is fun.   </li></ul><b>LA: </b>Thanks to both of you for your willingness to answer my questions and congratulations for the awards you will be receiving this summer!</div>
    </content>
    <updated>2019-05-26T22:52:00Z</updated>
    <published>2019-05-26T22:52:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2019-05-27T08:25:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/076</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/076" rel="alternate" type="text/html"/>
    <title>TR19-076 |  The Equivalences of Refutational QRAT | 

	Leroy Chew, 

	Judith Clymo</title>
    <summary>The solving of Quantified Boolean Formulas (QBF) has been advanced considerably in the last two decades. In response to this, several proof systems have been put forward to universally verify QBF solvers. 
QRAT by Heule et al. is one such example of this and builds on technology from DRAT, a checking format used in propositional logic. 
Recent advances have shown conditional optimality results for QBF systems that use extension variables.
Since QRAT can simulate Extended Q-Resolution, we know it is strong, but we do not know if QRAT has the strategy extraction property as Extended Q-Resolution does. In this paper, we partially answer this question by showing that QRAT with a restricted reduction rule has strategy extraction (and consequentially is equivalent to Extended Q-Resolution modulo NP).
We also extend equivalence to another system, as we show an augmented version of QRAT known as QRAT+, developed by Lonsing and Egly, is in fact equivalent to the basic QRAT. We achieve this by constructing a line-wise simulation of QRAT+ using only steps valid in QRAT.</summary>
    <updated>2019-05-26T10:13:58Z</updated>
    <published>2019-05-26T10:13:58Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-27T17:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09761</id>
    <link href="http://arxiv.org/abs/1905.09761" rel="alternate" type="text/html"/>
    <title>An Efficient Approach for Super and Nested Term Indexing and Retrieval</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Md Faisal Mahbub Chowdhury, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Farrell:Robert.html">Robert Farrell</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09761">PDF</a><br/><b>Abstract: </b>This paper describes a new approach, called Terminological Bucket Indexing
(TBI), for efficient indexing and retrieval of both nested and super terms
using a single method. We propose a hybrid data structure for facilitating
faster indexing building. An evaluation of our approach with respect to widely
used existing approaches on several publicly available dataset is provided.
Compared to Trie based approaches, TBI provides comparable performance on
nested term retrieval and far superior performance on super term retrieval.
Compared to traditional hash table, TBI needs 80\% less time for indexing.
</p></div>
    </summary>
    <updated>2019-05-26T23:25:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09750</id>
    <link href="http://arxiv.org/abs/1905.09750" rel="alternate" type="text/html"/>
    <title>Approximation schemes for the generalized extensible bin packing problem</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levin:Asaf.html">Asaf Levin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09750">PDF</a><br/><b>Abstract: </b>We present a new generalization of the extensible bin packing with unequal
bin sizes problem. In our generalization the cost of exceeding the bin size
depends on the index of the bin and not only on the amount in which the size of
the bin is exceeded. This generalization does not satisfy the assumptions on
the cost function that were used to present the existing polynomial time
approximation scheme (PTAS) for the extensible bin packing with unequal bin
sizes problem. In this work, we show the existence of an efficient PTAS (EPTAS)
for this new generalization and thus in particular we improve the earlier PTAS
for the extensible bin packing with unequal bin sizes problem into an EPTAS.
Our new scheme is based on using the shifting technique followed by a solution
of polynomial number of $n$-fold programming instances. In addition, we present
an asymptotic fully polynomial time approximation scheme (AFPTAS) for the
related bin packing type variant of the problem.
</p></div>
    </summary>
    <updated>2019-05-26T23:35:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09719</id>
    <link href="http://arxiv.org/abs/1905.09719" rel="alternate" type="text/html"/>
    <title>Price of Dependence: Stochastic Submodular Maximization with Dependent Items</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Shaojie.html">Shaojie Tang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09719">PDF</a><br/><b>Abstract: </b>In this paper, we study the stochastic submodular maximization problem with
dependent items subject to a variety of packing constraints such as matroid and
knapsack constraints. The input of our problem is a finite set of items, and
each item is in a particular state from a set of possible states. After picking
an item, we are able to observe its state. We assume a monotone and submodular
utility function over items and states, and our objective is to select a group
of items adaptively so as to maximize the expected utility. Previous studies on
stochastic submodular maximization often assume that items' states are
independent, however, this assumption may not hold in general. This motivates
us to study the stochastic submodular maximization problem with dependent
items. We first introduce the concept of \emph{degree of independence} to
capture the degree to which one item's state is dependent on others'. Then we
propose a non-adaptive policy based on a modified continuous greedy algorithm
and show that its approximation ratio is $\alpha(1 - e^{-\frac{\kappa}{2} +
\frac{\kappa}{18m^2}} - \frac{\kappa + 2}{3m\kappa})$ where the value of
$\alpha$ is depending on the type of constraints, e.g., $\alpha=1$ for matroid
constraint, $\kappa$ is the degree of independence, e.g., $\kappa=1$ for
independent items, and $m$ is the number of items.
</p></div>
    </summary>
    <updated>2019-05-26T23:36:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09656</id>
    <link href="http://arxiv.org/abs/1905.09656" rel="alternate" type="text/html"/>
    <title>On the Average Case of MergeInsertion</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Florian Stober, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei=szlig=:Armin.html">Armin Weiß</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09656">PDF</a><br/><b>Abstract: </b>MergeInsertion, also known as the Ford-Johnson algorithm, is a sorting
algorithm which, up to today, for many input sizes achieves the best known
upper bound on the number of comparisons. Indeed, it gets extremely close to
the information-theoretic lower bound. While the worst-case behavior is well
understood, only little is known about the average case.
</p>
<p>This work takes a closer look at the average case behavior. In particular, we
establish an upper bound of $n \log n - 1.4005n + o(n)$ comparisons. We also
give an exact description of the probability distribution of the length of the
chain a given element is inserted into and use it to approximate the average
number of comparisons numerically. Moreover, we compute the exact average
number of comparisons for $n$ up to 148.
</p>
<p>Furthermore, we experimentally explore the impact of different decision trees
for binary insertion. To conclude, we conduct experiments showing that a
slightly different insertion order leads to a better average case and we
compare the algorithm to the recent combination with (1,2)-Insertionsort by
Iwama and Teruyama.
</p></div>
    </summary>
    <updated>2019-05-26T23:25:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09624</id>
    <link href="http://arxiv.org/abs/1905.09624" rel="alternate" type="text/html"/>
    <title>COBS: a Compact Bit-Sliced Signature Index</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bingmann:Timo.html">Timo Bingmann</a>, Phelim Bradley, Florian Gauger, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iqbal:Zamin.html">Zamin Iqbal</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09624">PDF</a><br/><b>Abstract: </b>We present COBS, a compact bit-sliced signature index, which is a cross-over
between an inverted index and Bloom filters. Our target application is to index
$k$-mers of DNA samples or $q$-grams from text documents and process
approximate pattern matching queries on the corpus with a user-chosen coverage
threshold. Query results may contain a number of false positives which
decreases exponentially with the query length and the false positive rate of
the index determined at construction time. We compare COBS to seven other index
software packages on 100 000 microbial DNA samples. COBS' compact but simple
data structure outperforms the other indexes in construction time and query
performance with Mantis by Pandey et al. on second place. However, different
from Mantis and other previous work, COBS does not need the complete index in
RAM and is thus designed to scale to larger document sets.
</p></div>
    </summary>
    <updated>2019-05-26T23:35:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09595</id>
    <link href="http://arxiv.org/abs/1905.09595" rel="alternate" type="text/html"/>
    <title>Non-monotone DR-submodular Maximization: Approximation and Regret Guarantees</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uuml=rr:Christoph.html">Christoph Dürr</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thang:Nguyen_Kim.html">Nguyen Kim Thang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Srivastav:Abhinav.html">Abhinav Srivastav</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tible:L=eacute=o.html">Léo Tible</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09595">PDF</a><br/><b>Abstract: </b>Diminishing-returns (DR) submodular optimization is an important field with
many real-world applications in machine learning, economics and communication
systems. It captures a subclass of non-convex optimization that provides both
practical and theoretical guarantees. In this paper, we study the fundamental
problem of maximizing non-monotone DR-submodular functions over down-closed and
general convex sets in both offline and online settings. First, we show that
for offline maximizing non-monotone DR-submodular functions over a general
convex set, the Frank-Wolfe algorithm achieves an approximation guarantee which
depends on the convex set. Next, we show that the Stochastic Gradient Ascent
algorithm achieves a 1/4-approximation ratio with the regret of $O(1/\sqrt{T})$
for the problem of maximizing non-monotone DR-submodular functions over
down-closed convex sets. These are the first approximation guarantees in the
corresponding settings. Finally we benchmark these algorithms on problems
arising in machine learning domain with the real-world datasets.
</p></div>
    </summary>
    <updated>2019-05-26T23:34:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09505</id>
    <link href="http://arxiv.org/abs/1905.09505" rel="alternate" type="text/html"/>
    <title>Graph Searches and Their End Vertices</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cao:Yixin.html">Yixin Cao</a>, Guozhen Rong, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Jianxin.html">Jianxin Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Zhifeng.html">Zhifeng Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09505">PDF</a><br/><b>Abstract: </b>Graph search, the process of visiting vertices in a graph in a specific
order, has demonstrated magical powers in many important algorithms. But a
systematic study was only initiated by Corneil et al.~a decade ago, and only by
then we started to realize how little we understand it. Even the apparently
na\"{i}ve question "which vertex can be the last visited by a graph search
algorithm," known as the end vertex problem, turns out to be quite elusive. We
give a full picture of all maximum cardinality searches on chordal graphs,
which implies a polynomial-time algorithm for the end vertex problem of maximum
cardinality search. It is complemented by a proof of NP-completeness of the
same problem on weakly chordal graphs.
</p>
<p>We also show linear-time algorithms for deciding end vertices of
breadth-first searches on interval graphs, and end vertices of lexicographic
depth-first searches on chordal graphs. Finally, we present $2^n\cdot
n^{O(1)}$-time algorithms for deciding the end vertices of breadth-first
searches, depth-first searches, maximum cardinality searches, and maximum
neighborhood searches on general graphs.
</p></div>
    </summary>
    <updated>2019-05-26T23:25:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09434</id>
    <link href="http://arxiv.org/abs/1905.09434" rel="alternate" type="text/html"/>
    <title>Automated Process Planning for Turning: A Feature-Free Approach</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Behandish:Morad.html">Morad Behandish</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nelaturi:Saigopal.html">Saigopal Nelaturi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Verma:Chaman_Singh.html">Chaman Singh Verma</a>, Mats Allard <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09434">PDF</a><br/><b>Abstract: </b>Turning is the most commonly available and least expensive machining
operation, in terms of both machine-hour rates and tool insert prices. A
practical CNC process planner has to maximize the utilization of turning, not
only to attain precision requirements for turnable surfaces, but also to
minimize the machining cost, while non-turnable features can be left for other
processes such as milling. Most existing methods rely on separation of surface
features and lack guarantees when analyzing complex parts with interacting
features. In a previous study, we demonstrated successful implementation of a
feature-free milling process planner based on configuration space methods used
for spatial reasoning and AI search for planning. This paper extends the
feature-free method to include turning process planning. It opens up the
opportunity for seamless integration of turning actions into a mill-turn
process planner that can handle arbitrarily complex shapes with or without a
priori knowledge of feature semantics.
</p></div>
    </summary>
    <updated>2019-05-26T23:38:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09401</id>
    <link href="http://arxiv.org/abs/1905.09401" rel="alternate" type="text/html"/>
    <title>Optimum Low-Complexity Decoder for Spatial Modulation</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Al=Nahhal:Ibrahim.html">Ibrahim Al-Nahhal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Basar:Ertugrul.html">Ertugrul Basar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dobre:Octavia_A=.html">Octavia A. Dobre</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ikki:Salama.html">Salama Ikki</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09401">PDF</a><br/><b>Abstract: </b>In this paper, a novel low-complexity detection algorithm for spatial
modulation (SM), referred to as the minimum-distance of maximum-length (m-M)
algorithm, is proposed and analyzed. The proposed m-M algorithm is a smart
searching method that is applied for the SM tree-search decoders. The behavior
of the m-M algorithm is studied for three different scenarios: i) perfect
channel state information at the receiver side (CSIR), ii) imperfect CSIR of a
fixed channel estimation error variance, and iii) imperfect CSIR of a variable
channel estimation error variance. Moreover, the complexity of the m-M
algorithm is considered as a random variable, which is carefully analyzed for
all scenarios, using probabilistic tools. Based on a combination of the sphere
decoder (SD) and ordering concepts, the m-M algorithm guarantees to find the
maximum-likelihood (ML) solution with a significant reduction in the decoding
complexity compared to SM-ML and existing SM-SD algorithms; it can reduce the
complexity up to 94% and 85% in the perfect CSIR and the worst scenario of
imperfect CSIR, respectively, compared to the SM-ML decoder. Monte Carlo
simulation results are provided to support our findings as well as the derived
analytical complexity reduction expressions.
</p></div>
    </summary>
    <updated>2019-05-26T23:21:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09356</id>
    <link href="http://arxiv.org/abs/1905.09356" rel="alternate" type="text/html"/>
    <title>Convergence Analyses of Online ADAM Algorithm in Convex Setting and Two-Layer ReLU Neural Network</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fang:Biyi.html">Biyi Fang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klabjan:Diego.html">Diego Klabjan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09356">PDF</a><br/><b>Abstract: </b>Nowadays, online learning is an appealing learning paradigm, which is of
great interest in practice due to the recent emergence of large scale
applications such as online advertising placement and online web ranking.
Standard online learning assumes a finite number of samples while in practice
data is streamed infinitely. In such a setting gradient descent with a
diminishing learning rate does not work. We first introduce regret with rolling
window, a new performance metric for online streaming learning, which measures
the performance of an algorithm on every fixed number of contiguous samples. At
the same time, we propose a family of algorithms based on gradient descent with
a constant or adaptive learning rate and provide very technical analyses
establishing regret bound properties of the algorithms. We cover the convex
setting showing the regret of the order of the square root of the size of the
window in the constant and dynamic learning rate scenarios. Our proof is
applicable also to the standard online setting where we provide the first
analysis of the same regret order (the previous proofs have flaws). We also
study a two layer neural network setting with ReLU activation. In this case we
establish that if initial weights are close to a stationary point, the same
square root regret bound is attainable. We conduct computational experiments
demonstrating a superior performance of the proposed algorithms.
</p></div>
    </summary>
    <updated>2019-05-26T23:24:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.09320</id>
    <link href="http://arxiv.org/abs/1905.09320" rel="alternate" type="text/html"/>
    <title>Solving Random Systems of Quadratic Equations with Tanh Wirtinger Flow</title>
    <feedworld_mtime>1558828800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Zhenwei Luo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Ye.html">Ye Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.09320">PDF</a><br/><b>Abstract: </b>Solving quadratic systems of equations in n variables and m measurements of
the form $y_i = |a^T_i x|^2$ , $i = 1, ..., m$ and $x \in R^n$ , which is also
known as phase retrieval, is a hard nonconvex problem. In the case of standard
Gaussian measurement vectors, the wirtinger flow algorithm Chen and Candes
(2015) is an efficient solution. In this paper, we proposed a new form of
wirtinger flow and a new spectral initialization method based on this new
algorithm. We proved that the new wirtinger flow and initialization method
achieve linear sample and computational complexities. We further extended the
new phasing algorithm by combining it with other existing methods. Finally, we
demonstrated the effectiveness of our new method in the low data to parameter
ratio settings where the number of measurements which is less than
information-theoretic limit, namely, $m &lt; 2n$, via numerical tests. For
instance, our method can solve the quadratic systems of equations with gaussian
measurement vector with probability $\ge 97\%$ when $m/n = 1.7$ and $n = 1000$,
and with probability $\approx 60\%$ when $m/n = 1.5$ and $n = 1000$.
</p></div>
    </summary>
    <updated>2019-05-26T23:21:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/05/25/more-matching-mimicking</id>
    <link href="https://11011110.github.io/blog/2019/05/25/more-matching-mimicking.html" rel="alternate" type="text/html"/>
    <title>More matching-mimicking networks</title>
    <summary>My paper with Vijay Vazirani on parallel matching (soon to appear in SPAA) is based on the idea of a “matching-mimicking network”. If is a graph with a designated set of terminal vertices, then a matching-mimicking network for is another graph with the same terminals that has the same pattern of matchings. Here, by a pattern of matchings, I mean a family of subsets of , the subsets that can be covered by a matching that also covers all non-terminal vertices. We included a messy case analysis that, after some simplifications due to symmetry, had 21 cases for the matching mimicking networks on at most three terminals.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My <a href="https://11011110.github.io/blog/2018/02/01/parallel-matching-in.html">paper with Vijay Vazirani on parallel matching</a> (soon to appear in <a href="http://spaa.acm.org/2019/">SPAA</a>) is based on the idea of a “matching-mimicking network”. If  is a graph with a designated set  of terminal vertices, then a matching-mimicking network for  is another graph  with the same terminals that has the same pattern of matchings. Here, by a <em>pattern of matchings</em>, I mean a family of subsets of , the subsets that can be covered by a matching that also covers all non-terminal vertices. We included a messy case analysis that, after some simplifications due to symmetry, had 21 cases for the matching mimicking networks on at most three terminals.</p>

<p>By now, I think I understand patterns of matchings a lot better, enough to do the three-terminal case in only four cases and to extend the analysis to four terminals in only seven more cases. The starting point is the observation that these patterns of matchings are <a href="https://en.wikipedia.org/wiki/Delta-matroid">even Δ-matroids</a>.</p>

<p>One way to think of a Δ-matroid is that it’s just a convex polyhedron or polytope in Euclidean space of some dimension , with the properties that all vertex coordinates are  or  and all edge lengths are  or . An even Δ-matroid has the stronger property that all edge lengths are , as is true for the three-dimensional regular tetrahedron with vertex coordinates (written in a more compact form as bitvectors) , , , and .</p>

<p style="text-align: center;"><img alt="Regular tetrahedron formed from alternating vertices of a cube" src="https://11011110.github.io/blog/assets/2019/tet-in-cube.svg"/></p>

<p>Alternatively one can consider the same kind of structure to be a family of sets, drawn from a universe of  elements that correspond to the dimensions of the space. Each set in the family corresponds to a vertex of the polytope and includes the elements whose coordinates are one. So over the three-element set  the same regular tetrahedron can be written as the family of sets</p>



<p>When expressed in this way, the sets of a Δ-matroid obey an exchange axiom: if two sets  and  differ on whether they include some element , then there must exist an element  on which they also differ, so that the symmetric difference of sets  also belongs to the Δ-matroid. By repeatedly applying this axiom one can connect  to  by a geodesic path (in Hamming distance) of two-element moves. For the bases of a matroid, we have a stronger requirement that one of the two elements belongs to  and the other belongs to , or equivalently that all sets have the same size, but a Δ-matroid relaxes this requirement. It’s not even required that ! But in an even Δ-matroid  and  must be distinct, because otherwise the step would be along an edge of length one. Another way of expressing the extra requirements of an even Δ-matroid over an arbitrary Δ-matroid is that all sets must have the same parity (all have even size, or all have odd size).</p>

<p>So anyway, back to matching. Suppose that both  and  are sets drawn from a pattern of matchings. Choose arbitrarily a matching representing each set. Then the symmetric difference of these matchings is a collection of disjoint alternating paths and cycles, and we can get from  to  by 
a sequence of steps in which we take the symmetric difference of the current matching by one of the alternating paths. So this gives us not just one geodesic from  to  but a lot of different geodesics, one for each ordering of the alternating paths. Expressed as an exchange axiom, this means that when two sets  and  differ, the elements on which they differ can be partitioned into pairs, the symmetric differences with which can be performed independently. You can pick any subset of the pairs of differing elements, and change each of those pairs, leaving the rest alone. Because this is a strengthening of the even Δ-matroid axiom, every matching pattern is an even Δ-matroid.</p>

<p>Expressed in polyhedral terms, this stronger exchange axiom means that every two vertices at distance  from each other are connected by a -dimensional hypercube with side length . This is a little weird, because we started with a hypercube but then eliminated half of its vertices (by the parity condition) to get something else. Now we have hypercubes again, of lower dimension. They must be tilted with respect to the coordinate axes: each axis of one of these lower-dimensional hypercubes is tilted at a 45 degree angle with respect to the coordinate system of the overall polytope.</p>

<p>Not every even Δ-matroid obeys this sub-hypercube property. On the other hand, I was expecting the matching patterns that are matroids (all sets are the same size) to be transversal matroids (maximal subsets of vertices on one side of a bipartite graph that can be covered by a matching), and they aren’t. There is a six-element non-transversal matroid, whose six elements are the edges of a triangle with doubled edges and whose sets are pairs of edges from different sides of the triangle. But it is the pattern of matchings of a tree in which the (non-terminal) root has three children, each of which has two terminals as its children.</p>

<p>Conveniently, whether a 0-1 polyhedron can be represented by a pattern of matchings depends only on its shape and not on its orientation. You can obviously permute the coordinates of a polyhedron that represents a pattern of matchings, by relabeling which coordinate corresponds to which terminal vertex. But you can also reflect the polyhedron across any one of its coordinates by modifying the graph whose matchings represent it in the following way: turn the  terminal vertex for that coordinate into a non-terminal, and attach a new degree-one terminal vertex to it. These permutations and reflections generate all the symmetries of the hypercube in which the 0-1 polyhedron lives. So to find small matching-mimicking networks, we only need to look at one representative 0-1 polyhedron in each symmetry class. If we find a small network for this representative, we can modify it to create a different small matching-mimicking network for every other 0-1 polyhedron with the same shape.</p>

<p>So what are the possible shapes? Let’s define the dimension of a Δ-matroid to be the number of coordinates of the polytope that take both values,  and , at different vertices. Then a 0-dimensional even Δ-matroid must be a single point (), there are no 1-dimensional even Δ-matroids, and a two-dimensional even Δ-matroid must be a line segment (). There are two three-dimensional even Δ-matroids: a triangle  and the tetrahedron shown above, . The cube exchange axiom for patterns of matching starts to kick in for four-dimensional even Δ-matroids, whose vertices must be subsets of the four-dimensional hyperoctahedron . (This is the shape formed from a four-dimensional hypercube by keeping only vertices with the same parity as each other, just as we formed a regular tetrahedron by doing the same thing to a three-dimensional cube.) Here’s a drawing of  from <a href="https://11011110.github.io/blog/2010/09/26/in-response-to.html">an earlier post</a>:</p>

<p style="text-align: center;"><img alt="The hyperoctahedral graph K_{2,2,2,2}" src="https://11011110.github.io/blog/assets/2010/k7/cocktail2.svg"/></p>

<p>If there are no two opposite vertices, we get  (a regular tetrahedron, again, but embedded in a four-dimensional way into the hypercube). Otherwise, we must take at least two pairs of opposite vertices to form a square, and the cases are  (only the square),  (a square pyramid),  (an octahedron), ,  (an octahedral pyramid), and  (the hyperoctahedron). All of these polyhedra can be represented as matching-mimicking networks with the additional property that all vertices are terminals:</p>

<p style="text-align: center;"><img alt="Matching-mimicking networks for up to four terminals" src="https://11011110.github.io/blog/assets/2019/4-terminal-mm.svg"/></p>

<p>Based on these small examples, it’s tempting to guess that when a pattern of matchings includes the empty set, the whole pattern is just the set of matchings on a graph whose edges are the pairs of terminals in the pattern. But it isn’t true. The square pyramid , for instance, can represent the pattern of matchings</p>



<p>with the empty set at the apex of the pyramid. In this pattern, even though one can match terminal pairs <span style="white-space: nowrap;">— or —,</span> one can’t take the union of those two matchings and cover all four terminals. (This is what you get by reflecting the two middle terminals of the network shown above for ; its matching-mimicking network is a tree with two interior non-terminals and four terminal leaves.) My guess is that the number of patterns of matching should grow quickly relative to the number of graphs, so for large enough numbers of terminals it should not be possible to use graphs without non-terminals or their complements. But I haven’t taken the case analysis far enough to find an example of this.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102160605632804102">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-05-25T21:32:00Z</updated>
    <published>2019-05-25T21:32:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-05-26T05:06:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/075</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/075" rel="alternate" type="text/html"/>
    <title>TR19-075 |  Relations and Equivalences Between Circuit Lower Bounds and Karp-Lipton Theorems | 

	Lijie Chen, 

	Dylan McKay, 

	Cody Murray, 

	Ryan Williams</title>
    <summary>Relations and Equivalences Between Circuit Lower Bounds and Karp-Lipton Theorems

A frontier open problem in circuit complexity is to prove P^NP is not in SIZE[n^k] for all k; this is a necessary intermediate step towards NP is not in P/poly. Previously, for several classes containing P^NP, including NP^NP, ZPP^NP, and S_2 P, such lower bounds have been proved via Karp-Lipton-style Theorems: to prove C is not in SIZE[n^k] for all k, we show that C subset Ppoly implies a ``collapse'' D = C for some larger class D, where we already know D is not in SIZE[n^k] for all k. 

It seems obvious that one could take a different approach to prove circuit lower bounds for P^NP that does not require proving any Karp-Lipton-style theorems along the way. We show this intuition is wrong: (weak) Karp-Lipton-style theorems for P^NP are equivalent to fixed-polynomial size circuit lower bounds for P^NP. That is, P^NP not subset SIZE[n^k] for all k if and only if (NP is in P/poly implies PH is in i.o.-P^NP/n).
		
Next, we present new consequences of the assumption NP is in P/poly, towards proving similar results for NP circuit lower bounds. We show that under the assumption, fixed-polynomial circuit lower bounds for NP, nondeterministic polynomial-time derandomizations, and various fixed-polynomial time simulations of NP are all equivalent. Applying this equivalence, we show that circuit lower bounds for NP imply better Karp-Lipton collapses. That is, if NP is not in SIZE[n^k] for all k, then for all C in { ParP, PP, PSPACE, EXP }, C is in P/poly implies C is in i.o.-NP/n^eps for all eps &gt; 0. Note that unconditionally, the collapses are only to MA and not NP.
		
We also explore consequences of circuit lower bounds for a sparse language in NP. Among other results, we show if a polynomially-sparse NP language does not have n^(1+eps)-size circuits, then MA is in i.o.-NP/O(log n), MA is in i.o.-P^{NP[O(log n)]}, and NEXP is not in SIZE[2^o(m)]. Finally, we observe connections between these results and the ``hardness magnification'' phenomena described in recent works.</summary>
    <updated>2019-05-25T18:06:09Z</updated>
    <published>2019-05-25T18:06:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-27T17:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15910</id>
    <link href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/" rel="alternate" type="text/html"/>
    <title>Selected Papers at CCC 2019</title>
    <summary>Some papers from the accepted list of this year’s Computational Complexity Conference [ UB CSE ] Alan Selman is a long-time friend of Ken and I, and is a long-time researcher in complexity theory. Alan was the first president of the organizing body for the Computational Complexity Conferences (CCC). Today we salute the th edition […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Some papers from the accepted list of this year’s Computational Complexity Conference</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/unknown-122/" rel="attachment wp-att-15912"><img alt="" class="alignright  wp-image-15912" src="https://rjlipton.files.wordpress.com/2019/05/unknown-1.jpeg?w=150" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ UB CSE ]</font></td>
</tr>
</tbody>
</table>
<p>
Alan Selman is a long-time friend of Ken and I, and is a long-time researcher in complexity theory. Alan was the first president of the organizing <a href="https://www.computationalcomplexity.org/governance.php">body</a> for the Computational Complexity Conferences (CCC). </p>
<p>
Today we salute the <img alt="{0b100010}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0b100010%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0b100010}"/>th edition of the conference and discuss some of the accepted papers.</p>
<p>
The conference, and the governing body, have changed names over the years; by any name it remains an important conference. Alan <a href="https://www.computationalcomplexity.org/documents/first-cfp.pdf">chaired</a> the first program committee with Steve Mahaney and <a href="https://dl.acm.org/citation.cfm?id=648296&amp;picked=prox">edited</a> the first proceedings, in 1986. </p>
<p>
Ken recently saw Alan two weeks ago at the banquet for the <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">symposium</a> honoring Steve Cook at the University of Toronto. We will cover event, once <a href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/">exams</a> are done. Ken saw another of the CCC past presidents there—if you wish to guess who, a hint is it was one of Cook’s past students.</p>
<ul>
<li>
Dieter van Melkebeek, 2012-2018 <p/>
</li><li>
Peter Bro Miltersen, 2009-2012 <p/>
</li><li>
Pierre McKenzie, 2006-2009 <p/>
</li><li>
Lance Fortnow, 2000-2006 <p/>
</li><li>
Eric Allender, 1997-2000 <p/>
</li><li>
Steven Homer, 1994-1997 <p/>
</li><li>
Timothy Long, 1992-1994 <p/>
</li><li>
Stephen Mahaney, 1988-1992 <p/>
</li><li>
Alan Selman, 1985-1988
</li></ul>
<p>
Although we do not usually do announcements, we note from the conference <a href="https://computationalcomplexity.org">website</a>:</p>
<blockquote><p><b> </b> <em> Details of the local arrangements for CCC 2019 and the preceding events, including the DIMACS Day of Tutorials, are available. Early registration runs till June 26. </em>
</p></blockquote>
<p>
</p><p/><h2> Six Papers with Some Comments </h2><p/>
<p/><p>
Here are some papers that I, Dick, found interesting from the list of accepted papers. All accepted papers are interesting, of course. I selected six that were on topics that were directly connected with my interests.</p>
<p>
<b>Criticality of Regular Formulas</b>—<a href="http://www.math.toronto.edu/rossman/criticality.pdf">paper</a><br/>
Benjamin Rossman<br/>
<i>I thought this was about regular expressions. Shows something about me.</i> Here “regular” means the in-degree of gates being the same at each level of the circuit. This condition seems likely to be removable as Rossman conjectures, but I doubt it will be easy. The term “criticality” is a parameter that measures how much a random restriction reduces the size of a formula. Think switching lemma.</p>
<p>
<b>Typically-Correct Derandomization for Small Time and Space</b>—<a href="https://arxiv.org/abs/1711.00565">paper</a><br/>
William Hoza<br/>
<i>I like the notion of typically-correct.</i> Their algorithms work by treating the input as a source of randomness. This idea was pioneered by Oded Goldreich and Avi Wigderson. The title of their 2002 <a href="http://www.wisdom.weizmann.ac.il/~oded/p_rnd02.html">article</a> “Derandomization that is rarely wrong from short advice that is typically good”, gives away how one can prove such results. </p>
<p>
<b>Optimal Short-Circuit Resilient Formulas</b>—<a href="https://arxiv.org/abs/1807.05014">paper</a><br/>
Mark Braverman, Klim Efremenko, Ran Gelles, and Michael Yitayew <br/>
<i>This is on a kind of fault-tolerance.</i> They consider fault-tolerant boolean formulas in which the output of a faulty gate is stuck at one of the gate’s inputs. This is an interesting model of errors, and they show roughly: any formula can be converted into a formula that is not too much bigger and survives even if about <img alt="{1/5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/5}"/> of the gates are faulty. A surprise is that they use a method related to <i>blockchains</i>. Hmmmmm. Interesting.</p>
<p>
<b>Fourier and Circulant Matrices are Not Rigid</b>—<a href="https://arxiv.org/pdf/1902.07334.pdf">paper</a><br/>
Allen Liu and Zeev Dvir <br/>
<i>A matrix is rigid if its rank cannot be reduced significantly by changing a small number of entries.</i> As you probably know there are plenty of rigid matrices—take random ones—but no provable examples of explicit ones. Their beautiful results prove that specific families of matrices are not rigid. These families include ones that were long thought to be rigid. The highlight of this work could be that it suggests new families that may be rigid. </p>
<p>
<b>Average-Case Quantum Advantage with Shallow Circuits</b>—<a href="https://arxiv.org/pdf/1810.12792.pdf">paper</a><br/>
François Le Gall <br/>
<i>A quest, the quest that tops all others—is the search for evidence that quantum computers are better than classic ones.</i> Of course, this is nearly impossible, since P=PSPACE is an open problem. So one looks at special classes of computations. See <a href="https://arxiv.org/pdf/1612.05903.pdf">here</a> for how the quest for “quantum advantage” meets up with computational complexity.</p>
<p>
<b>Relations and Equivalences Between Circuit Lower Bounds and Karp-Lipton Theorems</b><br/>
<a href="https://eccc.weizmann.ac.il/report/2019/075/">paper</a><br/>
Lijie Chen, Dylan McKay, Cody Murray, and Ryan Williams <br/>
<i>Of course I think this is an interesting paper.</i> There is the famous H-score. Perhaps there could be a T-score. This would be the number of times your name is in the title of a published paper. Thus Ron Rivest, for example, has a huge T-score.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What are your selected papers? </p>
<p>
[Added link to Relations and Equivalences… paper]</p></font></font></div>
    </content>
    <updated>2019-05-25T15:10:21Z</updated>
    <published>2019-05-25T15:10:21Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="Results"/>
    <category term="complexity"/>
    <category term="conferences"/>
    <category term="papers"/>
    <category term="selected"/>
    <category term="Selman"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-27T17:21:45Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4982821151012814632</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4982821151012814632/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/logic-then-and-now.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4982821151012814632" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4982821151012814632" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/logic-then-and-now.html" rel="alternate" type="text/html"/>
    <title>Logic Then and Now</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This week I attended the <a href="https://asl2019.commons.gc.cuny.edu/">Association of Symbolic Logic North American Annual Meeting</a> in New York City, giving a talk on P v NP.<br/>
<br/>
First I must share the announcement that ASL member Tuna Antinel of Lyon 1 University has been arrested in Turkey for his political beliefs. <a href="http://math.univ-lyon1.fr/SoutienTunaAltinel/">This website</a> (<a href="https://webusers.imj-prg.fr/~adrien.deloro/">English version</a>) has details and how to show your support.<br/>
<br/>
I last attended the ASL annual meeting at Notre Dame in 1993 as a young assistant professor. Back then I talked about <a href="https://doi.org/10.1137/S0097539793248305">then recent work</a> using a special kind of generic oracle to make the Berman-Hartmanis isomorphism conjecture true. I remember someone coming up to me after the talk saying how excited they were to see such applications of logic. I'm not a theoretical computer scientist, I'm a applied logician.<br/>
<br/>
I asked at my talk this year and maybe 2-3 people were at that 1993 meeting. The attendance seemed smaller and younger, though that could be my memory playing tricks. I heard that the 2018 meeting in Macomb, Illinois drew a larger crowd. New York is expensive and logicians don't get large travel budgets.<br/>
<br/>
Logic like theoretical computer science has gotten more specialized so I was playing catch up trying to follow many of the talks. Mariya Soskova of Wisconsin talked about enumeration degrees that brought me back to the days I sat in logic classes and talks at the University of Chicago. A set A is enumeration reducible to B if from an enumeration of B you can compute an enumeration of A and Mariya gave a great overview of this area.<br/>
<br/>
I learned about the status of an open problem for Turing reducibility: Is there a non-trivial automorphism of the Turing Degrees? A degree is the equivalence class where each class are the languages all computably Turing-reducible to each other. So the question asks if there is a bijection f mapping degrees to degrees, other than identity, that preserves reducibility or lack thereof.<br/>
<br/>
Here's what's known: There are countably many such automorphisms. There is a definable degree C in the arithmetic hierarchy, such that if f(C) = C then f is the identity. Also if f is the identity on all the c.e.-degrees (those equivalence classes containing a computably enumerable set), then f is the identity on all the degrees. Still open if there is more than one automorphism.<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-05-24T13:14:00Z</updated>
    <published>2019-05-24T13:14:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-27T15:12:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=355</id>
    <link href="https://tcsplus.wordpress.com/2019/05/22/tcs-talk-wednesday-may-29th-lior-kamma-aarhus-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 29th — Lior Kamma, Aarhus University</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 29th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 19:00 Central European Summer Time, 17:00 UTC). Lior Kamma from Aarhus University will speak about “Lower Bounds for Multiplication via Network Coding” (abstract below). Please make sure you reserve a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 29th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 19:00 Central European Summer Time, 17:00 UTC). <strong>Lior Kamma</strong> from Aarhus University will speak about “<em>Lower Bounds for Multiplication via Network Coding</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Multiplication is one of the most fundamental computational problems, yet its true complexity remains elusive. The best known upper bound, very recently proved by Harvey and Van Der Hoven shows that two <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0" title="n"/>-bit numbers can be multiplied via a boolean circuit of size <img alt="O(n \lg n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n+%5Clg+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(n \lg n)"/>.</p>
<p>We prove that if a central conjecture in the area of network coding is true, then any constant degree Boolean circuit for multiplication must have size <img alt="\Omega(n \lg n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28n+%5Clg+n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Omega(n \lg n)"/>, thus (conditioned on the conjecture) completely settling the complexity of multiplication circuits. We additionally revisit classic conjectures in circuit complexity, due to Valiant, and show that the network coding conjecture also implies one of Valiant’s conjectures.</p>
<p>Joint work with Peyman Afshani, Casper Freksen and Kasper Green Larsen</p></blockquote>
<p><span id="more-355"/></p></div>
    </content>
    <updated>2019-05-22T20:56:24Z</updated>
    <published>2019-05-22T20:56:24Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-05-27T17:24:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=639</id>
    <link href="https://emanueleviola.wordpress.com/2019/05/22/statement-of-concern-regarding-marijuana-in-massachusetts/" rel="alternate" type="text/html"/>
    <title>Statement of concern regarding Marijuana in Massachusetts</title>
    <summary>You can read it here. If you don’t want to click, some key takeaways are: We disagree with how marijuana policy is being shaped in the Commonwealth. The science is clear; marijuana, specifically the psychoactive chemical THC (delta-9-tetrahydrocannabinol), has the potential to do significant harm to public health. Diversion of high THC products (≥10%), vapes […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>You can read it <a href="https://gallery.mailchimp.com/d93a0b5fcff1daf7b38ecd20c/files/4a4118a7-45ed-4910-b3d1-c64a5e822417/MA_MJ_Policy_Statement_of_Concern_5_9_19_FINAL.pdf">here</a>. If you don’t want to click, some key takeaways are:</p>
<ul>
<li><span class="fontstyle0">We disagree with how marijuana policy is being shaped in the Commonwealth.</span></li>
<li><span class="fontstyle0">The science is clear; marijuana, specifically the psychoactive chemical THC (delta-9-tetrahydrocannabinol), has the potential to do significant harm to public health.</span></li>
<li><span class="fontstyle0">Diversion of high THC products (≥10%), vapes and edibles, to MA youth is a growing concern.</span></li>
<li><span class="fontstyle0">When public health is not prioritized in the regulation of addictive substances, the public and our young people are put at risk.</span></li>
</ul>
<p>You can also find in the statement a list of negative effects of THC.  This is all signed by a dozen+ doctors. The various marijuana players with zero medical knowledge will probably dismiss the experts’ opinion with, at best, a shrug. Instead, they are looking into opening <a href="https://www.boston.com/news/local-news/2019/05/16/massachusetts-marijuana-cafes-social-consumption">marijuana cafes</a>. And the first marijuana retail store will open in Newton <a href="https://patch.com/massachusetts/newton/newtons-first-recreational-marijuana-shop-has-grand-opening-date">this Saturday</a>.</p>
<p>If you want to get even more worked up about marijuana reading my <a href="https://emanueleviola.wordpress.com/2019/01/27/selling-your-town-to-the-marijuana-industry/">previous post</a> might help.</p>
<p>Finally, <a href="http://www.mapreventionalliance.org/">on June 5^th there will be a luncheon event at the JFK Library titled: Marijuana: Addiction, Mental Health and Policy – Advances in Research…What have we learned in the past 5 years?</a></p></div>
    </content>
    <updated>2019-05-22T19:45:31Z</updated>
    <published>2019-05-22T19:45:31Z</published>
    <category term="Uncategorized"/>
    <category term="health"/>
    <category term="marijuana"/>
    <author>
      <name>Emanuele</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2019-05-27T17:23:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/074</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/074" rel="alternate" type="text/html"/>
    <title>TR19-074 |  Finding a Nash Equilibrium Is No Easier Than Breaking Fiat-Shamir | 

	Chethan Kamath, 

	Arka Rai Choudhuri, 

	Pavel Hubacek, 

	Krzysztof Pietrzak, 

	Alon Rosen, 

	Guy Rothblum</title>
    <summary>The Fiat-Shamir heuristic transforms a public-coin interactive proof into a non-interactive argument, by replacing the verifier with a cryptographic hash function that is applied to the protocol’s transcript. Constructing hash functions for which this transformation is sound is a central and long-standing open question in cryptography.

We show that solving the End-of-Metered-Line problem is no easier than breaking the soundness of the Fiat-Shamir transformation when applied to the sumcheck protocol. In particular, if the transformed protocol is sound, then any hard problem in #P gives rise to a hard distribution in the class CLS, which is contained in PPAD.

Our main technical contribution is a stateful incrementally verifiable procedure that, given a SAT instance over n variables, counts the number of satisfying assignments. This is accomplished via an exponential sequence of small steps, each computable in time poly(n). Incremental verifiability means that each intermediate state includes a sumcheck-based proof of its correctness, and the proof can be updated and verified in time poly(n).

Combining our construction with a hash family proposed by Canetti et al. [STOC 2019] gives rise to a distribution in the class CLS, which is provably hard under the assumption that any one of a class of fully homomorphic encryption (FHE) schemes has almost-optimal security against quasi-polynomial time adversaries, and under the additional worst-case assumption that there is no polynomial time algorithm for counting the number of satisfying assignments for formulas over a polylogarithmic number of variables.</summary>
    <updated>2019-05-22T18:28:29Z</updated>
    <published>2019-05-22T18:28:29Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-27T17:21:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/05/21/congratulations-dr-tillman</id>
    <link href="https://11011110.github.io/blog/2019/05/21/congratulations-dr-tillman.html" rel="alternate" type="text/html"/>
    <title>Congratulations, Dr. Tillman!</title>
    <summary>Today I participated in the successful dissertation defense of Bálint Tillman, a student of Athina Markopoulou in the UCI Graduate Program in Networked Systems.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Today I participated in the successful dissertation defense of Bálint Tillman, a student of Athina Markopoulou in the <a href="http://www.networkedsystems.uci.edu/">UCI Graduate Program in Networked Systems</a>.</p>

<p>Bálint has been investigating problems connected with the <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Gallai_theorem">Erdős–Gallai theorem</a>, which states that it is possible to test whether a sequence of numbers is the degree distribution of a graph (the sequence of numbers of vertices of each possible degree) and if so, find a graph with that degree distribution, in polynomial time. The degree distribution can be extended to a matrix called the <em>joint degree distribution</em>, which specifies the number of pairs of adjacent vertices with each combination of degrees, and to higher-order tensors specifying the degrees of subgraphs with more than two vertices.</p>

<p>In his INFOCOM 2015 paper “<a href="https://doi.org/10.1109/INFOCOM.2015.7218534">Construction of simple graphs with a target joint degree matrix and beyond</a>”, Bálint showed that one can recognize the joint degree distributions of simple graphs, and reconstruct a graph with that distribution, in polynomial time. The algorithm works equally well when the vertices are distinguished in other ways than by degree and the input matrix specifies the target number of edges with each pair of degrees between each class of vertices. Later, in KDD 2017, he extended these results to directed graphs and at NetSci 2018 he showed how to find a realization with as few connected components as possible.</p>

<p>On the other hand, if one adds only a little bit of extra information to the joint degree distribution, such as the total number of triangles in the graph, it becomes NP-complete to recognize whether the input describes a valid graph and NP-hard to reconstruct a graph that realizes a given description. This comes from a poster by Bálint with Will Devanny and me at NetSci 2016, where we found the reduction from graph 3-coloring depicted below.</p>

<p style="text-align: center;"><img alt="NP-completeness reduction from 3-coloring to realizability of joint degree matrices with numbers of triangles" src="https://11011110.github.io/blog/assets/2019/jdm-tri-hard.svg"/></p>

<p>To perform an NP-hardness reduction, one should start with a graph for which it’s hard to test 3-colorability, and translate it into an instance of whatever other problem you want to prove hard. But instead let’s pretend for now that we start with a little more information: a graph that’s known to be 3-colorable, and a specific 3-coloring of it.
To turn this into a hard problem for realizability of joint distribution plus number of triangles, we add a triangle to the graph, representing the three colors, and connect each original graph vertex to the new triangle vertex for its color. Then we add enough hair (in the form of degree-one vertices) to the augmented graph to make all vertices have distinct degrees, except within the triangle of new vertices where all three degrees should be the same. Now take as the result of the reduction the pair  of the joint degree distribution and number of triangles in the augmented graph.</p>

<p>But now the trick is that  can be computed directly from your starting graph, without knowing its coloring or even whether it is colorable.
Because the degrees are distinct, and  tells you the number of edges for each combination of degrees, any realization of  must contain a copy of your starting graph augmented by a triangle. The graph and the triangle might be connected to each other differently than they were before, but their connection pattern must still correspond to a different valid 3-coloring, because otherwise you would form some extra triangles in the graph (one for each invalidly-colored edge) and not correctly match the value of . So  is realizable if and only if your starting graph has a 3-coloring. This reduction proves that testing realizability of pairs  is NP-complete.</p>

<p>There’s even more material along these lines in Bálint’s dissertation, but some of it is not yet published. I think the plan is to get all of that submitted over the summer before
Bálint starts a new position at Google.
Congratulations, Bálint!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102137081740561085">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-05-21T18:20:00Z</updated>
    <published>2019-05-21T18:20:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-05-26T05:06:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15894</id>
    <link href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/" rel="alternate" type="text/html"/>
    <title>Making Up Tests</title>
    <summary>It’s harder to make up tests than to take them [ Recent photo ] Ken Regan has been busy these last few days working on making a final exam, giving the exam, and now grading the exam. Today Ken and I want to talk about tests. I also have a test for you. You can […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>It’s harder to make up tests than to take them</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/ken/" rel="attachment wp-att-15897"><img alt="" class="alignright  wp-image-15897" src="https://rjlipton.files.wordpress.com/2019/05/ken.png?w=150" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Recent photo ]</font></td>
</tr>
</tbody>
</table>
<p>
Ken Regan has been busy these last few days working on making a final exam, giving the exam, and now grading the exam. </p>
<p>
Today Ken and I want to talk about tests.</p>
<p>
I also have a test for you. You can jump right to our test of knowledge. Do not, please, use any search tools, especially Google.<span id="more-15894"/></p>
<p>
</p><p/><h2> Test Theory </h2><p/>
<p/><p>
Ken recently made up a final exam. We both have had to make countless tests over the years. I was never trained in how to make a good test. Nor how to make a test at all. I am still puzzled about how to do it.</p>
<p>
Avi Wigderson once told me that Michael Rabin only asked questions on his exams that he had stated already in lectures. Is there a theory of what makes a proper test? I do not know any.</p>
<p>
Suppose that before the exam we lectured and the students learned <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> and <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>: Here <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> are true statements and <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> are false statements. A rote type question might be: </p>
<blockquote><p><b> </b> <em> <i>Is the statement <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S}"/> true or false?</i> </em>
</p></blockquote>
<p>Here <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> would be in either <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> or <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. This type of question is purely a memory problem. </p>
<p>
A more difficult test would have questions like: </p>
<blockquote><p><b> </b> <em> <i>Is <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S}"/> true or false</i> </em>
</p></blockquote>
<p>Here <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> would be equivalent to some <img alt="{S'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S'}"/> that is in <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> or in <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. The equivalence between <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> and <img alt="{S'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S'}"/> would require only the application of a few simple logical rules. This is much harder for students. In the limit we could have <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> and <img alt="{S'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S'}"/> far apart, even could have it an open problem if <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> and <img alt="{S'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S'}"/> are equivalent. </p>
<p>
</p><p/><h2> Our Test </h2><p/>
<p/><p>
<b>No looking at Google please.</b></p>
<p>
<i>Question 1</i>: We all know that Dick Karp created the P=NP question. What is Dick’s middle name? </p>
<ol>
<li>
Mark<p/>
<p/></li><li>
Manning <p/>
</li><li>
Mathew <p/>
</li><li>
Richard
</li></ol>
<p>
<i>Question 2</i>: This year is the fifty-first anniversary of STOC. Where was the first one held? </p>
<ol>
<li>
Marina del Rey, CA <p/>
</li><li>
Massapequa, NY <p/>
</li><li>
Boston, MA <p/>
</li><li>
Chicago, IL
</li></ol>
<p>
<i>Question 3</i>: Which of these did <b>not</b> happen in 1969? </p>
<ol>
<li>
The first automatic teller machine in the United States is installed. <p/>
</li><li>
The $500 bills are officially removed from circulation. <p/>
</li><li>
The first The Limited store opens, in San Francisco. <p/>
</li><li>
The New York Mets win the World Series.
</li></ol>
<p>
<i>Question 4</i>: The first STOC conference program committee included: </p>
<ol>
<li>
No women. <p/>
</li><li>
A person named Mike. <p/>
</li><li>
A person named Pat. <p/>
</li><li>
All the above.
</li></ol>
<p>
<i>Question 5</i>: How do you tell if you are a “theoretical computer scientist”? </p>
<ol>
<li>
You wear flip-flops in the winter. <p/>
</li><li>
You regularly attend STOC. <p/>
</li><li>
You wear glasses. <p/>
</li><li>
You cannot program a computer.
</li></ol>
<p>
<i>Question 6</i>: “Cooking” a chess problem means: </p>
<ol>
<li>
Showing it is in a family of NP-complete problems on <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \times n}"/> boards. <p/>
</li><li>
Showing it has two or more solutions (or no solutions). <p/>
</li><li>
Showing it cannot be solved by Steve Cook. <p/>
</li><li>
Showing that it cannot be solved by the best-move strategy.
</li></ol>
<p>
<i>Question 7</i>: The other theory conference is called FOCS. Which of these is true about this conference: </p>
<ol>
<li>
The name was selected by a person named Edward. <p/>
</li><li>
It has never had parallel sessions. <p/>
</li><li>
It was originally called Symposium on Switching Circuit Theory and Logical Design. <p/>
</li><li>
The artwork for the proceedings cover is by an artist named Smith, who never published in the conference.
</li></ol>
<p>
<i>Question 8</i>: What do the STOC conferences have in common with last night’s final episode of <i>Game of Thrones</i>? </p>
<ol>
<li>
Both had flying horses and whistling pigs. <p/>
</li><li>
No dragons were harmed during either. <p/>
</li><li>
Both have left many big questions unanswered. <p/>
</li><li>
Both are explained by the “Prisoner’s Dilemma” game solution.
</li></ol>
<p>
<i>Question 9</i>: STOC has been held on each of these islands except: </p>
<ol>
<li>
Long Island, NY. <p/>
</li><li>
Puerto Rico. <p/>
</li><li>
Crete. <p/>
</li><li>
Vancouver Island.
</li></ol>
<p>
<i>Question 10</i>: What term appears in the titles of three award-winning STOC/FOCS papers since 2016? </p>
<ol>
<li>
Quantum. <p/>
</li><li>
Quadratic/subquadratic. <p/>
</li><li>
Quadtree. <p/>
</li><li>
Quasi/quasipolynomial.
</li></ol>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Answers: Note 1a means question 1 and answer 1 and so on. This is a wordpress issue.</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/answers4/" rel="attachment wp-att-15899"><img alt="" class="aligncenter size-medium wp-image-15899" height="47" src="https://rjlipton.files.wordpress.com/2019/05/answers4.png?w=300&amp;h=47" width="300"/></a></p>
<p/></font></font></div>
    </content>
    <updated>2019-05-21T12:48:11Z</updated>
    <published>2019-05-21T12:48:11Z</published>
    <category term="History"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="1969"/>
    <category term="FOCS"/>
    <category term="make test"/>
    <category term="quiz"/>
    <category term="STOC"/>
    <category term="Test"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-27T17:21:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4249</id>
    <link href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 5: Bregman Projections and Mirror Descent</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In this post we return to the generic form of the FTRL online optimization algorithm. If the cost functions are linear, as they will be in all the applications that I plan to talk about, the algorithm is: where is … <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 In this post we return to the generic form of the FTRL online optimization algorithm. If the cost functions are linear, as they will be in all the applications that I plan to talk about, the algorithm is:</p>
<p>
<a name="eq.ftrl.def"/></p><a name="eq.ftrl.def">
<p align="center"><img alt="\displaystyle   x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} \langle \ell_k, x \rangle \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++x_t+%3A%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+R%28x%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k%2C+x+%5Crangle+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   x_t := \arg\min_{x\in K} \ R(x) + \sum_{k=1}^{t-1} \langle \ell_k, x \rangle \ \ \ \ \ (1)"/></p>
</a><p><a name="eq.ftrl.def"/> where <img alt="{K\subseteq {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K\subseteq {\mathbb R}^n}"/> is the convex set of feasible solutions that the algorithm is allowed to produce, <img alt="{x \rightarrow \langle \ell_k , x \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Crightarrow+%5Clangle+%5Cell_k+%2C+x+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x \rightarrow \langle \ell_k , x \rangle}"/> is the linear loss function at time <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, and <img alt="{R: K \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%3A+K+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R: K \rightarrow {\mathbb R}}"/> is the strictly convex regularizer.</p>
<p>
If we have an unconstrained problem, that is, if <img alt="{K= {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%3D+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K= {\mathbb R}^n}"/>, then the optimization problem <a href="https://lucatrevisan.wordpress.com/feed/#eq.ftrl.def">(1)</a> has a unique solution: the <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> such that </p>
<p align="center"><img alt="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_t%29+%3D+-+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k "/></p>
<p> and we can usually both compute <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> efficiently in an algorithm and reason about <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> effectively in an analysis.</p>
<p>
Unfortunately, we are almost always interested in constrained settings, and then it becomes difficult both to compute <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> and to reason about it.</p>
<p>
A very nice special case happens when the regularizer <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> acts as a <em>barrier function</em> for <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>, that is, the (norm of the) gradient of <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> goes to infinity when one approaches the boundary of <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>. In such a case, it is impossible for the minimum of <a href="https://lucatrevisan.wordpress.com/feed/#eq.ftrl.def">(1)</a> to occur at the boundary and the solution will be again the unique <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> in <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> such that </p>
<p align="center"><img alt="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_t%29+%3D+-+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k "/></p>
<p>
We swept this point under the rug when we studied FTRL with negative-entropy regularizer in the settings of experts, in which <img alt="{K = \Delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK+%3D+%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K = \Delta}"/> is the set of probability distributions. When we proceeded to solve <a href="https://lucatrevisan.wordpress.com/feed/#eq.ftrl.def">(1)</a> using Lagrange multipliers, we ignored the non-negativity constraints. The reason why it was ok to do so was that the negative-entropy is a barrier function for the non-negative orthant <img alt="{({\mathbb R}_{\geq 0})^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%7B%5Cmathbb+R%7D_%7B%5Cgeq+0%7D%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{({\mathbb R}_{\geq 0})^n}"/>.</p>
<p>
Another important special case occurs when the regularizer <img alt="{R(x) = c || x||^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%3D+c+%7C%7C+x%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(x) = c || x||^2}"/> is a multiple of length-squared. In this case, we saw that we could “decouple” the optimization problem by first solving the unconstrained optimization problem, and then projecting the solution of the unconstrained problem to <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>:</p>
<p/><p align="center"><img alt="\displaystyle y_{t} = \arg\min_{y\in {\mathbb R}^n} \ c || y||^2 + \sum_{k=1}^{t-1} \langle \ell_k, y \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+y_%7Bt%7D+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+c+%7C%7C+y%7C%7C%5E2+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle y_{t} = \arg\min_{y\in {\mathbb R}^n} \ c || y||^2 + \sum_{k=1}^{t-1} \langle \ell_k, y \rangle "/></p>
<p align="center"><img alt="\displaystyle  x_t = \Pi_K (y_t) = \arg\min _{x\in K} || x - y_t || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5CPi_K+%28y_t%29+%3D+%5Carg%5Cmin+_%7Bx%5Cin+K%7D+%7C%7C+x+-+y_t+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t = \Pi_K (y_t) = \arg\min _{x\in K} || x - y_t || "/></p>
<p> Then we have the closed-form solution <img alt="{y_t = - \frac 1{2c} \sum_{k=1}^{t-1} \ell _k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_t+%3D+-+%5Cfrac+1%7B2c%7D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell+_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_t = - \frac 1{2c} \sum_{k=1}^{t-1} \ell _k}"/> and, depending on the set <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>, the projection might also have a nice closed-form, as in the case <img alt="{K= [0,1]^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%3D+%5B0%2C1%5D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K= [0,1]^n}"/> that comes up in results related to regularity lemmas.</p>
<p>
As we will see today, this approach of solving the unconstrained problem and then projecting on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> works for every regularizer, for an appropriate notion of projection called the <em>Bregman projection</em> (the projection will depend on the regularizer). </p>
<p>
To define the Bregman projection, we will first define the <em>Bregman divergence</em> with respect to the regularizer <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/>, which is a non-negative “distance” <img alt="{D(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D(x,y)}"/> defined on <img alt="{{\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb R}^n}"/> (or possibly a subset of <img alt="{{\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb R}^n}"/> for which the regularizer <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> is a barrier function). Then, the Bregman projection of <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is defined as <img alt="{\arg\min_{x\in K} \ D(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+D%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\arg\min_{x\in K} \ D(x,y)}"/>.</p>
<p>
Unfortunately, it is not so easy to reason about Bregman projections either, but the notion of Bregman divergence offers a way to reinterpret the FTRL algorithm from another point of view, called <em>mirror descent</em>. Via this reinterpretation, we will prove the regret bound </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,y_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D%28x%2Cx_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+D%28x_t%2Cy_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,y_{t+1}) "/></p>
<p> which carries the intuition that the regret comes from a combination of the “distance” of our initial solution from the offline optimum and of the “stability” of the algorithm, that is, the “distance” between consecutive soltuions. Nicely, the above bound measures both quantities using the same “distance” function.</p>
<p>
<span id="more-4249"/> </p>
<p>
</p><p><b>1. Bregman Divergence and Bregman Projection </b></p>
<p/><p>
For a strictly convex function <img alt="{R: {\mathbb R}^n \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%3A+%7B%5Cmathbb+R%7D%5En+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R: {\mathbb R}^n \rightarrow {\mathbb R}}"/>, we define the <em>Bregman divergence</em> associated to <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> as </p>
<p/><p align="center"><img alt="\displaystyle  D_R(x,y) := R(x) - R(y) - \langle \nabla R(y), x-y \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D_R%28x%2Cy%29+%3A%3D+R%28x%29+-+R%28y%29+-+%5Clangle+%5Cnabla+R%28y%29%2C+x-y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D_R(x,y) := R(x) - R(y) - \langle \nabla R(y), x-y \rangle "/></p>
<p> that is, the difference between the value of <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> at <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and the value of the linear approximation of <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> at <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> (centered at <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>). By the strict convexity of <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> we have <img alt="{D_R(x,y) \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_R%28x%2Cy%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_R(x,y) \geq 0}"/> and <img alt="{D_R(x,y) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_R%28x%2Cy%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_R(x,y) = 0}"/> iff <img alt="{x=y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%3Dy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x=y}"/>. These properties suggest that we may think of <img alt="{D_R(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_R%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_R(x,y)}"/> as a kind of “distance” between <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>, which is a useful intuition although it is important to keep in mind that the divergence need not be symmetric and need not satisfy the triangle inequality.</p>
<p>
Now we show that, assuming that <img alt="{R(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(\cdot)}"/> is well defined and strictly convex on all <img alt="{{\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb R}^n}"/>, and that the losses are linear, the constrained optimization problem <a href="https://lucatrevisan.wordpress.com/feed/#eq.ftrl.def">(1)</a> can be solved by first solving the unconstrained problem and then “projecting” the solution on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> by finding the point in <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> of smallest Bregman divergence from the unconstrained optimum:</p>
<p/><p align="center"><img alt="\displaystyle  y_t = \arg\min _{y\in {\mathbb R}^n} \ R(y) + \sum_{k=1}^{t-1} \langle \ell_k , y \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_t+%3D+%5Carg%5Cmin+_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+R%28y%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Clangle+%5Cell_k+%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_t = \arg\min _{y\in {\mathbb R}^n} \ R(y) + \sum_{k=1}^{t-1} \langle \ell_k , y \rangle "/></p>
<p align="center"><img alt="\displaystyle  x_t = \arg\min_{x \in K} \ D_R(x,y_t) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx+%5Cin+K%7D+%5C+D_R%28x%2Cy_t%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t = \arg\min_{x \in K} \ D_R(x,y_t) "/></p>
<p> The proof is very simple. The optimum of the unconstrained optimization problem is the unique <img alt="{y_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_t}"/> such that</p>
<p/><p align="center"><img alt="\displaystyle  \nabla \left( R(y_t) + \sum_{k=1}^{t-1} \ell_k \right) = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+%5Cleft%28+R%28y_t%29+%2B+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%5Cright%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \nabla \left( R(y_t) + \sum_{k=1}^{t-1} \ell_k \right) = 0 "/></p>
<p> that is, the unique <img alt="{y_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_t}"/> such that </p>
<p align="center"><img alt="\displaystyle  \nabla R(y_t) = - \sum_{k=1}^{t-1} \ell_k " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28y_t%29+%3D+-+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \nabla R(y_t) = - \sum_{k=1}^{t-1} \ell_k "/></p>
<p> On the other hand, <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> is defined as </p>
<p align="center"><img alt="\displaystyle  x_t = \arg\min_{x \in K} \ D_R(x,y_t) = \arg\min_{x \in K} R(x) - R(y_t) - \langle \nabla R(y_t) , x - y_t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx+%5Cin+K%7D+%5C+D_R%28x%2Cy_t%29+%3D+%5Carg%5Cmin_%7Bx+%5Cin+K%7D+R%28x%29+-+R%28y_t%29+-+%5Clangle+%5Cnabla+R%28y_t%29+%2C+x+-+y_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t = \arg\min_{x \in K} \ D_R(x,y_t) = \arg\min_{x \in K} R(x) - R(y_t) - \langle \nabla R(y_t) , x - y_t \rangle "/></p>
<p> that is, </p>
<p align="center"><img alt="\displaystyle  x_t = \arg\min_{x \in K} R(x) - R(y_t) + \langle \sum_{k=1}^{t-1} \ell_k , x - y_t \rangle = \arg\min_{x \in K} R(x) + \langle \sum_{k=1}^{t-1} \ell_k , x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx+%5Cin+K%7D+R%28x%29+-+R%28y_t%29+%2B+%5Clangle+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%2C+x+-+y_t+%5Crangle+%3D+%5Carg%5Cmin_%7Bx+%5Cin+K%7D+R%28x%29+%2B+%5Clangle+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t = \arg\min_{x \in K} R(x) - R(y_t) + \langle \sum_{k=1}^{t-1} \ell_k , x - y_t \rangle = \arg\min_{x \in K} R(x) + \langle \sum_{k=1}^{t-1} \ell_k , x \rangle "/></p>
<p> where the second equality above follows from the fact that two functions that differ by a constant have the same optimal solutions.</p>
<p>
Indeed we see that the above “decoupled” characterization of the FTRL algorithm would have worked for any definition of a function of the form</p>
<p/><p align="center"><img alt="\displaystyle  D(x,y) = R(x) - \langle \nabla R(y), x \rangle + \langle \mbox{ stuff that depends only on } y \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%2Cy%29+%3D+R%28x%29+-+%5Clangle+%5Cnabla+R%28y%29%2C+x+%5Crangle+%2B+%5Clangle+%5Cmbox%7B+stuff+that+depends+only+on+%7D+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D(x,y) = R(x) - \langle \nabla R(y), x \rangle + \langle \mbox{ stuff that depends only on } y \rangle "/></p>
<p> and that our particular choice of what “stuff dependent only on <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>” to add makes <img alt="{D(x,x) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%28x%2Cx%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D(x,x) = 0}"/> which is reasonable for something that we want to think of as a “distance function.”</p>
<p>
Note that, in all of the above, we can replace <img alt="{{\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb R}^n}"/> with a convex set <img alt="{K \subseteq S \subseteq {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK+%5Csubseteq+S+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K \subseteq S \subseteq {\mathbb R}^n}"/> provided that <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> is a barrier function for <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>. In that case</p>
<p/><p align="center"><img alt="\displaystyle  y_t = \arg\min_{y\in S} R(y) + \sum_k \ell_k " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_t+%3D+%5Carg%5Cmin_%7By%5Cin+S%7D+R%28y%29+%2B+%5Csum_k+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_t = \arg\min_{y\in S} R(y) + \sum_k \ell_k "/></p>
<p> is the unique <img alt="{y_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_t}"/> such that </p>
<p align="center"><img alt="\displaystyle  \nabla R(y_t) = - \sum _k \ell_k " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28y_t%29+%3D+-+%5Csum+_k+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \nabla R(y_t) = - \sum _k \ell_k "/></p>
<p> and everything else follows analogously.</p>
<p>
</p><p><b>2. Examples </b></p>
<p>
</p><p><b>  2.1. Bregman Divergence of Length-Squared </b></p>
<p/><p>
If <img alt="{R(x) = ||x ||^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%29+%3D+%7C%7Cx+%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(x) = ||x ||^2}"/>, then </p>
<p align="center"><img alt="\displaystyle  D(x,y) = ||x||^2 - ||y||^2 - \langle 2 y , x- y \rangle = ||x - y||^2 \ , " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%2Cy%29+%3D+%7C%7Cx%7C%7C%5E2+-+%7C%7Cy%7C%7C%5E2+-+%5Clangle+2+y+%2C+x-+y+%5Crangle+%3D+%7C%7Cx+-+y%7C%7C%5E2+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D(x,y) = ||x||^2 - ||y||^2 - \langle 2 y , x- y \rangle = ||x - y||^2 \ , "/></p>
<p> so Bregman divergence is distance-squared, and Bregman projection is just (Euclidean) projection.</p>
<p>
</p><p><b>  2.2. Bregman Divergence of Negative Entropy </b></p>
<p/><p>
If, for <img alt="{x\in ({\mathbb R}_{\geq 0})^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%28%7B%5Cmathbb+R%7D_%7B%5Cgeq+0%7D%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in ({\mathbb R}_{\geq 0})^n}"/>, we define</p>
<p/><p align="center"><img alt="\displaystyle  R(x) = \sum_{i=1}^n x_i \ln x_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+%3D+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  R(x) = \sum_{i=1}^n x_i \ln x_i "/></p>
<p> then the associated Bregman divergence is the generalized <em>KL divergence.</em></p>
<p/><p align="center"><img alt="\displaystyle  D(x,y) = \sum_{i=1}^n x_i \ln {x_i} \ - \sum_i y_i \ln y_i \ - \langle \nabla R(y), x-y \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%2Cy%29+%3D+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+%7Bx_i%7D+%5C+-+%5Csum_i+y_i+%5Cln+y_i+%5C+-+%5Clangle+%5Cnabla+R%28y%29%2C+x-y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D(x,y) = \sum_{i=1}^n x_i \ln {x_i} \ - \sum_i y_i \ln y_i \ - \langle \nabla R(y), x-y \rangle "/></p>
<p> where <img alt="{(\nabla R(y))_i = 1 + \ln y_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cnabla+R%28y%29%29_i+%3D+1+%2B+%5Cln+y_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\nabla R(y))_i = 1 + \ln y_i}"/> so that </p>
<p align="center"><img alt="\displaystyle D(x,y) = \sum_{i=1}^n x_i \ln x_i \ - \sum_i y_i \ln y_i \ - \sum_i x_i \ln y_i \ + \sum_i y_i \ln y_i \ - \sum_i x_i + \sum_i y_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+D%28x%2Cy%29+%3D+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+x_i+%5C+-+%5Csum_i+y_i+%5Cln+y_i+%5C+-+%5Csum_i+x_i+%5Cln+y_i+%5C+%2B+%5Csum_i+y_i+%5Cln+y_i+%5C+-+%5Csum_i+x_i+%2B+%5Csum_i+y_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle D(x,y) = \sum_{i=1}^n x_i \ln x_i \ - \sum_i y_i \ln y_i \ - \sum_i x_i \ln y_i \ + \sum_i y_i \ln y_i \ - \sum_i x_i + \sum_i y_i "/></p>
<p align="center"><img alt="\displaystyle  = \sum_{i=1}^n x_i \ln \frac{x_i} {y_i} \ - \sum_i x_i + \sum_i y_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cln+%5Cfrac%7Bx_i%7D+%7By_i%7D+%5C+-+%5Csum_i+x_i+%2B+%5Csum_i+y_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_{i=1}^n x_i \ln \frac{x_i} {y_i} \ - \sum_i x_i + \sum_i y_i "/></p>
<p>
Note that, if <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> are probability distributions, then the final two terms above cancel out, leaving just the KL divergence <img alt="{\sum_i x_i \ln \frac {x_i}{y_i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+x_i+%5Cln+%5Cfrac+%7Bx_i%7D%7By_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_i x_i \ln \frac {x_i}{y_i}}"/>.</p>
<p>
</p><p><b>3. Mirror Descent </b></p>
<p/><p>
We now introduce a new perspective on FTRL.</p>
<p>
In the unconstrained setting, if <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> is a strictly convex function and <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is the associated Bregman divergence, the <em>mirror descent</em> algorithm for online optimization has the update rule</p>
<p/><p align="center"><img alt="\displaystyle  x_t = \arg\min_{x\in {\mathbb R}^n} D(x,x_{t-1}) + \langle \ell_{t-1}, x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+D%28x%2Cx_%7Bt-1%7D%29+%2B+%5Clangle+%5Cell_%7Bt-1%7D%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t = \arg\min_{x\in {\mathbb R}^n} D(x,x_{t-1}) + \langle \ell_{t-1}, x \rangle "/></p>
<p> The idea is that we want to find a solution that is good for the past loss functions, but that does not “overfit” too much. If, in past steps, <img alt="{x_{t-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{t-1}}"/> had been chosen to be such a solution for the loss functions <img alt="{\ell_1,\ldots,\ell_{t-2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1%2C%5Cldots%2C%5Cell_%7Bt-2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_1,\ldots,\ell_{t-2}}"/>, then, in choosing <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/>, we want to balance staying close to <img alt="{x_{t-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{t-1}}"/> but also doing well with respect to <img alt="{\ell_{t-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_{t-1}}"/>, hence the above definition.</p>
<blockquote><p><b>Theorem 1</b> <em> Initialized with <img alt="{x_1 = \arg\min_{x\in {\mathbb R}^n} R(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+R%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1 = \arg\min_{x\in {\mathbb R}^n} R(x)}"/>, the unconstrained mirror descent algorithm is identical to FTRL with regularizer <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/>. </em></p></blockquote>
<p/><p>
<em>Proof:</em>  We will proceed by induction on <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. At <img alt="{t=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1}"/>, the definition of <img alt="{x_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1}"/> is the same. For larger <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, we know that FTRL will choose the unique <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> such that <img alt="{\nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+R%28x_t%29+%3D+-+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k}"/>, so we will assume that this is true for the mirror descent algorithm for <img alt="{x_{t-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{t-1}}"/> and prove it for <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/>. </p>
<p>
First, we note that the function <img alt="{x \rightarrow D(x,x_{t-1}) + \langle \ell_{t-1} , x \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Crightarrow+D%28x%2Cx_%7Bt-1%7D%29+%2B+%5Clangle+%5Cell_%7Bt-1%7D+%2C+x+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x \rightarrow D(x,x_{t-1}) + \langle \ell_{t-1} , x \rangle}"/> is strictly convex, because it equals </p>
<p align="center"><img alt="\displaystyle  R(x) - R(x_{t-1}) - \langle \nabla R(x_{t-1}), x - x_t \rangle + \langle \ell_{t-1} , x \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28x%29+-+R%28x_%7Bt-1%7D%29+-+%5Clangle+%5Cnabla+R%28x_%7Bt-1%7D%29%2C+x+-+x_t+%5Crangle+%2B+%5Clangle+%5Cell_%7Bt-1%7D+%2C+x+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  R(x) - R(x_{t-1}) - \langle \nabla R(x_{t-1}), x - x_t \rangle + \langle \ell_{t-1} , x \rangle"/></p>
<p> and so it is a sum of a strictly convex function <img alt="{R(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R(x)}"/>, linear functions in <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, and constants independent of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. This means that <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> is the unique point at which the gradient of the above function is zero, that is, </p>
<p align="center"><img alt="\displaystyle  \nabla R(x_t) - \nabla R(x_{t-1}) + \ell_{t-1} = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_t%29+-+%5Cnabla+R%28x_%7Bt-1%7D%29+%2B+%5Cell_%7Bt-1%7D+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \nabla R(x_t) - \nabla R(x_{t-1}) + \ell_{t-1} = 0 "/></p>
<p> and so </p>
<p align="center"><img alt="\displaystyle  \nabla R(x_t) = \nabla R(x_{t-1}) - \ell_{t-1} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_t%29+%3D+%5Cnabla+R%28x_%7Bt-1%7D%29+-+%5Cell_%7Bt-1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \nabla R(x_t) = \nabla R(x_{t-1}) - \ell_{t-1} "/></p>
<p> and, using the inductive hypothesis, we have </p>
<p align="center"><img alt="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_t%29+%3D+-+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \nabla R(x_t) = - \sum_{k=1}^{t-1} \ell_k "/></p>
<p> as desired. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
In the constrained case, there are two variants of mirror descent. Using the terminology from Elad Hazan’s survey, <em>agile</em> mirror descent is the natural generalization of the unconstrained algorithm:</p>
<p/><p align="center"><img alt="\displaystyle  x_t = \arg\min_{x\in K} \ D(x,x_{t-1}) + \langle \ell_{t-1}, x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+%5C+D%28x%2Cx_%7Bt-1%7D%29+%2B+%5Clangle+%5Cell_%7Bt-1%7D%2C+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t = \arg\min_{x\in K} \ D(x,x_{t-1}) + \langle \ell_{t-1}, x \rangle "/></p>
<p> Following the same steps as the proof in the previous section, it is possible to show that agile mirror descent is equivalent to solving, at each iteration, the “decoupled” optimization problems</p>
<p/><p align="center"><img alt="\displaystyle  y_t = \arg\min_{y\in {\mathbb R}^n} D(y,x_{t-1}) + \langle \ell_{t-1}, y \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_t+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+D%28y%2Cx_%7Bt-1%7D%29+%2B+%5Clangle+%5Cell_%7Bt-1%7D%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_t = \arg\min_{y\in {\mathbb R}^n} D(y,x_{t-1}) + \langle \ell_{t-1}, y \rangle "/></p>
<p align="center"><img alt="\displaystyle  x_t = \arg\min_{x\in K} D(x,y_t) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+D%28x%2Cy_t%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t = \arg\min_{x\in K} D(x,y_t) "/></p>
<p> That is, we can first solve the unconstrained problem and then project on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>. (Again, we can always replace <img alt="{{\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb R}^n}"/> by a set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> for which <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> is a barrier function and such that <img alt="{K \subseteq S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK+%5Csubseteq+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K \subseteq S}"/>.)</p>
<p>
The <em>lazy</em> mirror descent algorithm has the update rule </p>
<p align="center"><img alt="\displaystyle  y_t = \arg\min_{x\in {\mathbb R}^n} D(y,y_{t-1}) + \langle \ell_{t-1}, y \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+D%28y%2Cy_%7Bt-1%7D%29+%2B+%5Clangle+%5Cell_%7Bt-1%7D%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_t = \arg\min_{x\in {\mathbb R}^n} D(y,y_{t-1}) + \langle \ell_{t-1}, y \rangle "/></p>
<p align="center"><img alt="\displaystyle  x_t = \arg\min_{x\in K} D(x,y_t) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+D%28x%2Cy_t%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t = \arg\min_{x\in K} D(x,y_t) "/></p>
<p> The initialization is </p>
<p align="center"><img alt="\displaystyle  y_1 = \arg\min_{x\in {\mathbb R}^n} \ R(x) \ \ \ x_1 = \arg\min_{x\in K} R(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+R%28x%29+%5C+%5C+%5C+x_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+K%7D+R%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_1 = \arg\min_{x\in {\mathbb R}^n} \ R(x) \ \ \ x_1 = \arg\min_{x\in K} R(x)"/></p>
<blockquote><p><b>Fact 2</b> <em> Lazy mirror descent is equivalent to FTRL. </em></p></blockquote>
<p/><p>
<em>Proof:</em>  The solutions <img alt="{y_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_t}"/> are the unconstrained optimum of FTRL, and <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> is the Bregman projection of <img alt="{y_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_t}"/> on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>. We proved in the previous section that this characterizes constrained FTRL. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
What about agile mirror descent? In certain special cases it is equivalent to lazy mirror descent, and hence to FTRL, but it usually leads to a different set of solutions. </p>
<p>
We will provide an analysis of lazy mirror descent, but first we will give an analysis of the regret of unconstrained FTRL in terms of Bregman divergence, which will be the model on which we will build the proof for the constrained case.</p>
<p>
</p><p><b>4. A Regret Bound for FTRL in Terms of Bregman Divergence </b></p>
<p/><p>
In this section we prove the following regret bound.</p>
<blockquote><p><b>Theorem 3</b> <em> Unconstrained FTRL with regularizer <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> satisfies the regret bound </em></p><em>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,x_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D%28x%2Cx_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+D%28x_t%2Cx_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,x_{t+1}) "/></p>
</em><p><em> where <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is the Bregman divergence associated with <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/>. </em></p></blockquote>
<p/><p>
We will take the mirror descent view of unconstrained FTRL, so that</p>
<p/><p align="center"><img alt="\displaystyle  x_1 = \arg\min_{x\in {\mathbb R}^n} \ R(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+R%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_1 = \arg\min_{x\in {\mathbb R}^n} \ R(x) "/></p>
<p align="center"><img alt="\displaystyle  x_{t+1} = \arg\min_{x\in {\mathbb R}^n} \ D(x,x_t) + \langle \ell_t , x\rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+D%28x%2Cx_t%29+%2B+%5Clangle+%5Cell_t+%2C+x%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_{t+1} = \arg\min_{x\in {\mathbb R}^n} \ D(x,x_t) + \langle \ell_t , x\rangle "/></p>
<p> We proved that </p>
<p align="center"><img alt="\displaystyle  \nabla R(x_{t+1} ) = \nabla R(x_t) - \ell_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28x_%7Bt%2B1%7D+%29+%3D+%5Cnabla+R%28x_t%29+-+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \nabla R(x_{t+1} ) = \nabla R(x_t) - \ell_t "/></p>
<p> This means that we can rewrite the regret suffered at step <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> with respect to <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> as </p>
<p align="center"><img alt="\displaystyle  \langle \ell_t , x_t - x \rangle = \langle \nabla R(x_t) - \nabla R(x_{t+1}), x_t - x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x+%5Crangle+%3D+%5Clangle+%5Cnabla+R%28x_t%29+-+%5Cnabla+R%28x_%7Bt%2B1%7D%29%2C+x_t+-+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \langle \ell_t , x_t - x \rangle = \langle \nabla R(x_t) - \nabla R(x_{t+1}), x_t - x \rangle "/></p>
<p align="center"><img alt="\displaystyle  = D(x,x_t) - D(x,x_{t+1} ) +D(x_t, x_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+D%28x%2Cx_t%29+-+D%28x%2Cx_%7Bt%2B1%7D+%29+%2BD%28x_t%2C+x_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = D(x,x_t) - D(x,x_{t+1} ) +D(x_t, x_{t+1}) "/></p>
<p> and the theorem follows by adding up the above expression for <img alt="{t=1,\ldots,T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C%5Cldots%2CT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1,\ldots,T}"/> and recalling that <img alt="{D(x,x_{T+1}) \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%28x%2Cx_%7BT%2B1%7D%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D(x,x_{T+1}) \geq 0}"/>.</p>
<p>
Unfortunately I have no geometric intuition about the above identity, although, as you can check yourself, the algebra works neatly.</p>
<p>
</p><p><b>5. A Regret Bound for Agile Mirror Descent </b></p>
<p/><p>
In this section we prove the following generalization of the regret bound from the previous section.</p>
<blockquote><p><b>Theorem 4</b> <em> Agile mirror descent satisfies the regret bound </em></p><em>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,y_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T%28x%29+%5Cleq+D%28x%2Cx_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+D%28x_t%2Cy_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T(x) \leq D(x,x_1) + \sum_{t=1}^T D(x_t,y_{t+1}) "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
The first part of the update rule of agile mirror descent is </p>
<p align="center"><img alt="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} D(y,x_{t}) + \langle \ell_{t}, y \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7By%5Cin+%7B%5Cmathbb+R%7D%5En%7D+D%28y%2Cx_%7Bt%7D%29+%2B+%5Clangle+%5Cell_%7Bt%7D%2C+y+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y_{t+1} = \arg\min_{y\in {\mathbb R}^n} D(y,x_{t}) + \langle \ell_{t}, y \rangle "/></p>
<p> and, following steps that we have already carried out before, <img alt="{y_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_{t+1}}"/> satisfies </p>
<p align="center"><img alt="\displaystyle  \nabla R(y_{t+1} ) = \nabla R(x_t) - \ell_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+R%28y_%7Bt%2B1%7D+%29+%3D+%5Cnabla+R%28x_t%29+-+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \nabla R(y_{t+1} ) = \nabla R(x_t) - \ell_t "/></p>
<p> This means that we can rewrite the regret suffered at step <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> with respect to <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> as </p>
<p align="center"><img alt="\displaystyle  \langle \ell_t , x_t - x \rangle = \langle \nabla R(x_t) - \nabla R(y_{t+1}), x_t - x \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x+%5Crangle+%3D+%5Clangle+%5Cnabla+R%28x_t%29+-+%5Cnabla+R%28y_%7Bt%2B1%7D%29%2C+x_t+-+x+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \langle \ell_t , x_t - x \rangle = \langle \nabla R(x_t) - \nabla R(y_{t+1}), x_t - x \rangle "/></p>
<p align="center"><img alt="\displaystyle  = D(x,x_t) - D(x,y_{t+1} ) +D(x_t, y_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+D%28x%2Cx_t%29+-+D%28x%2Cy_%7Bt%2B1%7D+%29+%2BD%28x_t%2C+y_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = D(x,x_t) - D(x,y_{t+1} ) +D(x_t, y_{t+1}) "/></p>
<p> where the same mystery cancellations as before make the above identity true.</p>
<p>
Now I will wield another piece of magic, and I will state without proof the following fact about Bregman projections</p>
<blockquote><p><b>Lemma 5</b> <em> If <img alt="{x\in K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in K}"/> and <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> is the Bregman projection on <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> of a point <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>, then </em></p><em>
<p align="center"><img alt="\displaystyle  D(x,y) \geq D(x,z) + D(z,y) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%2Cy%29+%5Cgeq+D%28x%2Cz%29+%2B+D%28z%2Cy%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D(x,y) \geq D(x,z) + D(z,y) "/></p>
</em><p><em> </em></p></blockquote>
<p> That is, if we think of <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> as a “distance,” the distance from <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> to its closest point <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> in <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> plus the distance from <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> to <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is at most the distance from <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> to <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Note that this goes in the opposite direction as the triangle inequality (which ok, because <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> typically does not satisfy the triangle inequality).</p>
<p>
In particular, the above lemma gives us </p>
<p align="center"><img alt="\displaystyle  D(x,y_{t+1}) \geq D(x,x_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%2Cy_%7Bt%2B1%7D%29+%5Cgeq+D%28x%2Cx_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D(x,y_{t+1}) \geq D(x,x_{t+1}) "/></p>
<p> and so </p>
<p align="center"><img alt="\displaystyle  \langle \ell_t , x_t - x \rangle \leq D(x,x_t) - D(x,x_{t+1} ) +D(x_t, y_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+x_t+-+x+%5Crangle+%5Cleq+D%28x%2Cx_t%29+-+D%28x%2Cx_%7Bt%2B1%7D+%29+%2BD%28x_t%2C+y_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \langle \ell_t , x_t - x \rangle \leq D(x,x_t) - D(x,x_{t+1} ) +D(x_t, y_{t+1}) "/></p>
<p> Now summing over <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> and recalling that <img alt="{D(x,x_{T+1}) \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%28x%2Cx_%7BT%2B1%7D%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D(x,x_{T+1}) \geq 0}"/> we have our theorem.</p>
<p/></div>
    </content>
    <updated>2019-05-21T01:42:49Z</updated>
    <published>2019-05-21T01:42:49Z</published>
    <category term="theory"/>
    <category term="Bregman divergence"/>
    <category term="Bregman projections"/>
    <category term="mirror descent"/>
    <category term="online optimization"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-27T17:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/05/20/lecturer-tenured-assistant-professor-at-royal-holloway-university-of-london-apply-by-june-7-2019-at-royal-holloway-university-of-london-apply-by-june-7-2019/</id>
    <link href="https://cstheory-jobs.org/2019/05/20/lecturer-tenured-assistant-professor-at-royal-holloway-university-of-london-apply-by-june-7-2019-at-royal-holloway-university-of-london-apply-by-june-7-2019/" rel="alternate" type="text/html"/>
    <title>Lecturer (Tenured Assistant Professor) at Royal Holloway, University of London (apply by June 7, 2019)</title>
    <summary>The Department of Computer Science at Royal Holloway, University of London, invites applications for a Lecturer position (a full-time and permanent (tenured) post). We are recruiting for an academic member of staff who can strengthen our research, which falls broadly within Algorithms and Complexity, Artificial Intelligence, Distributed and Global Computing, and Software Language Engineering. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at Royal Holloway, University of London, invites applications for a Lecturer position (a full-time and permanent (tenured) post). We are recruiting for an academic member of staff who can strengthen our research, which falls broadly within Algorithms and Complexity, Artificial Intelligence, Distributed and Global Computing, and Software Language Engineering.</p>
<p>Website: <a href="https://jobs.royalholloway.ac.uk/vacancy.aspx?ref=0519-179">https://jobs.royalholloway.ac.uk/vacancy.aspx?ref=0519-179</a><br/>
Email: Jose.Fiadeiro@rhul.ac.uk</p></div>
    </content>
    <updated>2019-05-20T18:34:07Z</updated>
    <published>2019-05-20T18:34:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-05-27T17:22:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6282781001271163342</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6282781001271163342/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/notorious-lah-or-notorious-lah-or-you.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6282781001271163342" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6282781001271163342" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/notorious-lah-or-notorious-lah-or-you.html" rel="alternate" type="text/html"/>
    <title>Notorious L.A.H or Notorious LAH? OR You always need one more proofread</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I noticed a while back that even on the nth proofread of a document there are still corrections. So I decided to keep track of how many corrections there are in a paper I was working on. I chose a non-technical paper so that errors-in-the-math would not be the issue.  I chose<br/>
<br/>
                                           Guest Column: The Third P =?NP Poll (see <a href="https://blog.computationalcomplexity.org/2019/03/third-poll-on-p-vs-np-and-related.html">here</a>)<br/>
<br/>
that appeared in Lane Hemaspaandra's SIGACT News Complexity Column.<br/>
<br/>
I kept track of the following:<br/>
<br/>
1) Number of corrections. Anything that I changed. Could be style, a new thought, need not be (though could be) an error.<br/>
<br/>
2) Errors. These are things that really need to be corrected, like having `think' instead of  `thing' .<br/>
<br/>
Corrections vs  Errors, an Example:<br/>
<br/>
If I refer to Lane Hemaspaandra as <i>Notorious L.A.N</i> that is a correction and an  error, as he is Notorous <i>L.A.H.</i><br/>
<br/>
If I refer to Lane Hemaspaandra as<i> Notorious L.A.H</i> and decide to change it to <i>LAH </i>that is a correction that is not an error.<br/>
<br/>
I didn't keep track of serious errors vs typos, but after the first 3 proofreads there were no more serious errors--- sort of- --you'll see. Most serious was a f<b>onts-gone-wild thing where half the paper was in boldface.</b><br/>
<br/>
Here is a history of the number of corrections<br/>
<br/>
1) Lane proofread the first draft. κ corrections where κ is some cardinal between the cardinality of N and the cardinality of  2<sup>N</sup> . Its value depends on which model of set theory you are in. (My spellchecker thinks that cardinality is not a word. I checked and I am spelling it correctly but perhaps it's one of those things where I stare at it too much and keep misreading it.)<br/>
<br/>
Henceforth I omit the word <i>proofread</i> as it is understood<br/>
<br/>
<br/>
2) Bill G:  81 corrections, 29 of which were errors.<br/>
<br/>
3) Clyde: 64 corrections,  of which 17 were errors.<br/>
<br/>
4) Bill G: 40 corrections, of which 21 were errors (I had added a new section causing more errors)<br/>
<br/>
5) Clyde: 30 corrections of which 10 were errors.<br/>
<br/>
6) Bill G: 24 corrections of which 6 were errors.<br/>
<br/>
7) Clyde: 18 corrections of which 8 were errors.<br/>
<br/>
8) David Sekora (A CS grad student at Maryland who at one time wanted to be an English Major): f15 corrections of which 15 were errors. Really! Typos dagnabbit! (Spell check thinks that dagnabbit is spelled wrong. Um---in that case what is the correct spelling?)<br/>
<br/>
9) Nathan Grammel (A CS grad student at Maryland) :6 corrections of which  3 were errors.<br/>
<br/>
10) Bill G, proofreading backwards, a paragraph at a time: 29 corrections of which 5 were errors.<br/>
<br/>
11) Justin Hontz, an ugrad who TAs for me: 10 corrections of which 7 were errors.<br/>
<br/>
12) Karthik Abinav, a grad student in theory at Maryland: 2 corrections both of which were errors. Was this the end or are there still issues?<br/>
<br/>
13) Josh Twitty, an ugrad who TAs for me: 0 corrections. YEAH!<br/>
<br/>
14) Dan Smolyak, an ugrad CS and Eco major:4 corrections, all 4 errors. <i>Error  </i>sounds too strong. For example, one of them was to replace ?. with ?  Yes, its an error, but not that important. It DOES point to his carefulness as a proofreader.<br/>
<br/>
15) Clyde Kruskal :20 corrections, 10 of which were errors. To call them errors seems wrong when he corrects  <i>Group theory' </i>to  <i>Group Theory</i>. None of these corrections were caused by prior comments. I think all of the errors were in the paper early on, undetected until now!<br/>
<br/>
16)  Backwards Bill G again:  28 corrections,  14 of which were errors. Again, the errors were minor. Most of the errors were relatively recent. As an example, if I list out topics in math like:<br/>
<br/>
a) Group Theory, Set Theory, and  Ramsey Theory<br/>
<br/>
then I am supposed to use capital letters, but if I say in prose<br/>
<br/>
Lance Fortnow thinks that the techniques used will be group theory, set theory, and Ramsey theory<br/>
<br/>
then only the R in Ramsey Theory is in caps.  Makes me glad I'm in math.<br/>
<br/>
17) Lane got penultimate proofread. Lane found 75 (yes 75 WOW) of which 66 (yes 66 WOW) were errors. Many of these were spacing and latex things that I would never have noticed (indeed- I didn't notice) and most readers would not have noticed (hmmm- how do I know that?) but only an editor could catch (hmmm- when I've edited the book review column and now the open problems column and I never found more than 10 errors). So when all is said and done: KUDOS to Lane! And My point was that you can never get all the errors out. On that I am correct. I wonder if there are still errors? Yeah, but at most 10. However, I said that BEFORE giving it to Lane.<br/>
<br/>
18) Stephen Fenner, the editor of SIGACT news got FINAL proofread. He found that I spelled his name wrong . How many errors are left? I would bet at most 10. I would bet that I would lose that bet.<br/>
------------------------------------------------------------------------------------------------------------<br/>
<br/>
Why after multiple proofreadings are there still errors? (My spell check thinks proofreadings is not a word. Maybe my spell check is worried that if people get documents proofread a lot then they won't be needed anymore. This blog post refutes that thought.)<br/>
<br/>
1)  An error can occur from a correction. This caused a massive problem with another paper. Lane's next column will be by me and co-authors on The Muffin Problem. We had all kinds of problems with the colors and sizes--- Massive Magenta Muffins or Miniature Magenta Muffins? Sizes gone wild! Again Kudos to my proofreaders and to Lane for catching this rather important error.<br/>
<br/>
2) If some passage is added late in the process it will surely have errors.<br/>
<br/>
3) An error correction may clear away the brush so you can see other errors.<br/>
<br/>
4) With LaTeX (or Word for some) we have the ability to get things perfect. So there is no cost to keeping on perfecting things. This lead so many corrections that are not errors.<br/>
<br/>
5) I know of an adviser who would first say change A to B, and later change B back to A. (None of that happened with the paper discussed above).<br/>
<br/>
Are errors inevitable? Clyde Kruskal tells me that his father Martin Kruskal, as a teenager, read Courant and Robbins book <i>What is Mathematics</i> and found some errors in it. Martin's mother didn't believe him and marched him over to Courant's house:<br/>
<br/>
MARTIN MOTHER: Martin claims to have found errors in your book.<br/>
<br/>
COURANT:  (laughs) There are errors in every book.<br/>
<br/>
Courant was so impressed that ten (or so) years later Courant became Martin's PhD adviser.<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-05-20T14:14:00Z</updated>
    <published>2019-05-20T14:14:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-27T15:12:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4246</id>
    <link href="https://lucatrevisan.wordpress.com/2019/05/18/%e5%8a%a0%e6%b2%b9%e5%8f%b0%e7%81%a3%ef%bc%81/" rel="alternate" type="text/html"/>
    <title>加油台灣！</title>
    <summary>I would like to congratulate my Taiwanese readers for being in the first Asian country to introduce same-sex marriage.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><img alt="taiwan" class="alignnone size-full wp-image-4247" src="https://lucatrevisan.files.wordpress.com/2019/05/taiwan.jpg?w=584"/></p>
<p>I would like to congratulate my Taiwanese readers for being in the first Asian country to introduce same-sex marriage.</p></div>
    </content>
    <updated>2019-05-18T20:41:38Z</updated>
    <published>2019-05-18T20:41:38Z</published>
    <category term="history"/>
    <category term="Taiwan"/>
    <category term="things that are excellent"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-27T17:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15880</id>
    <link href="https://rjlipton.wordpress.com/2019/05/18/an-app-proof/" rel="alternate" type="text/html"/>
    <title>An App Proof</title>
    <summary>That is “app” as in an on-line application [ Leo Stein ] Leo Stein is an assistant professor in the department of Physics and Astronomy at the University of Mississippi. His research interests include general relativity from an astrophysical standpoint. Today I want to share an unusual proof of his. Mathematics and complexity theory are […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>That is “app” as in an on-line application</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/18/an-app-proof/leo/" rel="attachment wp-att-15881"><img alt="" class="alignright size-full wp-image-15881" src="https://rjlipton.files.wordpress.com/2019/05/leo.png?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Leo Stein ]</font></td>
</tr>
</tbody>
</table>
<p>
Leo Stein is an assistant professor in the department of Physics and Astronomy at the University of Mississippi. His research interests include general relativity from an astrophysical <a href="https://arxiv.org/abs/1809.09125">standpoint</a>. </p>
<p>
Today I want to share an unusual proof of his.<span id="more-15880"/></p>
<p>
Mathematics and complexity theory are all about proving theorems. Most of the time, so far, we prove the old way: we write out a humanly readable proof. At least we hope the proof is readable. Some of the time, we use a computer to check or even create the proof. Sometimes we do extensive numerical computations, but these are not proofs.</p>
<p>
</p><p/><h2> Solving Quadratic Equations </h2><p/>
<p/><p>
I have known, as I am sure you do, forever that a quadratic equation can be solved in closed form. That is 	</p>
<p align="center"><img alt="\displaystyle  x^{2} + bx + c = 0, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7B2%7D+%2B+bx+%2B+c+%3D+0%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^{2} + bx + c = 0, "/></p>
<p>has the two solutions 	</p>
<p align="center"><img alt="\displaystyle  -b/2 + 1/2\sqrt{b^{2}-4c} \text{ and } -b/2 - 1/2\sqrt{b^{2}-4c}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++-b%2F2+%2B+1%2F2%5Csqrt%7Bb%5E%7B2%7D-4c%7D+%5Ctext%7B+and+%7D+-b%2F2+-+1%2F2%5Csqrt%7Bb%5E%7B2%7D-4c%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  -b/2 + 1/2\sqrt{b^{2}-4c} \text{ and } -b/2 - 1/2\sqrt{b^{2}-4c}. "/></p>
<p>I have discussed this before <a href="https://rjlipton.wordpress.com/2014/04/30/bells-fifty-year-old-mistake/">here</a> and its relationship to the World’s Fair in Flushing Meadows. </p>
<p>
A natural question is: Are square roots needed in any formula for quadratic equations? The answer is “Yes”.</p>
<blockquote><p><b>Theorem 1</b> <em> There does not exist any continuous function from the space of quadratic polynomials to complex numbers which associates to any quadratic polynomial a root of that polynomial. </em>
</p></blockquote>
<p>
</p><blockquote><p><b>Corollary 2</b> <em> There is no quadratic formula built out of a finite combination of field operations and the functions <img alt="{\sin, \cos, \exp}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csin%2C+%5Ccos%2C+%5Cexp%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\sin, \cos, \exp}"/>, and the coefficients of the polynomial. </em>
</p></blockquote>
<p/><p>
The corollary uses the basic fact that <img alt="{\sin, \cos, \exp}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csin%2C+%5Ccos%2C+%5Cexp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sin, \cos, \exp}"/> are continuous functions. Note that each has a single branch on complex plane, whereas radicals and the logarithm function do not. So how do we prove the theorem?</p>
<p>
</p><p/><h2> An App Based Proof </h2><p/>
<p/><p>
Here is a novel, I think, proof that uses an app. Stein has written the app and it is <a href="https://duetosymmetry.com/tool/polynomial-roots-toy/">here</a>. He explains how to use it. I strongly suggest that you try this yourself. </p>
<blockquote><p><b> </b> <em> To get a feel for all this, drag the <img alt="{a_{0}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7B0%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{a_{0}}"/> coefficient to <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{-1}"/> and the <img alt="{a_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7B1%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{a_{1}}"/> coefficient to <img alt="{1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{1/2}"/>. You should have two real roots in root space (one at <img alt="{\approx -1.28}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Capprox+-1.28%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\approx -1.28}"/>, the other at <img alt="{\approx 0.78}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Capprox+0.78%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\approx 0.78}"/>). Let’s call <img alt="{r_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_%7B1%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r_{1}}"/> the negative root, and <img alt="{r_{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_%7B2%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r_{2}}"/> the positive root. Now move the coefficient <img alt="{a_{0}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7B0%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{a_{0}}"/> around in a small loop (i.e. move it around a little bit, and then return it to <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{-1}"/> where it started). Note that the roots move continuously, and then return to their original positions. Next, move <img alt="{a_{0}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7B0%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{a_{0}}"/> in a big loop (big enough that it orbits around <img alt="{r_{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_%7B2%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r_{2}}"/>). Something funny happens: the roots <img alt="{r_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_%7B1%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r_{1}}"/> and <img alt="{r_{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_%7B2%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r_{2}}"/> switch places. </em>
</p></blockquote>
<p/><p>
Leo Goldmakher says <a href="https://web.williams.edu/Mathematics/lg5/394/ArnoldQuintic.pdf">here</a>: </p>
<blockquote><p>
Pause and think about this for a second. This is really, really weird.
</p></blockquote>
<p>Here is one immediate consequence of this observation: </p>
<blockquote><p><b>Theorem 3</b> <em> There does not exist any continuous function from the space of quadratic polynomials to complex numbers which associates to any quadratic polynomial a root of that polynomial. </em>
</p></blockquote>
<p/><p>
And so the corollary follows.</p>
<p>
</p><p/><h2> A Standard Proof </h2><p/>
<p/><p>
Goldmakher writes out a more conventional proof in his paper titled <em>Arnold’s Elementary Proof Of The Insolvability Of The Quintic</em>. He also shows the following theorem: </p>
<blockquote><p><b>Theorem 4</b> <em> Fix a positive integer <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{N}"/>. Any quintic formula built out of the field operations, continuous functions, and radicals must have nesting of level more than <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{N}"/>. </em>
</p></blockquote>
<p>This says that there can be no fixed formula for fifth degree, quintic, polynomials. Of course, this follows from <a href="https://en.wikipedia.org/wiki/Galois_theory">Galois theory</a>, but his proof uses just calculus. The Arnold is Vladimir Arnold.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>Do you know other cases of an app with animation conveying the essence of a mathematical proof?  This means more than “proofs in pictures” or “proofs without words”—the animation and interactivity are crucial.  </p></font></font></div>
    </content>
    <updated>2019-05-18T20:03:11Z</updated>
    <published>2019-05-18T20:03:11Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Proofs"/>
    <category term="app"/>
    <category term="Arnold"/>
    <category term="check proofs"/>
    <category term="Galois"/>
    <category term="Proof"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-27T17:21:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/073</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/073" rel="alternate" type="text/html"/>
    <title>TR19-073 |  Parity helps to compute Majority | 

	Igor Carboni Oliveira, 

	Rahul Santhanam, 

	Srikanth Srinivasan</title>
    <summary>We study the complexity of computing symmetric and threshold functions by constant-depth circuits with Parity gates, also known as AC$^0[\oplus]$ circuits. Razborov (1987) and Smolensky (1987, 1993) showed that Majority requires depth-$d$ AC$^0[\oplus]$ circuits of size $2^{\Omega(n^{1/2(d-1)})}$. By using a divide-and-conquer approach, it is easy to show that Majority can be computed with depth-$d$ AC$^0[\oplus]$ circuits of size $2^{\widetilde{O}(n^{1/(d-1)})}$. This gap between upper and lower bounds has stood for nearly three decades.

Somewhat surprisingly, we show that neither the upper bound nor the lower bound above is tight for large $d$. We show for $d \geq 5$ that any symmetric function can be computed with depth-$d$ AC$^0[\oplus]$ circuits of size $\exp({\widetilde{O} (n^{\frac{2}{3} \cdot \frac{1}{(d - 4)}} )})$. Our upper bound extends to threshold functions (with a constant additive loss in the denominator of the double exponent). We improve the Razborov-Smolensky lower bound to show that for $d \geq 3$ Majority requires depth-$d$ AC$^0[\oplus]$ circuits of size $2^{\Omega(n^{1/(2d-4)})}$. For depths $d \leq 4$, we are able to refine our techniques to get almost-optimal bounds: the depth-$3$ AC$^0[\oplus]$ circuit size of Majority is $2^{\widetilde{\Theta}(n^{1/2})}$, while its depth-$4$ AC$^0[\oplus]$ circuit size is $2^{\widetilde{\Theta}(n^{1/4})}$.</summary>
    <updated>2019-05-17T21:48:44Z</updated>
    <published>2019-05-17T21:48:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-27T17:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/072</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/072" rel="alternate" type="text/html"/>
    <title>TR19-072 |  Broadcast Congested Clique: Planted Cliques and Pseudorandom Generators | 

	Lijie Chen, 

	Ofer Grossman</title>
    <summary>Consider the multiparty communication complexity model where there are n processors, each receiving as input a row of an n by n matrix M with entries in {0, 1}, and in each round each party can broadcast a single bit to all other parties (this is known as the BCAST(1) model). There are many lower bounds known for the number of rounds necessary for certain problems in this model, but they are all worst case lower bounds which apply only for very specifically constructed input distributions. We develop a framework for showing lower bounds in this setting for more natural input distributions, and apply the framework to show:
A lower bound for finding planted cliques in random inputs (i.e., each entry of the matrix is random, except there is a random subset a_1,..., a_k in [n] where M_{a_i,a_j} = 1 for all i and j). Specifically, we show that if k = n^(1/4 - eps), this problem requires a number of rounds polynomial in n.

A pseudo-random generator which fools the BCAST(1) model. That is, we show a distribution which is efficiently samplable using few random bits, and which is indistinguishable from uniform by a low-round BCAST(1) protocol. This allows us to show that every t = Omega(log n) round randomized algorithm in which each processor uses up to n random bits can be efficiently transformed into an O(t)-round randomized algorithm in which each processor uses only up to O(t) random bits, while maintaining a high success probability.

As a corollary of the pseudo-random generator, we also prove the first average case lower bound for the model (specifically, for the problem of determining whether the input matrix is full rank), as well as an average-case time hierarchy.</summary>
    <updated>2019-05-17T19:38:02Z</updated>
    <published>2019-05-17T19:38:02Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-27T17:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4244</id>
    <link href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 4: Regularity Lemmas</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">We now discuss how to view proofs of certain regularity lemmas as applications of the FTRL methodology. The Szemeredi Regularity Lemma states (in modern language) that every dense graph is well approximate by a graph with a very simple structure, … <a href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 We now discuss how to view proofs of certain <em>regularity lemmas</em> as applications of the FTRL methodology.</p>
<p>
The Szemeredi Regularity Lemma states (in modern language) that every dense graph is well approximate by a graph with a very simple structure, made of the (edge-disjoint) union of a constant number of weighted complete bipartite subgraphs. The notion of approximation is a bit complicated to describe, but it enables the proof of <em>counting lemmas</em>, which show that, for example, the number of triangles in the original graph is well approximated by the (appropriately weighted) number of triangles in the approximating graph. </p>
<p>
Analogous regularity lemmas, in which an arbitrary object is approximated by a low-complexity object, have been proved for hypergraphs, for subsets of abelian groups (for applications to additive combinatorics), in an analytic setting (for applications to graph limits) and so on. </p>
<p>
The <em>weak regularity lemma</em> of Frieze and Kannan provides, as the name suggests, a weaker kind of approximation than the one promised by Szemeredi’s lemma, but one that is achievable with a graph that has a much smaller number of pieces. If <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> is the “approximation error” that one is willing to tolerate, Szemeredi’s lemma constructs a graph that is the union of a <img alt="{2^{2^{\vdots}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B2%5E%7B%5Cvdots%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{2^{\vdots}}}"/> weighted complete bipartite subgraphs where the height of the tower of exponentials is polynomial in <img alt="{1/\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/\epsilon}"/>. In the Frieze-Kannan construction, that number is cut down to a single exponential <img alt="{2^{O(1/\epsilon^2)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7BO%281%2F%5Cepsilon%5E2%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{O(1/\epsilon^2)}}"/>. This result too can be generalized to graph limits, subsets of groups, and so on.</p>
<p>
With Tulsiani and Vadhan, we proved an abstract version of the Frieze-Kannan lemma (which can be applied to graphs, functions, distributions, etc.) in which the “complexity” of the approximation is <img alt="{O(1/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1/\epsilon^2)}"/>. In the graph case, the approximating graph is still the union of <img alt="{2^{O(1/\epsilon^2)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7BO%281%2F%5Cepsilon%5E2%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{O(1/\epsilon^2)}}"/> complete bipartite subgraphs, but it has a more compact representation. One consequence of this result is that for every high-min-entropy distribution <img alt="{\cal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal D}"/>, there is an efficiently samplable distribution with the same min-entropy as <img alt="{\cal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal D}"/>, that is indistinguishable from <img alt="{\cal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal D}"/>. Such a result could be taken to be a proof that what GANs attempt to achieve is possible in principle, except that our result requires an unrealistically high entropy (and we achieve “efficient samplability” and “indistinguishability” only in a weak sense).</p>
<p>
All these results are proved with a similar strategy: one starts from a trivial approximator, for example the empty graph, and then repeats the following iteration: if the current approximator achieves the required approximation, then we are done; otherwise take a counterexample, and modify the approximator using the counterexample. Then one shows that: </p>
<ul>
<li> The number of iterations is bounded, by keeping track of an appropriate potential function;
</li><li> The “complexity” of the approximator does not increase too much from iteration to iteration.
</li></ul>
<p>
Typically, the number of iterations is <img alt="{O(1/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1/\epsilon^2)}"/>, and the difference between the various results is given by whether at each iteration the “complexity” increases exponentially, or by a multiplicative factor, or by an additive term.</p>
<p>
Like in the post on pseudorandom constructions, one can view such constructions as an online game between a “builder” and an “inspector,” except that now the online optimization algorithm will play the role of the builder, and the inspector is the one acting as an adversary. The <img alt="{O(1/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1/\epsilon^2)}"/> bound on the number of rounds comes from the fact that the online optimization algorithms that we have seen so far achieve amortized error per round <img alt="{O(1/\sqrt T)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Csqrt+T%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1/\sqrt T)}"/> after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> rounds, so it takes <img alt="{O(1/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1/\epsilon^2)}"/> rounds for the error bound to go below <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>.</p>
<p>
We will see that the abstract weak regularity lemma of my paper with Tulsiani and Vadhan (and hence the graph weak regularity lemma of Frieze and Kannan) can be immediately deduced from the theory developed in the previous post. </p>
<p>
When I was preparing these notes, I was asked by several people if the same can be done for Szemeredi’s lemma. I don’t see a natural way of doing that. For such results, one should maybe use the online optimization techniques as a guide rather than as a black box. In general, iterative arguments (in which one constructs an object through a series of improvements) require the choice of a potential function, and an argument about how much the potential function changes at every step. The power of the FTRL method is that it creates the potential function and a big part of the analysis automatically and, even where it does not work directly, it can serve as an inspiration. </p>
<p>
One could imagine a counterfactual history in which people first proved the weak regularity lemma using online optimization out of the box, as we do in this post, and then decided to try and use an L2 potential function and an iterative method to get the Szemeredi lemma, subsequently trying to see what happens if the potential function is entropy, thus discovering Jacob Fox’s major improvement on the “triangle removal lemma,” which involves the construction of an approximator that just approximates the number of triangles.</p>
<p>
<span id="more-4244"/></p>
<p>
</p><p><b>1. A “vanilla” weak regularity lemma </b></p>
<p/><p>
Frieze and Kannan proved the following basic result about graph approximations, which has a number of algorithmic applications. If <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/> is a set of vertices which is understood from the context, and <img alt="{A,B \subseteq V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB+%5Csubseteq+V%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B \subseteq V}"/> are disjoint subsets of vertices, then let <img alt="{K_{A,B} = {\bf 1}_A \cdot {\bf 1}_B^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK_%7BA%2CB%7D+%3D+%7B%5Cbf+1%7D_A+%5Ccdot+%7B%5Cbf+1%7D_B%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K_{A,B} = {\bf 1}_A \cdot {\bf 1}_B^T}"/>, that is, the boolean matrix such that <img alt="{K_{A,B} (i,j) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK_%7BA%2CB%7D+%28i%2Cj%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K_{A,B} (i,j) = 1}"/> iff <img alt="{i\in A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%5Cin+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i\in A}"/> and <img alt="{j\in B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%5Cin+B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j\in B}"/>.</p>
<p>
The <em>cut norm</em> of a matrix <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M}"/> is </p>
<p align="center"><img alt="\displaystyle  || M ||_{\square} := \max_{A,B} | \langle M, K_{A,B} \rangle | " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+M+%7C%7C_%7B%5Csquare%7D+%3A%3D+%5Cmax_%7BA%2CB%7D+%7C+%5Clangle+M%2C+K_%7BA%2CB%7D+%5Crangle+%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || M ||_{\square} := \max_{A,B} | \langle M, K_{A,B} \rangle | "/></p>
<p>
In the following we will identify a graph with its adjacency matrix.</p>
<blockquote><p><b>Theorem 1</b> <em> Let <img alt="{G=(V,E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%3D%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G=(V,E)}"/> be an graph on <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> vertices and <img alt="{\epsilon &gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt;0}"/> be an approximation parameter. </em></p><em>
<p>
Then there are sets <img alt="{A_1,B_1,\ldots,A_T,B_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_1%2CB_1%2C%5Cldots%2CA_T%2CB_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_1,B_1,\ldots,A_T,B_T}"/> and scalars <img alt="{\alpha_1,\ldots,\alpha_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_1%2C%5Cldots%2C%5Calpha_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_1,\ldots,\alpha_T}"/>, where <img alt="{T \leq O(1/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%5Cleq+O%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T \leq O(1/\epsilon^2)}"/>, such that if we define</p>
<p/><p align="center"><img alt="\displaystyle  H:= \sum_{i=1}^T \alpha_i K_{A_i,B_i} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++H%3A%3D+%5Csum_%7Bi%3D1%7D%5ET+%5Calpha_i+K_%7BA_i%2CB_i%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  H:= \sum_{i=1}^T \alpha_i K_{A_i,B_i} "/></p>
<p> we have </p>
<p align="center"><img alt="\displaystyle  || G - H ||_{\square} \leq \epsilon n^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+G+-+H+%7C%7C_%7B%5Csquare%7D+%5Cleq+%5Cepsilon+n%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || G - H ||_{\square} \leq \epsilon n^2 "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
We will prove the following more general version.</p>
<blockquote><p><b>Theorem 2</b> <em> Let <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> be a set, <img alt="{g: X \rightarrow [0,1]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%3A+X+%5Crightarrow+%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g: X \rightarrow [0,1]}"/> be a bounded function, <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/> be a family of functions mapping <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> to <img alt="{[0,1]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[0,1]}"/> and <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> be an approximation parameter. Then there are functions <img alt="{f_1,\ldots,f_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,\ldots,f_T}"/> in <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/> and scalars <img alt="{\alpha_1,\ldots,\alpha_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_1%2C%5Cldots%2C%5Calpha_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_1,\ldots,\alpha_T}"/>, with <img alt="{T = O(1/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3D+O%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T = O(1/\epsilon^2)}"/>, such that if we define </em></p><em>
<p align="center"><img alt="\displaystyle  h := \sum_{i=1}^T \alpha_i f_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++h+%3A%3D+%5Csum_%7Bi%3D1%7D%5ET+%5Calpha_i+f_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  h := \sum_{i=1}^T \alpha_i f_i "/></p>
<p> we have </p>
<p align="center"><img alt="\displaystyle  \forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%7C+%5Clangle+f%2C+g-+h+%5Crangle+%7C+%5Cleq+%5Cepsilon+%7CX%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
We could also, with the same proof, argue about a possibly infinite set <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> with a measure <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/> such that <img alt="{\mu(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu(X)}"/> is finite, and, after defining the inner product</p>
<p/><p align="center"><img alt="\displaystyle  \langle f, g \rangle := \int_X f\cdot g\ d \mu \ , " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+f%2C+g+%5Crangle+%3A%3D+%5Cint_X+f%5Ccdot+g%5C+d+%5Cmu+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \langle f, g \rangle := \int_X f\cdot g\ d \mu \ , "/></p>
<p>
we could prove the same conclusion of the theorem, with <img alt="{\epsilon \cdot \mu(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%5Ccdot+%5Cmu%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon \cdot \mu(X)}"/> instead of <img alt="{\epsilon |X|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon |X|}"/> as an error bound.</p>
<p>
Here is the proof: run the FTRL algorithm with L2-squared regularizer in the setup in which the space of solutions <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is the set of all functions <img alt="{ X \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+X+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ X \rightarrow {\mathbb R}}"/> and the loss functions are linear. Every time the algorithm proposes a solution <img alt="{h_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h_t}"/>, if there is a function <img alt="{f_t \in {\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%5Cin+%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t \in {\cal F}}"/> such that either <img alt="{ \langle h_t - g , f_t \rangle &gt; \epsilon|X|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Clangle+h_t+-+g+%2C+f_t+%5Crangle+%3E+%5Cepsilon%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ \langle h_t - g , f_t \rangle &gt; \epsilon|X|}"/> or <img alt="{ \langle h_t - g , f_t \rangle &lt; - \epsilon|X|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Clangle+h_t+-+g+%2C+f_t+%5Crangle+%3C+-+%5Cepsilon%7CX%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ \langle h_t - g , f_t \rangle &lt; - \epsilon|X|}"/>, the adversary will pick, respectively, <img alt="{f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t}"/> or <img alt="{-f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-f_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-f_t}"/> as a loss function <img alt="{\ell_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t}"/>. When the adversary has no such choice, we stop and the function <img alt="{h_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h_t}"/> is our desired approximation.</p>
<p>
First of all, let us analyze the number of rounds. Here the maximum norm of the functions in <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/> is <img alt="{\sqrt {|X|}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt+%7B%7CX%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt {|X|}}"/>, so after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> rounds we have the regret bound</p>
<p/><p align="center"><img alt="\displaystyle  \forall h : \ \ \sum_{t=1}^T \langle \ell_t, h_t - h \rangle \leq \sqrt{|X|} \cdot || h || \cdot \sqrt{2T} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+h+%3A+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell_t%2C+h_t+-+h+%5Crangle+%5Cleq+%5Csqrt%7B%7CX%7C%7D+%5Ccdot+%7C%7C+h+%7C%7C+%5Ccdot+%5Csqrt%7B2T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall h : \ \ \sum_{t=1}^T \langle \ell_t, h_t - h \rangle \leq \sqrt{|X|} \cdot || h || \cdot \sqrt{2T} "/></p>
<p> Now let us consider <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> to be our offline solution: we have </p>
<p align="center"><img alt="\displaystyle  \epsilon T |X| &lt; \sum_{t=1}^T \langle \ell_t, h_t - g \rangle \leq |X| \cdot \sqrt{2T} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cepsilon+T+%7CX%7C+%3C+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell_t%2C+h_t+-+g+%5Crangle+%5Cleq+%7CX%7C+%5Ccdot+%5Csqrt%7B2T%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \epsilon T |X| &lt; \sum_{t=1}^T \langle \ell_t, h_t - g \rangle \leq |X| \cdot \sqrt{2T} "/></p>
<p> which implies </p>
<p align="center"><img alt="\displaystyle  T &lt; \frac 2{\epsilon^2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++T+%3C+%5Cfrac+2%7B%5Cepsilon%5E2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  T &lt; \frac 2{\epsilon^2} "/></p>
<p> Finally, recall that </p>
<p align="center"><img alt="\displaystyle  h_T = \sum_{t=1}^{T-1} - \frac 1 {2c} \ell_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++h_T+%3D+%5Csum_%7Bt%3D1%7D%5E%7BT-1%7D+-+%5Cfrac+1+%7B2c%7D+%5Cell_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  h_T = \sum_{t=1}^{T-1} - \frac 1 {2c} \ell_t "/></p>
<p> where <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> is the scaling constant in the definition of the regularizer (<img alt="{1/2c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F2c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/2c}"/> is of order of <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> when <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is order of <img alt="{1/\epsilon^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/\epsilon^2}"/>), and so our final approximator <img alt="{h_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h_T}"/> computed at the last round is a weighted sum of functions from <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/>.</p>
<p>
</p><p><b>2. The weak regularity lemma </b></p>
<p/><p>
Frieze and Kannan’s weak regularity lemma has the following form.</p>
<blockquote><p><b>Theorem 3</b> <em><a name="th.fk"/> Let <img alt="{G=(V,E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%3D%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G=(V,E)}"/> be an graph on <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> vertices and <img alt="{\epsilon &gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt;0}"/> be an approximation parameter. </em></p><em>
<p>
Then there is a partition of <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/> into <img alt="{k = 2^{O(1/\epsilon^2)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+2%5E%7BO%281%2F%5Cepsilon%5E2%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = 2^{O(1/\epsilon^2)}}"/> sets <img alt="{S_1,\ldots,S_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_1%2C%5Cldots%2CS_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_1,\ldots,S_k}"/>, and there are bounded weights <img alt="{0\leq p_{i,j} \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5Cleq+p_%7Bi%2Cj%7D+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0\leq p_{i,j} \leq 1}"/> for <img alt="{i,j \in \{1,\ldots, k\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj+%5Cin+%5C%7B1%2C%5Cldots%2C+k%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i,j \in \{1,\ldots, k\}}"/> such that if we defined the weighted graph <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> where the weight of the edge <img alt="{(u,v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(u,v)}"/> in <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> is <img alt="{p_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{i,j}}"/>, where <img alt="{u\in S_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%5Cin+S_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u\in S_i}"/> and <img alt="{v\in S_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%5Cin+S_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v\in S_j}"/>, then we have </p>
<p align="center"><img alt="\displaystyle  || G - H ||_{\square} \leq \epsilon n^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+G+-+H+%7C%7C_%7B%5Csquare%7D+%5Cleq+%5Cepsilon+n%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || G - H ||_{\square} \leq \epsilon n^2 "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
Notice that if we did not require the weights to be between 0 and 1 then the result of the previous section can also be cast in the above language, because we can take the partition <img alt="{S_1,\ldots,S_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_1%2C%5Cldots%2CS_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_1,\ldots,S_k}"/> to be the “Sigma-algebra generated by” the sets <img alt="{A_1,B_1,\ldots,A_T,B_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_1%2CB_1%2C%5Cldots%2CA_T%2CB_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_1,B_1,\ldots,A_T,B_T}"/>.</p>
<p>
For a scalar <img alt="{z\in {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%5Cin+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z\in {\mathbb R}}"/>, let <img alt="{\tau(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tau(z)}"/> be defined as </p>
<p align="center"><img alt="\displaystyle  \tau(z) = \left\{ \begin{array}{rl} 0 &amp; \mbox{ if } z &lt;0\\ z &amp; \mbox{ if } 0\leq z \leq 1\\ 1 &amp; \mbox{ if } z &gt; 1 \end{array} \right. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctau%28z%29+%3D+%5Cleft%5C%7B+%5Cbegin%7Barray%7D%7Brl%7D+0+%26+%5Cmbox%7B+if+%7D+z+%3C0%5C%5C+z+%26+%5Cmbox%7B+if+%7D+0%5Cleq+z+%5Cleq+1%5C%5C+1+%26+%5Cmbox%7B+if+%7D+z+%3E+1+%5Cend%7Barray%7D+%5Cright.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \tau(z) = \left\{ \begin{array}{rl} 0 &amp; \mbox{ if } z &lt;0\\ z &amp; \mbox{ if } 0\leq z \leq 1\\ 1 &amp; \mbox{ if } z &gt; 1 \end{array} \right. "/></p>
<p> where <img alt="{\tau}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tau}"/> stands for <em>t</em>runcation. Note that <img alt="{\tau(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tau(z)}"/> is the L2 projection of <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> on <img alt="{[0,1]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[0,1]}"/>.</p>
<p>
Theorem <a href="https://lucatrevisan.wordpress.com/feed/#th.fk">3</a> is a special case of the following result, proved in our paper with Tulsiani and Vadhan. </p>
<blockquote><p><b>Theorem 4</b> <em><a name="th.ttv"/> Let <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> be a set, <img alt="{g: X \rightarrow [0,1]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%3A+X+%5Crightarrow+%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g: X \rightarrow [0,1]}"/> be a bounded function, <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/> be a family of functions mapping <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> to <img alt="{[0,1]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[0,1]}"/> and <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> be an approximation parameter. Then there are functions <img alt="{f_1,\ldots,f_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,\ldots,f_T}"/> in <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/> and scalars <img alt="{\alpha_1,\ldots,\alpha_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_1%2C%5Cldots%2C%5Calpha_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_1,\ldots,\alpha_T}"/>, with <img alt="{T = O(1/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3D+O%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T = O(1/\epsilon^2)}"/>, such that if we define </em></p><em>
<p align="center"><img alt="\displaystyle  h := \tau\left ( \sum_{i=1}^T \alpha_i f_i \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++h+%3A%3D+%5Ctau%5Cleft+%28+%5Csum_%7Bi%3D1%7D%5ET+%5Calpha_i+f_i+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  h := \tau\left ( \sum_{i=1}^T \alpha_i f_i \right) "/></p>
<p> we have </p>
<p align="center"><img alt="\displaystyle  \forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%7C+%5Clangle+f%2C+g-+h+%5Crangle+%7C+%5Cleq+%5Cepsilon+%7CX%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
To prove Theorem <a href="https://lucatrevisan.wordpress.com/feed/#th.ttv">4</a> we play the same online game as in the previous section: the online algorithm proposes a solution <img alt="{h_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h_t}"/>; if <img alt="{\forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cforall+f%5Cin+%7B%5Ccal+F%7D%3A+%5C+%5C+%7C+%5Clangle+f%2C+g-+h+%5Crangle+%7C+%5Cleq+%5Cepsilon+%7CX%7C+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\forall f\in {\cal F}: \ \ | \langle f, g- h \rangle | \leq \epsilon |X| }"/> then we stop and output <img alt="{h=h_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%3Dh_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h=h_t}"/>, otherwise we let the loss function be a function <img alt="{\ell_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t}"/> such that either <img alt="{\ell_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t}"/> or <img alt="{-\ell_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-\ell_t}"/> is in <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/> and </p>
<p align="center"><img alt="\displaystyle  \langle \ell_t , g- h_t \rangle \geq \epsilon |X| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+%5Cell_t+%2C+g-+h_t+%5Crangle+%5Cgeq+%5Cepsilon+%7CX%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \langle \ell_t , g- h_t \rangle \geq \epsilon |X| "/></p>
<p>
The only difference is that we use the FTRL algorithm with L2 regularizer that has the set feasible solutions <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> defined to be the set of all functions <img alt="{h : X \rightarrow [0,1]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh+%3A+X+%5Crightarrow+%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h : X \rightarrow [0,1]}"/> rather than the set of all functions <img alt="{h: X \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%3A+X+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h: X \rightarrow {\mathbb R}}"/>. Then each function <img alt="{h_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h_T}"/> is the projection to <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> of <img alt="{\sum_{t=1}^{T-1} - \frac 1 {2c}\ell_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bt%3D1%7D%5E%7BT-1%7D+-+%5Cfrac+1+%7B2c%7D%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{t=1}^{T-1} - \frac 1 {2c}\ell_t}"/>, and the projection to <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is just composition with <img alt="{\tau}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tau}"/>. The bound on the number of steps is the same as the one in the previous section.</p>
<p>
Looking at the case in which <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> is the set of edges of a clique on <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/>, <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/> is the set of graphs of the form <img alt="{K_{A,B}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK_%7BA%2CB%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K_{A,B}}"/>, and considering the Sigma-algebra generated by <img alt="{A_1,B_1,\ldots,A_T,B_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_1%2CB_1%2C%5Cldots%2CA_T%2CB_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_1,B_1,\ldots,A_T,B_T}"/> gives Theorem <a href="https://lucatrevisan.wordpress.com/feed/#th.fk">3</a> from Theorem <a href="https://lucatrevisan.wordpress.com/feed/#th.ttv">4</a>.</p>
<p>
</p><p><b>3. Sampling High-Entropy Distributions </b></p>
<p/><p>
Finally we discuss the application to sampling high-entropy distributions. </p>
<p>
Suppose that <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is a distribution over <img alt="{\{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n}"/> of min-entropy <img alt="{\geq n-d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+n-d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\geq n-d}"/>, meaning that for every <img alt="{x\in \{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in \{ 0,1 \}^n}"/> we have </p>
<p align="center"><img alt="\displaystyle  D(x) \leq 2^{-(n-d)} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28x%29+%5Cleq+2%5E%7B-%28n-d%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D(x) \leq 2^{-(n-d)} "/></p>
<p> where we think of the <em>entropy deficiency</em> <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d}"/> as being small, such as a constant or <img alt="{O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\log n)}"/></p>
<p>
Let <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/> be a class of functions <img alt="{\{ 0,1 \}^n \rightarrow \{ 0,1\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n \rightarrow \{ 0,1\}}"/> that we think of as being “efficient.” For example, <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/> could be the set of all functions computable by circuits of size <img alt="{\leq S(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+S%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\leq S(n)}"/> for some size bound <img alt="{S(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S(\cdot)}"/>, such as, for example <img alt="{S(n) = 10 n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28n%29+%3D+10+n%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S(n) = 10 n^2}"/>. We will assume that <img alt="{f(x) \equiv 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%5Cequiv+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x) \equiv 1}"/> is in <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/>. Define </p>
<p align="center"><img alt="\displaystyle  g(x) = 2^{n-d} \cdot D(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%29+%3D+2%5E%7Bn-d%7D+%5Ccdot+D%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(x) = 2^{n-d} \cdot D(x) "/></p>
<p> to be a bounded function <img alt="{g: \{ 0,1 \}^n \rightarrow [0,1]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5B0%2C1%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g: \{ 0,1 \}^n \rightarrow [0,1]}"/>. Fix an approximation parameter <img alt="{\epsilon &gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt;0}"/>.</p>
<p>
Then from Theorem <a href="https://lucatrevisan.wordpress.com/feed/#th.ttv">4</a> we have that there are <img alt="{T = O(1/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3D+O%281%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T = O(1/\epsilon^2)}"/> functions <img alt="{f_1,\ldots,f_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_1%2C%5Cldots%2Cf_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_1,\ldots,f_T}"/>, and scalars <img alt="{\alpha_1,\ldots,\alpha_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_1%2C%5Cldots%2C%5Calpha_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_1,\ldots,\alpha_T}"/>, all equal to <img alt="{\pm 1/2c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1%2F2c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm 1/2c}"/> for a certain parameter <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/>, such that if we define </p>
<p align="center"><img alt="\displaystyle  h(x) = \tau \left( \sum_{t=1}^T \alpha_t f_t(x) \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++h%28x%29+%3D+%5Ctau+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Calpha_t+f_t%28x%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  h(x) = \tau \left( \sum_{t=1}^T \alpha_t f_t(x) \right) "/></p>
<p> we have <a name="eq.ttv.main"/></p><a name="eq.ttv.main">
<p align="center"><img alt="\displaystyle   \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( g(x) f(x) - h(x) f(x) \right ) \right | \leq \epsilon 2^n \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cforall+f+%5Cin+%7B%5Ccal+F%7D+%3A+%5C+%5C+%5C+%5Cleft+%7C+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Cleft%28+g%28x%29+f%28x%29+-+h%28x%29+f%28x%29+%5Cright+%29+%5Cright+%7C+%5Cleq+%5Cepsilon+2%5En+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( g(x) f(x) - h(x) f(x) \right ) \right | \leq \epsilon 2^n \ \ \ \ \ (1)"/></p>
</a><p><a name="eq.ttv.main"/> and so, multiplying by <img alt="{2^{-(n-d)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B-%28n-d%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{-(n-d)}}"/> <a name="eq.ttv.b"/></p><a name="eq.ttv.b">
<p align="center"><img alt="\displaystyle   \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - h(x)2^{-(n-d)} f(x) \right ) \right | \leq \epsilon 2^d \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cforall+f+%5Cin+%7B%5Ccal+F%7D+%3A+%5C+%5C+%5C+%5Cleft+%7C+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Cleft%28+D%28x%29+f%28x%29+-+h%28x%292%5E%7B-%28n-d%29%7D+f%28x%29+%5Cright+%29+%5Cright+%7C+%5Cleq+%5Cepsilon+2%5Ed+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - h(x)2^{-(n-d)} f(x) \right ) \right | \leq \epsilon 2^d \ \ \ \ \ (2)"/></p>
</a><p><a name="eq.ttv.b"/> Now define the probability distribution </p>
<p align="center"><img alt="\displaystyle  H(x) = \frac {h(x)}{\sum_{y\in \{ 0,1 \}^n} h(y) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++H%28x%29+%3D+%5Cfrac+%7Bh%28x%29%7D%7B%5Csum_%7By%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+h%28y%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  H(x) = \frac {h(x)}{\sum_{y\in \{ 0,1 \}^n} h(y) } "/></p>
<p> Applying <a href="https://lucatrevisan.wordpress.com/feed/#eq.ttv.main">(1)</a> to the case <img alt="{f(x) \equiv 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%5Cequiv+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x) \equiv 1}"/>, we have </p>
<p align="center"><img alt="\displaystyle  \left | \sum_{x\in \{ 0,1 \}^n} \left( g(x) - h(x) \right ) \right | \leq \epsilon 2^n " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft+%7C+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Cleft%28+g%28x%29+-+h%28x%29+%5Cright+%29+%5Cright+%7C+%5Cleq+%5Cepsilon+2%5En+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left | \sum_{x\in \{ 0,1 \}^n} \left( g(x) - h(x) \right ) \right | \leq \epsilon 2^n "/></p>
<p> and we know that <img alt="{\sum_x g(x) = 2^{n-d} \sum_x D(x) = 2^{n-d}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_x+g%28x%29+%3D+2%5E%7Bn-d%7D+%5Csum_x+D%28x%29+%3D+2%5E%7Bn-d%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_x g(x) = 2^{n-d} \sum_x D(x) = 2^{n-d}}"/>, so </p>
<p align="center"><img alt="\displaystyle  \left | 2^{n-d} - \sum_{x\in \{ 0,1 \}^n} h(x) \right | \leq \epsilon 2^n " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft+%7C+2%5E%7Bn-d%7D+-+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+h%28x%29+%5Cright+%7C+%5Cleq+%5Cepsilon+2%5En+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left | 2^{n-d} - \sum_{x\in \{ 0,1 \}^n} h(x) \right | \leq \epsilon 2^n "/></p>
<p> and we can rewrite <a href="https://lucatrevisan.wordpress.com/feed/#eq.ttv.b">(2)</a> as </p>
<p align="center"><img alt="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - H(x) \cdot (\sum_y h(y)) 2^{-(n-d)} f(x) \right ) \right | \leq \epsilon 2^d " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+f+%5Cin+%7B%5Ccal+F%7D+%3A+%5C+%5C+%5C+%5Cleft+%7C+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Cleft%28+D%28x%29+f%28x%29+-+H%28x%29+%5Ccdot+%28%5Csum_y+h%28y%29%29+2%5E%7B-%28n-d%29%7D+f%28x%29+%5Cright+%29+%5Cright+%7C+%5Cleq+%5Cepsilon+2%5Ed+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - H(x) \cdot (\sum_y h(y)) 2^{-(n-d)} f(x) \right ) \right | \leq \epsilon 2^d "/></p>
<p> and, finally </p>
<p align="center"><img alt="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - H(x) f(x) \right ) \right | \leq 2 \epsilon 2^d " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+f+%5Cin+%7B%5Ccal+F%7D+%3A+%5C+%5C+%5C+%5Cleft+%7C+%5Csum_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Cleft%28+D%28x%29+f%28x%29+-+H%28x%29+f%28x%29+%5Cright+%29+%5Cright+%7C+%5Cleq+2+%5Cepsilon+2%5Ed+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \sum_{x\in \{ 0,1 \}^n} \left( D(x) f(x) - H(x) f(x) \right ) \right | \leq 2 \epsilon 2^d "/></p>
<p> that is </p>
<p align="center"><img alt="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \Pr_{x\sim D} [f(x) = 1] - \Pr_{x\sim H} [f(x) =1 ] \right | \leq 2 \epsilon 2^d " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+f+%5Cin+%7B%5Ccal+F%7D+%3A+%5C+%5C+%5C+%5Cleft+%7C+%5CPr_%7Bx%5Csim+D%7D+%5Bf%28x%29+%3D+1%5D+-+%5CPr_%7Bx%5Csim+H%7D+%5Bf%28x%29+%3D1+%5D+%5Cright+%7C+%5Cleq+2+%5Cepsilon+2%5Ed+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall f \in {\cal F} : \ \ \ \left | \Pr_{x\sim D} [f(x) = 1] - \Pr_{x\sim H} [f(x) =1 ] \right | \leq 2 \epsilon 2^d "/></p>
<p> which says that <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> and <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> are <img alt="{\epsilon 2^{d+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+2%5E%7Bd%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon 2^{d+1}}"/>-indistinguishable by functions in <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/>. If we chose <img alt="{{\cal F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal F}}"/>, for example, to be the class of functions computable by circuits of size <img alt="{\leq S(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+S%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\leq S(n)}"/>, then <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> and <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> are <img alt="{\epsilon 2^{d+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+2%5E%7Bd%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon 2^{d+1}}"/>-indistinguishable by circuits of size <img alt="{\leq S(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+S%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\leq S(n)}"/>.</p>
<p>
But <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> is also samplable in a relatively efficient way using rejection sampling: pick a random <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, then output <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> with probability <img alt="{h(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h(x)}"/> and fail with probability <img alt="{1-h(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-h%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-h(x)}"/>. Repeat the above until the procedure does not fail. At each step, the probability of success is <img alt="{\mathop{\mathbb E}_{x\in \{ 0,1 \}^n} h(x) \geq 2{-d} - \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D_%7Bx%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D+h%28x%29+%5Cgeq+2%7B-d%7D+-+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathop{\mathbb E}_{x\in \{ 0,1 \}^n} h(x) \geq 2{-d} - \epsilon}"/>, so, assuming (because otherwise all of the above makes no sense) that, say, <img alt="{\epsilon &lt; 2^{-d-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3C+2%5E%7B-d-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &lt; 2^{-d-1}}"/>, the procedure succeeds on average in at most <img alt="{2^{d+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bd%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{d+1}}"/> attempts. And if each <img alt="{f_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t}"/> is computable by a circuit of size <img alt="{S(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S(n)}"/>, then <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> is computable by a circuit of size <img alt="{O(1/\epsilon^2) + S(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%2F%5Cepsilon%5E2%29+%2B+S%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1/\epsilon^2) + S(n)}"/>.</p>
<p>
The undesirable features of this result are that the complexity of sampling and the quality of indistinguishability depend exponentially on the randomness deficiency, and the sampling circuit is a non-uniform circuit that it’s not clear how to construct without advice. Impagliazzo’s recent results address both these issues.</p>
<p/></div>
    </content>
    <updated>2019-05-16T19:37:05Z</updated>
    <published>2019-05-16T19:37:05Z</published>
    <category term="theory"/>
    <category term="online optimization"/>
    <category term="Regularity Lemma"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-27T17:20:41Z</updated>
    </source>
  </entry>
</feed>
