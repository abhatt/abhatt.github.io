<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-04-11T04:21:57Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16921</id>
    <link href="https://rjlipton.wordpress.com/2020/04/10/nina-balcan-wins/" rel="alternate" type="text/html"/>
    <title>Nina Balcan Wins</title>
    <summary>Congrats and More [ CMU ] Nina Balcan is a leading researcher in the theory of machine learning. Nina is at Carnegie-Mellon and was previously at Georgia Tech—it was a major loss to have her leave Tech. Today we applaud her winning the ACM Hopper Award. ACM President Cherri Pancake says: Although she is still […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Congrats and More</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/10/nina-balcan-wins/unknown-137/" rel="attachment wp-att-16924"><img alt="" class="alignright  wp-image-16924" src="https://rjlipton.files.wordpress.com/2020/04/unknown.jpeg?w=170" width="170"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ CMU ]</font></td>
</tr>
</tbody>
</table>
<p>
Nina Balcan is a leading researcher in the theory of machine learning. Nina is at Carnegie-Mellon and was previously at Georgia Tech—it was a major loss to have her leave Tech.</p>
<p>
Today we applaud her winning the ACM Hopper Award.</p>
<p>
ACM President Cherri Pancake <a href="https://www.ml.cmu.edu/news/news-archive/2020/april/machine-learning-professor-balcan-receives-acm-grace-hopper-award.html">says</a>: </p>
<blockquote><p><b> </b> <em> Although she is still in the early stages of her career, she has already established herself as the world leader in the theory of how AI systems can learn with limited supervision. More broadly, her work has realigned the foundations of machine learning, and consequently ushered in many new applications that have brought about leapfrog advances in this exciting area of artificial intelligence. </em>
</p></blockquote>
<p>
</p><p/><h2> The Hopper Award </h2><p/>
<p/><p>
The ACM Grace Murray Hopper <a href="https://awards.acm.org/hopper/award-winners">Award</a> is given to: An outstanding young computer professional, on the basis of a single major contribution before the age of 35. Here are five of the most recent winners: </p>
<ul>
<li>
Constantinos Daskalakis, 	(<a href="https://awards.acm.org/award_winners/daskalakis_4121823">2018</a>)	 <p/>
</li><li>
Michael Freedman, 		(<a href="https://awards.acm.org/award_winners/freedman_6665293">2018</a>)	 <p/>
</li><li>
Amanda Randles, 		(<a href="https://awards.acm.org/award_winners/randles_0365390">2017</a>)	 <p/>
</li><li>
Jeffrey Heer, 			(<a href="https://awards.acm.org/award_winners/heer_1520709">2016</a>)	 <p/>
</li><li>
Brent Waters,			(<a href="https://awards.acm.org/award_winners/waters_3058089.cfm">2015</a>)
</li></ul>
<p>
</p><p/><h2> Nina’s Contribution </h2><p/>
<p/><p>
The Hopper award says it is for a “single” major contribution. I believe that Nina is almost a disproof of this statement: I fail to see how she only did one major contribution. In fact, the <a href="https://awards.acm.org/hopper">citation</a> lists three. In any event I thought we might look at one of her top results on learning. It is a <a href="http://www.cs.cmu.edu/~ninamf/papers/agnostic-active.pdf">paper</a> from 2006 with over 400 citations. The ttile is “Agnostic Active Learning” and is joint with Alina Beygelzimer and John Langford.</p>
<p>
<em>Active learning</em> follows a classic idea in computer theory: Making a protocol interactive can often decrease the cost, and almost always makes the protocol more complex to understand. In active learning one is given unlabeled examples. As usual the goal is to classify the samples. However, as the samples are unlabelled, the learning can ask for labels for elected samples—this is the active part of the learning. As you might imagine asking for labels has a cost, so the learner strives to ask for the fewest labels possible. </p>
<p>
The savings can be large when the labels are perfect—that is, noise-free. In general it is much more complex to understand when active learning helps. Nina’s work found examples where noise can be tamed. Her award citation says:</p>
<blockquote><p><b> </b> <em> Balcan established performance guarantees for active learning that hold even in challenging cases when “noise” is present in the data. These guarantees hold under arbitrary forms of noise, that is, anything that distorts or corrupts the data. This can include anything from a blurry photo, a unit of data that is improperly labeled, meaningless information, or data that the algorithm cannot interpret. </em>
</p></blockquote>
<p/><p>
See her papers for the details. </p>
<p>
</p><p/><h2> Other Awards </h2><p/>
<p/><p>
There are various awards for computer scientists, many are from the ACM. Since Alan Perlis won the first Turing award, there have been 69 more winners. Only three have been women:</p>
<ul>
<li>
Shafi Goldwasser, (2012) <p/>
</li><li>
Barbara Liskov, (2008) <p/>
</li><li>
Frances Allen, (2006)
</li></ul>
<p>Here is another quote from the president of the ACM: </p>
<blockquote><p><b> </b> <em> We typically receive one woman nominee [for the Turing Award] every five years. It’s very disturbing. </em>
</p></blockquote>
<p/><p>
The number of nominations is too small. There are plenty of strong women candidates for the Turing award, and for other awards. We need to do a better job. See <a href="https://slate.com/technology/2020/01/turing-award-acm-women-recipients.html">this</a> for more thoughts on this issue.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We do not know about the situation with nominations for the Hopper Award. Nina is only the seventh woman to win since 1971, but four of the last ten Hopper Award winners have been women. How can we recognize more women under 35 who are doing great work?</p>
<p>
Again we congratulate Nina Balcan on her richly deserved honor. </p>
<p>[Fixed typo “Congrats” ]</p></font></font></div>
    </content>
    <updated>2020-04-10T13:05:57Z</updated>
    <published>2020-04-10T13:05:57Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Results"/>
    <category term="active"/>
    <category term="interactive"/>
    <category term="learning"/>
    <category term="Nina Balcan"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-04-11T04:20:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.04666</id>
    <link href="http://arxiv.org/abs/2004.04666" rel="alternate" type="text/html"/>
    <title>Exploration with Limited Memory: Streaming Algorithms for Coin Tossing, Noisy Comparisons, and Multi-Armed Bandits</title>
    <feedworld_mtime>1586476800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Assadi:Sepehr.html">Sepehr Assadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Chen.html">Chen Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.04666">PDF</a><br/><b>Abstract: </b>Consider the following abstract coin tossing problem: Given a set of $n$
coins with unknown biases, find the most biased coin using a minimal number of
coin tosses. This is a common abstraction of various exploration problems in
theoretical computer science and machine learning and has been studied
extensively over the years. In particular, algorithms with optimal sample
complexity (number of coin tosses) have been known for this problem for quite
some time.
</p>
<p>Motivated by applications to processing massive datasets, we study the space
complexity of solving this problem with optimal number of coin tosses in the
streaming model. In this model, the coins are arriving one by one and the
algorithm is only allowed to store a limited number of coins at any point --
any coin not present in the memory is lost and can no longer be tossed or
compared to arriving coins. Prior algorithms for the coin tossing problem with
optimal sample complexity are based on iterative elimination of coins which
inherently require storing all the coins, leading to memory-inefficient
streaming algorithms.
</p>
<p>We remedy this state-of-affairs by presenting a series of improved streaming
algorithms for this problem: we start with a simple algorithm which require
storing only $O(\log{n})$ coins and then iteratively refine it further and
further, leading to algorithms with $O(\log\log{(n)})$ memory, $O(\log^*{(n)})$
memory, and finally a one that only stores a single extra coin in memory -- the
same exact space needed to just store the best coin throughout the stream.
</p>
<p>Furthermore, we extend our algorithms to the problem of finding the $k$ most
biased coins as well as other exploration problems such as finding top-$k$
elements using noisy comparisons or finding an $\epsilon$-best arm in
stochastic multi-armed bandits, and obtain efficient streaming algorithms for
these problems.
</p></div>
    </summary>
    <updated>2020-04-10T23:21:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.04586</id>
    <link href="http://arxiv.org/abs/2004.04586" rel="alternate" type="text/html"/>
    <title>Storing Set Families More Compactly with Top ZDDs</title>
    <feedworld_mtime>1586476800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matsuda:Kotaro.html">Kotaro Matsuda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Denzumi:Shuhei.html">Shuhei Denzumi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sadakane:Kunihiko.html">Kunihiko Sadakane</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.04586">PDF</a><br/><b>Abstract: </b>Zero-suppressed Binary Decision Diagrams (ZDDs) are data structures for
representing set families in a compressed form. With ZDDs, many valuable
operations on set families can be done in time polynomial in ZDD size. In some
cases, however, the size of ZDDs for representing large set families becomes
too huge to store them in the main memory. This paper proposes top ZDD, a novel
representation of ZDDs which uses less space than existing ones. The top ZDD is
an extension of top tree, which compresses trees, to compress directed acyclic
graphs by sharing identical subgraphs. We prove that navigational operations on
ZDDs can be done in time poly-logarithmicin ZDD size, and show that there exist
set families for which the size of the top ZDD is exponentially smaller than
that of the ZDD. We also show experimentally that our top ZDDs have smaller
size than ZDDs for real data.
</p></div>
    </summary>
    <updated>2020-04-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.04496</id>
    <link href="http://arxiv.org/abs/2004.04496" rel="alternate" type="text/html"/>
    <title>Near-Optimal Decremental SSSP in Dense Weighted Digraphs</title>
    <feedworld_mtime>1586476800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bernstein:Aaron.html">Aaron Bernstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gutenberg:Maximilian_Probst.html">Maximilian Probst Gutenberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wulff=Nilsen:Christian.html">Christian Wulff-Nilsen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.04496">PDF</a><br/><b>Abstract: </b>In the decremental Single-Source Shortest Path problem (SSSP), we are given a
weighted directed graph $G=(V,E,w)$ undergoing edge deletions and a source
vertex $r \in V$; let $n = |V|, m = |E|$ and $W$ be the aspect ratio of the
graph. The goal is to obtain a data structure that maintains shortest paths
from $r$ to all vertices in $V$ and can answer distance queries in $O(1)$ time,
as well as return the corresponding path $P$ in $O(|P|)$ time.
</p>
<p>This problem was first considered by Even and Shiloach [JACM'81], who
provided an algorithm with total update time $O(mn)$ for unweighted undirected
graphs; this was later extended to directed weighted graphs [FOCS'95, STOC'99].
There are conditional lower bounds showing that $O(mn)$ is in fact near-optimal
[ESA'04, FOCS'14, STOC'15, STOC'20]. In a breakthrough result, Forster et al.
showed that it is possible to achieve total update time $mn^{0.9+o(1)}\log W$
if the algorithm is allowed to return $(1+{\epsilon})$-approximate paths,
instead of exact ones [STOC'14, ICALP'15]. No further progress was made until
Probst Gutenberg and Wulff-Nilsen [SODA'20] provided a new approach for the
problem, which yields total time $\tilde{O}(\min{m^{2/3}n^{4/3}\log W,
(mn)^{7/8} \log W})$.
</p>
<p>Our result builds on this recent approach, but overcomes its limitations by
introducing a significantly more powerful abstraction, as well as a different
core subroutine. Our new framework yields a decremental
$(1+{\epsilon})$-approximate SSSP data structure with total update time
$\tilde{O}(n^2 \log^4 W)$. Our algorithm is thus near-optimal for dense graphs
with polynomial edge-weights. Our framework can also be applied to sparse
graphs to obtain total update time $\tilde{O}(mn^{2/3} \log^3 W)$.
</p>
<p>Our main technique allows us to convert SSSP algorithms for DAGs to ones for
general graphs, which we believe has significant potential to influence future
work.
</p></div>
    </summary>
    <updated>2020-04-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.04365</id>
    <link href="http://arxiv.org/abs/2004.04365" rel="alternate" type="text/html"/>
    <title>Computing skeletons for rectilinearly-convex obstacles in the rectilinear plane</title>
    <feedworld_mtime>1586476800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Volz:Marcus.html">Marcus Volz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brazil:Marcus.html">Marcus Brazil</a>, Charl Ras, Doreen Thomas <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.04365">PDF</a><br/><b>Abstract: </b>We introduce the concept of an obstacle skeleton which is a set of line
segments inside a polygonal obstacle $\omega$ that can be used in place of
$\omega$ when performing intersection tests for obstacle-avoiding network
problems in the plane. A skeleton can have significantly fewer line segments
compared to the number of line segments in the boundary of the original
obstacle, and therefore performing intersection tests on a skeleton (rather
than the original obstacle) can significantly reduce the CPU time required by
algorithms for computing solutions to obstacle-avoidance problems. A minimum
skeleton is a skeleton with the smallest possible number of line segments. We
provide an exact $O(n^2)$ algorithm for computing minimum skeletons for
rectilinear obstacles in the rectilinear plane that are rectilinearly-convex.
We show that the number of edges in a minimum skeleton is generally very small
compared to the number of edges in the boundary of the original obstacle, by
performing experiments on random rectilinearly-convex obstacles with up to 1000
vertices.
</p></div>
    </summary>
    <updated>2020-04-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.04344</id>
    <link href="http://arxiv.org/abs/2004.04344" rel="alternate" type="text/html"/>
    <title>A Pedagogically Sound yet Efficient Deletion algorithm for Red-Black Trees: The Parity-Seeking Delete Algorithm</title>
    <feedworld_mtime>1586476800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghiasi=Shirazi:Kamaledin.html">Kamaledin Ghiasi-Shirazi</a>, Taraneh Ghandi, Ali Taghizadeh, Ali Rahimi-Baigi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.04344">PDF</a><br/><b>Abstract: </b>Red-black (RB) trees are one of the most efficient variants of balanced
binary search trees. However, they have always been blamed for being too
complicated, hard to explain, and not suitable for pedagogical purposes.
Sedgewick (2008) proposed left-leaning red-black (LLRB) trees in which red
links are restricted to left children, and proposed recursive concise insert
and delete algorithms. However, the top-down deletion algorithm of LLRB is
still very complicated and highly inefficient. In this paper, we first consider
2-3 red-black trees in which both children cannot be red. We propose a
parity-seeking delete algorithm with the basic idea of making the deficient
subtree on a par with its sibling: either by fixing the deficient subtree or by
making the sibling deficient, as well, ascending deficiency to the parent node.
This is the first pedagogically sound algorithm for the delete operation in
red-black trees. Then, we amend our algorithm and propose a parity-seeking
delete algorithm for classical RB trees. Our experiments show that, despite
having more rotations, 2-3 RB trees are almost as efficient as RB trees and
twice faster than LLRB trees. Besides, RB trees with the proposed
parity-seeking delete algorithm have the same number of rotations and almost
identical running time as the classic delete algorithm. While being extremely
efficient, the proposed parity-seeking delete algorithm is easily
understandable and suitable for pedagogical purposes.
</p></div>
    </summary>
    <updated>2020-04-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.04250</id>
    <link href="http://arxiv.org/abs/2004.04250" rel="alternate" type="text/html"/>
    <title>An Improved Cutting Plane Method for Convex Optimization, Convex-Concave Games and its Applications</title>
    <feedworld_mtime>1586476800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Haotian.html">Haotian Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Yin_Tat.html">Yin Tat Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Song:Zhao.html">Zhao Song</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wong:Sam_Chiu=wai.html">Sam Chiu-wai Wong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.04250">PDF</a><br/><b>Abstract: </b>Given a separation oracle for a convex set $K \subset \mathbb{R}^n$ that is
contained in a box of radius $R$, the goal is to either compute a point in $K$
or prove that $K$ does not contain a ball of radius $\epsilon$. We propose a
new cutting plane algorithm that uses an optimal $O(n \log (\kappa))$
evaluations of the oracle and an additional $O(n^2)$ time per evaluation, where
$\kappa = nR/\epsilon$.
</p>
<p>$\bullet$ This improves upon Vaidya's $O( \text{SO} \cdot n \log (\kappa) +
n^{\omega+1} \log (\kappa))$ time algorithm [Vaidya, FOCS 1989a] in terms of
polynomial dependence on $n$, where $\omega &lt; 2.373$ is the exponent of matrix
multiplication and $\text{SO}$ is the time for oracle evaluation.
</p>
<p>$\bullet$ This improves upon Lee-Sidford-Wong's $O( \text{SO} \cdot n \log
(\kappa) + n^3 \log^{O(1)} (\kappa))$ time algorithm [Lee, Sidford and Wong,
FOCS 2015] in terms of dependence on $\kappa$.
</p>
<p>For many important applications in economics, $\kappa = \Omega(\exp(n))$ and
this leads to a significant difference between $\log(\kappa)$ and
$\mathrm{poly}(\log (\kappa))$. We also provide evidence that the $n^2$ time
per evaluation cannot be improved and thus our running time is optimal.
</p>
<p>A bottleneck of previous cutting plane methods is to compute leverage scores,
a measure of the relative importance of past constraints. Our result is
achieved by a novel multi-layered data structure for leverage score
maintenance, which is a sophisticated combination of diverse techniques such as
random projection, batched low-rank update, inverse maintenance, polynomial
interpolation, and fast rectangular matrix multiplication. Interestingly, our
method requires a combination of different fast rectangular matrix
multiplication algorithms.
</p></div>
    </summary>
    <updated>2020-04-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.04207</id>
    <link href="http://arxiv.org/abs/2004.04207" rel="alternate" type="text/html"/>
    <title>Computing Convex Partitions for Point Sets in the Plane: The CG:SHOP Challenge 2020</title>
    <feedworld_mtime>1586476800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fekete:S=aacute=ndor_P=.html">Sándor P. Fekete</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Keldenich:Phillip.html">Phillip Keldenich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krupke:Dominik.html">Dominik Krupke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitchell:Joseph_S=_B=.html">Joseph S. B. Mitchell</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.04207">PDF</a><br/><b>Abstract: </b>We give an overview of the 2020 Computational Geometry Challenge, which
targeted the problem of partitioning the convex hull of a given planar point
set P into the smallest number of convex faces, such that no point of P is
contained in the interior of a face.
</p></div>
    </summary>
    <updated>2020-04-10T23:28:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7686</id>
    <link href="https://windowsontheory.org/2020/04/09/experts-shmexperts/" rel="alternate" type="text/html"/>
    <title>Experts shmexperts</title>
    <summary>(If you’re not already following him, I highly recommend reading Luca Trevisan’s dispatches from Milan, much more interesting than what I write below.) On the topic of my last post, Ross Douthat writes in the New York Times that “In the fog of coronavirus, there are no experts”, even citing Scott Aaronson’s post. Both Aaronson […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>(If you’re not already following him, I highly recommend reading <a href="https://lucatrevisan.wordpress.com/">Luca Trevisan’s dispatches from Milan</a>, much more interesting than what I write below.)</em></p>



<p>On the topic of my <a href="https://windowsontheory.org/2020/04/04/in-defense-of-expertise/">last post</a>, Ross Douthat writes in the New York Times that <a href="https://www.nytimes.com/2020/04/07/opinion/coronavirus-science-experts.html?smid=fb-share&amp;fbclid=IwAR1KIcMblvNsaKaAQTh2vCpB2yARYL8O5TVy9j3ZxSqsXsRAMK2GGcvF81o">“In the fog of coronavirus, there are no experts”</a>, even citing <a href="https://www.scottaaronson.com/blog/?p=4695">Scott Aaronson’s post</a>. Both Aaronson and Douthat make the point that the COVID-19 crisis is so surprising and unprecedented, and experts were so much in the dark, that there is no reason to trust them over non expert “common sense” or “armchair epidemiologists”. <br/><br/>It’s true that the “expert models” have significant uncertainty, hardwired constants, noisy data, and dubious assumptions. It is also true that many countries (especially those that didn’t learn from the 2003 SARS epidemic) bungled their initial response. But do we really need to challenge the notion of expertise itself? To what extent was this pandemic not predicted by experts or progressed in ways defying their expectations?</p>



<p>Here is what some of these experts and institutions were saying in the recent past:</p>



<p><em>“The thing I’m most concerned about … is the emergence of a new virus that the body doesn’t have any background experience with, that is very transmissible, highly transmissible from person to person, and has a high degree of morbidity and mortality … a respiratory illness that can spread even before someone is so sick that you want to keep them in bed.”</em>  <a href="https://fivethirtyeight.com/features/dr-fauci-has-been-dreading-a-pandemic-like-covid-19-for-years/">Dr. Anthoni Fauci, 2019</a>.</p>



<p><em>“High-impact respiratory pathogens … pose particular global risks … [they] are spread via respiratory droplets; they can infect a large number of people very quickly and with today’s transportation infrastructure, move rapidly across multiple geographies. … There is insufficient R&amp;D investment and planning for innovative vaccine development and manufacture, broad-spectrum antivirals, … In addition, such a pandemic requires advance planning across multiple sectors … Epidemic control costs would completely overwhelm the current financing arrangements for emergency response.” </em><a href="https://apps.who.int/gpmb/assets/annual_report/GPMB_annualreport_2019.pdf">WHO world at risk report</a>, 2019.</p>



<p><em>“respiratory transmission …. is the transmission route posing the greatest pandemic risk   … [since it] can occur with coughing or simply breathing (in aerosol transmission), making containment much more challenging. …  If a pathogen is capable of causing asymptomatic or mildly symptomatic infections that either do not or only minimally interrupt activities of daily living, many individuals may be exposed. Viruses that cause the common cold, including coronaviruses, have this attribute.”</em> <a href="https://apps.who.int/gpmb/assets/thematic_papers/tr-6.pdf">JHU report</a>, 2019.</p>



<p>As an experiment, I also tried to compare the response of experts and “contrarians” in real time as the novel coronavirus was discovered, trying to see if it’s really the case that, as Douthat says, <em>“up until mid-March you were better off trusting the alarmists of anonymous Twitter than the official pronouncements from the custodians of public health”</em>.  I chose both experts and contrarians that are active on Twitter. I was initially planning to look at several people but due to laziness am just taking Imperial college’s <a href="https://twitter.com/Imperial_JIDEA/">J-IDEA institute</a> for the expert, and <a href="https://twitter.com/robinhanson">Robin Hanson</a> for the contrarian.  I also looked at <a href="https://twitter.com/DouthatNYT">Douthat’s twitter feed</a>, to see if he followed his own advice. Initially I thought I would go all the way to March but have no time so just looked at the period from January 1 till February 14th. I leave any conclusions to the reader.</p>



<h2><strong>January 1-19:</strong> </h2>



<p>(Context: novel coronavirus confirmed in Wuhan, initially unclear if there is human to human transmission – this was confirmed by China on January 20 though suspected before.)</p>



<p>Here is one of the many tweets by Imperial from this period:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">Substantial human to human transmission cannot be ruled out – size of novel <a href="https://twitter.com/hashtag/coronavirus?src=hash&amp;ref_src=twsrc%5Etfw">#coronavirus</a> in Wuhan outbreak likely over 1700 cases. <a href="https://twitter.com/MailOnline?ref_src=twsrc%5Etfw">@MailOnline</a> on <a href="https://twitter.com/MRC_Outbreak?ref_src=twsrc%5Etfw">@MRC_Outbreak</a>, <a href="https://twitter.com/Imperial_JIDEA?ref_src=twsrc%5Etfw">@Imperial_JIDEA</a>, <a href="https://twitter.com/imperialcollege?ref_src=twsrc%5Etfw">@imperialcollege</a> report today.<a href="https://t.co/Iq4hBmx4JL">https://t.co/Iq4hBmx4JL</a> <a href="https://t.co/3n5OMPYNdL">pic.twitter.com/3n5OMPYNdL</a></p>— J-IDEA (@Imperial_JIDEA) <a href="https://twitter.com/Imperial_JIDEA/status/1218295967683874816?ref_src=twsrc%5Etfw">January 17, 2020</a></blockquote></div>
</div></figure>



<p>I didn’t see any tweets from Hanson or Douthat on this topic.</p>



<h2><strong>January 20-31:</strong> </h2>



<p>(Context: first confirmed cases in several countries, including the US, WHO declares emergency in Jan 30, US restricts travel from China on Jan 31. By then there are about 10K confirmed cases and 213 deaths worldwide.)</p>



<p>On January 25th Imperial college estimated the novel coronavirus “R0” parameter as 2.6:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">UPDATE: Transmissibility estimates of <a href="https://twitter.com/hashtag/coronavirus?src=hash&amp;ref_src=twsrc%5Etfw">#coronavirus</a> <a href="https://twitter.com/hashtag/2019nCoV?src=hash&amp;ref_src=twsrc%5Etfw">#2019nCoV</a> at 2.6<br/><br/>Identification &amp; testing potential cases to be as extensive as permitted by healthcare &amp; testing capacity<br/><br/><img alt="&#x1F530;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f530.png" style="height: 1em;"/><a href="https://t.co/7zpZzG2JOs">https://t.co/7zpZzG2JOs</a><a href="https://twitter.com/neil_ferguson?ref_src=twsrc%5Etfw">@neil_ferguson</a> <a href="https://twitter.com/dr_anne_cori?ref_src=twsrc%5Etfw">@dr_anne_cori</a> <a href="https://twitter.com/SRileyIDD?ref_src=twsrc%5Etfw">@SRileyIDD</a> <a href="https://twitter.com/MarcBaguelin?ref_src=twsrc%5Etfw">@MarcBaguelin</a> <a href="https://twitter.com/IlariaDorigatti?ref_src=twsrc%5Etfw">@IlariaDorigatti</a> <a href="https://t.co/nmhjWsWpfa">pic.twitter.com/nmhjWsWpfa</a></p>— J-IDEA (@Imperial_JIDEA) <a href="https://twitter.com/Imperial_JIDEA/status/1221033477824532480?ref_src=twsrc%5Etfw">January 25, 2020</a></blockquote></div>
</div></figure>



<p>Hanson tweeted approvingly about China’s response and that this situation might help the “more authoritarian” U.S. presidential candidate:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">Seems to me the "parasite stress hypothesis of authoritarianism" suggests that if this China Coronavirus ends up being a big deal, that would push US voters toward the more authoritarian presidential candidate. Which one is that?<a href="https://t.co/cWmV2WkroJ">https://t.co/cWmV2WkroJ</a><a href="https://t.co/ysNiZIkfYD">https://t.co/ysNiZIkfYD</a></p>— Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1222184281050697728?ref_src=twsrc%5Etfw">January 28, 2020</a></blockquote></div>
</div></figure>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">I doubt we in US would be as fast to restrict travel in the face of such a still-small pandemic. If so, China is to be praised for their better abilities to coordinate in the face of such threats.<a href="https://t.co/Lro5kGVTvz">https://t.co/Lro5kGVTvz</a></p>— Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1220704857922985984?ref_src=twsrc%5Etfw">January 24, 2020</a></blockquote></div>
</div></figure>



<p>Still no tweet from Douthat on this topic though he did say in January 29th that compared to issues in the past the U.S.’s problems in the 2020’s are “problem of decadence” rather than any crisis like the late 1970’s:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">Belated, yes, America's problems in '20 are problems of decadence rather than late-1970s crisis, and economically '16 might have been a better year to gamble on Bernie …<a href="https://t.co/3DaHWXC1k0">https://t.co/3DaHWXC1k0</a></p>— Ross Douthat (@DouthatNYT) <a href="https://twitter.com/DouthatNYT/status/1222566830642024448?ref_src=twsrc%5Etfw">January 29, 2020</a></blockquote></div>
</div></figure>



<h2><strong>February 1-14: </strong></h2>



<p>(Context: Diamond princess cruise ship quaranteed, disease gets COVID-19 official name, first death in Europe)</p>



<p>Imperial continues to tweet extensively, including the following early estimates of the case fatality rates:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">UPDATE: <a href="https://twitter.com/hashtag/coronavirus?src=hash&amp;ref_src=twsrc%5Etfw">#coronavirus</a> <a href="https://twitter.com/hashtag/2019nCoV?src=hash&amp;ref_src=twsrc%5Etfw">#2019nCoV</a> Severity<br/><br/><img alt="&#x27A1;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/27a1.png" style="height: 1em;"/>Estimated fatality ratio for infections 1%<br/><img alt="&#x27A1;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/27a1.png" style="height: 1em;"/>Estimated CFR for travellers outside mainland China (mix severe &amp; milder cases) 1%-5%<br/><img alt="&#x27A1;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/27a1.png" style="height: 1em;"/>Estimated CFR for detected cases in Hubei (severe cases) 18%<br/><br/><img alt="&#x1F530;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f530.png" style="height: 1em;"/><a href="https://t.co/7zpZzG2JOs">https://t.co/7zpZzG2JOs</a> <a href="https://t.co/gtmzq1vOhq">pic.twitter.com/gtmzq1vOhq</a></p>— J-IDEA (@Imperial_JIDEA) <a href="https://twitter.com/Imperial_JIDEA/status/1226766907396718597?ref_src=twsrc%5Etfw">February 10, 2020</a></blockquote></div>
</div></figure>



<p>Robin Hanson correctly realizes this is going to spread wide:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">Look at this table and tell me is isn't all over China now, beyond recall: <a href="https://t.co/Ne9UgDlx9G">pic.twitter.com/Ne9UgDlx9G</a></p>— Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1227701511469387777?ref_src=twsrc%5Etfw">February 12, 2020</a></blockquote></div>
</div></figure>



<p>Hanson tweets quite a lot about this, including potential social implications. Up to February 13th there is nothing too “contrarian” at this point, but also no information that could not be gotten from the experts:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">My poll so far estimates ~40% chance that <a href="https://twitter.com/hashtag/COVID19?src=hash&amp;ref_src=twsrc%5Etfw">#COVID19</a> infects large % of world. So seems worth it for social scientists to ask themselves: using social sci, what non-obvious predictions can we make about social outcomes in that case?</p>— Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1227656071021334528?ref_src=twsrc%5Etfw">February 12, 2020</a></blockquote></div>
</div></figure>



<p>In February 14 Hansons makes a very contrarian position when he proposes “controlled infection” as a solution:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">Though it is a disturbing &amp; extreme option, we should seriously consider deliberately infecting folks with coronavirus, to spread out the number of critically ill people over time, and to ensure that critical infrastructure remains available to help sick. <a href="https://t.co/giIfo8z8v0">https://t.co/giIfo8z8v0</a></p>— Robin Hanson (@robinhanson) <a href="https://twitter.com/robinhanson/status/1228400896507367424?ref_src=twsrc%5Etfw">February 14, 2020</a></blockquote></div>
</div></figure>



<p>To the anticipated “you first” objection he responds <em>“I proposed compensating volunteers via cash or medical priority for associates, &amp; I’d seriously consider such offers.”</em>.  He doesn’t mention that he is much less strapped for cash than some of the would be “volunteers”. </p>



<p>Still no tweet from Douthat about COVID-19 though he does write that we live in an “age of decadence”:</p>



<figure class="wp-block-embed-twitter wp-block-embed is-type-rich"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">My Sunday essay excerpts my new book: The Age of Decadence: <a href="https://t.co/3cvLqNVQpv">https://t.co/3cvLqNVQpv</a></p>— Ross Douthat (@DouthatNYT) <a href="https://twitter.com/DouthatNYT/status/1225908487198330880?ref_src=twsrc%5Etfw">February 7, 2020</a></blockquote></div>
</div></figure>



<p/></div>
    </content>
    <updated>2020-04-09T19:40:32Z</updated>
    <published>2020-04-09T19:40:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-04-11T04:21:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4361</id>
    <link href="https://lucatrevisan.wordpress.com/2020/04/09/the-peak-the-plateau-and-the-phase-two/" rel="alternate" type="text/html"/>
    <title>The peak, the plateau, and the phase two</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">What has been happening in Italy in the last few days, and what can other Western countries expect in the next week or two? The national discourse has been obsessed with “The Peak,” that is, the time when things reach … <a href="https://lucatrevisan.wordpress.com/2020/04/09/the-peak-the-plateau-and-the-phase-two/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>What has been happening in Italy in the last few days, and what can other Western countries expect in the next week or two?</p>
<p>The national discourse has been obsessed with “The Peak,” that is, the time when things reach their worst point, and start improving after that. For the last several days, all indicators, such as new cases, deaths, and ICU occupancy, have been improving. Apparently, then,  “The Peak” is behind us. Virologists have been cautious to say that “peak” is the wrong mountain metaphor to use, and that we have rather reached a “plateau” in which things will change very slowly for a while. </p>
<p>Below is the number of confirmed covid-19 deaths in Italy updated with today’s data, showing that we reached the plateau a couple of weeks ago, meaning that the number of new cases started to plateau about a month ago, when the lockdown started.</p>
<p><img alt="italy-deaths" class="alignnone size-full wp-image-4365" src="https://lucatrevisan.files.wordpress.com/2020/04/italy-deaths.png?w=584"/></p>
<p>The data from New York City continues to track the data from Lombardy, so NYC should be just a few days away from its own plateau, if the match continues.</p>
<p><img alt="lombardy-nyc" class="alignnone size-full wp-image-4366" src="https://lucatrevisan.files.wordpress.com/2020/04/lombardy-nyc.png?w=584"/></p>
<p><img alt="lombardy-nyc-daily" class="alignnone size-full wp-image-4367" src="https://lucatrevisan.files.wordpress.com/2020/04/lombardy-nyc-daily.png?w=584"/></p>
<p>Given all this, people have been wondering when and how we will get out of the lockdown, and reach what everybody has been calling the “Phase Two” of this emergency.</p>
<p><span id="more-4361"/></p>
<p>The lockdown is set to expire this coming Monday, and it is expected that tomorrow or Saturday the prime minister will announce new measures. (Perhaps, according to precedent, he will do so on Sunday night.) It is expected that the stay-at-home order will be extended to early May, or even mid-May, but that the definition of “essential activities” will be relaxed to allow some manufacturing to restart sooner.</p>
<p>Meanwhile, an infrastructure to isolate new cases and trace their contacts, which should have been frantically under construction over the last month, is still non-existent. Last week, the government nominated a committee of 70+ experts to “begin thinking about mapping out possibilities” for what such an infrastructure might be like.</p>
<p>To be honest, I am not too confident that the “Phase Two” will be organized with Taiwanese, or even Korean, efficiency, and my only hope is that the number of cases in Lombardy has been so under-reported that we may already be close to herd immunity.</p>
<p>This is probably not the case, but not by a wide margin. The Italian Institute of Statistics has released 2019 vs 2020 all-cause mortality data from a representative sample of Italian towns. Apparently, during the worst days of March, all-cause mortality roughly doubled nation-wide, while the reported deaths caused by covid-19 account for only about half of the excess deaths. This might mean that there have been 20,000 covid-19 deaths and maybe 2 million infected people out of 10 million in Lombardy. A study of the Imperial College estimates, at the high end, that 6 million Italians have been infected, and since Lombardy’s  data has consistently accounted for half the national data on all measures, it would mean 3 million infected people in Lombardy, or 30%, which is within a factor of two of what might suffice for herd immunity. In any case we will not know until there is a randomized serologic study, which is something else for which experts are almost ready to begin mapping out ways of thinking about how to explore plans for … </p>
<p>What will life be like in “Phase Two”? If the epidemic continues at a slow burn, will we have to continue to keep a one-meter distance from strangers? Will trains and planes run with only every third seat occupied? Will tickets cost three times as much? Will beaches be open during the summer? Will there be riots if Italians are not allowed to go to the beach in August? Apart from the last question, whose answer is obviously yes, everything is up in the air.</p>
<p>What about me, after 32 days of lockdown? I was already in need of a haircut at the end of February, and lately the hair situation had become untenable, so I used my beard trimmer to blindly cut my hair. Mistakes were made, but I would not even rate it among my top ten worst haircuts ever. </p></div>
    </content>
    <updated>2020-04-09T16:55:15Z</updated>
    <published>2020-04-09T16:55:15Z</published>
    <category term="Milan"/>
    <category term="covid-19"/>
    <category term="exploring possibilities for thinking about roadmaps"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-04-11T04:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/044</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/044" rel="alternate" type="text/html"/>
    <title>TR20-044 |  Cryptography from Information Loss | 

	Marshall Ball, 

	Elette Boyle, 

	Akshay Degwekar, 

	Apoorvaa Deshpande, 

	Alon Rosen, 

	Vinod Vaikuntanathan, 

	Prashant Vasudevan</title>
    <summary>Reductions between problems, the mainstay of theoretical computer science, efficiently map an instance of one problem to an instance of another in such a way that solving the latter allows solving the former. The subject of this work is ``lossy'' reductions, where the reduction loses some information about the input instance. We show that such reductions, when they exist, have interesting and powerful consequences for lifting hardness into ``useful'' hardness, namely cryptography.

Our first, conceptual, contribution is a definition of lossy reductions in the language of mutual information. Roughly speaking, our definition says that a reduction $C$ is $t$-lossy if, for any distribution $X$ over its inputs, the mutual information $I(X;C(X)) \leq t$. Our treatment generalizes a variety of seemingly related but distinct notions such as worst-case to average-case reductions, randomized encodings (Ishai and Kushilevitz, FOCS 2000), homomorphic computations (Gentry, STOC 2009), and instance compression (Harnik and Naor, FOCS 2006).

We then proceed to show several consequences of lossy reductions:

1. We say that a language $L$ has an $f$-reduction to a language $L'$ for a Boolean function $f$ if there is a (randomized) polynomial-time algorithm $C$ that takes an $m$-tuple of strings $X = (x_1,\ldots,x_m)$, with each $x_i\in\{0,1\}^n$, and outputs a string $z$ such that with high probability, L'(z) = f(L(x_1),L(x_2),...,L(x_m))
        
2. Suppose a language $L$ has an $f$-reduction $C$ to $L'$ that is $t$-lossy. Our first result is that one-way functions exist if $L$ is worst-case hard and one of the following conditions holds:
   - $f$ is the OR function, $t \leq m/100$, and $L'$ is the same as $L$
   - $f$ is the Majority function, and $t \leq m/100$
   - $f$ is the OR function, $t \leq O(m\log{n})$, and the reduction has no error

This improves on the implications that follow from combining (Drucker, FOCS 2012) with (Ostrovsky and Wigderson, ISTCS 1993) that result in auxiliary-input one-way functions.

3. Our second result is about the stronger notion of $t$-compressing $f$-reductions -- reductions that only output $t$ bits. We show that if there is an average-case hard language $L$ that has a $t$-compressing Majority reduction to some language for $t=m/100$, then there exist collision-resistant hash functions.

This improves on the result of (Harnik and Naor, STOC 2006), whose starting point is a cryptographic primitive (namely, one-way functions) rather than average-case hardness, and whose assumption is a compressing OR-reduction of SAT (which is now known to be false unless the polynomial hierarchy collapses).

Along the way, we define a non-standard one-sided notion of average-case hardness, which is the notion of hardness used in the second result above, that may be of independent interest.</summary>
    <updated>2020-04-08T07:58:37Z</updated>
    <published>2020-04-08T07:58:37Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-04-11T04:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4719</id>
    <link href="https://www.scottaaronson.com/blog/?p=4719" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4719#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4719" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">When events make craziness sane</title>
    <summary xml:lang="en-US">This post is simply to say the following (and thereby to make it common knowledge that I said it, and that I no longer give a shit who thinks less of me for saying it): If the pandemic has radicalized you, I won’t think that makes you crazy. It’s radicalized me, noticeably shifted my worldview. […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post is simply to say the following (and thereby to make it common knowledge that I said it, and that I no longer give a shit who thinks less of me for saying it):</p>



<p>If the pandemic has radicalized you, I won’t think that makes you crazy.  It’s radicalized me, noticeably shifted my worldview.  And in some sense, I no more apologize for that, than I apologize for my worldview presumably differing from what it would’ve been in some parallel universe with no WWII.</p>



<p>If you suspect that all those earnest, well-intentioned plans and slogans about “flattening the curve” are wonderful and essential, but still, <em>“flattening” is only a desperate gambit to buy some time and nothing more</em>; still, flattening or no flattening, the fundamentals of the situation are that either</p>



<p>(1) a vaccine or cure gets discovered and deployed, or else</p>



<p>(2) we continue in quasi-lockdown mode for the rest of our lives, or else</p>



<p>(3) the virus spreads to the point where it definitely kills some people you know,</p>



<p>—if you suspect this, then at least in my book you’re not crazy.  I suspect the same.</p>



<p>If you still don’t understand, no matter how patiently it’s explained to you, why ~18 months is the <em>absolute bare minimum</em> needed to get a vaccine out; if all the talk of Phase 1, 2, and 3 trials and the need to learn more about rare side effects and so forth seems hard to square with the desperate world war that this is; if you wonder whether the Allied commanders and Allied medical authorities in WWII, transported to the present, would <em>agree</em> that 18 months is the bare minimum, or whether they’d already be distributing vaccines a month ago that probably work well enough and do bounded damage if they don’t—I hereby confess that I don’t understand it either.</p>



<p>If you wonder how the US will possibly hold an election in November that the world won’t rightly consider a sham—given that the only safe way will be universal vote-by-mail, but Trump and his five Vichy justices will never allow it—know that I wonder this too.</p>



<p>If you think that all those psychiatrists now doing tele-psychiatry should tell their patients, “listen, I’ve been noticing an unhealthy absence of panic attacks, obsessions about the government trying to kill your family, or compulsive disinfecting of doorknobs, so I think we’d better up your dose of pro-anxiety medication”—I’m with you.</p>



<p>If you see any US state that wants to avoid &gt;2% deaths, being pushed to the brink of openly defying the FDA, smuggling in medical supplies to escape federal confiscation, using illegal tests and illegal masks and illegal ventilators and illegal everything else, and you also see military commanders getting fired for going outside the chain of command to protect their soldiers’ lives, and you wonder whether this is the start of some broader dissolution of the Union—well, <em>I</em> don’t intend to repeat the mistake of underestimating this crisis.</p>



<p>If you think that the feds who <em>literally confiscate medical supplies before they can reach the hospitals</em>, might as well just shoot the patients as they’re wheeled into the ICU and say “we’re sorry, but this action was obligatory under directive 48c(7)”—I won’t judge you for feeling that way. </p>



<p>If you feel like, while there are still pockets of brilliance and kindness and inspiration and even heroism all over US territory, still, as a federal entity the United States <em>effectively no longer exists or functions</em>, at least not if you treat “try to stop the mass death of the population” as a nonnegotiable component of the “life, liberty, and happiness” foundation for the nation’s existence—if you think this, I won’t call you crazy.  I feel more like a citizen of nowhere every day.</p>



<p>If you’d jump, should the opportunity arise (as it won’t), to appoint Bill Gates as temporary sovereign for as long as this crisis lasts, and thereafter hold a new Constitutional Convention to design a stronger democracy, attempting the first-ever Version 2.0 (as opposed to 1.3, 1.4, etc.) of the American founders’ vision, this time with <em>even more</em> safeguards against destruction by know-nothings and demagogues—if you’re in for that, I don’t think you’re crazy.  I’m wondering where to sign up.</p>



<p>Finally, if you’re one of the people who constantly emails me wrong P=NP proofs or local hidden-variable explanations of quantum mechanics … sorry, I still think you’re crazy.  That stuff hasn’t been affected.</p>



<p>Happy Passover and Easter!</p></div>
    </content>
    <updated>2020-04-08T02:58:26Z</updated>
    <published>2020-04-08T02:58:26Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Embarrassing Myself"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-04-10T23:00:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3470</id>
    <link href="https://agtb.wordpress.com/2020/04/07/call-for-papers-the-2nd-workshop-on-behavioral-economics-and-computation/" rel="alternate" type="text/html"/>
    <title>Call for Papers: The 2nd Workshop on Behavioral Economics and Computation</title>
    <summary>We solicit research contributions and participants for the 2nd Workshop on Behavioral Economics and Computation, to be held in conjunction with the Twenty-First ACM Conference on Economics and Computation (ACM EC '20).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div><span><a href="https://sites.google.com/view/behavioralec2020/">https://sites.google.com/view/behavioralec2020/</a><br/>
July 17, 2020, Budapest, Hungary<br/>
At the 21st ACM Conference on Economics and Computation (ACM EC ’20)</span></div>
<div><span>**In the event the in-person conference does not happen due to the COVID-19 pandemic, we will hold the workshop virtually.</span></div>
<div/>
<div><strong><span style="color: #ff0000;">SUBMISSIONS DUE May 18, 2020, 11:59pm PDT.</span></strong></div>
<div/>
<div/>
<div><strong>Call for Papers: the 2nd Workshop on Behavioral Economics and Computation</strong></div>
<div/>
<div><span>We solicit research contributions and participants for the 2nd Workshop on Behavioral Economics and Computation, to be held in conjunction with the Twenty-First ACM Conference on Economics and Computation (ACM EC ’20). </span></div>
<div/>
<div><span>Based on the successful workshop last year, we aim to bring together again researchers and practitioners from diverse subareas of EC, who are interested in the intersection of human economic behavior and computation, to share new results and to discuss future directions for behavioral research related to economics and computation. It will be a full-day workshop, and will feature invited speakers, contributed paper presentations and a panel discussion. </span></div>
<div/>
<div><span>The gap between rationality-based analysis that assumes utility-maximizing agents and actual human behavior in the real world has been well recognized in economics, psychology and other social sciences. In recent years, there has been growing interest in conducting behavioral research across many of the sub-areas related to economics and computation to address this gap. In one direction, some of these studies leverage insights on human decision making from the behavioral economics and psychology literature to study economic and computational systems with human users. In the other direction, computational tools are used to study and gain insights on human behavior and a data-driven approach is used to learn behavior models from user-generated data.</span></div>
<div><span><br/>
The 2nd Behavioral EC workshop aims to provide a venue for researchers and practitioners from diverse fields, including but not limited to computer science, economics, psychology and sociology, to exchange ideas related to behavioral research in economics and computation. In addition to sharing new results, we hope the workshop will foster a lively discussion of future directions and methodologies for behavioral research related to economics and computation as well as fruitful cross-pollination of behavioral economics, cognitive psychology and computer science. </span></div>
<div><span><br/>
We welcome studies at the intersection of economic behavior and computation from a rich set of theoretical, experimental and empirical perspectives. The topics of interest for the workshop are behavioral research in all settings covered by EC, including but not limited to:</span></div>
<ul>
<li><span>Behavioral mechanism design and applied mechanism design</span></li>
<li><span>Boundedly-rational models of economic decision making</span></li>
<li><span>Empirical studies of human economic behavior</span></li>
<li><span>Model evaluation and selection based on behavioral data</span></li>
<li><span>Data-driven modelling</span></li>
<li><span>Online prediction markets, online experiments, and crowdsourcing platforms</span></li>
<li><span>Hybrid human-machine systems</span></li>
<li><span>Models and experiments about social considerations (e.g. fairness and trust) in decision making</span></li>
<li><span>Methods for behavioral EC: information aggregation, probability elicitation, quality control</span></li>
</ul>
<div/>
<div/>
<div><span><strong>Submission Instructions</strong><br/>
</span></div>
<div/>
<div><span style="color: #ff0000;">Submission deadline: May 18, 2020, 11:59pm PDT.</span></div>
<div><span style="color: #ff0000;">Notification: June 11, 2020</span></div>
<div/>
<div><span>All submissions will be peer reviewed. We will give priority to new (unpublished) research papers but will also consider ongoing research and recently published papers that may be of interest to the workshop audience. For submissions of published papers, authors must clearly state the venue of publication. Position papers and panel discussion proposals are also welcome. Papers will be reviewed for relevance, significance, originality, research contribution, and likelihood to catalyze discussion. </span></div>
<div><span><br/>
Submissions can be in any format and any length. We recommend the EC submission format. </span><span>The workshop will not have archival proceedings but will post accepted papers on the workshop website. At least one author of each accepted paper will be expected to attend and present their findings at the workshop.<p/>
<p/></span></div>
<div><span>Submissions should be uploaded to Easychair no later than May 18th, 2020, 11:59pm PDT. </span></div>
<div/>
<div/>
<div><span><strong>Organizing Committee</strong><br/>
</span></div>
<div><span><br/>
Yiling Chen, Harvard University<br/>
Dan Goldstein, Microsoft Research<br/>
Kevin Leyton-Brown, University of British Columbia<br/>
Shengwu Li, Harvard University<br/>
Gali Noti, Hebrew University</span></div>
<div/>
<div><span><br/>
<strong>More Information</strong><br/>
</span></div>
<div><span><br/>
For more information or questions, visit the workshop website:<br/>
<a href="https://sites.google.com/view/behavioralec2020/">https://sites.google.com/view/behavioralec2020/</a><br/>
or email the organizing committee: <a href="mailto:behavioralec2020@easychair.org">behavioralec2020@easychair.org</a></span></div>
			<div id="atatags-26942-5e8be264da029"/></div>
    </content>
    <updated>2020-04-07T02:15:59Z</updated>
    <published>2020-04-07T02:15:59Z</published>
    <category term="Uncategorized"/>
    <category term="Conferences"/>
    <author>
      <name>Kevin Leyton-Brown</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-04-11T04:20:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3272927110650993879</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3272927110650993879/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/return-of-vidcast.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3272927110650993879" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3272927110650993879" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/return-of-vidcast.html" rel="alternate" type="text/html"/>
    <title>Return of the Vidcast</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Bill and I just have a <a href="https://youtu.be/VTX3yiPri5c">discussion</a>, virtually of course.
<br/><br/>
<div><br/></div><div><br/></div></div>
    </content>
    <updated>2020-04-06T21:36:00Z</updated>
    <published>2020-04-06T21:36:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-04-10T09:42:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/04/06/professorship-in-theoretical-computer-science-at-university-of-copenhagen-apply-by-may-24-2020/</id>
    <link href="https://cstheory-jobs.org/2020/04/06/professorship-in-theoretical-computer-science-at-university-of-copenhagen-apply-by-may-24-2020/" rel="alternate" type="text/html"/>
    <title>Professorship in Theoretical Computer Science at University of Copenhagen (apply by May 24, 2020)</title>
    <summary>University of Copenhagen is seeking candidates for a full professorship in Theoretical Computer Science. More specifically, we are inviting exceptional candidates from the broad fields of algorithms, complexity, and cryptography including privacy. The application deadline is May 24, 2020. Enquiries are welcome and may be sent to Mikkel Thorup (mthorup@di.ku.dk) or Mads Nielsen (madsn@di.ku.dk). Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>University of Copenhagen is seeking candidates for a full professorship in Theoretical Computer Science. More specifically, we are inviting exceptional candidates from the broad fields of algorithms, complexity, and cryptography including privacy. The application deadline is May 24, 2020. Enquiries are welcome and may be sent to Mikkel Thorup (mthorup@di.ku.dk) or Mads Nielsen (madsn@di.ku.dk).</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18971&amp;ProjectId=151668">https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18971&amp;ProjectId=151668</a><br/>
Email: mthorup@di.ku.dk</p></div>
    </content>
    <updated>2020-04-06T19:13:00Z</updated>
    <published>2020-04-06T19:13:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-04-11T04:20:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16911</id>
    <link href="https://rjlipton.wordpress.com/2020/04/05/not-as-easy-as-abc/" rel="alternate" type="text/html"/>
    <title>Not As Easy As ABC</title>
    <summary>Is the claimed proof of the ABC conjecture correct? [ Photo courtesy of Kyodo University ] Shinichi Mochizuki is about to have his proof of the ABC conjecture published in a journal. The proof needs more than a ream of paper—that is, it is over 500 pages long. Today I thought we would discuss his […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Is the claimed proof of the ABC conjecture correct?</em><br/>
<font color="#000000"/></p><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/05/not-as-easy-as-abc/screen-shot-2020-04-05-at-8-22-38-pm/" rel="attachment wp-att-16914"><img alt="" class="alignright size-medium wp-image-16914" height="238" src="https://rjlipton.files.wordpress.com/2020/04/screen-shot-2020-04-05-at-8.22.38-pm.png?w=300&amp;h=238" width="300"/></a><p/>
<p>
<font color="#0044cc">
</font></p></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Photo courtesy of Kyodo University ]</font></td>
</tr>
</tbody>
</table>
<p>
Shinichi Mochizuki is about to have his proof of the ABC conjecture <a href="https://futurism.com/the-byte/mathematicians-shocked-paper-published">published</a> in a journal. The proof needs more than a ream of paper—that is, it is over 500 pages long. </p>
<p>
Today I thought we would discuss his claimed proof of this famous conjecture.</p>
<p>
The decision to published is also discussed in an <a href="https://www.nature.com/articles/d41586-020-00998-2">article</a> in <em>Nature</em>. Some of the discussion we have seen <a href="https://www.math.columbia.edu/~woit/wordpress/?p=11709">elsewhere</a> has been about personal factors. We will just comment briefly on the problem, the proof, and how to tell if a proof has problems. </p>
<p>
</p><p/><h2> The Problem </h2><p/>
<p/><p>
Number theory is hard because addition and multiplication do not play well together. Adding numbers is not too complex by its self; multiplication by its self is also not too hard. For those into formal logic the theory of addition for example is decidable. So in principle there is no hard problem that only uses addition. None. A similar point follows for multiplication. </p>
<p>
But together addition and multiplication is hard. Of course Kurt Gödel proved that the formal theory of arithmetic is hard. It is not complete, for example. There must be statements about addition and multiplication that are unprovable in Peano Arithmetic. </p>
<p>
The ABC conjecture states a property that is between addition and multiplication. Suppose that 	</p>
<p align="center"><img alt="\displaystyle  A + B = C, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A+%2B+B+%3D+C%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A + B = C, "/></p>
<p>for some integers <img alt="{1 \le A \le B \le C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Cle+A+%5Cle+B+%5Cle+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \le A \le B \le C}"/>. Then 	</p>
<p align="center"><img alt="\displaystyle  C \le ABC " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+ABC+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le ABC "/></p>
<p>is trivial. The ABC conjecture says that one can do better and get 	</p>
<p align="center"><img alt="\displaystyle  C \le F(ABC), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+F%28ABC%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le F(ABC), "/></p>
<p>for a function <img alt="{F(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(X)}"/> that is sometimes much smaller than <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>. The function <img alt="{F(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(X)}"/> depends not on the size of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> but on the multiplicative structure of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>. That is the function depends on the multiplicative structure of the integers. Note, the bound 	</p>
<p align="center"><img alt="\displaystyle  C \le ABC " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+ABC+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le ABC "/></p>
<p>only needed that <img alt="{A,B,C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B,C}"/> were numbers larger than <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. The stronger bound 	</p>
<p align="center"><img alt="\displaystyle  C \le F(ABC), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+F%28ABC%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le F(ABC), "/></p>
<p>relies essentially on the finer structure of the integers. </p>
<p>
Roughly <img alt="{F(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(X)}"/> operates as follows: Compute all the primes <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> that divide <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>. Let <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> be the product of all these primes. Then <img alt="{F(X) \le Q^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28X%29+%5Cle+Q%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(X) \le Q^{2}}"/> works: 	</p>
<p align="center"><img alt="\displaystyle  C \le Q^{2}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+Q%5E%7B2%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le Q^{2}. "/></p>
<p>The key point is: <i>Even if <img alt="{p^{100}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B100%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p^{100}}"/>, for example, divides <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>, we only include <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> in the product <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/>.</i> This is where the savings all comes from. This is why the ABC conjecture is hard: repeated factors are thrown away.</p>
<p>
Well not exactly, there is a constant missing here, the bound is 	</p>
<p align="center"><img alt="\displaystyle  C \le \alpha Q^{2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+%5Calpha+Q%5E%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le \alpha Q^{2} "/></p>
<p>where <img alt="{\alpha&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha&gt;0}"/> is a universal constant. We can replace <img alt="{Q^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q^{2}}"/> by a smaller number—the precise statement can be found <a href="https://en.wikipedia.org/wiki/Abc_conjecture">here</a>. This is the ABC conjecture. </p>
<p>
The point here is that in many cases <img alt="{F(ABC)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28ABC%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(ABC)}"/> is vastly smaller than <img alt="{ABC}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BABC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ABC}"/> and so that inequality 	</p>
<p align="center"><img alt="\displaystyle  C \le F(ABC), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+F%28ABC%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le F(ABC), "/></p>
<p>is much better than the obvious one of 	</p>
<p align="center"><img alt="\displaystyle  C \le ABC. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++C+%5Cle+ABC.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  C \le ABC. "/></p>
<p>For example, suppose that one wishes to know if 	</p>
<p align="center"><img alt="\displaystyle  5^{z} = 2^{x} + 3^{y}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++5%5E%7Bz%7D+%3D+2%5E%7Bx%7D+%2B+3%5E%7By%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  5^{z} = 2^{x} + 3^{y}, "/></p>
<p>is possible. The ABC conjecture shows that this cannot happen for <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> large enough. Note 	</p>
<p align="center"><img alt="\displaystyle  F(2^{x} 3^{y} 5^{z}) = 30 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%282%5E%7Bx%7D+3%5E%7By%7D+5%5E%7Bz%7D%29+%3D+30+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(2^{x} 3^{y} 5^{z}) = 30 "/></p>
<p>for positive integers <img alt="{x,y,z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y,z}"/>.</p>
<p>
</p><p/><h2> Is He Correct? </h2><p/>
<p/><p>
Eight years ago Mochizuki announced his <a href="http://www.kurims.kyoto-u.ac.jp/~motizuki/papers-english.html">proof</a>. Now it is about to be published in a journal. He is famous for work in part of number theory. He solved a major open problem there years ago. This gave him instant credibility and so his claim of solving the ABC conjecture was taken seriously. </p>
<p>
For example, one of his papers is <a href="https://www.math.uni-bielefeld.de/documenta/vol-kato/mochizuki.dm.pdf">The Absolute Anabelian Geometry of Canonical Curves</a>. The paper says: </p>
<blockquote><p><b> </b> <em> How much information about the isomorphism class of the variety <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{X}"/> is contained in the knowledge of the étale fundamental group? </em>
</p></blockquote>
<p/><p>
A glance at this paper shows that it is for specialists only. But it does seem to be math of the type that we see all the time. And indeed the proof in his paper is long believed to be correct. This is in sharp contrast to his proof of the ABC conjecture. </p>
<p>
</p><p/><h2> Indicators of Correctness </h2><p/>
<p/><p>
The question is: Are there ways to detect if a proof is (in)correct? Especially <a href="https://en.wikipedia.org/wiki/List_of_long_mathematical_proofs">long</a> proofs? Are there ways that rise above just checking the proof line by line? By the way:</p>
<blockquote><p><b> </b> <em> The length of unusually long proofs has increased with time. As a rough rule of thumb, 100 pages in 1900, or 200 pages in 1950, or 500 pages in 2000 is unusually long for a proof. </em>
</p></blockquote>
<p/><p>
There are some ways to gain confidence. Here are some in my opinion that are useful.</p>
<ol>
<li>
Is the proof understood by the experts? <p/>
</li><li>
Has the proof been generalized? <p/>
</li><li>
Have new proofs been found? <p/>
</li><li>
Does the proof have a clear roadmap?
</li></ol>
<p>
The answer to the first question (1) seems to be no for the ABC proof. At least two world experts have raised concerns—see this <a href="https://www.quantamagazine.org/titans-of-mathematics-clash-over-epic-proof-of-abc-conjecture-20180920/">article</a> in <em>Quanta</em>—that appear serious. The proof has not yet been generalized. This is an important milestone for any proof. Andrew Wiles famous proof that the Fermat equation 	</p>
<p align="center"><img alt="\displaystyle  x^{p} + y^{p} = z^{p}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7Bp%7D+%2B+y%5E%7Bp%7D+%3D+z%5E%7Bp%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^{p} + y^{p} = z^{p}, "/></p>
<p>has no solutions in integers for <img alt="{xyz \neq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bxyz+%5Cneq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{xyz \neq 0}"/> and <img alt="{p \ge 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%5Cge+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p \ge 3}"/> a prime has been extended. This certainly adds confidence to our belief that it is correct.</p>
<p>
Important problems eventually get other proofs. This can take some time. But there is almost always success in finding new and different proofs. Probably it is way too early for the ABC proof, but we can hope. Finally the roadmap issue: This means does the argument used have a nice logical flow. Proofs, even long proofs, often have a logic flow that is not too complex. A proof that says: Suppose there is a object <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> with this property. Then it follows that there must be an object <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/> so that <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/> Is more believable than one with a much more convoluted logical flow. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Ivan Fesenko of Nottingham has written an <a href="https://www.maths.nottingham.ac.uk/plp/pmzibf/rpp.pdf">essay</a> about the proof and the decision to publish. Among factors he notes is “the potential lack of mathematical infrastructure and language to communicate novel concepts and methods”—noting the steep learning curve of trying to grasp the language and framework in which Mochizuki has set his proof. Will the decision to publish change the dynamics of this effort?</p>
<p>[Fixed typo]</p></font></div>
    </content>
    <updated>2020-04-06T02:30:51Z</updated>
    <published>2020-04-06T02:30:51Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="ABC"/>
    <category term="claimed proof"/>
    <category term="conjecture"/>
    <category term="long proof"/>
    <category term="open problems"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-04-11T04:20:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/043</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/043" rel="alternate" type="text/html"/>
    <title>TR20-043 |  A combinatorial MA-complete problem | 

	Dorit Aharonov, 

	Alex Bredariol Grilo</title>
    <summary>Despite the interest in the complexity class MA, the randomized analog of NP, there is just a couple of known natural (promise-)MA-complete problems, the first due to Bravyi and Terhal (SIAM Journal of Computing 2009) and the second due to Bravyi (Quantum Information and Computation 2015). Surprisingly, both problems are stated using terminology from quantum computation. This fact makes it hard for classical complexity theorists to study these problems, and prevents possible progress, e.g., on the important question of derandomizing MA.

In this note we define a natural combinatorial problem called SetCSP and prove its MA-completeness. The problem generalizes the constraint satisfaction problem (CSP) into constraints on sets of strings. This note is, in fact, a combination of previously known works: the brilliant work of Bravyi and Terhal, together with an observation made in our previous work (Aharonov and Grilo, FOCS 2019) that a restricted case of the Bravyi and Terhal's MA complete problem (namely, the uniform case) is already complete, and moreover, that this restricted case can be stated using a classical, combinatorial description. Here we flesh out this observation.

This note, along with a translation of the main result of Aharonov and Grilo to the SetCSP language, implies that finding a gap-amplification procedure for SetCSP problems (namely a generalization to SetCSPs of the gap-amplification used in Dinur's PCP proof) would imply MA=NP. This would provide a resolution of the major problem of derandomizing MA; in fact, the problem of finding gap-amplification for SetCSP is in fact equivalent to the MA=NP problem.</summary>
    <updated>2020-04-05T19:37:00Z</updated>
    <published>2020-04-05T19:37:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-04-11T04:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1285</id>
    <link href="https://ptreview.sublinear.info/?p=1285" rel="alternate" type="text/html"/>
    <title>News for March 2020</title>
    <summary>I hope all of you are keeping safe and healthy in these difficult times. Thank heavens we have our research and our math to keep our sanity… This month has seen two papers, one on testing variable partitions and one on distributed isomorphism testing. Learning and Testing Variable Partitions by Andrej Bogdanov and Baoxiang Wang […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I hope all of you are keeping safe and healthy in these difficult times. Thank heavens we have our research and our math to keep our sanity…</p>



<p>This month has seen two papers, one on testing variable partitions and one on distributed isomorphism testing.</p>



<p><strong>Learning and Testing Variable Partitions</strong> by Andrej Bogdanov and Baoxiang Wang (<a href="https://arxiv.org/abs/2003.12990">arXiv</a>). Consider a function \(f:\Sigma^n \to G\), where \(G\) is Abelian group. Let \(V\) denote the set of variables. The function \(f\) is \(k\)-separable if there is a partition \(V_1, V_2, \ldots, V_k\) of $V$ such that \(f(V)\) can be expressed as the sum \(f_1(V_1) + f_2(V_2) + \ldots + f_k(V_k)\). This is an obviously natural property to study, though the specific application mentioned in the paper is high-dimensional reinforcement learning control. There are a number of learning results, but we’ll focus on the main testing result. The property of \(k\)-separability can be tested with \(O(kn^3/\varepsilon)\) queries, for \(\Sigma = \mathbb{Z}_q\) (and distance between functions is the usual Hamming distance). There is an analogous result (with different query complexity) for \(\Sigma = \mathbb{R}\). It is also shown that testing 2-separability requires \(\Omega(n)\) queries, even with 2-sided error. The paper, to its credit, also has empirical studies of the learning algorithm with applications to reinforcement learning.</p>



<p><strong>Distributed Testing of Graph Isomorphism in the CONGEST model </strong>by Reut Levi and Moti Medina (<a href="https://arxiv.org/pdf/2003.00468.pdf">arXiv</a>). This result follows a recent line of research in distributed property testing algorithms. The main aim is to minimize the number of rounds of (synchronous) communication for a property testing problem. Let \(G_U\) denote the graph representing the distributive network. The aim is to test whether an input graph \(G_K\) is isomorphic to \(G_U\). The main property testing results are as follows. For the dense graph case, isomorphism can be property tested (with two-sided error) in \(O(D + (\varepsilon^{-1}\log n)^2) \) rounds, where \(D\) is the diameter of the graph and \(n\) is the number of nodes. (And, as a reader of this blog, you probably know what \(\varepsilon\) is already…). There is a standard \(\Omega(D)\) lower bound for distributed testing problems. For various classes of sparse graphs (like bounded-degree minor-free classes), constant time isomorphism (standard) property testers are known. This paper provides a simulation argument showing that standard/centralized \(q\)-query property testers can be implemented in the distributed model, in \(O(Dq)\) rounds (this holds for any property, not just isomorphism). Thus, these simulations imply \(O(D)\)-round property testers for isomorphism for bounded-degree minor-free classes.</p></div>
    </content>
    <updated>2020-04-05T05:22:05Z</updated>
    <published>2020-04-05T05:22:05Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-04-10T23:52:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4714</id>
    <link href="https://www.scottaaronson.com/blog/?p=4714" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4714#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4714" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">If I used Twitter…</title>
    <summary xml:lang="en-US">I’m thinking of writing a novel where human civilization is threatened by a global pandemic, and is then almost singlehandedly rescued by one man … a man who reigned for decades as the world’s prototypical ruthless and arrogant tech billionaire, but who was then transformed by the love of his wife. That is, if the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>I’m thinking of writing a novel where human civilization is threatened by a global pandemic, and is then almost singlehandedly rescued by one man … a man who reigned for decades as the world’s prototypical ruthless and arrogant tech billionaire, but who was then transformed by the love of his wife.  That is, <em>if</em> the billionaire can make it past government regulators as evil as they are stupid.  I need some advice: how can I make my storyline a bit subtler, so critics don’t laugh it off as some immature nerd fantasy?</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Updates (April 5):</span></strong> Thanks to several commenters for emphasizing that the wife needs to be a central character here: I agree!  The other thing is, I don’t want Fox News cheering my novel for its <em>Atlas Shrugged</em> vibe.  So maybe the pandemic is only surging out of control in the US because of the incompetence of a Republican president?  I don’t want to go ridiculously overboard, but like, maybe the president is some thuggish conman with the diction of a 5-year-old, who the deluded Republicans cheer anyway?  And maybe he’s also a Bible-thumping fundamentalist?  OK, that’s too much, so maybe the fundamentalist is like the <em>vice</em> president or something, and he gets put in charge of the pandemic response and then sets about muzzling the scientists?  As I said, I really need advice on making the messages subtler.</p></div>
    </content>
    <updated>2020-04-04T23:34:39Z</updated>
    <published>2020-04-04T23:34:39Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Obviously I'm Not Defending Aaronson"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-04-10T23:00:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=413</id>
    <link href="https://tcsplus.wordpress.com/2020/04/04/tcs-talk-wednesday-april-8-ramon-van-handel-princeton/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, April 8 — Ramon van Handel, Princeton</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, April 8th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Ramon van Handel from Princeton will speak about how “Rademacher type and Enflo type coincide” (abstract below). You can reserve a spot as an individual or a group […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, April 8th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Ramon van Handel</strong> from Princeton will speak about how “<em>Rademacher type and Enflo type coincide</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. (The link will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to join, until the maximum capacity of 300 seats is reached.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: In Banach space theory, Rademacher type is an important invariant that controls many geometric and probabilistic properties of normed spaces. It is of considerable interest in various settings to understand to what extent such powerful tools extend to general metric spaces. A natural metric analogue of Rademacher type was proposed by Enflo in the 1960s-70s, and has found a number of interesting applications. Despite much work in the intervening years, however, the relationship between Rademacher type and Enflo type has remained unclear. This basic question is settled in joint work with Paata Ivanisvili and Sasha Volberg: in the setting of Banach spaces, Rademacher type and Enflo type coincide. The proof is based on a very simple but apparently novel insight on how to prove dimension-free inequalities on the Boolean cube. I will not assume any prior background in Banach space theory in the talk.</p></blockquote></div>
    </content>
    <updated>2020-04-04T22:46:48Z</updated>
    <published>2020-04-04T22:46:48Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-04-11T04:21:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7673</id>
    <link href="https://windowsontheory.org/2020/04/04/in-defense-of-expertise/" rel="alternate" type="text/html"/>
    <title>In defense of expertise</title>
    <summary>Scott Aaronson blogged in defense of “armchair epidemiology”. Scott makes several points I agree with, but he also advocates that rather than discounting ideas from “contrarians” who have no special expertise in the matter, each one of us should evaluate the input of such people on its merits. I disagree. I can judge on their […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Scott Aaronson blogged in <a href="https://www.scottaaronson.com/blog/?p=4695">defense of “armchair epidemiology”</a>. Scott makes several points I agree with, but he also advocates that rather than discounting ideas from “contrarians” who have no special expertise in the matter, each one of us should evaluate the input of such people on its merits.</p>



<p>I disagree. I can judge on their merits the validity of a proposed P vs NP proof or a quantum algorithm for SAT, but I have seen time and again smart and educated non-experts misjudge such proposals. As much as I’d like to think otherwise, I would probably be fooled just as easily by a well-presented proposal in area X that “brushes under the rug” subtleties that experts in X would immediately notice.</p>



<p>This is not to say that non experts should stay completely out of the matter. Just like scientific journalists such as Erica Klarreich and Kevin Hartnett of quanta can do a great job of explaining computer science topics to lay audience, so can other well-read people serve as “signal boosters” and highlight works of experts in epidemiology. Journalist Helen Branswell of <a href="https://www.statnews.com/">stat news</a> has been following the <a href="https://www.nytimes.com/2020/03/30/business/media/stat-news-boston-coronavirus.html">novel coronavirus since January 4th</a>. </p>



<p>The difference is that these journalists don’t pretend to see what the experts are missing but rather to highlight and simplify the works that experts are already doing. This is unlike “contrarians” such as Robin Hanson that do their own analysis on a spreadsheet and come up with a “home brewed” policy proposal such as <a href="http://www.overcomingbias.com/2020/02/consider-controlled-infection.html">deliberate infection</a> or <a href="http://www.overcomingbias.com/2020/03/variolation-may-cut-covid19-deaths-3-30x.html">variolation</a> (with “hero hotels” in which people go to be deliberately infected). I am not saying that such proposals are necessarily wrong, but I am saying that I (or anyone else without the experience in this field) am not qualified to judge them. Even if they did “make sense” to me (they don’t) I would not feel any more confident in judging them than I would in reviewing a paper in astronomy. There is a reason why Wikipedia has a <a href="https://en.wikipedia.org/wiki/Wikipedia:No_original_research">“no original research”</a> policy.</p>



<p>Moreover, the attitude of dismissing expertise can be dangerous, whether it comes in the form of <a href="https://en.wikipedia.org/wiki/Teach_the_Controversy">“teach the debate”</a> in the context of evolution, or <a href="https://en.wikipedia.org/wiki/Climatic_Research_Unit_email_controversy">“ClimateGate”</a> in the context of climate change. Unlike the narrative of few brave “dissenters” or “contrarians”, in the case of COVID-19, experts as well as the world health organization have been literally sounding the alarm (see also <a href="https://www.nytimes.com/article/coronavirus-timeline.html">timeline</a>, as well as this NPR story on the <a href="https://www.npr.org/2020/04/03/826945368/how-the-united-states-failed-to-see-the-coronavirus-crisis-coming">US response</a>). Yes, some institutions, and especially the U.S., failed in several aspects (most importantly in the early production of testing). But one of the most troubling aspects is the constant sense of  <a href="https://www.nytimes.com/2020/04/03/us/politics/coronavirus-trump-medical-advisers.html">“daylight”</a>  and <a href="https://www.politico.com/news/2020/02/26/trump-backers-coronavirus-conspiracy-117781">distrust</a> between the current U.S. administration and its own medical experts. Moreover, the opinions of people such as  <a href="https://www.newyorker.com/news/q-and-a/the-contrarian-coronavirus-theory-that-informed-the-trump-administration">law professor Richard Epstein</a> are listened to even when they are far out of their depth. It is one thing to entertain the opinion of non-expert contrarians when we have all the time in the world to debate, discuss and debunk. It’s quite another to do so in the context of a fast-moving health emergency.  COVID-19 is an emergency that has medical, social, economical, and technological aspects, but it would best be addressed if each person contributes according to their skill set and collaborates with people of complementary backgrounds.</p>



<p/>



<p/></div>
    </content>
    <updated>2020-04-04T16:54:24Z</updated>
    <published>2020-04-04T16:54:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-04-11T04:21:15Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-6367105878835308659</id>
    <link href="http://processalgebra.blogspot.com/feeds/6367105878835308659/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=6367105878835308659" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6367105878835308659" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6367105878835308659" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/04/an-interview-with-nancy-lynch-and.html" rel="alternate" type="text/html"/>
    <title>An interview with Nancy Lynch and Roberto Segala, CONCUR Test-of-Time Award recipients</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post is devoted to the second interview with the <a href="https://concur2020.forsyte.at/test-of-time/index.html">colleagues</a> who were selected for the first edition of the CONCUR  Test-of-Time Award. (See <a href="https://processalgebra.blogspot.com/2020/04/an-interview-with-davide-sangiorgi.html">here</a> for the interview with Davide Sangiorgi.) I asked <a href="https://people.csail.mit.edu/lynch/">Nancy Lynch</a> (MIT, USA) and <a href="http://profs.sci.univr.it/~segala/">Roberto Segala</a> (University of Verona, Italy) a few questions via email and I copy their answers below. Let  me thank Nancy and Roberto for their answers. I trust that readers of this blog will find them interesting and inspiring.<br/><br/>Luca:  You receive one of the two CONCUR ToT Awards for the period 1992-1995 for your paper "<a href="https://groups.csail.mit.edu/tds/papers/Segala/CONCUR94.pdf">Probabilistic simulations for probabilistic processes</a>", presented at CONCUR 1994. Could you tell us briefly what spurred you to develop the “simple” probabilistic automaton model introduced in that paper and how the ideas underlying that notion came about?<br/><br/>Roberto: We were studying randomized distributed algorithms and we noted that several algorithms suffered from subtle errors. We were looking for a formal, rigorous way to study the correctness of those algorithms and in particular we were trying to extend the hierarchical approach used for <a href="https://en.wikipedia.org/wiki/Input/output_automaton">I/O automata</a>.  We knew that there was already an extensive literature within concurrency theory, but we were not able to use the theory to express even the simple idea that you may choose internally between flipping a fair or a biased coin; the existing approaches were extending labeled transition systems by adding probabilities to the arcs, losing the ability to distinguish between probabilistic and nondeterministic choice. The fact that the axioms for nondeterministic choice were not valid in most probabilistic calculi was further evidence of the problem. Starting from the observation that in the non-probabilistic world a step of a distributed algorithm corresponds to a transition of an I/O automaton, we tried to extend the notion of transition so that the process of flipping a coin could be represented by the "new" entity.  This led to the idea of "replacing points with measures" within ordinary automata and/or ordinary labeled transition systems. The idea worked very well for our analysis of randomized distributed algorithms, but we also wanted to validate it by checking whether it would simplify the concurrency-theoretical approach to probability. We decided to use simulation relations for our test, and indeed it worked and led to the CONCUR 1994 paper.<br/><br/>Nancy: For background for this paper, we should recall that, at the time,  there was a divergence in styles of modeling for distributed systems, between different concurrency theory research communities.   The style I had been using since the late 70s for modeling distributed algorithms involved interactive state machines, such as I/O automata, which are based on a set-theoretic foundation.  Such models are good for describing distributed algorithms, and for analyzing them for both correctness and costs.  On the other hand, the predominant style in the European concurrency community was more syntactic, based on logical languages like PCTL or various process algebras.  These were good for formal verification, and for studying expressive power of different formalisms, but not great for analyzing complicated distributed algorithms.  Our models were in the general spirit of the Labeled Transition System (LTS) models previously studied in Europe.  When Roberto came to MIT, he and I set about to extend prior modeling work for distributed systems to the probabilistic setting.  To do this, we considered both the set-theoretic and logical approaches.  We   needed to bridge the gap between them, which led us to the ideas in this paper.<br/><br/>Luca:  How much of your later work has built on your CONCUR 1994 paper? What follow-up results of yours are you most proud of and why?<br/><br/>Roberto: Besides using the results of the paper for the analysis of several randomized distributed algorithms, we worked jointly as well as independently on the study of the theory and on the analysis of security protocols. In collaboration with <a href="http://www.cs.ru.nl/~fvaan/">Frits Vaandrager</a>, we were able to discover different ways to analyze security in a hierarchical and compositional way. Furthermore, since in simple probabilistic automata probability and nondeterminism are well separated, it was easy to include computational complexity into the analysis of security protocols. This is work I did in collaboration with Andrea Turrini. The clear separation between probability and nondeterminism turned out to extend our approach to real-time models, leading to notions like Probabilistic Timed Automata and to several of the elements behind the first sketches of the probabilistic model checker <a href="https://www.prismmodelchecker.org/">PRISM</a> in collaboration with the group of <a href="https://www.cs.ox.ac.uk/people/marta.kwiatkowska/">Marta Kwiatkowska</a>.<br/><br/>Nancy: Roberto and I continued working on probabilistic models, in the set-theoretic style, for several years.  As Roberto notes, Frits Vaandrager also became  a major collaborator.  Some of our more recent papers following this direction are:<br/><ul><li><a href="https://www.sws.cs.ru.nl/publications/papers/fvaan/CPA.html">Compositionality for Probabilistic Automata</a>, CONCUR 2003, </li><li><a href="https://www.sws.cs.ru.nl/publications/papers/fvaan/LSVsiam/">Observing Branching Structure through Probabilistic Contexts</a>, SIAM  Journal of Computing 2007, </li><li><a href="https://groups.csail.mit.edu/tds/papers/Kirli/dilsun-icalp-tr.pdf">Task-Structured Probabilistic I/O Automata</a>, JCSS 2018.  </li></ul>This last  paper provides a kind of compositional foundation for combining  security protocols.  The key idea was to weaken the power of the adversary, making it more "oblivious".  In other related work, my students and I have worked extensively on  probabilistic distributed algorithms since then.  The models are similar to those developed in this early paper.  Examples include  wireless network algorithms, and more recently, biologically-inspired  algorithms, such as insect colony algorithms and neural network   algorithms.  I can't pinpoint a specific result that relies heavily on  the 1994 paper, but that paper certainly provided inspiration and foundation for the  later work.<br/><br/>Luca:  Did you imagine at the time that the CONCUR 1994 paper and its journal version would have so much impact? In your opinion, what is the most interesting or unexpected use in the literature of the notions and techniques you developed in your award-winning paper?<br/><br/>Roberto: We knew that the change of view proposed in the paper would have simplified the study and extension of several other concepts within concurrency theory, but we did not have any specific expectations. The collaboration on the PRISM project and on Probabilistic Timed Automata made more evident that there was a connection with Markov Decision Processes, which lead to a fruitful cross-fertilization between artificial intelligence, model checking, and concurrency theory.  It was a long exchange of ideas between a world interested in existential quantification, that is, find an optimal policy to achieve a goal, and universal quantification, that is to make sure that under any scheduler, policy, adversary, a system behaves correctly. The consequences of such exchange were many and unexpected.<br/><br/>Nancy: No, I did not anticipate that it would have broad impact, though of course I thought the ideas were important.  But in general, I am  bad at predicting what will appeal to others and inspire them to further work.<br/><br/>Luca:  The journal version of your paper appears in the Nordic Journal on Computing, which is not a prime venue. Did you ever regret not publishing that work somewhere else?  What is your take on the trend of using the perceived quality of a publication venue to evaluate research quality?<br/><br/>Roberto: Within semantics I know of a few technical reports that are much more influential than most journal papers; I do not think that the venue of a paper should be given much importance. On the other hand, venue as an evaluation tool is more objective, allows an evaluator to consider many papers without reading them, and protects the evaluator from any type of claim against decisions, which sometimes may have even legal consequences.  Needless to say that I do not agree with any of the ideas above.  I do not agree as well with the other emerging trend of counting papers to evaluate quality. One good paper is much better than ten mediocre papers. What worries me the most is that young researchers may have a hard time to concentrate on quality if they have to focus on venue and quantity. I feel I was lucky not to have to worry about these issues when we submitted our paper for the special issue of the Nordic Journal of Computing.<br/><br/>Nancy:   I certainly never regretted the venue for this paper.  In general, I haven't paid too much attention to choice of publication venue.  The main thing is to reach the audience you want to reach, which can be done through prestigious journals, less prestigious journals, conferences, or even ArXiv technical reports and some publicity.  It’s good to get feedback from referees, though. For evaluations, say for hiring or promotion, I think it’s fair to take many factors into account in evaluating research quality.  Venues, number of citations, special factors about different fields,… all can be discussed by hiring and promotion committees.  But I hope that those committees also take the time to actually read and consider the work itself.<br/><br/>Luca:  The last thirty years have seen a huge amount of work on probabilistic models of computation and on the development of proof techniques and tools based on them. What advice would you give to a young researcher interested in working on probabilistic models in concurrency today?<br/><br/>Roberto: My first advice would be to keep things simple and to focus on key ideas, possibly under the guidance of multiple application scenarios. When we study probabilistic concurrency, especially in contexts where the external world is governed by non-discrete laws, the interplay between different features of the model may become overwhelmingly complex. Of course it is a nice exercise to dive into all the details and see what it leads to, or to extend known results to the new scenarios, but how about checking whether some simple changes to one of the building blocks could make life easier?  Unfortunately, working on building blocks is risky. So, my second advice is to dive into details and write papers, but take some breaks and think whether there may be nicer ways to solve the same problems.<br/><br/>Nancy: Pick some particular application domain, and make sure that the models  and techniques you develop work well for that application.  Don't work  on the theory in a vacuum.  The models will turn out much better!  Perhaps simpler, as Roberto says.<br/><br/>Luca:  What are the research topics that currently excite you the most?<br/><br/>Roberto: Difficult question. There are many things I like to look at, but at the moment I am very curious about how quantum computing can fit in a nice and elementary way into a concurrency framework.<br/><br/>Nancy: Probabilistic algorithms, especially those that are flexible (work in  different environments), robust to failures and noise, and adaptive to  changes.  This includes such topics as wireless network algorithms,   robot swarms, and biologically-inspired algorithms.<br/><br/><b>Acknowledgements:</b> Many thanks to <a href="http://www-sop.inria.fr/members/Ilaria.Castellani/">Ilaria Castellani</a>, who pointed out some typos in the original version of this text.  </div>
    </content>
    <updated>2020-04-03T15:22:00Z</updated>
    <published>2020-04-03T15:22:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-04-09T10:14:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2445</id>
    <link href="https://francisbach.com/computer-aided-analyses/" rel="alternate" type="text/html"/>
    <title>Computer-aided analyses in optimization</title>
    <summary>In this blog post, I want to illustrate how computers can be great allies in designing (and verifying) convergence proofs for first-order optimization methods. This task can be daunting, and highly non-trivial, but nevertheless usually unavoidable when performing complexity analyses. A notable example is probably the convergence analysis of the stochastic average gradient (SAG) [1],...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">In this blog post, I want to illustrate how computers can be great allies in designing (and verifying) convergence proofs for first-order optimization methods. This task can be daunting, and highly non-trivial, but nevertheless usually unavoidable when performing complexity analyses. A notable example is probably the convergence analysis of the stochastic average gradient (SAG) [<a href="https://arxiv.org/pdf/1309.2388.pdf">1</a>], whose original proof was computer assisted.</p>



<p class="justify-text">To this end, we will mostly spend time on what is referred to as <em>performance estimation problems</em> (PEPs), introduced by Yoel Drori and Marc Teboulle [<a href="https://link.springer.com/article/10.1007/s10107-013-0653-0">2</a>]. Performance estimation is also closely related to the topic of <em>integral quadratic constraints</em> (IQCs), introduced in the context of optimization by Laurent Lessard, Benjamin Recht and Andrew Packard [<a href="https://epubs.siam.org/doi/abs/10.1137/15M1009597">3</a>]. In terms of presentations, IQCs  leverages control theory, whereas PEPs might seem more natural in the optimization community. This blog post essentially presents PEPs from the point of view of [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>], instantiated on a running example.</p>



<h2>Overview, motivations</h2>



<p class="justify-text">First-order methods for continuous optimization belong to the large panel of algorithms that are usually approached via worst-case analyses. In this context, analyses rely on combining inequalities (that are due to assumptions on the problem classes), in potentially long, non-intuitive, and technical, proofs. For the insiders, those proofs all look very similar. For the outsiders, those proofs all look rather repelling, technical (long pages of chained inequalities), probably not interesting, and like computer codes: usually intuitive mostly for their authors.</p>



<p class="justify-text">In what follows, I want to show how (and why) those proofs are indeed all very similar. On the way, I want to emphasize how those combinations of inequalities are related to the “true essence” of worst-case analyses (which rely on computing worst-case scenarios), and to provide examples on how to constructively obtain them.</p>



<p class="justify-text">We take the stand of illustrating the PEP approach on a single iteration of gradient descent, as it essentially contains all necessary ingredients to understand the methodology in other contexts as well. Certain details of the following text are (probably unavoidably) a bit technical. However, going through the detailed computations is not essential, and the text should contain the necessary ingredients for understanding the essence of the methodology.</p>



<h2>Running example: gradient descent</h2>



<p class="justify-text">Let us consider a naive, but standard, example: unconstrained convex minimization $$x_\star= \underset{x\in\mathbb{R}^d}{\mathrm{arg min}} f(x)$$with gradient descent: \(x_{k+1}=x_k-\gamma \nabla f(x_k)\). Let us assume \(f(\cdot)\) to be continuously differentiable, to have a \(L\)-Lipschitz gradient (a.k.a., \(L\)-smoothness), and to be \(\mu\)-strongly convex. Those functions satisfy, for all \(x,y\in\mathbb{R}^d\):<br/>– strong convexity, see e.g., [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>, Definition 2.1.2]: $$\tag{1}f(x) \geqslant      f(y)+\langle{\nabla f(y)}; {x-y}\rangle+\tfrac{\mu}{2} \lVert x-y\rVert^2,$$- smoothness, see e.g., [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>, Theorem 2.1.5]: $$\tag{2} f(x) \leqslant      f(y)+\langle{\nabla f(y)}; {x-y}\rangle+\tfrac{L}{2}\lVert x-y\rVert^2.$$Let us recall that in the case of twice continuously differentiable functions, smoothness and strong convexity amount to requiring  that $$\mu I \preccurlyeq \nabla^2 f(x) \preccurlyeq L I,$$ for some \(0&lt; \mu&lt;L&lt; \infty\) and for all \(x\in\mathbb{R}^d\) (in other words, all eigenvalues of \(\nabla^2 f(x)\) are between \(\mu\) and \(L\)). In what follows, we denote by \(\mathcal{F}_{\mu,L}\) the class of \(L\)-smooth \(\mu\)-strongly convex functions (irrespective of the dimension \(d\)).</p>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="204" src="https://www.di.ens.fr/~ataylor/BlogPost/SmoothStronglyConvex.png" width="473"/>Figure 1: the blue function is \(L\)-smooth and \(\mu\)-strongly convex (it is possible to create respectively global upper and lower quadratic bounds from every \(x\in\mathbb{R}^d\) with respectively curvatures \(L\) and \(\mu\)).</figure></div>
</div></div>



<p class="justify-text">In this context, convergence of gradient descent can be studied in many ways. Here, for the sake of the example, we will do it in terms of two base quantities: distance to optimality \(\lVert x_k-x_\star\rVert\), and function value accuracy \(f(x_k)-f(x_\star)\). There are, of course, infinitely many other possibilities, such as gradient norm \(\rVert \nabla f(x_k)\lVert\), Bregman divergence \(f(x_\star)-f(x_k)-\langle{\nabla f(x_k)};{x_\star-x_k}\rangle\), or even best function value observed throughout the iterations \(\min_{0\leq i\leq k} \{f(x_i)-f(x_\star)\}\): the reader can adapt the lines below for his/her favorite criterion. </p>



<p class="justify-text">For later reference, let us provide another inequality that is known to  hold for all \(x,y\in\mathbb{R}^d\) for any \(L\)-smooth \( \mu\)-strongly convex function: <br/>– bound on inner product, see, e.g., [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>,  Theorem 2.1.11]: $$\langle{\nabla f(x)-\nabla f(y)};{x-y}\rangle  \geqslant        \tfrac{1}{L+\mu} \lVert{\nabla f(x)-\nabla f(y)}\rVert^2+\tfrac{\mu  L}{L+\mu}\lVert{x-y}\rVert^2.\tag{3}$$ In the case \(\mu=0\) this inequality is known as “cocoercivity”. This (perhaps mysterious) inequality happens to play an important role in convergence proofs.</p>



<h3>A standard convergence result</h3>



<p class="justify-text">Let us start by stating two known results along with their simple proofs (see, e.g.,  [<a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">5</a>, Theorem 2.1.14] or [6, Section 1.4.2, Theorems 2 &amp; 3]):<br/>– convergence in distance:  $$\begin{array}{rl}    \rVert{x_{k+1}-x_\star}\lVert^2&amp;= \lVert{x_k-x_\star}\rVert^2+\gamma^2\lVert{\nabla f(x_k)}\rVert-2\gamma\langle{\nabla f(x_k)};{x_k-x_\star}\rangle \\ \     &amp; \leqslant      \left(1-\tfrac{2\gamma L \mu}{L+\mu}\right)\lVert{x_k-x_\star}\rVert^2+\gamma\left(\gamma-\tfrac2{L+\mu}\right)\lVert{\nabla f(x_k)}\rVert^2, \end{array} $$ where the second line follows from smoothness and strong convexity of \(f\) via the bound (3) on the inner product (with \(x=x_k\) and \(y=x_\star\)). For the particular choice \(\gamma=\tfrac2{L+\mu}\), the second term on the right hand side disappears, and we end up with<br/>     $$\lVert{x_{k+1}-x_\star}\rVert^2 \leqslant      \left(\tfrac{L-\mu}{L+\mu}\right)^2\lVert{x_k-x_\star}\rVert^2,$$ which, following from \(0&lt;\mu&lt;L&lt;\infty\), satisfies \(0&lt; \tfrac{L-\mu}{L+\mu}&lt;1\), hence proving linear convergence of gradient descent in this setup, by recursively applying the previous inequality: $$ \lVert{x_{k}-x_\star}\rVert^2 \leqslant      \left(\tfrac{L-\mu}{L+\mu}\right)^{2k}\lVert{x_0-x_\star}\rVert^2.$$ – Convergence in function values: one can simply use the result in distance along with the previous basic inequalities (1) and (2) characterizing smoothness and strong convexity (both with \(y=x_\star\)):<br/>     $$f(x_k)-f(x_\star) \leqslant \hspace{-.15cm}\tfrac{L}{2}\hspace{-.1cm}\rVert{x_k-x_\star}\lVert^2  \leqslant  \hspace{-.15cm}    \tfrac{L}{2}\hspace{-.1cm}\left(\tfrac{L-\mu}{L+\mu}\right)^{\hspace{-.1cm}2k} \rVert{x_0-x_\star}\lVert^2 \leqslant  \hspace{-.15cm}     \tfrac{L}{\mu}\hspace{-.1cm} \left(\tfrac{L-\mu}{L+\mu}\right)^{\hspace{-.1cm}2k}(f(x_0)-f(x_\star)).$$  It is also possible to directly look for convergence in terms of function values, but it is then usually unclear in the literature what inequalities to use, and I am not aware of any such proof leading to the same rate without the leading \(\tfrac{L}{\mu}\) (except the proof presented below).</p>



<p class="justify-text">At this point, even in this toy example, a few very legitimate questions can be raised:<br/>– can we improve anything? Can gradient descent really behaves like that on this class of functions?<br/>– How could we have guessed the inequality to use, and the shape of the corresponding proof? Obviously, the obscure fact is to arrive to inequality (1).  Therefore, is there a principled way for choosing the right inequalities to use, for example for studying convergence in terms of other quantities, such as  function values?<br/>– Is this the unique way to arrive to the desired result? If yes, how likely are we to find such proofs for more complicated cases (algorithms and/or function class)?</p>



<p class="justify-text">For the specific step size choice \(\gamma=\tfrac2{L+\mu}\), a partial answer to the first question is obtained by the observation that the rate is actually achieved on the quadratic function<br/> $$f(x)=\tfrac12 \, x^\top \begin{bmatrix}<br/> L &amp; 0\\ 0 &amp; \mu<br/> \end{bmatrix}x.$$ The following lines precisely target the missing answers.</p>



<h2>Worst-case analysis through worst-case scenarios</h2>



<p class="justify-text">Let us start by rephrasing our goal, and restrict ourselves to the study of a single iteration. We fix our target to finding the smallest possible value of \(\rho\) such that the inequality<br/> $$ \lVert{x_{k+1}-x_\star}\rVert^2  \leqslant       \rho^2 \lVert{x_k-x_\star}\rVert^2 $$ is valid for all \(x_k\) and \(x_{k+1}=x_k-\gamma \nabla f(x_k)\) (hence \(\rho\) is a function of \(\gamma\)). In other words, our goal is to solve<br/>$$ \rho^2(\gamma):= \sup \left\{ \frac{\lVert{x_{k+1}-x_\star}\rVert^2}{\lVert{x_k-x_\star}\rVert^2}\, \big|\, f\in\mathcal{F}_{\mu,L},\, x_{k+1}=x_k-\gamma \nabla f(x_k),\, \nabla f(x_\star)=0\right\}.$$<br/>Alternatively, we could be interested in studying convergence in other forms: for function values, we could target to solve the slightly modified problem:<br/> $$ \sup  \left\{ \frac{f(x_{k+1})-f(x_\star)}{f(x_{k})-f(x_\star)}\, \big|\, f\in\mathcal{F}_{\mu,L},\, x_{k+1}=x_k-\gamma \nabla f(x_k),\, \nabla f(x_\star)=0 \right\}.$$ It turns out that in both cases, the problem can be solved both numerically to high precision, and analytically, and that the answer is \(\rho^2(\gamma)=\max\{(1-\mu\gamma)^2,(1-L\gamma)^2\}\).</p>



<p class="justify-text">The only thing we did, so far, was to explicitly reformulate the problem of finding the best (smallest) convergence rate as the problem of finding the worst-case scenario, nothing more. In what follows, some parts might become slightly technical, but the overall idea is only to reformulate this problem of finding the worst-case scenarios, for solving it.</p>



<h3>Dealing with an infinite-dimensional variable: the function \(f\)</h3>



<p class="justify-text">The first observation is that the problem of computing \(\rho\) is stated as an infinite-dimensional optimization problem: we are looking for the worst possible problem instance (a function \(f\) and an initial point \(x_k\)) within a predefined class of problems. The first step we take to work around this is to reformulate it in the following equivalent  form (note that we maximize also over the dimension \(d\)—we discuss later how to remove it):<br/>$$\begin{array}{rl} \rho^2:= \underset{f,\, x_k,\,x_\star,\, g_k,\,d}{\sup} &amp;\displaystyle \frac{\lVert{x_{k}-\gamma g_k-x_\star}\rVert^2}{\lVert{x_k-x_\star}\rVert^2}\\<br/> \text{s.t. }    &amp; \exists f\in\mathcal{F}_{\mu,L}:\, g_k= \nabla f(x_k),\, 0=\nabla f(x_\star).<br/>\end{array}$$<br/>This problem intrinsically does not look better (it contains an  existence constraint), but it allows using mathematical tools which are referred to as  <em>interpolation,</em> or <em>extension</em>, theorems [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, <a href="https://arxiv.org/pdf/1603.00241.pdf">7</a>, <a href="https://hal.archives-ouvertes.fr/hal-01530908/file/DHLL_appendix.pdf">8</a>]. The problem is depicted on Figure 2:</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="209" src="https://www.di.ens.fr/~ataylor/BlogPost/Interpolation.png" width="490"/>Figure 2: discrete interpolation (or extension) problem: given a set of triplets \(\{(\text{coordinate}, \text{gradient}, \text{function value})\}\) can we recover a function within a determined class that explains those triplets?</figure></div>



<p class="justify-text">It turns out that convex interpolation (that is, neglecting smoothness and strong convexity) is actually rather simple:</p>



<ul class="justify-text"><li>given a convex function and an index set \(I\), any set of samples \(\{(x_i,g_i,f_i)\}_{i\in I}\) of the form \(\{(\text{coordinate}, \text{(sub)gradient}, \text{function value})\}\)) satisfies, for all \(i,j\in I\): $$f_i \geqslant      f_j+\langle g_j; x_i-x_j\rangle,$$ by definition of subgradient, as illustrated on Figure 3.</li></ul>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" height="205" src="https://www.di.ens.fr/~ataylor/BlogPost/SamplingCvx.png" width="485"/>Figure 3: sampling from a convex function.</figure></div>



<ul class="justify-text"><li>In the other direction, given a set of triplets \(\{(x_i,g_i,f_i)\}_{i\in I}\) satisfying the previous inequality for all pairs \(i,j\in I\), one can simply recover a  convex function by the following construction: $$f(x)=\underset{i\in I}{\max}\{ f_i+\langle g_i;x-x_i\rangle\},$$ which is depicted on Figure 4.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="207" src="https://www.di.ens.fr/~ataylor/BlogPost/InterpolateCvx.png" width="487"/>Figure 4: some set \(\{(x_i,g_i,f_i)\}_{i\in I}\) and its piecewise affine interpolant \(f(x)=\underset{i\in I}{\max}\{ f_i+\langle g_i;x-x_i\rangle\}\).</figure></div>



<ul class="justify-text"><li>Formally, the reasoning allows arriving to the following “convex interpolation” (or “convex extension”) result, where we denote the set of (closed, proper) convex functions by \(\mathcal{F}_{0,\infty}\) (to be understood as \(L\)-smooth \(\mu\)-strongly convex functions with \(\mu=0\) and \(L=\infty\)): $$\begin{array}{c}\exists f\in\mathcal{F}_{0,\infty}: \,  g_k\in\partial f(x_k) \text{ and } f_k=f(x_k) \ \ \forall k\in  I\\ \Leftrightarrow\\ f_i \geqslant       f_j+\langle{g_j};{x_i-x_j}\rangle\quad \forall i,j\in I,\end{array}$$ where \(\partial f(x)\) denotes the subdifferential of \(f\) at \(x\).</li></ul>



<p class="justify-text">In the next section, we use a similar interpolation result for taking smoothness and strong convexity into account. The result is a bit more technical, but follows from similar constructions as those for convex interpolation—the main difference being that the interpolation is done on the Fenchel conjugate instead, in order to incorporate smoothness, see [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, Section 2].</p>



<h3>Reformulation through convex interpolation</h3>



<p class="justify-text">Back to the problem of computing worst-case scenarios, we can now reformulate the existence constraint <em>exactly</em> using the following result (see [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>,  Theorem 4]): let \(I\) be a finite index set and let \( S=\{(x_i,g_i,f_i)\}_{i\in I}\) be a set of triplets, then<br/>  $$\begin{array}{c}\exists f\in\mathcal{F}_{\mu,L}: \,  g_i=\nabla f(x_i) \text{ and } f_i=f(x_i) \text{ for all } i\in  I\\ \Leftrightarrow\\  f_i \geqslant      f_j+\langle{g_j};{x_i-x_j}\rangle+\frac{1}{2L}\lVert{g_i-g_j}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_i-x_j-\frac{{1}}{L}(g_i-g_j)}\rVert^2  \,\,\, \forall i,j\in I.\end{array}$$ Therefore, the previous problem can be reformulated as (recalling that \(g_\star=0\))<br/>$$  \begin{array}{rl} \underset{{f_k,\,f_\star,\, x_k,\,x_\star,\, g_k,\,d}} {\sup}&amp;\displaystyle\frac{\rVert{x_{k}-\gamma  g_k-x_\star}\lVert^2}{\rVert{x_k-x_\star}\lVert^2}\\<br/>      \text{s.t. }    &amp; f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2\\<br/>      &amp; f_k \geqslant      f_\star+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2.<br/>\end{array}$$ </p>



<h3>Quadratic reformulation</h3>



<p class="justify-text">The next step is to remove the ratio appearing in the objective function, which we do via an homogeneity argument, as follows.</p>



<p class="justify-text">Starting from a feasible point, scale \(x_k,\,x_\star,g_k\) by some \(\alpha&gt;0\) and \(f_k,\,f_\star\) by \(\alpha^2\) and observe it does not change the value of the objective, while still being a feasible point. Therefore, the problem can be reformulated as a nonconvex QCQP (quadratically constrained quadratic program): $$  \begin{array}{rl} \underset{{f_k,\,f_\star,\, x_k,\,x_\star,\, g_k,\,d}} {\sup}&amp;\displaystyle{\rVert{x_{k}-\gamma  g_k-x_\star}\lVert^2}\\<br/>       \text{s.t. }    &amp; f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2\\<br/>       &amp; f_k \geqslant      f_\star+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2\\ &amp;{\rVert{x_k-x_\star}\lVert^2} \leqslant      1,<br/> \end{array}$$  which is quadratic in \(x_k\), \(x_\star\) and  \(g_k\), and linear in \(f_\star\) and \(f_k\). Actually, in the current form, nonconvexity comes from the term “\(\langle{g_k};{x_\star-x_k}\rangle\)” in the second constraint (and from the objective, due to maximization). It turns out that this problem can be reformulated <em>losslessly</em> using semidefinite programming (this is due to the maximization over \(d\), as commented at the end of the next section). </p>



<h3>Semidefinite reformulation</h3>



<p class="justify-text">At the end of this section, we will be able to compute, numerically, the values of the rate \(\rho^2(\gamma)\) for given values of the parameters \(\mu,\,L\), and \(\gamma\).</p>



<p class="justify-text">The last step in the reformulation goes as follows: the previous problem can be reformulated as a semidefinite program, as it is linear in terms of the entries of the following Gram matrix<br/>$$G = \begin{pmatrix}<br/>     \lVert{x_k-x_\star}\rVert^2 &amp; \langle{g_k};{x_k-x_\star}\rangle \\ \langle{g_k};{x_k-x_\star}\rangle &amp; \lVert{g_k}\rVert^2<br/>     \end{pmatrix}\succcurlyeq 0,$$ and in terms of the function values \(f_k\) and \(f_\star\). From those variables, one reformulate the previous problem as $$\begin{array}{rl} \underset{f_k,\,f_\star,\, G\succeq 0}{\sup} \, &amp;{\mathrm{Tr} (A_\text{num} G)}\\<br/>      \text{s.t. }    &amp; f_k-f_\star+\mathrm{Tr} (A_1 G)\leqslant 0\\<br/>      &amp;  f_\star-f_k+\mathrm{Tr} (A_2 G)\leqslant 0 \\<br/>      &amp;\mathrm{Tr} (A_\text{denom} G) \leqslant      1,\end{array}$$ which is a gentle semidefinite program where we picked matrices \(A_{\text{num}}\), \(A_{\text{denom}}\), \(A_1\) and \(A_2\) for encoding the previous terms. That is, we choose those matrices such that<br/> $$\begin{array}{rl}<br/>\mathrm{Tr}(A_{\text{denom}} G)&amp;=\lVert{x_k-x_\star}\rVert^2,\\  \mathrm{Tr}(A_{\text{num}} G)&amp;=\lVert{x_k-\gamma g_k-x_\star}\rVert^2,\\ \mathrm{Tr}(A_1G)&amp;=\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2,\\ \mathrm{Tr}(A_2G)&amp;=\tfrac{1}{2L}\lVert g_k\rVert^2+\tfrac{\mu}{2(1-\mu/L)}\lVert x_k-x_\star-\tfrac1L g_k\rVert^2.\end{array}$$ One possibility is to choose \(A_{\text{num}}\), \(A_{\text{denom}}\), \(A_1\) and \(A_2\) as symmetric matrices, as follows: $$\begin{array}{cc}<br/> A_{\text{denom}}=\begin{pmatrix}     1 &amp; 0\\ 0 &amp; 0     \end{pmatrix}, &amp; A_{\text{num}}=\begin{pmatrix}     1 &amp; -\gamma\\ -\gamma &amp; \gamma^2     \end{pmatrix}, \\ A_1=\begin{pmatrix}\tfrac{\mu}{2(1-\mu/L)} &amp; -\tfrac12-\tfrac{\mu}{2(L-\mu)} \\ -\tfrac12-\tfrac{\mu}{2(L-\mu)} &amp; \tfrac{1}{2L}+\tfrac{\mu}{2L(L-\mu)}\end{pmatrix}, &amp;  A_2=\begin{pmatrix}\tfrac{\mu}{2(1-\mu/L)} &amp; -\tfrac{\mu}{2(L-\mu)} \\ -\tfrac{\mu}{2(L-\mu)} &amp; \tfrac{1}{2L}+\tfrac{\mu}{2L(L-\mu)} \end{pmatrix}.\end{array}$$</p>



<p class="justify-text">All those steps can be carried out in the exact same way for the problem of computing the convergence rate for function values, reaching a similar problem with \(6\) inequality constraints instead—because interpolation conditions have to be imposed on all pairs of points in a set of \(3\) points: \(x_k\), \(x_{k+1}\) and \(x_\star\), instead of only \(2\) for the distance problem. The objective function is then \(f_{k+1}-f_\star\), the de-homogenization constraint (arising from the denominator of the objective function) is \(f_{k}-f_\star \leqslant     1\), and the Gram matrix is \(3\times 3\):<br/>$$G=\begin{pmatrix}<br/>     \lVert{x_k-x_\star}\rVert^2 &amp; \langle{g_k};{x_k-x_\star}\rangle&amp; \langle{g_{k+1}};{x_k-x_\star}\rangle\\ \langle{g_k};{x_k-x_\star}\rangle &amp; \lVert{g_k}\rVert^2 &amp; \langle{g_k};{g_{k+1}}\rangle \\<br/>     \langle{g_{k+1}};{x_k-x_\star}\rangle &amp; \langle{g_{k+1}};{g_k}\rangle &amp; \lVert{g_{k+1}}\rVert^2<br/>     \end{pmatrix}\succcurlyeq 0, $$ and the function values variables are \(f_k\), \(f_{k+1}\) and \(f_\star\).</p>



<p class="justify-text">We provide the numerical optimal values of those semidefinite programs on Figure 5 for both convergence in distances and in function values. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="324" src="https://www.di.ens.fr/~ataylor/BlogPost/ObjectiveValues.png" width="534"/>Figure 5: worst-cases of the ratio \(\frac{\lVert{x_{k+1}-x_\star}\rVert^2}{\lVert{x_k-x_\star}\rVert^2}\) (red) and \(\frac{f(x_{k+1})-f_\star}{f(x_k)-f_\star}\) (dashed blue) as functions of the step size \(\gamma\), for the case where \(f\) is \(1\)-smooth and \(0.1\)-strongly convex. The results match exactly the expected \(\max\{(1-\gamma L)^2,(1-\gamma\mu)^2\}\) in both cases. Note that the corresponding SDPs can be solved both for “good and bad” choices of step sizes: if the step size is chosen wisely then \(\rho(\gamma)&lt;1\), and otherwise \(\rho(\gamma)\geqslant 1\). The SDP confirms the common knowledge that \(\gamma\in (0,2/L)\Rightarrow \rho(\gamma)&lt; 1\). Numerical values obtained through YALMIP [<a href="https://yalmip.github.io/">9</a>] and Mosek [<a href="https://www.mosek.com/">10</a>].</figure></div>



<p class="justify-text">As a conclusion for this section, let us note that we showed how to compute the “best” rates that are dimension independent. In general, requiring the iterates and gradient (e.g., \(x_k\) and \(g_k\) for the problem in terms of distance, and \(x_k\), \(g_k\) and \(g_{k+1}\) for function values, and potentially more vectors when dealing with more complex settings) to lie in \(\mathbb{R}^d\) is equivalent to adding a rank constraint in the SDP. </p>



<h2>Duality between worst-case scenarios and combinations of inequalities</h2>



<p class="justify-text">Any feasible point to the previous SDP corresponds to a <em>lower bound</em>: a sampled version of a potentially difficult function for gradient descent. If we want to find <em>upper bounds</em> on the rate, a natural way to proceed is to go to the dual side of the previous SDPs, where any feasible point will naturally correspond to an upper bound on the convergence rate (by <em>weak duality</em>). As the primal problems were SDPs, their Lagrangian duals are SDPs as well. Let us associate one multiplier per constraint: $$ \begin{array}{rl}<br/>f_k-f_\star+\mathrm{Tr} (A_1 G)\leqslant 0&amp;:\lambda_1\\<br/>f_\star-f_k+\mathrm{Tr} (A_2 G)\leqslant 0&amp;:\lambda_2\\<br/>\mathrm{Tr}(A_\text{denom} G) \leqslant 1&amp;: \tau.<br/>\end{array}$$The dual is then<br/>$$\begin{array}{rl}<br/> \underset{\tau,\,\lambda_1,\,\lambda_2}{\min} &amp; \, \tau \\<br/> \text{s.t. } &amp; \lambda_1=\lambda_2,\\<br/> &amp; S:=A_\text{num}-\tau A_\text{denom}-\lambda_1A_1-\lambda_2A_2 \preccurlyeq 0,\\<br/> &amp;\tau,\lambda_1,\lambda_2 \geqslant  0.<br/> \end{array}$$ Hence, by weak duality, any feasible point to this last SDP corresponds to an upper bound on the rate: \(\tau \geqslant \rho^2\). A mere rephrasing of weak duality can be obtained through the following reasoning: assume we received some feasible \(\tau,\lambda_1,\lambda_2\) (and hence \(\lambda_1=\lambda_2\) and a corresponding \(S\preccurlyeq 0\)), we then get, for any primal feasible \(G\succcurlyeq0\):<br/>$$\begin{array}{rl}\mathrm{Tr}(SG)&amp;=\mathrm{Tr}(A_{\text{num}}G)-\tau\mathrm{Tr}(A_{\text{denom}}G)-\lambda_1\mathrm{Tr}(A_1G)-\lambda_2\mathrm{Tr}(A_2G)\\&amp;=\lVert x_{k+1}-x_\star\rVert^2-\tau\lVert x_{k}-x_\star\rVert^2\\ \,&amp;\,\,\,-\lambda_1[     f_k-f_\star+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2]\\ &amp;\,\,\,-\lambda_2 [f_\star-f_k+\frac{1}{2L}\lVert{g_k}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}g_k}\rVert^2]\\ &amp;\geqslant \lVert x_{k+1}-x_\star\rVert^2-\tau\lVert x_{k}-x_\star\rVert^2, \end{array}$$ where the first equality follows from the definition of \(S\), the second equality corresponds to the definitions of \(A_{\text{num}}\), \(A_{\text{denom}}\), \(A_1\) and \(A_2\), and the last inequality follows from the sign of the interpolation inequalities (constraints in the primal) for any primal feasible point. Hence, we indeed have that any feasible \(\tau\) corresponds to a valid upper bound on the convergence rate, as $$S\preccurlyeq 0 \,\,\Rightarrow \,\, \mathrm{Tr}(SG)\leqslant 0\,\,\Rightarrow  \lVert x_{k+1}-x_\star\rVert^2-\tau\lVert x_{k}-x_\star\rVert^2\leqslant 0.$$ In order to obtain analytical proofs, we therefore need to find analytical dual feasible points, and numerics can of course help in this process! Let’s look at what the optimal dual solutions look like for our two running examples.</p>



<ul class="justify-text"><li> in Figure 6, we provide the numerical values for \(\lambda_1\) and \(\lambda_2\) for the distance problem.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="316" src="https://www.di.ens.fr/~ataylor/BlogPost/Multipliers_distance.png" width="467"/>Figure 6: numerical values of optimal dual variables: \(\lambda_1\) (red) and \(\lambda_2\) (dashed blue) as functions of the step size \(\gamma\), for the case where \(f\) is \(1\)-smooth and \(0.1\)-strongly convex. The results match \(\lambda_1=\lambda_2=2\gamma \rho(\gamma)\) with \(\rho(\gamma)=\max\{|1-\gamma L|,|1-\gamma\mu|\}\). Numerical values obtained with YALMIP [<a href="https://yalmip.github.io/">9</a>] and Mosek [<a href="https://www.mosek.com/">10</a>].</figure></div>



<ul class="justify-text"><li>For function values, the SDP is slightly more complicated, as more inequalities are involved (6 interpolation inequalities). We provide raw numerical values for the six multipliers in Figure 7.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" height="348" src="https://www.di.ens.fr/~ataylor/BlogPost/Multipliers_function.png" width="463"/>Figure 7: numerical values of optimal dual variables (for the rate in function values): \(\lambda_1,\lambda_2, …,\lambda_6\) as functions of the step size \(\gamma\), for the case where \(f\) is \(1\)-smooth and \(0.1\)-strongly convex. Numerical values obtained with YALMIP [<a href="https://yalmip.github.io/">9</a>] and Mosek [<a href="https://www.mosek.com/">10</a>].</figure></div>



<p>For those who want a bit more details, here are a few additional pointers:</p>



<ul class="justify-text"><li>Strong duality holds—a way to prove it is to show that there exists a Slater point in the primal, see e.g., [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, Theorem 6]—, and hence primal and dual optimal values match.</li><li>There might be different ways to optimaly combine the interpolation inequalities for proving the desired results. In other words: dual optimal solutions are often not unique—which is, in fact, quite a good news: I am sure nobody want to find the analytical version of the multipliers provided in Figure 7.</li><li>It is often possible to simplify the proofs by using fewer, or weaker, inequalities. This might lead to ”cleaner” results, typically (but not always) at the cost of ”weaker” rates. This was done for designing the proof for function values, later in this text.</li></ul>



<h2>Combinations of inequalities: same proofs without SDPs</h2>



<p class="justify-text">So far, we showed that computing convergence rates can be done in a very principled way. To this end, one can solve semidefinite programs—which may have arbitrarily complicated analytical solutions. Here, I want to emphasize that the process of <em>verifying</em> a solution can be quite different to that of<em> finding</em> a solution. Put in other words, although the dual certificates (a.k.a., the proofs) might have been found by solving SDPs, they can be formulated in ways that do not require the reader to know anything about the PEP methodology, nor on any SDP material, for verifying them. This fact might actually not be very surprising to the reader, as many proofs arising in the first-order optimization literature actually “only consists” in linear combinations of (quadratic) inequalities. On the one hand, those proofs can be seen as feasible points to “dual SDPs”, although generally not explicitely proved as such. On the other hand, proofs arising from the SDPs might therefore be expected to be writable without any explicit reference to semidefinite programing and performance estimation problems.<br/></p>



<p class="justify-text">In what follows, we provide the proofs for gradient descent, using the previous numerical inspiration, but without explicitly relying on any semidefinite program. The reader is not expected to verify any of those computations, as our goal is rather to emphasize that the principles underlying both proofs are exactly the same: reformulating linear combinations of inequalities.</p>



<p class="justify-text">For both proofs below, we limit ourselves to the step size regime \(0\leq   \gamma \leq \tfrac{2}{L+\mu}\), and we prove  that, in  this regime, \(\rho(\gamma)=(1-\gamma\mu)\)—actually we only proof the upper bounds, but one can easily verify that they are <em>tight</em> on simple quadratic functions.  The complete proofs (for the proximal gradient method),  can be found in [<a href="https://arxiv.org/pdf/1705.04398.pdf">11</a>]. </p>



<h3>Example 1: distance to optimality</h3>



<p class="justify-text">Recall the notations: \(g_k:=\nabla f(x_k)\), \(f_k:= f(x_k)\), \(g_\star:=\nabla f(x_\star)\), and \(f_\star:= f(x_\star)\).</p>



<p class="justify-text">For distance to optimality, sum the following inequalities with their corresponding weights: $$\begin{array}{r}     f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k-g_\star}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}(g_k-g_\star)}\rVert^2  :\lambda_1,  \\     f_k \geqslant      f_\star+\langle{g_\star};{x_k-x_\star}\rangle+\frac{1}{2L}\lVert{g_k-g_\star}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{{1}}{L}(g_k-g_\star)}\rVert^2:\lambda_2.     \end{array}$$ We use the following values for the multipliers: \(\lambda_1=\lambda_2=2\gamma\rho(\gamma) \geqslant      0\) (see Figure 6). </p>



<p class="justify-text">After appropriate substitutions of \(g_\star\), \(x_{k+1}\), and \(\rho(\gamma)\), using respectively \(g_\star=0\), \(x_{k+1}=x_k-\gamma g_k\) and \(\rho(\gamma)=(1-\gamma\mu)\), and with little effort, one can check that the previous weighted sum of inequalities can be written in the form: $$ \begin{array}{rl}    \lVert{x_{k+1}-x_\star}\rVert^2  \leqslant      &amp; \left(1-\gamma \mu \right)^2\lVert{x_{k}-x_\star}\rVert^2 -\frac{\gamma(2-\gamma (L+\mu))}{L-\mu} \lVert{\mu {(x_k  -x_\star)} – g_k}\rVert^2. \end{array}$$ This statement can be checked simply by expanding both expressions (i.e., the weighted sum and its reformulation) and verifying that all terms indeed match.</p>



<p class="justify-text">Finally, using $$\gamma(2-\gamma (L+\mu)) \geqslant      0,  \text{ and } L-\mu \geqslant      0,$$ which are nonnegative by assumptions on the values of \(L\in(0,\infty)\), \(\mu\in (0,L)\) and \(\gamma\in(0,2/(L+\mu))\), we arrive to the desired $$ \lVert{x_{k+1}-x_\star}\rVert^2 \leqslant      \left(1-\gamma \mu \right)^2\lVert{x_{k}-x_\star}\rVert^2.$$</p>



<p class="justify-text">Note that, by using \(\lambda_1=\lambda_2\), the weighted sum exactly corresponds to the (scaled by a positive constant) inequality introduced in the early stage of this note for studying distance to optimality. However, the resulting expression is tight for all values of the step size here, whereas it was only tight for \(\gamma=2/(L+\mu)\) earlier, due to a different choice of weights! </p>



<p class="justify-text">The curious reader might wonder how to find such a reformulation. Actually, back in terms of SDPs, and using the expressions for the multipliers, it simply corresponds to $$\mathrm{Tr}(SG)=-\frac{\gamma(2-\gamma (L+\mu))}{L-\mu} \lVert{\mu {(x_k  -x_\star)} – g_k}\rVert^2.$$ In the example below, the reformulation is a bit more tricky—as \(\mathrm{Tr}(SG)\)  has two nonnegative terms, which were simply obtained by doing an analytical Cholesky factorization of the term \(\mathrm{Tr}(SG)\)—, but the idea is exactly the same.</p>



<h3>Example 2: function values</h3>



<p class="justify-text">For function values, we combine the following inequalities after multiplication with their respective coefficients:    </p>



<p class="justify-text">$$\scriptsize \begin{array}{lr}     f_k \geqslant      f_{k+1}+\langle{g_{k+1}};{x_k-x_{k+1}}\rangle+\frac{1}{2L}\lVert{g_k-g_{k+1}}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_{k+1}-\frac{1}{L}(g_k-g_{k+1})}\rVert^2    &amp;:\lambda_1,\\<br/>f_\star \geqslant      f_k+\langle{g_k};{x_\star-x_k}\rangle+\frac{1}{2L}\lVert{g_k-g_\star}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_k-x_\star-\frac{1}{L}(g_k-g_\star)}\rVert^2  &amp;:\lambda_2, \\<br/>f_\star \geqslant      f_{k+1}+\langle{g_{k+1}};{x_\star-x_{k+1}}\rangle+\frac{1}{2L}\lVert{g_\star-g_{k+1}}\rVert^2+\frac{\mu}{2(1-\mu/L)}\lVert{x_\star-x_{k+1}-\frac{1}{L}(g_\star-g_{k+1})}\rVert^2  &amp;:\lambda_3.     \end{array}$$ We use the following multipliers \(\lambda_1=\rho(\gamma)\), \(\lambda_2=(1-\rho(\gamma))\rho(\gamma)\), and  \(\lambda_3=1-\rho(\gamma)\) (obtained by greedily trying to set different combinations of multipliers to \(0\) in the SDP—see Figure 7 for the values without such simplifications).</p>



<p class="justify-text">Again, after appropriate substitutions of \(g_\star\), \(x_{k+1}\), and \(\rho(\gamma)\), using respectively \(g_\star=0\), \(x_{k+1}=x_k-\gamma g_k\) and \(\rho(\gamma)=(1-\gamma\mu)\), we obtain that the weighted sum of inequalities can be reformulated exactly as $$  \begin{array}{rl}          f(x_{k+1})-f_\star \leqslant      &amp;\left(1-\gamma \mu\right)^2 \left(f(x_k)-f_\star\right)\\&amp;-\frac{1}{2 (L-\mu)}\lVert \nabla f(x_{k+1})-(1-\gamma  (L+\mu))\nabla f(x_k) +\gamma  \mu  L (x_\star-x_k)\rVert^2\\<br/>&amp;-\frac{\gamma  L(2- \gamma  (L+\mu))}{2 (L-\mu )}\lVert \nabla f(x_k)+\mu  (x_\star-x_k)\rVert^2.\end{array}$$ Again, this statement can be checked simply by expanding both expressions (i.e., the weighted sum and its reformulation), and verifying that all terms match. The desired conclusion $$ f(x_{k+1})-f_\star \leqslant \left(1-\gamma \mu\right)^2 \left(f(x_k)-f_\star\right), $$ follows from the signs of the leading coefficients: \(\gamma(2-\gamma (L+\mu)) \geqslant      0\), and \(L-\mu \geqslant      0\).</p>



<h3>To go further</h3>



<p class="justify-text">Before finishing, let us mention that we only dealt with linear convergence through a single iteration of gradient descent.</p>



<p class="justify-text">There are quite a few ways to handle both more iterations and sublinear convergence rates. Using SDPs, probably the most natural approach is to directly incorporate several iterations in the problem by  studying, for example, ratios of the form  $$\sup_{f\in\mathcal{F}_{\mu,L},\, x_0}  \frac{f(x_{N})-f_\star}{\lVert x_0-x_\star\rVert^2}. $$ This type of approach was used in the work of Drori and Teboulle [<a href="https://arxiv.org/pdf/1206.3209.pdf">2</a>] and in most consecutive PEP-related works: it has the advantage of providing  comfortable “non-improvable results” [<a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">4</a>, <a href="https://arxiv.org/pdf/1512.07516.pdf">12</a>]   (by providing matching lower bounds) for any given \(N\), but requires solving larger and larger SDPs. Alternatively, simpler proofs can often be obtained through the use of  Lyapunov (or potential) functions—i.e., study a single iteration to produce recursable inequalities; a nice introduction is provided in [<a href="http://www.theoryofcomputing.org/articles/v015a004/v015a004.pdf">13</a>]. This idea can be exploited in PEPs [<a href="https://arxiv.org/pdf/1902.00947.pdf">14</a>] by enforcing the proofs to have a certain structure. Those principles are also at the heart of the related approach using integral quadratic constraints [<a href="https://epubs.siam.org/doi/abs/10.1137/15M1009597">3</a>, <a href="http://proceedings.mlr.press/v70/hu17a/hu17a.pdf">15</a>]. </p>



<h2>Take-home message and conclusions</h2>



<p class="justify-text">The overall message of this note is that first-order methods can often be studied directly using the definition of their “worst-cases” (i.e., by trying to find worst-case scenarios), along with their dual counterparts (linear combinations of inequalities), by translating them into semidefinite programs.</p>



<p class="justify-text">What we saw might look like an overkill for studying gradient descent. However, as long as we deal with Euclidean spaces, the same approach actually works beyond this simple case. In particular, the same technique applies to first-order methods performing explicit, projected, proximal, conditional, and inexact (sub)gradient steps [<a href="https://arxiv.org/pdf/1512.07516.pdf">12</a>].</p>



<p class="justify-text">Finally, let us mention a few previous works illustrating that the use of such computer-assisted proofs allowed obtaining results that are apparently too complicated for us to find bare-handed—even in apparently simple contexts.  Reasonable examples include the direct proof for convergence rates in  function values [<a href="https://arxiv.org/pdf/1705.04398.pdf">11</a>] presented above, but also proofs arising in the context of optimized numerical schemes [<a href="https://arxiv.org/pdf/1206.3209.pdf">2</a>, <a href="https://arxiv.org/abs/1409.2636">16</a>, <a href="https://arxiv.org/pdf/1406.5468.pdf">17</a>, <a href="https://arxiv.org/pdf/1803.06600.pdf">18</a>]—in particular [<a href="https://arxiv.org/pdf/1803.06600.pdf">18</a>] presents a method for minimizing the gradient norm at the last iterate, in smooth convex minimization—,  in the context of monotone inclusions,  and even for more general fixed-point problems (e.g., for Halpern iterations [<a href="http://www.optimization-online.org/DB_FILE/2017/11/6336.pdf">19</a>]).</p>



<h3>Toolbox</h3>



<p class="justify-text">The PErformance EStimation TOolbox (PESTO) [<a href="https://perso.uclouvain.be/julien.hendrickx/availablepublications/PESTO_CDC_2017.pdf">20</a>, see <a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox/graphs/traffic">Github</a>] allows a quick access to the methodology without worrying about details of semidefinite reformulations. The toolbox contains many  examples (about 50) in different settings, and include progresses on the approach, and results, by other groups (which are much more thoroughly referenced in the <a href="https://github.com/AdrienTaylor/Performance-Estimation-Toolbox/blob/master/UserGuide.pdf">user guide</a>). In particular, we included standard classes of functions and operators, along with examples for analyzing recent optimized methods.</p>



<h2>References</h2>



<p class="justify-text">[1] Mark Schmidt, Nicolas Le Roux, Francis Bach. <a href="https://arxiv.org/pdf/1309.2388.pdf">Minimizing finite sums with the stochastic average gradient</a>. <em>Mathematical Programming</em>, <em>162</em>(1-2), 83-112, 2017.<br/>[2] Yoel Drori, Marc Teboulle. <a href="https://arxiv.org/pdf/1206.3209.pdf">Performance of first-order methods for smooth convex minimization: a novel approach</a>. <em>Mathematical Programming</em>, 145(1-2), 451-482, 2014.<br/>[3] Laurent Lessard, Benjamin Recht, Andrew Packard. <a href="https://epubs.siam.org/doi/abs/10.1137/15M1009597">Analysis and design of  optimization algorithms via integral quadratic constraints</a>. <em>SIAM Journal on Optimization</em>, 26(1), 57-95, 2016.<br/>[4] Adrien Taylor,  Julien Hendrickx, François Glineur. <a href="https://link.springer.com/article/10.1007/s10107-016-1009-3">Smooth strongly convex interpolation and exact worst-case performance of first-order methods</a>. <em>Mathematical Programming</em>, 161(1-2), 307-345, 2017.<br/>[5] Yurii Nesterov. <a href="https://link.springer.com/book/10.1007/978-1-4419-8853-9">Introductory Lectures on Convex Optimization : a Basic Course</a>. <em>Applied optimization</em>. Kluwer Academic Publishing, 2004.<br/>[6]  Boris Polyak. Introduction to Optimization. Optimization Software New York, 1987.<br/>[7] Daniel Azagra, Carlos Mudarra. <a href="https://arxiv.org/pdf/1603.00241.pdf">An extension theorem for convex functions of class \(C^{1,1}\) on Hilbert spaces</a>. <em>Journal of Mathematical Analysis and Applications</em>, 446(2):1167–1182, 2017.<br/>[8] Aris Daniilidis, Mounir Haddou, Erwan Le Gruyer, Olivier Ley. <a href="https://hal.archives-ouvertes.fr/hal-01530908/file/DHLL_appendix.pdf">Explicit formulas for \(C^{1,1}\) Glaeser-Whitney extensions of 1-Taylor fields in Hilbert spaces</a>. <em>Proceedings of the American Mathematical Society</em>, 146(10):4487–4495, 2018.<br/>[9] Johan Löfberg. <a href="https://yalmip.github.io/">YALMIP : A toolbox for modeling and optimization in MATLAB</a>. <em>Proceedings of the CACSD Conference</em>, 2004.<br/>[10] APS Mosek. <a href="https://www.mosek.com/">The MOSEK optimization software</a>. Online at http://www.mosek.com, 54, 2010.<br/>[11] Adrien Taylor, Julien Hendrickx, François Glineur. <a href="https://arxiv.org/pdf/1705.04398.pdf">Exact worst-case convergence rates of the proximal gradient method for  composite convex minimization</a>. <em>Journal of Optimization Theory and Applications</em>, vol. 178, no 2, p. 455-476, 2018.<br/>[12] Adrien Taylor, Julien Hendrickx, François Glineur. <a href="https://arxiv.org/pdf/1512.07516.pdf">Exact worst-case performance of first-order methods for composite convex optimization</a>. <em>SIAM Journal on Optimization</em>, vol. 27, no 3, p. 1283-1313, 2017.<br/>[13] Nikhil Bansal, Anupam Gupta. <a href="http://www.theoryofcomputing.org/articles/v015a004/v015a004.pdf">Potential-Function Proofs for Gradient Methods. <em>Theory of Computing</em></a>, <em>15</em>(1), 1-32, 2019.<br/>[14] Adrien Taylor, Francis Bach. <a href="https://arxiv.org/pdf/1902.00947.pdf">Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions</a>, <em>Proceedings of the 32nd Conference on Learning Theory (COLT)</em>, 99:2934-2992, 2019.  <br/>[15] Bin Hu, Laurent Lessard. <a href="http://proceedings.mlr.press/v70/hu17a/hu17a.pdf">Dissipativity theory for Nesterov’s accelerated method</a>, <em>Proceedings of the 34th International Conference on Machine Learning</em>, 70:1549-1557, 2017. <br/>[16] Yoel Drori, Marc Teboulle. <a href="https://arxiv.org/abs/1409.2636">An optimal variant of Kelley’s cutting-plane method</a>. <em>Mathematical Programming</em> 160.1-2: 321-351, 2016.<br/>[17] Donghwan<strong> </strong>Kim, Jeffrey Fessler. <a href="https://arxiv.org/pdf/1406.5468.pdf">Optimized first-order methods for smooth convex minimization</a>, <em>Mathematical programming</em>, <em>159</em>(1-2), 81-107, 2016.<br/>[18] Donghwan Kim, Jeffrey Fessler. <a href="https://arxiv.org/pdf/1803.06600.pdf">Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions</a>, <em>preprint arXiv:1803.06600</em>, 2018.   <br/>[19] Felix Lieder. <a href="http://www.optimization-online.org/DB_FILE/2017/11/6336.pdf">On the convergence rate of the Halpern-iteration</a>. Technical Report, 2019.<br/>[20] Adrien Taylor, Julien Hendrickx, François Glineur.  <a href="https://perso.uclouvain.be/julien.hendrickx/availablepublications/PESTO_CDC_2017.pdf">Performance estimation toolbox (PESTO): automated worst-case analysis of  first-order optimization methods</a>,<em> Proceedings of the 56th Annual Conference on Decision and Control (CDC)</em>, pp. 1278-1283, 2017.</p></div>
    </content>
    <updated>2020-04-03T11:37:27Z</updated>
    <published>2020-04-03T11:37:27Z</published>
    <category term="Machine learning"/>
    <category term="Tools"/>
    <author>
      <name>Adrien Taylor</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-04-11T04:21:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=19411</id>
    <link href="https://gilkalai.wordpress.com/2020/04/03/trees-not-cubes-memories-of-boris-tsirelson/" rel="alternate" type="text/html"/>
    <title>Trees not Cubes! Memories of Boris Tsirelson</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post is devoted to a few memories of Boris Tsirelson who passed away at the end of January. I would like to mention that a few days ago graph theorist Robin Thomas passed away after long battle with ALS. … <a href="https://gilkalai.wordpress.com/2020/04/03/trees-not-cubes-memories-of-boris-tsirelson/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post is devoted to a few memories of Boris Tsirelson who passed away at the end of January. I would like to mention that a few days ago graph theorist Robin Thomas passed away after long battle with ALS. I hope  to tell about Robin’s stunning mathematics in a future post.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/03/tsirelson_boris.jpg"><img alt="" class="alignnone size-full wp-image-19552" height="480" src="https://gilkalai.files.wordpress.com/2020/03/tsirelson_boris.jpg?w=640&amp;h=480" width="640"/></a></p>
<p>Boris Tsirelson (1950 – 2020); <a href="http://www.math.tau.ac.il/~tsirel/">Boris’ home-page</a>, and <a href="https://en.wikipedia.org/wiki/Boris_Tsirelson">Wikipedia</a>. (More links, below.)</p>
<p>The title of the post is taken from the title of a very interesting 1999 paper by Boris Tsirelson and Oded Schramm: <a href="https://arxiv.org/abs/math/9902116">Trees, not cubes: hypercontractivity, cosiness, and noise stability</a></p>
<p>I was very sad and shocked to hear that Boris Tsirelson had passed away. Boris was one of the greatest Israeli mathematicians, and since 1997 or so we established mathematical connections surrounding several matters of common interest.  Here are a few memories.</p>
<h3>Love for coding</h3>
<p>1) One thing that Boris told me was that he loves to code. Being a “refusnik”, he could not get into Academia and (luckily) he could work as a programmer. And he told me that afterwards deciding what he liked more – programming or doing mathematical research – was no longer a trivial question for him.  Boris chose to go back to mathematical research, but he continued to enjoy programming, and when he needed it in his mathematical research, he could easily program.</p>
<h3>Love for quantum</h3>
<p>2) Another thing that Boris loved is “quantum”, the mathematics and physics of quantum mechanics and various connections to mathematics. Early on he proved his famous Tsirelson’s bound related to Bell’s inequalities, and later he was enthusiastic about the area of quantum computing. (And he learned it quickly, taught a course about it in 1997, and his 1997 lecture notes are still considered very useful.)</p>
<h3>Black Noise and noise sensitivity</h3>
<p>3) Perhaps the most significant mathematical connection between us was in the late 90s, and was centered around the theory of noise stability and noise sensitivity by Benjamini, Schramm and myself, which was closely related to a theory initiated by Boris Tsirelson and Anatoly Vershik. The translation between the different languages that we used and that Boris used was awkward, since the analog of Boolean functions that we studied was the “noise” that Boris studied, and the analog of noise sensitive Boolean functions in our language was “black noise” in Boris’s language. In any case, we had email discussions and we also met a few times with Itai and Oded regarding this connection.</p>
<h3>Black Noise and noise sensitivity II</h3>
<p>4)  Boris developed a very rich theory of black noise with relations to various areas of probability theory and operator algebras. He also found hypercontractivity that we used in our work quite useful to his applications, and also in this theory, he considered both classical and quantum aspects. I know only a little about Boris Tsirelson’s theory and its applications, but as far as tangent points with our Boolean interests are concerned, I can mention that Boris was enthusiastic by the <a href="https://arxiv.org/abs/1101.5820">result</a> of Schramm and Stas Smirnov that percolation is a “black noise” and also that, in 1999, Boris and Oded Schramm wrote a paper whose title started with “Trees not cubes!”, presenting a different angle on this theory.</p>
<p>Tsirelson saw white noise (what we call noise stability) as manifesting “linearity” while “black noise” (what we call noise sensitivity) as manifesting “non-linearity”. Over the years, I often asked him to explain this to me.</p>
<h3>Tsireslon’s Banach spaces</h3>
<p>5) Geometry of Banach spaces is a very strong area in Israel so naturally I heard as a graduate student about “Tsirelson’s space” from 1974 and some subsequent developments in the 80s. Boris Tsirelson constructed a  Banach space that does not contain an imbedding of <img alt="\ell_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ell_p"/> or <img alt="c_0" class="latex" src="https://s0.wp.com/latex.php?latex=c_0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_0"/>.</p>
<h3>Bible codes</h3>
<p>6) My first personal connection with Boris was related to claims regarding a hidden Bible Code, and a 1994 paper claiming a statistical proof of the existence of these Bible codes. For many years my attitude was that these claims should be ignored, but around 1997, I changed my mind and did some work to see what was going on. Now, Boris kept a site linked in his homepage devoted to developments regarding the Bible Code claims. In this site Boris kindly reported about my first 1997 paper on the topic, my observation that the proximity of two reported p-values for the two Bible code experiments was “too good to be true”, and my interpretation that this suggests that the claimed results manifest naïve statistical expectations rather than scientific reality.  A few weeks later, Boris reported about a much stronger evidence (by McKay and Bar Nathan) against the Bible Code claims (they demonstrated codes of similar quality in Tolstoy’s “War and Peace”) and subsequently after some time he lost interest in this topic.</p>
<h3>Quantum computing skepticism</h3>
<p>6) In 2005 we had some correspondence and meetings regarding my quantum computing skepticism. In his first email he told me that my reference to “decoherence” seemed strange and I realized that I consistently referred to “entanglement” as “decoherence” and to “decoherence” as “entanglement”.</p>
<p>7) In his 1997 lecture notes on quantum computing (that I cannot find on the web, so I count on my memory), Boris addressed the concerns of early quantum computers skeptics like Rolf Landauer. He did not accept the analogy between quantum computing and analog computation, but he also regarded the analogy with digital computation as problematic. Rather, he regarded quantum information based on qubits as something (at least a priori) different from both these examples. (Update: I found one non-broken link to the lecture notes; indeed the subtitle of Chapter 9 is “neither analog nor digital”.)</p>
<p>A joke that I heard from Boris at that time</p>
<p>8) I remember that once when I asked him about some aspects of quantum fault tolerance he told me the following joke: A student is entitled to a special exam, he arrives at the professor’s office, is given three questions to answer and he fails to do so. He request and is granted a make-up exam two weeks later. When the student shows up at the office two weeks later the professor, who forgot all about it, gave him the same three questions. “This is extremely unfair”, said the student “you ask me questions that you already know that I cannot answer.”</p>
<h3>Noise sensitivity and high energy physics</h3>
<p>9) In 2006 I came up with the idea that noise sensitivity might be a great idea for physics. Knowing very little physics, I wrote a little manifesto with this idea and tried it, among other people, on Boris. As it turned out, Boris had the idea that noise sensitivity could add a useful modeling power to physics (especially high energy physics) well before that time. (And by 2006 he was already a bit skeptical regarding his own idea.) He also told me that one of the motivations of his 1998 paper with Tolya Vershik came from some mathematical ideas related to physics of the big bang. When I asked him if this was written somewhere in the paper itself, he answered: “Of course, not!”</p>
<h3>Boris Tsirelson’s lecture at Oded’s memorial school</h3>
<p>10) in 2009 we organized a meeting in memory of Oded Schramm and Boris gave a lecture related to the Schramm-Smirnov “percolation is black noise” result with a single theorem. And what was remarkable about it that it was that he presented a classical theorem with a quantum proof. You can find the videotaped lecture here  (And here are the slides. Boris never wrote up this result.) Following this lecture we had a short correspondence with Scott A. and Greg K. about quantum proofs to classical theorems. (Namely theorems that do not mention quantum in the statement).</p>
<h3>Tsirelson’s problem</h3>
<p>11) Our last correspondence in 2019 was about Thomas Vidick’s  Notices AMS article about Tsirelson’s problem. (This was a couple of months before the announcement of the solution.) Boris was pleased to hear about these developments, as he was regarding earlier developments in this area. He humorously refers to the history of his problem on his homepage and this interview.</p>
<p>12) People who knew Boris regarded him as a genius from a very early age, and former students have fond memories of his classes.</p>
<p> </p>
<h3>More resources:</h3>
<p>Boris’s <a href="http://www.math.tau.ac.il/~tsirel/">home page</a> contains  “Museum to my courses” with many useful lecture notes; link to a small <a href="http://www.math.tau.ac.il/~tsirel/Research/qcomp/main.html">page on quantum computation</a> with a link to Boris’ <a href="http://www.math.tau.ac.il/~tsirel/Courses/QuantInf/syllabus.html">1997 lecture notes on quantum computing</a>.  Links to comments on some of Tsirelson’s famous papers. <a href="http://www.math.tau.ac.il/~tsirel/Research/mybound/main.html">Tsirelson’s 1980 bound</a>. Boris <a href="http://www.math.tau.ac.il/~tsirel/Research/refereed.html">published papers</a>, and his “<a href="http://www.math.tau.ac.il/~tsirel/Research/self-publ.html">self-published</a>” papers.</p>
<p>Boris was a devoted Wikipedian and his Wikipidea <a href="https://en.wikipedia.org/wiki/User:Tsirel">user page</a> is now devoted to his memory; Here is <a href="https://www.iqoqi-vienna.at/en/blog/article/boris-tsirelson/?fbclid=IwAR1PrVvK0u5XmnFLLoPMzMN3x9rY1WIdp1wrYZ_yYPlqSGRpXDkollYTCR0">a great interview</a> with Boris; A very nice <a href="https://www.scottaaronson.com/blog/?p=4626">memorial post</a> on Freeman Dyson and Boris Tsirelson on the Shtetl Optimized; Tim Gowers explains some ideas behind Tsirelson’s space <a href="https://twitter.com/wtgowers/status/1231704267641249794">over Twitter;</a> and here in <a href="https://gowers.wordpress.com/2009/02/17/must-an-explicitly-defined-banach-space-contain-c_0-or-ell_p/">Polymath2</a>.</p>
<p>Below the fold some emails of interest from Boris, mainly where he explained to me various mathematics. (More can be found in this page.)</p>
<p><span id="more-19411"/></p>
<h2>Some email correspondence</h2>
<h3>Oct. 2019 Vidick’s paper</h3>
<p>Dear Boris<br/>
<a href="https://www.ams.org/journals/notices/201910/rnoti-p1618.pdf">This paper</a> by Thomas Vidick may interest you,<br/>
best regards and shana tova Gil</p>
<p>Oh yes, sure!<br/>
Thank you.<br/>
Shana metuka, Boris</p>
<p>(Remark: “metuka” means “sweet” in Hebrew.)</p>
<h3>Dec 2006 (about noise sensitivity and physics)</h3>
<p>(Dec 2006) My very first idea in this field (inspired by conversations with Vershik) was<br/>
rather physical (that Big Bang could be a natural occurrence of black noise),<br/>
and in fact the main example of “Tsirelson and Vershik 1998” follows this line<br/>
(not explicitly, of course).</p>
<p>In local (not Big Bang related) physics, I think, nonlinearity could produce<br/>
such effect. And then the very idea of `the field operator at a point’ (on<br/>
the level of operator-valued Schwartz distributions or something like that)<br/>
will fail. However, physicists do not want to consider this possibility<br/>
without very serious indications that it really is used by the nature. And<br/>
they are right…</p>
<h3>2005 quantum computer skepticism</h3>
<p>Subject: Re: Noise and more</p>
<p>Dear Gil:<br/>
Yes, of course, we can meet and speak.</p>
<p>For now, I am not much bothered. I am not an expert in quantum error<br/>
correction, but anyway, my feeling is that all physically reasonable<br/>
“attacks” of Nature are repelled. Especially, your three-qubit attack<br/>
looks to me not dangerous. And, “der Herr Gott is raffiniert, aber<br/>
boschaft ist er nicht”; Nature never attacks like an enemy.</p>
<p>Yours, Boris.</p>
<h3>Quick 2000 comments on a (sloppy) draft of my survey paper</h3>
<p>Dear Gil:</p>
<p>Thank you for the text; I am reading it.<br/>
For now, only a trivial remark: “Tsilerson” should be “Tsirelson” in<br/>
[133] and [134]; and in [132] “” should be “Tsirelson”…</p>
<p>Shabat shalom,<br/>
Yours, Boris.</p>
<h3>November 1998: Noise sensitivity and black noise</h3>
<p>Dear Gil,</p>
<p>I am reading your (with Itai and Oded) paper. Thanks.</p>
<p>Moreover, I am thinking about changing the title of my future talk in<br/>
Vien accordingly: from “The five noises” to “The six noises” (or even<br/>
more).<br/>
To this end, however, I need to answer the following question.</p>
<p>Is there a mesh refinement limit for the percolation?</p>
<p>That is, take the lattice with a small pitch \eps. Choose two<br/>
“electrodes”, say, two vertical intervals on two parallel vertical<br/>
lines, and ask about the probability that they are connected (via the<br/>
bond percolation on the whole band between the two vertical lines).<br/>
Let the electrodes be macroscopic; that is, they do not depend on<br/>
\eps. Does the probability of the event have a limit for \eps \to 0 ?<br/>
If it does, then one more question: what about the joint distribution<br/>
for a finite collection of such events? That is, I want to see a weak<br/>
limit of these “discrete” random processes. It seems to me, the<br/>
question is well-known and was discussed. However, do you know the<br/>
answer?</p>
<p>Yours, Boris.</p>
<p>Dear Itai, Oded, and Gil,</p>
<p>Thank you for the information. I see that for now we have a<br/>
conditional result: if there exists “the noise of percolation”, then<br/>
it is not a white noise.<br/>
Yours, Boris.</p>
<p>Dear Gil:<br/>
&gt; I dont understand yet the concept of “noise” precisely</p>
<p>One of ways is this. A noise is a scaling limit for coin tossing.<br/>
You choose a class of “macroscopic observables” and look, whether<br/>
their joint distribution converges, when n\to\infty. If it does, you<br/>
get a noise. (For percolation we do not know, does it or not.)<br/>
Now, if all “macroscopic observables” are noise insensitive, it means<br/>
that the noise is white. For a white noise, there is only one<br/>
invariant, its dimension (or multiplicity).<br/>
If all “macroscopic observables” are noise sensitive, the noise is<br/>
black. Probably, there are a lot of black noises, but for now we have<br/>
only two examples, without knowing, whether they are isomorphic, or<br/>
not. Spectra may be used for classifying black noises. Say, it may<br/>
happen that for each “macroscopic observable”, its spectrum is<br/>
concentrated on sets having Hausdorf dimension less than something.<br/>
If some “macroscopic observables” are noise sensitive but some others<br/>
are not (except for constants, of course), then the noise is neither<br/>
white nor black. It may happen that it is a direct sum of a white<br/>
noise and a black noise. However, it may happen that it is not. We<br/>
have for now two such examples: “noise if splitting” and “noise of<br/>
stickiness” (they are probably non-isomorphic); both are found by Jonathan<br/>
Warren. I am trying to understand, whether your matter can give more<br/>
examples.</p>
<p> </p>
<h3>August 1998: Bible code story</h3>
<p>Dear Gil,</p>
<p>Thanks for the text.<br/>
As for me, it is already an `overkill’, since for me the WRR94<br/>
is basically dead. But maybe for others…</p>
<p>Yours, Boris.</p>
<p> </p></div>
    </content>
    <updated>2020-04-03T08:03:55Z</updated>
    <published>2020-04-03T08:03:55Z</published>
    <category term="Combinatorics"/>
    <category term="Obituary"/>
    <category term="Probability"/>
    <category term="Quantum"/>
    <category term="Boris Tsirelson"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-04-11T04:20:46Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6956243767390574114</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6956243767390574114/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/lets-hear-it-for-cloud.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6956243767390574114" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6956243767390574114" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/lets-hear-it-for-cloud.html" rel="alternate" type="text/html"/>
    <title>Let's Hear It for the Cloud</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>Since March 19th I have worked out of home. I've had virtual meetings, sometimes seven or eight a day, on Zoom, Bluejeans, Google Hangouts, Google Meet, Blackboard Collaborate Ultra and Microsoft Teams. I take notes on my iPad using Penultimate which syncs with Evernote. I store my files in Dropbox and collaborate in Google Drive. I communicate by Google Chat, Gmail, Facebook messenger and a dozen other platforms. I continue to tweet and occasionally post in this blog. </div><div><br/></div><div>A billion of my closest friends around the world are also working out of home and using the same and similar tools. Yet outside of some pretty minor issues, all of these services continue to work and work well. Little of this would have been possible fifteen years ago. </div><div><br/></div><div>As Amazon scaled up their web operations to handle their growing business in the early 2000's they realized they could sell computing services. AWS, Amazon Web Services, started in 2006. Microsoft Azure, Google and others followed. These sites powered smartphones and their apps that push heavy processing to the cloud, small startups who don't need to run their own servers, and companies like Zoom when they need to scale up quickly and scale down like Expedia when they don't need as much use. Amazon and Microsoft makes most of their profit on cloud services. Amazon can't get me toilet paper but they can make sure Blackboard continues to work when all of our classes move online. </div><div><br/></div><div>Just for fun I like to occasionally look over the large collection of <a href="https://aws.amazon.com/products">Amazon Cloud Products</a>. Transcribe an audio recording and translate to Portuguese, not a problem. </div><div><br/></div><div>The cloud can't allow all of us to work from home. We have many who still go to work including front-line health care workers putting their lives on the line. Many have lost their jobs. Then of course there are those sick with the virus, many of whom will never recover. We can't forget about the reason we stay indoors.</div><div><br/></div><div>But every now and then it's good to look back and see how a technology has changed our world in a very short time. If we had this virus in the 90's we'd still be having to go to work, or simply stop teaching and other activities all together.</div><div><br/></div><div>And how will our universities and other work spaces look like in the future now that we find we can work reasonably well from home and even better technologies develop? Only time will tell.</div><div><br/></div><div><br/></div><div><br/></div></div>
    </content>
    <updated>2020-04-02T19:20:00Z</updated>
    <published>2020-04-02T19:20:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-04-10T09:42:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-6301869508435164660</id>
    <link href="http://processalgebra.blogspot.com/feeds/6301869508435164660/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=6301869508435164660" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6301869508435164660" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/6301869508435164660" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/04/an-interview-with-davide-sangiorgi.html" rel="alternate" type="text/html"/>
    <title>An interview with Davide Sangiorgi, CONCUR Test-of-Time Award recipient</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The <a href="https://concur2020.forsyte.at/">International Conference on Concurrency Theory (CONCUR)</a> and the I<a href="https://concurrency-theory.org/organizations/ifip">FIP 1.8 Working Group on Concurrency Theory</a> are happy to announce the first edition of the CONCUR  Test-of-Time Award. The purpose of the award is to recognize important  achievements in Concurrency Theory that were published at the CONCUR  conference and have stood the test of time. All papers published in  CONCUR between 1990 and 1995 were eligible.<br/><br/>The award winners for the  CONCUR ToT Awards 2020 may be found <a href="https://concur2020.forsyte.at/test-of-time/index.html">here</a>, together with the citations for the awars. They were selected by a jury composed of Jos Baeten, Patricia  Bouyer-Decitre, Holger Hermanns, Alexandra Silva and myself.<br/><br/>This post is the first of a series in which I interview the recipients of the  CONCUR ToT Awards 2020. I asked <a href="http://www.cs.unibo.it/~sangio/">Davide Sangiorgi </a>(University of Bologna, Italy) a small number of questions via email and I report his answers below in a slightly edited form. Let  me thank Davide for his willingness to take part in an interview and for his inspiring answers, which I hope will be of interest to readers of this blog and will inspire young researchers to take up work on Concurrency Theory.<br/><br/>In what follows, LA stands for Luca Aceto and DS for Davide Sangiorgi.<br/><br/>LA: You receive one of the two CONCUR ToT Awards for the period 1992-1995 for your paper "<a href="http://www.lfcs.inf.ed.ac.uk/reports/93/ECS-LFCS-93-270/">A Theory of Bisimulation for the pi-Calculus</a>", presented at CONCUR 1993. Could you tell us briefly what spurred you to develop open bisimilarity and how the ideas underlying that notion came about?<br/><br/>DS: I developed the paper on open bisimulation during 1992 and 1993.  I was in Robin Milner's group, in Edinburgh.  We were studying and questioning basic aspects of the theory of the <a href="https://en.wikipedia.org/wiki/%CE%A0-calculus">pi-calculus</a>. One such aspect was the definition of equality on processes; thus a very fundamental aspect, underlying the whole theory.  The equality had to be a form of bisimulation, in the same way as it was for CCS.  The two forms of bisimilarity that were around for the pi-calculus, late and early bisimilarity, are not congruence relations.  In both cases, the input clause of bisimilarity uses a universal quantification on the possible instantiations of the bound name. As  a consequence, neither bisimilarity is preserved by the input prefix (forbidding substitutions in the input clause would make things worse, as congruence  would fail for parallel composition). Therefore, one has to introduce separately the induced congruence, by universally quantifying the bisimilarities over all name substitutions.  In other words,  the two bisimilarities are  not fully substitutive ('equal' terms cannot be replaced, one for the other,  in an arbitrary context).   On the other hand, the  congruences induced by the bisimilarities  are not themselves  bisimilarities. Hence in this case 'equal' terms, after some actions,  need not be 'equal' anymore.  Thus, for instance, such relations do not support dynamic modifications of the context surrounding related terms.<br/><br/>This situation was not fully satisfactory. The same could be said for the algebraic theory: there were proof systems for the two bisimilarities (of course, on finitary processes) but, because of the above congruence issue, there were no axiomatisations.  (In those years I was also working with Joachim Parrow on axiomatisations of these relations.)<br/><br/>The universal quantification on substitutions in the input clause of the bisimilarites and in the definitions of the induced congruences was also unsatisfactory because it  could make checking equalities cumbersome.<br/><br/>All these were  motivations for looking at possible variations of the definition of bisimulation. The specific hints towards open bisimulation came from thinking at two key facets of the pi-calculus model that were somehow neglected in the definitions of  early and late bisimilarities.  The first facet has to do with the pi-calculus rejection of the separation between channels and variables (`channel' here meaning a 'constant identifier'). In the pi-calculus ,there is only one syntactic category, that of <i>names</i>, with no formal distinction between channels and variables. This contributes to the elegance of the model and its theory. However, both in early and in late bisimilarity, the free names of processes are treated as channels, whereas the bound names of  inputs are treated as variables because of their immediate  instantiation  in the bisimilarity clause.   There was somehow a discrepancy between the syntax and the semantics. <br/><br/>The second facet of the pi-calculus that contributed to the definition of open bisimilarity is the lack of the mismatch operator: the pi-calculus, at least in its original proposal, has a match operator to test equality between names, but not the dual mismatch, to test for inequality.  Mismatching had been excluded for the preservation of a monotonicity property on transitions, intuitively asserting that substitutions may only increase the action capabilities of a  process.  (Both facets above represented major differences between the pi-calculus and its closest ancestor----Engberg and Nielsen's Extended CCS.)  Thus I started playing with the idea of avoiding the name instantiation in the input clause and, instead, allowing, at any moment, for arbitrary instantiations (i.e., substitutions) of the names of the processes---the latter justified by the above monotonicity property of transitions. By adding the requirement of being a congruence, the definition of open bisimilarity came about.<br/><br/>Still, I was not sure that such a bisimulation could be interesting and robust.  Two further developments helped here. One was the axiomatisation (over recursion-free terms).  It was a pure axiomatisation, it was  simple,  and with a completeness proof that leads to the construction of canonical and minimal (in some syntactic sense) representatives for the equivalence classes of the bisimilarity.  For other bisimilarities, or related congruences, obtaining canonical representatives seems hard; at best such representatives are parametrised upon a set of free names and even in these cases minimality is not guaranteed.<br/><br/>The other development has to do with a symbolic or "efficient" characterisation of the bisimilarity. The initial definition of open bisimulation makes heavy use of substitutions.  In the symbolic characterisation, substitutions are performed only when needed (for instance, the unification of two names <i>a</i> and <i>b</i> is required if there is an input at <i>a</i> and an output at <i>b</i> that can interact), somehow echoing the call-by-need style of  functional languages.  Such a characterisation seemed promising for automated or semi-automated verification. <br/><br/>LA: How much of your later work has built on your CONCUR 1993 paper? What results of yours are you most proud of and why?<br/><br/>DS: The most basic idea in open bisimulation is to avoid the instantiation of the bound name of an input, possibly making such a bound name a free name of the derivative term.  The use of substitutions, elsewhere in the definition, is necessary to obtain a congruence relation for the pi-calculus.  I was surprised to discover, in the following years, that such substitutions are not necessary in two relevant subsets of the pi-calculus.  I called the variant of open bisimulation without substitutions <i>ground</i> bisimulation (I think the name came from Robin).  One subset is Honda and <a href="https://hal.inria.fr/inria-00076939/en">Boudol</a>'s Asynchronous pi-calculus, whose main constraint is to make outputs floating particles that do not trigger the activation of a continuation (other limitations concern sum and matching).  The other subset is the Internal (or Private) pi-calculus, in which only private (i.e., restricted) names may be transmitted.  I designed the Internal pi-calculus with ground bisimilarity in mind.  People seem to have found this calculus useful in many ways, partly because of its expressiveness combined with its  simple theory (in many aspects similar to that of CCS), partly because it allows one to limit or control aliasing between names, which can be useful for carrying out proofs about behavioural properties of processes, or for designing and reasoning about type systems, or for representing the calculus in logical systems.<br/><br/>Sometimes, the possibility of using ground bisimulation can considerably simplify proofs of equalities of terms. For instance, in my works on comparisons between pi-calculus and lambda-calculus, when I had to translate the latter into the former I have always used one of the above subcalculi (sometimes even combining them, e.g., the Asynchronous Internal pi-calculus), precisely for being able to use ground bisimilarity.<br/><br/>I  consider both ground bisimilarity and the Internal pi-calculus spin-offs of the work on open bisimilarity.<br/><br/>While working on open bisimilarity for the pi-calculus, in a different paper, I applied the idea of open bisimilarity to the lambda-calculus.  I kept the name 'open'  but the bisimulation is really 'ground', as there are no substitutions involved.  I remember Robin encouraging me to keep the name 'open' because it conveyed well the idea of setting a bisimulation on open terms, rather than on closed terms as usually done. In  open bisimulation for the lambda-calculus, a lambda-abstraction<i> lambda x. M</i> yields an action with label <i>lambda x</i> that should be matched (modulo  alpha-conversion) by the same action by a bisimilar term. (Of course additional bisimulation clauses are needed when a free variable is found in evaluation position.) In contrast, in the ordinary bisimulation for the lambda-calculus,  Abramsky's applicative bisimilarity, the bound variable of an abstraction has to be instantiated with all closed terms, which is heavy. In general, open bisimilarity is finer than applicative bisimilarity and contextual equivalence (the reference equivalence in the lambda-calculus) but they often coincide in examples of concrete interest. Moreover, open bisimilarity does coincide with contextual equivalence in appropriate extensions of the lambda-calculus.  In short, open bisimilarity offers us a technique for reasoning on higher-order languages  using  'first-order' tools, somehow similarly to what game semantics does.<br/><br/>This line of work about open bisimilarity in higher-order languages has been very fruitful, and is still studied a lot, for various forms of higher-order languages, sometimes under the name of 'normal-form' bisimulation. <br/><br/>LA: In your opinion, what is the most interesting or unexpected use in the literature of the notions and techniques you developed in your award-winning paper?<br/><br/>DS: I mentioned the hope, when working on open bisimilarity, that its symbolic "efficient" characterisation could be useful for automated or semi-automated tools for reasoning about behavioural properties.  Shortly after introducing open bisimulation,   Björn Victor and Faron Moller, both in Edinburgh at the time,   exploited it to design the Mobility Workbench. I also worked on an algorithm and a prototype tool for on-the-fly checking, with Marco Pistore.  <br/><br/>However the most surprising applications in this direction have arrived later, when the 'lazy' instantiation of bound names of open bisimilarity has been applied to languages richer than pi-calculus.  For instance, Chaki, Rajamani, Rehof have used open similarity in their methods and tools for model checking distributed message-passing software.  Others have applied open bisimilarity to languages for security and cryptography, like the spi-calculus and applied pi-calculus. These include S. Brias, U. Nestmann and colleagues at EPFL and Berlin.   R. Horne, A. Tiu and colleagues in Singapore and Luxembourg have pushed significantly in this direction, with verification techniques and tools. For instance very recently they have discovered a privacy vulnerability for e-passports.  <br/><br/>Similarly, Yuxin Deng and colleagues have applied the idea of open bisimulation to quantum processes, with analogous motivations --- avoiding the universal quantification in the  instantiation of variables, algorithmically unfeasible in the quantum setting as quantum states constitute a continuum.<br/><br/>Another line of work that I found interesting and surprising concerns abstract frameworks for concurrency, including logical frameworks. Here forms of open bisimulation are often the 'natural' bisimulation that come up. These frameworks may be based, for instance on  coalgebras and category theory  (e.g., works by M. Fiore and S. Staton, N. Ghani,   K. Yemane, and B. Victor), category theory for reactive systems (e.g., works by F. Bonchi, B. König and U. Montanari), nominal SOS rule formats  (e.g., works by M. Cimini,  M. R. Mousavi, and  M. A. Reniers), higher-order logic languages (e.g., works by A.  Tiu, G. Nadathur, and D. Miller). <br/><br/>There have been works that have pushed the idea of open bisimulation of avoiding the instantiation of  bound names in interactions with an external observer one step further: such bound names are not instantiated even in interactions<i> internal </i>to the processes. The substitutions produced by the interactions are added to the calculus, producing particles sometimes called fusions.  This mechanism resembles the explicit substitutions of the lambda-calculus, but it goes beyond that; for instance the addition of fusions leads to modifications of  input and output prefixes that produce pleasant syntactic and semantic symmetry properties.  Various people have worked on this, including B. Victor, J. Parrow, Y. Fu, C. Laneve, P. Gardner and L. Wischik.<br/><br/>I should also mention the recent work by K. Y. Ahn, R. Horne, and A. Tiu  on logical interpretations of open bisimilarity. Remarkably, they explain the difference between the original (late and early)  formulations of bisimilarity in the pi-calculus  and open bisimilarity as the difference between  intuitionistic and classical versions of modal logics. <br/><br/>Apologies for not mentioning everybody!<br/><br/>LA: To your mind, how has the focus of CONCUR changed since its first edition in 1990?<br/><br/>DS: I remember that in the early years of CONCUR there was  a tremendous excitement about the conference.  A forum for grouping the (fast-growing) community had been long-awaited.  In Edinburgh, every year after the conference,   the people with an interest in concurrency   would meet (multiple times!)  and discuss the contents of the proceedings.  Several of us every year would attend the conference. Attending the conference was very useful and interesting: one was sure to meet a lot of people, hear about excellent papers, have lively discussions.  We would  try to go,  even without  a paper in the proceedings.   I vividly remember the 1993 edition, where I presented the paper on open bisimulation.  It had been organised by Eike Best in Hildesheim, Germany. It was an  enjoyable and exciting week,  I met and knew a number of people of our community, and learned a lot. (How sad and unwise that the Computer Science department in Hildesheim, so strong in concurrency  at the time, was shut down a few years later.)  Over the years, the CONCUR community has kept increasing its size. The conference has substantially broadened its scope,  rightly including new emerging topics. Perhaps it is more difficult than in the past to (more or less) understand most of the presented papers, both because of the diversity of the topics and of their technical specialisation.  On the other hand, there are now satellite events, covering  a number of areas. Hence  there are always plenty of interesting presentations and talks to attend (this definitely occurred to me in the last edition, in Amsterdam!).  I should also mention here the activity on the IFIP WG 1.8 on Concurrency Theory, currently chaired by Ilaria Castellani, that in many ways  supports and promotes CONCUR.<br/><br/>The quality of the papers at CONCUR is still very high. This is very important. As a community we should strive to maintain, and possibly even increase, the excellence and prestige of the conference, first of all, by submitting our best papers to the conference. CONCUR must be a reference conference in Computer Science, which is essential for injecting new people and energy into the community.<br/><br/><b>Acknowledgements:</b> Many thanks to <a href="http://www-sop.inria.fr/members/Ilaria.Castellani/">Ilaria Castellani</a>, who pointed out a number of typos in the original version of this text. </div>
    </content>
    <updated>2020-04-02T16:16:00Z</updated>
    <published>2020-04-02T16:16:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-04-09T10:14:57Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-04-02-bilinear-accumulators-for-cryptocurrency/</id>
    <link href="https://decentralizedthoughts.github.io/2020-04-02-bilinear-accumulators-for-cryptocurrency/" rel="alternate" type="text/html"/>
    <title>Bilinear Accumulators for Cryptocurrency Enthusiasts</title>
    <summary>Accumulator schemes are an alternative to Merkle Hash Trees (MHTs) for committing to sets of elements. Their main advantages are: Constant-sized membership and non-membership proofs, an improvement over logarithmic-sized proofs in MHTs, Algebraic structure that enables more efficient proofs about committed elements1 (e.g., ZeroCoin2 uses RSA accumulators for anonymity), Constant-sized...</summary>
    <updated>2020-04-02T08:10:00Z</updated>
    <published>2020-04-02T08:10:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-04-10T23:52:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1644</id>
    <link href="https://theorydish.blog/2020/04/01/approx-random-2020-is-virtual-from-the-get-go/" rel="alternate" type="text/html"/>
    <title>APPROX/RANDOM 2020 is Virtual From the Get Go</title>
    <summary>While APPROX/RANDOM 2020 was scheduled for September, we decided to reduce uncertainty in these stormy days and declare it to be virtual already in the CFP. We hope to have APPROX/RANDOM 2021 hosted in Seattle by UW, instead of this year,  So if you never sent papers to APPROX/RANDOM because you hate travel, this is your year to start!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>While <a href="https://randomconference.com/random-2020-home/">APPROX/RANDOM 2020</a> was scheduled for September, we decided to reduce uncertainty in these stormy days and declare it to be virtual already in the <a href="https://randomconference.files.wordpress.com/2020/04/randomapprox2020cfp-3.pdf">CFP</a>. We hope to have APPROX/RANDOM 2021 hosted in Seattle by UW, instead of this year,  So if you never sent papers to APPROX/RANDOM because you hate travel, this is your year to start!</p></div>
    </content>
    <updated>2020-04-01T22:37:07Z</updated>
    <published>2020-04-01T22:37:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-04-11T04:21:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-7256726075360411284</id>
    <link href="http://processalgebra.blogspot.com/feeds/7256726075360411284/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=7256726075360411284" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7256726075360411284" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/7256726075360411284" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/04/magnus-m-halldorsson-eatcs-fellow-2020.html" rel="alternate" type="text/html"/>
    <title>Magnus M. Halldorsson: EATCS Fellow 2020</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">As announced <a href="http://eatcs.org/index.php/component/content/article/1-news/2851--eatcs-fellows-class-of-2020-named">here</a>, the EATCS Fellows vintage 2020 are<br/><ul><li><a href="http://pages.di.unipi.it/degano/">Pierpaolo Degano</a>, Universita di Pisa, Italy: for his contributions in concurrency theory and applications in security and for biological systems. </li><li><a href="http://www.cs.umd.edu/~hajiagha/">Mohammad Taghi Hajiaghayi,</a> University of Maryland, USA: for his contributions to the theory of algorithms, in particular algorithmic graph theory, game theory, and distributed computing. </li><li><a href="https://www.ru.is/~mmh/">Magnus Mar Halldorsson</a>, Reyjavik University, Iceland: for his contributions to the theory of approximation and graph algorithms as well as to the study of wireless algorithmics. </li></ul>Congratulations to all of them! However, I trust that Mohammad and Pierpaolo will forgive me if I devote this post to celebrate Magnus, his work and his contributions to the TCS community.<br/><br/>So, why was Magnus chosen as one of the EATCS Fellows 2020? Here are some reasons why.<br/><br/>Magnús has offered seminal contributions to the theory of approximation and graph algorithms as well as to the study of wireless algorithmics. His research career and contributions so far can be roughly divided into two phases. The first phase spans the time from the beginning of his career until roughly ten years ago. During that time, Magnús made significant contributions to approximation algorithms for maximum independent set and graph colouring, amongst many other problems. In the second phase, which started a bit more than ten years ago, he has worked on the algorithmics of realistic models for wireless computation. I think that it is fair to say that Magnús is currently <i>the</i> expert on wireless algorithmics based on the SINR model.<br/><br/>These two phases are not at all disjoint. Indeed, the typical problems studied in the SINR model, such as determining the capacity of wireless networks or how to schedule messages in such networks, can be seen as independent set and colouring problems, respectively, and his experience with those problems in graph algorithmics certainly helped Magnús in obtaining breakthrough results in wireless algorithmics. Throughout his career, Magnús has also given significant contributions to the computation of independent sets and colourings in restricted computational models, such as the online model of computation and the data streaming model. His sustained research productivity, both in quality and in quantity, is all the more remarkable since it has largely been achieved working in the difficult research environment in Iceland, where he was largely isolated until the establishment of the <a href="http://icetcs.ru.is/">Icelandic Centre of Excellent in Theoretical Computer Science (ICE-TCS)</a> in 2005. (Magnus has been the scientific director of the centre for 15 years.)<br/><br/>In addition to his seminal research achievements, Magnús has served the theoretical computer science community by sitting on prestigious award committees, organizing conferences and workshops in Iceland and elsewhere, serving on steering committees and by acting as an inspiring mentor for young researchers in computer science who have come to Iceland explicitly to work with him. By way of example, Magnús is a member of the steering committees for SIROCCO (chair), ALGOSENSORS and SWAT, Scandinavian Symposium and Workshops on Algorithm Theory (chair). He was a member of the Council of the EATCS and has organized the best attended ICALP conference to date (ICALP 2008). Amongst many other such duties, he was PC chair for Track C of ICALP 2015 and of ESA 2011.<br/><br/>Magnús was also one of the initiators and first editors of “<a href="http://www.nada.kth.se/%20%CC%83viggo/wwwcompendium/">A compendium of NP optimization problems”</a>, which is catalog of approximability results for NP optimization problems and has been a useful resource for researchers in that field for a long time. <br/><br/>Summing up, Magnús is a true stalwart of the algorithmics research community, and a great example for many of us. In my, admittedly biased, opinion, he richly deserves the recognition of being named an EATCS Fellow. I have no doubt that he will continue to lead by example in the coming years.<br/><br/><br/><br/></div>
    </content>
    <updated>2020-04-01T21:54:00Z</updated>
    <published>2020-04-01T21:54:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-04-09T10:14:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=19719</id>
    <link href="https://gilkalai.wordpress.com/2020/04/01/an-update-from-israel-and-memories-from-singapore-partha-dasgupta-robin-mason-frank-ramsey-and-007/" rel="alternate" type="text/html"/>
    <title>A small update from Israel and memories from Singapore: Partha Dasgupta, Robin Mason, Frank Ramsey, and 007</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A small update about the situation here in Israel Eight weeks ago I wrote that my heart goes out to the people of Wuhan and China, and these days my heart goes out to people in Italy, Spain, the US, … <a href="https://gilkalai.wordpress.com/2020/04/01/an-update-from-israel-and-memories-from-singapore-partha-dasgupta-robin-mason-frank-ramsey-and-007/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3>A small update about the situation here in Israel</h3>
<p>Eight weeks ago <a href="https://gilkalai.wordpress.com/2020/02/03/thinking-about-the-people-of-wuhan-and-china/">I wrote</a> that my heart goes out to the people of Wuhan and China, and these days my heart goes out to people in Italy, Spain, the US, Iran, France, the United Kingdom, Germany, Netherland and many other countries all over the world. Of course, I am especially worried about the situation here in my beloved country Israel, and let me tell you a little about it.</p>
<p>The pandemic started here late but it hit us pretty hard with 5,358 identified cases yesterday. Severe measures of social distancing were gradually introduced, and right now it is too early to tell if the pandemic is under control.</p>
<p>My part in this struggle is to stay at home. (Many Israeli scientists are making various endeavors and proposing ideas of various kind for fighting the disease and I salute them all for their efforts.) Like all of us I am very thankful to medical and other essential workers who are in the front-lines. As a scientist, I am especially impressed by the people from the Ministry of Health who manage the crisis and communicate with the public. They represent the very best we can offer in terms of science and medicine, decision making, gathering information, communicating with the public, and managing the crisis. In the picture below you can see three of the leaders – Moshe Bar Siman Tov (middle) Prof. Itamar Grotto (right) and Professor Sigal Sadetzki (left).</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/03/listen.png"><img alt="" class="alignnone size-full wp-image-19695" height="371" src="https://gilkalai.files.wordpress.com/2020/03/listen.png?w=640&amp;h=371" width="640"/></a></p>
<h2>And now for today’s post</h2>
<p>We had a tradition of sharing entertaining taxi-and-more stories and this post belongs to this category. We note that our highest quality story teller Michal Linial, a prominent Israeli biologist, is now involved in various aspects of the struggle against the disease. Our post today is part of<a href="https://gilkalai.files.wordpress.com/2020/03/gil-michal.docx"> a report by Michal Feldman and me on our experience from the ICA3 conferences in Singapore and Birmingham</a>.</p>
<h2>Partha Dasgupta, Robin Mason, Frank Ramsey, and James Bond</h2>
<p>After hearing about him for many years, it was a great pleasure for both Michal Feldman and myself to finally meet Partha Dasgupta in person and to listen to his lecture. Partha who is the Frank Ramsey Professor of Economics at Cambridge was introduced by a person, who entered the room directly from an intercontinental flight, whom we did not know but who made a strong impression on us. He devoted part of his introduction to Frank Ramsey who was a mathematician, philosopher and economist, and who had made fundamental contributions to algebra and had developed the canonical model of saving in economic growth, before he died at the young age of 26. (And yes! also Ramsey’s theorem!)</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/03/james-bond.png"><img alt="" class="alignnone size-full wp-image-19699" height="423" src="https://gilkalai.files.wordpress.com/2020/03/james-bond.png?w=640&amp;h=423" width="640"/></a></p>
<p>Seeing the introducer, Robin Mason, three words came into our minds (more precisely two words, one repeated twice): “Bond, James Bond.”</p>
<p>Indeed, this has led to the following sequence of profound ideas:</p>
<p>1) Robin Mason is a perfect choice for a new generation James Bond.</p>
<p>2) The name “James Bond” is overused. “Robin Mason” is a perfect name to replace the name “James Bond”.</p>
<p>3) Espionage is a little obsolete and it lost much of its prestige and charm. Science and academia is the new thing! An international interdisciplinary academics is the perfect profession which, at present, deserves the prestige formely associated with espionage.</p>
<p>In summary, we came a full circle. Robin Mason is the perfect new choice for James Bond, “Robin Mason” is the perfect new name to replace the name “James Bond,” and Mason’s academic activities and title of Pro-Vice-Chancellor (International) are the perfect replacement for Bond’s activities and the title ‘007’.</p>
<p>(The option of Mason playing his role on the movies rather than in real life should be considered. ‘Q’ could be handy for science as well. )</p>
<p><a href="https://youtu.be/dMSHmHc0z-E">Clique here</a> for Robin’s introduction and Partha’s lectur</p></div>
    </content>
    <updated>2020-04-01T19:44:35Z</updated>
    <published>2020-04-01T19:44:35Z</published>
    <category term="Algebra"/>
    <category term="Combinatorics"/>
    <category term="Conferences"/>
    <category term="Economics"/>
    <category term="Taxi-and-other-stories"/>
    <category term="Updates"/>
    <category term="Frank Ramsey"/>
    <category term="Itamar Grotto"/>
    <category term="Michal Feldman"/>
    <category term="Michal Linial"/>
    <category term="Moshe Bar Siman Tov"/>
    <category term="Partha Dasgupta"/>
    <category term="Robin Mason"/>
    <category term="Sigal Sadetzki"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-04-11T04:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blog.simons.berkeley.edu/?p=146</id>
    <link href="https://blog.simons.berkeley.edu/2020/04/lattice-blog-reduction-part-i-bkz/" rel="alternate" type="text/html"/>
    <title>Lattice Blog Reduction – Part I: BKZ</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is the first entry in a (planned) series of at least three, potentially four or five, posts about lattice block reduction. The purpose of this series is to give a high level introduction to the most popular algorithms and … <a href="https://blog.simons.berkeley.edu/2020/04/lattice-blog-reduction-part-i-bkz/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is the first entry in a (planned) series of at least three, potentially four or five, posts about lattice block reduction. The purpose of this series is to give a high level introduction to the most popular algorithms and their analysis, with pointers to the literature for more details. The idea is to start with the obvious – the classic BKZ algorithm. In the next two posts we will look at two lesser known algorithm, which allow to highlight useful tools in lattice reduction. These three posts will focus on provable results. I have not decided how to proceed from there, but I could see the series being extended to topics involving heuristic analyses, practical considerations, and/or a survey of more exotic algorithms that have been considered in the literature.</p>
<h4 id="target-audience">Target Audience</h4>
<p>I will assume that readers of this series are already familiar with basic concepts of lattices, e.g. bases, determinants, successive minima, Minkowski’s bound, Gram-Schmidt orthogonalization, dual lattices and dual bases, etc. If any of these concepts seem new to you, there are great resources to familiarize yourself with them first (see e.g. lecture notes by <a href="http://cseweb.ucsd.edu/classes/fa19/cse206A-a/">Daniele</a>, <a href="https://cims.nyu.edu/~regev/teaching/lattices_fall_2009/index.html">Oded</a>, <a href="https://homepages.cwi.nl/~dadush/teaching/lattices-2018/">Daniel/Léo</a>). It will probably help if you are familiar with the LLL algorithm (also covered in aforementioned notes), but I’ll try to phrase everything so it is understandable even if if you aren’t.</p>
<p>Ok, so let’s get started. Before we look at BKZ in particular, first some comments about lattice block reduction in general.</p>
<h1 id="sec:basics">The Basics</h1>
<h4 id="the-goal">The Goal</h4>
<p>Why would anyone use block reduction? There are (at least) two reasons.</p>
<h4 id="section"/>
<p>1) Block reduction allows you to find short vectors in a lattice. Recall that finding the shortest vector in a lattice (i.e. solving SVP) is really hard (as far as we know, this takes at least <span class="math inline">\(2^{\Omega(n)}\)</span> time or even <span class="math inline">\(n^{\Omega(n)}\)</span> if you are not willing to also spend exponential amounts of memory). On the other hand, finding somewhat short vectors that are longer than the shortest vector by “only” an exponential factor is really easy (see LLL). So what do you do if you need something that is shorter than what LLL gives you, but you don’t have enough time to actually find the shortest vector? (This situation arises practically every time you use lattice reduction for cryptanalysis.) You can try to find something in between and hope that it doesn’t take as long. This is where lattice reduction comes in: it gives you a smooth trade-off between the two settings. It is worth mentioning that when it comes to approximation algorithms, block reduction is essentially the only game in town, i.e. there are, as far as I know, no non-trivial approximation algorithms that cannot be viewed as block reduction. (In fact, this is related to an open problem that Noah stated during the program: to come up with a non-trivial approximation algorithm that does not rely on a subroutine to find the shortest lattice vector in smaller dimensions.) The only exception to this are quantum algorithms that are able to find subexponential approximations in polynomial time in lattices with certain (cryptographically highly relevant) structure (see <span class="citation">[CDPR16]</span> and follow up work).</p>
<h4 id="section-1"/>
<p>2) Block reduction actually gives you more than just short vectors. It gives you guarantees on the “quality” of the basis. What do we mean by the quality of the basis? Consider the Gram-Schmidt vectors <span class="math inline">\({\mathbf{b}}_i^*\)</span> (GSO vectors) associated to a lattice basis <span class="math inline">\({\mathbf{B}}\)</span>. What we want is that the length of these Gram-Schmidt vectors (the GSO norms) does not drop off too quickly. The reason why this is a useful measure of quality for lattice bases is that it gives a sense of how orthogonal the basis vectors are: conditioned on being bases of the same lattice, the less accentuated the drop off in the GSO vectors, the more orthogonal the basis, and the more useful this basis is to solve several problems in a lattice. In fact, recall that the product of the GSO norms is equal to the determinant of the lattice and thus remains constant. Accordingly, if the GSO norms do not drop off too quickly, the first vector can be shown to be relatively short. So by analyzing the quality of the basis that block reduction achieves, a guarantee on the length of the first vector comes for free (see goal 1)). If you are familiar with the analysis of LLL, this should not come as a surprise to you.</p>
<h4 id="tools">Tools</h4>
<p>In order to ensure that the GSO norms do not drop off to quickly, it seems useful to be able to reduce them locally. To this end, we will work with projected lattice blocks (this is where the term “block” in block reduction comes from). More formally, given a basis <span class="math inline">\({\mathbf{B}}\)</span> we will consider the block <span class="math inline">\({\mathbf{B}}_{[i,j]}\)</span> for <span class="math inline">\(i &lt; j\)</span> as the basis formed by the basis vectors <span class="math inline">\({\mathbf{b}}_i, {\mathbf{b}}_{i+1}, \dots, {\mathbf{b}}_{j}\)</span> <em>projected orthogonally to the first <span class="math inline">\(i-1\)</span> basis vectors</em>. So <span class="math inline">\({\mathbf{B}}_{[i,j]}\)</span> is a basis for the lattice given by the sublattice formed by <span class="math inline">\({\mathbf{b}}_1, {\mathbf{b}}_{2}, \dots, {\mathbf{b}}_{j}\)</span> projected onto the orthogonal subspace of the vectors <span class="math inline">\({\mathbf{b}}_1, {\mathbf{b}}_{2}, \dots, {\mathbf{b}}_{i-1}\)</span>. Notice that the first vector of <span class="math inline">\({\mathbf{B}}_{[i,j]}\)</span> is exactly <span class="math inline">\({\mathbf{b}}^*_i\)</span> – the <span class="math inline">\(i\)</span>-th GSO vector. Another way to view this is to consider the QR-factorization of <span class="math inline">\({\mathbf{B}} = {\mathbf{Q}} {\mathbf{R}}\)</span>, where <span class="math inline">\({\mathbf{B}}\)</span> is the matrix whose columns are the basis vectors <span class="math inline">\({\mathbf{b}}_i\)</span>. Since <span class="math inline">\({\mathbf{Q}}\)</span> is orthonormal, it represents a rotation of the lattice and we can consider the lattice generated by the columns of <span class="math inline">\({\mathbf{R}}\)</span> instead, which is an upper triangular matrix. For an upper triangular basis, the projection of a basis vector orthogonal to the previous basis vectors simply results in dropping the first entries from the vector. So considering a projected block <span class="math inline">\({\mathbf{R}}_{i,j}\)</span> is simply to consider the square submatrix of <span class="math inline">\({\mathbf{R}}\)</span> consisting of the rows and columns with index <span class="math inline">\(k\)</span> between <span class="math inline">\(i \leq k \leq j\)</span>.</p>
<p>Now we need a tool that allows us to control these GSO vectors, which we view as the first basis vectors in projected sublattices. For this, we will fall back to algorithms that solve SVP. Recall that this is very expensive, so we will not call this on the basis <span class="math inline">\({\mathbf{B}}\)</span> but rather on the projected blocks <span class="math inline">\({\mathbf{B}}_{[i,j]}\)</span>, where we ensure that the dimension <span class="math inline">\(k = j-i+1\)</span> of the lattice generated by this projected block is not too large. In fact, the maximum dimension <span class="math inline">\(k\)</span> that we call the SVP algorithm on will control the time/quality trade-off achieved by our block reduction algorithms and is usually denoted by the block size. So we will assume that we have access to such an SVP algorithm. Actually, we will assume something slightly stronger: we will assume access to a subroutine that takes as input the basis <span class="math inline">\({\mathbf{B}}\)</span> and indices <span class="math inline">\(i,j\)</span> and outputs a basis <span class="math inline">\({\mathbf{C}}\)</span> such that</p>
<ul>
<li><p>the lattice generated by the basis remains the same</p></li>
<li><p>the first <span class="math inline">\(i-1\)</span> and the last vectors starting from <span class="math inline">\(j+1\)</span> remain unchanged</p></li>
<li><p>the projected block <span class="math inline">\({\mathbf{C}}_{[i,j]}\)</span> is <em>SVP reduced</em>, meaning that <span class="math inline">\({\mathbf{c}}^*_i\)</span> is the shortest vector in the lattice generated by <span class="math inline">\({\mathbf{C}}_{[i,j]}\)</span>. Additionally, if <span class="math inline">\({\mathbf{B}}_{[i,j]}\)</span> is already SVP reduced, we assume that the basis <span class="math inline">\({\mathbf{B}}\)</span> is left unchanged.</p></li>
</ul>
<p>We will call an algorithm that achieves this an <em>SVP oracle</em>. Such an oracle can be implemented given any algorithm that solves SVP (for arbitrary lattices). The technical detail of filling in the gap is left as homework to the reader.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-156" height="315" src="https://blog.simons.berkeley.edu/wp-content/uploads/2020/04/svp.png" width="267"/>Effect of a call to the SVP oracle. GSO log norms of the input in black, of the output in red. Note that the sum of the GSO log norms is a constant, so reducing the first vector, increases the (average of the) remaining vectors.</figure></div>



<p>For the analysis we need to know what such an SVP oracle buys us. This is where Minkowski’s theorem comes in: we know that for any <span class="math inline">\(n\)</span>-dimensional lattice <span class="math inline">\(\Lambda\)</span> we have <span class="math inline">\(\lambda_1(\Lambda) \leq \sqrt{\gamma_n} \det(\Lambda)^{1/n}\)</span> (where <span class="math inline">\(\lambda_1(\Lambda)\)</span> is the length of the shortest vector in <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\gamma_n = \Theta(n)\)</span> is Hermite’s constant). This tells us that after we’ve applied the SVP oracle to a projected block <span class="math inline">\({\mathbf{B}}_{[i,i+k-1]}\)</span>, we have <span class="math display">\[\|{\mathbf{b}}^*_i \| \leq \sqrt{\gamma_{k}} \left(\prod_{j = i}^{i+k-1} \|{\mathbf{b}}_j^* \| \right)^{1/k}.\]</span> Almost all of the analyses of block reduction algorithms, at least in terms of their output quality, rely on this single inequality.</p>
<h4 id="disclaimer">Disclaimer</h4>
<p>Before we finally get to talk about BKZ, I want to remark that throughout this series I will punt on a technical (but very important) topic: the number of arithmetic operations (outside of the oracle calls) and the size of the numbers. The number of arithmetic operations is usually not a problem, since it will be dominated by the calls to the SVP oracle. We will only compute projections of sublattices corresponding to projected blocks as described above to pass them to the oracle, which can be done efficiently using the Gram-Schmidt orthogonalization. The size of the numbers is a more delicate issue. We need to ensure that the required precision for these projections does not explode somehow. This is usually addressed by interleaving the calls to the SVP oracle with calls to LLL. If you are familiar with the LLL algorithm, it should be intuitive that this allows to control the size of the number. For a clean example of how this can be handled, we refer to e.g. <span class="citation">[GN08a]</span>. So, in summary, we will measure the running time of our algorithms thoughout simply in the number of calls to the SVP oracle.</p>
<h1 id="sec:bkz">BKZ</h1>
<p>Schnorr <span class="citation">[S87]</span> introduced the concept of BKZ reduction in the 80’s as a generalization of LLL. The first version of the BKZ algorithm as we consider it today was proposed by Schnorr and Euchner <span class="citation">[SE94]</span> a few years later. With our setup above, the algorithm can be described in a very simple way. Let <span class="math inline">\({\mathbf{B}}\)</span> be a lattice basis of an <span class="math inline">\(n\)</span>-dimensional lattice and <span class="math inline">\(k\)</span> be the block size. Recall that this is a parameter that will determine the time/quality trade-off as we shall see in the analysis. We start by calling the SVP oracle on the first block <span class="math inline">\({\mathbf{B}}_{[1,k]}\)</span> of size <span class="math inline">\(k\)</span>. Once this block is SVP reduced, we shift our attention to the next block <span class="math inline">\({\mathbf{B}}_{[2,k+1]}\)</span> and call the oracle on that. Notice that SVP reduction of <span class="math inline">\({\mathbf{B}}_{[2,k+1]}\)</span> may change the lattice generated by <span class="math inline">\({\mathbf{B}}_{[1,k]}\)</span> and <span class="math inline">\({\mathbf{b}}_1\)</span> may not be the shortest vector in the first block anymore, i.e. it can potentially be reduced even further. However, instead of going back and fixing that, we will simply leave this as a problem to “future us”. For now, we continue in this fashion until we reach the end of the basis, i.e. until we called the oracle on <span class="math inline">\({\mathbf{B}}_{n-k,n}\)</span>. Note that so far this can be viewed as considering a constant sized window moving from the start of the basis to the end and reducing the first vector of the projected block in this window as much as possible using the oracle. Once we have reached the end of the basis, we start reducing the window size, i.e. we call the oracle on <span class="math inline">\({\mathbf{B}}_{n-k+1,n}\)</span>, then on <span class="math inline">\({\mathbf{B}}_{n-k+2,n}\)</span>, etc. This whole process is called a <em>BKZ tour</em>.</p>
<p>Now that we have finished a tour, it is time to go back and fix the blocks that are not SVP reduced anymore. We do this simply by running another tour. Again, if the second tour modified the basis, there is no guarantee that all the blocks are SVP redcued. So we simply repeat, and repeat, and … you get the idea. We run as many tours as required until the basis does not change anymore. That’s it. If this looks familiar to you, that’s not a coincidence: if we plug in <span class="math inline">\(k=2\)</span> as our block size, we obtain (a version of) LLL! So BKZ is a proper generalization of LLL.</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-157" src="https://blog.simons.berkeley.edu/wp-content/uploads/2020/04/bkz.png"/>BKZ in one picture: apply the SVP oracle to the projected blocks from start to finish and when you reach the end, repeat.</figure>



<p>The obvious questions now are: what can we expect from the output? And how long does it take?</p>
<h4 id="the-good">The Good</h4>
<p>We will now take a closer look at the approximation factor achieved by BKZ. If you want to follow this analysis along, you might want to get out pen and paper. Otherwise, feel free to trust me on the calculations (I wouldn’t!) and/or jump ahead to the end of this section for the result (no spoilers!). Let’s assume for now that the BKZ algorithm terminates. If it does, we know that the projected block <span class="math inline">\({\mathbf{B}}_{[i, i+k-1]}\)</span> is SVP reduced for every <span class="math inline">\(i \in [1,\dots,n-k+1]\)</span>. This means that we have <span class="math display">\[\|{\mathbf{b}}^*_i \|^k \leq \gamma_{k}^{k/2} \prod_{j = i}^{i+k-1} \|{\mathbf{b}}_j^* \|\]</span> for all these <span class="math inline">\(n-k+1\)</span> values of <span class="math inline">\(i\)</span>. Multiplying all of these inequalities and canceling terms gives the inequality <span class="math display">\[\|{\mathbf{b}}^*_1 \|^{k-1}\|{\mathbf{b}}^*_2 \|^{k-2} \dots \|{\mathbf{b}}^*_{k-1} \| \leq \gamma_{k}^{\frac{(n-k+1)k}{2}} \|{\mathbf{b}}_{n-k+2}^* \|^{k-1} \|{\mathbf{b}}_{n-k+3}^* \|^{k-2} \dots \|{\mathbf{b}}_{n}^* \|.\]</span> Now we make two more observations: 1) not only is <span class="math inline">\({\mathbf{B}}_{[1, k]}\)</span> SVP reduced, but so is <span class="math inline">\({\mathbf{B}}_{[1, i]}\)</span> for every <span class="math inline">\(i &lt; k\)</span>. (Why? Think about it for 2 seconds!) This means we can multiply the inequalities <span class="math display">\[\|{\mathbf{b}}^*_1 \|^i \leq \gamma_{i}^{i/2} \prod_{j = 1}^{i} \|{\mathbf{b}}_j^* \|\]</span> for all <span class="math inline">\(i \in [2,k-1]\)</span> together with the trivial inequality <span class="math inline">\(\|{\mathbf{b}}^*_1 \| \leq \|{\mathbf{b}}^*_1 \|\)</span>, which gives <span class="math display">\[\|{\mathbf{b}}^*_1 \|^{\frac{k(k-1)}{2}} \leq \left(\prod_{i = 2}^{k-1} \gamma_{i}^{i/2} \right) \prod_{i = 1}^{k-1} \|{\mathbf{b}}_i^* \|^{k-1}\]</span> Now we use the fact that <span class="math inline">\(\gamma_k^k \geq \gamma_i^i\)</span> for all <span class="math inline">\(i \leq k\)</span> (Why? Homework!) and combine with our long inequality above to get <span class="math display">\[\|{\mathbf{b}}^*_1 \|^{\frac{k(k-1)}{2}} \leq \gamma_k^{\frac{k(n-1)}{2}} \|{\mathbf{b}}_{n-k+2}^* \|^{k-1} \|{\mathbf{b}}_{n-k+3}^* \|^{k-2} \dots \|{\mathbf{b}}_{n}^* \|.\]</span> (I’m aware that this is a lengthy calculation for a blog post, but we’re almost there, so bear with me. It’s worth it!)</p>
<p>We now use one final observation, which is a pretty common trick in lattice algorithms: w.l.o.g. assume that for some shortest vector <span class="math inline">\({\mathbf{v}}\)</span> in our lattice its projection orthogonal to the first <span class="math inline">\(n-1\)</span> basis vectors is non-zero (if it is zero for all of the shortest vectors, simply drop the last vector from the basis, the result is still BKZ reduced, so use induction). Then we must have that <span class="math inline">\(\lambda_1 = \| {\mathbf{v}} \| \geq \|{\mathbf{b}}_i^* \|\)</span> for all <span class="math inline">\(i \in [n-k+2, \dots, n]\)</span>, since otherwise the projected block <span class="math inline">\({\mathbf{B}}_{i,n}\)</span> would not be SVP reduced. This means, we have <span class="math inline">\(\lambda_1 \geq \max_{i \in [n-k+2, \dots, n]} \|{\mathbf{b}}_i^* \|\)</span>. This is the final puzzle piece to get our approximation bound: <span class="math display">\[\|{\mathbf{b}}^*_1 \| \leq \gamma_{k}^{\frac{n-1}{k-1}} \lambda_1.\]</span> Note that this analysis (dating back to Schnorr <span class="citation">[S94]</span>) is reminiscent of the analysis of LLL and if we plug in <span class="math inline">\(k=2\)</span>, we get exactly what we’d expect from LLL. Though we do note a gap in the other extreme: if we plug in <span class="math inline">\(k=n\)</span>, we know that the approximation factor is <span class="math inline">\(1\)</span> (we are solving SVP in the entire lattice), but the bound above yields a factor <span class="math inline">\(\gamma_n = \Theta(n)\)</span>.</p>
<h4 id="the-bad">The Bad</h4>
<p>Now that we’ve looked at the output quality of the basis, let’s see what we can say about the running time (recall that our focus is on the number of calls to the SVP oracle). The short answer is: not much and that’s very unfortunate. Ideally, we’d want a bound on the number of SVP calls that is polynomial in <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span>. This would mean that the overall running time for large <span class="math inline">\(k\)</span> is dominated by the running time of the SVP oracle in dimension <span class="math inline">\(k\)</span> and the block size would give us exactly the expected trade-off. However, an LLL style analysis has so far only yielded a bound on the number of tours which is <span class="math inline">\(O(k^n)\)</span> <span class="citation">[HPS11, Appendix]</span>. This is quite bad – for large <span class="math inline">\(k\)</span> the number of calls will be the dominating factor in the running time.</p>
<h4 id="the-ugly">The Ugly</h4>
<p>Recall that the analysis of LLL does not only provide a bound on the approximation factor, but also on the Hermite factor, i.e. on the ratio of <span class="math inline">\(\| {\mathbf{b}}_1\|/\det(\Lambda)^{1/n}\)</span>. Since an LLL-style analysis worked out nicely for the approximation factor of BKZ, it stands to reason that a similar analysis should yield a similar bound for BKZ. By extrapolating from LLL, one could expect a bound along the lines of <span class="math inline">\(\| {\mathbf{b}}_1\|/\det(\Lambda)^{1/n} \leq \gamma_{k}^{n/2k}\)</span> (note the square root improvement w.r.t. the trivial bound obtained from the approximation factor). And, in fact, a bound of <span class="math inline">\(\gamma_{k}^{\frac{n-1}{2(k-1)} + 1}\)</span> has been claimed in <span class="citation">[GN08b]</span> but without proof (as pointed out in <span class="citation">[HPS11]</span>) and it is not clear, how one would prove this. (<span class="citation">[GN08b]</span> claims that one can use a similar argument as we did for the approximation factor, but I don’t see it.)</p>
<h4 id="the-rescue">The Rescue</h4>
<p>So it seems different techniques are necessary to complete the analysis of BKZ. The work of <span class="citation">[HPS11]</span> introduced such a new technique based on the analysis of dynamical systems. This work applied the technique successfully to BKZ, but the analysis is quite involved. What it shows is that one can terminate BKZ after a polynomial number of tours and still get a guarantee on the output quality, which is very close to the conjectured bound on the Hermite factor above. (Caveat: Technically, <span class="citation">[HPS11]</span> only showed this result for a slight variant of BKZ, but the difference to the standard BKZ algorithm only lies in the scope of the interleaving LLL applications, which is something that we glossed over above.) This is in line with experimental studies <span class="citation">[SE94,GN08b,MW16]</span>, which show that BKZ produces high quality bases after a few tours already.</p>
<p>We will revisit this approach when considering a different block reduction variant, SDBKZ, where the analysis is much cleaner. As a teaser for the next post though, recall that BKZ can be viewed as a generalization of LLL (which corresponds to BKZ with block size <span class="math inline">\(k=2\)</span>). Since the analysis of LLL did not carry entirely to BKZ, one could wonder if there is a different generalization of LLL such that an LLL-style analysis also generalizes naturally. The answer to this is yes, and we will consider such an algorithm in the next post.</p>



<ul><li>[CDPR16] Cramer, Ducas, Peikert, Regev. <em>Recovering short generators of principal ideals in cyclotomic rings.</em> EUROCRYPT 2016</li><li>[GN08a] Gama, Nguyen. <em>Finding short lattice vectors within Mordell’s inequality</em>. STOC 2008</li><li>[GN08b] Gama, Nguyen. <em>Predicting lattice reduction</em>. EUROCRYPT 2008</li><li>[HPS11] Hanrot, Pujol, Stehlé.<em> Analyzing blockwise lattice algorithms using dynamical systems.</em> CRYPTO 2011</li><li>[MW16] Micciancio, Walter. <em>Practical, predictable lattice basis reduction.</em> EUROCRYPT 2016</li><li>[SE94] Schnorr, Euchner. <em>Lattice basis reduction: Improved practical algorithms and solving subset sum problems.</em> Mathematical Programming 1994</li><li>[S87] Schnorr. <em>A hierarchy of polynomial time lattice basis reduction algorithms.</em> Theoretical Computer Science 1987</li><li>[S94] Schnorr. <em>Block reduced lattice bases and successive minima.</em> Combinatorics, Probability and Computing 1994</li></ul></div>
    </content>
    <updated>2020-04-01T15:17:39Z</updated>
    <published>2020-04-01T15:17:39Z</published>
    <category term="General"/>
    <category term="BKZ"/>
    <category term="Lattice Block Reduction"/>
    <author>
      <name>Michael Walter</name>
    </author>
    <source>
      <id>https://blog.simons.berkeley.edu</id>
      <link href="https://blog.simons.berkeley.edu/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blog.simons.berkeley.edu" rel="alternate" type="text/html"/>
      <subtitle>What's New at the Simons Institute for the Theory of Computing.</subtitle>
      <title>Calvin Café: The Simons Institute Blog</title>
      <updated>2020-04-10T23:51:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16904</id>
    <link href="https://rjlipton.wordpress.com/2020/04/01/research-at-home/" rel="alternate" type="text/html"/>
    <title>Research at Home</title>
    <summary>An idea for human-interest interviews Pixabay free src Dr. Lofa Polir is, like many of us, working from home. When we last wrote about her two years ago, she had started work for the Livingston, Louisiana branch of LIGO. They sent her and the rest of the staff home on March 19 and suspended observations […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>An idea for human-interest interviews</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/01/research-at-home/femalefool/" rel="attachment wp-att-16906"><img alt="" class="alignright wp-image-16906" height="213" src="https://rjlipton.files.wordpress.com/2020/03/femalefool.png?w=137&amp;h=213" width="137"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Pixabay free <a href="https://pixabay.com/illustrations/toon-figure-female-fool-funny-4292442/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Dr. Lofa Polir is, like many of us, working from home. When we last <a href="https://rjlipton.wordpress.com/2018/04/01/the-entropy-of-baseball/">wrote</a> about her two years ago, she had started work for the Livingston, Louisiana branch of <a href="https://www.ligo.caltech.edu/page/about">LIGO</a>. They sent her and the rest of the staff home on March 19 and <a href="https://www.ligo.caltech.edu/news/ligo20200326">suspended</a> observations on the 26th. Since Polir’s duties already included public outreach, she is looking to continue that online.</p>
<p>
Today we helped Dr. Polir interview another pandemic-affected researcher.</p>
<p>
We liked her idea of interviewing young people just starting their careers, who are facing unexpected uncertainties. Her first choice was a new graduate of Cambridge University doing fundamental work related to LIGO. Unfortunately, he had been unable to install a current version of Zoom on his handheld device, or maybe afraid owing to security <a href="https://twitter.com/random_walker/status/1244987617676050434?s=11">issues</a>. So she requested the special equipment we have used to <a href="https://rjlipton.wordpress.com/2011/10/31/an-interview-with-kurt-gdel/">interview</a> people <a href="https://rjlipton.wordpress.com/2012/11/03/more-interview-with-kurt-godel/">in</a> the <a href="https://rjlipton.wordpress.com/2013/11/15/the-graph-of-math/">past</a>. </p>
<p>
He replied at the speed of light that he was willing to do the interview so long as we respected some privacy measures. As for what name to use, he said we could just call him Izzy—Izzy Jr., in fact. So Dick, I, and Dr. Polir all used our own Zoom to port into our machine’s console room. The connection worked right away as Izzy’s head glimmered into view.</p>
<p>
</p><p/><h2> Starting The Interview </h2><p/>
<p/><p>
At first glimpse, all we could see was his long, light-brown hippie hair. This really surprised us—not the image we had of Cambridge—and we gasped about it before even saying hello. He replied that it was fashion from the Sixties. We asked how his family was doing and he said his fathers had passed on but mother and young siblings were at home and fine. We think he said “fathers” plural—the machine rendered him in a drawl like Mick Jagger and he was hard to follow.</p>
<p>
Izzy picked up on our discomfort and immediately assured us he hadn’t been doing any drugs: “You can’t get them anyway because they’re all being diverted to treat the sick.” But he did open up to us that he was in some kind of withdrawal. He confessed that he had resorted to looking at the sun with one eye. “It was ecstasy but bad—I still can see only reds and blues with that eye, and I need to use an extra-large rectangular cursor to read text.” We were curious what brand of handheld device he was using because of his problems with Zoom, and he told us it was a Napier 1660 by <a href="http://www.oughtred.org/history.shtml">Oughtred, Ltd.</a> We hadn’t heard of that model but he said he’d connected three of them into a good home lab setup.</p>
<p>
We asked how he was coping with distance teaching, but he said he hadn’t yet started his faculty position at Trinity College. We were surprised to learn that lecture attendance at Cambridge University is optional. “I shall be required to give the lectures but nobody will come to them so that’s all the same now—at least here I’ll have a cat for audience. No dogs and not my mother or siblings—I’d sooner burn the house down.” He quickly added, “Oh, my mother and I get along fine now and I love playing teatime with my little sisters.”</p>
<p>
We really didn’t want to go into Izzy’s personal life, and I tried to shift the small-talk by noting a little chess set on a shelf behind him. He snapped that he shouldn’t have spent money on it and he was a poor player anyway. We thought, wow, either this guy’s really down on himself or the cabin fever of the pandemic is getting to him. So Dick, always quick to pick up on things and find ways of encouragement, said:</p>
<p>
“Dr. Polir here works on gravity and we’re told you have some great new ideas about it. We’d love to hear them.”</p>
<p>
“Yes, I do—or did. But something happened yesterday that is making me realize that it’s all wrong, rubbish really…”</p>
<p>
</p><p/><h2> In the Garden </h2><p/>
<p/><p>
Izzy started by explaining that it’s a basic principle of alchemy that all objects have humors that can manifest as kinds of magnetism. (“Alchemy”? did we hear him right?) If you realize that the Earth and Sun are objects just like any other then you can model gravity that way. You just need to assign each object a number called its “mass” and then you get the equation </p>
<p align="center"><img alt="\displaystyle  F = G \frac{m_1 m_2}{r^2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F+%3D+G+%5Cfrac%7Bm_1+m_2%7D%7Br%5E2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F = G \frac{m_1 m_2}{r^2} "/></p>
<p>for the force of attraction, where <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> is the distance between the objects with masses <img alt="{m_1,m_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm_1%2Cm_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m_1,m_2}"/> and <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is a constant that depends on your units.</p>
<p>
“We understand all that,” said Dr. Polir.</p>
<p>
Izzy said the point is that <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> depends <b>only</b> on your units and is the same regardless of where you are on Earth or on the Moon or wherever. It is very small, though. Then he went into his story of yesterday.</p>
<blockquote><p><b> </b> <em> “I was in our garden by the path to the neighbor’s farm. I was supposed to be watching my little brother Benjamin who wanted to help harvest squash but I hate farming so I let him go without me. I was lying under an apple tree for shade when an apple fell and I realized all my mistakes.” </em>
</p></blockquote>
<p/><p>
“What?,” we thought silently. We didn’t need to speak up—Izzy launched right into his litany of error:</p>
<blockquote><p><b> </b> <em> “First, I’d thought the force was in what made the apple fall, but that’s nonsense. The apple would fall naturally because down is the shortest path it would be on if the tree branch were not holding it back. The only force is the tensile strength of the branch which was restraining it. I think that the tensile force really is magnetism, by the way.”</em></p><em>
<p>
“Second, it’s ridiculous to think the force is coming <b>from</b> the Earth. On first principles, it could come from the ground, but that’s not what the equations say. They could have it all coming from one point in the <b>center</b> of the Earth. Just one point—four thousand miles deep!”</p>
</em><p><em>
“Third and worst, though, is when you apply it to the Sun and the Earth. My equation means they are exerting force on each other instantaneously. But they are millions of miles apart. Whereas, the tree was <b>touching</b> the apple. Force can work only by touch, not by some kind of spooky action at a distance.” </em>
</p></blockquote>
<p/><p>
We realized what he was driving at. Dick again always likes to encourage, so he said:</p>
<p>
“But the math you developed for this force theory—surely it is good for calculations…?”</p>
<blockquote><p><b> </b> <em> “No it’s not—it’s the Devil’s own box. I can calculate two bodies—the Earth and the Sun, or the Moon and the Earth if you suppose there is no Sun, but as soon as you have all three bodies it’s a bog. Worst of all, I can arrange five bodies so that one of them gets accelerated to infinite velocity—<a href="https://rjlipton.wordpress.com/2019/10/31/hobgoblins-in-our-equations/">in finite time</a>. This is a clear impossibility, a contradiction, so by <em>modus tollens</em>… it can all go in the bin.” </em>
</p></blockquote>
<p/><p>
We didn’t think it would help to tell him that his math was good enough to <a href="https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19720022040.pdf">calculate</a> a Moon landing but not to locate a friend’s house while driving. He supplied his own <em>coup-de-grâce</em> anyway:</p>
<blockquote><p><b> </b> <em> “And even the two-body calculations are tainted. I can calculate the orbits of the planets but the equations I get aren’t stable. I would wind up having to postulate something like God keeps the planets on their tracks. Yes, you need an intelligent Agent to start the planets going—all in one plane, basically—but to need such intervention all the time defeats the point of having equations.” </em>
</p></blockquote>
<p>
</p><p/><h2> Inklings </h2><p/>
<p/><p>
We asked Izzy what he was going to do. He said that the one blessing of enforced solitude is that one gets time to reflect on things and deepen the foundations. And he said he’d had an idea later that afternoon.</p>
<blockquote><p><b> </b> <em> “Toward supper I realized I needed to get Benjamin home. The path to the farm is straight except it goes over a mound. I was sauntering along and when I got to the hill I realized that if I didn’t watch it I’d have fallen right into it. So that got me thinking. First, what I thought was straight on the path was really a curve—the Earth is after all a ball. We think space is straight, but maybe it too is curved. So when I’m standing here, perhaps I would really be moving in a diagonally down direction, but the Earth is stopping me. The Irish blessing says, ‘may the road rise up to meet you.’ Perhaps it does.” </em>
</p></blockquote>
<p/><p>
“So are you doing math to work that out?,” I ventured.</p>
<blockquote><p><b> </b> <em> “I started after supper. One good thing is that it allows light to be affected by gravity—which I was already convinced of—even if light has no mass. But a problem is that it appears Time would have to be included as curved. That does not make sense either.” </em>
</p></blockquote>
<p/><p>
We asked when he might write up all this. He said he didn’t want to be quick to publish something so flawed on the one hand, or incomplete on the other, “unless someone else be about to publish the same.” We noted that there weren’t going to be any in-person conferences to present papers at for awhile anyway.</p>
<blockquote><p><b> </b> <em> “Besides, that’s not what I’m most eager to do. What the respite is really giving me time for is to start writing up my work on Theology. That’s most important—it could have stopped thirty years of war. For one thing, <a href="https://en.wikipedia.org/wiki/Homoiousios">homoiousios</a>, not <a href="https://en.wikipedia.org/wiki/Homoousios">homoousios</a>, is the right rendering. There will be a time and times and the dividing of times in under 400 years <a href="https://www.questia.com/library/journal/1G1-116141910/a-time-and-times-and-the-dividing-of-time-isaac">anyway</a>.” </em>
</p></blockquote>
<p/><p>
That last statement somehow did not reassure us. We thanked Izzy Jr. for the interview and he gave consent to publish it posthumously.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We hope that your April Fool’s Day is such as to allow a time to laugh. But also seriously, would you be interested in the idea of our interviewing people during these times? Is there anyone you would like to suggest?</p>
<p/></font></font></div>
    </content>
    <updated>2020-04-01T05:10:02Z</updated>
    <published>2020-04-01T05:10:02Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Teaching"/>
    <category term="April Fool"/>
    <category term="gravity"/>
    <category term="interview"/>
    <category term="Lofa Polir"/>
    <category term="outreach"/>
    <category term="pandemic"/>
    <category term="Physics"/>
    <category term="working from home"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-04-11T04:20:52Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/03/31/linkage</id>
    <link href="https://11011110.github.io/blog/2020/03/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Still at home, hoping the coffee arrives tomorrow as scheduled.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Still at home, hoping the coffee arrives tomorrow as scheduled.</p>

<ul>
  <li>
    <p><a href="https://twitter.com/scienceshitpost">Science diagrams that look like shitposts</a> (<a href="https://mathstodon.xyz/@11011110/103834852903279134"/>, <a href="https://www.metafilter.com/186088/Science-diagrams-that-look-like-shitposts">via</a>).</p>
  </li>
  <li>
    <p><a href="https://www.cambridge.org/core/what-we-publish/textbooks">All Cambridge University Press textbooks are free-to-read until May</a> (<a href="https://mathstodon.xyz/@JordiGH/103841002082854377"/>).</p>
  </li>
  <li>
    <p><a href="https://www.rweber.net/projects/non-gray-grayscales/">Non-gray grayscales</a> (<a href="https://mathstodon.xyz/@11011110/103848490584549963"/>). Rebecca Weber finds a method to produce off-gray colors in a range of lightness with visually-matching hues and saturations. It’s not just a matter of plotting a straight line in HSL colorspace. (Found while looking for more information on her book <em>Computability Theory</em>, but nowadays there isn’t much actual math on her website.)</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.08456">Convex hulls of random order types</a> (<a href="https://mathstodon.xyz/@11011110/103853512504928724"/>). This is one of my favorite papers in the list accepted to SoCG 2020, which now will be online-only. Point sets whose order type is uniformly random are different from randomly drawn points, and harder to study. Xavier Goaoc and Emo Welzl observe that the projective transformations of a random order type are equally likely, and use this idea to prove that these point sets typically have very small convex hulls.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Greedy_coloring">Greedy coloring</a> (<a href="https://mathstodon.xyz/@11011110/103856462402785120"/>). Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.theguardian.com/science/2019/oct/21/can-you-solve-it-the-four-points-two-distances-problem">The four points, two distances problem</a> (<a href="https://mathstodon.xyz/@11011110/103864906436854951"/>). Can you find all of the ways of arranging four distinct points in the plane so that they form only two distances? The link is not a spoiler but it has a separate link to the solution. “Nearly everyone misses at least one” says Peter Winkler; can you guess the one I missed?</p>
  </li>
  <li>
    <p><a href="https://mastodon.social/@sarielhp/103853624792571796">Sariel Har-Peled makes some suggestions for online conferences</a>: all papers above threshold should be accepted, rather than imposing artificial acceptance rates, and authors should provide versions of their talks in multiple lengths.</p>
  </li>
  <li>
    <p><a href="https://community.wolfram.com/groups/-/m/t/1904335">Minimal-stick examples of the knots , , , , and </a> (<a href="https://mathstodon.xyz/@shonk/103868182994018206"/>).</p>
  </li>
  <li>
    <p>Two linocut interpretations of a rhombic dodecahedron by the same artist, Josh Millard: <a href="https://mastodon.social/@joshmillard/103876129272051551">abstract</a> and <a href="https://mastodon.social/@joshmillard/103881150094999086">physical</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/103887968806441789"/>).</span></p>
  </li>
  <li>
    <p>Three recent “Did you know?” (<a href="https://mathstodon.xyz/@11011110/103893665729668173"/>):</p>

    <ul>
      <li>
        <p>… that <a href="https://en.wikipedia.org/wiki/Chiara_Daraio">Chiara Daraio</a> used Newton’s cradle to create sound bullets, and ball bearing filled walls to create one-way sound barriers?</p>
      </li>
      <li>
        <p>… that <a href="https://en.wikipedia.org/wiki/Heronian_tetrahedron">a tetrahedron with integer edge lengths, face areas, and volume</a> can be given integer coordinates?</p>
      </li>
      <li>
        <p>… that former college basketball star <a href="https://en.wikipedia.org/wiki/Amy_Langville">Amy Langville</a> is an expert in ranking systems, and has applied her ranking expertise to basketball bracketology?</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="http://www.mi.sanu.ac.rs/vismath/mart.htm">VisMath MathArt</a> (<a href="https://mathstodon.xyz/@11011110/103898916402221482"/>). Many linked galleries of images of mathematical art, from the 1990s-style web (occasional broken links and all).</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2020/03/robin-thomas.html">Sadly, graph theorist Robin Thomas has died</a> (<a href="https://mathstodon.xyz/@11011110/103901957462987629"/>).</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=IK7nBOLYzdE">What happens when half a cellular automaton runs Conway’s Game of Life and the other half runs a rolling version of Rule 30 pushing chaos across the border</a> (<a href="https://mathstodon.xyz/@11011110/103915202241542224"/>)? I wish I could see a larger scale of time and space to get an idea of how far the effects penetrate. If the boundary emitted gliders at a constant rate they’d collide far away in a form of ballistic annihilation but the boundary junk and glider-collision junk makes it more complicated.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/356220/440">Monotone subsets of uncountable plane sets</a> (<a href="https://mathstodon.xyz/@11011110/103919594138003386"/>). I ask on MathOverflow about infinite generalizations of the Erdős–Szekeres theorem on the existence of square-root-sized monotone subsets of finite sets of points in the plane.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-03-31T21:25:00Z</updated>
    <published>2020-03-31T21:25:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-04-01T04:49:01Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4912143677920692930</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4912143677920692930/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/03/length-of-descriptions-for-dfa-nfa-cfg.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4912143677920692930" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4912143677920692930" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/03/length-of-descriptions-for-dfa-nfa-cfg.html" rel="alternate" type="text/html"/>
    <title>Length of Descriptions for DFA, NFA, CFG</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
We will be looking at the size of descriptions<br/>
<br/>
 For DFAs and NFAs this is the number of states.<br/>
<br/>
For CFG's we will assume they are in Chomsky Normal Form. So for this post CFG means CFG in Chomsky normal form. The length of a Chomsky Normal Form CFL is the number of rules.<br/>
<br/>
1) It is known there is a family of languages L_n such that<br/>
<br/>
DFA for L_n requires roughly 2^n states.<br/>
<br/>
NFA for L_n can be done with roughly n states.<br/>
<br/>
L_n =  (a,b)^* a (a,b)^n<br/>
<br/>
Also note that there is a CFG for L_n with roughly n rules. (one can show this directly or by some theorem that goes from an NFA of size s to a CFG of size roughly s).<br/>
<br/>
So L_n shows there is an exp blowup between DFAs and NFA's<br/>
<br/>
2) It is known that there is a family of languages L_n such that<br/>
<br/>
DFA for L_n requires roughly 2^n states<br/>
<br/>
NFA for L_n requires roughly 2^n states<br/>
<br/>
CFG for L_n can be done with roughly n rules<br/>
<br/>
L_n = { a^{2^n}  }<br/>
<br/>
So L_n shows there is an exp blowup between NFAs and CFGs.<br/>
<br/>
<br/>
3) Is there a family of languages L_n such that<br/>
<br/>
NFA for L_n requires 2^{2^n} states<br/>
<br/>
CFG for L_n can be done with roughly n rules.<br/>
<br/>
The answer is not quite- and perhaps open.  There is a set of family of languages L_n such that for infinitely many n he above holds. These languages have to do with Turing Machines. In fact, you can replace<br/>
<br/>
2^{2^n}} with any function f  \le_T  INF (so second level of undecidability).<br/>
<br/>
For this blog this is NOT what we are looking for. (For more on this angle see <a href="https://arxiv.org/pdf/1503.08847.pdf">here</a><br/>
<br/>
<br/>
4) OPEN (I think) Is there a family of langs L_n such that for ALL n<br/>
<br/>
NFA for L_n requires 2^{2^n} (or some other fast growing function<br/>
<br/>
CFG for L_n can be done with roughly n states (we'll take n^{O(1)})<br/>
<br/>
5) OPEN (I think) Is there a family of langs L_n such that for ALL n<br/>
(or even for just inf many n)<br/>
<br/>
DFA for L_n requires 2^{2^n}} states<br/>
<br/>
NFA for L_n requires 2^n states and can be done in 2^n<br/>
<br/>
CFG for L_n can be done with n rules.<br/>
<br/>
(we'll settle for not quite as drastic, but still want to see DFA, NFA, CFG all<br/>
far apart).<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2020-03-31T17:53:00Z</updated>
    <published>2020-03-31T17:53:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-04-10T09:42:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/042</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/042" rel="alternate" type="text/html"/>
    <title>TR20-042 |  Poly-time blackbox identity testing for sum of log-variate constant-width ROABPs | 

	Pranav Bisht, 

	Nitin Saxena</title>
    <summary>Blackbox polynomial identity testing (PIT) affords 'extreme variable-bootstrapping' (Agrawal et al, STOC'18; PNAS'19; Guo et al, FOCS'19). This motivates us to study log-variate read-once oblivious algebraic branching programs (ROABP). We restrict width of ROABP to a constant and study the more general sum-of-ROABPs model. We give the first poly($s$)-time blackbox PIT for sum of constant-many, size-$s$, $O(\log s)$-variate constant-width ROABPs. The previous best for this model was quasi-polynomial time (Gurjar et al, CCC'15; CC'16) which is comparable to brute-force in the log-variate setting. Also, we handle unbounded-many such ROABPs if each ROABP computes a homogeneous polynomial. 

Our new techniques comprise-- (1) an ROABP computing a homogeneous polynomial can be made syntactically homogeneous in the same width; and (2) overcome the hurdle of unknown variable order in sum-of-ROABPs in the log-variate setting (over any field).</summary>
    <updated>2020-03-31T13:31:36Z</updated>
    <published>2020-03-31T13:31:36Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-04-11T04:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4695</id>
    <link href="https://www.scottaaronson.com/blog/?p=4695" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4695#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4695" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">On “armchair epidemiology”</title>
    <summary xml:lang="en-US">Update (March 31): Since commenter after commenter seems to have missed my point—or rather, rounded the point to something different that I didn’t say—let me try one more time. My faith in official pronouncements from health authorities, and in institutions like the CDC and the FDA, was clearly catastrophically misplaced—and if that doesn’t force significant […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><font color="red">Update (March 31):</font></strong> Since commenter after commenter seems to have missed my point—or rather, rounded the point to something different that I didn’t say—let me try one more time.  My faith in official pronouncements from health authorities, and in institutions like the CDC and the FDA, was clearly <em>catastrophically </em>misplaced—and if that doesn’t force significant revisions to my worldview, then I’m beyond hope.  Maybe the failures are because these organizations are at the mercy of political incompetents—meaning ultimately Trump and the people who put him in office.  Or maybe the rot started long before Trump.  Maybe it’s specific to the US, or maybe it’s everywhere.  I still don’t know the answers to those questions.</p>



<p>On the other hand, my faith in my ability to listen to individual people, whether they’re expert epidemiologists or virologists or just technologists or rationalists or anyone else (who in turn listened to the experts), and to say “yes, this person clearly has good judgment and has thought about it carefully, and if they’re worried then I should be too”—my faith in <em>that</em> has only gone up.  The problem is simply that I didn’t do enough of that back in January and February, and when I did, I didn’t sufficiently act on it.</p>



<p><strong><font color="red">End of Update</font></strong></p>



<p>On Feb. 4, a friend sent me an email that read, in part:</p>



<blockquote class="wp-block-quote"><p>Dr. A,<br/>What do you make of this coronavirus risk? …  I don’t know what level of precaution is necessary!  Please share your view.  </p></blockquote>



<p>This was the first time that I’d been prompted to give this subject any thought whatsoever.  I sent a quick reply two minutes later:</p>



<blockquote class="wp-block-quote"><p>For now, I think the risk from the ordinary flu is much much greater!  But worth watching to see if it becomes a real pandemic. </p></blockquote>



<p>Strictly speaking, this reply was “correct”—even “reasonable” and “balanced,” admitting the possibility of changing circumstances.  Yet if I could go back in time, I’d probably send a slightly different message—one that would fare better in the judgment of history.  Something like this, maybe:</p>



<blockquote class="wp-block-quote"><p><strong>HOLY SHIT!!!!!—GET YOUR PARENTS SOMEWHERE SAFE—CANCEL ALL TRAVEL PLANS—STOCK UP ON FOOD AND MASKS AND HAND SANITIZERS.  SELL ALL STOCK YOU OWN!!!  SHORT THE MARKET IF YOU KNOW HOW, OTHERWISE GET CASH AND BONDS.  HAVE AN ISOLATED PLACE TO ESCAPE TO.  IF YOU’RE FEELING ALTRUISTIC, JOIN GROUPS MAKING THEIR OWN MASKS AND VENTILATORS.</strong></p><p><strong>DO NOT RELY ON OFFICIAL PRONOUNCEMENTS, OR REASSURING ARTICLES FROM MAINSTREAM SOURCES LIKE <em>VOX</em> OR <em>THE WASHINGTON POST</em>.  THEY’RE FULL OF IT.  THE CDC AND OTHER FEDERAL AGENCIES ARE ASLEEP AT THE WHEEL, HOLLOWED-OUT SHELLS OF WHAT YOU IMAGINE THEM TO BE.  FOR ALL IT WILL DO IN ITS MOMENT OF ULTIMATE NEED, IT WOULD BE BETTER IF THE CDC NEVER EXISTED.</strong></p><p><strong>WHO THEN SHOULD YOU LISTEN TO?  CONTRARIAN, RATIONALIST NERDS AND TECH TYCOONS ON SOCIAL MEDIA.  BILL GATES, BALAJI SRINIVASAN, PAUL GRAHAM, GREG COCHRAN, ROBIN HANSON, SARAH CONSTANTIN, ELIEZER YUDKOWSKY, NICHOLAS CHRISTAKIS, ERIC WEINSTEIN.  NO, NOT ALL SUCH PEOPLE—NOT ELON MUSK, FOR EXAMPLE—BUT YOU’LL DO RIDICULOUSLY BETTER THAN AVERAGE THIS WAY. </strong></p><p><strong>BASICALLY, THE MORE SNEERCLUB WOULD SNEER AT A GIVEN PERSON, THE MORE THEY’D CALL THEM AN AUTODIDACT STEMLORD DUNNING-KRUGER ASSHOLE WHO’S THE EMBODIMENT OF EVERYTHING WRONG WITH NEOLIBERAL CAPITALISM, THE MORE YOU SHOULD LISTEN TO THAT PERSON RIGHT NOW FOR THE SAKE OF YOUR AND YOUR LOVED ONES’ FUCKING LIVES.  </strong></p><p><strong>DON’T WORRY: WITHIN 6-8 WEEKS, WHAT THE CONTRARIANS ARE SAYING TODAY WILL <em>BE</em> CONVENTIONAL WISDOM.  THE PUBLICATIONS THAT NOW SNEER AT PANDEMIC PREPPERS WILL TURN AROUND AND SNEER AT THE IRRESPONSIBLE NON-PREPPERS, WITHOUT EVER ADMITTING ERROR.  WE’LL ALWAYS HAVE BEEN AT WAR WITH OCEANIA—OR RATHER CORONIA.  TRUTH, OFFICIAL RECOMMENDATIONS, AND PROGRESSIVE POLITICS WILL GET BACK INTO ALIGNMENT JUST LIKE THEY NORMALLY ARE, AND WE’LL ALL BE SHARING MEMES JUSTLY DENOUNCING TRUMP AND THE CRAVEN REPUBLICAN SENATORS AND EVANGELICAL PASTORS AND NUTTY CONSPIRACY THEORISTS WHO DON’T CARE HOW MANY LIVES THEY SACRIFICE WITH THEIR DENIALS.</strong></p><p><strong>BUT EVEN THOUGH THE ENLIGHTENED MAINSTREAM WILL FIGURE OUT THE TRUTH IN A MONTH OR SO—AND EVEN THOUGH THAT’S FAR BETTER THAN OUR IDIOT PRESIDENT AND MILLIONS OF HIS FOLLOWERS, WHO WILL UNDERSTAND ONLY AFTER THE TRENCHES OVERFLOW WITH BODIES, IF THEN—EVEN SO, WE DON’T HAVE A MONTH.  IF YOU WANT TO BE AHEAD OF THE SENSIBLE MAINSTREAM, THEN ALMOST BY DEFINITION, THAT MEANS YOU NEED TO LISTEN TO THE POLITICALLY INCORRECT, CRAZY-SOUNDING ICONOCLASTS: TO THOSE WHO, UNLIKE YOU AND ALSO UNLIKE ME, HAVE DEMONSTRATED THAT THEY DON’T CARE IF PEOPLE SNEER AT THEM. </strong> </p></blockquote>



<p>Of course, I would never have sent such an email, and not only because of the bold and all-caps.  My whole personality stands against every sentence.  I’ve always taken my cues from “mainstream, reasonable, balanced” authorities, in any subject where I’m not personally expert.  That heuristic has generally been an excellent way to maximize expected rightness.  But when it fails … holy crap!</p>



<p>Now, and for the rest of my life, I’ll face the question: what was wrong with me, such that I would never have sent a “nutty” email like the one above?  Can I fix it?</p>



<p>More specifically, was my problem intellectual or emotional?  I lean toward the latter.  By mid-to-late February, as more and more of my smartest friends started panicking and telling me why I should too, I got intellectually fully on board with the idea that millions of people might die as the new virus spread around the world, and I affirmed as much on Facebook and elsewhere.  And yet it still took me a few more weeks to get from “millions could die” to “<strong>HOLY SHIT MILLIONS COULD DIE—PANIC—DROP EVERYTHING ELSE—BUILD MORE VENTILATORS!!!!</strong>“</p>



<p>A <a href="https://medium.com/@noahhaber/flatten-the-curve-of-armchair-epidemiology-9aa8cf92d652">viral article</a> implores us to “flatten the curve of armchair epidemiology”—that is, to listen only to authoritive sources like the CDC, not random people spouting on social media.  This was notable to me for being the diametric opposite of the <em>actual</em> lesson of the past two months.  It would be like taking the lesson from the 2008 financial crisis that from now on, you would only trust serious rating agencies, like Moody’s or Standard &amp; Poor.</p>



<p>Oh, but I forgot to tell you the punchline.  A couple days ago, the same friend who emailed me on February 4, emailed again to tell me that both of her parents (who live outside the US) now have covid-19.  Her father had to go to the emergency room and tested positive.  Her mother stayed home with somewhat milder symptoms.  Given the overloaded medical system in their country, neither can expect a high standard of care.  My friend has spent the past few days desperately trying to get anyone from the hospital on the phone.</p>



<p>This post represents my apology to her.  Like, it’s one thing to be so afraid of the jeers of the enlightened that you feign asexuality and live as an ascetic for a decade.  It’s worse to be so afraid that you fail adequately to warn your friends when you see an exponential function coming to kill their loved ones.</p></div>
    </content>
    <updated>2020-03-31T00:05:43Z</updated>
    <published>2020-03-31T00:05:43Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-04-10T23:00:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1640</id>
    <link href="https://theorydish.blog/2020/03/30/forc-2020-going-strong-going-virtual/" rel="alternate" type="text/html"/>
    <title>FORC 2020: Going Strong, Going Virtual</title>
    <summary>FORC 2020 accepted papers are out. Despite the last minute announcement and minimal advertising of this new conference, we have an exciting and strong program. This confirms our conviction that the new conference fills an important need for a home to the TOC sub-community that works on the societal aspects of computation. Unfortunately, but non-surprisingly, the conference will be virtual this year. But I’m sure that, thanks to Aaron Roth and to the inaugural PC, we will make the best what’s possible and have a great event.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>FORC 2020 <a href="https://responsiblecomputing.org/accepted-papers/?fbclid=IwAR0NTKaoKheiOmzYYyBqbDVtnkrPL8ooELD2RvfRFe7BHMF5tbgFg_bV840">accepted papers are out</a>. Despite the <a href="https://theorydish.blog/2019/10/30/toc-for-society/">last minute announcement</a> and minimal advertising of this new conference, we have an exciting and strong program. This confirms our conviction that the new conference fills an important need for a home to the TOC sub-community that works on the societal aspects of computation.</p>
<p>Unfortunately, but non-surprisingly, the conference will be virtual this year. But I’m sure that, thanks to <a href="https://www.cis.upenn.edu/~aaroth/">Aaron Roth</a> and to the inaugural PC, we will make the best what’s possible and have a great event.</p></div>
    </content>
    <updated>2020-03-30T20:35:21Z</updated>
    <published>2020-03-30T20:35:21Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-04-11T04:21:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-25562705.post-4686387049269740360</id>
    <link href="http://aaronsadventures.blogspot.com/feeds/4686387049269740360/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=4686387049269740360" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/4686387049269740360" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/4686387049269740360" rel="self" type="application/atom+xml"/>
    <link href="http://aaronsadventures.blogspot.com/2020/03/forc-2020-accepted-papers.html" rel="alternate" type="text/html"/>
    <title>FORC 2020 Accepted Papers</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">When we <a href="https://aaronsadventures.blogspot.com/2019/10/forc-new-conference-you-should-know.html">announced </a>the new conference <a href="https://responsiblecomputing.org/accepted-papers/">FORC (Foundations of Responsible Computing)</a> we really had no idea what kind of papers folks would send us.<br/><br/>Fortunately, we got a really high quality set of submissions, from which we have accepted the papers that will make up the program of the inaugural FORC. Check out the accepted papers here: <a href="https://responsiblecomputing.org/accepted-papers/">https://responsiblecomputing.org/accepted-papers/</a></div>
    </content>
    <updated>2020-03-30T00:38:00Z</updated>
    <published>2020-03-30T00:38:00Z</published>
    <author>
      <name>Aaron</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/09952936358739421126</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-25562705</id>
      <category term="game theory"/>
      <category term="news"/>
      <author>
        <name>Aaron</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/09952936358739421126</uri>
      </author>
      <link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <title>Adventures in Computation</title>
      <updated>2020-03-30T00:38:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16861</id>
    <link href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/" rel="alternate" type="text/html"/>
    <title>Logic and Star-Free, Part Deux</title>
    <summary>A visual proof with no abstract-algebra overhead Composite crop of src1, src2 Dominique Perrin and Jean-Éric Pin are French mathematicians who have done significant work in automata theory. Their 1986 paper “First-Order Logic and Star-Free Sets” gave a new proof that first-order logic plus the relation characterizes star-free regular sets. Today we present their proof […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A visual proof with no abstract-algebra overhead</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/perrinpin/" rel="attachment wp-att-16863"><img alt="" class="alignright wp-image-16863" height="154" src="https://rjlipton.files.wordpress.com/2020/03/perrinpin.png?w=200&amp;h=154" width="200"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite crop of <a href="http://www-igm.univ-mlv.fr/~perrin/">src1</a>, <a href="https://www.ae-info.org/ae/Member/Pin_Jean-Eric">src2</a></font></td>
</tr>
</tbody>
</table>
<p>
Dominique Perrin and Jean-Éric Pin are French mathematicians who have done significant work in automata theory. Their 1986 <a href="https://core.ac.uk/download/pdf/82354218.pdf">paper</a> “First-Order Logic and Star-Free Sets” gave a new proof that first-order logic plus the <img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/> relation characterizes star-free regular sets.</p>
<p>
Today we present their proof in a new visual way, using “stacked words” rather than their “marked words.” We also sidestep algebra by appealing to the familiar theorem that every regular language has a unique minimal deterministic finite automaton (DFA).<br/>
<span id="more-16861"/></p>
<p>
The first <a href="https://rjlipton.wordpress.com/2020/03/21/star-free-regular-languages-and-logic/">post</a> in this series defined the classes <img alt="{\mathsf{SF}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BSF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{SF}}"/> for star-free regular and <img alt="{\mathsf{FO}[&lt;]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFO%7D%5B%3C%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FO}[&lt;]}"/> for first-order logic where variables <img alt="{u,v,w,\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%2Cv%2Cw%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u,v,w,\dots}"/> range over the indices <img alt="{0,\dots,n-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2C%5Cdots%2Cn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0,\dots,n-1}"/> of a string <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. For any character <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> in the alphabet <img alt="{\Sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Sigma}"/> there is the predicate <img alt="{X_c(u)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_c%28u%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_c(u)}"/> saying that the character in position <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> equals <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/>. The first part proved that for any language <img alt="{A \in \mathsf{SF}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Cin+%5Cmathsf%7BSF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \in \mathsf{SF}}"/> there is a sentence <img alt="{\psi_A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi_A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi_A}"/> using only predicates <img alt="{X_c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_c}"/> and the relation <img alt="{u &lt; v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu+%3C+v%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u &lt; v}"/> besides the usual quantifiers and Boolean operations of first-order logic. such that <img alt="{\psi_A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi_A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi_A}"/> defined <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>. That is, it proved <img alt="{\mathsf{SF}\subseteq\mathsf{FO}[&lt;]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BSF%7D%5Csubseteq%5Cmathsf%7BFO%7D%5B%3C%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{SF}\subseteq\mathsf{FO}[&lt;]}"/>.</p>
<p>
A key trick was to focus not on <img alt="{\psi_A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi_A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi_A}"/> but on a formula <img alt="{\phi_A(i,j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi_A%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi_A(i,j)}"/> expressing that the part of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> from position <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> (inclusive) to <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> (exclusive) belongs to <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>. To prove the converse direction, <img alt="{\mathsf{FO}[&lt;]\subseteq\mathsf{SF}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFO%7D%5B%3C%5D%5Csubseteq%5Cmathsf%7BSF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FO}[&lt;]\subseteq\mathsf{SF}}"/>, we focus not on middles of strings but on prefixes and suffixes. The previous <a href="https://rjlipton.wordpress.com/2020/03/21/star-free-regular-languages-and-logic/">post</a> proved two lemmas showing that if <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> is star-free then for any string <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>, so are its prefix and suffix languages: </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  L\backslash y &amp;=&amp; \{z: yz \in L\}\\ L/y &amp;=&amp; \{x: xy \in L\}. \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++L%5Cbackslash+y+%26%3D%26+%5C%7Bz%3A+yz+%5Cin+L%5C%7D%5C%5C+L%2Fy+%26%3D%26+%5C%7Bx%3A+xy+%5Cin+L%5C%7D.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}  L\backslash y &amp;=&amp; \{z: yz \in L\}\\ L/y &amp;=&amp; \{x: xy \in L\}. \end{array} "/></p>
<p>We will piece together star-free expressions for a multitude of <img alt="{L/y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%2Fy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L/y}"/> and <img alt="{L \backslash y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL+%5Cbackslash+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L \backslash y}"/> sets that come from analyzing minimum DFAs <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M}"/> at each induction step, until we have built a big star-free expression <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> for <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>. This sounds complex—and is complex—but all the steps are elementary. At the end we’ll try to see how big <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> gets. <b>Again the rest of this post up to end notes is written by Daniel Winton</b>.</p>
<p>
</p><p>
</p><p/><h2> Stacked Words and FO Formulas </h2><p/>
<p/><p>
We implement the “marked words” idea of Perrin and Pin by using extra dimensions rather than marks. Consider any formula <img alt="{\phi(u_1, \dots, u_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28u_1%2C+%5Cdots%2C+u_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi(u_1, \dots, u_j)}"/> in FO[<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{&lt;}"/>] where <img alt="{u_1,\dots,u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_1%2C%5Cdots%2Cu_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_1,\dots,u_j}"/> are the free variables. Our "stacked words" have <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> extra rows underneath the top level holding the string <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Each row <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> has the same length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> as <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and contains a single <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> and <img alt="{(n-1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28n-1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(n-1)}"/> <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>s. The column <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> in which the <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> lies gives the value of the variable <img alt="{u_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_i}"/>. Thus the alphabet of our stacked words is <img alt="{\Sigma^{(j)} = \Sigma\times\{0,1\}^j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma%5E%7B%28j%29%7D+%3D+%5CSigma%5Ctimes%5C%7B0%2C1%5C%7D%5Ej%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Sigma^{(j)} = \Sigma\times\{0,1\}^j}"/>. Here is an example:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/stackedword/" rel="attachment wp-att-16865"><img alt="" class="aligncenter wp-image-16865" height="248" src="https://rjlipton.files.wordpress.com/2020/03/stackedword.jpg?w=242&amp;h=248" width="242"/></a></p>
<p/><p><br/>
Each column is really a single character <img alt="{C \in \Sigma^{(j)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%5Cin+%5CSigma%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C \in \Sigma^{(j)}}"/>, but we picture its entries as characters in <img alt="{\Sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Sigma}"/> or <img alt="{\{0,1\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1\}}"/>. For any row <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>, define <img alt="{Y_{i,0}(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY_%7Bi%2C0%7D%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y_{i,0}(\cdot)}"/> to be the disjunction of <img alt="{X_C(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_C%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_C(\cdot)}"/> over all <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> that have a <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> in entry <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>, and define <img alt="{Y_{i,1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY_%7Bi%2C1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y_{i,1}}"/> similarly. Then the following sentence expresses the “format condition” that row <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> has exactly one <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>: </p>
<p align="center"><img alt="\displaystyle  (\exists k)[Y_{i,1}(k) \land (\forall \ell) [\ell \neq k \rightarrow Y_{i,0}(k)]]. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28%5Cexists+k%29%5BY_%7Bi%2C1%7D%28k%29+%5Cland+%28%5Cforall+%5Cell%29+%5B%5Cell+%5Cneq+k+%5Crightarrow+Y_%7Bi%2C0%7D%28k%29%5D%5D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (\exists k)[Y_{i,1}(k) \land (\forall \ell) [\ell \neq k \rightarrow Y_{i,0}(k)]]. "/></p>
<p>This readily transforms into a star-free expression for the language of stacked words with exactly one <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in row <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>: </p>
<p align="center"><img alt="\displaystyle  \alpha_i = C_{i,0}^* C_{i,1} C_{i,0}^*, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Calpha_i+%3D+C_%7Bi%2C0%7D%5E%2A+C_%7Bi%2C1%7D+C_%7Bi%2C0%7D%5E%2A%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \alpha_i = C_{i,0}^* C_{i,1} C_{i,0}^*, "/></p>
<p>Here <img alt="{C_{i,0}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%7Bi%2C0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_{i,0}}"/> is the union of all stacked characters that have a <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> in entry <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>, and <img alt="{C_{i,1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%7Bi%2C1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_{i,1}}"/> is similarly defined. This is where the examples in the previous <a href="https://rjlipton.wordpress.com/2020/03/21/star-free-regular-languages-and-logic/">post</a> about finite unions inside stars come in handy. The upshot is that </p>
<p align="center"><img alt="\displaystyle  \alpha_0 = \bigcap_{i=1}^j \alpha_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Calpha_0+%3D+%5Cbigcap_%7Bi%3D1%7D%5Ej+%5Calpha_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \alpha_0 = \bigcap_{i=1}^j \alpha_i "/></p>
<p>is a <img alt="{\mathsf{SF}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BSF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{SF}}"/> expression that enforces the “format condition” over all rows. We will use this at the crux of the proof; whether <img alt="{\alpha_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_0}"/> is really needed is a question at the end. If we take the format for granted, then the main advantage of our stacked words will be visualizing how to decompose automata according to when the <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in a critical row is read.</p>
<p>
</p><p/><h2> From FO To SF </h2><p/>
<p/><p>
The proof manipulates formulas <img alt="{\phi(u_1,\dots,u_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28u_1%2C%5Cdots%2Cu_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi(u_1,\dots,u_j)}"/> having the <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> free variables shown. The corresponding language <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/> is the set of stacked words that make <img alt="{\phi(k_1,\dots,k_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%28k_1%2C%5Cdots%2Ck_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi(k_1,\dots,k_j)}"/> true, where for each <img alt="{i \leq j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%5Cleq+j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i \leq j}"/>, <img alt="{k_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k_i}"/> is the position of the lone <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in row <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>, which gives the value of <img alt="{u_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_i}"/>. </p>
<blockquote><p><b>Theorem 1</b> <em> For all formulas <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\phi}"/> over <img alt="{\mathsf{FO}[&lt;]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFO%7D%5B%3C%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{FO}[&lt;]}"/>, <img alt="{L_\phi\in\mathsf{SF}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%5Cin%5Cmathsf%7BSF%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{L_\phi\in\mathsf{SF}}"/>. </em>
</p></blockquote>
<p/><p>
Without loss of generality we may suppose <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> is in <em>prenex form</em>, meaning that it has all quantifiers out front. If there are <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> variables of which <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> are free, this means </p>
<p align="center"><img alt="\displaystyle  \phi = (Q_{j+1} u_{j+1})\cdots (Q_r u_r)\mu, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cphi+%3D+%28Q_%7Bj%2B1%7D+u_%7Bj%2B1%7D%29%5Ccdots+%28Q_r+u_r%29%5Cmu%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \phi = (Q_{j+1} u_{j+1})\cdots (Q_r u_r)\mu, "/></p>
<p>where each <img alt="{Q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_i}"/> is <img alt="{\forall}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cforall%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\forall}"/> or <img alt="{\exists}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cexists%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\exists}"/> and <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/> has no quantifiers. If we picture the variables <img alt="{u_1,\dots,u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_1%2C%5Cdots%2Cu_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_1,\dots,u_j}"/> as once having had quantifiers <img alt="{Q_1,\dots,Q_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_1%2C%5Cdots%2CQ_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_1,\dots,Q_j}"/>, then the proof—after dealing with <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/>—restores them one at a time, beginning with <img alt="{Q_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_j}"/>. That is the induction. We prove the base case <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/> here and the induction case in the coming sections.</p>
<p>
<em>Proof:</em>  Quantifier free formulas in <img alt="{\mathsf{FO}[&lt;]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFO%7D%5B%3C%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FO}[&lt;]}"/> are Boolean combinations of the <em>atomic predicates</em> <img alt="{X_c(u_i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_c%28u_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_c(u_i)}"/> and <img alt="{u_h &lt; u_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_h+%3C+u_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_h &lt; u_i}"/>. Here <img alt="{X_c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_c}"/> is only for the characters <img alt="{c \in \Sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cin+%5CSigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c \in \Sigma}"/>, not the stacked characters <img alt="{C \in \Sigma^{(j)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%5Cin+%5CSigma%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C \in \Sigma^{(j)}}"/>. But inside <img alt="{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu}"/> (when we have <img alt="{j = r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj+%3D+r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j = r}"/>) we have to deal with the whole stack to represent <img alt="{X_c(u_i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_c%28u_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_c(u_i)}"/>. Putting <img alt="{\Sigma = \{c_1,\dots,c_{\ell}\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma+%3D+%5C%7Bc_1%2C%5Cdots%2Cc_%7B%5Cell%7D%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Sigma = \{c_1,\dots,c_{\ell}\}}"/>, the expression can be viewed schematically as:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/xcstackedexp/" rel="attachment wp-att-16867"><img alt="" class="aligncenter size-large wp-image-16867" height="126" src="https://rjlipton.files.wordpress.com/2020/03/xcstackedexp.jpg?w=600&amp;h=126" width="600"/></a></p>
<p/><p><br/>
As illustrated above, using the initial lemma in the part-1 <a href="https://rjlipton.wordpress.com/2020/03/21/star-free-regular-languages-and-logic/">post</a>, this is equivalent to a star-free regular expression. The other atomic predicate, <img alt="{u_h &lt; u_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_h+%3C+u_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_h &lt; u_i}"/>, is true when we have a stacked word <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/> of the form: </p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/ltstackedword2/" rel="attachment wp-att-16891"><img alt="" class="aligncenter wp-image-16891" height="250" src="https://rjlipton.files.wordpress.com/2020/03/ltstackedword2.jpg?w=300&amp;h=250" width="300"/></a></p>
<p/><p><br/>
It is not important to have <img alt="{h &lt; i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh+%3C+i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h &lt; i}"/>; these are just the labels of the variables. Their values <img alt="{k_1,k_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk_1%2Ck_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k_1,k_2}"/> must obey <img alt="{k_1 &lt; k_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk_1+%3C+k_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k_1 &lt; k_2}"/>, meaning that the <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in row <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> comes in an earlier column than the <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in row <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>. The characters of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> in the top row are immaterial. We can capture this condition over all strings <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Y}"/> over <img alt="{\Sigma^{(j)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Sigma^{(j)}}"/> by the expression:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/ltstackedexp/" rel="attachment wp-att-16872"><img alt="" class="aligncenter size-large wp-image-16872" height="212" src="https://rjlipton.files.wordpress.com/2020/03/ltstackedexp.jpg?w=600&amp;h=212" width="600"/></a></p>
<p/><p><br/>
Again by the lemma in Part 1, this yields a star-free expression. To complete the basis, we note that if <img alt="{L_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_1}"/> and <img alt="{L_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_2}"/> have star-free expressions <img alt="{\alpha_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_1}"/> and <img alt="{\alpha_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_2}"/> corresponding to <img alt="{\phi_1, \phi_2\in \mathsf{FO}[&lt;]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi_1%2C+%5Cphi_2%5Cin+%5Cmathsf%7BFO%7D%5B%3C%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi_1, \phi_2\in \mathsf{FO}[&lt;]}"/>, then <img alt="{\lnot\phi_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clnot%5Cphi_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lnot\phi_1}"/> is represented by <img alt="{\sim{\alpha_1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csim%7B%5Calpha_1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sim{\alpha_1}}"/> and <img alt="{\phi_1\lor\phi_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi_1%5Clor%5Cphi_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi_1\lor\phi_2}"/> by <img alt="{\alpha_1\cup\alpha_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_1%5Ccup%5Calpha_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_1\cup\alpha_2}"/>, both of which are clearly star-free. </p>
<p>
</p><p/><h2> Strategy For the Induction Case </h2><p/>
<p/><p>
Assume that all formulas in <img alt="{\mathsf{FO}[&lt;]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFO%7D%5B%3C%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FO}[&lt;]}"/> with <img alt="{\ell = r - j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3D+r+-+j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell = r - j}"/> quantifiers have star-free translations. We will show that any formula in <img alt="{\mathsf{FO}[&lt;]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFO%7D%5B%3C%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FO}[&lt;]}"/> with <img alt="{\ell+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell+1}"/> quantifiers also has a star-free translation. Let <img alt="{\psi(u_1,... ,u_{j-1})=\exists u_j \phi(u_1,..., u_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%28u_1%2C...+%2Cu_%7Bj-1%7D%29%3D%5Cexists+u_j+%5Cphi%28u_1%2C...%2C+u_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi(u_1,... ,u_{j-1})=\exists u_j \phi(u_1,..., u_j)}"/> where <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> is a formula in <img alt="{\mathsf{FO}[&lt;]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFO%7D%5B%3C%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FO}[&lt;]}"/> that has <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> total variables, <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> free variables and <img alt="{m-j=\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm-j%3D%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m-j=\ell}"/> quantifiers. We are allowed to assume the first quantifier in <img alt="{\psi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi}"/> is existential as <img alt="{\exists \equiv \lnot (\forall \lnot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cexists+%5Cequiv+%5Clnot+%28%5Cforall+%5Clnot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\exists \equiv \lnot (\forall \lnot)}"/> and we observed above that the star-free expressible formulas in <img alt="{\mathsf{FO}[&lt;]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BFO%7D%5B%3C%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{FO}[&lt;]}"/> are closed under negation. </p>
<p>
The language of <img alt="{\psi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi}"/> is given by: </p>
<p>
<img alt="{L_\psi=\{z\in(\Sigma\times \{0,1\}^{j-1})^*:}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cpsi%3D%5C%7Bz%5Cin%28%5CSigma%5Ctimes+%5C%7B0%2C1%5C%7D%5E%7Bj-1%7D%29%5E%2A%3A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\psi=\{z\in(\Sigma\times \{0,1\}^{j-1})^*:}"/> we can add a <img alt="{{(j+1)}^{th}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%28j%2B1%29%7D%5E%7Bth%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{(j+1)}^{th}}"/> row to each of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> characters of <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> using <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> minus one <img alt="{0'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0'}"/>s and one <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> such that the resulting string <img alt="{z'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z'}"/> belongs to <img alt="{L_\phi\}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%5C%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi\}.}"/> </p>
<p>We must show that <img alt="{L_\psi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\psi}"/> is star-free. As <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> has <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell}"/> quantifiers, then by assumption <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/> is star-free regular. </p>
<p>
Note that <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/> has one more row than <img alt="{L_\psi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\psi}"/> because it has one more free variable. Without loss of generality we may suppose it is the bottom row. </p>
<p>
</p><p/><h3> The Tricky Point </h3><p/>
<p/><p>
Since we have an existential quantifier we might think we can simply delete the last row of <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/>. But here is a counterexample to show why we must be careful not to “lose information”:</p>
<p>
Let <img alt="{\alpha=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha=}"/> </p>
<p>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/alphastackedexp/" rel="attachment wp-att-16875"><img alt="" class="aligncenter size-large wp-image-16875" height="56" src="https://rjlipton.files.wordpress.com/2020/03/alphastackedexp.jpg?w=600&amp;h=56" width="600"/></a></p>
<p>
and <img alt="{\alpha'=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%27%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha'=}"/> </p>
<p>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/alphaprime/" rel="attachment wp-att-16876"><img alt="" class="aligncenter size-large wp-image-16876" height="43" src="https://rjlipton.files.wordpress.com/2020/03/alphaprime.jpg?w=600&amp;h=43" width="600"/></a></p>
<p>
Notice that <img alt="{\alpha'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha'}"/> equals <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> with its final row removed. We have <img alt="{L_\alpha=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Calpha%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\alpha=}"/></p>
<p>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/lalpha/" rel="attachment wp-att-16878"><img alt="" class="aligncenter wp-image-16878" height="44" src="https://rjlipton.files.wordpress.com/2020/03/lalpha.jpg?w=357&amp;h=44" width="357"/></a></p>
<p>
and <img alt="{L_{\alpha'}=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7B%5Calpha%27%7D%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{\alpha'}=}"/></p>
<p>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/lalphaprime/" rel="attachment wp-att-16880"><img alt="" class="aligncenter wp-image-16880" height="44" src="https://rjlipton.files.wordpress.com/2020/03/lalphaprime.jpg?w=300&amp;h=44" width="300"/></a></p>
<p/><p><br/>
Then <img alt="{L_\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\alpha}"/> is the language of strings with at least three zeroes in a row and <img alt="{L_{\alpha'}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7B%5Calpha%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{\alpha'}}"/> is the language of strings with at least two zeroes in a row.</p>
<p>
We cannot add a second row consisting of zeroes and one 1 to each word <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> in <img alt="{L_\alpha'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Calpha%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\alpha'}"/> to force <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> to be in <img alt="{L_\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\alpha}"/>. For a trivial example, consider the word <img alt="{w=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w=}"/></p>
<p>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/w00/" rel="attachment wp-att-16881"><img alt="" class="aligncenter wp-image-16881" height="36" src="https://rjlipton.files.wordpress.com/2020/03/w00.jpg?w=75&amp;h=36" width="75"/></a></p>
<p/><p/>
<p>
The fault can be interpreted as <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> not being recoverable uniquely from <img alt="{\alpha'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha'}"/> owing to a loss of information. We could also pin the fault in this case on the central <img alt="{\cap}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccap%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cap}"/>, noting that <img alt="{(\exists u)(f(u) \wedge g(u))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cexists+u%29%28f%28u%29+%5Cwedge+g%28u%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\exists u)(f(u) \wedge g(u))}"/> is generally not equivalent to <img alt="{(\exists u)f(u) \wedge (\exists u)g(u)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cexists+u%29f%28u%29+%5Cwedge+%28%5Cexists+u%29g%28u%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\exists u)f(u) \wedge (\exists u)g(u)}"/>. </p>
<p>
Either way, this means we must be careful with how we express (the final row of) <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/>. The idea is to give <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/> in segments such that the final row of each segment is all-zero or a single <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, so that the act of removing the final row could be inverted. This is the crux of the proof. To handle it we employ the unique DFA for the regular expression already obtained by induction for <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/>. </p>
<p>
</p><p/><h2> Main Part of the Proof </h2><p/>
<p/><p>
By the Myhill-Nerode Theorem, there must exist a unique minimal automaton <img alt="{M_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M_\phi}"/> over the alphabet of our stacked words defining the language <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/> of <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/>. </p>
<p>
The state set <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> of <img alt="{M_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M_\phi}"/> can be partitioned into the set <img alt="{Q_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_1}"/> of states before any <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> is read in the final row, the set <img alt="{Q_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_2}"/> of states reached after reading a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in the final row, plus a dead state <img alt="{q_D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_D}"/> belonging to neither <img alt="{Q_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_1}"/> nor <img alt="{Q_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_2}"/>. The start state belongs to <img alt="{Q_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_1}"/> and all accepting states belong to <img alt="{Q_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_2}"/>. Since any stacked word and therefore accepting computation has exactly one <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in any row, all arcs within <img alt="{Q_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_1}"/> and <img alt="{Q_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_2}"/> must have a only <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>s in their final row, while the letters connecting states in <img alt="{Q_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_1}"/> and <img alt="{Q_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_2}"/> must have a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in their final row. The minimality of <img alt="{M_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M_\phi}"/> enforces these properties. Here is a sketch, omitting <img alt="{q_D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_D}"/>: </p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/03/29/logic-and-star-free-part-deux/lphidfa/" rel="attachment wp-att-16883"><img alt="" class="aligncenter size-large wp-image-16883" height="324" src="https://rjlipton.files.wordpress.com/2020/03/lphidfa.jpg?w=600&amp;h=324" width="600"/></a></p>
<p/><p><br/>
Let <img alt="{E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E}"/> be the set of edges that cross from <img alt="{Q_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_1}"/> to <img alt="{Q_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_2}"/>. With <img alt="{m = |E|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+%7CE%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m = |E|}"/> we can label their origins by states <img alt="{p_1,\dots,p_m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_1%2C%5Cdots%2Cp_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_1,\dots,p_m}"/> (not necessarily all distinct), and similarly label their characters by <img alt="{c_1,\dots,c_m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_1%2C%5Cdots%2Cc_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_1,\dots,c_m}"/> and the destination states by <img alt="{r_1,\dots,r_m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_1%2C%5Cdots%2Cr_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_1,\dots,r_m}"/>. Then <img alt="{C = \{c_1,\dots,c_m\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%3D+%5C%7Bc_1%2C%5Cdots%2Cc_m%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C = \{c_1,\dots,c_m\}}"/> collects all the characters used by <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M}"/> that have a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in row <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> (except those into or at the dead state <img alt="{q_D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_D}"/>). We can identify <img alt="{E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E}"/> with the set of instructions <img alt="{\{(p_e, c_e, r_e): 1 \leq e \leq m\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B%28p_e%2C+c_e%2C+r_e%29%3A+1+%5Cleq+e+%5Cleq+m%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{(p_e, c_e, r_e): 1 \leq e \leq m\}}"/>. </p>
<p>
Finally let <img alt="{L_{p,q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bp%2Cq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{p,q}}"/> denote the set of strings that take <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M}"/> from state <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> to state <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/>, and let <img alt="{L_{r,F} = \cup_{f \in F} L_{r,f}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Br%2CF%7D+%3D+%5Ccup_%7Bf+%5Cin+F%7D+L_%7Br%2Cf%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{r,F} = \cup_{f \in F} L_{r,f}}"/>. Then we can define <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/> by the expression <a name="expansion"/></p><a name="expansion">
<p align="center"><img alt="\displaystyle  L_\phi =\bigcup_{(p_e, c_e, r_e)\in E} L_{s,p_e}\cdot c_e \cdot L_{r_e,F}. \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_%5Cphi+%3D%5Cbigcup_%7B%28p_e%2C+c_e%2C+r_e%29%5Cin+E%7D+L_%7Bs%2Cp_e%7D%5Ccdot+c_e+%5Ccdot+L_%7Br_e%2CF%7D.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L_\phi =\bigcup_{(p_e, c_e, r_e)\in E} L_{s,p_e}\cdot c_e \cdot L_{r_e,F}. \ \ \ \ \ (1)"/></p>
</a><p><a name="expansion"/> What remains is to find <img alt="{\mathsf{SF}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BSF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{SF}}"/> expressions for <img alt="{L_{s,p_e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bs%2Cp_e%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{s,p_e}}"/> and <img alt="{L_{r_e,F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Br_e%2CF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{r_e,F}}"/> for each <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/>. The latter is easy: Pick any string <img alt="{y_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_e}"/> in <img alt="{L_{s,p_e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bs%2Cp_e%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{s,p_e}}"/>. Then </p>
<p align="center"><img alt="\displaystyle  L_{r_e,F} = \{z: y_e\cdot c_e \cdot z \in L(M)\} = L_{\phi} \backslash y_e c_e. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_%7Br_e%2CF%7D+%3D+%5C%7Bz%3A+y_e%5Ccdot+c_e+%5Ccdot+z+%5Cin+L%28M%29%5C%7D+%3D+L_%7B%5Cphi%7D+%5Cbackslash+y_e+c_e.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L_{r_e,F} = \{z: y_e\cdot c_e \cdot z \in L(M)\} = L_{\phi} \backslash y_e c_e. "/></p>
<p>The closure of <img alt="{\mathsf{SF}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BSF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{SF}}"/> under left quotients supplies expressions <img alt="{\beta_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta_e}"/> for <img alt="{L_{r_e,F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Br_e%2CF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{r_e,F}}"/>. </p>
<p>
The case of <img alt="{L_{s,p_e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bs%2Cp_e%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{s,p_e}}"/>, however, is harder. What we would like to do is choose some (any) string <img alt="{w_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_e}"/> that goes from <img alt="{r_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_e}"/> to an accepting state and claim that <img alt="{L_{s,p_e} = L(M)/c_e w_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bs%2Cp_e%7D+%3D+L%28M%29%2Fc_e+w_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{s,p_e} = L(M)/c_e w_e}"/>. The problem is that <img alt="{w_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_e}"/> might be accepted from some state <img alt="{r_d \neq r_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_d+%5Cneq+r_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_d \neq r_e}"/> that has an incoming arc on the same character <img alt="{c_d = c_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_d+%3D+c_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_d = c_e}"/> from some other state <img alt="{p_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_d}"/>. Then <img alt="{L_{s,p_d}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bs%2Cp_d%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{s,p_d}}"/>, which is disjoint from <img alt="{L_{s,p_e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bs%2Cp_e%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{s,p_e}}"/>, is included also in <img alt="{L(M)/c_e w_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%28M%29%2Fc_e+w_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L(M)/c_e w_e}"/>. There may, however, be strings <img alt="{y_d \in L_{s,p_d}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_d+%5Cin+L_%7Bs%2Cp_d%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_d \in L_{s,p_d}}"/> and <img alt="{w'_e \in L_{r_e,F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27_e+%5Cin+L_%7Br_e%2CF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'_e \in L_{r_e,F}}"/> such that <img alt="{y = y_d c_e w'_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By+%3D+y_d+c_e+w%27_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y = y_d c_e w'_e}"/> is <b>not</b> accepted by <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M}"/>. Then the string <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> would be wrongly included if we substituted <img alt="{L(M)/c_e z_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%28M%29%2Fc_e+z_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L(M)/c_e z_e}"/> for <img alt="{L_{s,p_e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bs%2Cp_e%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{s,p_e}}"/> (or for <img alt="{L_{s,p_e} \cup L_{s,p_d}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bs%2Cp_e%7D+%5Ccup+L_%7Bs%2Cp_d%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{s,p_e} \cup L_{s,p_d}}"/>) in (1). </p>
<p>
There is also a second issue when crossing edges on the same character <img alt="{c_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_e}"/> come in from distinct states <img alt="{p_d,p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_d%2Cp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_d,p_e}"/> to the same state <img alt="{r_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_e}"/>. To fix all this, we need to use sets of strings that distinguish <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> from all the other states <img alt="{p_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_d}"/>. This requires the most particular use of the minimality of <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M}"/>: If <img alt="{p_d \neq p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_d+%5Cneq+p_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_d \neq p_e}"/>, then there is either (or both):</p>
<ul>
<li>
a string <img alt="{z_{e,d}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_%7Be%2Cd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_{e,d}}"/> in <img alt="{L_{p_e,F} - L_{p_d,F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bp_e%2CF%7D+-+L_%7Bp_d%2CF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{p_e,F} - L_{p_d,F}}"/>, or <p/>
</li><li>
a string <img alt="{z_{d,e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_%7Bd%2Ce%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_{d,e}}"/> in <img alt="{L_{p_d,F} - L_{p_e,F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bp_d%2CF%7D+-+L_%7Bp_e%2CF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{p_d,F} - L_{p_e,F}}"/>.
</li></ul>
<p>
The strings in question need not begin with a character in <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>—they need not cross right away. Let <img alt="{Z_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z_e}"/> stand for a choice of strings of the former kind, <img alt="{Z'_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%27_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z'_e}"/> the latter kind, covering all states <img alt="{p_d \neq p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_d+%5Cneq+p_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_d \neq p_e}"/>. Then <a name="distinct"/></p><a name="distinct">
<p align="center"><img alt="\displaystyle  L_{s,p_e} = \left(\bigcap_{z \in Z_e} L_{\phi}/z\right) \cap \left(\bigcap_{z' \in Z'_e} (\alpha_0 \cap \tilde{L}_{\phi})/z'\right). \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_%7Bs%2Cp_e%7D+%3D+%5Cleft%28%5Cbigcap_%7Bz+%5Cin+Z_e%7D+L_%7B%5Cphi%7D%2Fz%5Cright%29+%5Ccap+%5Cleft%28%5Cbigcap_%7Bz%27+%5Cin+Z%27_e%7D+%28%5Calpha_0+%5Ccap+%5Ctilde%7BL%7D_%7B%5Cphi%7D%29%2Fz%27%5Cright%29.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L_{s,p_e} = \left(\bigcap_{z \in Z_e} L_{\phi}/z\right) \cap \left(\bigcap_{z' \in Z'_e} (\alpha_0 \cap \tilde{L}_{\phi})/z'\right). \ \ \ \ \ (2)"/></p>
</a><p><a name="distinct"/> We could not use just the complement <img alt="{\tilde{L}_{\phi}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BL%7D_%7B%5Cphi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde{L}_{\phi}}"/> of <img alt="{L_{\phi}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7B%5Cphi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{\phi}}"/> because that would allow strings that violate the “one <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>” condition in the rows. Note that <img alt="{\alpha_0 \cap \tilde{L}_{\phi} = L_{\neg\phi}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_0+%5Ccap+%5Ctilde%7BL%7D_%7B%5Cphi%7D+%3D+L_%7B%5Cneg%5Cphi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_0 \cap \tilde{L}_{\phi} = L_{\neg\phi}}"/>. Whether one can do the induction for <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> and <img alt="{\neg\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneg%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neg\phi}"/> in tandem without invoking <img alt="{\alpha_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_0}"/> is a riddle we pose at the end. </p>
<p>
Either way, the closure of <img alt="{\mathsf{SF}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BSF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{SF}}"/> under right quotients yields a star-free expression <img alt="{\alpha_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_e}"/> for <img alt="{L_{s,p_e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bs%2Cp_e%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{s,p_e}}"/>. Then we just have to plug our expressions <img alt="{\alpha_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_e}"/> and <img alt="{\beta_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta_e}"/> into (1) to get a star-free expression <img alt="{\beta_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta_\phi}"/> for <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/>. Now we are ready for the crude final step:</p>
<blockquote><p><b> </b> <em> Form <img alt="{\beta'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%27%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\beta'}"/> by knocking out the last entry in every character occurring in <img alt="{\beta_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta_%5Cphi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\beta_\phi}"/>. Then <img alt="{\beta'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%27%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\beta'}"/> represents <img alt="{L_{\psi}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7B%5Cpsi%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{L_{\psi}}"/>. </em>
</p></blockquote>
<p/><p>
To prove this final statement, first suppose <img alt="{w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'}"/> is a stacked word in <img alt="{L_\psi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\psi}"/>. Then <img alt="{w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'}"/> satisfies <img alt="{(\exists u)\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cexists+u%29%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\exists u)\phi}"/>, so there is a value <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> so that <img alt="{w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'}"/> satisfies <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> with <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> set equal to the value <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>. This means that the stacked word <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> formed from <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> and a last row with <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in position <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> belongs to <img alt="{L_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_\phi}"/> and hence matches <img alt="{\beta_\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta_%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta_\phi}"/>. Since <img alt="{w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'}"/> is obtained by knocking out the last row of <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/>, <img alt="{w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'}"/> matches <img alt="{\beta'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta'}"/>. </p>
<p>
The converse is the part where the tricky point we noted at the outset could trip us up. Suppose <img alt="{w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'}"/> matches <img alt="{\beta'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta'}"/>. From the form of (1) as a union of terms <img alt="{\alpha_e c_e \beta_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_e+c_e+%5Cbeta_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_e c_e \beta_e}"/>, we can trace how <img alt="{w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'}"/> is matched to identify a place in <img alt="{w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'}"/> that matches a “middle character” <img alt="{c'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c'}"/> obtained from a <img alt="{c_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_e}"/> in a crossing edge. Putting a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in a new row underneath this place and <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>s elsewhere hence makes a word <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> that matches the unique regular expression <img alt="{\beta''}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta''}"/> obtained by appending a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> component to every “middle character” <img alt="{c'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c'}"/> and a <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> to all other characters in <img alt="{\beta'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta'}"/>. Then <img alt="{\beta''}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta''}"/> is equivalent to <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\beta}"/>, so <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> satisfies <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/>. Since the last row that was added to <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> amounts to supplying a value <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> for the variable <img alt="{u_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_j}"/>, it follows that <img alt="{w'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w'}"/> satisfies <img alt="{\phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi}"/> with <img alt="{u_j = k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_j+%3D+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_j = k}"/>, hence satisfies <img alt="{\psi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi}"/>. This yields <img alt="{L(\beta') = L_{\psi}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%28%5Cbeta%27%29+%3D+L_%7B%5Cpsi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L(\beta') = L_{\psi}}"/> and so the entire induction goes through. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
First, as an end note, the end of the proof wound up different from what we envisioned until polishing this post. It originally applied the distinct-states argument to states <img alt="{r_d,r_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_d%2Cr_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_d,r_e}"/> on the far side of the crossing, but we had to backtrack on having previously thought that the “second issue” did not matter. Two other questions we pose for our readers:</p>
<ul>
<li>
Do the definitions of <img alt="{Z_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z_e}"/> and <img alt="{Z'_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%27_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z'_e}"/> need to extend over all states <img alt="{p \in Q_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%5Cin+Q_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p \in Q_1}"/>, not just the other states <img alt="{p_d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_d}"/> on the border? <p/>
</li><li>
Is <img alt="{\alpha_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_0}"/> needed in (2), or in the proof overall?
</li></ul>
<p>
A larger question concerns how the size of the translated expressions increases as we add more quantifiers. We discuss the blowup in terms of quantifier depth <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> and length <img alt="{n=|x|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D%7Cx%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=|x|}"/>. Let <img alt="{\psi_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi_0}"/> be a first-order formula with no quantifiers and let <img alt="{\psi_m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi_m}"/> denote <img alt="{\psi_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi_0}"/> with <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> quantifiers applied to it. Also let <img alt="{\alpha_m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_m}"/> be a star-free translation of <img alt="{\psi_m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi_m}"/> and the length of <img alt="{\alpha_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_0}"/>, denoted by len<img alt="{(\alpha_0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Calpha_0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\alpha_0)}"/>, equal <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. </p>
<p>
To obtain <img alt="{\alpha_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_{1}}"/> from <img alt="{\alpha_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_0}"/> we represent <img alt="{\alpha_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha_0}"/> by a union over crossing edges between <img alt="{Q_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_1}"/> and <img alt="{Q_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_2}"/> and the final states of <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> of the concatenation of prefix and suffix languages and a crossing character between them. We have that <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> gives a decent approximation for—worst-case—length of the prefix languages, suffix languages and the product of crossing edges and final states. Using these approximations we have <img alt="{\mathit{len}(\alpha_1)\approx(2n+1)(n)\approx n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathit%7Blen%7D%28%5Calpha_1%29%5Capprox%282n%2B1%29%28n%29%5Capprox+n%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathit{len}(\alpha_1)\approx(2n+1)(n)\approx n^2}"/>. Iterating gives <img alt="{\mathit{len}(\alpha_2)\approx(2\mathit{len}(\alpha_1)+1)\mathit{len}(\alpha_1)\approx n^4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathit%7Blen%7D%28%5Calpha_2%29%5Capprox%282%5Cmathit%7Blen%7D%28%5Calpha_1%29%2B1%29%5Cmathit%7Blen%7D%28%5Calpha_1%29%5Capprox+n%5E4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathit{len}(\alpha_2)\approx(2\mathit{len}(\alpha_1)+1)\mathit{len}(\alpha_1)\approx n^4}"/>, <img alt="{\mathit{len}(\alpha_3)\approx n^8}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathit%7Blen%7D%28%5Calpha_3%29%5Capprox+n%5E8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathit{len}(\alpha_3)\approx n^8}"/>, and in general, <img alt="{\mathit{len}(\alpha_m)\approx ({n^{2^m}})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathit%7Blen%7D%28%5Calpha_m%29%5Capprox+%28%7Bn%5E%7B2%5Em%7D%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathit{len}(\alpha_m)\approx ({n^{2^m}})}"/>. This says that our blowup could be doubly exponential. Is there a tighter estimate?</p>
<p>
Our final question is: how well do our “stacked words” help to visualize the proof? Are they helpful for framing proofs of equivalences between language classes and logics that are higher up?</p>
<p/></font></font></div>
    </content>
    <updated>2020-03-29T19:40:03Z</updated>
    <published>2020-03-29T19:40:03Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Proofs"/>
    <category term="Teaching"/>
    <category term="trick"/>
    <category term="Daniel Winton"/>
    <category term="descriptive complexity"/>
    <category term="Dominique Perrin"/>
    <category term="formal languages"/>
    <category term="Jean-Eric Pin"/>
    <category term="Logic"/>
    <category term="marked words"/>
    <category term="stacked words"/>
    <category term="star-free regular sets"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-04-11T04:20:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4346</id>
    <link href="https://lucatrevisan.wordpress.com/2020/03/29/4346/" rel="alternate" type="text/html"/>
    <title>Another dispatch from Milan</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">By virtue of being one or two weeks ahead of the rest of the Western world, Italy has been giving advance notices to other countries about what to expect in the covid19 epidemic. For this reason, friends from other countries … <a href="https://lucatrevisan.wordpress.com/2020/03/29/4346/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>By virtue of being one or two weeks ahead of the rest of the Western world, Italy has been giving advance notices to other countries about what to expect in the covid19 epidemic. For this reason, friends from other countries have been frequently asking me some questions, whose answers I would like to share.</p>
<p><span id="more-4346"/></p>
<p><b>Will lockdowns work in countries that are not big on rules-following?</b></p>
<p>Apparently, yes. The number of confirmed cases is not a very reliable signal of what is happening, because it is tied as much to the testing capacity as to the actual diffusion of the infection, while the number of deaths is a more informative one (although read below about some issues with how this number is reported). Schools were closed 34 days ago in Lombardy, which went on lockdown 21 days ago, and all of Italy went on lockdown 19 days ago. For the last several days there has been a slowing down in the number of reported deaths, and daily numbers have been decreasing for the last two days. This is a graph updated today of the cumulative number of deaths:</p>
<p><img alt="italy" class="alignnone size-full wp-image-4349" src="https://lucatrevisan.files.wordpress.com/2020/03/italy.png?w=584"/><br/>
(Data from Protezione Civile, chart by me)</p>
<p><b>How does this compare to other places?</b></p>
<p>This graph shows cumulative number of deaths in NYC and in Lombardy, which have a similar population, shifting the NYC data by 17 days:</p>
<p><img alt="lombardy-nyc" class="alignnone size-full wp-image-4350" src="https://lucatrevisan.files.wordpress.com/2020/03/lombardy-nyc.png?w=584"/><br/>
(Data from Protezione Civile and NYT, chart by me)</p>
<p>In Lombardy, schools were closed on February 24 (along with universities, museums, cinemas and several other places) and the lockdown started on March 8; in the shifted timeline, the New York lockdown started on March 3, so one should hope for an earlier slowdown and that NYC will do a lot better than Lombardy.</p>
<p><b>But how bad would things be without restrictive measures?</b></p>
<p>Some smaller towns in Lombardy, where the virus might have circulated for several weeks before the lockdown, have had substantial increases in all-cause deaths. These go from 4x the usual number over the last three months to 7x the usual number during the first three weeks of March. These excess deaths are more than the number of reported covid19 deaths from such towns, and this should be taken into account when looking at reported mortality in Lombardy. Already, the excess deaths in these small towns account for about 0.7% of the population, suggesting that worst-case scenarios of 2 million deaths in the US and of half a million in the UK without mitigation/containment measures made sense.</p>
<p><b>What’s it like after three weeks of lockdown?</b></p>
<p>The mood in the country seems to be shifting: after all  the singing from the balconies, and the baking of cakes, and the practicing of yoga and the posting on Instagram of all of the above, the mood is souring a bit. Unions and employers are at odds on when to reopen factories and many families are living on their savings and wondering how long they can manage to do so. People whose work relates to tourism and hospitality (which accounts for a  very large fraction of the country’s GDP) are wondering not just when  they will be able to reopen their business or return to their job, but if they will be able to do so.</p>
<p>Because of the way the Euro works, Italy cannot just decide to embark on a massive stimulus program. This is because each state’s budget deficit “creates” Euros, and so there is a common policy in the “Eurozone” that limits each state deficit-spending. While this is being negotiated, the Italian government has committed to 25 billions of extra spending, and there is already a big scramble to lobby for who and how should get various parts of this stimulus program.</p>
<p><b>What will it be like after the lockdown is lifted?</b></p>
<p>I am worried that there is no official plan for that. </p>
<p>I hope that there are lots of competent experts working in secret on a plan that takes the best of the contact-tracing and isolation strategies of Korea, Singapore and Taiwan, injects in it the strong tradition of privacy laws that (little known fact) Italy pioneered in Europe even before the GDPR, and creates a wonderfully functioning system.</p>
<p>Hey, once you are hoping, you may as well hope big!</p>
<p><b>What are good sources for predictions of future scenarios?</b></p>
<p>Memes! It has been mind-bending how yesterday’s satire becomes tomorrow’s news.</p>
<p>For example, this meme started circulating on March 14, in which Johnson says “because of the coronavirus, we should prepare to lose some of our loved ones,” to which the queen replies “I am sorry for you and your family.” Johnson tested positive two weeks later.</p>
<p><img alt="fullsizeoutput_18d8" class="alignnone size-full wp-image-4351" src="https://lucatrevisan.files.wordpress.com/2020/03/fullsizeoutput_18d8.jpeg?w=584"/></p>
<p>On March 15, the Pope visited a church in (a deserted) central Rome, where there is a crucifix that, according to tradition, survived unscathed a fire in 1519 and, in 1522, was taken around the city and stopped an epidemic of bubonic plague. The pope prayed for the end of the covid19 epidemic.</p>
<p>On March 24, Lercio, the Italian equivalent of The Onion, published an article titled <a href="https://www.lercio.it/forse-non-ha-capito-bene-papa-francesco-chiede-di-nuovo-a-dio-di-fermare-lepidemia/">“‘Maybe He didn’t hear me the first time’: Pope Francis asks God again to stop the epidemic”</a>.</p>
<p>Sure enough, on March 27 the Pope prayed again for the end of the epidemic, alone in a deserted Saint Peter square.</p>
<p><img alt="Pope Francis extraordinary Urbi et Orbi blessing during the coronavirus crisis" class="alignnone size-full wp-image-4352" src="https://lucatrevisan.files.wordpress.com/2020/03/8e6a2f2ff6542ea2870a1a4c13f6fd06.jpg?w=584"/><br/>
(Photo credit: Yara Nardi and Vatican Press Office)</p>
<p>The images of the pope walking alone at dusk toward the stage on which he prayed were something no disaster movie had prepared us for. Of course people were able to see the humor in that as well.</p>
<p><img alt="IMG_9423" class="alignnone size-full wp-image-4354" src="https://lucatrevisan.files.wordpress.com/2020/03/img_9423.jpg?w=584"/></p>
<p><b>Edited to add:</b> On March 13, The Onion published an article titled <a href="https://www.theonion.com/health-experts-worry-coronavirus-will-overwhelm-america-1842314132">Health Experts Worry Coronavirus Will Overwhelm America’s GoFundMe System</a>. On March 26, The New York Times published an article titled <a href="https://www.nytimes.com/2020/03/26/style/gofundme-coronavirus.html">GoFundMe Confronts Coronavirus Demand</a>.</p></div>
    </content>
    <updated>2020-03-29T18:27:41Z</updated>
    <published>2020-03-29T18:27:41Z</published>
    <category term="Milan"/>
    <category term="New York"/>
    <category term="covid-19"/>
    <category term="Pope Francis"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-04-11T04:20:25Z</updated>
    </source>
  </entry>
</feed>
