<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-07-14T17:39:04Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18952</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/" rel="alternate" type="text/html"/>
    <title>Socially Reproduced Experiments</title>
    <summary>We must avoid becoming one Cropped from USA Today source José Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseball’s most dramatic cheating accusation of 2019. Today, at baseball’s All-Star break, we review this and other social […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>We must avoid becoming one</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/altuvehr/" rel="attachment wp-att-18954"><img alt="" class="alignright size-full wp-image-18954" height="151" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AltuveHR.jpg?resize=151%2C151&amp;ssl=1" width="151"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from USA Today <a href="https://www.usatoday.com/story/sports/mlb/2021/07/11/astros-jose-altuve-walk-off-hr-shirt-ripped-off-yankees/7933018002/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
José Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseball’s most dramatic cheating accusation of 2019.</p>
<p>
Today, at baseball’s All-Star break, we review this and other social experiments that have quite a bit more data.</p>
<p>
Altuve won the 2019 American League Championship series with a pinch-hit homer in the bottom of the ninth against the Yankees’ closer, Aroldis Chapman. As he approached home plate, he was <a href="https://www.youtube.com/watch?v=MTNBnk1dz6g">seen</a> telling his teammates waiting to mob him at the plate not to rip off his jersey in celebration. He subsequently <a href="https://youtu.be/-ryvOya4PoE?t=247">scooted</a> into the corridor behind the dugout, then re-emerged into the on-field celebration. This fed accusations that he had been wired with a buzzer to know what kind of pitch was coming from Chapman, in line with <a href="https://en.wikipedia.org/wiki/Houston_Astros_sign_stealing_scandal">sign-stealing</a> by other means from the Astros’ 2017 championship season and 2018 that was proven and punished by Major League Baseball. </p>
<p>
Almost the same scenario was reproduced Sunday: Houston down 7-5 against the Yankees with two out and two on base in the ninth, Altuve up against the Yankees’ closer (Chad Green recently supplanting Chapman). Altuve <a href="https://www.youtube.com/watch?v=OHrc6OdYOpo">socked</a> a homer to the same part of the ballpark to complete a shocking six-run comeback. Immediately upon touching home, he had his shirt ripped off to reveal nothing but the top half of his birthday suit. This was the most direct way possible to witness that he could have hit the other homer without illegal information.</p>
<p>
</p><p/><h2> Examples and Non-Examples </h2><p/>
<p/><p>
I have dealt with chess-cheating cases in which electronic buzzing has been specifically alleged, including the two <a href="https://en.wikipedia.org/wiki/Borislav_Ivanov#Retirement_from_competitive_chess_and_brief_return_to_chess-related_activities">most</a> prominent <a href="https://www.nytimes.com/2013/08/18/crosswords/chess/a-master-is-disqualified-over-suspicions-of-cheating.html">cases</a> of 2013. I will not take this post further in this direction, however, but rather to pose this question:</p>
<blockquote><p><b> </b> <em> What is considered a “social proof” of an assertion—especially when there are elements of scientific control and reproduction? </em>
</p></blockquote>
<p/><p>
A simple example is a police lineup. This tries to control for whether a witness has previously seen the accused by including the accused among usually four or five similarly represented people. Picking the right person is considered to prove the previous encounter. Statistically, however, this is a <a href="https://en.wikipedia.org/wiki/P-value"><img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-value</a> of only 0.20 or 0.167, which are not considered significant at even the weakest level of “statistical proof.” Allowing <a href="https://www.apa.org/monitor/julaug04/lineups">null</a> lineups does not change the statistics much.</p>
<p>
Baseball gives a non-example that surprises me. One of the bad performances that cost Chapman his closer role was losing an 8-4 lead against the Los Angeles Angels on June 30. As a fantasy-baseball player, I’ve regularly observed poor pitching (by the closers on my “fantasy team”) when the lead is too large to earn credit for a coveted <a href="https://en.wikipedia.org/wiki/Save_(baseball)">save</a>. Does the data reproduce a phenomenon of closers bearing down less when way ahead, with no “save” to gain? A <a href="https://www.beyondtheboxscore.com/2014/1/27/5344580/the-closer-mentality-part-1-closers-in-non-save-situations">study</a> after the 2013 season, which cleverly represented performances by the same <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scores I use in chess, found none. “Meltdowns” like Chapman’s are offset by cases where closers pitched better. The <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scores in the study are all in the range -1.25 to +1.50 anyway, which count as statistically random.</p>
<p>
This study used a reasonably large data set, one that is well-defined and admits controlling factors such as normalizing for game circumstances and the quality of the opposing hitters. At least it is more than the two instances of Altuve. In-between would be an attempt to determine whether certain national soccer teams are consistently worse at penalty-kick tiebreaks. England’s and Italy’s teams brought their long tortured histories together in the tiebreak of Sunday’s European Cup final. The Italians missed two of five kicks, a score that often spells doom, but the English missed three.</p>
<p>
</p><p/><h2> Larger Scale </h2><p/>
<p/><p>
Dick and I are really interested in “experiments” that have spilled into society, with minimal controls but large data. One sphere of this is cybersecurity. </p>
<p>
It seems to us that only in the past decade have security experts begun formalizing their research as experimental science with repeatability and reproducibility as explicit criteria. The NSA devoted a special 2012 <a href="https://www.nsa.gov/Portals/70/documents/resources/everyone/digital-media-center/publications/the-next-wave/TNW-19-2.pdf">issue</a> of their <em>Next Wave</em> series to what they titled as “Developing a blueprint for a science of cybersecurity.” Among the contents are:</p>
<ul>
<li>
An introductory essay by Carl Landwehr titled, “Cybersecurity: From engineering to science.” <p/>
</li><li>
A linchpin <a href="https://www.cs.dartmouth.edu/~ccpalmer/teaching/cs55/Resources/Papers/TNW_19_2_BlueprintScienceCybersecurity_Schneider.pdf">paper</a> by Fred Schneider titled, “Blueprint for a science of cybersecurity.” <p/>
</li><li>
A <a href="https://www.cs.cmu.edu/~maxion/pubs/Maxion12.pdf">paper</a> by Roy Maxion titled, “Making experiments dependable,” which came from a 2011 Springer LNCS <a href="https://link.springer.com/book/10.1007/978-3-642-24541-1">Festschrift</a>.
</li></ul>
<p>
Maxion’s main example is <a href="https://en.wikipedia.org/wiki/Keystroke_dynamics">keystroke biometrics</a>. This covers inferences made from typing style on a computer keyboard or mouse or similar handheld input device. This can be used to verify identity or screen for malfeasant activities. Online chess playing platforms collect data of this nature—okay we could not resist adding chess example. </p>
<p>
Another area is experiments designed to simulate attacks and test defenses against them. Schneider’s paper begins with a contrast between <em>predictive</em> modeling versus <em>reactive</em> handling of them. About the latter, he draws an analogy with health care:</p>
<blockquote><p><b> </b> <em> “Some health problems are best handled in a reactive manner. We know what to do when somebody breaks a finger, and each year we create a new influenza vaccine in anticipation of the flu season to come. But only after making significant investments in basic medical sciences are we starting to understand the mechanisms by which cancers grow, and a cure seems to require that kind of deep understanding.” </em>
</p></blockquote>
<p/><p>
He goes on to outline the kind of scientific foundation that could hopefully underlie a ‘cure’ for intrusion and malware and the like. </p>
<p>
What we have seen happen especially in the past months, however—in both health and security—is uncontrolled experiments with society as the domain. Large-scale ransomware attacks are becoming as frequent as hurricanes and heat waves. And of course, the pandemic. These share with Altuve the property of being one-off instances, but have large data on the receiving end.</p>
<p>
</p><p/><h2> Summer Pandemic Update </h2><p/>
<p/><p>
The following chart updates our June 20 <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">post</a> on the state of the pandemic and its projection for the summer—for Florida and the United Kingdom in particular:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/flukcases071321/" rel="attachment wp-att-18955"><img alt="" class="aligncenter wp-image-18955" height="440" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/FLUKCases071321.png?resize=450%2C440&amp;ssl=1" width="450"/></a></p>
<p>
The vertical line shows about where the charts were on June 20. The past few days are the first where we can point to a significant rise in Florida, though Missouri had a similar rise last week and it is showing up in some other states.  The charts are taken from the <em>Worldometer</em> coronavirus <a href="https://www.worldometers.info/coronavirus/">pages</a>. </p>
<p>
The UK rise looks ghastly. It was a subtext of our previous post to worry that allowing the large dense soccer crowds at London’s Wembley Stadium for the semis and final—and anything similar in baseball—would stoke the rise in our respective countries even more. However, the rate of hospitalizations in the UK has remained largely flat. This Fortune <a href="https://fortune.com/2021/07/08/kids-vulnerable-covid-delta-variant-vaccinated-europe/">article</a> last week is one of several attesting that the new cases are mostly in children or in vaccinated people with enough immunity to contain the “breakthrough” positive. The UK is going ahead with large-scale re-openings later this month, with the portion of those 18 and older who have had one dose approaching 90% and those fully vaccinated coming past 70%. The latter number in relation to the whole population is about 52%.</p>
<p>
The US looks like becoming an experiment in how the local vaccination rate affects the numbers. The rates of those fully vaccinated by state are currently eerily <a href="https://www.cnn.com/2021/07/08/politics/electoral-map-vaccine-map-covid-19/index.html">similar</a> to Joe Biden’s vote percentage in the state. One aspect of scientific reproducibility is the size of the simplest classifier of the results. For a presidential vote to have simpler explaining power than any factors of biology or other life circumstances would make a strange experiment indeed.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Dick and I tried to come up with other examples—from computer security in particular—to sustain what has been occupying our thoughts about standards of proof for policy. We would welcome some examples from you, our readers. </p>
<p>
And of course, we have been concerned about the present course of the pandemic amid re-openings since the referenced post last month. In the meantime, if it is your taste, please enjoy the All-Star Game, which Altuve is, ironically, <a href="https://calltothepen.com/2021/07/11/houston-astros-officially-skipping-star-game/">skipping</a>.</p>
<p/><p><br/>
[some word fixes and changes]</p></font></font></div>
    </content>
    <updated>2021-07-13T19:47:02Z</updated>
    <published>2021-07-13T19:47:02Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="baseball"/>
    <category term="coronavirus"/>
    <category term="Jose Altuve"/>
    <category term="pandemic"/>
    <category term="reproducibility"/>
    <category term="science"/>
    <category term="security"/>
    <category term="social process"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-07-14T17:37:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/102</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/102" rel="alternate" type="text/html"/>
    <title>TR21-102 |  Tight bounds on the Fourier growth of bounded functions on the hypercube | 

	Siddharth Iyer, 

	Anup Rao, 

	Victor Reis, 

	Thomas Rothvoss, 

	Amir Yehudayoff</title>
    <summary>We give tight bounds on the degree $\ell$  homogenous parts $f_\ell$ of a bounded function $f$ on the cube. We show that if $f: \{\pm 1\}^n \rightarrow [-1,1]$ has degree $d$, then $\| f_\ell \|_\infty$ is bounded by $d^\ell/\ell!$, and $\| \hat{f}_\ell \|_1$ is bounded by $d^\ell e^{{\ell+1 \choose 2}} n^{\frac{\ell-1}{2}}$. We describe applications to pseudorandomness and learning theory. We use similar methods to generalize the classical Pisier's inequality from convex analysis. Our analysis  involves properties of real-rooted polynomials that may be useful elsewhere.</summary>
    <updated>2021-07-13T17:55:40Z</updated>
    <published>2021-07-13T17:55:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-14T17:37:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/101</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/101" rel="alternate" type="text/html"/>
    <title>TR21-101 |  A Parallel Repetition Theorem for the GHZ Game: A Simpler Proof | 

	Uma Girish, 

	Justin Holmgren, 

	Kunal Mittal, 

	Ran Raz, 

	Wei Zhan</title>
    <summary>We give a new proof of the fact that the parallel repetition of the (3-player) GHZ game reduces the value of the game to zero polynomially quickly. That is, we show that the value of the $n$-fold GHZ game is at most $n^{-\Omega(1)}$. This was first established by Holmgren and Raz [HR20]. We present a new proof of this theorem that we believe to be simpler and more direct. Unlike most previous works on parallel repetition, our proof makes no use of information theory, and relies on the use of Fourier analysis.

The GHZ game [GHZ89] has played a foundational role in the understanding of quantum information theory, due in part to the fact that quantum strategies can win the GHZ game with probability $1$. It is possible that improved parallel repetition bounds may find applications in this setting.

Recently, Dinur, Harsha, Venkat, and Yuen [DHVY17] highlighted the GHZ game as a simple three-player game, which is in some sense maximally far from the class of multi-player games whose behavior under parallel repetition is well understood. Dinur et al. conjectured that parallel repetition decreases the value of the GHZ game exponentially quickly, and speculated that progress on proving this would shed light on parallel repetition for general multi-player (multi-prover) games.</summary>
    <updated>2021-07-13T14:48:27Z</updated>
    <published>2021-07-13T14:48:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-14T17:37:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://sarielhp.org/blog/?p=9420</id>
    <link href="https://sarielhp.org/blog/?p=9420" rel="alternate" type="text/html"/>
    <title>FSTTCS 2021 deadline is this Monday…</title>
    <summary>The link is here. Due to the pandemic it is going to be virtual. Quoting Bob Dylan, this blog is not dead, it is just asleep.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The link is <a href="https://www.fsttcs.org.in/2021/" rel="noreferrer noopener" target="_blank">here</a>. Due to the pandemic it is going to be virtual.</p>



<p>Quoting Bob Dylan,  this blog is not dead, it is just asleep.</p></div>
    </content>
    <updated>2021-07-13T03:38:25Z</updated>
    <published>2021-07-13T03:38:25Z</published>
    <category term="Research"/>
    <category term="cs theory"/>
    <author>
      <name>Sariel</name>
    </author>
    <source>
      <id>https://sarielhp.org/blog</id>
      <link href="https://sarielhp.org/blog/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://sarielhp.org/blog" rel="alternate" type="text/html"/>
      <subtitle>Sariel's blog</subtitle>
      <title>Vanity of Vanities, all is Vanity</title>
      <updated>2021-07-14T17:37:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05355</id>
    <link href="http://arxiv.org/abs/2107.05355" rel="alternate" type="text/html"/>
    <title>On the Computational Complexity of the Chain Rule of Differential Calculus</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Naumann:Uwe.html">Uwe Naumann</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05355">PDF</a><br/><b>Abstract: </b>Many modern numerical methods in computational science and engineering rely
on derivatives of mathematical models for the phenomena under investigation.
The computation of these derivatives often represents the bottleneck in terms
of overall runtime performance. First and higher derivative tensors need to be
evaluated efficiently.
</p>
<p>The chain rule of differentiation is the fundamental prerequisite for
computing accurate derivatives of composite functions which perform a
potentially very large number of elemental function evaluations. Data flow
dependences amongst the elemental functions give rise to a combinatorial
optimization problem. We formulate {\sc Chain Rule Differentiation} and we
prove it to be NP-complete. Pointers to research on its approximate solution
are given.
</p></div>
    </summary>
    <updated>2021-07-13T22:38:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05296</id>
    <link href="http://arxiv.org/abs/2107.05296" rel="alternate" type="text/html"/>
    <title>Separating LREC from LFP</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dawar:Anuj.html">Anuj Dawar</a>, Felipe Ferreira Santos <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05296">PDF</a><br/><b>Abstract: </b>LREC= is an extension of first-order logic with a logarithmic recursion
operator. It was introduced by Grohe et al. and shown to capture the complexity
class L over trees and interval graphs. It does not capture L in general as it
is contained in FPC - fixed-point logic with counting. We show that this
containment is strict. In particular, we show that the path systems problem, a
classic P-complete problem which is definable in LFP - fixed-point logic - is
not definable in LREC= This shows that the logarithmic recursion mechanism is
provably weaker than general least fixed points. The proof is based on a novel
Spoiler-Duplicator game tailored for this logic.
</p></div>
    </summary>
    <updated>2021-07-13T22:42:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05198</id>
    <link href="http://arxiv.org/abs/2107.05198" rel="alternate" type="text/html"/>
    <title>Finding a Maximum Clique in a Grounded 1-Bend String Graph</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Keil:J=_Mark.html">J. Mark Keil</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mondal:Debajyoti.html">Debajyoti Mondal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moradi:Ehsan.html">Ehsan Moradi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nekrich:Yakov.html">Yakov Nekrich</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05198">PDF</a><br/><b>Abstract: </b>A grounded 1-bend string graph is an intersection graph of a set of polygonal
lines, each with one bend, such that the lines lie above a common horizontal
line $\ell$ and have exactly one endpoint on $\ell$. We show that the problem
of finding a maximum clique in a grounded 1-bend string graph is APX-hard, even
for strictly $y$-monotone strings. For general 1-bend strings, the problem
remains APX-hard even if we restrict the position of the bends and end-points
to lie on at most three parallel horizontal lines. We give fast algorithms to
compute a maximum clique for different subclasses of grounded segment graphs,
which are formed by restricting the strings to various forms of $L$-shapes.
</p></div>
    </summary>
    <updated>2021-07-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05161</id>
    <link href="http://arxiv.org/abs/2107.05161" rel="alternate" type="text/html"/>
    <title>Bounds for Multiple Packing and List-Decoding Error Exponents</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Yihan.html">Yihan Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vatedka:Shashank.html">Shashank Vatedka</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05161">PDF</a><br/><b>Abstract: </b>We revisit the problem of high-dimensional multiple packing in Euclidean
space. Multiple packing is a natural generalization of sphere packing and is
defined as follows. Let $ N&gt;0 $ and $ L\in\mathbb{Z}_{\ge2} $. A multiple
packing is a set $\mathcal{C}$ of points in $ \mathbb{R}^n $ such that any
point in $ \mathbb{R}^n $ lies in the intersection of at most $ L-1 $ balls of
radius $ \sqrt{nN} $ around points in $ \mathcal{C} $. We study the multiple
packing problem for both bounded point sets whose points have norm at most
$\sqrt{nP}$ for some constant $P&gt;0$ and unbounded point sets whose points are
allowed to be anywhere in $ \mathbb{R}^n $. Given a well-known connection with
coding theory, multiple packings can be viewed as the Euclidean analog of
list-decodable codes, which are well-studied for finite fields. In this paper,
we derive various bounds on the largest possible density of a multiple packing
in both bounded and unbounded settings. A related notion called average-radius
multiple packing is also studied. Some of our lower bounds exactly pin down the
asymptotics of certain ensembles of average-radius list-decodable codes, e.g.,
(expurgated) Gaussian codes and (expurgated) Poisson Point Processes. To this
end, we apply tools from high-dimensional geometry and large deviation theory.
Some of our lower bounds on the optimal multiple packing density are the best
known lower bounds. These bounds are obtained via a proxy known as error
exponent. The latter quantity is the best exponent of the probability of
list-decoding error when the code is corrupted by a Gaussian noise. We
establish a curious inequality which relates the error exponent, a quantity of
average-case nature, to the list-decoding radius, a quantity of worst-case
nature. We derive various bounds on the error exponent in both bounded and
unbounded settings which are of independent interest beyond multiple packing.
</p></div>
    </summary>
    <updated>2021-07-13T22:38:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05128</id>
    <link href="http://arxiv.org/abs/2107.05128" rel="alternate" type="text/html"/>
    <title>Karchmer-Wigderson Games for Hazard-free Computation</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ikenmeyer:Christian.html">Christian Ikenmeyer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Komarath:Balagopal.html">Balagopal Komarath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saurabh:Nitin.html">Nitin Saurabh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05128">PDF</a><br/><b>Abstract: </b>We present a Karchmer-Wigderson game to study the complexity of hazard-free
formulas. This new game is both a generalization of the monotone
Karchmer-Wigderson game and an analog of the classical Boolean
Karchmer-Wigderson game. Therefore, it acts as a bridge between the existing
monotone and general games.
</p>
<p>Using this game, we prove hazard-free formula size and depth lower bounds
that are provably stronger than those possible by the standard technique of
transferring results from monotone complexity in a black-box fashion. For the
multiplexer function we give (1) a hazard-free formula of optimal size and (2)
an improved low-depth hazard-free formula of almost optimal size and (3) a
hazard-free formula with alternation depth $2$ that has optimal depth. We then
use our optimal constructions to obtain an improved universal worst-case
hazard-free formula size upper bound. We see our results as a significant step
towards establishing hazard-free computation as an independent missing link
between Boolean complexity and monotone complexity.
</p></div>
    </summary>
    <updated>2021-07-13T22:41:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05082</id>
    <link href="http://arxiv.org/abs/2107.05082" rel="alternate" type="text/html"/>
    <title>On Universal D-Semifaithful Coding for Memoryless Sources with Infinite Alphabets</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Jorge_F=.html">Jorge F. Silva</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Piantanida:Pablo.html">Pablo Piantanida</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05082">PDF</a><br/><b>Abstract: </b>The problem of variable length and fixed-distortion universal source coding
(or D-semifaithful source coding) for stationary and memoryless sources on
countably infinite alphabets ($\infty$-alphabets) is addressed in this paper.
The main results of this work offer a set of sufficient conditions (from weaker
to stronger) to obtain weak minimax universality, strong minimax universality,
and corresponding achievable rates of convergences for the worse-case
redundancy for the family of stationary memoryless sources whose densities are
dominated by an envelope function (or the envelope family) on
$\infty$-alphabets. An important implication of these results is that universal
D-semifaithful source coding is not feasible for the complete family of
stationary and memoryless sources on $\infty$-alphabets. To demonstrate this
infeasibility, a sufficient condition for the impossibility is presented for
the envelope family. Interestingly, it matches the well-known impossibility
condition in the context of lossless (variable-length) universal source coding.
More generally, this work offers a simple description of what is needed to
achieve universal D-semifaithful coding for a family of distributions
$\Lambda$. This reduces to finding a collection of quantizations of the product
space at different block-lengths -- reflecting the fixed distortion restriction
-- that satisfy two asymptotic requirements: the first is a universal
quantization condition with respect to $\Lambda$, and the second is a vanishing
information radius (I-radius) condition for $\Lambda$ reminiscent of the
condition known for lossless universal source coding.
</p></div>
    </summary>
    <updated>2021-07-13T23:00:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05036</id>
    <link href="http://arxiv.org/abs/2107.05036" rel="alternate" type="text/html"/>
    <title>Algorithms for Floor Planning with Proximity Requirements</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klawitter:Jonathan.html">Jonathan Klawitter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klesen:Felix.html">Felix Klesen</a>, Alexander Wolff <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05036">PDF</a><br/><b>Abstract: </b>Floor planning is an important and difficult task in architecture. When
planning office buildings, rooms that belong to the same organisational unit
should be placed close to each other. This leads to the following NP-hard
mathematical optimization problem. Given the outline of each floor, a list of
room sizes, and, for each room, the unit to which it belongs, the aim is to
compute floor plans such that each room is placed on some floor and the total
distance of the rooms within each unit is minimized.
</p>
<p>The problem can be formulated as an integer linear program (ILP). Commercial
ILP solvers exist, but due to the difficulty of the problem, only small to
medium instances can be solved to (near-) optimality. For solving larger
instances, we propose to split the problem into two subproblems; floor
assignment and planning single floors. We formulate both subproblems as ILPs
and solve realistic problem instances. Our experimental study shows that the
problem helps to reduce the computation time considerably. Where we were able
to compute the global optimum, the solution cost of the combined approach
increased very little.
</p></div>
    </summary>
    <updated>2021-07-13T23:12:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.05018</id>
    <link href="http://arxiv.org/abs/2107.05018" rel="alternate" type="text/html"/>
    <title>CLAP: A New Algorithm for Promise CSPs</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Lorenzo Ciardo, Stanislav Živný <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.05018">PDF</a><br/><b>Abstract: </b>We propose a new algorithm for Promise Constraint Satisfaction Problems
(PCSPs). It is a combination of the Constraint Basic LP relaxation and the
Affine IP relaxation (CLAP). We give a characterisation of the power of CLAP in
terms of a minion homomorphism. Using this characterisation, we identify a
certain weak notion of symmetry which, if satisfied by infinitely many
polymorphisms of PCSPs, guarantees tractability.
</p>
<p>We demonstrate that there are PCSPs solved by CLAP that are not solved by any
of the existing algorithms for PCSPs; in particular, not by the BLP+AIP
algorithm of Brakensiek and Guruswami [SODA'20] and not by a reduction to
tractable finite-domain CSPs.
</p></div>
    </summary>
    <updated>2021-07-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04997</id>
    <link href="http://arxiv.org/abs/2107.04997" rel="alternate" type="text/html"/>
    <title>Efficient and Effective Algorithms for Revenue Maximization in Social Advertising Efficient and Effective Algorithms for Revenue Maximization in Social Advertising</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Han:Kai.html">Kai Han</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Benwei.html">Benwei Wu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Jing.html">Jing Tang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cui:Shuang.html">Shuang Cui</a>, Cigdem Aslay, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lakshmanan:Laks_V=_S=.html">Laks V. S. Lakshmanan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04997">PDF</a><br/><b>Abstract: </b>We consider the revenue maximization problem in social advertising, where a
social network platform owner needs to select seed users for a group of
advertisers, each with a payment budget, such that the total expected revenue
that the owner gains from the advertisers by propagating their ads in the
network is maximized. Previous studies on this problem show that it is
intractable and present approximation algorithms. We revisit this problem from
a fresh perspective and develop novel efficient approximation algorithms, both
under the setting where an exact influence oracle is assumed and under one
where this assumption is relaxed. Our approximation ratios significantly
improve upon the previous ones. Furthermore, we empirically show, using
extensive experiments on four datasets, that our algorithms considerably
outperform the existing methods on both the solution quality and computation
efficiency.
</p></div>
    </summary>
    <updated>2021-07-13T23:02:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04993</id>
    <link href="http://arxiv.org/abs/2107.04993" rel="alternate" type="text/html"/>
    <title>The Mixed Page Number of Graphs</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alam:Jawaherul_Md=.html">Jawaherul Md. Alam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bekos:Michael_A=.html">Michael A. Bekos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gronemann:Martin.html">Martin Gronemann</a>, Michael Kaufmann, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pupyrev:Sergey.html">Sergey Pupyrev</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04993">PDF</a><br/><b>Abstract: </b>A linear layout of a graph typically consists of a total vertex order, and a
partition of the edges into sets of either non-crossing edges, called stacks,
or non-nested edges, called queues. The stack (queue) number of a graph is the
minimum number of required stacks (queues) in a linear layout. Mixed linear
layouts combine these layouts by allowing each set of edges to form either a
stack or a queue. In this work we initiate the study of the mixed page number
of a graph which corresponds to the minimum number of such sets.
</p>
<p>First, we study the edge density of graphs with bounded mixed page number.
Then, we focus on complete and complete bipartite graphs, for which we derive
lower and upper bounds on their mixed page number. Our findings indicate that
combining stacks and queues is more powerful in various ways compared to the
two traditional layouts.
</p></div>
    </summary>
    <updated>2021-07-13T23:05:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04919</id>
    <link href="http://arxiv.org/abs/2107.04919" rel="alternate" type="text/html"/>
    <title>Analysis of Smooth Heaps and Slim Heaps</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hartmann:Maria.html">Maria Hartmann</a>, László Kozma, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sinnamon:Corwin.html">Corwin Sinnamon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tarjan:Robert_E=.html">Robert E. Tarjan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04919">PDF</a><br/><b>Abstract: </b>The smooth heap is a recently introduced self-adjusting heap [Kozma,
Saranurak, 2018] similar to the pairing heap [Fredman, Sedgewick, Sleator,
Tarjan, 1986]. The smooth heap was obtained as a heap-counterpart of Greedy
BST, a binary search tree updating strategy conjectured to be
\emph{instance-optimal} [Lucas, 1988], [Munro, 2000]. Several adaptive
properties of smooth heaps follow from this connection; moreover, the smooth
heap itself has been conjectured to be instance-optimal within a certain class
of heaps. Nevertheless, no general analysis of smooth heaps has existed until
now, the only previous analysis showing that, when used in \emph{sorting mode}
($n$ insertions followed by $n$ delete-min operations), smooth heaps sort $n$
numbers in $O(n\lg n)$ time.
</p>
<p>In this paper we describe a simpler variant of the smooth heap we call the
\emph{slim heap}. We give a new, self-contained analysis of smooth heaps and
slim heaps in unrestricted operation, obtaining amortized bounds that match the
best bounds known for self-adjusting heaps. Previous experimental work has
found the pairing heap to dominate other data structures in this class in
various settings. Our tests show that smooth heaps and slim heaps are
competitive with pairing heaps, outperforming them in some cases, while being
comparably easy to implement.
</p></div>
    </summary>
    <updated>2021-07-13T22:51:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04763</id>
    <link href="http://arxiv.org/abs/2107.04763" rel="alternate" type="text/html"/>
    <title>Hitting Weighted Even Cycles in Planar Graphs</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=ouml=ke:Alexander.html">Alexander Göke</a>, Jochen Koenemann, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mnich:Matthias.html">Matthias Mnich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Hao.html">Hao Sun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04763">PDF</a><br/><b>Abstract: </b>A classical branch of graph algorithms is graph transversals, where one seeks
&lt;a minimum-weight subset of nodes in a node-weighted graph $G$ which intersects
all copies of subgraphs $F$ from a fixed family $\mathcal F$.
</p>
<p>Many such graph transversal problems have been shown to admit polynomial-time
approximation schemes (PTAS) for planar input graphs $G$, using a variety of
techniques like the shifting technique (Baker, J. ACM 1994), bidimensionality
(Fomin et al., SODA 2011), or connectivity domination (Cohen-Addad et al., STOC
2016).
</p>
<p>These techniques do not seem to apply to graph transversals with parity
constraints, which have recently received significant attention, but for which
no PTASs are known.
</p>
<p>In the even-cycle transversal (ECT) problem, the goal is to find a
minimum-weight hitting set for the set of even cycles in an undirected graph.
</p>
<p>For ECT, Fiorini et al. (IPCO 2010) showed that the integrality gap of the
standard covering LP relaxation is $\Theta(\log n)$, and that adding sparsity
inequalities reduces the integrality gap to~10.
</p></div>
    </summary>
    <updated>2021-07-13T22:57:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04754</id>
    <link href="http://arxiv.org/abs/2107.04754" rel="alternate" type="text/html"/>
    <title>Lower Bounds for Prior Independent Algorithms</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Jason Hartline, Aleck Johnsen <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04754">PDF</a><br/><b>Abstract: </b>The prior independent framework for algorithm design considers how well an
algorithm that does not know the distribution of its inputs approximates the
expected performance of the optimal algorithm for this distribution. This paper
gives a method that is agnostic to problem setting for proving lower bounds on
the prior independent approximation factor of any algorithm. The method
constructs a correlated distribution over inputs that can be generated both as
a distribution over i.i.d. good-for-algorithms distributions and as a
distribution over i.i.d. bad-for-algorithms distributions. Prior independent
algorithms are upper-bounded by the optimal algorithm for the latter
distribution even when the true distribution is the former. Thus, the ratio of
the expected performances of the Bayesian optimal algorithms for these two
decompositions is a lower bound on the prior independent approximation ratio.
The techniques of the paper connect prior independent algorithm design, Yao's
Minimax Principle, and information design. We apply this framework to give new
lower bounds on several canonical prior independent mechanism design problems.
</p></div>
    </summary>
    <updated>2021-07-13T22:51:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04706</id>
    <link href="http://arxiv.org/abs/2107.04706" rel="alternate" type="text/html"/>
    <title>Smaller ACC0 Circuits for Symmetric Functions</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chapman:Brynmor.html">Brynmor Chapman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Ryan.html">Ryan Williams</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04706">PDF</a><br/><b>Abstract: </b>What is the power of constant-depth circuits with $MOD_m$ gates, that can
count modulo $m$? Can they efficiently compute MAJORITY and other symmetric
functions? When $m$ is a constant prime power, the answer is well understood:
Razborov and Smolensky proved in the 1980s that MAJORITY and $MOD_m$ require
super-polynomial-size $MOD_q$ circuits, where $q$ is any prime power not
dividing $m$. However, relatively little is known about the power of $MOD_m$
circuits for non-prime-power $m$. For example, it is still open whether every
problem in $EXP$ can be computed by depth-$3$ circuits of polynomial size and
only $MOD_6$ gates.
</p>
<p>We shed some light on the difficulty of proving lower bounds for $MOD_m$
circuits, by giving new upper bounds. We construct $MOD_m$ circuits computing
symmetric functions with non-prime power $m$, with size-depth tradeoffs that
beat the longstanding lower bounds for $AC^0[m]$ circuits for prime power $m$.
Our size-depth tradeoff circuits have essentially optimal dependence on $m$ and
$d$ in the exponent, under a natural circuit complexity hypothesis.
</p>
<p>For example, we show for every $\varepsilon &gt; 0$ that every symmetric
function can be computed with depth-3 $MOD_m$ circuits of
$\exp(O(n^{\varepsilon}))$ size, for a constant $m$ depending only on
$\varepsilon &gt; 0$. That is, depth-$3$ $CC^0$ circuits can compute any symmetric
function in \emph{subexponential} size. This demonstrates a significant
difference in the power of depth-$3$ $CC^0$ circuits, compared to other models:
for certain symmetric functions, depth-$3$ $AC^0$ circuits require
$2^{\Omega(\sqrt{n})}$ size [H{\aa}stad 1986], and depth-$3$ $AC^0[p^k]$
circuits (for fixed prime power $p^k$) require $2^{\Omega(n^{1/6})}$ size
[Smolensky 1987]. Even for depth-two $MOD_p \circ MOD_m$ circuits,
$2^{\Omega(n)}$ lower bounds were known [Barrington Straubing Th\'erien 1990].
</p></div>
    </summary>
    <updated>2021-07-13T22:39:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04699</id>
    <link href="http://arxiv.org/abs/2107.04699" rel="alternate" type="text/html"/>
    <title>Approximation algorithms for the directed path partition problems</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Yong.html">Yong Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Zhi=Zhong.html">Zhi-Zhong Chen</a>, Curtis Kennedy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Guohui.html">Guohui Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Yao.html">Yao Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:An.html">An Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04699">PDF</a><br/><b>Abstract: </b>Given a directed graph $G = (V, E)$, the $k$-path partition problem is to
find a minimum collection of vertex-disjoint directed paths each of order at
most $k$ to cover all the vertices of $V$. The problem has various applications
in facility location, network monitoring, transportation and others. Its
special case on undirected graphs has received much attention recently, but the
general directed version is seemingly untouched in the literature. We present
the first $k/2$-approximation algorithm, for any $k \ge 3$, based on a novel
concept of augmenting path to minimize the number of singletons in the
partition. When $k \ge 7$, we present an improved $(k+2)/3$-approximation
algorithm based on the maximum path-cycle cover followed by a careful $2$-cycle
elimination process. When $k = 3$, we define the second novel kind of
augmenting paths and propose an improved $13/9$-approximation algorithm.
</p></div>
    </summary>
    <updated>2021-07-13T22:43:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04674</id>
    <link href="http://arxiv.org/abs/2107.04674" rel="alternate" type="text/html"/>
    <title>Preserving Diversity when Partitioning: A Geometric Approach</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perez=Salazar:Sebastian.html">Sebastian Perez-Salazar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Torrico:Alfredo.html">Alfredo Torrico</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Verdugo:Victor.html">Victor Verdugo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04674">PDF</a><br/><b>Abstract: </b>Diversity plays a crucial role in multiple contexts such as team formation,
representation of minority groups and generally when allocating resources
fairly. Given a community composed by individuals of different types, we study
the problem of partitioning this community such that the global diversity is
preserved as much as possible in each subgroup. We consider the diversity
metric introduced by Simpson in his influential work that, roughly speaking,
corresponds to the inverse probability that two individuals are from the same
type when taken uniformly at random, with replacement, from the community of
interest. We provide a novel perspective by reinterpreting this quantity in
geometric terms. We characterize the instances in which the optimal partition
exactly preserves the global diversity in each subgroup. When this is not
possible, we provide an efficient polynomial-time algorithm that outputs an
optimal partition for the problem with two types. Finally, we discuss further
challenges and open questions for the problem that considers more than two
types.
</p></div>
    </summary>
    <updated>2021-07-13T22:57:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04660</id>
    <link href="http://arxiv.org/abs/2107.04660" rel="alternate" type="text/html"/>
    <title>Optimal Space and Time for Streaming Pattern Matching</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mai:Tung.html">Tung Mai</a>, Anup Rao, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rossi:Ryan_A=.html">Ryan A. Rossi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seddighin:Saeed.html">Saeed Seddighin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04660">PDF</a><br/><b>Abstract: </b>In this work, we study longest common substring, pattern matching, and
wildcard pattern matching in the asymmetric streaming model. In this streaming
model, we have random access to one string and streaming access to the other
one. We present streaming algorithms with provable guarantees for these three
fundamental problems. In particular, our algorithms for pattern matching
improve the upper bound and beat the unconditional lower bounds on the memory
of randomized and deterministic streaming algorithms. In addition to this, we
present algorithms for wildcard pattern matching in the asymmetric streaming
model that have optimal space and time.
</p></div>
    </summary>
    <updated>2021-07-13T23:07:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04657</id>
    <link href="http://arxiv.org/abs/2107.04657" rel="alternate" type="text/html"/>
    <title>Constant Delay Lattice Train Schedules</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carufel:Jean=Lou_De.html">Jean-Lou De Carufel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hill:Darryl.html">Darryl Hill</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maheshwari:Anil.html">Anil Maheshwari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roy:Sasanka.html">Sasanka Roy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silveira:Lu=iacute=s_Fernando_Schultz_Xavier_da.html">Luís Fernando Schultz Xavier da Silveira</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04657">PDF</a><br/><b>Abstract: </b>The following geometric vehicle scheduling problem has been considered: given
continuous curves $f_1, \ldots, f_n : \mathbb{R} \rightarrow \mathbb{R}^2$,
find non-negative delays $t_1, \ldots, t_n$ minimizing $\max \{ t_1, \ldots,
t_n \}$ such that, for every distinct $i$ {and $j$} and every time $t$, $| f_j
(t - t_j) - f_i (t - t_i) | &gt; \ell$, where~$\ell$ is a given safety distance.
We study a variant of this problem where we consider trains (rods) of fixed
length $\ell$ that move at constant speed and sets of train lines (tracks),
each of which consisting of an axis-parallel line-segment with endpoints in the
integer lattice $\mathbb{Z}^d$ and of a direction of movement (towards $\infty$
{or $- \infty$}). We are interested in upper bounds on the maximum delay we
need to introduce on any line to avoid collisions, but more specifically on
universal upper bounds that apply no matter the set of train lines. We show
small universal constant upper bounds for $d = 2$ and any given $\ell$ and also
for $d = 3$ and $\ell = 1$. Through clique searching, we are also able to show
that several of these upper bounds are tight.
</p></div>
    </summary>
    <updated>2021-07-13T23:08:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.04654</id>
    <link href="http://arxiv.org/abs/2107.04654" rel="alternate" type="text/html"/>
    <title>Realizable piecewise linear paths of persistence diagrams with Reeb graphs</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Rehab Alharbi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chambers:Erin_Wolf.html">Erin Wolf Chambers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munch:Elizabeth.html">Elizabeth Munch</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.04654">PDF</a><br/><b>Abstract: </b>Reeb graphs are widely used in a range of fields for the purposes of
analyzing and comparing complex spaces via a simpler combinatorial object.
Further, they are closely related to extended persistence diagrams, which
largely but not completely encode the information of the Reeb graph. In this
paper, we investigate the effect on the persistence diagram of a particular
continuous operation on Reeb graphs; namely the (truncated) smoothing
operation. This construction arises in the context of the Reeb graph
interleaving distance, but separately from that viewpoint provides a
simplification of the Reeb graph which continuously shrinks small loops. We
then use this characterization to initiate the study of inverse problems for
Reeb graphs using smoothing by showing which paths in persistence diagram space
(commonly known as vineyards) can be realized by a path in the space of Reeb
graphs via these simple operations. This allows us to solve the inverse problem
on a certain family of piecewise linear vineyards when fixing an initial Reeb
graph.
</p></div>
    </summary>
    <updated>2021-07-13T23:11:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.13362</id>
    <link href="http://arxiv.org/abs/2104.13362" rel="alternate" type="text/html"/>
    <title>There is no APTAS for 2-dimensional vector bin packing: Revisited</title>
    <feedworld_mtime>1626134400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ray:Arka.html">Arka Ray</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.13362">PDF</a><br/><b>Abstract: </b>We study vector bin packing and vector bin covering problems,
multidimensional generalizations of the classical bin packing and bin covering
problems, respectively. In Vector Bin Packing we are given a set of
$d$-dimensional vectors from $(0,1]^d$, and the aim is to partition the set
into the minimum number of bins such that for each bin $B$, we have
$\left\|\sum_{v\in B}v\right\|_\infty\leq 1$. Woeginger [Woe97] claimed that
the problem has no APTAS for dimensions greater than or equal to 2. We note
that there was a slight oversight in the original proof. Hence, we give a
revised proof using some additional ideas from [BCKS06, CC09]. In fact, we show
that it is NP-hard to get an asymptotic approximation ratio better than
$\frac{600}{599}$, for $d=2$.
</p>
<p>In the vector bin covering problem given a set of $d$-dimensional vectors
from $(0,1]^d$, the aim is to obtain a family of disjoint subsets (called bins)
with the maximum cardinality such that for each bin $B$, we have $\sum_{v\in
B}v\geq \mathbf 1$. Using ideas similar to our vector bin packing result, we
show that for vector bin covering there is no APTAS for dimensions greater than
or equal to 2. In fact, we show that it is NP-hard to get an asymptotic
approximation ratio better than $\frac{998}{997}$.
</p></div>
    </summary>
    <updated>2021-07-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4532</id>
    <link href="https://lucatrevisan.wordpress.com/2021/07/12/what-a-difference-a-few-months-can-make/" rel="alternate" type="text/html"/>
    <title>What a difference a few months can make</title>
    <summary>Piazza Duomo, in Milan, on December 26, 2020 Piazza Duomo, in Milan, on July 11, 2021</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Piazza Duomo, in Milan, on December 26, 2020</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure>



<p>Piazza Duomo, in Milan, on July 11, 2021</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure></div>
    </content>
    <updated>2021-07-12T17:04:51Z</updated>
    <published>2021-07-12T17:04:51Z</published>
    <category term="Milan"/>
    <category term="things that are excellent"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2021-07-14T17:37:09Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/exponential-mechanism-bounded-range/</id>
    <link href="https://differentialprivacy.org/exponential-mechanism-bounded-range/" rel="alternate" type="text/html"/>
    <title>A Better Privacy Analysis of the Exponential Mechanism</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A basic and frequent task in data analysis is <em>selection</em> – given a set of options \(\mathcal{Y}\), output the (approximately) best one, where “best” is defined by some loss function \(\ell : \mathcal{Y} \times \mathcal{X}^n \to \mathbb{R}\) and a dataset \(x \in \mathcal{X}^n\). That is, we want to output some \(y \in \mathcal{Y}\) that approximately minimizes \(\ell(y,x)\). Naturally, we are interested in <em>private selection</em> – i.e., the output should be differentially private in terms of the dataset \(x\).
This post discusses algorithms for private selection – in particular, we give an improved privacy analysis of the popular exponential mechanism.</p>

<h2 id="the-exponential-mechanism">The Exponential Mechanism</h2>

<p>The most well-known algorithm for private selection is the <a href="https://en.wikipedia.org/wiki/Exponential_mechanism_(differential_privacy)"><em>exponential mechanism</em></a> <a href="https://doi.org/10.1109/FOCS.2007.66" title="Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007."><strong>[MT07]</strong></a>. The exponential mechanism \(M : \mathcal{X}^n \to \mathcal{Y} \) is a randomized algorithm given by \[\forall x \in \mathcal{X}^n ~ \forall y \in \mathcal{Y} ~~~~~ \mathbb{P}[M(x) = y] = \frac{\exp(-\frac{\varepsilon}{2\Delta} \ell(y,x))}{\sum_{y’ \in \mathcal{Y}} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x)) }, \tag{1}\] where \(\Delta\) is the sensitivity of the loss function \(\ell\) given by \[\Delta = \sup_{x,x’ \in \mathcal{X}^n : d(x,x’) \le 1} \max_{y\in\mathcal{Y}} |\ell(y,x) - \ell(y,x’)|,\tag{2}\] where the supremum is taken over all datasets \(x\) and \(x’\) differing on the data of a single individual (which we denote by \(d(x,x’)\le 1\)).</p>

<p>In terms of utility, we can easily show that <a href="https://arxiv.org/abs/1511.02513" title="Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, Jonathan Ullman. Algorithmic Stability for Adaptive Data Analysis. STOC 2016."><strong>[BNSSSU16]</strong></a> \[\mathbb{E}[\ell(M(x),x)] \le \min_{y \in \mathcal{Y}} \ell(y,x) + \frac{2\Delta}{\varepsilon} \log |\mathcal{Y}|\] for all \(x \in \mathcal{X}^n\) (and we can also give high probability bounds).</p>

<p>It is easy to show that the exponential mechanism satisfies \(\varepsilon\)-differential privacy.
But there is more to this story! We’re going to look at a more refined privacy analysis.</p>

<h2 id="bounded-range">Bounded Range</h2>

<p>The privacy guarantee of the exponential mechanism is more precisely characterized by <em>bounded range</em>. This was observed and defined by David Durfee and Ryan Rogers <a href="https://arxiv.org/abs/1905.04273" title="David Durfee, Ryan Rogers. Practical Differentially Private Top-k Selection with Pay-what-you-get Composition. NeurIPS 2019"><strong>[DR19]</strong></a> and further analyzed later <a href="https://arxiv.org/abs/1909.13830" title="Jinshuo Dong, David Durfee, Ryan Rogers. Optimal Differential Privacy Composition for Exponential Mechanisms. ICML 2020."><strong>[DDR20]</strong></a>.</p>

<blockquote>
  <p><strong>Definition 1 (Bounded Range).</strong><sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1" rel="footnote">1</a></sup> 
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\eta\)-bounded range if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, there exists some \(t \in \mathbb{R}\) such that \[\forall y \in \mathcal{Y} ~~~~~ \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]}\right) \in [t, t+\eta].\] Here \(t\) may depend on the pair of input datasets \(x,x’\), but not on the output \(y\).</p>
</blockquote>

<p>To interpret this definition, we <a href="https://differentialprivacy.org/flavoursofdelta/">recall the definition of the privacy loss random variable</a>: Define \(f : \mathcal{Y} \to \mathbb{R}\) by \[f(y) = \log\left(\frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]}\right).\] Then the privacy loss random variable \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) is given by \(Z = f(M(x))\).</p>

<p>Pure \(\varepsilon\)-differential privacy is equivalent to demanding that the privacy loss is bounded by \(\varepsilon\) – i.e., \(\mathbb{P}[|Z|\le\varepsilon]=1\). Approximate \((\varepsilon,\delta)\)-differential privacy is, roughly, equivalent to demanding that \(\mathbb{P}[Z\le\varepsilon]\ge1-\delta\).<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2" rel="footnote">2</a></sup></p>

<p>Now \(\eta\)-bounded range is simply demanding that the privacy loss \(Z\) is supported on some interval of length \(\eta\). This interval \([t,t+\eta]\) may depend on the pair \(x,x’\).</p>

<p>Bounded range and pure differential privacy are equivalent up to a factor of 2 in the parameters:</p>

<blockquote>
  <p><strong>Lemma 2 (Bounded Range versus Pure Differential Privacy).</strong></p>
  <ul>
    <li>\(\varepsilon\)-differential privacy implies \(\eta\)-bounded range with \(\eta \le 2\varepsilon\).</li>
    <li>\(\eta\)-bounded range implies \(\varepsilon\)-differential privacy with \(\varepsilon \le \eta\).</li>
  </ul>
</blockquote>

<p><em>Proof.</em> The first part of the equivalence follows from the fact that pure \(\varepsilon\)-differential privacy implies the privacy loss is supported on the interval \([-\varepsilon,\varepsilon]\). Thus, if we set \(t=-\varepsilon\) and \(\eta=2\varepsilon\), then \([t,t+\eta] = [-\varepsilon,\varepsilon]\).
The second part follows from the fact that the support of the privacy loss \([t,t+\eta]\) must straddle \(0\). That is, the privacy loss cannot be always positive nor always negative, so \(0 \in [t,t+\eta]\) and, hence, \([t,t+\eta] \subseteq [-\eta,\eta]\). Otherwise \(\forall y ~ f(y)&gt;0\) or \(\forall y ~ f(y)&lt;0\)  would imply \(\forall y ~ \mathbb{P}[M(x)=y]&gt;\mathbb{P}[M(x’)=y]\) or \(\forall y ~ \mathbb{P}[M(x)=y]&lt;\mathbb{P}[M(x’)=y]\), contradicting the fact that \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x)=y] = 1\) and \(\sum_{y \in \mathcal{Y}} \mathbb{P}[M(x’)=y] = 1\). ∎</p>

<p>OK, back to the exponential mechanism:</p>

<blockquote>
  <p><strong>Lemma 3 (The Exponential Mechanism is Bounded Range).</strong>
The exponential mechanism (given in Equation 1 above) satisfies \(\varepsilon\)-bounded range .<sup id="fnref:3"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:3" rel="footnote">3</a></sup></p>
</blockquote>

<p><em>Proof.</em>
We have \[e^{f(y)} = \frac{\mathbb{P}[M(x)=y]}{\mathbb{P}[M(x’)=y]} = \frac{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x))}{\exp(-\frac{\varepsilon}{2\Delta}\ell(y,x’))} \cdot \frac{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x’))}{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x))}.\]
Setting \(t = \log\left(\frac{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x’))}{\sum_{y’} \exp(-\frac{\varepsilon}{2\Delta} \ell(y’,x))}\right) - \frac{\varepsilon}{2}\), we have \[ f(y) = \frac{\varepsilon}{2\Delta} (\ell(y,x’)-\ell(y,x)+\Delta) + t.\]
By the definition of sensitivity (given in Equation 2), we have \( 0 \le \ell(y,x’)-\ell(y,x)+\Delta \le 2\Delta\), whence \(t \le f(y) \le t + \varepsilon\). ∎</p>

<p>Bounded range is not really a useful privacy definition on its own. Thus we’re going to relate it to a relaxed version of differential privacy next.</p>

<h2 id="concentrated-differential-privacy">Concentrated Differential Privacy</h2>

<p>Concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a> and its variants <a href="https://arxiv.org/abs/1603.01887" title="Cynthia Dwork, Guy N. Rothblum. Concentrated Differential Privacy. 2016."><strong>[DR16]</strong></a> <a href="https://arxiv.org/abs/1702.07476" title="Ilya Mironov. R&#xE9;nyi Differential Privacy. CCS 2017."><strong>[M17]</strong></a> are relaxations of pure differential privacy with many nice properties. In particular, it composes very cleanly.</p>

<blockquote>
  <p><strong>Definition 4 (Concentrated Differential Privacy).</strong>
A randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, 
\[\forall \lambda &gt; 0 ~~~~~ \mathbb{E}[\exp( \lambda Z)] \le \exp(\lambda(\lambda+1)\rho),\tag{3}\]
where \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) is the privacy loss random variable.<sup id="fnref:4"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:4" rel="footnote">4</a></sup></p>
</blockquote>

<p>Intuitively, concentrated differential privacy requires that the privacy loss is subgaussian. Specifically, the bound on the moment generating function of \(\rho\)-concentrated differential privacy is tight if the privacy loss \(Z\) follows the distribution \(\mathcal{N}(\rho,2\rho)\). Indeed, the privacy loss random variable of the Gaussian mechanism has such a distribution.<sup id="fnref:5"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:5" rel="footnote">5</a></sup></p>

<p>OK, back to the exponential mechanism:
We know that \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>.
This, of course, applies to the exponential mechaism. A cool fact – that we want to draw more attention to – is that we can do better! 
Specifically, \(\eta\)-bounded range implies \(\frac18 \eta^2\)-concentrated differential privacy <a href="https://arxiv.org/abs/2004.07223" title="Mark Cesar, Ryan Rogers. Bounding, Concentrating, and Truncating: Unifying Privacy Loss Composition for Data Analytics. ALT 2021."><strong>[CR21]</strong></a>.
What follows is a proof of this fact following that of Mark Cesar and Ryan Rogers, but with some simplification.</p>

<blockquote>
  <p><strong>Theorem 5 (Bounded Range implies Concentrated Differential Privacy).</strong>
If \(M\) is \(\eta\)-bounded range, then it is \(\frac18\eta^2\)-concentrated differentially private.</p>
</blockquote>

<p><em>Proof.</em>
Fix datasets \(x,x’ \in \mathcal{X}^n\) differing on a single individual’s data.
Let \(Z \gets \mathsf{PrivLoss}(M(x)\|M(x’))\) be the privacy loss random variable of the mechanism \(M\) on this pair of datasets.
By the definition of bounded range (Definition 1), there exists some \(t \in \mathbb{R}\) such that \(Z \in [t, t+\eta]\) with probability 1.
Now we employ <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_lemma">Hoeffding’s Lemma</a> <a href="https://doi.org/10.1080%2F01621459.1963.10500830" title="Wassily Hoeffding. Probability inequalities for sums of bounded random variables. JASA 1963."><strong>[H63]</strong></a>:</p>
<blockquote>
  <p><strong>Lemma 6 (Hoeffding’s Lemma).</strong>
Let \(X\) be a random variable supported on the interval \([a,b]\). Then, for all \(\lambda \in \mathbb{R}\), we have \[\mathbb{E}[\exp(\lambda X)] \le \exp \left( \mathbb{E}[X] \cdot \lambda + \frac{(b-a)^2}{8} \cdot \lambda^2 \right).\]</p>
</blockquote>

<p>Applying the lemma to the privacy loss gives \[\forall \lambda \in \mathbb{R} ~~~~~  \mathbb{E}[\exp(\lambda Z)] \le \exp \left( \mathbb{E}[Z] \cdot \lambda + \frac{\eta^2}{8} \cdot \lambda^2 \right).\]
The only remaining thing we need is to show is that \(\mathbb{E}[Z] \le \frac18 \eta^2\).<sup id="fnref:6"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:6" rel="footnote">6</a></sup></p>

<p>If we set \(\lambda = -1 \), then we get \( \mathbb{E}[\exp( - Z)] \le \exp \left( -\mathbb{E}[Z] + \frac{\eta^2}{8} \right)\), which rearranges to \(\mathbb{E}[Z] \le \frac18 \eta^2 - \log \mathbb{E}[\exp( - Z)]\). 
Now we have \[ \mathbb{E}[\exp( - Z)] \!=\! \sum_y \mathbb{P}[M(x)\!=\!y] \exp(-f(y)) \!=\! \sum_y \mathbb{P}[M(x)\!=\!y]  \!\cdot\! \frac{\mathbb{P}[M(x’)\!=\!y]}{\mathbb{P}[M(x)\!=\!y]} \!=\! 1.\]
∎</p>

<p>This brings us to the TL;DR of this post:</p>

<blockquote>
  <p><strong>Corollary 7.</strong> The exponential mechanism (given by Equation 1) is \(\frac18 \varepsilon^2\)-concentrated differentially private.</p>
</blockquote>

<p>This is great news. The standard analysis only gives \(\frac12 \varepsilon^2\)-concentrated differential privacy. Constants matter when applying differential privacy, and we save a factor of 4 in the concentrated differential privacy analysis of the exponential mechanism for free with this improved analysis.</p>

<p>Combining Lemma 2 with Theorem 5 also gives a simpler proof of the conversion from pure differential privacy to concentrated differential privacy <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun, Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016."><strong>[BS16]</strong></a>:</p>

<blockquote>
  <p><strong>Corollary 8.</strong> \(\varepsilon\)-differential privacy implies \(\frac12 \varepsilon^2\)-concentrated differential privacy.</p>
</blockquote>

<h2 id="beyond-the-exponential-mechanism">Beyond the Exponential Mechanism</h2>

<p>The exponential mechanism is not the only algorithm for private selection. A closely-related algorithm is <em>report noisy max/min</em>:<sup id="fnref:7"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:7" rel="footnote">7</a></sup> Draw independent noise \(\xi_y\) from some distribution for each \(y \in \mathcal{Y}\) then output \[M(x) = \underset{y \in \mathcal{Y}}{\mathrm{argmin}} ~ \ell(y,x) - \xi_y.\]</p>

<p>If the noise distribution is an appropriate <a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbel distribution</a>, then report noisy max is exactly the exponential mechanism. (This equivalence is known as the “Gumbel max trick.”)</p>

<p>We can also use the Laplace distribution or the exponential distribution. Report noisy max with the exponential distribution is equivalent to the <em>permute and flip</em> algorithm <a href="https://arxiv.org/abs/2010.12603" title="Ryan McKenna, Daniel Sheldon. Permute-and-Flip: A new mechanism for differentially private selection . NeurIPS 2020."><strong>[MS20]</strong></a> <a href="https://arxiv.org/abs/2105.07260" title="Zeyu Ding, Daniel Kifer, Sayed M. Saghaian N. E., Thomas Steinke, Yuxin Wang, Yingtai Xiao, Danfeng Zhang. The Permute-and-Flip Mechanism is Identical to Report-Noisy-Max with Exponential Noise. 2021."><strong>[DKSSWXZ21]</strong></a>. However, these algorithms don’t enjoy the same improved bounded range and concentrated differential privacy guarantees as the exponential mechanism.</p>

<p>There are also other variants of the selection problem. For example, in some cases we can assume that only a few options have low loss and the rest of the options have high loss – i.e., there is a gap between the minimum loss and the second-lowest loss (or, more generally, the \(k\)-th lowest loss). In this case there are algorithms that attain better accuracy than the exponential mechanism under relaxed privacy definitions <a href="https://arxiv.org/abs/1409.2177" title="Kamalika Chaudhuri, Daniel Hsu, Shuang Song. The Large Margin Mechanism for Differentially Private Maximization. NIPS 2014."><strong>[CHS14]</strong></a> <a href="https://dl.acm.org/doi/10.1145/3188745.3188946" title=" Mark Bun, Cynthia Dwork, Guy N. Rothblum, Thomas Steinke. Composable and versatile privacy via truncated CDP. STOC 2018."><strong>[BDRS18]</strong></a> <a href="https://arxiv.org/abs/1905.13229" title="Mark Bun, Gautam Kamath, Thomas Steinke, Zhiwei Steven Wu. Private Hypothesis Selection. NeurIPS 2019."><strong>[BKSW19]</strong></a>.</p>

<p>There are a lot of interesting aspects of private selection, including questions for further research! We hope to have further posts about some of these topics.</p>

<hr/>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>For simplicity, we restrict our discussion here to finite sets of outputs, although the definitions, algorithms, and results can be extended to infinite sets. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>To be more precise, \((\varepsilon,\delta)\)-differential privacy is equivalent to demanding that \(\mathbb{E}[\max\{0,1-\exp(\varepsilon-Z)\}]\le\delta\) <a href="https://arxiv.org/abs/2004.00010" title="Cl&#xE9;ment L. Canonne, Gautam Kamath, Thomas Steinke. The Discrete Gaussian for Differential Privacy. NeurIPS 2020."><strong>[CKS20]</strong></a>. (To be completely precise, we must appropriately deal with the \(Z=\infty\) case, which we ignore in this discussion for simplicity.) <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">↩</a></p>
    </li>
    <li id="fn:3">
      <p>This proof actually gives <a href="https://dongjs.github.io/2020/02/10/ExpMech.html">a slightly stronger result</a>: We can replace the sensitivity \(\Delta\) (defined in Equation 2) by half the range \[\hat\Delta = \frac12 \sup_{x,x’ \in \mathcal{X}^n : d(x,x’) \le 1} \left( \max_{\overline{y}\in\mathcal{Y}} \ell(\overline{y},x) - \ell(\overline{y},x’) - \min_{\underline{y}\in\mathcal{Y}} \ell(\underline{y},x) - \ell(\underline{y},x’) \right).\] We always have \(\hat\Delta \le \Delta\) but it is possible that \(\hat\Delta &lt; \Delta\) and the privacy analysis of the exponential mechanism still works if we replace \(\Delta\) by \(\hat\Delta\). <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:3">↩</a></p>
    </li>
    <li id="fn:4">
      <p>Equivalently, a randomized algorithm \(M : \mathcal{X}^n \to \mathcal{Y}\) satisfies \(\rho\)-concentrated differential privacy if, for all pairs of inputs \(x, x’ \in \mathcal{X}^n\) differing only on the data of a single individual, \[\forall \lambda &gt; 0 ~~~~~ \mathrm{D}_{\lambda+1}(M(x)\|M(x’)) \le \lambda(\lambda+1)\rho,\] where \(\mathrm{D}_{\lambda+1}(M(x)\|M(x’)))\) is the order \(\lambda+1\) Rényi divergence of \(M(x)\) from \(M(x’)\). <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:4">↩</a></p>
    </li>
    <li id="fn:5">
      <p>To be precise, if \(M(x) = q(x) + \mathcal{N}(0,\sigma^2I)\), then \(M : \mathcal{X}^n \to \mathbb{R}^d\) satisfies \(\frac{\Delta_2^2}{2\sigma^2}\)-concentrated differential privacy, where \(\Delta_2 = \sup_{x,x’\in\mathcal{X}^n : d(x,x’)\le1} \|q(x)-q(x’)\|_2\) is the 2-norm sensitivity of \(q:\mathcal{X}^n \to \mathbb{R}^d\). Furthermore, the privacy loss of the Gaussian mechanism is itself a Gaussian and it makes the inequality defining concentrated differential privacy (Equation 3) an equality for all \(\lambda\) <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:5">↩</a></p>
    </li>
    <li id="fn:6">
      <p>Note that the expectation of the privacy loss is simply the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a>: \(\mathbb{E}[Z] = \mathrm{D}_1( M(x) \| M(x’) )\). <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:6">↩</a></p>
    </li>
    <li id="fn:7">
      <p>We have presented selection here in terms of minimization, but most of the literature is in terms of maximization. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:7">↩</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2021-07-12T17:00:00Z</updated>
    <published>2021-07-12T17:00:00Z</published>
    <author>
      <name>Thomas Steinke</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-07-13T23:15:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/100</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/100" rel="alternate" type="text/html"/>
    <title>TR21-100 |  Karchmer-Wigderson Games for Hazard-free Computation | 

	Christian Ikenmeyer, 

	Balagopal Komarath, 

	Nitin Saurabh</title>
    <summary>We present a Karchmer-Wigderson game to study the complexity of hazard-free formulas. This new game is both a generalization of the monotone Karchmer-Wigderson game and an analog of the classical Boolean Karchmer-Wigderson game. Therefore, it acts as a bridge between the existing monotone and general games.

Using this game, we prove hazard-free formula size and depth lower bounds that are provably stronger than those possible by the standard technique of transferring results from monotone complexity in a black-box fashion.
For the multiplexer function
we give (1) a hazard-free formula of optimal size and (2) an improved low-depth hazard-free formula of almost optimal size and (3) a hazard-free formula with alternation depth $2$ that has optimal depth.
We then use our optimal constructions to obtain an improved universal worst-case hazard-free formula size upper bound.
We see our results as a significant step towards establishing hazard-free computation as an independent missing link between Boolean complexity and monotone complexity.</summary>
    <updated>2021-07-12T07:35:35Z</updated>
    <published>2021-07-12T07:35:35Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-14T17:37:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6138760775046652444</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6138760775046652444/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6138760775046652444" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6138760775046652444" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-2.html" rel="alternate" type="text/html"/>
    <title>Would you take this bet (Part 2) ?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Recall from my last post (<a href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html">here</a>)</p><p><br/></p><p>I offer you the following bet: </p><p>I will flip a coin.</p><p>If  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>If  HEADS you get 2 dollars and we end there.</p><p>If  TAILS I flip again</p><p><br/></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>The expected value is infinity.</p><p><br/></p><p>Would you pay $1000 to play this game?</p><p>Everyone who responded said NO. Most gave reasons similar to what I have below. </p><p>This is called The St Petersburg Paradox. Not sure it's a paradox, but it is odd. The concrete question of <i>would you pay $1000 to play</i> might be a paradox since most people would say NO even though the expected value is infinity.  See <a href="https://en.wikipedia.org/wiki/St._Petersburg_paradox">here</a> for more background.</p><p>Shapley (see <a href="https://www.sciencedirect.com/science/article/abs/pii/0022053177901429">here</a>) gives a good reason why you would not  pay $1000 to play the game, and also how much you should pay to play the game (spoiler alert: not much). I will summarize his argument and then add to it. </p><p><br/></p><p>1) Shapley's argument: Lets say the game goes for 40 rounds. Then you are owed 2^{40} dollars. </p><p>The amount of money in the world is, according to <a href="https://bibloteka.com/how-much-money-is-there-in-the-world/#:~:text=Short%20Answer%3A%20Money%20in%20circulation%20in%20the%20world,the%20medium%20of%20trade%20for%20goods%20and%20services.">this article</a> around 1.2 quadrillion dollars  which is roughly 2^{40} dollars. </p><p>So the expected value calculation has to be capped at (say) 40 rounds. This means you expect to get 20 dollars! So pay 19 to play. </p><p><br/></p><p>2) My angle which is very similar: at what point is more money not going to change your life at all? For me it is way less than 2^{40} dollars. Hence I would not pay 1000. Or even 20. </p><p><i>Exercise</i>: If you think the game will go at most R rounds and you only wand D dollars, how much should you pay to play? You can also juggle more parameters - the bias of the coin, how much they pay out when you win. </p><p>Does Shapley's discussions  <i>resolve</i> the paradox? It depends on what you consider paradoxical. If the paradox is that people would NOT pay 1000 even though the expected value is infinity, then Shapley  resolves the paradox  by contrasting the real world to the math world. </p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-07-12T00:53:00Z</updated>
    <published>2021-07-12T00:53:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-14T15:39:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/099</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/099" rel="alternate" type="text/html"/>
    <title>TR21-099 |  Improved Product-Based High-Dimensional Expanders | 

	Louis Golowich</title>
    <summary>High-dimensional expanders generalize the notion of expander graphs to higher-dimensional simplicial complexes. In contrast to expander graphs, only a handful of high-dimensional expander constructions have been proposed, and no elementary combinatorial construction with near-optimal expansion is known. In this paper, we introduce an improved combinatorial high-dimensional expander construction, by modifying a previous construction of Liu, Mohanty, and Yang (ITCS 2020), which is based on a high-dimensional variant of a tensor product. Our construction achieves a spectral gap of $\Omega(\frac{1}{k^2})$ for random walks on the $k$-dimensional faces, which is only quadratically worse than the optimal bound of $\Theta(\frac{1}{k})$. Previous combinatorial constructions, including that of Liu, Mohanty, and Yang, only achieved a spectral gap that is exponentially small in $k$. We also present reasoning that suggests our construction is optimal among similar product-based constructions.</summary>
    <updated>2021-07-11T07:39:29Z</updated>
    <published>2021-07-11T07:39:29Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-14T17:37:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/098</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/098" rel="alternate" type="text/html"/>
    <title>TR21-098 |  On the Probabilistic Degree of an $n$-variate Boolean Function | 

	Srikanth Srinivasan, 

	S Venkitesh</title>
    <summary>Nisan and Szegedy (CC 1994) showed that any Boolean function $f:\{0,1\}^n\to\{0,1\}$ that depends on all its input variables, when represented as a real-valued multivariate polynomial $P(x_1,\ldots,x_n)$, has degree at least $\log n - O(\log \log n)$. This was improved to a tight $(\log n - O(1))$ bound by Chiarelli, Hatami and Saks (Combinatorica 2020). Similar statements are also known for other Boolean function complexity measures such as Sensitivity (Simon (FCT 1983)), Quantum query complexity, and Approximate degree (Ambainis and de Wolf (CC 2014)). 

In this paper, we address this question for \emph{Probabilistic degree}. The function $f$ has probabilistic degree at most $d$ if there is a random real-valued polynomial of degree at most $d$ that agrees with $f$ at each input with high probability. Our understanding of this complexity measure is significantly weaker than those above: for instance, we do not even know the probabilistic degree of the OR function, the best-known bounds put it between $(\log n)^{1/2-o(1)}$ and $O(\log n)$ (Beigel, Reingold, Spielman (STOC 1991); Tarui (TCS 1993); Harsha, Srinivasan (RSA 2019)).

Here we can give a near-optimal understanding of the probabilistic degree of $n$-variate functions $f$, \emph{modulo} our lack of understanding of the probabilistic degree of OR. We show that if the probabilistic degree of OR is $(\log n)^c$, then the minimum possible probabilistic degree of such an $f$ is at least $(\log n)^{c/(c+1)-o(1)}$, and we show this is tight up to $(\log n)^{o(1)}$ factors.</summary>
    <updated>2021-07-11T04:50:01Z</updated>
    <published>2021-07-11T04:50:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-14T17:37:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/07/10/angles-arc-triangles</id>
    <link href="https://11011110.github.io/blog/2021/07/10/angles-arc-triangles.html" rel="alternate" type="text/html"/>
    <title>Angles of arc-triangles</title>
    <summary>Piecewise-circular curves or, if you like, arc-polygons are a very old topic in mathematics. Archimedes and Pappus studied the arbelos, a curved triangle formed from three semicircles, and Hippocrates of Chios found that the lune of Hippocrates, a two-sided figure bounded by a semicircle and a quarter-circle, has the same area as an isosceles right triangle stretched between the same two points. The history of the Reuleaux triangle, bounded by three sixths of circles, stretches back well past Reuleaux to the shapes of of Gothic church windows and its use by Leonardo da Vinci for fortress floor plans and world map projections. But despite their long history and frequent use (for instance in the design of machined parts), there are some basic properties of arc-polygons that seem to have been unexplored so far.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Piecewise-circular curves or, if you like, arc-polygons are a very old topic in mathematics. Archimedes and Pappus studied the <a href="https://en.wikipedia.org/wiki/Arbelos">arbelos</a>, a curved triangle formed from three semicircles, and Hippocrates of Chios found that the <a href="https://en.wikipedia.org/wiki/Lune_of_Hippocrates">lune of Hippocrates</a>, a two-sided figure bounded by a semicircle and a quarter-circle, has the same area as an isosceles right triangle stretched between the same two points. The history of the <a href="https://en.wikipedia.org/wiki/Reuleaux_triangle">Reuleaux triangle</a>, bounded by three sixths of circles, stretches back well past Reuleaux to the shapes of of Gothic church windows and its use by Leonardo da Vinci for fortress floor plans and world map projections. But despite their long history and frequent use (for instance in the design of machined parts), there are some basic properties of arc-polygons that seem to have been unexplored so far.</p>

<p>I looked at one of these properties, <a href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html">the ability of arc-triangles to tile the plane</a>, in an earlier post. Another of these properties involves the feasible combinations of angles of these shapes. As is well known, in a straight-sided triangle in the plane, the three interior angles always sum to exactly \(\pi\), and any three positive angles summing to \(\pi\) are possible. Let \(T\) be the set of triples of angles \((\theta_1,\theta_2,\theta_3)\) from triangles, and reinterpret these triples as coordinates of points in Euclidean space. Then \(T\) is itself an equilateral triangle, with corners at the three points \((\pi,0,0)\), \((0,\pi,0)\), and \((0,0\pi)\). (More precisely, it’s the relative interior of this triangle.)</p>

<p>What about arc-triangles? Are their angles similarly constrained? What shape do their triples of angles make? First of all, their angles don’t have a fixed sum (except for the tilers, for which this sum is again \(\pi\)). The arbelos has three interior angles that are all zero, summing to zero. The Reuleux triangle has three angles of \(2\pi/3\), summing to \(2\pi\). <a href="https://11011110.github.io/blog/2021/05/15/linkage.html">Boscovitch’s cardioid</a>, below, uses three semicircles like the arbelos, but with one interior angle of \(2\pi\) and two others equal to \(\pi\), summing to \(4\pi\). The <a href="https://en.wikipedia.org/wiki/Trefoil">trefoil</a>, a common architectural motif, bulges outward from its three corners, forming interior angles that are much larger, up to \(2\pi\) each for a trefoil made from three \(5/6\)-circle arcs, for a total interior angle of \(6\pi\).</p>

<p style="text-align: center;"><img alt="Boscovich's cardioid" src="https://11011110.github.io/blog/assets/2021/boscovich.svg"/></p>

<p>Nevertheless, for a non-self-crossing arc-triangle, not all combinations of angles are possible. For instance, it’s not possible to have one angle that is zero and two that are \(2\pi\). My new preprint, “Angles of arc-polygons and lombardi drawings of cacti” (<a href="https://arxiv.org/abs/2107.03615">arXiv:2107.03615
</a>, with UCI students Daniel Frishberg and Martha Osegueda, to appear at CCCG) proves a precise characterization: beyond the obvious requirement that each angle \(\theta_i\) be in the range \(0\le\theta_i\le 2\pi\), we have only the additional inequalities</p>

\[-\pi &lt; \frac{\pi - \theta_i + \theta_{i+1} - \theta_{i+2}}{2} &lt; \pi\]

<p>where the index arithmetic is done modulo three. The formula in the middle of each of these inequalities is itself an angle, the angle of incidence between one of the circular arcs of the arc-triangle and the circle through its three corners. Where the straight triangles had an equilateral-triangle feasible region, the arc-triangles have a more complicated shape. The obvious constraints \(0\le\theta_i\le 2\pi\) would produce a cubical feasible region \([0,2\pi]^3\), but the additional inequalities above cut off six corners of the cube, leaving a feasible region looking like this:</p>

<p style="text-align: center;"><img alt="The feasible region for triples of angles of arc-triangles" src="https://11011110.github.io/blog/assets/2021/feasible-arc-triangles.svg"/></p>

<p>The motivating application for all of this is graph drawing, and more specifically Lombardi drawing, in which edges are circular arcs meeting at equal angles at each vertex. Using our new understanding of arc-polygons, we prove that every <a href="https://en.wikipedia.org/wiki/Cactus_graph">cactus graph</a> has a planar Lombardi drawing for its natural embedding (the one in which each cycle of the cactus forms a face) but might not for some other embeddings, including the one below.</p>

<p style="text-align: center;"><img alt="An embedded cactus that has no planar Lombardi drawing" src="https://11011110.github.io/blog/assets/2021/badhat.svg"/></p>

<p>But beyond graph drawing, I think that the long history and many applications of arc-polygons justifies more study of their general properties. For instance, what about arc-polygons with more than three sides? What can their angles be? Our paper has a partial answer, enough to answer the questions we asked in our Lombardi drawing application, but a complete characterization for arc-polygons of more than three sides is still open.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106557711895868173">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-07-10T11:06:00Z</updated>
    <published>2021-07-10T11:06:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-07-10T18:31:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/097</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/097" rel="alternate" type="text/html"/>
    <title>TR21-097 |  Number of Variables for Graph Identification and the Resolution of GI Formulas | 

	Jacobo Toran, 

	Florian Wörz</title>
    <summary>We show that the number of variables and the quantifier depth needed to distinguish a pair of graphs by first-order logic sentences exactly match the complexity measures of clause width and positive depth needed to refute the corresponding graph isomorphism formula in propositional narrow resolution. 

Using this connection, we obtain upper and lower bounds for refuting graph  isomorphism formulas in (normal) resolution. In particular, we show that if $k$ is the minimum number of variables needed to distinguish two graphs with $n$ vertices each, then there is an $n^{\mathrm{O}(k)}$ resolution refutation size upper bound for the corresponding isomorphism formula, as well as lower bounds of $2^{k-1}$ and $k$ for the tree-like resolution size and resolution clause space for this formula. We also show a resolution size lower bound of ${\exp} \big( \Omega(k^2/n) \big)$ for the case of colored graphs with constant color class size.

Applying these results, we prove the first exponential lower bound for graph isomorphism formulas in the proof system SRC-1, a system that extends resolution with a global symmetry rule, thereby answering an open question posed by Schweitzer and Seebach.</summary>
    <updated>2021-07-09T15:37:56Z</updated>
    <published>2021-07-09T15:37:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-14T17:37:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/07/09/spanners-unit-ball</id>
    <link href="https://11011110.github.io/blog/2021/07/09/spanners-unit-ball.html" rel="alternate" type="text/html"/>
    <title>Spanners for unit ball graphs in doubling spaces</title>
    <summary>My student Hadi Khodabandeh had a paper with me on spanners earlier this year at SoCG, in which we showed that the greedy spanner algorithm for points in the Euclidean plane produces graphs with few crossings and small separators. Now we have another spanner preprint: “Optimal spanners for unit ball graphs in doubling metrics”, arXiv:2106.15234.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My student Hadi Khodabandeh had a paper with me on spanners earlier this year at SoCG, in which we showed that <a href="https://11011110.github.io/blog/2020/02/17/spanners-have-sparse.html">the greedy spanner algorithm for points in the Euclidean plane produces graphs with few crossings and small separators</a>. Now we have another spanner preprint: <a href="https://arxiv.org/abs/2106.15234">“Optimal spanners for unit ball graphs in doubling metrics”, arXiv:2106.15234</a>.</p>

<p><a href="https://en.wikipedia.org/wiki/Doubling_space">Doubling metrics</a> are a generalization of Euclidean spaces. Like Euclidean spaces, they have a dimension, the <em>doubling dimension</em>, but it might not be an integer. Even the doubling dimension of the Euclidean plane itself is \(\log_2 7\approx 2.807355\); this means that every circular disk of radius \(r\) in the plane can be covered by seven closed disks of radius \(r/2\).</p>

<p style="text-align: center;"><img alt="Seven disks of radius $$r/2$$ cover a single disk of radius $$r$$" src="https://11011110.github.io/blog/assets/2021/doubling-dim.svg"/></p>

<p>Analogously, the <em>doubling constant</em> of a metric space (if it exists) is the smallest number \(c\) such that every closed metric ball (the set of points within some radius \(r\) of a fixed point) can be covered by \(c\) balls of half the radius. The <em>doubling dimension</em> is the binary logarithm of the doubling constant. A metric space is a <em>doubling metric</em> or <em>doubling space</em> if it has a doubling constant and doubling dimension. This is true for all Euclidean spaces: for instance, if you cover a ball with a grid of hypercubes, sized small enough that their long diagonal has length at most the radius of the ball, you obtain doubling constant at most \(\lceil 2\sqrt{d}\rceil^d\), although it seems difficult to compute the precise doubling constant in general.</p>

<p style="text-align: center;"><img alt="Nine disks of radius $$r/2$$ cover a square grid that covers a single disk of radius $$r$$" src="https://11011110.github.io/blog/assets/2021/grid-doubling.svg"/></p>

<p>The hyperbolic plane provides a natural example of a space that is not a doubling space: arbitrarily large-radius disks require arbitrarily many half-radius disks to cover them.</p>

<p>Many results in computational geometry can be generalized to doubling metrics, but not always, and sometimes with difficulty. That includes results on spanners, as we consider in our paper. A spanner of a weighted graph is a subgraph whose shortest path distances approximate the distances in the full graph, and often for spanners of metric spaces one uses the complete graph, weighted by the metric distance between each pair of points. But here, we are using a different graph, the unit ball graph. The unit ball graph, for points in a continuous space, has an edge whenever two unit balls centered at two of the points have a nonempty intersection, and it can be extended to discrete point sets by instead including an edge whenever two points are at distance at most 2 (or, as in our paper, distance at most 1; scaling doesn’t really change anything). The weights are the same as in the complete graph. If a spanner accurately approximates all edge weights, it approximates all paths.</p>

<p>The unit ball graph has fewer edges to approximate than the complete graph. But that actually makes it harder to approximate, because by the same token there are fewer edges that can be used in the spanner. Despite that, the greedy spanner algorithm still produces a spanner, but one of its key properties is lost when going from Euclidean to doubling spaces: Euclidean greedy spanners have bounded degree, but greedy spanners in doubling spaces do not. Instead, our paper provides different spanner algorithms that apply to unit ball graphs, approximate paths in these graphs arbitrarily well, have bounded degree, have total weight a constant times that of the minimum spanning tree, and can be constructed efficiently in a distributed model of computing. I think the details are too technical to go into here, so see the paper for more.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106551534535329957">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-07-09T09:06:00Z</updated>
    <published>2021-07-09T09:06:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-07-10T18:31:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/096</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/096" rel="alternate" type="text/html"/>
    <title>TR21-096 |  Keep That Card in Mind: Card Guessing with Limited Memory | 

	Boaz Menuhin, 

	Moni Naor</title>
    <summary>A card guessing game is played between two players, Guesser and Dealer. At the beginning of the game, the Dealer holds a deck of $n$ cards (labeled $1, ..., n$). For $n$ turns, the Dealer draws a card from the deck, the Guesser guesses which card was drawn, and then the card is discarded from the deck. The Guesser receives a point for each correctly guessed card. 

With perfect memory, a Guesser can keep track of all cards that were played so far and pick at random a card that has not appeared so far, yielding in expectation $\ln n$ correct guesses. With no memory, the best a Guesser can do will result in a single guess in expectation. 

We consider the case of a memory bounded Guesser that has $m &lt; n$ memory bits. We show that the performance of such a memory bounded Guesser depends much on the behavior of the Dealer. In more detail, we show that there is a gap between the static case, where the Dealer draws cards from a properly shuffled deck or a prearranged one, and the adaptive case, where the Dealer draws cards thoughtfully, in an adversarial manner. Specifically: 

1. We show a Guesser with $O(\log^2 n)$ memory bits that scores a near optimal result against any static Dealer. 

2. We show that no Guesser with $m$ bits of memory can score better than $O(\sqrt{m})$ correct guesses, thus, no Guesser can score better than $\min \{\sqrt{m}, \ln n\}$, i.e., the above Guesser is optimal. 

3. We show an efficient adaptive Dealer against which no Guesser with $m$ memory bits can make more than $\ln m + 2 \ln \log n + O(1)$ correct guesses in expectation. 

These results are (almost) tight, and we prove them using compression arguments that harness the guessing strategy for encoding.</summary>
    <updated>2021-07-08T21:32:56Z</updated>
    <published>2021-07-08T21:32:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-14T17:37:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2021/07/08/imp-reg-tf/</id>
    <link href="http://offconvex.github.io/2021/07/08/imp-reg-tf/" rel="alternate" type="text/html"/>
    <title>Implicit Regularization in Tensor Factorization&amp;#58; Can Tensor Rank Shed Light on Generalization in Deep Learning?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In effort to understand implicit regularization in deep learning, a lot of theoretical focus is being directed at matrix factorization, which can be seen as linear neural networks.
This post is based on our <a href="https://arxiv.org/pdf/2102.09972.pdf">recent paper</a> (to appear at ICML 2021), where we take a step towards practical deep learning, by investigating <em>tensor factorization</em> — a model equivalent to a certain type of non-linear neural networks.
It is well known that <a href="https://arxiv.org/pdf/0911.1393.pdf">most tensor problems are NP-hard</a>, and accordingly, the common sentiment is that working with tensors (in both theory and practice) entails extreme difficulties.
However, by adopting a dynamical systems view, we manage to avoid such difficulties, and establish an implicit regularization towards low <em>tensor rank</em>.
Our results suggest that tensor rank may shed light on generalization in deep learning.</p>

<h2 id="challenge-finding-a-right-measure-of-complexity">Challenge: finding a right measure of complexity</h2>

<p>Overparameterized neural networks are mysteriously able to generalize even when trained without any explicit regularization.
Per conventional wisdom, this generalization stems from an <em>implicit regularization</em> — a tendency of gradient-based optimization to fit training examples with predictors of minimal ‘‘complexity.’’
A major challenge in translating this intuition to provable guarantees is that we lack measures for predictor complexity that are quantitative (admit generalization bounds), and at the same time, capture the essence of natural data (images, audio, text etc.), in the sense that it can be fit with predictors of low complexity.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/imp_reg_tf_data_complexity.png" style="width: 500px; padding-bottom: 10px; padding-top: 5px;"/>
<br/>
<i><b>Figure 1:</b> 
To explain generalization in deep learning, a complexity <br/>
measure must allow the fit of natural data with low complexity. On the <br/>
other hand, when fitting data which does not admit generalization, <br/>
e.g. random data, the complexity should be high. 
</i>
</div>
<p><br/></p>

<h2 id="a-common-testbed-matrix-factorization">A common testbed: matrix factorization</h2>

<p>Without a clear complexity measure for practical neural networks, existing analyses usually focus on simple settings where a notion of complexity is obvious. 
A common example of such a setting is <em>matrix factorization</em> — matrix completion via linear neural networks. 
This model was discussed pretty extensively in previous posts (see <a href="http://www.offconvex.org/2019/06/03/trajectories/">one</a> by Sanjeev, <a href="http://www.offconvex.org/2019/07/10/trajectories-linear-nets/">one</a> by Nadav and Wei and <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">another one</a> by Nadav), but for completeness we present it again here.</p>

<p>In <em>matrix completion</em> we’re given a subset of entries from an unknown matrix $W^* \in \mathbb{R}^{d, d’}$, and our goal is to predict the unobserved entries. 
This can be viewed as a supervised learning problem with $2$-dimensional inputs, where the label of the input $( i , j )$ is $( W^* )_{i,j}$.
Under such a viewpoint, the observed entries are the training set, and the average reconstruction error over unobserved entries is the test error, quantifying generalization.
A predictor can then be thought of as a matrix, and a natural notion of complexity is its <em>rank</em>.
Indeed, in many real-world scenarios (a famous example is the <a href="https://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a>) one is interested in <a href="https://arxiv.org/pdf/1601.06422.pdf">recovering a low rank matrix from incomplete observations</a>.</p>

<p>A ‘‘deep learning approach’’ to matrix completion is matrix factorization, where the idea is to use a linear neural network (fully connected neural network with no non-linearity), and fit observations via gradient descent (GD). 
This amounts to optimizing the following objective:</p>

<div style="text-align: center;">
\[
 \min\nolimits_{W_1 , \ldots , W_L} ~ \sum\nolimits_{(i,j) \in observations} \big[ ( W_L \cdots W_1 )_{i , j} - (W^*)_{i,j} \big]^2 ~.
\]
</div>

<p>It is obviously possible to constrain the rank of the produced solution by limiting the shared dimensions of the weight matrices $\{ W_j \}_j$. 
However, from an implicit regularization standpoint, the most interesting case is where rank is unconstrained and the factorization can express any matrix. 
In this case there is no explicit regularization, and the kind of solution we get is determined implicitly by the parameterization and the optimization algorithm.</p>

<p>As it turns out, in practice, matrix factorization with near-zero initialization and small step size tends to accurately recover low rank matrices.
This phenomenon (first identified in <a href="https://papers.nips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf">Gunasekar et al. 2017</a>) manifests some kind of implicit regularization, whose mathematical characterization drew a lot of interest.
It was initially conjectured that matrix factorization implicitly minimizes nuclear norm (<a href="https://papers.nips.cc/paper/2017/file/58191d2a914c6dae66371c9dcdc91b41-Paper.pdf">Gunasekar et al. 2017</a>), but recent evidence points to implicit rank minimization, stemming from incremental learning dynamics (see <a href="https://papers.nips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf">Arora et al. 2019</a>; <a href="https://papers.nips.cc/paper/2020/file/f21e255f89e0f258accbe4e984eef486-Paper.pdf">Razin &amp; Cohen 2020</a>; <a href="https://openreview.net/pdf/e29b53584bc9017cb15b9394735cd51b56c32446.pdf">Li et al. 2021</a>). 
Today, it seems we have a relatively firm understanding of generalization in matrix factorization.
There is a complexity measure for predictors — matrix rank — by which implicit regularization strives to lower complexity, and the data itself is of low complexity (i.e. can be fit with low complexity). 
Jointly, these two conditions lead to generalization.</p>

<h2 id="beyond-matrix-factorization-tensor-factorization">Beyond matrix factorization: tensor factorization</h2>

<p>Matrix factorization is interesting on its own behalf, but as a theoretical surrogate for deep learning it is limited.
First, it corresponds to <em>linear</em> neural networks, and thus misses the crucial aspect of non-linearity. 
Second, viewing matrix completion as a prediction problem, it doesn’t capture tasks with more than two input variables.
As we now discuss, both of these limitations can be lifted if instead of matrices one considers tensors.</p>

<p>A tensor can be thought of as a multi-dimensional array.
The number of axes in a tensor is called its <em>order</em>.
In the task of <em>tensor completion</em>, a subset of entries from an unknown tensor $\mathcal{W}^* \in \mathbb{R}^{d_1, \ldots, d_N}$ are given, and the goal is to predict the unobserved entries. 
Analogously to how matrix completion can be viewed as a prediction problem over two input variables, order-$N$ tensor completion can be seen as a prediction problem over $N$ input variables (each corresponding to a different axis).
In fact, any multi-dimensional prediction task with discrete inputs and scalar output can be formulated as a tensor completion problem.
Consider for example the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>, and for simplicity assume that image pixels hold one of two values, i.e. are either black or white. 
The task of predicting labels for the $28$-by-$28$ binary images can be seen as an order-$784$ (one axis for each pixel) tensor completion problem, where all axes are of length $2$ (corresponding to the number of values a pixel can take). 
For further details on how general prediction tasks map to tensor completion problems see <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a>.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/pred_prob_to_tensor_comp.png" style="width: 550px; padding-bottom: 10px; padding-top: 5px;"/>
<br/>
<i><b>Figure 2:</b> 
Prediction tasks can be viewed as tensor completion problems. <br/>
For example, predicting labels for input images with $3$ pixels, each taking <br/>
one of $5$ grayscale values, corresponds to completing a $5 \times 5 \times 5$ tensor.
</i>
</div>
<p><br/>
Like matrices, tensors can be factorized. 
The most basic scheme for factorizing tensors, named CANDECOMP/PARAFAC (CP), parameterizes a tensor as a sum of outer products (for information on this scheme, as well as others, see the <a href="http://www.kolda.net/publication/TensorReview.pdf">excellent survey</a> of Kolda and Bader).
In <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a> and this post, we use the term <em>tensor factorization</em> to refer to solving tensor completion by fitting observations via GD over CP parameterization, i.e. over the following objective ($\otimes$ here stands for outer product):</p>

<div style="text-align: center;">
\[
\min\nolimits_{ \{ \mathbf{w}_r^n \}_{r , n} } \sum\nolimits_{ (i_1 , ... , i_N) \in observations } \big[ \big(  {\textstyle \sum}_{r = 1}^R \mathbf{w}_r^1 \otimes \cdots \otimes \mathbf{w}_r^N \big)_{i_1 , \ldots , i_N} - (\mathcal{W}^*)_{i_1 , \ldots , i_N} \big]^2 ~.
\]
</div>

<p>The concept of rank naturally extends from matrices to tensors.
The <em>tensor rank</em> of a given tensor $\mathcal{W}$ is defined to be the minimal number of components (i.e. of outer product summands) $R$ required for CP parameterization to express it.
Note that for order-$2$ tensors, i.e. for matrices, this exactly coincides with matrix rank.
We can explicitly constrain the tensor rank of solutions found by tensor factorization via limiting the number of components $R$. 
However, since our interest lies on implicit regularization, we consider the case where $R$ is large enough for any tensor to be expressed.</p>

<p>By now you might be wondering what does tensor factorization have to do with deep learning.
Apparently, as Nadav mentioned in an <a href="http://www.offconvex.org/2020/11/27/reg_dl_not_norm/">earlier post</a>, analogously to how matrix factorization is equivalent to matrix completion (two-dimensional prediction) via linear neural networks, tensor factorization is equivalent to tensor completion (multi-dimensional prediction) with a certain type of <em>non-linear</em> neural networks (for the exact details behind the latter equivalence see <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a>). 
It therefore represents a setting one step closer to practical neural networks.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/mf_lnn_tf_nonlinear.png" style="width: 900px; padding-bottom: 10px; padding-top: 5px;"/>
<br/>
<i><b>Figure 3:</b> 
While matrix factorization corresponds to a linear neural network, <br/>
tensor factorization corresponds to a certain non-linear neural network.
</i>
</div>
<p><br/>
As a final piece of the analogy between matrix and tensor factorizations, in a <a href="https://arxiv.org/pdf/2005.06398.pdf">previous paper</a> (described in an <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">earlier post</a>) Noam and Nadav demonstrated empirically that (similarly to the phenomenon discussed above for matrices) tensor factorization with near-zero initialization and small step size tends to accurately recover low rank tensors.
Our goal in the <a href="https://arxiv.org/pdf/2102.09972.pdf">current paper</a> was to mathematically explain this finding. 
To avoid the <a href="https://arxiv.org/pdf/0911.1393.pdf">notorious difficulty of tensor problems</a>, we chose to adopt a dynamical systems view, and analyze directly the trajectories induced by GD.</p>

<h2 id="dynamical-analysis-implicit-tensor-rank-minimization">Dynamical analysis: implicit tensor rank minimization</h2>

<p>So what can we say about the implicit regularization in tensor factorization? 
At the core of our analysis is the following dynamical characterization of component norms:</p>

<blockquote>
  <p><strong>Theorem:</strong>
Running gradient flow (GD with infinitesimal step size) over a tensor factorization with near-zero initialization leads component norms to evolve by:
[ \frac{d}{dt} || \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) || \propto \color{brown}{|| \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) ||^{2 - 2/N}} ~,
]
where $\mathbf{w}_r^1 (t), \ldots, \mathbf{w}_r^N (t)$ denote the weight vectors at time $t \geq 0$.</p>
</blockquote>

<p>According to the theorem above, component norms evolve at a rate proportional to their size exponentiated by $\color{brown}{2 - 2 / N}$ (recall that $N$ is the order of the tensor to complete).
Consequently, they are subject to a momentum-like effect, by which they move slower when small and faster when large. 
This suggests that when initialized near zero, components tend to remain close to the origin, and then, after passing a critical threshold, quickly grow until convergence. 
Intuitively, these dynamics induce an incremental process where components are learned one after the other, leading to solutions with a few large components and many small ones, i.e. to (approximately) low tensor rank solutions!</p>

<p>We empirically verified the incremental learning of components in many settings. 
Here is a representative example from one of our experiments (see <a href="https://arxiv.org/pdf/2102.09972.pdf">the paper</a> for more):</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/tf_dyn_exps.png" style="width: 800px; padding-bottom: 15px; padding-top: 10px;"/>
<br/>
<i><b>Figure 4:</b> 
Dynamics of component norms during GD over tensor factorization. <br/>
An incremental learning effect is enhanced as initialization scale decreases, <br/>
leading to accurate completion of a low rank tensor.
</i>
</div>
<p><br/>
Using our dynamical characterization of component norms, we were able to prove that with sufficiently small initialization, tensor factorization (approximately) follows a trajectory of rank one tensors for an arbitrary amount of time. 
This leads to:</p>

<blockquote>
  <p><strong>Theorem:</strong>
If tensor completion has a rank one solution, then under certain technical conditions, tensor factorization will reach it.</p>
</blockquote>

<p>It’s worth mentioning that, in a way, our results extend to tensor factorization the incremental rank learning dynamics known for matrix factorization (cf. <a href="https://papers.nips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf">Arora et al. 2019</a> and <a href="https://arxiv.org/pdf/2012.09839v1.pdf">Li et al. 2021</a>). 
As typical when transitioning from matrices to tensors, this extension entailed various challenges that necessitated use of different techniques.</p>

<h2 id="tensor-rank-as-measure-of-complexity">Tensor rank as measure of complexity</h2>

<p>Going back to the beginning of the post, recall that a major challenge towards understanding implicit regularization in deep learning is that we lack measures for predictor complexity that capture natural data. 
Now, let us recap what we have seen thus far:
$(1)$ tensor completion is equivalent to multi-dimensional prediction; 
$(2)$ tensor factorization corresponds to solving the prediction task with certain non-linear neural networks; 
and 
$(3)$ the implicit regularization of these non-linear networks, i.e. of tensor factorization, minimizes tensor rank.
Motivated by these findings, we ask the following:</p>

<blockquote>
  <p><strong>Question:</strong> 
Can tensor rank serve as a measure of predictor complexity?</p>
</blockquote>

<p>We empirically explored this prospect by evaluating the extent to which tensor rank captures natural data, i.e. to which natural data can be fit with predictors of low tensor rank.
As testbeds we used <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> and <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> datasets, comparing the resulting errors against those obtained when fitting two randomized variants: one generated via shuffling labels (‘‘rand label’’), and the other by replacing inputs with noise (‘‘rand image’’).</p>

<p>The following plot, displaying results for Fashion-MNIST (those for MNIST are similar), shows that with predictors of low tensor rank the original data is fit way more accurately than the randomized datasets. 
Specifically, even with tensor rank as low as one the original data is fit relatively well, while the error in fitting random data is close to trivial (variance of the label). 
This suggests that tensor rank as a measure of predictor complexity has potential to capture aspects of natural data! 
Note also that an accurate fit with low tensor rank coincides with low test error, which is not surprising given that low tensor rank predictors can be described with a small number of parameters.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/reg_tf/exp_complexity_fmnist.png" style="width: 600px; padding-bottom: 15px; padding-top: 10px;"/>
<br/>
<i><b>Figure 5:</b> 
Evaluation of tensor rank as a measure of complexity — standard datasets <br/>
can be fit accurately with predictors of low tensor rank (far beneath what is required by <br/>
random datasets), suggesting it may capture aspects of natural data. Plot shows mean <br/>
error of predictors with low tensor rank over Fashion-MNIST. Markers correspond <br/>
to separate runs differing in the explicit constraint on the tensor rank.
</i>
</div>

<h2 id="concluding-thoughts">Concluding thoughts</h2>

<p>Overall, <a href="https://arxiv.org/pdf/2102.09972.pdf">our paper</a> shows that tensor rank captures both the implicit regularization of a certain type of non-linear neural networks, and aspects of natural data. 
In light of this, we believe tensor rank (or more advanced notions such as hierarchical tensor rank) might pave way to explaining both implicit regularization in more practical neural networks, and the properties of real-world data translating this implicit regularization to generalization.</p>

<p><a href="https://noamrazin.github.io/">Noam Razin</a>, <a href="https://asafmaman101.github.io/">Asaf Maman</a>, <a href="http://www.cohennadav.com/">Nadav Cohen</a></p></div>
    </summary>
    <updated>2021-07-08T09:00:00Z</updated>
    <published>2021-07-08T09:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2021-07-13T23:15:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/095</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/095" rel="alternate" type="text/html"/>
    <title>TR21-095 |  LEARN-Uniform Circuit Lower Bounds and Provability in Bounded Arithmetic | 

	Valentine Kabanets, 

	Igor Oliveira, 

	Marco Carmosino, 

	Antonina Kolokolova</title>
    <summary>We investigate randomized LEARN-uniformity, which captures the power of randomness and equivalence queries (EQ) in the construction of Boolean circuits for an explicit problem. This is an intermediate notion between P-uniformity and non-uniformity motivated by connections to learning, complexity, and logic.  Building on a number of techniques, we establish the first unconditional lower bounds against LEARN-uniform circuits:

-- For all $c\geq 1$, there is $L \in P$ that is not computable by circuits of size $n \cdot (\log n)^c$ generated in deterministic polynomial time with $o(\log n/\log \log n)$ equivalence queries to $L$. In other words, small circuits for $L$ cannot be efficiently learned using a bounded number of EQs.
-- For each $k\geq 1$, there is $L \in NP$ such that circuits for $L$ of size $O(n^k)$ cannot be learned in deterministic polynomial time with access to $n^{o(1)}$ EQs.
--  For each $k\geq 1$, there is a problem in promise-ZPP that is not in FZPP-uniform $SIZE[n^k]$.
-- Conditional and unconditional lower bounds against LEARN-uniform circuits in the general setting that combines randomized uniformity and access to EQs.

In all these lower bounds, the learning algorithm is allowed to run in arbitrary polynomial time, while the hard problem is computed in some fixed polynomial time.

We employ these results to investigate the (un)provability of non-uniform circuit upper bounds (e.g., Is NP contained in $SIZE[n^3]$?) in theories of bounded arithmetic. Some questions of this form have been addressed in recent papers of Krajicek-Oliveira (2017), Muller-Bydzovsky (2020), and Bydzovsky-Krajicek-Oliveira (2020) via a mixture of techniques from proof theory, complexity theory, and model theory. In contrast, by extracting computational information from proofs via a direct translation to LEARN-uniformity, we establish robust unprovability theorems that unify, simplify, and extend nearly all previous results. In addition, our lower bounds against randomized LEARN-uniformity yield unprovability results for theories augmented with the \emph{dual weak pigeonhole principle}, such as $APC^1$ (Jerabek, 2007), which is known to formalize a large fragment of modern complexity theory.

Finally, we make precise potential limitations of theories of bounded arithmetic such as PV (Cook, 1975) and Jerabek's theory $APC^1$, by showing unconditionally that these theories cannot prove statements like ``$NP\not\subseteq BPP \wedge NP\subset io$-P/poly'', i.e., that NP is uniformly ``hard'' but non-uniformly ``easy'' on infinitely many input lengths. In other words, if we live in such a complexity world, then this cannot be established feasibly.</summary>
    <updated>2021-07-07T21:27:49Z</updated>
    <published>2021-07-07T21:27:49Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-14T17:37:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/open-problem-optimal-query-release/</id>
    <link href="https://differentialprivacy.org/open-problem-optimal-query-release/" rel="alternate" type="text/html"/>
    <title>Open Problem - Optimal Query Release for Pure Differential Privacy</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Releasing large sets of statistical queries is a centerpiece of the theory of differential privacy.  Here, we are given a <em>dataset</em> \(x = (x_1,\dots,x_n) \in [T]^n\), and a set of <em>statistical queries</em> \(f_1,\dots,f_k\), where each query is defined by some bounded function \(f_j : [T] \to [-1,1]\), and (abusing notation) is defined as
\[
f_j(x) = \frac{1}{n} \sum_{i=1}^{n} f_j(x_i).
\]
We use \(f(x) = (f_1(x),\dots,f_k(x))\) to denote the vector consisting of the true answers to all these queries.
Our goal is to design an \((\varepsilon, \delta)\)-differentially private algorithm \(M\) that takes a dataset \(x\in [T]^n\) and outputs a random vector \(M(x)\in \mathbb{R}^k\) such that \(\| M(x) - f(x) \|\) is small in expectation for some norm \(\|\cdot\|\). Usually algorithms for this problem also give high probability bounds on the error, but we focus on expected error for simplicity.</p>

<p>This problem has been studied for both <em>pure differential privacy</em> (\(\delta = 0\)) and <em>appproximate differential privacy</em> (\(\delta &gt; 0\)), and for both \(\ell_\infty\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{\infty} ) \leq \alpha,
\]
and \(\ell_2\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{2} ) \leq \alpha k^{1/2},
\]
giving four variants of the problem.  By now we know tight worst-case upper and lower bounds for two of these variants, and nearly tight bounds (up to logarithmic factors) for a third. The tightest known upper bounds are given in the following table.</p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>Pure DP</td>
      <td>Approx DP</td>
    </tr>
    <tr>
      <td>\( \ell_2 \)<br/>error</td>
      <td>\( \alpha \lesssim \left(\frac{\log^2 k ~\cdot~ \log^{3/2}T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://arxiv.org/abs/1212.0297">NTZ13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://guyrothblum.files.wordpress.com/2014/11/drv10.pdf">DRV10</a>]</td>
    </tr>
    <tr>
      <td>\( \ell_\infty \)<br/>error</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log T}{\varepsilon n} \right)^{1/3} \)  <br/> [<a href="https://arxiv.org/abs/1109.2229">BLR13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://guyrothblum.files.wordpress.com/2014/11/hr10.pdf">HR10</a>, <a href="https://arxiv.org/abs/1107.3731">GRU12</a>]</td>
    </tr>
  </tbody>
</table>

<p>The bounds for approximate DP are known to be tight [<a href="https://arxiv.org/abs/1311.3158">BUV14</a>].  Our two open problems both involve improving the best known upper bounds for pure differential privacy.</p>

<blockquote>
  <p><b>Open Problem 1:</b> What is the best possible \(\ell_\infty\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the known upper bound in the table can be improved to
\[
\alpha = \left(\frac{\log k \cdot \log T}{\varepsilon n} \right)^{1/2},
\]
which is known to be the best possible [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1].</p>

<blockquote>
  <p><b>Open Problem 2:</b> What is the best possible \(\ell_2\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the upper bound can be improved to
\[
\alpha = \left(\frac{\log T}{\varepsilon n} \right)^{1/2}.
\]
The construction used in [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1] can be analyzed to show this bound would be tight. Note, in particular, that this conjecture implies that the tight upper bound has no dependence on the number of queries, similarly to the case of \(\ell_2\) error and approximate DP.</p></div>
    </summary>
    <updated>2021-07-07T17:45:00Z</updated>
    <published>2021-07-07T17:45:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-07-13T23:15:56Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8892467103290123546</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8892467103290123546/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8892467103290123546" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8892467103290123546" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html" rel="alternate" type="text/html"/>
    <title>Would you take this bet (Part 1) ?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I am going to present a well known paradox (I didn't know it until last week, but the source I read said it was well known) and ask your opinion in this post, and reveal my thoughts in my next post.</p><p>I don't want you to go to the web and find out about it, I want your natural thoughts. Of course I can't stop you, but note that I did not give the name of the paradox. </p><p>Here it is:</p><p>I offer you the following bet: </p><p>I will flip a coin.</p><p>If  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>If  HEADS you get 2 dollars and we end there.</p><p>If  TAILS I flip again</p><p><br/></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>etc. </p><p>1) Expected value: </p><p>Prob of getting 1 dollar is 1/2</p><p>Prob of getting 2 dollars is 1/2^2</p><p>Prob of getting 2^2 dollars is 1/2^3</p><p>etc</p><p>Hence the Expected Value is </p><p>1/2 + 1/2 + 1/2 + ... = INFINITY</p><p><br/></p><p>QUESTION: Would you pay $1000 to play the game?</p><p>Leave your answer in the comments and you may say whatever you want as well,</p><p>but I request you don't give the name of the paradox if you know it. </p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-07-07T00:58:00Z</updated>
    <published>2021-07-07T00:58:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-14T15:39:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=879</id>
    <link href="https://emanueleviola.wordpress.com/2021/07/06/windows-never-changes/" rel="alternate" type="text/html"/>
    <title>Windows never changes</title>
    <summary>For months, Windows 10 complained that it didn’t have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the “storage” function that supposedly tells you what’s taking space wasn’t even close to the truth. This became so bad that I was forced to remove some […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>For months, Windows 10 complained that it didn’t have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the “storage” function that supposedly tells you what’s taking space wasn’t even close to the truth. This became so bad that I was forced to remove some things I didn’t want to remove, often with a lot of effort, because space was so tight that Windows didn’t even have enough to run the uninstaller! In the end I became so desperate that I installed <em>TreeSize Free</em>. It quickly revealed that <em>crash plan </em> was taking up a huge amount of space. This revealed to be associated to the <em>Code42</em> program — a program that the system was listing as taking 200MB. Well, uninstalling Code42 freed SIXTY PERCENT of the hard disk space, 140GB.</p></div>
    </content>
    <updated>2021-07-06T16:42:32Z</updated>
    <published>2021-07-06T16:42:32Z</published>
    <category term="Uncategorized"/>
    <category term="tech"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2021-07-14T17:38:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1546</id>
    <link href="https://ptreview.sublinear.info/?p=1546" rel="alternate" type="text/html"/>
    <title>News for June 2021</title>
    <summary>A quieter month after last month’s bonanza. One (applied!) paper on distribution testing, a paper on tolerant distribution testing, and a compendium of open problems. (Ed: alas, I missed the paper on tolerant distribution testing, authored by one of our editors. Sorry, Clément!) Learning-based Support Estimation in Sublinear Time by Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p id="block-79f77829-3f22-4392-8cc9-3f466129d855">A quieter month after last month’s bonanza. One (applied!) paper on distribution testing, a paper on tolerant distribution testing, and a compendium of open problems. <em>(Ed: alas, I missed the paper on tolerant distribution testing, authored by one of our editors. Sorry, Clément!)</em></p>



<p id="block-db4c65bf-46c4-4c3e-ad70-d3348de84ff3"><strong>Learning-based Support Estimation in Sublinear Time</strong> by Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner (<a href="https://arxiv.org/abs/2106.08396">arXiv</a>). A classic problem in distribution testing is that of estimating the support size \(n\) of an unknown distribution \(\mathcal{D}\). (Assume that all elements in the support have probability at least \(1/n\).) A fundamental result of <a href="http://theory.stanford.edu/~valiant/papers/VV_stoc11.pdf">Valiant-Valiant</a> (2011)  proves that the sample complexity of this problem is \(\Theta(n/\log n)\). A line of work has emerged in trying to reduce this complexity, with additional sources of information. <a href="http://dspace.mit.edu/bitstream/handle/1721.1/101001/Rubinfeld_Testing%20probability.pdf;sequence=1">Canonne-Rubinfeld (2014)</a> showed that, if one can query the exact probabilities of elements, then the complexity can be made independent of \(n\). This paper studies a robust version of this assumption: suppose, we can get constant factor approximations to the probabilities. Then, the main result is that we can get a query complexity of \(n^{1-1/\log(\varepsilon^{-1})} \ll n/\log n\) (where the constant \(\varepsilon\) denotes the additive approximation to the support size). This paper also does empirical experiments to show that the new algorithm is indeed better in practice. Moreover, it shows that existing methods degraded rapidly with poorer probability estimates, while the new algorithm maintains its accuracy even with such estimates. </p>



<p><strong>The Price of Tolerance in Distribution Testing</strong> by Clément L. Canonne, Ayush Jain, Gautam Kamath, and Jerry Li (<a href="https://arxiv.org/abs/2106.13414">arXiv</a>). While we have seen many results in distribution testing, the subject of tolerance is one that hasn’t received as much attention. Consider the problem of testing if unknown distribution \(\mathcal{p}\) (over domain \([n]\)) is the same as known distribution \(\mathcal{q}\). We wish to distinguish \(\varepsilon_1\)-close from \(\varepsilon_2\)-far, under total variation distance. When \(\varepsilon_1\) is zero, this is the standard property testing setting, and classic results yield \(\Theta(\sqrt{n})\) sample complexity. If \(\varepsilon_1 = \varepsilon_2/2\), then we are looking for a constant factor approximation to the distance. And the complexity is \(\Theta(n/\log n)\). Surprisingly, nothing was known in better. Until this paper, that is. The main result gives a complete characterization of sample complexity (up to log factors), for all values of \(\varepsilon_1, \varepsilon_2\). Remarkably, the sample complexity has an additive term \((n/\log n) \cdot (\varepsilon_1/\varepsilon^2_2)\). Thus, when \(\varepsilon_1 &gt; \sqrt{\varepsilon_2}\), the sample complexity is \(\Theta(n/\log n)\). When \(\varepsilon_1\) is smaller, the main result gives a smooth dependence on the sample complexity. One the main challenges is that existing results use two very different techniques for the property testing vs constant-factor approximation regimes. The former uses simpler \(\ell_2\)-statistics (e.g. collision counting), while the latter is based on polynomial approximations (estimating moments). The upper bound in this paper shows that simpler statistics based on just the first two moments suffice to getting results for all regimes of \(\varepsilon_1, \varepsilon_2\).</p>



<p><strong>Open Problems in Property Testing of Graphs</strong> by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2021/088/">ECCC</a>). As the title clearly states, this is a survey covering a number of open problems in graph property testing. The broad division is based on the query model: dense graphs, bounded degree graphs, and general graphs. A reader will see statements of various classic open problems, such as the complexity of testing triangle freeness for dense graphs and characterizing properties that can be tested in \(poly(\varepsilon^{-1})\) queries. Arguably, there are more open problems (and fewer results) for testing in bounded degree graphs, where we lack broad characterizations of testable properties. An important, though less famous (?), open problem is that of the complexity of testing isomorphism. It would appear that the setting of general graphs, where we know the least, may be the next frontier for graph property testing. A problem that really caught my eye: can we transform testers that work for bounded degree graphs into those that work for bounded arboricity graphs? The latter is a generalization of bounded degree that has appeared in a number of recent results on sublinear graph algorithms.</p></div>
    </content>
    <updated>2021-07-06T02:27:19Z</updated>
    <published>2021-07-06T02:27:19Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-07-13T23:15:32Z</updated>
    </source>
  </entry>
</feed>
