<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-05-14T23:39:14Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.06399</id>
    <link href="http://arxiv.org/abs/2105.06399" rel="alternate" type="text/html"/>
    <title>Frequent Pattern Mining in Continuous-time Temporal Networks</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jazayeri:Ali.html">Ali Jazayeri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Christopher_C=.html">Christopher C. Yang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06399">PDF</a><br/><b>Abstract: </b>Networks are used as highly expressive tools in different disciplines. In
recent years, the analysis and mining of temporal networks have attracted
substantial attention. Frequent pattern mining is considered an essential task
in the network science literature. In addition to the numerous applications,
the investigation of frequent pattern mining in networks directly impacts other
analytical approaches, such as clustering, quasi-clique and clique mining, and
link prediction. In nearly all the algorithms proposed for frequent pattern
mining in temporal networks, the networks are represented as sequences of
static networks. Then, the inter- or intra-network patterns are mined. This
type of representation imposes a computation-expressiveness trade-off to the
mining problem. In this paper, we propose a novel representation that can
preserve the temporal aspects of the network losslessly. Then, we introduce the
concept of constrained interval graphs (CIGs). Next, we develop a series of
algorithms for mining the complete set of frequent temporal patterns in a
temporal network data set. We also consider four different definitions of
isomorphism to allow noise tolerance in temporal data collection. Implementing
the algorithm for three real-world data sets proves the practicality of the
proposed algorithm and its capability to discover unknown patterns in various
settings.
</p></div>
    </summary>
    <updated>2021-05-14T22:59:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.06357</id>
    <link href="http://arxiv.org/abs/2105.06357" rel="alternate" type="text/html"/>
    <title>On Minimizing the Number of Running Buffers for Tabletop Rearrangement</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Kai.html">Kai Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Si_Wei.html">Si Wei Feng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Jingjin.html">Jingjin Yu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06357">PDF</a><br/><b>Abstract: </b>For tabletop rearrangement problems with overhand grasps, storage space
outside the tabletop workspace, or buffers, can temporarily hold objects which
greatly facilitates the resolution of a given rearrangement task. This brings
forth the natural question of how many running buffers are required so that
certain classes of tabletop rearrangement problems are feasible. In this work,
we examine the problem for both the labeled (where each object has a specific
goal pose) and the unlabeled (where goal poses of objects are interchangeable)
settings. On the structural side, we observe that finding the minimum number of
running buffers (MRB) can be carried out on a dependency graph abstracted from
a problem instance, and show that computing MRB on dependency graphs is
NP-hard. We then prove that under both labeled and unlabeled settings, even for
uniform cylindrical objects, the number of required running buffers may grow
unbounded as the number of objects to be rearranged increases; we further show
that the bound for the unlabeled case is tight. On the algorithmic side, we
develop highly effective algorithms for finding MRB for both labeled and
unlabeled tabletop rearrangement problems, scalable to over a hundred objects
under very high object density. Employing these algorithms, empirical
evaluations show that random labeled and unlabeled instances, which more
closely mimics real-world setups, have much smaller MRBs.
</p></div>
    </summary>
    <updated>2021-05-14T22:58:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.06349</id>
    <link href="http://arxiv.org/abs/2105.06349" rel="alternate" type="text/html"/>
    <title>Disjoint Paths and Connected Subgraphs for H-Free Graphs</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kern:Walter.html">Walter Kern</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martin:Barnaby.html">Barnaby Martin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paulusma:Dani=euml=l.html">Daniël Paulusma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smith:Siani.html">Siani Smith</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leeuwen:Erik_Jan_van.html">Erik Jan van Leeuwen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06349">PDF</a><br/><b>Abstract: </b>The well-known Disjoint Paths problem is to decide if a graph contains k
pairwise disjoint paths, each connecting a different terminal pair from a set
of k distinct pairs. We determine, with an exception of two cases, the
complexity of the Disjoint Paths problem for $H$-free graphs. If $k$ is fixed,
we obtain the $k$-Disjoint Paths problem, which is known to be polynomial-time
solvable on the class of all graphs for every $k \geq 1$. The latter does no
longer hold if we need to connect vertices from terminal sets instead of
terminal pairs. We completely classify the complexity of $k$-Disjoint Connected
Subgraphs for $H$-free graphs, and give the same almost-complete classification
for Disjoint Connected Subgraphs for $H$-free graphs as for Disjoint Paths.
</p></div>
    </summary>
    <updated>2021-05-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.06322</id>
    <link href="http://arxiv.org/abs/2105.06322" rel="alternate" type="text/html"/>
    <title>Hedging Against Sore Loser Attacks in Cross-Chain Transactions</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xue:Yingjie.html">Yingjie Xue</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Herlihy:Maurice.html">Maurice Herlihy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06322">PDF</a><br/><b>Abstract: </b>A *sore loser attack* in cross-blockchain commerce rises when one party
decides to halt participation partway through, leaving other parties' assets
locked up for a long duration. Although vulnerability to sore loser attacks
cannot be entirely eliminated, it can be reduced to an arbitrarily low level.
This paper proposes new distributed protocols for hedging a range of
cross-chain transactions in a synchronous communication model, such as
two-party swaps, $n$-party swaps, brokered transactions, and auctions.
</p></div>
    </summary>
    <updated>2021-05-14T22:45:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.06287</id>
    <link href="http://arxiv.org/abs/2105.06287" rel="alternate" type="text/html"/>
    <title>Analysis of Busy-Time Scheduling on Heterogeneous Machines</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Mozhengfu Liu, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Xueyan.html">Xueyan Tang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06287">PDF</a><br/><b>Abstract: </b>This paper studies a generalized busy-time scheduling model on heterogeneous
machines. The input to the model includes a set of jobs and a set of machine
types. Each job has a size and a time interval during which it should be
processed. Each job is to be placed on a machine for execution. Different types
of machines have distinct capacities and cost rates. The total size of the jobs
running on a machine must always be kept within the machine's capacity, giving
rise to placement restrictions for jobs of various sizes among the machine
types. Each machine used is charged according to the time duration in which it
is busy, i.e., it is processing jobs. The objective is to schedule the jobs
onto machines to minimize the total cost of all the machines used. We develop
an $O(1)$-approximation algorithm in the offline setting and an
$O(\mu)$-competitive algorithm in the online setting (where $\mu$ is the
max/min job length ratio), both of which are asymptotically optimal.
</p></div>
    </summary>
    <updated>2021-05-14T22:53:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.06166</id>
    <link href="http://arxiv.org/abs/2105.06166" rel="alternate" type="text/html"/>
    <title>The Dynamic k-Mismatch Problem</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Clifford:Rapha=euml=l.html">Raphaël Clifford</a>, Paweł Gawrychowski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kociumaka:Tomasz.html">Tomasz Kociumaka</a>, Daniel P. Martin, Przemysław Uznański <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06166">PDF</a><br/><b>Abstract: </b>The text-to-pattern Hamming distances problem asks to compute the Hamming
distances between a given pattern of length $m$ and all length-$m$ substrings
of a given text of length $n\ge m$. We focus on the $k$-mismatch version of the
problem, where a distance needs to be returned only if it does not exceed a
threshold $k$. We assume $n\le 2m$ (in general, one can partition the text into
overlapping blocks). In this work, we show data structures for the dynamic
version of this problem supporting two operations: An update performs a
single-letter substitution in the pattern or the text, and a query, given an
index $i$, returns the Hamming distance between the pattern and the text
substring starting at position $i$, or reports that it exceeds $k$.
</p>
<p>First, we show a data structure with $\tilde{O}(1)$ update and $\tilde{O}(k)$
query time. Then we show that $\tilde{O}(k)$ update and $\tilde{O}(1)$ query
time is also possible. These two provide an optimal trade-off for the dynamic
$k$-mismatch problem with $k \le \sqrt{n}$: we prove that, conditioned on the
strong 3SUM conjecture, one cannot simultaneously achieve $k^{1-\Omega(1)}$
time for all operations.
</p>
<p>For $k\ge \sqrt{n}$, we give another lower bound, conditioned on the Online
Matrix-Vector conjecture, that excludes algorithms taking $n^{1/2-\Omega(1)}$
time per operation. This is tight for constant-sized alphabets: Clifford et al.
(STACS 2018) achieved $\tilde{O}(\sqrt{n})$ time per operation in that case,
but with $\tilde{O}(n^{3/4})$ time per operation for large alphabets. We
improve and extend this result with an algorithm that, given $1\le x\le k$,
achieves update time $\tilde{O}(\frac{n}{k} +\sqrt{\frac{nk}{x}})$ and query
time $\tilde{O}(x)$. In particular, for $k\ge \sqrt{n}$, an appropriate choice
of $x$ yields $\tilde{O}(\sqrt[3]{nk})$ time per operation, which is
$\tilde{O}(n^{2/3})$ when no threshold $k$ is provided.
</p></div>
    </summary>
    <updated>2021-05-14T23:01:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.06145</id>
    <link href="http://arxiv.org/abs/2105.06145" rel="alternate" type="text/html"/>
    <title>Efficient Stepping Algorithms and Implementations for Parallel Shortest Paths</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Xiaojun.html">Xiaojun Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gu:Yan.html">Yan Gu</a>, Yihan Sun, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Yunming.html">Yunming Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06145">PDF</a><br/><b>Abstract: </b>In this paper, we study the single-source shortest-path (SSSP) problem with
positive edge weights, which is a notoriously hard problem in the parallel
context. In practice, the $\Delta$-stepping algorithm proposed by Meyer and
Sanders has been widely adopted. However, $\Delta$-stepping has no known
worst-case bounds for general graphs. The performance of $\Delta$-stepping also
highly relies on the parameter $\Delta$. There have also been lots of
algorithms with theoretical bounds, such as Radius-stepping, but they either
have no implementations available or are much slower than $\Delta$-stepping in
practice.
</p>
<p>We propose a stepping algorithm framework that generalizes existing
algorithms such as $\Delta$-stepping and Radius-stepping. The framework allows
for similar analysis and implementations of all stepping algorithms. We also
propose a new ADT, lazy-batched priority queue (LaB-PQ), that abstracts the
semantics of the priority queue needed by the stepping algorithms. We provide
two data structures for LaB-PQ, focusing on theoretical and practical
efficiency, respectively. Based on the new framework and LaB-PQ, we show two
new stepping algorithms, $\rho$-stepping and $\Delta^*$-stepping, that are
simple, with non-trivial worst-case bounds, and fast in practice.
</p>
<p>The stepping algorithm framework also provides almost identical
implementations for three algorithms: Bellman-Ford, $\Delta^*$-stepping, and
$\rho$-stepping. We compare our code with four state-of-the-art
implementations. On five social and web graphs, $\rho$-stepping is 1.3--2.5x
faster than all the existing implementations. On two road graphs, our
$\Delta^*$-stepping is at least 14\% faster than existing implementations,
while $\rho$-stepping is also competitive. The almost identical implementations
for stepping algorithms also allow for in-depth analyses and comparisons among
the stepping algorithms in practice.
</p></div>
    </summary>
    <updated>2021-05-14T22:57:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.06131</id>
    <link href="http://arxiv.org/abs/2105.06131" rel="alternate" type="text/html"/>
    <title>A Fast Algorithm for SAT in Terms of Formula Length</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Junqiang.html">Junqiang Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xiao:Mingyu.html">Mingyu Xiao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06131">PDF</a><br/><b>Abstract: </b>In this paper, we prove that the general CNF satisfiability problem can be
solved in $O^*(1.0646^L)$ time, where $L$ is the length of the input
CNF-formula (i.e., the total number of literals in the formula), which improves
the current bound $O^*(1.0652^L)$ given by Chen and Liu 12 years ago. Our
algorithm is a standard branch-and-search algorithm analyzed by using the
measure-and-conquer method. We avoid the bottleneck in Chen and Liu's algorithm
by simplifying the branching operation for 4-degree variables and carefully
analyzing the branching operation for 5-degree variables. To simplify
case-analyses, we also introduce a general framework for analysis, which may be
able to be used in other problems.
</p></div>
    </summary>
    <updated>2021-05-14T22:47:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.06030</id>
    <link href="http://arxiv.org/abs/2105.06030" rel="alternate" type="text/html"/>
    <title>Changeable Sweep Coverage Problem</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liang:Dieyan.html">Dieyan Liang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shen:Hong.html">Hong Shen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.06030">PDF</a><br/><b>Abstract: </b>Sweep coverage is to realize the periodic coverage of targets by planning the
periodic sweeping paths of mobile sensors. It is difficult for sensors to
reduce energy consumption by reducing the moving distances. Therefore, charging
technology is the best way to extend the lifetime of the sweep coverage
network. This paper studies the sweep coverage of rechargeable sensors: the
sensors are rechargeable, constantly sweep between the target points and the
charging stations not only tp meet the periodic coverage requirement of the
target points, but also need to return to the charging stations during the
charging period to avoid running out of energy. This paper proposes the general
definition of Chargeable Sweep Coverage (CSC) problem for the first time, and
studies the complexity of the CSC problem by analyzing CSC problems under
different constraints, and then proposes two kinds of CSC problems under
special constraints: 1) The sensors need to return to their original charging
stations for charging; 2) The sensors can go to different charging stations for
charging, and the number of charging stations is 2. Both of these problems are
NP-hard. In this paper, these two problems are modeled as the maximum set
coverage problem, and the approximation algorithms are obtained by reducing the
number of candidate paths to polynomials. The validity and scalability of the
proposed algorithms is proved by theoretical proof and experimental simulation.
</p></div>
    </summary>
    <updated>2021-05-14T22:44:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.05984</id>
    <link href="http://arxiv.org/abs/2105.05984" rel="alternate" type="text/html"/>
    <title>Sparse Nonnegative Convolution Is Equivalent to Dense Nonnegative Convolution</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bringmann:Karl.html">Karl Bringmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Nick.html">Nick Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakos:Vasileios.html">Vasileios Nakos</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05984">PDF</a><br/><b>Abstract: </b>Computing the convolution $A\star B$ of two length-$n$ vectors $A,B$ is an
ubiquitous computational primitive. Applications range from string problems to
Knapsack-type problems, and from 3SUM to All-Pairs Shortest Paths. These
applications often come in the form of nonnegative convolution, where the
entries of $A,B$ are nonnegative integers. The classical algorithm to compute
$A\star B$ uses the Fast Fourier Transform and runs in time $O(n\log n)$.
</p>
<p>However, often $A$ and $B$ satisfy sparsity conditions, and hence one could
hope for significant improvements. The ideal goal is an $O(k\log k)$-time
algorithm, where $k$ is the number of non-zero elements in the output, i.e.,
the size of the support of $A\star B$. This problem is referred to as sparse
nonnegative convolution, and has received considerable attention in the
literature; the fastest algorithms to date run in time $O(k\log^2 n)$.
</p>
<p>The main result of this paper is the first $O(k\log k)$-time algorithm for
sparse nonnegative convolution. Our algorithm is randomized and assumes that
the length $n$ and the largest entry of $A$ and $B$ are subexponential in $k$.
Surprisingly, we can phrase our algorithm as a reduction from the sparse case
to the dense case of nonnegative convolution, showing that, under some mild
assumptions, sparse nonnegative convolution is equivalent to dense nonnegative
convolution for constant-error randomized algorithms. Specifically, if $D(n)$
is the time to convolve two nonnegative length-$n$ vectors with success
probability $2/3$, and $S(k)$ is the time to convolve two nonnegative vectors
with output size $k$ with success probability $2/3$, then
$S(k)=O(D(k)+k(\log\log k)^2)$.
</p>
<p>Our approach uses a variety of new techniques in combination with some old
machinery from linear sketching and structured linear algebra, as well as new
insights on linear hashing, the most classical hash function.
</p></div>
    </summary>
    <updated>2021-05-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.05923</id>
    <link href="http://arxiv.org/abs/2105.05923" rel="alternate" type="text/html"/>
    <title>Open-end bin packing: new and old analysis approaches</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Epstein:Leah.html">Leah Epstein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05923">PDF</a><br/><b>Abstract: </b>We analyze a recently introduced concept, called the price of clustering, for
variants of bin packing called open-end bin packing problems (OEBP). Input
items have sizes, and they also belong to a certain number of types. The new
concept deals with the comparison of optimal solutions for the cases where
items of distinct types can and cannot be packed together, respectively. The
problem is related to greedy bin packing algorithms and to batched bin packing,
and we discuss some of those concepts as well. We analyze max-OEBP, where a
packed bin is valid if by excluding its largest item, the total size of items
is below 1. For this variant, we study the case of general item sizes, and the
parametric case with bounded item sizes, which shows the effect of small items.
Finally, we briefly discuss min-OEBP, where a bin is valid if the total size of
its items excluding the smallest item is below 1, which is known to be an
entirely different problem.
</p></div>
    </summary>
    <updated>2021-05-14T22:54:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.05911</id>
    <link href="http://arxiv.org/abs/2105.05911" rel="alternate" type="text/html"/>
    <title>The Power of the Weisfeiler-Leman Algorithm for Machine Learning with Graphs</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Christopher Morris, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fey:Matthias.html">Matthias Fey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kriege:Nils_M=.html">Nils M. Kriege</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05911">PDF</a><br/><b>Abstract: </b>In recent years, algorithms and neural architectures based on the
Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism
problem, emerged as a powerful tool for (supervised) machine learning with
graphs and relational data. Here, we give a comprehensive overview of the
algorithm's use in a machine learning setting. We discuss the theoretical
background, show how to use it for supervised graph- and node classification,
discuss recent extensions, and its connection to neural architectures.
Moreover, we give an overview of current applications and future directions to
stimulate research.
</p></div>
    </summary>
    <updated>2021-05-14T22:57:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.05879</id>
    <link href="http://arxiv.org/abs/2105.05879" rel="alternate" type="text/html"/>
    <title>Sketching with Kerdock's crayons: Fast sparsifying transforms for arbitrary linear maps</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Tim Fuchs, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gross:David.html">David Gross</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krahmer:Felix.html">Felix Krahmer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kueng:Richard.html">Richard Kueng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mixon:Dustin_G=.html">Dustin G. Mixon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.05879">PDF</a><br/><b>Abstract: </b>Given an arbitrary matrix $A\in\mathbb{R}^{n\times n}$, we consider the
fundamental problem of computing $Ax$ for any $x\in\mathbb{R}^n$ such that $Ax$
is $s$-sparse. While fast algorithms exist for particular choices of $A$, such
as the discrete Fourier transform, there is currently no $o(n^2)$ algorithm
that treats the unstructured case. In this paper, we devise a randomized
approach to tackle the unstructured case. Our method relies on a representation
of $A$ in terms of certain real-valued mutually unbiased bases derived from
Kerdock sets. In the preprocessing phase of our algorithm, we compute this
representation of $A$ in $O(n^3\log n)$ operations. Next, given any unit vector
$x\in\mathbb{R}^n$ such that $Ax$ is $s$-sparse, our randomized fast transform
uses this representation of $A$ to compute the entrywise $\epsilon$-hard
threshold of $Ax$ with high probability in only $O(sn +
\epsilon^{-2}\|A\|_{2\to\infty}^2n\log n)$ operations. In addition to a
performance guarantee, we provide numerical results that demonstrate the
plausibility of real-world implementation of our algorithm.
</p></div>
    </summary>
    <updated>2021-05-14T22:42:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.12347</id>
    <link href="http://arxiv.org/abs/2012.12347" rel="alternate" type="text/html"/>
    <title>Beating Random Assignment for Approximating Quantum 2-Local Hamiltonian Problems</title>
    <feedworld_mtime>1620950400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parekh:Ojas.html">Ojas Parekh</a>, Kevin Thompson <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.12347">PDF</a><br/><b>Abstract: </b>The quantum k-Local Hamiltonian problem is a natural generalization of
classical constraint satisfaction problems (k-CSP) and is complete for QMA, a
quantum analog of NP. Although the complexity of k-Local Hamiltonian problems
has been well studied, only a handful of approximation results are known. For
Max 2-Local Hamiltonian where each term is a rank 3 projector, a natural
quantum generalization of classical Max 2-SAT, the best known approximation
algorithm was the trivial random assignment, yielding a 0.75-approximation. We
present the first approximation algorithm beating this bound, a classical
polynomial-time 0.764-approximation. For strictly quadratic instances, which
are maximally entangled instances, we provide a 0.801 approximation algorithm,
and numerically demonstrate that our algorithm is likely a 0.821-approximation.
We conjecture these are the hardest instances to approximate. We also give
improved approximations for quantum generalizations of other related classical
2-CSPs. Finally, we exploit quantum connections to a generalization of the
Grothendieck problem to obtain a classical constant-factor approximation for
the physically relevant special case of strictly quadratic traceless 2-Local
Hamiltonians on bipartite interaction graphs, where a inverse logarithmic
approximation was the best previously known (for general interaction graphs).
Our work employs recently developed techniques for analyzing classical
approximations of CSPs and is intended to be accessible to both quantum
information scientists and classical computer scientists.
</p></div>
    </summary>
    <updated>2021-05-14T22:44:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/070</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/070" rel="alternate" type="text/html"/>
    <title>TR21-070 |  SOS lower bound for exact planted clique | 

	Shuo Pang</title>
    <summary>We prove a SOS degree lower bound for the planted clique problem on Erd{\"o}s-R\'enyi random graphs $G(n,1/2)$. The bound we get is degree $d=\Omega(\epsilon^2\log n/\log\log n)$ for clique size $\omega=n^{1/2-\epsilon}$, which is almost tight. This improves the result of \cite{barak2019nearly} on the ``soft'' version of the problem, where the family of equality-axioms generated by $x_1+...+x_n=\omega$ was relaxed to one inequality $x_1+...+x_n\geq\omega$.

As a technical by-product, we also ``naturalize'' previous techniques developed for the soft problem. This includes a new way of defining the pseudo-expectation and a more robust method to solve the coarse diagonalization of the moment matrix.</summary>
    <updated>2021-05-13T17:19:11Z</updated>
    <published>2021-05-13T17:19:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-14T23:37:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18720</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/" rel="alternate" type="text/html"/>
    <title>Matrix—The Meeting</title>
    <summary>That’s how it is with people. Nobody cares how it works as long as it works—Councillor Hamann Santosh Vempala and Nikhil Srivastava announced the first in hopefully a series of online meetings about matrix algorithms. Not about the Matrix—the—movie. Santosh and Nikhil said: we expect to have an attendance of people. Wrong. It was over […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>That’s how it is with people. Nobody cares how it works as long as it works—Councillor Hamann</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/vempalasrivastava/" rel="attachment wp-att-18739"><img alt="" class="alignright wp-image-18739" height="128" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/VempalaSrivastava.png?resize=192%2C128&amp;ssl=1" width="192"/></a></p>
<p>
Santosh Vempala and Nikhil Srivastava announced the first in hopefully a series of online meetings about matrix algorithms. Not about the <i>Matrix</i>—the—<a href="https://rjlipton.wpcomstaging.com/feed/">movie</a>. Santosh and Nikhil said: we expect to have an attendance of <img alt="{20-60}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B20-60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> people. Wrong. It was over 200 today.</p>
<p>
Today I thought we would talk about the Zoom meeting and future ones being planned. </p>
<p>
Zoom feels closer to the world of the <i>Matrix</i> movies. If you haven’t seen them, all you need to know is the premise of humanity being diverted in a virtual reality.  How do we know the little figures in those boxes are real people?  More concretely, it seems obvious to Ken and me that simulated human online agents will arrive much earlier than person-like robots.  </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/the-matrix-architects-room/" rel="attachment wp-att-18742"><img alt="" class="alignright wp-image-18742" height="216" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/the-matrix-architects-room.jpg?resize=384%2C216&amp;ssl=1" width="384"/></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2"><i>Matrix Reloaded</i> virtual <a href="https://virtualbackgrounds.site/background/the-matrix-architects-room">background</a><br/>
</font>
</td>
</tr>
</tbody></table>
<p>
In particular, how much does it take to automate giving a lecture online?  Ken has spent much time this term upgrading his lecture notes in two courses to broadcast quality.  Delivering them remotely trades against the spontaneity of drawing pictures on a whiteboard or document camera and developing proofs and algorithms step-by-step.  It should be easier to develop an AI capable of reacting to questions put in Zoom chat than with in-class situations, where “reading the room” is also important for modulating the speed and manner of presentation.  </p>
<p>
</p><h2> The Meetings </h2><p/>
<p>
Daniel Kressner, Mike Mahoney, Cleve Moler, Alex Townsend, and Joel Tropp were also organizers of this smeeting on matrix computation. This Wednesday was the first in a series of online meetings. The speakers for today were Peter Bürgisser, Nick Higham, and Cameron Musco, and the panelists were Jim Demmel, Ilse Ipsen, and Richard Peng.</p>
<p>
The blurb for the meetings is:</p>
<blockquote><p><b> </b> <em> We are organizing an online seminar series on “Complexity of Matrix Computations”, whose goal is to bridge the gap between how numerical linear algebra and theoretical computer science researchers view and study the fundamental computational problems of linear algebra. This gap includes foundational issues such as: what is the computational model? What does it mean to solve a problem? On which criteria do we compare algorithms? We also hope to discuss which techniques in theoretical computer science might be useful in numerical linear algebra and vice versa. </em>
</p></blockquote>
<p>
I love seeing the words “fundamental” and “foundational”, and one question resonated even more.</p>
<p>
</p><p/><h2> The Question </h2><p/>
<p/><p>
What does it mean to solve a problem? In this case what does it mean to solve a linear equation? This is the question that was discussed the most—especially at the end of the meeting. </p>
<p>
I have always thought there is an answer to this. The answer is based on asking what the client wants. Imagine Alice is asked by Bob to <tt>solve</tt> a linear system 	</p>
<p align="center"><img alt="\displaystyle  Ax = b " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Ax+%3D+b+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Alice could go off and return the <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that solves the system. Or she could say there is no such <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Or she could say there are many such <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>‘s. Which is the correct answer? </p>
<p>
I believe the right answer is: Alice should ask Bob:</p>
<blockquote><p><b> </b> <em> Bob, what will you do with the answer to this? </em>
</p></blockquote>
<p>
Bob could say, for example: </p>
<ol>
<li>
I plan to compute the inner product of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> I have. <p/>
</li><li>
I plan to see what the norm of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is. <p/>
</li><li>
Or, I plan to see what <img alt="{x_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is. <p/>
</li><li>
Or, I could be just happy to know that there is some <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. <p/>
</li><li>
Or, and so on.
</li></ol>
<p>
Thus, I believe, the answer only makes sense if Alice knows what will be done next with the “solution”. What do you think?</p>
<p>
</p><p/><h2> One View </h2><p/>
<p/><p>
What does it mean to solve the equation <img alt="{Ax=b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%3Db%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, for an invertible matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? What do precision, accuracy, conditioning, and complexity mean in this context?</p>
<p>
Jim Demmel’s view is captured in his notes that he was kind enough to download to the site <a href="https://app.slack.com/client/T021927P7ST/C021PQXNPEE">SLACK</a>. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What do you think about this series of meetings? Did you attend them initially? Will you look in next time so we can break 200 attendees?</p>
<p>Santosh says: To join the seminar, please send an email<br/>
<a href="mailto: cmc-l-request@cornell.edu">Join Zoom</a><br/>
after adding the subject “join”. Information about how to connect to the Zoom conference call will be circulated via email to all registered attendees prior to each seminar.</p></font></font></div>
    </content>
    <updated>2021-05-13T12:17:45Z</updated>
    <published>2021-05-13T12:17:45Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="matrix"/>
    <category term="online"/>
    <category term="open problems"/>
    <category term="practice"/>
    <category term="Theory"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-05-14T23:37:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1929264998273205739</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1929264998273205739/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1929264998273205739" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1929264998273205739" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html" rel="alternate" type="text/html"/>
    <title>Cryptocurrency, Blockchains and NFTs</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I first wrote about bitcoin in this blog <a href="https://blog.computationalcomplexity.org/2011/11/making-money-computationally-hard-way.html">ten years ago</a> after I gave a lecture in a cryptography class I taught at Northwestern. Two years later I had a <a href="https://blog.computationalcomplexity.org/2013/12/bitcoins-revisited.html">follow-up post</a>, noting the price moved from $3 to $1000 with a market cap of about $11 Billion. My brother who thought they were a scam back then has since become a cryptocurrency convert. The bitcoin market cap is now over a trillion dollars and other cryptocurrencies are not far behind. No longer can we view cryptocurrencies as simply a neat exercise in applied cryptography now that it has serious value.</p><p>The main uses of cryptocurrencies are for speculation or illegal activities, such as drug sales, ransoms, money laundering and tax evasion. Sure you can buy a Tesla with bitcoins but that's more of a gimmick. Cryptocurrency spending is simply too slow, expensive and volatile right now to replace other methods of electronic payment. </p><p>Non-fungible tokens (NFTs) truly puzzle me. They are just a digital certificate of authentication. What could you do with them you couldn't do with docusign? Collectibles of publicly available digital goods is a fad already fading.</p><p>I'm not a fan of a fiat currency governed by strict rules not under governmental control. Bad things could happen. However thinking of cryptocurrencies and the blockchain technology that underlies them have brought up real needs for our digital world.</p><p/><ul style="text-align: left;"><li>An easy way to pay online without significant fees, expenses or energy consumption.</li><li>An easy and cheap way to transfer money between different countries.</li><li>A distributed database to allow tracking of supply chains, credentials and financial transactions for example. I see less a need to make these databases decentralized.</li><li>A need, for some, to have a digital replacement for the anonymity of cash.</li><li>People need something to believe in once they have given up believing in religion and a functioning democracy. </li></ul><div>Don't change your investing habits based on anything I write in this post. Speculation and illegal activities are powerful forces. Or it could all collapse. Make your bets, or don't.</div><div><br/></div><div><b>Note</b>: Since I wrote this post yesterday, Elon Musk <a href="https://twitter.com/elonmusk/status/1392602041025843203">tweeted</a> that Tesla will no longer accept bitcoins, and the bitcoin market cap has dropped below a trillion.</div><p/></div>
    </content>
    <updated>2021-05-13T11:55:00Z</updated>
    <published>2021-05-13T11:55:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-14T23:29:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry</id>
    <link href="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry.html" rel="alternate" type="text/html"/>
    <title>The constructive solid geometry of piecewise-linear functions</title>
    <summary>My latest preprint, “A stronger lower bound on parametric minimum spanning trees” (arXiv:2105.05371, to appear at WADS) gives examples of graphs, with edge weights that are linear functions of a parameter \(\lambda\), such that different choices of \(\lambda\) lead to \(\Omega(m\log n)\) different minimum spanning trees, improving a bound of \(\Omega\bigl(m\alpha(n)\bigr)\) from one of my earlier papers. But it was almost about a different problem in discrete geometry rather than graph theory, and it almost didn’t happen at all. I thought I had a bound for another related problem until the proof fell apart, irreparably. I was in the process of throwing away my mostly-written draft when I found a different proof, allowing me to rescue the paper.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest preprint, “A stronger lower bound on parametric minimum spanning trees” (<a href="https://arxiv.org/abs/2105.05371">arXiv:2105.05371</a>, to appear at WADS) gives examples of graphs, with edge weights that are linear functions of a parameter \(\lambda\), such that different choices of \(\lambda\) lead to \(\Omega(m\log n)\) different minimum spanning trees, improving a bound of \(\Omega\bigl(m\alpha(n)\bigr)\) from <a href="https://doi.org/10.1007/PL00009396">one of my earlier papers</a>. But it was almost about a different problem in discrete geometry rather than graph theory, and it almost didn’t happen at all. I thought I had a bound for another related problem until the proof fell apart, irreparably. I was in the process of throwing away my mostly-written draft when I found a different proof, allowing me to rescue the paper.</p>

<p>Here’s the problem I thought I was solving when I started writing the paper: Suppose you want to <a href="https://en.wikipedia.org/wiki/Constructive_solid_geometry">construct a complicated shape using unions and intersections of simpler shapes</a>. For the version of the problem I was considering, the shapes belong to the Euclidean plane, and the simple shapes that you start with are the half-planes above a line. When you take unions or intersections of these shapes, the more complicated shapes that you get are sets of points above a piecewise linear \(x\)-monotone curve. Another way to understand the same setup is that you’re starting with linear functions and building more-complicated piecewise linear functions by taking pointwise maxima or minima. And what I wanted to know was: If you have a formula expressing a shape using unions and intersections of \(n\) upper halfplanes, or equivalently expressing a piecewise-linear function using maxima and minima of \(n\) linear functions, how complicated can the result be? I thought I had a proof that one could construct shapes with \(\Omega(n\log n)\) vertices, or piecewise-linear functions with \(\Omega(n\log n)\) breakpoints, and when it broke I thought I didn’t have a paper any more.</p>

<p style="text-align: center;"><img alt="Recursive construction of a piecewise linear function by maxima and minima of simpler functions" src="https://11011110.github.io/blog/assets/2021/minmax.svg"/></p>

<p>The figure above illustrates what I thought was the recursive construction. The base case, in the upper left, is a linear function (a piecewise-linear function with one piece, generated by a max-min formula with one term). In middle left we have a second-level function, the pointwise maximum of two of these linear functions, with two pieces. On the bottom left we have a third-level function, the pointwise minimum of two second-level functions, with six pieces. And the large image on the right shows a fourth-level function, the pointwise maximum of two third-level functions, with 16 pieces. At each level of recursion, you replace each line by two perturbed copies, getting a breakpoint where they cross. When you take a maximum, each breakpoint that looked like a local maximum expands to three breakpoints, while each breakpoint that looked like a local minimum stays as just a single breakpoint; the case of taking a minimum is symmetric. Setting up and solving a recurrence for the numbers of breakpoints of each type gives \(\Omega(n\log n)\).</p>

<p>The problem was that I couldn’t control the resulting piecewise-linear functions well enough to ensure that I could expand all of the local maxima into triple breakpoints and produce new breakpoints for each pair of crossing lines. These two issues are related, because you get a tripled breakpoint only for pairs of pairs of lines that have a certain above-below relation, and the breakpoint of a pair of crossing lines changes their above-below relation. It works for each step in the figure, but that’s because these cases are still too small for the problems to show up. So the analysis above breaks down.</p>

<p>As well as unions and intersections of shapes, or minima and maxima of functions, there’s another graph-theoretical interpretation of the same problem, and that’s where the rewritten paper comes in. The piecewise linear functions that you get from recursive unions and intersections correspond to parametric solutions to the <em>bottleneck shortest path problem</em>: find a path that connects two fixed vertices of a graph, whose heaviest edge is as light as possible, and let \(\beta\) (the bottleneck) be the weight of this edge. How does \(\beta\) vary as a function of \(\lambda\)? For <a href="https://en.wikipedia.org/wiki/Series%E2%80%93parallel_graph">series-parallel graphs</a>, the two vertices should be the two terminals, series composition of graphs gives you the maximum of their bottleneck functions, and parallel composition of graphs gives you the minimum of their bottleneck functions. So for these graphs, the parametric bottleneck shortest path problem is the same one that I didn’t solve.</p>

<p>However, the bottleneck shortest path problem is solved by the minimum spanning tree, in the sense that the path between two vertices in a minimum spanning tree is always a bottleneck shortest path (although there may be other equally good paths). Some of the breakpoints of the bottleneck function, the ones that look locally like a minimum of two linear functions, come from combinatorial changes in the parametric minimum spanning tree, and (by negating everything and swapping min for max if necessary) we can ensure that at least half of the changes in the worst-case bottleneck function come from spanning tree changes in this way. Therefore, lower bounds on the bottleneck problem extend to minimum spanning trees, and upper bounds on minimum spanning trees extend to the bottleneck problem. In fact, my previous \(\Omega\bigl(m\alpha(n)\bigr)\) bound on the spanning tree problem came from a \(\Omega\bigl(n\alpha(n)\bigr)\) bound on two-level piecewise linear functions (minima of maxima of linear functions), and a previous \(O(mn^{1/3})\) <a href="https://doi.org/10.1007/PL00009354">upper bound of Tamal Dey on the spanning tree problem</a> implies an \(O(n^{4/3})\) upper bound on the bottleneck problem.</p>

<p>So when my lower bound for the bottleneck problem fell apart, I instead started thinking about trying to find a similar recursive lower bound for spanning trees instead of bottleneck paths, and was more successful. It works more easily because I don’t have to control the piecewise linear functions so carefully in order to keep their crossings and breakpoints intact; instead, I can just take three copies of the lower level of the construction, flatten them by linear transformations so they are each close to a line, with their breakpoints in disjoint intervals of the \(\lambda\)-axis, and combine them as if they were linear. It wouldn’t work for the bottleneck problem because you would only get a constant number of new breakpoints where one of the recursive copies crosses over to the other, but for the spanning tree problem you’re combining trees rather than functions so you get more breakpoints in these regions.</p>

<p>The figure below gives an example of the construction, a series-parallel graph with six vertices (upper right) and linear edge weight functions (upper left) that produces 12 parametric minimum spanning trees (bottom). The red, blue, and green parts show the three copies of the recursive construction that are combined to form this example. For a more detailed explanation see the preprint.
The preprint also includes a packing argument that transforms the resulting \(\Omega(n\log n)\) bound for \(n\)-vertex series-parallel graphs into an \(\Omega(m\log n)\) bound for graphs whose number \(m\) of edges can be significantly larger than \(n\), but I think that’s more just a technicality.</p>

<p style="text-align: center;"><img alt="Six-vertex series-parallel graph with 12 parametric minimum spanning trees" src="https://11011110.github.io/blog/assets/2021/parametric-mst.svg"/></p>

<p>It would be interesting either to find a different construction proving that the halfspace / piecewise linear function / bottleneck path problem can have complexity \(\Omega(n\log n)\), matching this new result, or to prove an upper bound separating this problem from the lower bound on the parametric minimum spanning tree problem, but that will have to wait for another day.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106225329805495195">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-05-12T17:21:00Z</updated>
    <published>2021-05-12T17:21:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-13T01:39:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/069</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/069" rel="alternate" type="text/html"/>
    <title>TR21-069 |  PPSZ is better than you think | 

	Dominik Scheder</title>
    <summary>PPSZ, for long time the fastest known algorithm for k-SAT, works by going through the variables of the input formula in random order; each variable is then set randomly to 0 or 1, unless the correct value can be inferred by an efficiently implementable rule (like small-width resolution; or being implied by a small set of clauses).
We show that PPSZ performs exponentially better than previously known, for all k &gt;= 3. For Unique-3-SAT we bound its running time by O(1.306973n), which is somewhat better than the algorithm of Hansen, Kaplan, Zamir, and Zwick.
All improvements are achieved without changing the original PPSZ. The core idea is to pretend that PPSZ does not process the variables in uniformly random order, but according to a carefully designed distribution. We write "pretend" since this can be done without any actual change to the algorithm.</summary>
    <updated>2021-05-12T08:47:32Z</updated>
    <published>2021-05-12T08:47:32Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-14T23:37:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/glm_saga/</id>
    <link href="https://gradientscience.org/glm_saga/" rel="alternate" type="text/html"/>
    <title>Debuggable Deep Networks: Sparse Linear Models (Part 1)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><!-- <script src="//code.jquery.com/jquery-1.10.2.js"></script> -->


<!-- <script src="https://code.jquery.com/jquery-1.12.4.js"></script> -->
<!-- <script src=https://code.jquery.com/ui/1.12.1/jquery-ui.min.js></script> -->










<p><a class="bbutton" href="https://arxiv.org/abs/2105.04857" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/DebuggableDeepNetworks" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<!-- <a class="bbutton" href="/breeds_class_hierarchy">
<i class="fa fa-tree"></i>
&nbsp;&nbsp; Hierarchies
</a> -->
<br/></p>

<p><i>This two-part series overviews our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on constructing deep networks that perform well while, at the same time, being easier to debug. Part 1 (below) describes our toolkit for building such networks and how it can be leveraged in the context of typical language and vision tasks. This toolkit applies the classical primitive of sparse linear classification on top of feature representations derived from deep networks, and includes a custom solver for fitting such sparse linear models at scale. <a href="https://gradientscience.org/debugging">Part 2</a> outlines a suite of human-in-the-loop experiments that we designed to evaluate the debuggability of such networks. These evaluations demonstrate, in particular, that simply inspecting the sparse final decision layer of these networks can facilitate detection of unintended model behaviours—e.g., spurious correlations and input patterns that cause misclassifications. </i></p>

<p>As ML models are being increasingly deployed in the real world, a question that jumps to the forefront is: how do we know these models are doing “the right thing”? In particular, how can we be sure that models aren’t relying on brittle or undesirable correlations extracted from the data, which undermines their robustness and reliability?</p>

<p>It turns out that, as things stand today, we often can’t. In fact, numerous recent studies have pointed out that seemingly accurate ML models base their predictions on data patterns that are unintuitive or unexpected, leading to a variety of  downstream failures. For instance, in a <a href="https://gradientscience.org/adv/">previous post</a> we discussed how adversarial examples arise because models make decisions based on imperceptible features in the data. There are many other examples of this—e.g., image pathology detection models relying on <a href="https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf">pen marks made by radiologists</a>; and toxic comment classification systems being disproportionately sensitive to <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-group related keywords</a>.</p>

<p>These examples highlight a growing need for model debugging tools: techniques which can facilitate the <i>semi-automatic</i> discovery of such failure modes. In fact, a closely related problem of interpretability—i.e., the task of precisely characterizing how and why models make their decisions, is already a major focus of the ML community.</p>

<h2 id="how-to-debug-your-deep-network">How to debug your deep network?</h2>

<p>A natural approach to model debugging is to inspect the model directly. While this may be feasible in certain settings (e.g., for small linear classifiers or decision trees), it quickly becomes  infeasible as we move towards large, complex models such as deep networks. To work around such scale issues, current approaches (spearheaded in the context of interpretability) attempt to understand  model behavior in a somewhat localized or decomposed manner. In particular, there exist two prominent families of deep network interpretability methods—one that attempts to explain what individual neurons do [<a href="https://arxiv.org/abs/1506.06579">Yosinski et al. 2015</a>, <a href="https://arxiv.org/abs/1704.05796">Bau et al. 2018</a>] and the other one aiming to discern how the model makes decisions for specific inputs [<a href="https://arxiv.org/abs/1312.6034">Simonyan et al. 2013</a>, <a href="https://arxiv.org/abs/1602.04938">Ribeiro et al. 2016</a>]. The challenge however is that, as shown in recent studies [<a href="https://arxiv.org/abs/1810.03292">Adebayo et al., 2018</a>, <a href="https://arxiv.org/abs/2011.05429">Adebayo et al., 2020</a>, <a href="https://arxiv.org/abs/2010.12016">Leavitt &amp; Morcos, 2020</a>], such localized interpretations can be hard to aggregate, are easily fooled, and overall, may not give a clear picture of the model’s reasoning process.</p>

<p>Our work thus takes an alternative approach. First, instead of trying to directly obtain a complete characterization of how and why a deep network makes its decision (which is the goal in  interpretability research), we focus on the more actionable problem of debugging unintended model behaviors. Second, instead of attempting to grapple with the challenge of analyzing these networks in a purely “post hoc” manner, we <i>train</i> them to make them inherently more debuggable.</p>

<p>The specific way we accomplish this goal is motivated by a natural view of a deep network as a composition of a <i>feature extractor</i> and a <i>linear decision layer</i> (see the figure below). From this viewpoint, we can break down the problem of inspecting and understanding a deep network into two subproblems: (1) interpreting the deep features (also known in the literature as neurons—that we will refer to as features henceforth) and (2) understanding how these features are aggregated in the (final) linear decision layer to make predictions.</p>

<p><img alt="Overview" src="https://gradientscience.org/assets/glm_saga/figures/intro.png"/></p>
<div class="footnote">
    <b> Overview of our approach to construct deep networks that are more debuggable:</b> We train a sparse decision layer on (pre-trained) deep feature embeddings and then view the network’s decision process as a linear combination of these features.
</div>

<p>Let us now discuss both of these subproblems in more detail.</p>

<h3 id="task-1-interpreting-deep-features">Task 1: Interpreting (deep) features</h3>

<p>Given the architectural complexity of deep networks, precisely characterizing the role of even a single neuron (in any layer) is challenging. However, research in ML interpretability has brought us a number of heuristics geared towards identifying the input patterns that cause specific neurons (or features) to activate. Thus, for the first task, we leverage some of these existing feature interpretation techniques—specifically, feature visualization, in case of vision models <a href="https://arxiv.org/abs/1904.08939">[Nguyen et al. 2019]</a> and LIME, in case of vision/language models <a href="https://arxiv.org/abs/1602.04938">[Ribeiro et al. 2016]</a>. While these methods have certain limitations, they turn out to be surprisingly effective for model debugging within our framework. Also, note that our approach is fairly modular, and we can substitute these methods with any other/better variants.</p>

<div class="footnote">
    Although LIME was originally used to interpret the predicted outputs of a network, in our work we adapt it to interpret individual neurons instead (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for more details). 
</div>

<p><img alt="Examples of feature visualization" src="https://gradientscience.org/assets/glm_saga/figures/fv_examples_both.png"/></p>
<div class="footnote">
    <b>Examples of feature visualizations for ImageNet classifiers:</b> Feature visualizations for standard vision models (<i>top</i>) are often hard to parse despite significant research on this front. This may be a side effect of these models relying on human-unintelligible features to make their predictions (discussed in a <a href="https://gradientscience.org/adv/">previous post</a>). On the other hand, robust vision models (<i>bottom</i>) tend to have more human-aligned features <a href="https://arxiv.org/abs/1906.00945">[Engstrom et al. 2019]</a>.
</div>

<p><img alt="Examples of word cloud visualization" src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_sst_6.png"/></p>
<div class="footnote">
    <b>Feature interpretation for language models</b>: Examples of a word cloud visualization for the positive and negative activation of a single neuron for a text sentiment classifier. We generate these by aggregating LIME explanations for features, with the whole process described in our <a href="https://arxiv.org/abs/2105.04857">paper</a>. 
</div>

<h3 id="task-2-examining-the-decision-layer">Task 2: Examining the decision layer</h3>

<p>At first glance, the task of making sense of the decision layer of a deep network appears trivial. Indeed, this layer is linear and interpreting a linear model is a routine task in statistical analysis.  However, this intuition is deceptive—the decision layers of modern deep networks often contain upwards of thousands of (deep) features and millions of parameters—making human inspection intractable.</p>

<p><img alt="Feature visualization dump" src="https://gradientscience.org/assets/glm_saga/figures/small_fv_dump.png"/></p>
<div class="footnote">
    <b>Scale of typical decision layers</b>: Feature visualizations for one quarter (512 out of 2048) of all the features of a robust ImageNet classifier. A typical dense decision layer will rely on a weighted sum of <i>all</i> of these features to produce a single prediction.
</div>

<p>So what can we do about this?</p>

<p>Recall that the major roadblock here is the size of the decision layer. What if we just constrained ourselves only to the “important” weights/features within this layer though? Would that allow us to understand the model?</p>

<p>To test this, we focus our attention on the features that are assigned large weights (in terms of magnitude) by the decision layer.  (Note that all the features are standardized to have zero mean and unit variance to make such a weight comparison more meaningful.)</p>

<p>In the figure below, we evaluate the performance of the decision layer when it is restricted to using: (a) only the “important features” or (b) all features <i>but</i> the important ones. The expectation here is that if the important features are to suffice for model debugging, they should at the very least be enough to let the model match its original performance.</p>

<div>
    <div class="ablation_dense">
        <canvas height="200" id="ablation_dense_chart" width="400"/>
    </div>
</div>
<div class="footnote">
     <b>Feature importance in dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. 
</div>

<p>As we can see, this is not the case for typical deep networks. Indeed, for all but one task, the top-k features (k is 10 for vision and 5 for language task) are far from sufficient to recover model performance. Further, there seems to be a great deal of redundancy in the standard decision layer—the model can perform quite well even without using any of the seemingly important features. Clearly, inspecting only the highest-weighted features does not seem to be sufficient from a debugging standpoint.</p>

<h4 id="our-solution-retraining-with-sparsity">Our solution: retraining with sparsity</h4>

<p>To make inspecting the decision layer more tractable for humans and also deal with feature redundancy, we replace that layer entirely. Specifically, rather than finding better heuristics for identifying salient features within the standard (dense) decision layer, we <i>retrain</i> it (on top of the existing feature representations) to be sparse.</p>

<p>To this end, we leverage a classic primitive from statistics: <i>sparse linear classifiers</i>. Concretely, we use the <a href="https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf">elastic net</a> approach to train regularized linear decision layers on top of the fixed (pre-trained) feature representation.</p>

<p>The elastic net is a popular approach for fitting linear models in statistics, that combines the benefits of both L1 and L2 regularization.  Elastic net solvers yield not one but a series of sparse linear models—each with different sparsity/accuracy—based on the strength of regularization. We can then let our application-specific accuracy vs sparsity needs guide our choice of a specific sparse decision layer from this series.</p>

<p>However, when employing this approach to modern deep networks, we hit an obstacle—existing solvers for training regularized linear models simply cannot scale to the number of datapoints and input features that we would typically have in deep learning. To overcome this problem, we develop a custom, efficient solver for fitting regularized generalized linear models at scale. This solver leverages recent advances in <a href="https://arxiv.org/abs/1902.00071">variance reduced gradient methods</a> and combines them with <a href="https://web.stanford.edu/~hastie/Papers/glmnet.pdf">path-algorithms</a> from statistics to get fast and stable convergence at ImageNet scales. We won’t go into much detail here, but we point the curious reader to our <a href="https://arxiv.org/abs/2105.04857">paper</a> and our <a href="https://github.com/madrylab/glm_saga">standalone PyTorch package</a> (which might be of independent interest) for more information.</p>

<p>To summarize—the elastic net gives us a sparse decision layer that, in turn, enables us  to debug the resulting network by applying the existing feature interpretation methods to a now-significantly-reduced number of features (i.e., only the ones used by the sparse decision layer).</p>

<h2 id="what-do-we-gain-from-sparity">What do we gain from sparity?</h2>

<p>Now that we have our methodology in place, we can apply it to standard ML tasks and measure the impact of enforcing sparsity of the final decision layer. Specifically, we discuss the results of applying it to ResNet-50 classifiers trained on ImageNet and Places-10 (a 10-class subset of Places365), as well as BERT models trained on the Stanford Sentiment Treebank and Wikipedia toxic comment classification tasks.</p>

<h3 id="sparsity-at-the-last-layer-is-almost-free">Sparsity at the last layer is (almost) free</h3>

<p>Needless to say, the usefulness of our method hinges on the degree of sparsity in the decision layer that we can achieve without losing much accuracy. So how far can we turn the sparsity dial? The answer turns out to be: <i>a lot</i>! For instance, the final decision layer of an ImageNet classifier with 2048 features can be reduced by two orders of
magnitude, i.e., to use only 20 features per class, at the cost of only 2% test 
accuracy loss.</p>

<p>In the following demonstration, one can move the slider to the right to increase the density of the final decision layer of a standard ImageNet classifier. And, indeed, with only 2% of weights being non-zero, the model can already essentially match the performance (74%) of a fully dense layer.</p>

<div>
    <div id="reg_acc">
        <img id="reg" src="https://gradientscience.org/feed.xml"/>
        <div id="reg_slider"/>
        <div class="quarterblock"> </div>
        <div class="quarterblock" style="text-align: center;">Accuracy: <span id="reg_accuracy"/>%</div>
        <div class="quarterblock" style="text-align: center;">Non-zero: <span id="reg_sparsity"/>%</div>
        <div class="quarterblock"> </div>
    </div>
</div>
<div class="footnote">
    <b>Sparsity-accuracy trade-off:</b> A visualization of the sparsity of an ImageNet decision layer and its corresponding accuracy as a function of the regularization strength. Move the slider all the way to the right to get the fully dense layer (no regularization, 74% accuracy), or all the way to the left to get the fully sparse layer (maximum regularization, 5% accuracy). 
</div>

<h3 id="a-closer-look-at-sparse-decision-layers">A closer look at sparse decision layers</h3>

<p>Our key motivation for constructing sparse decision layers was that it enables us to manually examine the (reduced set of) features that a network uses. As we saw above, our modified decision layers rely on substantially fewer features per class—which already significantly aids their inspection by a human. But what if we go one step further and look only at the “important” features of our sparse decision layer, as we tried to do with the dense decision layer earlier?</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc ablation_button" id="ablation_dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc ablation_button" id="ablation_sparse">Sparse</div>
        </div>
    </div>
    <div class="ablation">
        <canvas height="200" id="ablation_chart" width="400"/>
    </div>
</div>
<div class="footnote">
    <b>Feature importance in sparse and dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. Try toggling between the two to see the effects of sparsity. 
</div>

<p>As we can see below, for models with a sparse decision layer, the top 5-10 important features are necessary and almost sufficient for capturing the model’s performance. That is, (i) accuracy drops to near chance levels (1/number of classes) if the model does not leverage these features and (ii) using these features alone, the model can nearly recover its original performance. This indicates that the sparsity constraint not only reduces the number of features used by the model, but also makes it easier to rank features based on their importance.</p>

<h3 id="sparse-decision-layers-an-interactive-demonstration">Sparse decision layers: an interactive demonstration</h3>

<p>In the following interactive demonstration, you can explore a subset of the decision layer of a (robust) ResNet-50 on ImageNet with either a sparse or dense decision layer:</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc glm_button" id="dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc glm_button" id="sparse">Sparse</div>
        </div>
    </div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc class_button" id="576">Gondola</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="415">Bakery</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="292">Tiger</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="537">Dogsled</div>
        </div>
    </div>
    <div class="" id="linear">
        <div class="block sc" id="glm_class_name">Tiger</div>
        
            
            
            
            
            
            
            
            
            
            
        
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
    </div>
    <div id="zoom">
        <img id="feature_big" src="https://gradientscience.org/feed.xml"/>
        
    </div>
</div>
<div class="footnote">
    <b>An interactive demo of the sparse decision layer:</b> Select a dense or sparse model and a corresponding ImageNet class to visualize the features and weights for the corresponding decision layer. The opacity of each features corresponds to the magnitude of its weight in the decision layer, and you can click on a feature to see a larger version of it. 
</div>

<p>Finally, one should note that the features used by sparse decision layers seem somewhat more human-aligned than the ones used by the standard (dense) decision layers. This observation coupled with our previous ablations studies indicate that sparse decision layers could offer a path towards more debuggable deep networks. But, is this really the case? In our <a href="https://gradientscience.org/debugging">next post</a>, we will evaluate whether models obtained via our methodology are indeed easier for humans to understand, and whether they truly aid the diagnosis of unexpected model behaviors.</p></div>
    </summary>
    <updated>2021-05-12T00:00:00Z</updated>
    <published>2021-05-12T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2021-05-14T23:02:59Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/debugging/</id>
    <link href="https://gradientscience.org/debugging/" rel="alternate" type="text/html"/>
    <title>Debuggable Deep Networks: Usage and Evaluation (Part 2)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><!-- <script src="//code.jquery.com/jquery-1.10.2.js"></script> -->


<!-- <script src="https://code.jquery.com/jquery-1.12.4.js"></script> -->
<!-- <script src=https://code.jquery.com/ui/1.12.1/jquery-ui.min.js></script> -->










<!-- fancybox -->




<p><a class="bbutton" href="https://arxiv.org/abs/2105.04857" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/DebuggableDeepNetworks" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<!-- <a class="bbutton" href="/breeds_class_hierarchy">
<i class="fa fa-tree"></i>
&nbsp;&nbsp; Hierarchies
</a> -->
<br/></p>

<p><i>This is the second part of the overview of our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on training more debuggable deep networks. In our <a href="https://gradientscience.org/glm_saga">previous post</a>, we outlined our toolkit for constructing such networks, which involved training (very) sparse linear classifiers on (pre-trained) deep feature embeddings and viewing the network’s decision process as a linear combination of these features. In this post, we will delve deeper into evaluating to what extent these networks are amenable to debugging. Specifically, we want to get a sense of whether humans are able to intuit their behavior and pinpoint their failure modes.</i></p>

<h2 id="do-our-sparse-decision-layers-truly-aid-human-understanding">Do our sparse decision layers truly aid human understanding?</h2>

<p>Although our toolkit enables us to greatly simplify the network’s decision layer (by reducing the number of its weights and thus the features it relies on), it is not immediately obvious whether this will make debugging such models significantly easier.  To properly examine  this, we need to factor humans into the equation. One way to do that is to leverage the notion of <a href="https://arxiv.org/abs/1606.03490">simulatibility</a> used in the context of ML interpretability. According to this notion, an interpretability method is “good” if it can enable a human to reproduce the model’s decision. In our setup, this translates into evaluating how sparsity of the final decision layer influences humans’ ability to predict the model’s classification decision (irrespective of whether this decision is “correct” or not).</p>

<h4 id="the-simulatibility-study">The “simulatibility” study</h4>

<p>One approach to assess simulatibility  would be to ask annotators to guess what the model will label an input (e.g., an image) as, given an interpretation corresponding to that input. However, for non-expert annotators, this might be challenging due to the large number of (often fine-grained) classes that a typical dataset contains. Additionally, human cognitive biases may also muddle the evaluation—e.g., it may be hard for annotators to decouple “what they think the model should label the input as” from “what the interpretation suggests the model actually does” (and we are interested in the latter).</p>

<p>To alleviate these difficulties, we resort instead to the following task setup (conducted using an ImageNet-trained ResNet):</p>

<ol>
  <li>We pick a target class at random, and show annotators visualizations of five randomly-selected features used by the sparse decision layer to detect objects of this class, along with their relative weights.</li>
  <li>We present the annotators with three images from the validation set with varying (but still non-trivial) probabilities of being classified by the model as the target class. (Note that each of these images can potentially belong to different, non-target classes.)</li>
  <li>Finally, we ask annotators to pick which one among these three images they believe to best match the target class.</li>
</ol>

<div class="footnote">
    As mentioned in <a href="https://gradientscience.org/glm_saga">part 1</a>, feature visualizations for standard vision models are often hard to parse, so we use <a href="https://arxiv.org/abs/1906.00945">adversarially-trained models</a> for this study. 
</div>

<p>Here is a sample task (click to enlarge):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png"><img height="350" src="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png"/></a></p>

<p>Overall, our intention is to gauge whether humans can intuit which image (out of three) is most prototypical for the target class <i>according to the model</i>. Note that we do not show annotators any information about the target class—such as its name or description—other than illustrations of some of the features that the model uses to identify it.  As discussed previously, this is intentional: we want annotators to select the image that <i>visually</i> matches the features used by the model, instead of using their prior knowledge to associate images with the target label itself.  For instance, if the annotators know that the target label was “car”, they might end up choosing the image that most closely resembles their idea of a car—independent of (or even in contradiction to) how the model actually detects cars. In fact, the “most activating image” in our setup may not even belong to the target class.</p>

<p>Now, how well do humans do on this task?</p>

<p>We find that (MTurk) annotators are pretty good at simulating the behavior of our modified networks—they correctly guess the top activating image (out of three) 63% of the time! In contrast, they essentially fail, with only a 35% success rate (i.e., near chance), when this task is performed using models with standard, i.e., dense, decision layers. This suggests that even with a very simple setup—showing non-experts some of the features the sparse decision layer uses to recognize a target class—humans are actually able to emulate the behavior of our modified networks.</p>

<h2 id="debuggability-via-sparsity">Debuggability via Sparsity</h2>

<p>So far, we identified a number of advantages of employing sparse decision layers, such as having fewer components to analyze, selected features being more influential, and better human simulatibility. But what unintended model behaviors can we (semi-automatically) identify by just probing such decision layers?</p>

<h3 id="uncovering-spurious-correlations-and-biases">Uncovering (spurious) correlations and biases</h3>

<p>Let’s start with trying to uncover model biases. After all, it is by now evident that deep networks rely on undesired correlations extracted from the training data (e.g. <a href="https://gradientscience.org/background">backgrounds</a>, <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-related keywords</a>). But can we pinpoint this behavior without resorting to a targeted examination?</p>

<h4 id="bias-in-toxic-comment-classification">Bias in toxic comment classification</h4>

<p>In 2019, Jigsaw hosted a <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">competition on Kaggle</a> around creating  toxic comment detection systems. This effort was prompted by that fact that the systems available at the time were found to have incorrectly learned to associate the names of frequently attacked identities (e.g., nationality, religion or sexual identity) with toxicity, and so the goal of the competition was to construct a
“debiased” system. Can we understand to what extent this effort succeeded?</p>

<p>To answer this question we leverage our methodology and fit a sparse decision layer to the debiased model released by the contest organizers, and then inspect the utilized deep features. An example result is shown below:</p>

<p><img alt="Wordcloud visualization of feature used in unbiased BERT" src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_jigsaw-alt-toxic_6_redacted.png"/></p>
<div class="footnote">
    <b>Interpreting the deep features of a debiased sentiment classifier:</b>
    A word cloud visualization (with some of the words redacted) for a deep feature of the debiased model (with a sparse decision layer). The negative activation of this feature turns out to be influenced by Christianity-related words. 
</div>

<p>Looking at this visualization, we can observe that the debiased model no longer positively associates identity terms with toxicity (refer our <a href="https://arxiv.org/abs/2105.04857">paper</a> for a similar visualization corresponding to the original biased model). This seems to be a success—after all, the goal of the competition was to correct the over-sensitivity of prior models to identity-group keywords. However, upon closer inspection, one will note that the model has actually learned a strong, <i>negative</i> association between these keywords and comment toxicity. For example, one can take a word such as “christianity” and append it to toxic sentences to trick the model into thinking that these are non-toxic 74% of the time. One can try it by selecting words to add to the sentence below:</p>

<div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc toxic_button" id="toxic_" value="">None</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" id="toxic_christianity" value="christianity">+christianity</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" id="toxic_African" value="African">+African</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" id="toxic_Catholic" value="Catholic">+Catholic</div>
        </div>
    </div>
    <div id="toxic_confidence">
        <b>Sentence:</b> Jeez Ed, you seem like a ******* ****** ********* <span id="toxic_add"/>
        <canvas height="200" id="toxic" width="400"/>
    </div>
</div>
<div class="footnote">
    <b>Bias detection in language models: </b> using sparse decision layers we find that the debiased model is still oversensitive to keywords corresponding to frequently attacked identity group, although in the opposite sense from the previous model.
</div>

<p>So, what we see is that rather than being debiased, newer toxic comment detection systems remain disproportionately sensitive to identity terms—it is just the nature of the sensitivity that changed.</p>

<h4 id="spurious-correlations-in-imagenet">Spurious correlations in ImageNet</h4>

<p>In the NLP setting, we can directly measure correlations between the model’s predictions and input data patterns by toggling specific words or phrases in the input corpus. However, it is not obvious how to replicate such analysis in the vision setting. After all, we don’t have automated tools to decompose images into a set of human understandable patterns akin to words or phrases (e.g., “dog ears” or “wheels”).</p>

<p>We thus leverage instead a human-in-the-loop approach that uses (sparse) decision layer inspection as a primitive. Specifically, we enlist annotators on MTurk to identify and describe data patterns that activate individual features that the sparse decision layer uses (for a given class). This in turn allows us to pinpoint the correlations the model has learned between the input data and that class.</p>

<p>Concretely, to identify the data patterns that are positively correlated with a particular (deep) feature, we present to MTurk annotators a set of images that strongly activate it. The expectation here is that if a set of images activate a given  feature, these images should share a common input pattern and the annotators will be able to identify it.</p>

<div class="footnote">
Note that we show annotators images from multiple (two) classes that strongly activate a single feature. This is because images from any single class may have many input patterns in common—only some of which actually activate a specific feature. 
</div>

<p>We then ask annotators: (a) whether they see a common pattern in the images, and, if so, (b) to provide a free text description of that pattern. If the annotators identify a common input pattern, we also ask them if the identified pattern belongs to the class object (“spurious”) or its surroundings (“non-spurious”) for each of the two classes.</p>

<div class="footnote">
In general, we recognize that precisely defining spurious correlations might be challenging and context-dependent. Our definition of spurious correlations was chosen to be objective and easy for annotators to assess.
</div>

<p>Here is an example of the annotation task (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png"><img height="350" src="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png"/></a></p>

<p>Here are a few examples of (spurious) correlations identified by annotators:</p>

<div class="widget">
<span class="widgetheading" id="spurious">Select a class pair:</span>
<div class="choices_one_diff" id="sp"/>
<div class="choices_one_half" id="spuriousimages"> </div>
<div class="choices_one_quarter" id="wcimage"> </div>
<!-- <div class="choices_info">
    <div class="choices_info_text" id="spuriousinfo"> </div>
  </div> -->
</div>
<div style="clear: both;"/>
<div class="footnote">
<b>Detecting input-class correlations in vision models: </b> Select a class pair on the top to see the annotator-provided description for the deep feature that is activated by images of these classes (<i>left</i>). The free-text description provided by the annotators is visualized as a wordcloud (<i>right</i>), along with their selections for whether this input pattern is part of the class object ("non-spurious") or its surroundings ("spurious").
</div>

<p>Note that, one can, in principle, use the same human-in-the-loop methodology to identify input correlations extracted by standard deep networks (with dense decision layers). However, since these models rely on a large number of (deep) features to detect objects of every class, this process can quickly become intractable (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for details).</p>

<p>The above studies demonstrate that for typical vision and NLP tasks, sparsity in the decision layer makes it easier to look deeper into the model and understand what patterns it has extracted from its training corpus.</p>

<h3 id="creating-effective-counterfactuals">Creating effective counterfactuals</h3>

<p>Our second approach for characterizing model failure modes uses the lens of counterfactuals. We specifically focus on counterfactuals that are (minor) variations of given inputs that prompt the model to make a different prediction. Counterfactuals can be very helpful from a debugging standpoint—they can confirm that specific input patterns are not just correlated with the model prediction but actually causally influence them. Additionally, such counterfactuals can be used to provide recourse to users—e.g., to let them realize what attributes (e.g., credit rating) they should change to get the desired outcome (e.g., granting a loan). We will now discuss how to leverage the correlations identified in the previous section to construct counterfactuals for models with sparse decision layers.</p>

<h4 id="language-counterfactuals-in-sentiment-classification">Language counterfactuals in sentiment classification</h4>

<p>In sentiment classification, the task is to label a given sentence as having either positive or negative sentiment. Here, we consider counterfactuals via word substitution, effectively asking “what word could I have used instead to change the sentiment predicted by the model for a given sentence?”</p>

<p>To this end, we consider the words that are positively and negatively correlated with features used by the sparse decision layer as candidates for word substitution. For example, the word “astounding” activates a feature that a BERT model uses to detect positive sentiment, whereas the word “condescending” is negatively correlated with the activation of this feature. By substituting such a positively-correlated word with its negatively-correlated counterpart, we can effectively “flip” the corresponding feature. A demonstration of this process is shown below:</p>

<div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th class="reg_header" colspan="3">Positive activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">impressed</td><td class="positive_cell">brings</td><td class="positive_cell">marvel</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">exhilirated</td><td class="positive_cell main_cell rbutton cf_button">astounding</td><td class="positive_cell">completes</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">hilariously</td><td class="positive_cell">successfully</td><td class="positive_cell">yes</td>
            </tr>
        </tbody></table>
    </div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th class="reg_header" colspan="3">Negative activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">idiots</td><td class="negative_cell rbutton cf_button">inconsistent</td><td class="negative_cell rbutton cf_button">maddening</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">cheat</td><td class="negative_cell rbutton cf_button">condescending</td><td class="negative_cell rbutton cf_button">failure</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">dahmer</td><td class="negative_cell rbutton cf_button">pointless</td><td class="negative_cell rbutton cf_button">unseemly</td>
            </tr>
        </tbody></table>
    </div>
    <div id="sst_counterfactual">
        <b>Sentence:</b> The acting, costumes, music, cinematography and sound are all <i>[<span id="word_counterfactual">astounding</span>]</i> given the proudction's austere locales.
        <canvas height="200" id="sst_canvas" width="400"/>
    </div>
</div>
<div class="footnote">
<b>Language counterfactuals:</b> A wordcloud visualization for a deep feature (used by the sparse decision layer) that positively activates for the  sentence shown above. By replacing the specific word that activated this feature (in this case "astounding"), with any word that  deactivates it (<i>select on the right</i>), we can effectively flip the sentiment predicted by the model. In this way, we can construct counterfactuals for our modified deep networks via one-word substitutions.
</div>

<p>It turns out that these one-word modifications are indeed already quite successful (i.e., they cause a change in the model’s prediction 73% of the time). The obtained sentence pairs—which can be viewed as counterfactuals for one another—allow us to gain insight into data patterns that cause the model to predict a certain outcome. Finally, we find that for standard models finding effective counterfactuals that flip the model’s prediction is harder—the one-word modifications described above can  only change the model’s decision in 52% of cases.</p>

<h4 id="imagenet-counterfactuals">ImageNet counterfactuals</h4>
<p>For ImageNet-trained models, we can directly use the patterns <a href="https://gradientscience.org/feed.xml#spurious-correlations-in-imagenet">previously</a> identified by the annotators to generate counterfactual images that change its prediction. To this end, we manually modify images to add or subtract these patterns and observe the effect of this operation on the model’s decision.</p>

<p>For example, annotators identify a background feature “chainlink fence” to be spuriously
correlated with “ballplayers”. Using this information, we can then take images
of people playing basketball or tennis (correctly labeled as “basketball” or
“racket” by the model) and manually insert a “chainlink fence” into the
background, which successfully changes the model’s prediction to “ballplayer”.</p>

<p><img alt="ImageNet counterfactuals" src="https://gradientscience.org/assets/glm_saga/figures/counterfactuals.png"/></p>
<div class="footnote">
<b>Counterfactuals for ImageNet classifiers:</b> By adding specific spurious patterns to correctly-classified images (<i>top</i>), we can fool the model into predicting the desired class (<i>bottom</i>). 
</div>

<p>Thus, the counterfactuals that our methodology produced indeed allow us to identify data patterns that are causally linked to the model’s decision making process.</p>

<h3 id="identifying-reasons-for-misclassification">Identifying reasons for misclassification</h3>
<p>Finally, we turn our attention to debugging model errors. After all, when our models are wrong, it would be helpful to know why this was the case.</p>

<p>In the ImageNet setting, we find that many (over 30%) of the misclassifications of the 
sparse-decision-layer models can be attributed to a single “problematic”
feature. That is, manually removing this feature results in a correct prediction. One can thus view the feature interpretation for this problematic feature as a justification for the model’s error.</p>

<p><img alt="Problematic features" src="https://gradientscience.org/assets/glm_saga/figures/problematic.png"/></p>
<div class="footnote">
<b>A closer look at ImageNet misclassifications:</b> Examples of erroneously classified ImageNet images (<i>top</i>), along with the feature visualization for the "problematic feature" from the incorrect class (<i>bottom</i>). We find that manually setting the activation of this problematic feature to zero is sufficient to fix the model's mistake in each of these cases.
</div>

<p>Ideally, given such a justification, we would like humans to be able to identify the part of the image corresponding to the problematic feature that caused the model to make a mistake. How can we evaluate whether this is the case?
Namely, can we obtain an unbiased assessment of whether the data patterns that activate the problematic feature are noticeably present in the misclassified image?</p>

<p>To answer this question, we conduct a study on MTurk wherein we present annotators with an image, along with feature visualizations for: (i) the most activated feature from the true class and (ii) the problematic feature that is activated for the erroneous class. We do not explicitly tell the annotators what classes these features correspond to. We then ask annotators to select the patterns (feature visualizations) that match the image, and to determine which pattern is a better match if they select both.</p>

<p>Here is an example of a task we present to the annotators (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png"><img height="350" src="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png"/></a></p>

<div class="footnote">
As a control, we also rerun this experiment while replacing the problematic feature with a randomly-chosen feature. This serves as a baseline to compare annotator selection for the features from the true/incorrect classes. 
</div>

<p>It turns out that not only do annotators frequently (70% of the time) identify the top feature from the wrongly-predicted class as present in the image, but also that this feature is actually a better match than the top feature for the ground truth class (60% of the time). In contrast, annotators select the control (randomly-chosen) deep feature to be a match for the image only 16% of the time. One can explore some examples here:</p>

<div class="widget">
<span class="widgetheading" id="misclass">Inspect misclassified images:</span>
<div class="choices_one_full" id="mis"/>
  <div class="blocktxt" id="mislabels" style="float: none;"> </div>
  <div id="misimages" style="clear: both;"> </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<b>Misclassifications validated by MTurk annotators: </b> Select an image on the top to see its true and predicted labels, along with the most highly activated deep feature (of those used by the sparse decision layer) for both these classes. In all cases, annotators select the top feature from the (incorrect) predicted class to be present in the image, and to be a better match than the top feature from the true class.
</div>

<p>This experiment validates (devoid of confirmation biases from the class label) that humans can identify the data patterns that trigger the error-inducing problematic deep features. Note that once these patterns have been identified, one can examine them to better understand the root cause (e.g., issues with the training data) for model errors.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Over the course of this two-part series, we have shown that a natural approach of fitting sparse linear models over deep feature representations can already be surprisingly effective in creating more debuggable deep networks. In particular, we saw that models constructed using this methodology are more concise and amenable to human understanding—making it easier to detect and analyze unintended behaviors such as biases and misclassification. Going forward, this methodology of modifying the network architecture to make it inherently easier to probe can offer an attractive alternative to the existing paradigm of purely post-hoc debugging. Additionally, our analysis introduces a suite of human-in-the-loop techniques for model debugging at scale and thus can help guide further work in this field.</p>







<span class="choices_info_text"/><br/><span class="choices_info_text" style="color: red;"><b/></span><br/><span class="choices_info_text" style="color: green;"><b/></span><br/><hr/><h3 style="text-align: center;"><h3><div class="sp_txt" style="text-align: center; font-weight: 300; margin: 0.75em auto;"/><div class="wc_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;wc_&quot; + pair + &quot;.png"/><hr style="margin: 0.75em auto;"/><div class="sp_txt" style="text-align: center; font-weight: 200;"><span/></div><hr style="margin: 0.3em auto;"/><h3 style="text-align: center;"><h3><div class="sp_txt" style="text-align: center;"><span style="font-weight: 200;"/></div><br/><span/></h3></h3></div><div class="sp_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;sample_&quot; + pair + &quot;_&quot; + i + &quot;.png"/></div><div class="mis_txt blocktxt thirdblock"><span class="widgetheading"/><span class="choices_info_text"/><br/><span class="choices_info_text"/><div class="mis_txt blocktxt thirdblock"><br/><span class="widgetheading"/></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + origSrc + &quot;"/></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + base +                     &quot;dst_&quot; + pair + &quot;_&quot; + i + &quot;.png"/></div></div></h3></h3></div>
    </summary>
    <updated>2021-05-12T00:00:00Z</updated>
    <published>2021-05-12T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2021-05-14T23:03:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8117</id>
    <link href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/" rel="alternate" type="text/html"/>
    <title>STOC Test of time award</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A reminder: the deadline to submit nominations for the STOC Test of Time Award is May 24. You can nominate papers for the 10 year award – STOC 2007-2011 20 year award – STOC 1997-2001 30 year award – STOC 1987-1991 The award website ( https://sigact.org/prizes/stoc_tot.html ) helpfully contains links to the papers published in … <a class="more-link" href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/">Continue reading <span class="screen-reader-text">STOC Test of time award</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A reminder: the deadline to submit nominations for the <a href="https://sigact.org/prizes/stoc_tot.html">STOC Test of Time Award</a> is <strong>May 24</strong>.  You can nominate papers for the </p>



<ul><li>10 year award – STOC 2007-2011</li><li>20 year award – STOC 1997-2001</li><li>30 year award – STOC 1987-1991<br/><br/>The award website ( <a href="https://sigact.org/prizes/stoc_tot.html">https://sigact.org/prizes/stoc_tot.html </a>) helpfully contains links to the papers published in all these conferences. <br/><br/>Please nominate the papers you think have most influenced our field!</li></ul>



<p/></div>
    </content>
    <updated>2021-05-11T18:28:47Z</updated>
    <published>2021-05-11T18:28:47Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-05-14T23:38:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/" rel="alternate" type="text/html"/>
    <title>Two PhD/Postdoc Positions in Algorithms and Complexity Theory at Goethe-University of Frankfurt (apply by June 15, 2021)</title>
    <summary>The research group conducts research on fundamental questions of computation and information, is driven by curiosity, and provides a friendly, open-minded, and positive social environment. Potential research topics include algebraic graph algorithms, the theory of machine learning on graphs, circuit complexity, pseudorandomness, fine-grained and parameterized complexity. Includes some teaching. Website: https://tcs.uni-frankfurt.de/positions/ Email: tcs-applications@dlist.uni-frankfurt.de</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The research group conducts research on fundamental questions of computation and information, is driven by curiosity, and provides a friendly, open-minded, and positive social environment. Potential research topics include algebraic graph algorithms, the theory of machine learning on graphs, circuit complexity, pseudorandomness, fine-grained and parameterized complexity. Includes some teaching.</p>
<p>Website: <a href="https://tcs.uni-frankfurt.de/positions/">https://tcs.uni-frankfurt.de/positions/</a><br/>
Email: tcs-applications@dlist.uni-frankfurt.de</p></div>
    </content>
    <updated>2021-05-10T15:29:28Z</updated>
    <published>2021-05-10T15:29:28Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-05-14T23:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5486</id>
    <link href="https://www.scottaaronson.com/blog/?p=5486" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5486#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5486" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Three updates</title>
    <summary xml:lang="en-US">For those who read my reply to Richard Borcherds on “teapot supremacy”: seeking better data, I ordered a dozen terra cotta flowerpots, and smashed eight of them on my driveway with my 4-year-old son, dropping each one from approximately 2 meters. For each flowerpot, we counted how many pieces it broke into, seeking insight about […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><ol><li>For those who read my <a href="https://www.scottaaronson.com/blog/?p=5460">reply to Richard Borcherds on “teapot supremacy”</a>: seeking better data, I ordered a dozen terra cotta flowerpots, and smashed eight of them on my driveway with my 4-year-old son, dropping each one from approximately 2 meters.  For each flowerpot, we counted how many pieces it broke into, seeking insight about the distribution over that number.  Unfortunately, it <em>still</em> proved nearly impossible to get good data, for a reason commenters had already warned me about: namely, there were typically 5-10 largeish shards, followed by “long tail” of smaller and smaller shards (eventually, just terra cotta specks), with no obvious place to draw the line and stop counting.  Nevertheless, when I attempted to count only the shards that were “fingernail-length or larger,” here’s what I got: 1 pot with 9 shards, 1 with 11 shards, 2 with 13 shards, 2 with 15 shards, 1 with 17 shards, 1 with 19 shards.  This is a beautiful (too beautiful?) symmetric distribution centered around a mean of 14 shards, although it’s anyone’s guess whether it approximates a Gaussian or something else.  I have <em>no idea</em> why every pot broke into an odd number of shards, unless of course it was a 1-in-256-level fluke, or some cognitive bias that made me preferentially stop counting the shards at odd numbers.<br/></li><li>Thanks so much to everyone who congratulated me for the <a href="https://www.scottaaronson.com/blog/?p=5448">ACM Prize</a>, and especially those who (per my request) suggested charities to which to give bits of the proceeds!  Tonight, after going through the complete list of suggestions, I made my first, but far from last, round of donations: $1000 each to the <a href="https://www.evidenceaction.org/dewormtheworld/">Deworm the World Initiative</a>, <a href="https://www.givedirectly.org/?gclid=CjwKCAjwkN6EBhBNEiwADVfya1RLgM2x4aobbEZ9yTMwTgLbgCdW77zHuI1h5avh0ysXmUHvLYw_vxoCWtcQAvD_BwE">GiveDirectly</a>, the <a href="https://support.worldwildlife.org/site/Donation2?df_id=14650&amp;14650.donation=form1&amp;s_src=AWE2010OQ18507A04091RX&amp;gclid=CjwKCAjwkN6EBhBNEiwADVfya2ZHOOTObCbQVxvbv-R-KF6XGSu8klv7OL_F8WJwFaFyCIgaCBIXexoCaeUQAvD_BwE">World Wildlife Fund</a>, the <a href="https://www.nature.org/en-us/">Nature Conservancy</a>, and <a href="https://www.mathcamp.org/">Canada/USA Mathcamp</a> (which had a huge impact on me when I attended it as a 15-year-old).  One constraint, which might never arise in a decade of moral philosophy seminars, ended up being especially important in practice: if the donation form was confusing or buggy, or if it wouldn’t accept my donation without some onerous confirmation step involving a no-longer-in-use cellphone, then I simply moved on to the next charity.<br/></li><li>Bobby Kleinberg asked me to advertise the <a href="https://sigact.org/prizes/stoc_tot.html">call for nominations</a> for the brand-new STOC Test of Time Award.  The nomination deadline is coming up soon: May 24. </li></ol>



<p/></div>
    </content>
    <updated>2021-05-10T05:47:52Z</updated>
    <published>2021-05-10T05:47:52Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-05-10T05:47:52Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8609684815037895352</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8609684815037895352/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8609684815037895352" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8609684815037895352" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/trump-facebook-and-complexityblog.html" rel="alternate" type="text/html"/>
    <title>Trump, Facebook, and ComplexityBlog</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I care about the Facebook decision to ban Trump, but I do not have a strong opinion about it. I have heard arguments on both sides now, from up and down, and still somehow... I don't know how I feel. So instead of posting my opinion I post other opinions and my opinion of them.</p><p>1) Facebook is a private company. If they want to have liberal bias or a free for all or whatever then  it is not the governments place to interfere. If enough people don't like what they see then they will lose customers. The invisible hand of the market will regulate it enough. Libertarians and honest small-gov republicans might believe this. On a personal level, I don't want someone else telling Lance and I that we can't block some comment; however, for now, more people use Facebook then read Complexity Blog. </p><p>2) Facebook is a private company but they need to follow standard business practices of having their uses sign an agreement and stick to it. Since the user signed the agreement, Facebook need only stick to that agreement. This is problematic in that (1) if the agreement is not that rigorous then Facebook can be arbitrary and capricious, but (2) if the agreement is to rigorous then people can game the system. Imagine if Lance and me had  rule that you could not use profanity in the comments. Then someone could comment </p><p><i>People who think P vs NP is ind of ZFC can go Fortnow themselves. They are so full of</i> <i>Gasarch</i>.</p><p> (Something like this was the subplot of an episode of <i>The Good Fight</i>)</p><p>3) Facebook is so big that it has an obligation to let many voices be heard, within reason. This could lead to contradictions and confusions:</p><p>a) Facebook cannot ban political actors. What is a political actor? (Jon Voight is pro-trump and Dwayne ``The Rock'' Johnson is anti-trump, but that's not what I mean.) High level people in the two main parties qualify (how high level?). IMHO third parties (Libertarian and Green come to mind) need the most protection since they don't have as many other ways to get out their message and they are serious. (I wonder if Libertarians would object to the Government  forcing Facebook to not ban them). What about the <a href="https://en.wikipedia.org/wiki/Gracie_Allen#Publicity_stunts">Surprise Party</a> or the <a href="https://en.wikipedia.org/wiki/Kanye_West#2020_presidential_campaign">Birthday Party</a> (which did have a platform see <a href="https://kanye2020.country/">here</a>). And what about people running for Mayors of small towns (much easier to do now with Facebook)? Should just running be enough to ban banning? </p><p>b) Facebook can ban posts that are a threat to public health and safety. I am thinking of anti-vaxers and insurrectionists, though I am always wary of making them free speech martyrs. </p><p>c) Fortunately a and b above have never conflicted. But they could. I can imagine a president who has lost an election urging his followers to storm the capitol. Then what should Facebook do?  (ADDED LATER- A commenter points to a case where a and b conflicted that is not the obvious case.) </p><p>4) Facebook is so big that it has an obligation to block posts that put people in danger. This may have some of the same problems as point 3---who decides? </p><p>5)  Facebook is so big and controls so much of the discourse that it should be heavily regulated (perhaps like a utility).  This has some of the same problems as above- who decides how to regulate it and how?</p><p>6) As a country we want to encourage free speech and a diversity of viewpoints. There are times when blocking someone from posting may be <i>better for free speech</i> then letting them talk. When? When that person is advocating nonsensical views that stifle the public discussion. But I am talking about what the country should want. What do they want? What does Facebook want? Does either entity even know what they want? These are all ill defined questions. </p><p>7) Facebook is a monopoly so use Anti-Trust laws on it. Anti-Trust was originally intended to protect the consumer from price-gouging. Since Facebook is free this would require a new interpretation of antitrust. Judicial activism? The Justices solving a problem that the elected branches of government are currently unable to solve? Is that a bad precedent? What does it mean to break up Facebook anyway--- its a network and hence breaking it up probably doesn't make sense (maybe use MaxCut). </p><p>(ADDED LATER- a commenter pointed out that anti-trust is NOT just for consumer protection, but also about market manipulation to kill small innovators.) </p><p>8) Lets say that Facebook and Society and the Government and... whoever, finally agree on some sort of standards. Then we're done! Not so fast. Facebook is so vast that it would be hard to monitor everything. </p><p>9) As a side note- because Facebook and Twitter have banned or tagged some kinds of speech or even some people, there have been some alternative platforms set up. They always claim that they are PRO FREE SPEECH. Do liberals post on those sites? Do those sights  ban anyone? Do they have SOME rules of discourse? I ask non rhetorically. </p><p><br/></p></div>
    </content>
    <updated>2021-05-10T00:08:00Z</updated>
    <published>2021-05-10T00:08:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-14T23:29:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings</id>
    <link href="https://11011110.github.io/blog/2021/05/09/arc-triangle-tilings.html" rel="alternate" type="text/html"/>
    <title>Arc-triangle tilings</title>
    <summary>Every triangle tiles the plane, by 180° rotations around the midpoints of each side; some triangles have other tilings as well. But if we generalize from triangles to arc-triangles (shapes bounded by three circular arcs), it is no longer true that everything tiles. Within any large region of the plane, the lengths of bulging-outward arcs of each radius must be balanced by equal lengths of bulging-inward arcs of each radius, and the only way to achieve this with a single tile shape is to keep that same balance between convex and concave length on each tile. Counting line segments as degenerate cases of circular arcs, this gives us three possibilities:</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Every triangle tiles the plane, by 180° rotations around the midpoints of each side; some triangles have other tilings as well. But if we generalize from triangles to arc-triangles (shapes bounded by three circular arcs), it is no longer true that everything tiles. Within any large region of the plane, the lengths of bulging-outward arcs of each radius must be balanced by equal lengths of bulging-inward arcs of each radius, and the only way to achieve this with a single tile shape is to keep that same balance between convex and concave length on each tile. Counting line segments as degenerate cases of circular arcs, this gives us three possibilities:</p>

<ul>
  <li>
    <p>Ordinary triangles, with three straight sides, which always tile in the ordinary way.</p>

    <p style="text-align: center;"><img alt="Tiling by ordinary triangles" src="https://11011110.github.io/blog/assets/2021/ordinary-triangle-tiling.svg"/></p>
  </li>
  <li>
    <p>Arc-triangles with two congruent curved sides (one bulging out and one in) and one straight side. These always tile, by matching up the curved sides to form strips of triangles bounded by their straight sides. Some of these arc-triangles also have other tilings.</p>

    <p style="text-align: center;"><img alt="Tiling by arc-triangles with two curved sides" src="https://11011110.github.io/blog/assets/2021/wave-triangle-tiling.svg"/></p>
  </li>
  <li>
    <p>Arc-triangles with three sides of the same curvature, the shorter two having equal total length to the longest side. The long side must bulge outwards and the other two sides must bulge inwards. Again, these always tile, although the tiling cannot be edge-to-edge.</p>

    <p style="text-align: center;"><img alt="Tiling by arc-triangles with three curved sides" src="https://11011110.github.io/blog/assets/2021/scale-triangle-tiling.svg"/></p>
  </li>
</ul>

<p>The ordinary triangles tile by translation and rotation, and the three-curved-side arc-triangles tile by translation only, without even needing rotations. However, the two-curved-side triangles generally need reflections for their tilings. If tilings by translation and rotation are desired, then only some of these tile: I think only the ones with angles of \(\pi/3\), \(\pi/2\), or \(2\pi/3\) at the vertex between the two curved sides.</p>

<p style="text-align: center;"><img alt="Tiling by arc-triangles with two curved sides, without reflection" src="https://11011110.github.io/blog/assets/2021/pinwheels.svg"/></p>

<p>A curious property of the arc-triangles that tile is that they all have interior angles summing to \(\pi\), something that is not true of most arc-triangles. On the other hand, it is easy to find arc-triangles with angles summing to \(\pi\) that do not tile, so the angle sum does not completely characterize the tilers among the arc-triangles.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106207851143984141">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-05-09T16:20:00Z</updated>
    <published>2021-05-09T16:20:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-13T01:39:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1512</id>
    <link href="https://ptreview.sublinear.info/?p=1512" rel="alternate" type="text/html"/>
    <title>News for April 2021</title>
    <summary>A somewhat “sublinear” month of April, as far as property testing is concerned, with only one paper. (We may have missed some; if so, please let us know in the comments!) Graph Streaming Lower Bounds for Parameter Estimation and Property Testing via a Streaming XOR Lemma, by Sepehr Assadi and Vishvajeet N (arXiv). This paper […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A somewhat “sublinear” month of April, as far as property testing is concerned, with only one paper.<em> (We may have missed some; if so, please let us know in the comments!)</em></p>



<p><strong>Graph Streaming Lower Bounds for Parameter Estimation and Property Testing via a Streaming XOR Lemma</strong>, by Sepehr Assadi and Vishvajeet N (<a href="https://arxiv.org/abs/2104.04908">arXiv</a>). This paper establishes space vs. pass trade-offs lower bounds for streaming algorithms, for a variety of graph tasks: that is, of the sort “any \(m\)-pass-streaming algorithm for task \(\mathcal{T}\) must use memory at least \(f(m)\).” The tasks considered include graph property estimation (size of the maximum matching, of the max cut, of the  weight of the MST) and property testing for sparse graphs (connectivity, bipartiteness, and cycle-freeness). The authors obtained exponentially improved lower bounds for those, via reductions to a relatively standard problem, (noisy) gap cycle counting, for which they establish their main lower bound. As a key component of their proof, they prove a general direct product result (XOR lemma) for the streaming setting, showing that the advantage for solving the XOR of \(\ell\) copies of a streaming predicate \(f\) decreases exponentially with \(\ell\). </p></div>
    </content>
    <updated>2021-05-08T13:00:22Z</updated>
    <published>2021-05-08T13:00:22Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-05-14T23:04:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/068</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/068" rel="alternate" type="text/html"/>
    <title>TR21-068 |  Quantum Proofs of Proximity | 

	Marcel Dall&amp;#39;Agnol, 

	Tom Gur, 

	Subhayan Roy Moulik, 

	Justin Thaler</title>
    <summary>We initiate the systematic study of QMA algorithms in the setting of property testing, to which we refer as QMA proofs of proximity (QMAPs). These are quantum query algorithms that receive explicit access to a sublinear-size untrusted proof and are required to accept inputs having a property $\Pi$ and reject inputs that are $\varepsilon$-far from $\Pi$, while only probing a minuscule portion of their input.

Our algorithmic results include a general-purpose theorem that enables quantum speedups for testing an expressive class of properties, namely, those that are succinctly decomposable. Furthermore, we show quantum speedups for properties that lie outside of this family, such as graph bipartitneness.

We also investigate the complexity landscape of this model, showing that QMAPs can be exponentially stronger than both classical proofs of proximity and quantum testers. To this end, we extend the methodology of Blais, Brody and Matulef (Computational Complexity, 2012) to prove quantum property testing lower bounds via reductions from communication complexity, thereby resolving a problem raised by Montanaro and de Wolf (Theory of Computing, 2016).</summary>
    <updated>2021-05-08T11:19:08Z</updated>
    <published>2021-05-08T11:19:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-14T23:37:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/067</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/067" rel="alternate" type="text/html"/>
    <title>TR21-067 |  Variety Evasive Subspace Families | 

	Zeyu Guo</title>
    <summary>We introduce the problem of constructing explicit variety evasive subspace families. Given a family $\mathcal{F}$ of subvarieties of a projective or affine space, a collection $\mathcal{H}$ of projective or affine $k$-subspaces is $(\mathcal{F},\epsilon)$-evasive if for every $\mathcal{V}\in\mathcal{F}$, all but at most $\epsilon$-fraction of $W\in\mathcal{H}$ intersect every irreducible component of $\mathcal{V}$ with (at most) the expected dimension. The problem of constructing such an explicit subspace family generalizes both deterministic black-box polynomial identity testing (PIT) and the problem of constructing explicit (weak) lossless rank condensers. 

Using Chow forms, we construct explicit $k$-subspace families of polynomial size that are evasive for all varieties of bounded degree in a projective or affine $n$-space. As one application, we obtain a complete derandomization of Noether's normalization lemma for varieties of bounded degree in a projective or affine $n$-space. In another application, we obtain a simple polynomial-time black-box PIT algorithm for depth-4 arithmetic circuits with bounded top fan-in and bottom fan-in that are not in the Sylvester-Gallai configuration, improving and simplifying a result of Gupta (ECCC TR 14-130).

As a complement of our explicit construction, we prove a lower bound for the size of $k$-subspace families that are evasive for degree-$d$ varieties in a projective $n$-space. When $n-k=\Omega(n)$, the lower bound is superpolynomial unless $d$ is bounded. The proof uses a dimension-counting argument on Chow varieties that parametrize projective subvarieties.</summary>
    <updated>2021-05-08T06:04:55Z</updated>
    <published>2021-05-08T06:04:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-14T23:37:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias</id>
    <link href="https://11011110.github.io/blog/2021/05/07/congratulations-dr-matias.html" rel="alternate" type="text/html"/>
    <title>Congratulations, Dr. Matias!</title>
    <summary>Pedro Ascensao Ferreira Matias, one of the students working with Mike Goodrich in the UC Irvine Center for Algorithms and Theory of Computation, passed his Ph.D. defense today.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://pmatias.com/">Pedro Ascensao Ferreira Matias</a>, one of the students working with Mike Goodrich in the UC Irvine <a href="https://www.ics.uci.edu/~theory/">Center for Algorithms and Theory of Computation</a>, passed his Ph.D. defense today.</p>

<p>Pedro is Portuguese, and came to UCI after a bachelor’s degree from the University of Coimbra in Portugal and a master’s degree from Chalmers University of Technology in Sweden.</p>

<p>The general topic of Pedro’s research is “exact learning”, the inference of structured information from queries or other smaller pieces of data. I’ve written here before about my work with Matias on <a href="https://11011110.github.io/blog/2019/02/21/mutual-nearest-neighbors.html">nearest-neighbor chains</a> and on <a href="https://11011110.github.io/blog/2019/08/17/footprints-in-snow.html">tracking paths in planar graphs</a>, the problem of placing sensors on a small subset of vertices so that, by detecting the order in which a path reaches each sensor, you can uniquely determine the whole path. His dissertation combines the tracking paths work with a second paper on tracking paths (“How to Catch Marathon Cheaters: New Approximation Algorithms for Tracking Paths”, <a href="https://arxiv.org/abs/2104.12337">arXiv:2104.12337</a>, to appear at WADS 2021), and a paper on reconstructing periodic and near-periodic strings from sublinear numbers of queries (“Adaptive Exact Learning in a Mixed-Up World: Dealing with Periodicity, Errors and Jumbled-Index Queries in String Reconstruction”, <a href="https://arxiv.org/abs/2007.08787">arXiv:2007.08787</a>, in SPIRE 2020). He also has recent papers on reconstructing trees in SPAA 2020 and ESA 2020.</p>

<p>After finishing his doctorate, Pedro’s next position will be working for Facebook.</p>

<p>Congratulations, Pedro!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106196168129163033">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-05-07T14:50:00Z</updated>
    <published>2021-05-07T14:50:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-13T01:39:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/07/faculty-full-professor-at-university-of-bamberg-germany-apply-by-june-18-2021/" rel="alternate" type="text/html"/>
    <title>Faculty Full Professor at University of Bamberg, Germany (apply by June 18, 2021)</title>
    <summary>The Faculty of Information Systems and Applied Computer Sciences invites applications for the position of Full Professor (W3 level) in Algorithms and Complexity Theory with a focus on algorithms and complexity theory for distributed and concurrent software systems as well for the acquisition, processing and visualisation of data in networked systems. Website: https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/ Email: michael.mendler@uni-bamberg.de</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Faculty of Information Systems and Applied Computer Sciences invites applications for the position of Full Professor (W3 level) in Algorithms and Complexity Theory with a focus on algorithms and complexity theory for distributed and concurrent software systems as well for the acquisition, processing and visualisation of data in networked systems.</p>
<p>Website: <a href="https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/">https://www.uni-bamberg.de/abt-personal/stellenausschreibung/professuren/</a><br/>
Email: michael.mendler@uni-bamberg.de</p></div>
    </content>
    <updated>2021-05-07T14:12:35Z</updated>
    <published>2021-05-07T14:12:35Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-05-14T23:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=1659</id>
    <link href="https://toc4fairness.org/fair-clustering-with-probabilistic-group-membership/" rel="alternate" type="text/html"/>
    <title>Fair Clustering with Probabilistic Group Membership</title>
    <summary>This post briefly summarizes a NeurIPS-20 paper, Probabilistic Fair Clustering, which I coauthored with Brian Brubach, Leonidas Tsepenekas, and John P. Dickerson. Clustering is possibly the most fundamental problem of ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post briefly summarizes a NeurIPS-20 paper, <em><a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">Probabilistic Fair Clustering</a></em>, which I coauthored with <a href="https://bbrubach.github.io/">Brian Brubach</a>, Leonidas Tsepenekas, and <a href="http://jpdickerson.com/">John P. Dickerson</a>.<br/><br/>Clustering is possibly the most fundamental problem of unsupervised learning. Like many other paradigms of machine learning, there has been a focus on fair variants of clustering. Perhaps the definition which has received the most attention is the group fairness definition of [1]. The notion is based on disparate impact and simply states that each cluster should contain points belonging to the different demographic groups with “appropriate” proportions. A natural interpretation of appropriate would imply that each demographic group appears in close to population-level proportions in each cluster. More specifically, if we were to endow each point with a color <img alt="h \in {\cal H}" class="latex" src="https://s0.wp.com/latex.php?latex=h+%5Cin+%7B%5Ccal+H%7D&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> to designate its group membership and we were to consider the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>-means clustering objective, then this notion of fair clustering amounts to the following constrained optimization problem:</p>



<p class="has-text-align-center"><img alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq |C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/></p>



<p>Here, <img alt="l_h" class="latex" src="https://s0.wp.com/latex.php?latex=l_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> and <img alt="u_h" class="latex" src="https://s0.wp.com/latex.php?latex=u_h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> are the lower and upper pre-set proportionality bounds for color <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>, <img alt="C_i" class="latex" src="https://s0.wp.com/latex.php?latex=C_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> denotes the points in cluster <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>, and <img alt="C^h_i" class="latex" src="https://s0.wp.com/latex.php?latex=C%5Eh_i&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/> denotes the subset of those points with color <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>. See figure 1 for a comparison between the outputs of color-agnostic and fair clustering.<br/></p>



<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-1735" height="151" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_1.png?resize=800%2C151&amp;ssl=1" width="800"/>Figure 1: The outputs of color-agnostic vs fair clustering. The clusters of the group-fair output have a proportional mixture of both colors whereas the color-agnostic clusters consist of only one color.</figure></div>



<p>If one were to use clustering for market segmentation and targeted advertisement, then the above definition of fair clustering would roughly ensure that each demographic group receives the same exposure to every type of ad. Similarly if we were to cluster news articles and let the source of each article indicate its membership then we could ensure that each cluster has a good mixture of news from different sources [2].</p>



<p>Significant progress has been made in this notion of fair clustering starting from only considering the two color case and under-representation bounds, to the multi-color case with both under- and over-representation bounds [3.4.5]. Scalable methods for larger datasets have also been proposed [6, 7].</p>



<p>Clearly, like the majority of the methods in group-fair supervised learning, it is assumed that the group membership of each point in the dataset is known. This setting conflicts with a common situation in practice where group memberships are either imperfectly known or completely unknown [8,9,10,11,12]. We take the first step in generalizing fair clustering to this setting; specifically, we assume that while we do not know the exact group membership of each point, we instead have a probability distribution over the group memberships. A natural generalization of the previous optimization problem would be the following:</p>



<p class="has-text-align-center"><img alt="\begin{aligned} &amp; \text{min} \sum_{j \in C_i}  \sum_{i \in \lbrack k\rbrack } d(j,\mu_i)^2 \\ &amp; \text{s.t. }\forall i \in S, \forall h \in \mathcal{H}: l_h |C_i| \leq \mathbb{E}|C^h_i| \leq u_h |C_i| \end{aligned} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Ctext%7Bmin%7D+%5Csum_%7Bj+%5Cin+C_i%7D++%5Csum_%7Bi+%5Cin+%5Clbrack+k%5Crbrack+%7D+d%28j%2C%5Cmu_i%29%5E2+%5C%5C+%26+%5Ctext%7Bs.t.+%7D%5Cforall+i+%5Cin+S%2C+%5Cforall+h+%5Cin+%5Cmathcal%7BH%7D%3A+l_h+%7CC_i%7C+%5Cleq+%5Cmathbb%7BE%7D%7CC%5Eh_i%7C+%5Cleq+u_h+%7CC_i%7C+%5Cend%7Baligned%7D+&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/></p>



<p>Where the proportionality constraints were simply changed to hold in expectation instead of deterministically. Clearly, this constraint reduces to the original constraint when the group memberships are completely known. Figure 2 helps visualize how the input to probabilistic fair clustering looks like and the output we expect.</p>



<p><br/></p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" class="wp-image-1737" height="210" src="https://i2.wp.com/toc4fairness.org/wp-content/uploads/2021/05/fig_2.png?resize=800%2C210&amp;ssl=1" width="800"/>Figure 2: In the above example, the given set of points in the top row are blue and red with probability almost 1 whereas the bottom are blue and red with probability around 0.6. To maintain almost equal color proportions in expectation probabilistic fair clustering would yield the given clustering.</figure></div>



<p> </p>



<p>Despite the innocuous modification to the constraint, the problem becomes significantly more difficult. In our <a href="https://papers.nips.cc/paper/2020/file/95f2b84de5660ddf45c8a34933a2e66f-Paper.pdf">paper</a>, we consider the center-based clustering objectives of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>-center, <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>-median, and <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000&amp;s=0&amp;c=20201002"/>-means and produce solutions with approximation ratio guarantees for two given cases:</p>



<ul><li><strong>Two-Color Case</strong>: We see that even the two color case is not easy to handle. The key difficulty lies in the rounding method. However, we give a rounding method that maintains the fairness constraint with a worst-case additive violation of 1 matching the deterministic fair clustering case.</li><li><strong>Multi-Color Case with Large Enough Clusters</strong>: At a high level, if the clusters have a sufficiently large size then through a Chernoff bound we can show that independent sampling would result in a deterministic fair clustering instance which we could solve using deterministic fair clustering algorithms. This essentially forms a reduction from the probabilistic to the deterministic instance.</li></ul>



<p>While our solutions perform well empirically, we are left with a collection of problems. For example, guaranteeing that the color proportions are maintained in expectation is not the best constraint one should hope for, since when the colors are realized a cluster could entirely consist of one color. A more preferable constraint would instead bound the probability of obtaining an “unfair” clustering. Moreover, a setting that assumes access to the probability distribution for a given point over all colors could still be assuming too much. A more reasonable setting could instead take a robust-optimization-based approach, where we have the distribution of each point but allow the distribution of each point to belong to an uncertainty set. This effectively allows our probabilistic knowledge to be imperfect as well—as could be the case if, for example, a machine learning model were predicting group membership with a systematic bias against a particular subset of colors. Lastly, being able to handle the multi-color case in an assumption-free manner would also be interesting.</p>



<p><strong>References:</strong></p>



<ol><li>Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through fairlets. In Advances in Neural Information Processing Systems, 2017.</li><li>Sara Ahmadian, Alessandro Epasto, Ravi Kumar, and Mohammad Mahdian. Clustering without over-representation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 2019.</li><li>Melanie Schmidt, Chris Schwiegelshohn, and Christian Sohler. Fair coresets and streaming algorithms for fair k-means. In the International Workshop on Approximation and Online Algorithms, 2019.</li><li>Ioana O. Bercea, Martin Groß, Samir Khuller, <em>Aounon Kumar</em>, Clemens Rösner, Daniel R. Schmidt, Melanie Schmidt. On the cost of essentially fair clusterings, In the International Conference on Approximation Algorithms for Combinatorial Optimization Problems 2019.</li><li>Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fair algorithms for clustering. In Advances in Neural Information Processing Systems, 2019.</li><li>Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. Scalable fair clustering. In the International Conference on Machine Learning, 2019.</li><li>Lingxiao Huang, Shaofeng Jiang, and Nisheeth Vishnoi. Coresets for clustering with fairness constraints. In Advances in Neural Information Processing Systems, 2019.</li><li>Pranjal Awasthi, Matth¨aus Kleindessner, and Jamie Morgenstern. Equalized odds postprocessing under imperfect group information. In the International Conference on Artificial Intelligence and Statistics, 2020.</li><li>Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, NithumThain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. In Advances in Neural InformationProcessing Systems, 2020.</li><li>David Pujol, Ryan McKenna, Satya Kuppam, Michael Hay, AshwinMachanavajjhala, and Gerome Miklau. Fair decision making using privacy-protected data. In Proceedings of the Conference on Fairness, Accountability, and Transparency, 2020.</li><li>Hussein Mozannar, Mesrob Ohannessian, and Nathan Srebro. Fair learning with private demographic data. In the International Conference on Machine Learning, 2020.</li><li>Nathan Kallus, Xiaojie Mao, and Angela Zhou. Assessing algorithmic fairness with unobserved protected class using data combination. Management Science, 2021.</li></ol>



<p/></div>
    </content>
    <updated>2021-05-07T14:09:42Z</updated>
    <published>2021-05-07T14:09:42Z</published>
    <category term="Blog"/>
    <author>
      <name>seyed2357</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i1.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2021-05-14T23:39:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/07/ph-d-student-at-idsia-usi-supsi-lugano-switzerland-apply-by-june-30-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/07/ph-d-student-at-idsia-usi-supsi-lugano-switzerland-apply-by-june-30-2021/" rel="alternate" type="text/html"/>
    <title>Ph.D. Student at IDSIA, USI-SUPSI, Lugano, Switzerland (apply by June 30, 2021)</title>
    <summary>IDSIA opens two 4-year Ph.D. positions, starting on November 2021, in the area of algorithms and complexity, with a focus on approximation algorithms. The gross year salary is around 50K CHF. Candidates should hold a Master Degree in Computer Science or related areas. The interested candidates should email Prof. Fabrizio Grandoni a detailed CV and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>IDSIA opens two 4-year Ph.D. positions, starting on November 2021, in the area of algorithms and complexity, with a focus on approximation algorithms.<br/>
The gross year salary is around 50K CHF. Candidates should hold a Master Degree in Computer Science or related areas.<br/>
The interested candidates should email Prof. Fabrizio Grandoni a detailed CV and contact details of 2-3 references.</p>
<p>Website: <a href="https://people.idsia.ch//~grandoni/">https://people.idsia.ch//~grandoni/</a><br/>
Email: fabrizio@idsia.ch</p></div>
    </content>
    <updated>2021-05-07T09:47:11Z</updated>
    <published>2021-05-07T09:47:11Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-05-14T23:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21716</id>
    <link href="https://gilkalai.wordpress.com/2021/05/06/alef-corner-icm2022/" rel="alternate" type="text/html"/>
    <title>Alef Corner: ICM2022</title>
    <summary>Alef’s new piece for ICM 2022 will surely cheer you up!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><img alt="icm2022" class="alignnone size-full wp-image-21717" height="2100" src="https://gilkalai.files.wordpress.com/2021/05/icm2022.jpg" width="2100"/><strong><span style="color: #ff0000;">Alef’s new piece for ICM 2022 will surely cheer you up!</span> </strong></p>


<p/></div>
    </content>
    <updated>2021-05-06T19:45:22Z</updated>
    <published>2021-05-06T19:45:22Z</published>
    <category term="Art"/>
    <category term="Combinatorics"/>
    <category term="Geometry"/>
    <category term="ICM2022"/>
    <category term="Alef's corner"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-05-14T23:37:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.let-all.com/blog/?p=51</id>
    <link href="https://www.let-all.com/blog/2021/05/06/alt-highlights-a-report-on-the-first-alt-mentoring-workshop/" rel="alternate" type="text/html"/>
    <title>ALT Highlights – A Report on the First ALT Mentoring Workshop</title>
    <summary>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference ALT 2021, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/">Learning Theory Alliance</a>, and overseen by <a href="http://www.gautamkamath.com/">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">Learning Theory Alliance blog</a>.</p>



<p>This is the fifth post in the series, coverage of the first <a href="https://www.let-all.com/alt.html">ALT Mentoring Workshop</a> organized by the Learning Theory Alliance, written by <a href="https://www.let-all.com/blog/feed/knaggita@ttic.edu">Keziah Naggita</a> and <a href="https://www.comp.nus.edu.sg/~sutanu/">Sutanu Gayen</a>.</p>



<hr class="wp-block-separator"/>



<h2><strong>1 Introduction</strong></h2>



<p class="has-text-align-left has-text-align-justify">The Learning Theory Alliance (Let-All) is an online initiative aimed at developing a supportive learning theory community, founded by (1) <a href="https://www.cs.utexas.edu/~surbhi/" rel="noopener noreferrer" target="_blank">Surbhi Goel</a>, a postdoctoral researcher at Microsoft Research New York, (2) <a href="https://people.eecs.berkeley.edu/~nika/" rel="noopener noreferrer" target="_blank">Nika Haghtalab</a>, an assistant professor at UC Berkeley EECS, (3) and <a href="https://vitercik.github.io" rel="noopener noreferrer" target="_blank">Ellen Vitercik</a>, a Ph.D. Student at CMU; and advised by <a href="https://www.stat.berkeley.edu/~bartlett/" rel="noopener noreferrer" target="_blank">Peter Bartlett</a>, <a href="https://home.ttic.edu/~avrim/" rel="noopener noreferrer" target="_blank">Avrim Blum</a>, <a href="https://people.csail.mit.edu/stefje/" rel="noopener noreferrer" target="_blank">Stefanie Jegelka</a>, <a href="https://www.dpmms.cam.ac.uk/%7Epll28/" rel="noopener noreferrer" target="_blank">Po-Ling Loh</a>, and <a href="http://www.jennwv.com" rel="noopener noreferrer" target="_blank">Jenn Wortman Vaughan</a>. The goal of the alliance is to ensure healthy community growth by fostering inclusive community engagement and encouraging active contributions from researchers at all stages of their careers. Let-All’s efforts towards realizing these goals include a series of ongoing and future activities, such as the first ALT mentoring workshop, coordinating the ALT Highlights blog series, and other upcoming community initiatives. This article reports on  Let-All’s <a href="https://let-all.com/alt.html" rel="noopener noreferrer" target="_blank">first Mentoring Workshop</a>, which was affiliated with the 32<sup>nd</sup> International Conference on Algorithmic Learning Theory.</p>



<p class="has-text-align-left has-text-align-justify">The workshop had two main sessions to cater to the time zone differences of the participants.  These sessions had three main components: an academic program, which included how-to-talks, Ask Me Anythings (AMAs), and presentation dissections; a technical program, which included research talks; and a social program, which included discussion tables and other activities.</p>



<p class="has-text-align-left has-text-align-justify">The workshop participants included students, researchers, and industry professionals, all at different levels of familiarity with learning theory. Because of the ongoing COVID-19 pandemic, the workshop was virtual. It was held on the online platforms Zoom and Gather town, a virtual interactive environment that mimics an in-person workshop setting. For accessibility, the workshop organizers opened up the workshop free of cost to all registered participants. </p>



<h2><strong>2 Program Highlights</strong></h2>



<h3><strong>2.1 Academic Program</strong></h3>



<p class="has-text-align-left has-text-align-justify">To kick off the workshop, one of the organizers began with a welcome lecture: Surbhi in session one and Nika in session two.  They read out the code of conduct and who to contact in case of issues, outlined the workshop’s purpose, and gave attendees demographic information. They explained how participants could navigate the workshop-themed Gather town workspace and then ended the introduction with encouragement for participants to mingle. </p>



<p class="has-text-align-left has-text-align-justify">The <strong><em>How-to-Talks</em></strong> sessions covered writing papers, giving talks, and networking. In Session 1, <a href="https://www.cs.cmu.edu/~praveshk/" rel="noopener noreferrer" target="_blank">Pravesh Kothari</a> talked in great detail about the dos and don’ts of what to add in the abstract, overview, introduction, and appendix when advising participants on how to best structure research papers. He told attendees to always put effort into understanding their intended reader or talk audience.  Pravesh encouraged attendees to consider the expertise and interests of the reader or listener to capture their attention since these highly determine the attention span and interest in the information presented to them.  He strongly recommended attendees watch the <em><a href="https://www.youtube.com/watch?v=vtIzMaLkCaM" rel="noopener noreferrer" target="_blank">Leadership Lab: The Craft of Writing Effectively</a></em> by Larry McEnerney , Director of the University of Chicago Writing Program.<em> </em>In session 2 of the workshop, <a href="https://home.cs.colorado.edu/~raf/" rel="noopener noreferrer" target="_blank">Rafael Frongillo</a>, similar to Pravesh, discussed how to capture the intended audience when one writes a paper, reviews, and talks.</p>



<p class="has-text-align-left has-text-align-justify">In the first <strong><em>networking session</em></strong>, <a href="https://www.cc.gatech.edu/~jabernethy9/" rel="noopener noreferrer" target="_blank">Jacob Abernethy</a> encouraged participants to seek out horizontal and vertical networking, for example, through collaborations, talks, and reach outs. He said that currently, in academia, Ph.D. admissions, faculty hiring, and tenure appointments are heavily risk-averse. Therefore, people seek out candidates based on their network. For this reason, it is crucial for students to network from early on in their careers. He gave great examples of how junior researchers can reach out and forge relationships with other researchers. For example, when you meet academics, faculty/postdocs at events, ask to give a talk at their lab. Jacob also candidly talked about his earlier failures at MIT and how they shaped his journey. He talked about luck and how <a href="https://www.microsoft.com/en-us/research/people/jcl/" rel="noopener noreferrer" target="_blank">John Langford</a>, who was at Toyota Technological Institute at Chicago at the time, took a chance on him that forever changed his life. Jacob, therefore, advised academics to take chances on people as this would change the course of the field. </p>



<p class="has-text-align-left has-text-align-justify"><a href="https://jamiemorgenstern.com" rel="noopener noreferrer" target="_blank">Jamie Morgenstern</a> discussed different networking methods in the <strong><em>second How-to-talks session</em></strong>. She emphasized that for junior researchers, it’s important to attend conferences and to network with others, to advertise their research through talks, and to reach out to faculty for collaboration. To introduce oneself and capture the listener’s attention, Jamie said, for conferences, prepare to do so in two minutes, for social four minutes, bar 12 minutes, and faculty interview 25 minutes. Senior grad students may help introduce the juniors during lunch/poster sessions. Finally, when emailing faculty about research, she said one should avoid discussion about other people’s work and instead should stick to the recipient’s work – “showing deep understanding and possibly open questions which might lead to collaboration.” </p>



<p class="has-text-align-left has-text-align-justify">In both workshop sessions, there were<strong><em> two parallel talk dissections, </em></strong>in which senior faculty members gave both positive and constructive feedback on talks junior researchers presented. In the first session, <a href="https://www.cs.cornell.edu/~rdk/" rel="noopener noreferrer" target="_blank">Bobby Kleinberg</a> discussed <a href="https://www.emilyruthdiana.com" rel="noopener noreferrer" target="_blank">Emily Diana</a>‘s talk titled “Minimax and Lexicographically Fair Learning: Algorithms, Experiments, and Generalization”. He highlighted parts that were impressive, those that needed improvement, and gave general advice on structuring an audience-based presentation. When Bobby suggested including more diagrams than text, a few people made suggestions of free tools including tikz, matcha.io, PowerPoint, and draw.io. In parallel, <a href="http://cseweb.ucsd.edu/~kamalika/" rel="noopener noreferrer" target="_blank">Kamalika Chaudhuri</a> dissected a talk on “Efficient, Noise-Tolerant, and Private Learning via Boosting” by <a href="https://marco.ntime.org" rel="noopener noreferrer" target="_blank">Marco Carmosino</a>. Two main takeaways of this talk dissection were the balance of technical and nontechnical content (e.g., explaining ideas with fun pictures, etc.) and having one main and clear idea as the talk’s takeaway. </p>



<p class="has-text-align-left has-text-align-justify">In the second session, <a href="https://sites.google.com/site/acmonsterqiao/" rel="noopener noreferrer" target="_blank">Mingda Qiao</a> gave a talk titled: “Stronger Calibration Lower Bounds via Sidestepping” which <a href="https://praneethnetrapalli.org" rel="noopener noreferrer" target="_blank">Praneeth Netrapalli</a> dissected. Praneeth remarked that theory folks often jump into the problem straight away without covering much background. In conferences, this might be fine due to time pressure and specific interests. However, in broader settings such as departmental seminars, he advised the speaker to allocate more time to introduce the problem lucidly and concisely. In parallel, <a href="https://sites.google.com/site/marywootters/" rel="noopener noreferrer" target="_blank">Mary Wootters</a> dissected a talk titled “List-Decodable Subspace Recovery: Dimension Independent Error in Polynomial Time” that <a href="http://aineshbakshi.com" rel="noopener noreferrer" target="_blank">Ainesh Bakshi</a> presented. </p>



<p class="has-text-align-left has-text-align-justify">In the first <strong><em>AMA session</em></strong> moderated by <a href="https://www.stat.cmu.edu/~aramdas/" rel="noopener noreferrer" target="_blank">Aaditya Ramdas</a>, <a href="https://web.stanford.edu/~lmackey/" rel="noopener noreferrer" target="_blank">Lester Mackey</a> refreshingly answered several of the attendees’ well-curated questions about what makes strong collaborations, how to get into grad school, and whether or not he ever felt like quitting his Ph.D., among others.  He encouraged students to take classes with professors they are interested in as it makes it easy to ask for a mentorship opportunity. Lester talked about collaborations and imposter syndrome and encouraged attendees to look on the brighter side of things, to remember that we all are working towards one big goal, creating positive changes in the world. Therefore if someone discovers a result before us, we should applaud them, collaborate if possible, and move onto new problems. He said he did not necessarily plan to do a Ph.D. but got into it towards the end of his undergraduate degree due to an internship that made him fall in love with doing research. </p>



<p class="has-text-align-left has-text-align-justify">In the evening, there was an AMA session with <a href="http://people.csail.mit.edu/shafi/" rel="noopener noreferrer" target="_blank">Shafi Goldwasser</a> moderated by Nika. Shafi gave thoughtful and candid answers to attendees’ captivating questions about research, life in academia, collaborations, among others. Shafi told attendees that healthy competition, trust, and overlap of research interest, is crucial for successful research in the early stage of the career. She also asserted that fundamental science is always impactful. She mentioned that the high points of her career were working on problems she was curious about: cryptography, pseudo-randomness, and zero-knowledge proofs. Finally, when asked about what advice she wished she had during the early stage of her career, interestingly Shafi replied: “having good colleagues, good friends at work, very important, most important – having a listening, promoting and supportive cohort of friends rather than an individualistic path as a scholar is priceless.” </p>



<h3><strong>2.2 Technical Program</strong></h3>



<p class="has-text-align-left has-text-align-justify"><strong><em>Two research talks</em></strong> happened in the first session. First, Po-Ling Loh gave a talk titled “Mean estimation for entangled single-sample distributions.” Then <a href="https://web.stanford.edu/~vsharan/" rel="noopener noreferrer" target="_blank">Vatsal Sharan</a> talked about “Sample Amplification: Increasing Dataset Size even when Learning is Impossible”.<br/>Similarly in the second session, <a href="https://www.cohennadav.com" rel="noopener noreferrer" target="_blank">Nadav Cohen</a> gave the first talk about tensor and matrix completion problems and the importance of understanding the theory behind deep learning from theoretical and practical perspectives. After, <a href="https://i.cs.hku.hk/~zhiyi/" rel="noopener noreferrer" target="_blank">Zhiyi Huang</a> gave a talk titled “Setting the Sample Complexity of Single-parameter Revenue Maximization.” </p>



<h3><strong>2.3 Social Program</strong></h3>



<p class="has-text-align-left has-text-align-justify">In both sessions, during the social hours, <a href="https://sites.google.com/view/sumegha-garg/home" rel="noopener noreferrer" target="_blank">Sumegha Garg</a>, <a href="http://sgunasekar.github.io" rel="noopener noreferrer" target="_blank">Suriya Gunasekar</a>, and <a href="https://teddlyk.github.io" rel="noopener noreferrer" target="_blank">Thodoris Lykouris</a> organized the <strong><em>table topics</em></strong> to help attendees meet and interact with senior researchers and professors on different topics. The table topics included the following; starting on ML research, research agendas, ML+X: multidisciplinary research, advisor-advisee relationships, collaborators, communicating research and networking, beyond your institution: internships and research visits, planning after grad school: academia versus industry, Grad school applications, Work ethics, and Open research discussion. </p>



<p class="has-text-align-left has-text-align-justify">The table topics were chaired by; Jacob Abernethy, <a href="https://www.shivani-agarwal.net" rel="noopener noreferrer" target="_blank">Shivani Agarwal</a>, <a href="https://ericbalkanski.com" rel="noopener noreferrer" target="_blank">Eric Balkanski</a>, Peter Bartlett, Avrim Blum, <a href="http://sbubeck.com/" rel="noreferrer noopener" target="_blank">Sébastien Bubeck</a>, Kamalika Chaudhuri, Nadav Cohen, Sumegha Garg, Surbhi Goel, Suriya Gunasekar, Nika Haghtalab,  <a href="https://www.cs.columbia.edu/~djhsu/" rel="noopener noreferrer" target="_blank">Daniel Hsu</a>, <a href="https://www.prateekjain.org" rel="noopener noreferrer" target="_blank">Prateek Jain</a>, <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/jordan.html" rel="noopener noreferrer" target="_blank">Mike Jordan</a>, <a href="https://homes.cs.washington.edu/~sham/" rel="noopener noreferrer" target="_blank">Sham Kakade</a>, <a href="https://www.microsoft.com/en-us/research/people/adum/" rel="noopener noreferrer" target="_blank">Adam Kalai</a>, Pravesh Kothari, <a href="https://people.cs.umass.edu/~akshay/" rel="noopener noreferrer" target="_blank">Akshay Krishnamurthy</a>, <a href="https://jerryzli.github.io" rel="noopener noreferrer" target="_blank">Jerry Li</a>, Po-Ling Loh, Thodoris Lykouris, <a href="https://www.tau.ac.il/~mansour/" rel="noopener noreferrer" target="_blank">Yishay Mansour</a>, <a href="https://pasin30055.github.io" rel="noopener noreferrer" target="_blank">Pasin Manurangsi</a>, <a href="https://vmuthukumar.ece.gatech.edu" rel="noopener noreferrer" target="_blank">Vidya Muthukumar</a>, Praneeth Netrapalli, <a href="https://wensun.github.io" rel="noopener noreferrer" target="_blank">Wen Sun</a>, <a href="https://www.bowaggoner.com" rel="noopener noreferrer" target="_blank">Bo Waggoner</a>, <a href="https://mzampet.com" rel="noopener noreferrer" target="_blank">Manolis Zampetakis</a>, and <a href="https://cyrilzhang.com/">Cyril Zhang</a>. </p>



<p class="has-text-align-left has-text-align-justify">Lastly, at the end of the two general research talks in sessions one and two of the workshop, attendees assembled on Gather town to close the workshop. The social event included 1:1 social interactions with other attendees, an attempt at forging relationships, and activities like dancing.</p>



<h2><strong>3 Attendance Statistics, Testimonials and Feedback</strong></h2>



<h3><strong>3.1 Participants Statistics</strong></h3>



<p class="has-text-align-left has-text-align-justify">ALT mentoring workshop welcomed talented academics, researchers, and professionals from a wide array of backgrounds. Of the 438 registered to attend the workshop, 197 were new to the learning theory community, 37 attended at least one ALT/COLT conference in the past, and 146 hadn’t attended ALT/COLT but had attended machine learning conferences (STOC, NeurIPS, etc.) as shown in Figure 1. </p>



<p class="has-text-align-left has-text-align-justify">The workshop participants came from different parts of the world, were of different genders, races, and seniority levels. We use Figures 2, 3b, 4a, and 4b to highlight this demographic information about the participants. Some participants chose session one, and others chose session two, a choice driven by their schedules and time zones. The attendance composition is as shown in figure 3a.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-58" height="310" src="https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.28-AM.png?resize=515%2C310&amp;ssl=1" width="515"/>Figure 1: Registrants familiarity with the community</figure></div>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-61" height="401" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.11-AM-2.png?resize=678%2C401&amp;ssl=1" width="678"/>Figure 2: Career stages of participants</figure></div>



<figure class="wp-block-gallery columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img alt="" class="wp-image-70" height="414" src="https://i2.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.33.54-AM-1.png?resize=678%2C414&amp;ssl=1" width="678"/></figure></li><li class="blocks-gallery-item"><figure><img alt="" class="wp-image-71" height="406" src="https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.35.25-AM-1.png?resize=678%2C406&amp;ssl=1" width="678"/></figure></li></ul>Figure 3: Session preferences (a) and locations of participants (b)</figure>



<figure class="wp-block-gallery aligncenter columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img alt="" class="wp-image-75" height="404" src="https://i2.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.57-AM.png?resize=678%2C404&amp;ssl=1" width="678"/></figure></li><li class="blocks-gallery-item"><figure><img alt="" class="wp-image-77" height="404" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.34.42-AM-1.png?resize=678%2C404&amp;ssl=1" width="678"/></figure></li></ul>Figure 4: Race (a) and Gender (b) distribution of participants</figure>



<p/>



<h3><strong>3.2 Testimonials and Feedback</strong></h3>



<p class="has-text-align-left has-text-align-justify">In this section, we give a recount of testimonials from participants who we interviewed after the workshop. We also highlight some of the common themes in feedback from participants.</p>



<p class="has-text-align-left has-text-align-justify">In general, participants loved the content delivered in the sessions. They said it was informative, intuitive, and rare to find. Several participants loved interacting with peers and senior members and wished they had more time and activities to do it. The How-to-talks session (focused on networking skills, structuring papers, talks, and reviews) was the most popular session among attendees. 76.7% of the survey respondents said the session helped them gain new technical skills or hone existing skills, see opportunities in academia and how to use them, and see barriers in academia and ways to overcome them. Figure 5 highlights attendees’ ratings of the skills acquired from the workshop. </p>



<figure class="wp-block-image size-large is-resized is-style-default"><img alt="" class="wp-image-80" height="404" src="https://i0.wp.com/www.let-all.com/blog/wp-content/uploads/2021/05/Screen-Shot-2021-05-04-at-4.35.11-AM.png?resize=678%2C404&amp;ssl=1" width="678"/>Figure 5: Usefulness ratings of skills gained from the workshop</figure>



<p><strong>Below is a recount of the workshop experiences of the interviewed attendees.</strong></p>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My highlight was the How-to-Talks since they provided a lot of more personal information/inputs that you cannot easily find online and which was very valuable. The event helped me to remember and reflect upon which qualities are crucial to becoming a good researcher. I even made a list in a place that I see every day to keep them in mind.” – <em><a href="https://www.michaelaerni.com" rel="noopener noreferrer" target="_blank">Michael Aerni</a>, MSc student at ETH Zurich.</em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“The workshop highlights for me were the How-to-talks, the AMA session with Lester, and the social tables. The How-to-talks were extremely valuable as they discussed topics such as structuring papers and networking in the community. These are subtle aspects that are not often explicitly talked about in the community. I, therefore, learned a lot from them. The AMA session was refreshingly honest and open. Finally, the social tables were also great as I got to meet and talk to some well-established senior community members like Sebastien Bubeck, Shivani Agarwal, and Akshay Krishnamurthy.” – <em><a href="https://people.eecs.berkeley.edu/~tgautam23/" rel="noopener noreferrer" target="_blank">Tanmay Gautam</a>, a second-year Ph.D. student at UC Berkeley. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“It was awesome to have Lester Mackey answer my questions, indubitably. I learned about staying true to the research questions I genuinely believe in regardless of external opinions and rewards. As an NYU AI School organizer, I can appreciate how much effort went into organizing the workshop. The organizers did a stellar job! I think a version of the same event again would be perfect.” – <em><a href="https://swapneelm.github.io" rel="noopener noreferrer" target="_blank">Swapneel Mehta</a>, a Data Science Ph.D. student at New York University. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My highlights were getting to talk with senior members of the community. These opportunities rarely come by for someone who lives in a foreign country. For a timid person like myself, I am also thankful for the senior members for helping me (and other participants) breaking the ice and easing us into the conversations. Thanks to this event, I am now more confident in engaging with other researchers.”- <em><a href="http://www.donlapark.cmustat.com" rel="noopener noreferrer" target="_blank">Donlapark Ponnoprat</a>, a Statistics lecturer at Chiang Mai University. </em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My main highlights were Dr. Goldwasser’s session with Dr. Haghtalab and the socials. In the socials, I was able to ask professors and senior researchers for advice on varied topics based on the tables. Insights from Dr. Cohen’s lecture on tensor rank and implicit regularisation gave me several pointers to ideas in the literature that I was not aware of as a junior researcher from a slightly different AI specialty. These ideas might be beneficial for my research in the long term. </p><p class="has-text-align-left has-text-align-justify"> I send a sincere thank you to all the organizers. The mentorship workshop was a great event, and it models concrete actions, what it means to foster a welcoming community. It is clear how kind and dedicated folks are here as some researchers even stayed beyond midnight in their time zones to answer questions that attendees had. If an event like this happens again, I am most definitely signing up to come.” – <em><a href="https://github.com/esraa-saleh" rel="noopener noreferrer" target="_blank">Esra’a Saleh</a>, a Masters in Computer Science student at the University of Alberta, affiliated with AMII and RLAI.</em></p></blockquote>



<blockquote class="wp-block-quote"><p class="has-text-align-left has-text-align-justify">“My main takeaway from the event was an inside look at academia. As an undergrad, my only experience in academia has been the little experience I have with my advisory professors. While this is an invaluable experience, this event was nice as it was one of the very few that cater to students, including undergrads, with the intent of bringing them into the academia fold. Getting to know new people and talking to them was extremely interesting, especially during lockdown when connecting with others is a much more valuable commodity.” – <em><a href="https://pages.cs.wisc.edu/~shrey/" rel="noopener noreferrer" target="_blank">Shrey Shah</a>, a penultimate year undergraduate student at the University of Wisconsin-Madison. 
</em></p></blockquote>



<p class="has-text-align-left has-text-align-justify">Several participants enjoyed the workshop sessions and hoped that Let-All holds more similar themed workshops in conferences. Attendees suggested ways for attendees to interact more with each other. Some of the suggestions included the following: ‘beginner-friendly open problems sessions where attendees can collaborate’ – Esra’a Saleh, ‘an icebreaker session at the beginning that encourages attendees to mingle’ – Michael Aerni, and ‘a poster session for participants to present their work’ – Shrey Shah. </p>



<h2><strong>4 Conclusion</strong></h2>



<p class="has-text-align-left has-text-align-justify">The ALT mentorship workshop organized by the Learning Theory Alliance brought together many academics and researchers. It was, and we hope it continues to be, an opportunity for the budding researchers to learn about research and meta-research, forge collaborations, and be inspired. Kudos to the organizers and the Alliance in general for dreaming such a positive vision and then striving to make it a great success! </p>



<p class="has-text-align-left has-text-align-justify"><em>Thanks to <a href="http://www.gautamkamath.com" rel="noreferrer noopener" target="_blank">Gautam Kamath</a>, <a href="https://web.stanford.edu/~mglasgow/" rel="noreferrer noopener" target="_blank">Margalit Glasgow</a>, Surbhi Goel, Nika Haghtalab</em> <em>and Ellen Vitercik for helpful conversations and comments.</em></p></div>
    </content>
    <updated>2021-05-06T15:44:40Z</updated>
    <published>2021-05-06T15:44:40Z</published>
    <category term="ALT Highlights"/>
    <author>
      <name>Keziah</name>
    </author>
    <source>
      <id>https://www.let-all.com/blog</id>
      <logo>https://i1.wp.com/www.let-all.com/blog/wp-content/uploads/2021/04/logo.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://www.let-all.com/blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://www.let-all.com/blog" rel="alternate" type="text/html"/>
      <title>The Learning Theory Alliance Blog</title>
      <updated>2021-05-14T23:39:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-21129445.post-4927313116100787313</id>
    <link href="http://mysliceofpizza.blogspot.com/feeds/4927313116100787313/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=21129445&amp;postID=4927313116100787313" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/21129445/posts/default/4927313116100787313" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/21129445/posts/default/4927313116100787313" rel="self" type="application/atom+xml"/>
    <link href="http://mysliceofpizza.blogspot.com/2021/05/scaling-research-training.html" rel="alternate" type="text/html"/>
    <title>Scaling Research Training</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>There is a lot of need in the Industry for engineers who know the art (edited to <i>craft</i>)of research (pursue and find the right literature and algorithms, understand prior art, adopt and modify for a specific context with domain awareness, interpret results,  dive deeper into the unique insights), or researchers who can apply this art with engineering finesse. I wondered yesterday in a meeting if our technological lessons from COVID times have helped us identify a way to train many more in the ways of research, more than what we produce as PhDs in universities (the MS programs seem to train advanced engineers more than padawan researchers). <br/></p></div>
    </content>
    <updated>2021-05-06T13:46:00Z</updated>
    <published>2021-05-06T13:46:00Z</published>
    <category scheme="http://www.blogger.com/atom/ns#" term="aggregator"/>
    <author>
      <name>metoo</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/07192519900962182610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-21129445</id>
      <category term="aggregator"/>
      <category term="Non-CS"/>
      <author>
        <name>metoo</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/07192519900962182610</uri>
      </author>
      <link href="http://mysliceofpizza.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/21129445/posts/default/-/aggregator" rel="self" type="application/atom+xml"/>
      <link href="http://mysliceofpizza.blogspot.com/search/label/aggregator" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/21129445/posts/default/-/aggregator/-/aggregator?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>books, stories, poems, algorithms, math and computer science. 

some art and anecdotes too.</subtitle>
      <title>my slice of pizza</title>
      <updated>2021-05-06T15:39:21Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6832816500979478899</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6832816500979478899/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/negotiations.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6832816500979478899" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6832816500979478899" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/negotiations.html" rel="alternate" type="text/html"/>
    <title>Negotiations</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>So you got an offer to be an assistant professor in the computer science department at Prestigious U. Congratulations! </p><p>Time to negotiate your offer with the chair. Don't be nervous. This shouldn't be adversarial. Both of you have the same goal in mind--for you to come to Prestigious and be successful. </p><p>Let's discuss the different aspects of each package.</p><p><b>Research </b></p><p>Funds for supporting your research such as equipment, graduate student support, travel and postdocs. Here you should explain what you need to be successful. This will vary by subdiscipline, a systems researcher will need more equipment and students than a theorist. Keep in mind the university is giving you funds for 2-4 years to start your research, after which you are expected to fund your own research via grants.</p><p>I don't recommend taking on a postdoc right at the start of your first academic appointment. Postdocs require good mentoring while you need to spend the first year getting your research up and running. If you do ask for postdoc money, ask to have a flexible start time.</p><p>Many departments give course reductions to get your research going. I'd suggest asking to spend your first semester teaching a graduate topics course based on your thesis research to pick up some PhD students followed by a semester with no classes to get you research program going.</p><p><b>Salary</b></p><p>This includes actual salary, which is also the base for future raises, and summer salary in the first couple of years. Feel free to ask for more salary, but often these numbers are fixed for new assistant professors. There is more give if you take an academic job later in your career. You could also say something like, "Well if you can't give me more salary maybe you could give me another semester of grad student support?"</p><p><b>Partner</b></p><p>It seems 80% of the time, a job candidate has a partner that needs accommodating. Don't wait until the end of negotiations, bring it up early. The more time we have, the better we can help. Doesn't matter what job they want--we know people and we know people who know people.</p><p><b>Thesis</b></p><p>Many schools won't hire you as an assistant professor if you haven't finished your thesis. Has to do with college rankings work. Don't worry--they will generally give you some other role with the same package until you finish. This might delay your tenure clock though.</p><p><b>Delayed start time</b></p><p>A January start is usually fine with good reason but if you weren't planning to start until the fall of 2022 why are you on the market this year? If you do get the department to hold a position for you, remember you are also making a commitment--this is not an opportunity to try again for something better.</p><p><b>Overall</b></p><p>You may not get all that you want after a negotiation--don't take it personally. You shouldn't necessarily choose the place that gives you the biggest package. It's far more important in the long run that you pick a place where you can best succeed both professionally and personally, and the package is just a small piece of that puzzle.</p></div>
    </content>
    <updated>2021-05-06T13:07:00Z</updated>
    <published>2021-05-06T13:07:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-14T23:29:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=1628</id>
    <link href="https://toc4fairness.org/self-fulfilling-and-self-negating-predictions-a-short-tale-of-performativity-in-machine-learning/" rel="alternate" type="text/html"/>
    <title>Self-fulfilling and self-negating predictions: a short tale of performativity in machine learning</title>
    <summary>This post is based on results and discussions from a series of joint works with Moritz Hardt, Celestine Mendler-Dünner, John Miller, and Juan C. Perdomo. In 1998, Michel Callon wrote ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><blockquote class="wp-block-quote has-text-align-center"><p><em>This post is based on results and discussions from a series of joint works with Moritz Hardt, Celestine Mendler-Dünner, John Miller, and Juan C. Perdomo.</em></p></blockquote>



<p>In 1998, Michel Callon wrote what would be the first in an ongoing series of controversial publications in economic sociology [1]. He was the first to propose the idea that “the economy is not embedded in society but in economics”. With this, he challenged the conventional view that economic theories and models passively observe markets and infer their behavior, just like laws of physics passively describe the principles governing natural phenomena. Instead, Callon argued that economic theories are <em>performative:</em> they induce the economy, creating the phenomena they aim to describe.</p>



<p>One example that is often cited in support of Callon’s claims is the impact of the celebrated Black-Scholes-Merton options pricing model [2, 3]. MacKenzie and Millo [4] investigated the role of this model in the economy and found that it “made itself true”. In their words,</p>



<p class="has-text-align-center"><em>“Black, Scholes, and Merton’s model did not describe an already existing world: when first formulated, its assumptions were quite unrealistic, and empirical prices differed systematically from the model. Gradually, though, the financial markets changed in a way that fitted the model”.</em></p>



<p>Indeed, participants in the market started making decisions assuming the market obeys the mathematical laws implied by the Black-Scholes-Merton model. As MacKenzie and Millo put it, “pricing models came to shape the very way participants thought and talked about options”.</p>



<p>This phenomenon — whereby models and predictions inform decision-making and thus alter the target of prediction itself — is by no means special to economic forecasts.</p>



<p>Predictive policing, for example, develops algorithms that use historical data to estimate the likelihood of crime at a given location. Those locations where criminal behavior is deemed likely by the system typically get more police patrols and surveillance in general. In a kind of self-fulfilling prophecy [5], these actions resulting from prediction might further increase the <em>perceived</em> crime rate at the patrolled locations, thus biasing the data used for future decisions.</p>



<p>A similar feedback loop arises in traffic predictions, when drivers decide which route to take based on the estimated time of arrival (ETA) calculated by a traffic prediction system. If the predictive system estimates low ETA for a given route, many drivers take the route, potentially leading to an overflow of traffic and making the ETA prediction inaccurate as a result. Contrary to the previous example, traffic predictions arguably exhibit a self-negating prophecy: low ETA might imply a longer travel time, and vice versa.</p>



<p>While the previous examples deal with qualitatively different feedback mechanisms, the interplay of predictions and decision-making is similar. First, one uses historical data to build a predictive model. Then, the predictions of the model feed into and inform consequential decisions. Finally, these decisions trigger changes in the environment, making future observations differ from those in the initial dataset.</p>



<p class="has-text-align-center"><img height="81" src="https://lh3.googleusercontent.com/C3qD-SK6AD4gDpkJznllugJS8OZaJwGvXSNi96qYKPu2ALr5vSxPMhteCScLq-CCdKOvh8bZ8KB-gvTRVkbZALVEuML0WhSV0pX8YM1MHupTLPjZ8PKEydiNu_h3iUdlVAYUcFDT" width="624"/></p>



<p>We refer to prediction problems that exhibit this feedback-loop behavior as <em>performative prediction</em> problems.</p>



<p>In the language of machine learning, such a change in patterns would often be called <em>distribution shift</em>. Notably, however, performative distribution shift is not due to external factors independent of the model, such as, say, when traffic patterns change due to seasonal effects. Rather, the distribution shift is triggered directly by the choice of predictive model. (Of course, distribution shifts can also be caused by a combination of external factors and model choice.)</p>



<p>To formalize performative prediction mathematically, it is instructive to contrast performative prediction problems with supervised learning problems. In supervised learning, the decision-maker observes pairs of features and outcomes <img height="15" src="https://lh5.googleusercontent.com/2U92MhGBO6DJqe607HL2T4uWicPXKFgrU2PoSaeymNLxbOteL6r_dhkuuFo91W2AXoAzrhxt8Ndg1jfhf8KVqZMbID1koi01cpGcXz-CTACfH_b54DohHMqpO2hJ5bEtfdE6v6Ag" width="72"/> drawn from a <em>fixed</em> distribution <img height="13" src="https://lh6.googleusercontent.com/Rj1cE6eMb1304KzXnhY0e_dxrgXvPNd9q-IzgZLuKKxFYfn3q82FIiQUwb0_DO8G-xyuGdp8T5Y-EN5bKYPPxSd5QaMMO1irMwSeI6O5sy08Ubriy2cFliObESx_XOoFzPwZy6Wi" width="14"/>. The key difference in performative prediction is that there is no longer an unknown static distribution generating observations; rather, data is drawn from a <em>model-dependent distribution</em> <img height="17" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/>, where <img height="12" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/> is a parameter vector specifying the deployed model. For example, <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/> could be the weights of a neural network, or a vector of linear regression coefficients. For a given choice of parameters <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/>, <img height="17" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/> should be thought of as the distribution over features and outcomes that results from making decisions according to the model specified by <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/>. In the context of the traffic prediction example, <img height="17" src="https://lh4.googleusercontent.com/HDm0VWvPNAUIdPfMSLc0ptEDS7Tph1p00WbC6TlSbQ8oYjK2nJSuSMiLuKN8IAA89L2H8rDLRPf9GWwc37Zb_0_qLxN-UuVVsz_hkBB43hQcTOtV4HUg30BtsHel_hpCFHfYJa6E" width="32"/> could be a distribution over traffic conditions and travel times, given that drivers make routing decisions in response to ETA forecasts by model <img height="13" src="https://lh3.googleusercontent.com/TkPLvHJ9t0soCQkx43reFppYsx4b0sJrNUBWd_Yx_rarIU4nS92GpBFBn-D_jhvDNAU04Aee4NMsVzHEozOLxXN11DMl4h1O2TvHfNih9IqHi0R3wM1otXYEp1MbkZlbI0faSV1i" width="8"/>.</p>



<p>In supervised learning, the quality of a model <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/> is typically measured by its <em>risk</em>, namely, the expected loss of the model on instances from distribution <img height="13" src="https://lh6.googleusercontent.com/Rj1cE6eMb1304KzXnhY0e_dxrgXvPNd9q-IzgZLuKKxFYfn3q82FIiQUwb0_DO8G-xyuGdp8T5Y-EN5bKYPPxSd5QaMMO1irMwSeI6O5sy08Ubriy2cFliObESx_XOoFzPwZy6Wi" width="14"/> as measured via a loss function <img height="16" src="https://lh5.googleusercontent.com/iLwCF0E2qkddKEmiLhziUoOB171Na3z6aLjqWSYTPykfGKIx5PBtNT9qsZXIMPLc3hVXkkupbpqILbC4Fx9p_h5G8a1GmlYVL3rcx787SVO-24HKrL39LxAwPGaQ2i8wuPURBoyS" width="9"/>:</p>



<p class="has-text-align-center"><img height="26" src="https://lh5.googleusercontent.com/vP7Ll-u4VopJmW24L2lMvqwf8N9bOrKaoLFDDlmcZgmML4CM8bM53_lMEG--5Om2mTYOQ5uNqUBkzlp2Hgd9i3EO8Y8bzC8kvkFgQKSfqOgHoWiMpK2MVasGJmdFTdERwIUg0RzK" width="153"/></p>



<p>Since performative prediction does not admit one true data-generating distribution, but rather a family of distributions <img height="18" src="https://lh6.googleusercontent.com/un2s0qA36MFKtNGHoDb7sQdPfToUoZjYy45nbwmSy2jQhSDJbiouNjATQWsSwDlpUF-PfOk1ElFPDb8Rsr_TAAB0GUemVV5Y_bG1EISEUD7M9N5RGxug1gGdvpXa-D70ATQSafzT" width="58"/>, evaluating a model <img height="13" src="https://lh4.googleusercontent.com/zJsapVnRoEAmwRciJFyPDFM3pc3YOuB_6gM71GJX1Aa_9Tm09RLD4mj_DOEkH0CzKHFPV93LesWVf1AUH9QlwQoU0x07xusBHiT8-EeAApf2qMsufNc8Vuzc77LWu4bxEuYwgxXp" width="8"/> calls for a new risk concept. Arguably the most natural counterpart of the risk in supervised learning is the expected loss on the distribution that arises once the model is deployed and feeds into consequential decisions. This leads to the notion of <em>performative risk</em>, defined as:</p>



<p class="has-text-align-center"><img height="30" src="https://lh5.googleusercontent.com/zWCmnf2uG6j8LZG7w68p8y4I2wHcKc82ZULfh_MUmGc3YI6UByQP8dB3nBDugFmg8t0VybMz6RV4tzviUsIyHmVY1uUH1Xjv2XKTSEQGpFW3-o0e50-rxKeouEula7UNdqC6HYHr" width="181"/></p>



<p>Adopting the performative risk as the single overarching measure of quality, a model would be optimal if it minimizes the performative risk. While an appealing solution concept, performative optimality is difficult to achieve, seeing the double dependence of the risk function on <img height="13" src="https://lh3.googleusercontent.com/ywU1sy8RfVug9ZI2C0ETvBvXg7_gqi_Xx9a1-nILxAhSafCu1OvR5095fDFr3PJN9bmBa6qoPoLzz0bFrjPtTEw5-BX3vvKHqTUyvDFaDwt7mc0B41XPpPgZZ_T4gb72PAFMDwAK" width="8"/>. One of the main computational difficulties is the fact that, even if the loss <img height="16" src="https://lh5.googleusercontent.com/WZWkgjmj0CvaBREmhCbwBw1i5JfL1Tviq96JbXG1SePOG-BXJ8uv_fUUE2KKJCw2qvd5zpA_GUhHZ9azV6AUENm0qVsKnTPMg6WzLvT5QGGLNwcLyhRU0iOhYtRy6Fylx66uMGRS" width="9"/> is convex in <img height="13" src="https://lh3.googleusercontent.com/ywU1sy8RfVug9ZI2C0ETvBvXg7_gqi_Xx9a1-nILxAhSafCu1OvR5095fDFr3PJN9bmBa6qoPoLzz0bFrjPtTEw5-BX3vvKHqTUyvDFaDwt7mc0B41XPpPgZZ_T4gb72PAFMDwAK" width="8"/>, <img height="17" src="https://lh4.googleusercontent.com/-syiEeOCPlb5lZOfPAK063xD0s6EraAKMtHoPUR8cIfpnP0AdkrnINBFw6Ejb8nr1jF3G7QO7v5K7o28nSnnI74SPxaJH7apiBu7Bpd7DiWnZ6Hx79Yi4v3iwJQR8j0eUJh2Qzfk" width="43"/> need not be convex. Prior work on strategic classification [6] implies a set sufficient conditions for <img height="17" src="https://lh4.googleusercontent.com/-syiEeOCPlb5lZOfPAK063xD0s6EraAKMtHoPUR8cIfpnP0AdkrnINBFw6Ejb8nr1jF3G7QO7v5K7o28nSnnI74SPxaJH7apiBu7Bpd7DiWnZ6Hx79Yi4v3iwJQR8j0eUJh2Qzfk" width="43"/> to be convex in a binary classification context, and in recent work [7] we identified a complementary set of conditions when the family of distributions <img height="18" src="https://lh4.googleusercontent.com/Ti5C4ibszTKUqTM5evbcuaHojWHAN_Rq20XcnDlunJNG1M4lBKsjB7nJOKEN4lTSdWNmgKkcxqAwPPRahQcZ4uL8kCZHn_AuaB_Z9T35eztMk2CZzMcK50GMGnH69Jdjd0euwDtI" width="58"/> forms an appropriate location-scale family. That said, in many practical settings convexity might be an unrealistic and unnecessarily strong guarantee to aim for. As we know from present-day machine learning, even non-convex problems can sometimes be amenable to simple optimization algorithms. Understanding the optimization landscape of the performative risk beyond convex settings is a fruitful direction going forward.</p>



<p>A seemingly less ambitious target is to find a model that is <em>locally</em> optimal in some appropriate sense. For example, one could optimize for models that are optimal on the distribution that they induce:</p>



<p class="has-text-align-center"><img height="22" src="https://lh6.googleusercontent.com/ENuLNHEI7mNEDONR7rUA2FzwbkvqbINkwHJfO7jiiODrZ3_Ko9NZmJAt4Qv0cPNF4-_C68yI3CE4UMcFjzP8v5UHdu_LgBkdrCnUj4A-E6uccXlclWWZef2onVzsd5X2sHq5tTlh" width="233"/></p>



<p>We call a model that satisfies the fixed-point equation above <em>performatively stable</em>. Performative stability arises naturally when the decision-maker applies the heuristic of myopically updating the model based on the distribution resulting from the previous deployment:</p>



<p class="has-text-align-center"><img height="20" src="https://lh4.googleusercontent.com/RlZ0ZmF3E0VHyj_ktWb3QtAxsXTY_WjoGS29Jss3v_5cXnDVsWbeEvaTAA4l4tUYa5ccYvJPWcPXy5oLa3BnBT0Yqws89mNind-2UT8IA2ip3_pt3PbFhiNVcgIhgMS1omjfgzI1" width="248"/></p>



<p>If this retraining strategy converges, then it necessarily converges to a performatively stable solution. This is an appealing property, since it says that stability eliminates the need for retraining. Several existing works [8, 9, 10] have identified necessary and sufficient conditions for the above retraining heuristic, and some of its efficient approximations, to converge to a stable point. Roughly speaking, retraining converges to a stable solution if the loss is well-behaved and the performative feedback effects are not too strong. If either of those two conditions is violated, there is no guarantee of convergence.</p>



<p>In the language of game theory, one can think of performative prediction as a two-player game between a decision-maker, who decides which predictive model to deploy, and the model’s environment, which generates observations according to <img height="16" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/>. If <img height="16" src="https://lh5.googleusercontent.com/F3-xnNlZ_3rgUWEAAGQu5mVaxNF0xbZ2WYw-7ngo-2XVG2D5Ru1YfSAHKXhr_IxvU0E4REBQEm7YWPjKMVo6y2-ln9CNmazsQQjq2fc7O7UlKnuVE8IvDIXHg6HS3G8ZzeMHkd3g" width="32"/> is thought of as the “best response” (according to some underlying utility) of the model’s environment to the deployment of model <img height="13" src="https://lh5.googleusercontent.com/04N5kfodbA5qOBQcA_TM6UP8ebiTodk6WUAUxAQ2fP2kQ1bYnrF0JGVYLtPG4-Xu_gE7s0ZvqA0URG3Q1iT4mlmvqrIJblikhbq42prczmRFp93QX_iRvD0XEfNINQSTktlPQpDZ" width="8"/>, then a performatively stable solution corresponds to a <em>Nash</em> equilibrium, while a performatively optimal solution corresponds to a <em>Stackelberg</em> equilibrium with the decision-maker acting as the leader.</p>



<p>Only in special cases, such as in well-behaved zero-sum games, it is known that Nash equilibria coincide with Stackelberg equilibria. Therefore, whenever performative prediction is a well-behaved zero-sum game, all stable solutions are also performatively optimal. However, <em>performative prediction is typically not a zero-sum game</em>. For example, if the decision-maker’s loss simply measures predictive accuracy, it seems odd that the environment’s primary objective is to hurt the model’s accuracy. Indeed, a typical performative prediction problem is a general-sum game without much structure. This implies that stable solutions and performative optima can be <em>very different</em>. And, since naive retraining strategies only converge to stability, this means that such myopic updates can be an inadequate method of overcoming performative distribution shifts and achieving low performative risk. This observation further motivates understanding the optimization landscape of the performative risk, as well as developing efficient algorithms for optimizing it. Recent work has explored several algorithmic solutions [11, 7], appropriate in convex settings.</p>



<p>Performative prediction relates to many other areas beyond game theory, including bandits, reinforcement learning, control theory. These frameworks are flexible enough to capture performative prediction as a special case, however performativity arises via distinctive feedback mechanisms and as such deserves its own specialized analysis. There is a long way to go in understanding the properties of performative distribution shifts, how they connect to feedback mechanisms in other disciplines, and how to tackle these shifts in practice. Furthermore, it is unclear whether a single distribution <img height="16" src="https://lh6.googleusercontent.com/a-1tKaIQT9pJwi6WQ4eV4ZO3xcFy1e_joEh0tz1QdG-cj7x-60nDwfRyyztRSzjEaL-0xbImihmV5LDB8njPUhSlsIeyJABtowIBOktz79sRW7hAY8B75CePuDxp47xEFOZ68gSA" width="32"/> is expressive enough to describe the observations after model deployment; in practice there are different kinds of memory effects [12] and self-reinforcing loops that make the data distribution evolve with time, even when the model is kept fixed. Finally, to make the existing theoretical insights actionable, going forward we need to think about what is the right solution concept — both statistically and ethically — to optimize for in performative settings.</p>



<p><br/>[1] M. Callon. Introduction: the embeddedness of economic markets in economics. <em>The Sociological Review</em>, 1998<br/>[2] F. Black, M. Scholes. The pricing of options and corporate liabilities. <em>The Journal of Political Economy</em>, 1973<br/>[3] R. C. Merton. Theory of rational option pricing. <em>The Bell Journal of Economics and Management Science</em>, 1973<br/>[4] D. MacKenzie, Y. Millo. Constructing a market, performing theory: The historical sociology of a financial derivatives exchange. <em>American Journal of Sociology</em>, 2003<br/>[5] D. Ensign, S. A. Friedler, S. Neville, C. Scheidegger, S. Venkatasubramanian. Runaway feedback loops in predictive policing. <em>ACM Conference on Fairness, Accountability and Transparency</em>, 2018<br/>[6] J. Dong, A. Roth, Z. Schutzman, B. Waggoner, Z. S. Wu. Strategic classification from revealed preferences. <em>ACM Conference on Economics and Computation</em>, 2018<br/>[7] J. Miller, J. C. Perdomo, T. Zrnic. Outside the echo chamber: Optimizing the performative risk. <em>arXiv preprint</em>, 2021<br/>[8] J. C. Perdomo, T. Zrnic, C. Mendler-Dünner, M. Hardt. Performative prediction. <em>International Conference on Machine Learning</em>, 2020<br/>[9] C. Mendler-Dünner, J. C. Perdomo, T. Zrnic, M. Hardt. Stochastic optimization for performative prediction. <em>Conference on Neural Information Processing Systems</em>, 2020<br/>[10] D. Drusvyatskiy, L. Xiao. Stochastic optimization with decision-dependent distributions. <em>arXiv preprint</em>, 2020<br/>[11] Z. Izzo, L. Ying, J. Zou. How to learn when data reacts to your model: Performative gradient descent. <em>arXiv preprint</em>, 2021<br/>[12] G. Brown, S. Hod, I. Kalemaj. Performative prediction in a stateful world. <em>arXiv preprint</em>, 2020</p></div>
    </content>
    <updated>2021-05-06T12:42:52Z</updated>
    <published>2021-05-06T12:42:52Z</published>
    <category term="Blog"/>
    <author>
      <name>tijanazrnic</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i1.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2021-05-14T23:39:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/066</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/066" rel="alternate" type="text/html"/>
    <title>TR21-066 |  Dimension-free Bounds and Structural Results in Communication Complexity | 

	Lianna Hambardzumyan, 

	Hamed Hatami, 

	Pooya Hatami</title>
    <summary>The purpose of this article is to initiate a systematic study of dimension-free relations between basic communication and query complexity measures  and various  matrix norms.  In other words, our goal is to obtain    inequalities that bound a parameter   solely as a function of another parameter. This is in contrast to perhaps the more common framework in communication complexity  where  poly-logarithmic dependencies on the number of input bits are   tolerated. 


Dimension-free bounds are also closely related to structural results, where one seeks to describe the structure of Boolean matrices and functions that have low complexity.  We prove such  theorems for several communication and query complexity measures as well as various matrix and operator norms. In several other cases we show that such bounds do not exist. 


We propose several conjectures, and establish that, in addition to applications in complexity theory, these   problems are central to  characterization of the idempotents of the algebra of Schur multipliers, and could lead to new extensions of  Cohen's celebrated idempotent theorem regarding the Fourier algebra.</summary>
    <updated>2021-05-05T18:59:23Z</updated>
    <published>2021-05-05T18:59:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-14T23:37:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/tpdp21-cfp/</id>
    <link href="https://differentialprivacy.org/tpdp21-cfp/" rel="alternate" type="text/html"/>
    <title>Call for Papers - Workshop on the Theory and Practice of Differential Privacy (TPDP 2021)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Work on differential privacy spans a number of different research communities, including theoretical computer science, machine learning, statistics, security, law, databases, cryptography, programming languages, social sciences, and more.
Each of these communities may choose to publish their work in their own community’s venues, which could result in small groups of differential privacy researchers becoming isolated.
To alleviate these issues, we have the Workshop on the <a href="https://tpdp.journalprivacyconfidentiality.org/">Theory and Practice of Differential Privacy</a> (TPDP), which is intended to bring these subcommunities together under one roof (well, a virtual one at least for 2020 and 2021).</p>

<p>We have just posted the <a href="https://tpdp.journalprivacyconfidentiality.org/2021/TPDP2021CfP.pdf">Call for Papers</a> for <a href="https://tpdp.journalprivacyconfidentiality.org/2021/">TPDP 2021</a>, which will be a workshop affiliated with <a href="https://icml.cc/Conferences/2021/">ICML 2021</a>.
The submission deadline is Friday, May 28, 2021, Anywhere on Earth (conveniently, two days after the deadline for NeurIPS 2021).
Submissions are extended abstracts of up to four pages in length, and will undergo a lightweight review process, based mostly on relevance and interest to the differential privacy community.
The workshop is non-archival, so feel free to submit recent work at any stage of publication.
Submissions will be on <a href="https://openreview.net/group?id=ICML.cc/2021/Workshop/TPDP">OpenReview</a>, but since submitted work may be preliminary, the process will be “closed” similar to traditional review processes.
One goal of the workshop is to be inclusive and welcoming to newcomers to the differential privacy community, so please consider participating even if you are new to the field.</p>

<p>Most papers will be presented as posters at a (virtual) poster session, while a few papers will be selected for spotlight talks.
There will also be plenary talks by <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a> (Hebrew University of Jerusalem) and <a href="https://www2.math.upenn.edu/~ryrogers/">Ryan Rogers</a> (LinkedIn).
The program co-chairs are <a href="https://sites.gatech.edu/rachel-cummings/">Rachel Cummings</a> and <a href="http://www.gautamkamath.com/">myself</a>.
Please submit your best work on differential privacy, and hope to see you there!</p>
<p align="center">
  <img src="https://differentialprivacy.org/images/Ligett.png"/>
  <img src="https://differentialprivacy.org/images/Rogers.png"/> <br/>
    <i>Invited speakers Katrina Ligett and Ryan Rogers</i>
</p></div>
    </summary>
    <updated>2021-05-05T14:00:00Z</updated>
    <published>2021-05-05T14:00:00Z</published>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-05-14T23:04:28Z</updated>
    </source>
  </entry>
</feed>
