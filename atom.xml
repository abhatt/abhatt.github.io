<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-08-06T05:22:20Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1380</id>
    <link href="https://ptreview.sublinear.info/?p=1380" rel="alternate" type="text/html"/>
    <title>Videos from the WoLA 2020 workshop</title>
    <summary>The 4th Workshop on Local Algorithms (WoLA 2020) recently concluded: aimed at “fostering dialogue and cross-pollination of ideas between the various communities” related to local algorithms, broadly construed, it featured invited and contributed talks on a variety of topics, many (if not most) very relevant to the sublinear algorithms and property testing community. Because of […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The 4th <a href="https://www.mit.edu/~mahabadi/workshops/WOLA-2020.html">Workshop on Local Algorithms (WoLA 2020)</a> recently concluded: aimed at <em>“fostering dialogue and cross-pollination of ideas between the various communities</em>” related to local algorithms, broadly construed, it featured invited and contributed talks on a variety of topics, many (if not most) very relevant to the sublinear algorithms and property testing community.</p>



<p>Because of the online format of the workshop (imposed by the current circumstances), all talks were recorded and posted online. As such, all videos all available on the workshop’s <a href="https://www.mit.edu/~mahabadi/workshops/WOLA-2020.html">website</a> and <a href="https://www.youtube.com/channel/UCMBSZ3q2FKJntpcK72h9eeA/videos">YouTube channel</a>: a good list of resources to peruse!</p></div>
    </content>
    <updated>2020-08-06T00:33:58Z</updated>
    <published>2020-08-06T00:33:58Z</published>
    <category term="Conference reports"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-08-06T02:07:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.02269</id>
    <link href="http://arxiv.org/abs/2008.02269" rel="alternate" type="text/html"/>
    <title>Computational Barriers to Estimation from Low-Degree Polynomials</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schramm:Tselil.html">Tselil Schramm</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wein:Alexander_S=.html">Alexander S. Wein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.02269">PDF</a><br/><b>Abstract: </b>One fundamental goal of high-dimensional statistics is to detect or recover
structure from noisy data. In many cases, the data can be faithfully modeled by
a planted structure (such as a low-rank matrix) perturbed by random noise. But
even for these simple models, the computational complexity of estimation is
sometimes poorly understood. A growing body of work studies low-degree
polynomials as a proxy for computational complexity: it has been demonstrated
in various settings that low-degree polynomials of the data can match the
statistical performance of the best known polynomial-time algorithms for
detection. While prior work has studied the power of low-degree polynomials for
the task of detecting the presence of hidden structures, it has failed to
address the estimation problem in settings where detection is qualitatively
easier than estimation.
</p>
<p>In this work, we extend the method of low-degree polynomials to address
problems of estimation and recovery. For a large class of "signal plus noise"
problems, we give a user-friendly lower bound for the best possible mean
squared error achievable by any degree-D polynomial. To our knowledge, this is
the first instance in which the low-degree polynomial method can establish
low-degree hardness of recovery problems where the associated detection problem
is easy. As applications, we give a tight characterization of the low-degree
minimum mean squared error for the planted submatrix and planted dense subgraph
problems, resolving (in the low-degree framework) open problems about the
computational complexity of recovery in both cases.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.02258</id>
    <link href="http://arxiv.org/abs/2008.02258" rel="alternate" type="text/html"/>
    <title>The Expected Size of Random Convex Layers and Convex Shells</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guo:Zhengyang.html">Zhengyang Guo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yi.html">Yi Li</a>, Shaoyu Pei <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.02258">PDF</a><br/><b>Abstract: </b>Given a planar point set $X$, we study the convex shells and the convex
layers. We prove that when $X$ consists of points independently and uniformly
sampled inside a convex polygon with $k$ vertices, the expected number of
vertices on the first $t$ convex shells is $O(kt\log{n})$ for $t=O(\sqrt{n})$,
and the expected number of vertices on the first $t$ convex layers is
$O\left(kt^{3}\log{\frac{n}{t^2}}\right)$. We also show a lower bound of
$\Omega(t\log n)$ for both quantities in the special cases where $k=3,4$. The
implications of those results in the average-case analysis of two computational
geometry algorithms are then discussed.
</p></div>
    </summary>
    <updated>2020-08-06T01:47:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.02215</id>
    <link href="http://arxiv.org/abs/2008.02215" rel="alternate" type="text/html"/>
    <title>A Time Leap Challenge for SAT Solving</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Johannes K. Fichte, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hecher:Markus.html">Markus Hecher</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Szeider:Stefan.html">Stefan Szeider</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.02215">PDF</a><br/><b>Abstract: </b>We compare the impact of hardware advancement and algorithm advancement for
SAT solving over the last two decades. In particular, we compare 20-year-old
SAT-solvers on new computer hardware with modern SAT-solvers on 20-year-old
hardware. Our findings show that the progress on the algorithmic side has at
least as much impact as the progress on the hardware side.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.02167</id>
    <link href="http://arxiv.org/abs/2008.02167" rel="alternate" type="text/html"/>
    <title>GeoTree: a data structure for constant time geospatial search enabling a real-time mix-adjusted median property price index</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miller:Robert.html">Robert Miller</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maguire:Phil.html">Phil Maguire</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.02167">PDF</a><br/><b>Abstract: </b>A common problem appearing across the field of data science is $k$-NN
($k$-nearest neighbours), particularly within the context of Geographic
Information Systems. In this article, we present a novel data structure, the
GeoTree, which holds a collection of geohashes (string encodings of GPS
co-ordinates). This enables a constant $O\left(1\right)$ time search algorithm
that returns a set of geohashes surrounding a given geohash in the GeoTree,
representing the approximate $k$-nearest neighbours of that geohash.
Furthermore, the GeoTree data structure retains $O\left(n\right)$ memory
requirement. We apply the data structure to a property price index algorithm
focused on price comparison with historical neighbouring sales, demonstrating
an enhanced performance. The results show that this data structure allows for
the development of a real-time property price index, and can be scaled to
larger datasets with ease.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.02146</id>
    <link href="http://arxiv.org/abs/2008.02146" rel="alternate" type="text/html"/>
    <title>Reducing Isotropy and Volume to KLS: An $O(n^3\psi^2)$ Volume Algorithm</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jia:He.html">He Jia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Laddha:Aditi.html">Aditi Laddha</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Yin_Tat.html">Yin Tat Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vempala:Santosh_S=.html">Santosh S. Vempala</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.02146">PDF</a><br/><b>Abstract: </b>We show that the the volume of a convex body in ${\mathbb R}^{n}$ in the
general membership oracle model can be computed with
$\widetilde{O}(n^{3}\psi^{2}/\varepsilon^{2})$ oracle queries, where $\psi$ is
the KLS constant ($\widetilde{O}$ suppresses polylogarithmic terms.
$O^{*}$suppresses dependence on error parameters as well as polylogarithmic
terms.). With the current bound of $\psi\lesssim n^{\frac{1}{4}}$, this gives
an $\widetilde{O}(n^{3.5}/\varepsilon^{2})$ algorithm, the first general
improvement on the Lov\'{a}sz-Vempala $\widetilde{O}(n^{4}/\varepsilon^{2})$
algorithm from 2003. The main new ingredient is\emph{ }an
$\widetilde{O}(n^{3}\psi^{2})$ algorithm for isotropic transformation,
following which we can apply the $\widetilde{O}(n^{3}/\varepsilon^{2})$ volume
algorithm of Cousins and Vempala for well-rounded convex bodies. A positive
resolution of the KLS conjecture would imply an
$\widetilde{O}(n^{3}/\epsilon^{2})$ volume algorithm. We also give an efficient
implementation of the new algorithm for convex polytopes defined by $m$
inequalities in ${\mathbb R}^{n}$: polytope volume can be estimated in time
$\widetilde{O}(mn^{c}/\varepsilon^{2})$ where $c&lt;3.7$ depends on the current
matrix multiplication exponent and improves on the the previous best bound.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.02108</id>
    <link href="http://arxiv.org/abs/2008.02108" rel="alternate" type="text/html"/>
    <title>Identifying the $k$ Best Targets for an Advertisement Campaign via Online Social Networks</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonomo:Mariella.html">Mariella Bonomo</a>, Armando La Placa, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rombo:Simona_E=.html">Simona E. Rombo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.02108">PDF</a><br/><b>Abstract: </b>We propose a novel approach for the recommendation of possible customers
(users) to advertisers (e.g., brands) based on two main aspects: (i) the
comparison between On-line Social Network profiles, and (ii) neighborhood
analysis on the On-line Social Network. Profile matching between users and
brands is considered based on bag-of-words representation of textual contents
coming from the social media, and measures such as the Term Frequency-Inverse
Document Frequency are used in order to characterize the importance of words in
the comparison. The approach has been implemented relying on Big Data
Technologies, allowing this way the efficient analysis of very large Online
Social Networks. Results on real datasets show that the combination of profile
matching and neighborhood analysis is successful in identifying the most
suitable set of users to be used as target for a given advertisement campaign.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.02071</id>
    <link href="http://arxiv.org/abs/2008.02071" rel="alternate" type="text/html"/>
    <title>Persistent Homology in $\ell_{\infty}$ Metric</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Gabriele Beltramo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Skraba:Primoz.html">Primoz Skraba</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.02071">PDF</a><br/><b>Abstract: </b>Proximity complexes and filtrations are a central construction in topological
data analysis. Built using distance functions or more generally metrics, they
are often used to infer connectivity information from point clouds. We
investigate proximity complexes and filtrations built over the Chebyshev
metric, also known as the maximum metric or $\ell_{\infty}$ metric, rather than
the classical Euclidean metric. Somewhat surprisingly, the $\ell_{\infty}$ case
has not been investigated thoroughly. Our motivation lies in that this metric
has the far simpler numerical tests which can lead to computational speedups
for high-dimensional data analysis. In this paper, we examine a number of
classical complexes under this metric, including the \v{C}ech, Vietoris-Rips,
and Alpha complexes. We also introduce two new complexes which we call the
Alpha clique and Minibox complexes. We provide results on topological
properties of these, as well as computational experiments which show that these
can often be used to reduce the number of high-dimensional simplices included
in \v{C}ech filtrations and so speed up the computation of persistent homology.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.02060</id>
    <link href="http://arxiv.org/abs/2008.02060" rel="alternate" type="text/html"/>
    <title>A Note on a Recent Algorithm for Minimum Cut</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Paweł Gawrychowski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mozes:Shay.html">Shay Mozes</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weimann:Oren.html">Oren Weimann</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.02060">PDF</a><br/><b>Abstract: </b>Given an undirected edge-weighted graph $G=(V,E)$ with $m$ edges and $n$
vertices, the minimum cut problem asks to find a subset of vertices $S$ such
that the total weight of all edges between $S$ and $V \setminus S$ is
minimized. Karger's longstanding $O(m \log^3 n)$ time randomized algorithm for
this problem was very recently improved in two independent works to $O(m \log^2
n)$ [ICALP'20] and to $O(m \log^2 n + n\log^5 n)$ [STOC'20]. These two
algorithms use different approaches and techniques. In particular, while the
former is faster, the latter has the advantage that it can be used to obtain
efficient algorithms in the cut-query and in the streaming models of
computation. In this paper, we show how to simplify and improve the algorithm
of [STOC'20] to $O(m \log^2 n + n\log^3 n)$. We obtain this by replacing a
randomized algorithm that, given a spanning tree $T$ of $G$, finds in $O(m \log
n+n\log^4 n)$ time a minimum cut of $G$ that 2-respects (cuts two edges of) $T$
with a simple $O(m \log n+n\log^2 n)$ time deterministic algorithm for the same
problem.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01961</id>
    <link href="http://arxiv.org/abs/2008.01961" rel="alternate" type="text/html"/>
    <title>An Algorithm Framework for the Exact Solution and Improved Approximation of the Maximum Weighted Independent Set Problem</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Kai.html">Kai Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dannenhoffer:John_F=.html">John F. Dannenhoffer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roy:Utpal.html">Utpal Roy</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01961">PDF</a><br/><b>Abstract: </b>The Maximum Weighted Independent Set (MWIS) problem, which considers a graph
with weights assigned to nodes and seeks to discover the "heaviest" independent
set, that is, a set of nodes with maximum total weight so that no two nodes in
the set are connected by an edge. The MWIS problem arises in many application
domains, including the resource-constrained scheduling, error-correcting
coding, complex system analysis and optimization, and communication networks.
Since solving the MWIS problem is the core function for finding the optimum
solution of our novel graph-based formulation of the resource-constrained
Process Planning and Scheduling (PPS) problem, it is essential to have
"good-performance" algorithms to solve the MWIS problem. In this paper, we
propose a Novel Hybrid Heuristic Algorithm (NHHA) framework in a
divide-and-conquer structure that yields optimum feasible solutions to the MWIS
problem. The NHHA framework is optimized to minimize the recurrence. Using the
NHHA framework, we also solve the All Maximal Independent Sets Listing (AMISL)
problem, which can be seen as the subproblem of the MWIS problem. Moreover,
building composed MWIS algorithms that utilizing fast approximation algorithms
with the NHHA framework is an effective way to improve the accuracy of
approximation MWIS algorithms (e.g., GWMIN and GWMIN2 (Sakai et al., 2003)).
Eight algorithms for the MWIS problem, the exact MWIS algorithm, the AMISL
algorithm, two approximation algorithms from the literature, and four composed
algorithms, are applied and tested for solving the graph-based formulation of
the resource-constrained PPS problem to evaluate the scalability, accuracy, and
robustness.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01820</id>
    <link href="http://arxiv.org/abs/2008.01820" rel="alternate" type="text/html"/>
    <title>Lower Bounds on Circuit Depth of the Quantum Approximate Optimization Algorithm</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>James Ostroswki, Rebekah Herrman, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Humble:Travis_S=.html">Travis S. Humble</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Siopsis:George.html">George Siopsis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01820">PDF</a><br/><b>Abstract: </b>The quantum approximate optimization algorithm (QAOA) is a method of
approximately solving combinatorial optimization problems. While QAOA is
developed to solve a broad class of combinatorial optimization problems, it is
not clear which classes of problems are best suited for it. One factor in
demonstrating quantum advantage is the relationship between a problem instance
and the circuit depth required to implement the QAOA method. As errors in NISQ
devices increases exponentially with circuit depth, identifying lower bounds on
circuit depth can provide insights into when quantum advantage could be
feasible. Here, we identify how the structure of problem instances can be used
to identify lower bounds for circuit depth for each iteration of QAOA and
examine the relationship between problem structure and the circuit depth for a
variety of combinatorial optimization problems including MaxCut and MaxIndSet.
Specifically, we show how to derive a graph, $G$, that describes a general
combinatorial optimization problem and show that the depth of circuit is at
least the chromatic index of $G$. By looking at the scaling of circuit depth,
we argue that MaxCut, MaxIndSet, and some instances of Vertex Covering and
Boolean satisifiability problems are suitable for QAOA approaches while
Knapsack and Traveling Sales Person problems are not.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01803</id>
    <link href="http://arxiv.org/abs/2008.01803" rel="alternate" type="text/html"/>
    <title>GPLAN: Computer-Generated Dimensioned Floorplans for given Adjacencies</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shekhawat:Krishnendra.html">Krishnendra Shekhawat</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Upasani:Nitant.html">Nitant Upasani</a>, Sumit Bisht, Rahil Jain <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01803">PDF</a><br/><b>Abstract: </b>In this paper, we present GPLAN, software aimed at constructing dimensioned
floorplan layouts based on graph-theoretical and optimization techniques. GPLAN
takes user requirements as input in the following two forms: i. Adjacency
graph: It allows user to draw an adjacency graph on a GUI(graphical user
interface) corresponding to which GPLAN produces a set of dimensioned
floorplans with a rectangular boundary, where each floorplan is topologically
distinct from others. ii. Dimensionless layout: Here, user can draw any layout
with rectangular or non-rectangular boundary on a GUI and GPLAN transforms it
into a dimensioned floorplan while preserving adjacencies, positions, shapes of
the rooms. The above approaches represent different ways of inserting
adjacencies and GPLAN generate dimensioned floorplans corresponding to the
given adjacencies. The larger aim is to provide alternative platforms to user
for producing dimensioned floorplans for all given (architectural) constraints,
which can be further refined by architects.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01768</id>
    <link href="http://arxiv.org/abs/2008.01768" rel="alternate" type="text/html"/>
    <title>A Data-Structure for Approximate Longest Common Subsequence of A Set of Strings</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aghamolaei:Sepideh.html">Sepideh Aghamolaei</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01768">PDF</a><br/><b>Abstract: </b>Given a set of $k$ strings $I$, their longest common subsequence (LCS) is the
string with the maximum length that is a subset of all the strings in $I$. A
data-structure for this problem preprocesses $I$ into a data-structure such
that the LCS of a set of query strings $Q$ with the strings of $I$ can be
computed faster. Since the problem is NP-hard for arbitrary $k$, we allow an
error that allows some characters to be replaced by other characters.
</p>
<p>We define the approximation version of the problem with an extra input $m$,
which is the length of the regular expression (regex) that describes the input,
and the approximation factor is the logarithm of the number of possibilities in
the regex returned by the algorithm, divided by the logarithm regex with the
minimum number of possibilities.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01765</id>
    <link href="http://arxiv.org/abs/2008.01765" rel="alternate" type="text/html"/>
    <title>Bucket Oblivious Sort: An Extremely Simple Oblivious Sort</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Asharov:Gilad.html">Gilad Asharov</a>, T-H. Hubert Chan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nayak:Kartik.html">Kartik Nayak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pass:Rafael.html">Rafael Pass</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ren:Ling.html">Ling Ren</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shi:Elaine.html">Elaine Shi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01765">PDF</a><br/><b>Abstract: </b>We propose a conceptually simple oblivious sort and oblivious random
permutation algorithms called bucket oblivious sort and bucket oblivious random
permutation. Bucket oblivious sort uses $6n\log n$ time (measured by the number
of memory accesses) and $2Z$ client storage with an error probability
exponentially small in $Z$. The above runtime is only $3\times$ slower than a
non-oblivious merge sort baseline; for $2^{30}$ elements, it is $5\times$
faster than bitonic sort, the de facto oblivious sorting algorithm in practical
implementations.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01759</id>
    <link href="http://arxiv.org/abs/2008.01759" rel="alternate" type="text/html"/>
    <title>Tailoring for Every Body: Reshaping Convex Polyhedra</title>
    <feedworld_mtime>1596672000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Joseph O'Rourke, Costin Vilcu <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01759">PDF</a><br/><b>Abstract: </b>Given any two convex polyhedra P and Q, we prove as one of our main results
that the surface of P can be reshaped to a homothet of Q by a finite sequence
of "tailoring" steps. Each tailoring excises a digon surrounding a single
vertex and sutures the digon closed. One phrasing of this result is that, if Q
can be "sculpted" from P by a series of slices with planes, then Q can be
tailored from P. And there is a sense in which tailoring is finer than
sculpting in that P may be tailored to polyhedra that are not achievable by
sculpting P. It is an easy corollary that, if S is the surface of any convex
body, then any convex polyhedron P may be tailored to approximate a homothet of
S as closely as desired. So P can be "whittled" to e.g., a sphere S.
</p>
<p>Another main result achieves the same reshaping, but by excising more
complicated shapes we call "crests," still each enclosing one vertex. Reversing
either digon-tailoring or crest-tailoring leads to proofs that any Q inside P
can be enlarged to P by cutting Q and inserting and sealing surface patches.
</p>
<p>One surprising corollary of these results is that, for Q a subset of P, we
can cut-up Q into pieces and paste them non-overlapping onto an isometric
subset of P. This can be viewed as a form of "unfolding" Q onto P.
</p>
<p>All our proofs are constructive, and lead to polynomial-time algorithms.
</p></div>
    </summary>
    <updated>2020-08-06T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-08-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5510</id>
    <link href="https://adamsheffer.wordpress.com/2020/08/05/mind-benders-for-the-quarantined/" rel="alternate" type="text/html"/>
    <title>Mind-Benders for the Quarantined</title>
    <summary>Peter Winkler is a world expert on mathematical puzzles (he is also an excellent researcher and this year’s resident mathematician of the MoMath). I just learned about two things that he is currently up to. Winkler is running Mind-Benders for the Quarantined. After signing up to this free service, you receive a mathematical puzzle every […]</summary>
    <updated>2020-08-05T22:51:31Z</updated>
    <published>2020-08-05T22:51:31Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-08-06T05:21:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1091</id>
    <link href="http://corner.mimuw.edu.pl/?p=1091" rel="alternate" type="text/html"/>
    <title>Faster PageRank on MPC</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Years ago when I learned about Google PageRank algorithm, my first reaction was this is not the way it should be done! There should be some proof. This probably just shows that my CS education was too theoretical ;). Years … <a href="http://corner.mimuw.edu.pl/?p=1091">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Years ago when I learned about Google PageRank algorithm, my first reaction was this is not the way it should be done! There should be some proof. This probably just shows that my CS education was too theoretical ;). Years later I have learned that indeed there are some nice tools to argue about the running time of PageRank algorithm. And very recently we were able to give some new parallel (in MPC model) algorithms for computing vanilla PageRank. We improved the number of rounds needed from O(log n) to O(log^2 log n) time. You can hear Solbodan talking out it here:  <a href="https://www.youtube.com/watch?v=xoodhmjJ9Xs">https://www.youtube.com/watch?v=xoodhmjJ9Xs</a> .<br/> </p></div>
    </content>
    <updated>2020-08-05T18:55:13Z</updated>
    <published>2020-08-05T18:55:13Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2020-08-05T23:37:50Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/open-problem-all-pairs/</id>
    <link href="https://differentialprivacy.org/open-problem-all-pairs/" rel="alternate" type="text/html"/>
    <title>Open Problem: Private All-Pairs Distances</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Background:</strong> Suppose we are interested in computing the distance between two vertices in a graph. Under edge or node differential privacy, this problem is not promising because the removal of a single edge can make distances change from 1 to \(n − 1\) or can even disconnect the graph. However, a different setting that makes sense to consider is that of a weighted graph \((G, w)\) whose topology \(G = (V, E)\) is publicly known but edge weight function \(w : E \to \mathbb{R}^+\) must be kept private. (For instance, consider transit times on a road network. The topology of the road network may be publicly available as a map, but the edge weights corresponding to transit times may be based on private GPS locations of individual cars.)</p>

<p>Suppose that two weight functions \(w\) and \(w’\) of the same graph \(G\) are considered to be neighbors if they differ by at most 1 in \(\ell^1\) norm. Then the length of any fixed path is sensitivity-one, so the distance between any pair of vertices is also sensitivity-one and can be released privately via the Laplace mechanism. But what if we want to release all \(\Theta(n^2)\) distances between all pairs of vertices in \(G\)? We can do this with accuracy roughly \(O(n \log n )/\varepsilon\) by adding noise to each edge, or roughly \(O(n \sqrt{\log(1/\delta)}/\varepsilon)\) using composition theorems. Both of these are roughly \(n/\varepsilon\). But is this linear dependence on \(n\) inherent, or is it possible to release all-pairs distances with error sublinear in \(n\)?</p>

<p>This setting and question were considered in [<a href="https://arxiv.org/abs/1511.04631">S16</a>].</p>

<p><strong>Problem 1:</strong> Let \(G\) be an arbitrary public graph, and \(w : E \to \mathbb{R}^+\) be an edge weight function. Can we release approximate all-pairs distances in \((G, w)\) with accuracy sublinear in \(n\) while preserving the privacy of the edge weight function, where two weight functions \(w, w’\) are neighbors if \(\|w − w’\|_1 \le 1\)? Or can we show that any private algorithm must have error \(\Omega(n)\)? A weaker (but nontrivial) lower bound would also be nice.</p>

<p><strong>Reward:</strong> A bar of chocolate.</p>

<p><strong>Other related work:</strong> [<a href="https://arxiv.org/abs/1511.04631">S16</a>] provided algorithms with better error for two special cases, trees and graphs of a priori bounded weight. For trees, it is possible to release all-pairs distances with error roughly \(O(\log^{1.5} n)/\varepsilon\), while for arbitrary graphs with edge weights restricted to the interval \([0,M]\), it is possible to release all-pairs distances with error roughly \(O( \sqrt{nM\varepsilon^{-1}\log(1/\delta)})\)</p>

<p><em>Submitted by <a href="http://www.mit.edu/~asealfon/">Adam Sealfon</a> on April 9, 2019.</em></p></div>
    </summary>
    <updated>2020-08-05T18:00:00Z</updated>
    <published>2020-08-05T18:00:00Z</published>
    <author>
      <name>Audra McMillan</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-08-05T23:39:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=755</id>
    <link href="https://emanueleviola.wordpress.com/2020/08/05/previous-vs-concurrent-independent-work/" rel="alternate" type="text/html"/>
    <title>Previous vs. concurrent/independent work</title>
    <summary>Should Paper X cite Paper Y as previous or concurrent/independent work? This is sometimes tricky: maybe Paper Y circulated privately before Paper X, maybe the authors of Paper X knew about Paper Y maybe not — nobody can know for sure. One can say that the authors of Paper Y should have posted Paper Y […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Should Paper X cite Paper Y as previous or concurrent/independent work?  This is sometimes tricky: maybe Paper Y circulated privately before Paper X, maybe the authors of Paper X knew about Paper Y maybe not — nobody can know for sure.  One can say that the authors of Paper Y should have posted Paper Y online earlier to prevent this issue, but <a href="https://emanueleviola.wordpress.com/2014/07/02/only-papers-on-the-arxiv-can-be-submitted-for-publication/">that is not standard practice </a>and might lead to other problems, <a href="https://emanueleviola.wordpress.com/2016/08/10/paper-x-on-the-arxiv-keeps-getting-rejected/">including Paper Y never getting published!</a></p>



<p>I propose the following guiding principle:</p>



<p>“If a different accept/reject outcome would have forced paper X to cite paper Y as previous work, then paper X should cite paper Y as previous work.”<br/></p>



<p>The reasons behind my principle seem to me especially valid in the fast-moving theoretical computer science community, where papers are typically sent to conferences and thus seen by the entire program committee plus around 3 external referees, who are typically experts — only to be rejected.  Moreover, the progress is extremely fast, with the next conference cycle making obsolete a number of papers in just the previous cycle.</p></div>
    </content>
    <updated>2020-08-05T14:50:33Z</updated>
    <published>2020-08-05T14:50:33Z</published>
    <category term="Uncategorized"/>
    <category term="utopia-tcs"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2020-08-06T05:21:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01722</id>
    <link href="http://arxiv.org/abs/2008.01722" rel="alternate" type="text/html"/>
    <title>Well-Conditioned Methods for Ill-Conditioned Systems: Linear Regression with Semi-Random Noise</title>
    <feedworld_mtime>1596585600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jerry.html">Jerry Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sidford:Aaron.html">Aaron Sidford</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tian:Kevin.html">Kevin Tian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Huishuai.html">Huishuai Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01722">PDF</a><br/><b>Abstract: </b>Classical iterative algorithms for linear system solving and regression are
brittle to the condition number of the data matrix. Even a semi-random
adversary, constrained to only give additional consistent information, can
arbitrarily hinder the resulting computational guarantees of existing solvers.
We show how to overcome this barrier by developing a framework which takes
state-of-the-art solvers and "robustifies" them to achieve comparable
guarantees against a semi-random adversary. Given a matrix which contains an
(unknown) well-conditioned submatrix, our methods obtain computational and
statistical guarantees as if the entire matrix was well-conditioned. We
complement our theoretical results with preliminary experimental evidence,
showing that our methods are effective in practice.
</p></div>
    </summary>
    <updated>2020-08-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01616</id>
    <link href="http://arxiv.org/abs/2008.01616" rel="alternate" type="text/html"/>
    <title>Automorphism groups of maps in linear time</title>
    <feedworld_mtime>1596585600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kawarabayashi:Ken=ichi.html">Ken-ichi Kawarabayashi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mohar:Bojan.html">Bojan Mohar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nedela:Roman.html">Roman Nedela</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zeman:Peter.html">Peter Zeman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01616">PDF</a><br/><b>Abstract: </b>By a map we mean a $2$-cell decomposition of a closed compact surface, i.e.,
an embedding of a graph such that every face is homeomorphic to an open disc.
Automorphism of a map can be thought of as a permutation of the vertices which
preserves the vertex-edge-face incidences in the embedding. When the underlying
surface is orientable, every automorphism of a map determines an
angle-preserving homeomorphism of the surface. While it is conjectured that
there is no "truly subquadratic'' algorithm for testing map isomorphism for
unconstrained genus, we present a linear-time algorithm for computing the
generators of the automorphism group of a map, parametrized by the genus of the
underlying surface. The algorithm applies a sequence of local reductions and
produces a uniform map, while preserving the automorphism group. The
automorphism group of the original map can be reconstructed from the
automorphism group of the uniform map in linear time. We also extend the
algorithm to non-orientable surfaces by making use of the antipodal
double-cover.
</p></div>
    </summary>
    <updated>2020-08-05T23:25:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01590</id>
    <link href="http://arxiv.org/abs/2008.01590" rel="alternate" type="text/html"/>
    <title>List $k$-Colouring $P_t$-Free Graphs with No Induced $1$-Subdivision of $K_{1,s}$: a Mim-width Perspective</title>
    <feedworld_mtime>1596585600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brettell:Nick.html">Nick Brettell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munaro:Andrea.html">Andrea Munaro</a>, Daniel Paulusma <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01590">PDF</a><br/><b>Abstract: </b>A colouring of a graph $G=(V,E)$ is a mapping $c\colon V\to \{1,2,\ldots\}$
such that $c(u)\neq c(v)$ for every two adjacent vertices $u$ and $v$ of $G$.
The List $k$-Colouring problem is to decide whether a graph $G=(V,E)$ with a
list $L(u)\subseteq \{1,\ldots,k\}$ for each $u\in V$ has a colouring $c$ such
that $c(u)\in L(u)$ for every $u\in V$. Let $P_t$ be the path on $t$ vertices
and let $K_{1,s}^1$ be the graph obtained from the $(s+1)$-vertex star
$K_{1,s}$ by subdividing each of its edges exactly once. Recently, Chudnovsky,
Spirkl and Zhong proved that List $3$-Colouring is polynomial-time solvable for
$(K_{1,s}^1,P_t)$-free graphs for every $t\geq 1$ and $s\geq 1$. We generalize
their result to List $k$-Colouring for every $k\geq 1$. Our result also
generalizes the known result that for every $k\geq 1$ and $s\geq 0$, List
$k$-Colouring is polynomial-time solvable for $(sP_1+P_5)$-free graphs. We show
our result by proving that for every $k\geq 1$, $s\geq 1$, $t\geq 1$, the class
of $(K_k,K_{1,s}^1,P_t)$-free graphs has bounded mim-width and that a
corresponding branch decomposition is "quickly computable".
</p></div>
    </summary>
    <updated>2020-08-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01573</id>
    <link href="http://arxiv.org/abs/2008.01573" rel="alternate" type="text/html"/>
    <title>Top-k Connected Overlapping Densest Subgraphs in Dual Networks</title>
    <feedworld_mtime>1596585600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dondi:Riccardo.html">Riccardo Dondi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guzzi:Pietro_Hiram.html">Pietro Hiram Guzzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hosseinzadeh:Mohammad_Mehdi.html">Mohammad Mehdi Hosseinzadeh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01573">PDF</a><br/><b>Abstract: </b>Networks are largely used for modelling and analysing data and relations
among them. Recently, it has been shown that the use of a single network may
not be the optimal choice, since a single network may misses some aspects.
Consequently, it has been proposed to use a pair of networks to better model
all the aspects, and the main approach is referred to as dual networks (DNs).
DNs are two related graphs (one weighted, the other unweighted) that share the
same set of vertices and two different edge sets. In DNs is often interesting
to extract common subgraphs among the two networks that are maximally dense in
the conceptual network and connected in the physical one. The simplest instance
of this problem is finding a common densest connected subgraph (DCS), while we
here focus on the detection of the Top-k Densest Connected subgraphs, i.e. a
set k subgraphs having the largest density in the conceptual network which are
also connected in the physical network. We formalise the problem and then we
propose a heuristic to find a solution, since the problem is computationally
hard. A set of experiments on synthetic and real networks is also presented to
support our approach.
</p></div>
    </summary>
    <updated>2020-08-05T23:23:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01316</id>
    <link href="http://arxiv.org/abs/2008.01316" rel="alternate" type="text/html"/>
    <title>Fractional Pseudorandom Generators from the $k$th Fourier Level</title>
    <feedworld_mtime>1596585600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chattopadhyay:Eshan.html">Eshan Chattopadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gaitonde:Jason.html">Jason Gaitonde</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shetty:Abhishek.html">Abhishek Shetty</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01316">PDF</a><br/><b>Abstract: </b>In recent work by [CHHL19, CHLT19], the authors exhibit a flexible
construction of pseudorandom generators for classes of that satisfy $L_1$
Fourier bounds. [CHHL19] show that if a class satisfies such tail bounds at all
levels, this implies a PRG through their random walk framework that composes
together fractional PRGs that polarize quickly to the Boolean hypercube.
[CHLT19] show by derandomizing the analysis of [RT19] that just level-two
Fourier bounds suffice to construct a PRG but naturally obtain exponentially
worse dependence on the error in the seed length compared to [CHHL19].
Moreover, this derandomization relies on simulating nearly independent
Gaussians for the fractional PRG, which necessitates the polynomial dependence
on $1/\epsilon$ in each fractional step.
</p>
<p>In this work, we attempt to bridge the gap between these two results. We
partially answer an open question by [CHLT19] that nearly interpolates between
them. We show that if one has bounds up to the level-$k$ $L_1$ Fourier mass of
a closely related class of functions, where $k&gt;2$, one can obtain improved seed
length, the degree to which is determined by how high $k$ can be taken. Our
analysis shows that for error $\epsilon=1/\text{poly}(n)$, one needs control at
just level $O(\log n)$ to recover the seed length of [CHHL19], without
assumptions on the entire tail. We avoid this by providing a simple, alternate
analysis of their fractional PRG that instead relies on Taylor's theorem and
$p$-biased Fourier analysis to avoid assumptions on the weights of the
higher-order terms. This further allows us to show that this framework can
handle the class of low-degree polynomials over $\mathbb{F}_2$, with slightly
worse dependence than the state-of-the-art. We hope that this alternate
analysis will be fruitful in improving the understanding of this new and
powerful framework.
</p></div>
    </summary>
    <updated>2020-08-05T23:20:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-08-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01297</id>
    <link href="http://arxiv.org/abs/2008.01297" rel="alternate" type="text/html"/>
    <title>An improved Bayesian TRIE based model for SMS text normalization</title>
    <feedworld_mtime>1596585600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sikdar:Abhinava.html">Abhinava Sikdar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chatterjee:Niladri.html">Niladri Chatterjee</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01297">PDF</a><br/><b>Abstract: </b>Normalization of SMS text, commonly known as texting language, is being
pursued for more than a decade. A probabilistic approach based on the Trie data
structure was proposed in literature which was found to be better performing
than HMM based approaches proposed earlier in predicting the correct
alternative for an out-of-lexicon word. However, success of the Trie based
approach depends largely on how correctly the underlying probabilities of word
occurrences are estimated. In this work we propose a structural modification to
the existing Trie-based model along with a novel training algorithm and
probability generation scheme. We prove two theorems on statistical properties
of the proposed Trie and use them to claim that is an unbiased and consistent
estimator of the occurrence probabilities of the words. We further fuse our
model into the paradigm of noisy channel based error correction and provide a
heuristic to go beyond a Damerau Levenshtein distance of one. We also run
simulations to support our claims and show superiority of the proposed scheme
over previous works.
</p></div>
    </summary>
    <updated>2020-08-05T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2008.01252</id>
    <link href="http://arxiv.org/abs/2008.01252" rel="alternate" type="text/html"/>
    <title>Erratum: Fast and Simple Horizontal Coordinate Assignment</title>
    <feedworld_mtime>1596585600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brandes:Ulrik.html">Ulrik Brandes</a>, Julian Walter, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zink:Johannes.html">Johannes Zink</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2008.01252">PDF</a><br/><b>Abstract: </b>We point out two flaws in the algorithm of Brandes and K\"opf (Proc. GD
2001), which is often used for the horizontal coordinate assignment in
Sugiyama's framework for layered layouts. One of them has been noted and fixed
multiple times, the other has not been documented before and requires a
non-trivial adaptation. On the bright side, neither running time nor extensions
of the algorithm are affected adversely.
</p></div>
    </summary>
    <updated>2020-08-05T23:21:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-08-05T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/118</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/118" rel="alternate" type="text/html"/>
    <title>TR20-118 |  On Testing Asymmetry in the Bounded Degree Graph Model | 

	Oded Goldreich</title>
    <summary>We consider the problem of testing asymmetry in the bounded-degree graph model, where a graph is called asymmetric if the identity permutation is its only automorphism. Seeking to determine the query complexity of this testing problem, we provide partial results. Considering the special case of $n$-vertex graphs with connected components of size at most $s(n)=\Omega(\log n)$, we show that the query complexity of $\epsilon$-testing asymmetry (in this case) is at most $O({\sqrt n}\cdot s(n)/\epsilon)$, whereas the query complexity of $o(1/s(n))$-testing asymmetry (in this case) is at least $\Omega({\sqrt{n/s(n)}})$.

In addition, we show that testing asymmetry in the dense graph model is almost trivial.</summary>
    <updated>2020-08-04T21:41:44Z</updated>
    <published>2020-08-04T21:41:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-06T05:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=2561</id>
    <link href="https://francisbach.com/integration-by-parts-abel-transformation/" rel="alternate" type="text/html"/>
    <title>The many faces of integration by parts – I : Abel transformation</title>
    <summary>Integration by parts is a highlight of any calculus class. It leads to multiple classical applications for integration of logarithms, exponentials, etc., and it is the source of an infinite number of exercises and applications to special functions. In this post, I will look at a classical discrete extension that is useful in machine learning...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Integration by parts is a highlight of any calculus class. It leads to multiple classical applications for integration of logarithms, exponentials, etc., and it is the source of an infinite number of exercises and applications to <a href="https://en.wikipedia.org/wiki/Special_functions">special functions</a>. In this post, I will look at a classical discrete extension that is useful in machine learning and optimization, namely <a href="https://en.wikipedia.org/wiki/Summation_by_parts">Abel transformation</a>, with applications to convergence proofs for the (stochastic) <a href="https://en.wikipedia.org/wiki/Subgradient_method">subgradient method</a>. Next month, extensions to higher dimensions will be considered, with applications to score functions [<a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">2</a>, <a href="https://www.jstor.org/stable/1914309">3</a>] and randomized smoothing [4, <a href="https://arxiv.org/pdf/2002.08676">5</a>].</p>



<h2>Abel transformation: from continuous to discrete</h2>



<p class="justify-text">The most classical version of integration by parts goes as follows. Given two continuously differentiable functions from \(\mathbb{R}\) to \(\mathbb{R}\), we have: $$ \int_a^b \!\!\!\!f(x)g'(x) dx = \Big[ f(x) g(x) \Big]_a^b \!-\! \int_a^b\!\!\! \!f'(x) g(x) dx =  f(b) g(b)\, – f(a)g(a)-\! \int_a^b\! \!\!\! f'(x) g(x) dx.$$ This is valid for less regular functions, but this is not the main concern here. The proof follows naturally from the derivative of a product, but there is a nice “proof without words” (see, e.g., [1, p. 42] or <a href="https://en.wikipedia.org/wiki/Integration_by_parts#Visualization">here</a>).</p>



<p class="justify-text">There is a discrete analogue referred to as <a href="https://en.wikipedia.org/wiki/Summation_by_parts">Abel transformation</a> or summation by parts, where derivatives are replaced by increments: given two real-valued sequences \((a_n)_{n \geq 0}\) and \((b_n)_{n \geq 0}\) (the second sequence could also be taken vector-valued), we can expand $$ \sum_{k=1}^n a_k ( b_k\, – b_{k-1}) =\sum_{k=1}^n a_k  b_k \ – \sum_{k=1}^n a_k  b_{k-1} = \sum_{k=1}^n a_k b_k \ – \sum_{k=0}^{n-1} a_{k+1} b_{k},$$ using a simple index increment in the second sum.  Rearranging terms, this leads to $$ \sum_{k=1}^n a_k ( b_k\, – b_{k-1}) = a_n b_n \ – a_0 b_0\  – \sum_{k=0}^{n-1} ( a_{k+1} – a_{k } ) b_k.$$ In other words, we can transfer the first-order difference from the sequence \((b_k)_{k \geq 0}\) to the sequence \((a_k)_{k \geq 0}\).  A few remarks:</p>



<ul class="justify-text"><li><strong>Warning</strong>! It is very easy/common to make mistakes with indices and signs.</li><li>I gave the direct proof but a proof through explicit integration by part is also possible, by introducing the piecewise-constant function \(f\) equal to \(a_k\) on \([k,k+1)\), and \(g\) continuous  piecewise affine equal to \(b_{k} + (t-k) ( b_{k+1}-b_{k})\) for \(t \in [k,k+1]\), and integrating between \(0+\) and \(n+\). </li></ul>



<p class="justify-text">There are classical applications for the convergence of series (see <a href="https://en.wikipedia.org/wiki/Summation_by_parts">here</a>), but in this post, I will show how it can lead to an elegant result for stochastic gradient descent for non-smooth functions and <em>decaying</em> step-sizes.</p>



<h2>Decaying step-sizes in stochastic gradient descent</h2>



<p class="justify-text">The Abel summation formula is quite useful when analyzing optimization algorithms, and we give a simple example below. We consider a sequence of random potentially <em>non-smooth</em> convex functions \((f_k)_{k \geq 0}\) which are independent and identically distributed functions from \(\mathbb{R}^d \) to \(\mathbb{R}\), with expectation \(F\). The goal is to find a minimizer \(x_\ast\) of \(F\) over a some convex bounded set \(\mathcal{C}\), only being given access to some stochastic gradients of \(f_k\) at well-chosen points. The most classical example is supervised machine learning, where \(f_k(\theta)\) is the loss of a random observation for the predictor parameterized by \(\theta\).  The difficulty here is the potential non-smoothness of the function \(f_k\) (e.g., for the <a href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a> and the <a href="https://en.wikipedia.org/wiki/Support_vector_machine">support vector machine</a>).</p>



<p class="justify-text">We consider the projected stochastic subgradient descent method. The deterministic version of this method dates back to Naum Shor [6] in 1962 (see nice history <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/43_goffin-jean-louis.pdf">here</a>). The method goes as follows: starting from some \(\theta_0 \in \mathbb{R}^d\), we perform the iteration $$ \theta_{k} = \Pi_{ \mathcal{C} } \big( \theta_{k-1} – \gamma_k  \nabla f_k(\theta_{k-1}) \big),$$ where \(\Pi_{ \mathcal{C}}: \mathbb{R}^d \to \mathbb{R}^d\) is the orthogonal projection onto the set \(\mathcal{C}\), and \(\nabla f_k(\theta_{k-1})\) is any subgradient of \(f_k\) at \(\theta_{k-1}\). </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4324" height="211" src="https://francisbach.com/wp-content/uploads/2020/07/gradient_contours_projection-1024x410.png" width="528"/>One step of projected (sub)gradient descent: from a vector \(\theta\), we go down the direction of a negative subgradient \(\nabla f(\theta)\) of the function \(f\) (here typically a random function) and an orthogonal projection is performed to obtain the new vector \(\theta_+\).</figure></div>



<p class="justify-text">We make the following standard assumptions: (a) the set \(\mathcal{C}\) is convex and compact with diameter \(\Delta\) (with respect to the \(\ell_2\)-norm), (b) the functions \(f_k\) are almost surely convex and \(B\)-Lipschitz-continuous (or equivalently with gradients bounded in \(\ell_2\)-norm by \(B\)). We denote by \(\theta_\ast\) a minimizer of \(f\) on \(\mathcal{C}\) (there can be multiple ones). </p>



<p class="justify-text">For non-smooth problems, choosing a constant step-size does not lead to an algorithm converging to a global minimizer: decaying step-sizes are then needed.</p>



<h2>Convergence proof through Lyapunov functions</h2>



<p class="justify-text">Since the functions \(f_k\) are non-smooth, we cannot use Taylor expansions, and we rely on a now classical proof technique dating back from the 1960’s (see, e.g., a <a href="http://www.mathnet.ru/links/5d71a255cae8f1a313ac599b8f20a123/dan33049.pdf">paper</a> by Boris Polyak [7] in Russian), that has led to several extensions in particular for online learning [<a href="http://www.cs.cmu.edu/~maz/publications/techconvex.pdf">8</a>]. The proof relies on the concept of “<a href="https://en.wikipedia.org/wiki/Lyapunov_function">Lyapunov functions</a>“, often also referred to as “potential functions”. This is a non-negative function \(V(\theta_k)\) of the iterates \(\theta_k\), that is supposed to go down along iterations (at least in expectation). In optimization, standard Lyapunov functions are \(V(\theta)  = F(\theta)\, – F(\theta_\ast)\) or \(V(\theta) = \| \theta \ – \theta_\ast\|_2^2\). </p>



<p class="justify-text">For the subgradient method, we will not be able to show that the Lyapunov function is decreasing, but this will lead through a manipulation which is standard in linear dynamical system analysis to a convergence proof for the averaged iterate: that is, if \(V(\theta_k) \leqslant V(\theta_{k-1})\ – W(\theta_{k-1}) + \varepsilon_k\),  for a certain function \(W\) and extra positive terms \(\varepsilon_k\), then, using telescoping sums, $$ \frac{1}{n} \sum_{k=1}^n W(\theta_{k-1}) \leqslant \frac{1}{n} \big( V(\theta_0)\ – V(\theta_n) \big) + \frac{1}{n} \sum_{k=1}^n \varepsilon_k.$$ We can then either use <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">Jensen’s inequality</a> to get a bound on \(W \big( \frac{1}{n} \sum_{k=1}^n \theta_{k-1} \big)\), or directly get a bound on \(\min_{k \in \{1,\dots,n\}} W(\theta_{k-1})\). The first solution gives a performance guarantee for a well-defined iterate, while the second solution only shows that among the first \(n-1\) iterates, one of them has a performance guarantee; in the stochastic set-up where latex \(W\) is an expectation, it is not easily possible to know which one, so we will consider only averaging below.</p>



<p class="justify-text"><strong>Standard inequality. </strong>We have, by contractivity of orthogonal projections: $$ \|\theta_k \ – \theta_\ast\|_2^2 =  \big\|  \Pi_{ \mathcal{C} } \big( \theta_{k-1} – \gamma_k   \nabla f_k(\theta_{k-1}) \big) – \Pi_{ \mathcal{C} } (\theta_\ast)  \big\|_2^2 \leqslant  \big\|   \theta_{k-1} – \gamma_k  \nabla f_k(\theta_{k-1}) -\   \theta_\ast  \big\|_2^2.$$ We can then expand the squared Euclidean norm to get: $$ \|\theta_k – \theta_\ast\|_2^2 \leqslant  \|\theta_{k-1} – \theta_\ast\|_2^2 \ – 2\gamma_k (\theta_{k-1} – \theta_\ast)^\top \nabla f_k (\theta_{k-1}) + \gamma_k^2 \|  \nabla f_k(\theta_{k-1})\|_2^2.$$ The last term is upper-bounded by \(\gamma_k^2 B^2\) because of the regularity assumption on \(f_k\). For the middle term, we use the convexity of \(f_k\), that is,  the function \(f_k\) is greater than its tangent at \(\theta_{k-1}\). See figure below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4331" height="220" src="https://francisbach.com/wp-content/uploads/2020/07/tangent_convex-1-1024x440.png" width="513"/>Convex function above its tangent at \(\theta_{k-1}\), leading to the desired inequality.</figure></div>



<p class="justify-text">We then obtain $$ f_k(\theta_\ast) \geqslant f_k(\theta_{k-1}) + \nabla f_k(\theta_{k-1})^\top ( \theta_{\ast} – \theta_{k-1}).$$</p>



<p class="justify-text">Putting everything together, this leads to $$ \|\theta_k \ – \theta_\ast\|_2^2 \leqslant  \|\theta_{k-1}\  – \theta_\ast\|_2^2 \ – 2\gamma_k  \big[ f_k(\theta_{k-1}) \ – f_k(\theta_\ast) \big] + \gamma_k^2 B^2.$$ At this point, except the last term, all terms are random. We can now take expectations, with a particular focus on the term \(\mathbb{E} \big[ f_k(\theta_{k-1}) \big]\), for which we can use the fact that the random function \(f_k\) is independent from the past, so that $$ \mathbb{E} \big[ f_k(\theta_{k-1}) \big] =  \mathbb{E} \Big[  \mathbb{E} \big[ f_k(\theta_{k-1}) \big| f_{1},\dots,f_{k-1}  \big] \Big] =\mathbb{E} \big[   F(\theta_{k-1})   \big] . $$ We thus get $$ \mathbb{E} \big[ \|\theta_k – \theta_\ast\|_2^2\big] \leqslant  \mathbb{E} \big[ \|\theta_{k-1} – \theta_\ast\|_2^2\big]  – 2\gamma_k \big( \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast) \big) + \gamma_k^2 B^2.$$ As above, we can now isolate the excess in function values as: $$ \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast)  \leqslant \frac{1}{2 \gamma_k} \Big( \mathbb{E} \big[ \|\theta_{k-1} – \theta_\ast\|_2^2\big] – \mathbb{E} \big[ \|\theta_{k} – \theta_\ast\|_2^2\big] \Big) + \frac{\gamma_k}{2} B^2.$$ At this point, the “optimization part” of the proof is done. Only algebraic manipulations are needed to obtain a convergence rate. This is where Abel transformation will come in.</p>



<h2>From fixed horizon to anytime algorithms</h2>



<p class="justify-text"><strong>The lazy way.</strong> At this point, many authors (including me sometimes) will take a constant step-size \(\gamma_k = \gamma\) so as to obtain a telescopic sum, leading to $$ \frac{1}{n} \sum_{k=1}^n \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast) \leqslant \frac{1}{2n\gamma}     \Big( \mathbb{E} \big[ \|\theta_{0} \ – \theta_\ast\|_2^2\big] – \mathbb{E} \big[ \|\theta_{n}\  – \theta_\ast\|_2^2\big] \Big) + \frac{\gamma}{2} B^2,$$ which is less than \(\displaystyle \frac{\Delta^2}{2n \gamma} + \frac{\gamma}{2} B^2\), and minimized for \(\displaystyle \gamma = \frac{ \Delta}{B \sqrt{n}}\), leading to a convergence rate less than \(\displaystyle \frac{ B \Delta}{\sqrt{n}}\). Using Jensen’s inequality, we then get for \(\bar{\theta}_n = \frac{1}{n} \sum_{k=1}^n \theta_{k-1}\): $$\mathbb{E} \big[ F(\bar{\theta}_{n}) \big] – F(\theta_\ast) \leqslant \frac{ B \Delta}{\sqrt{n}} .$$ This result leads to the desired rate but can be improved in at least one way: the step-size currently has to depend on the “horizon” \(n\) (which has to be known in advance), and the algorithm is not “anytime”, which is not desirable in practice (where one often launches an algorithm and stops it when it the performance gains have plateaued or when the user gets bored waiting).</p>



<p class="justify-text"><strong>Non-uniform averaging.</strong> Another way [<a href="https://www2.isye.gatech.edu/~nemirovs/SIOPT_RSA_2009.pdf">9</a>] is to consider the non-uniform average $$ \eta_{k} =   \frac{\sum_{k=1}^n \gamma_{k} \theta_{k-1}}{\sum_{k=1}^n \gamma_{k}}, $$ for which telescoping sums apply as before, to get $$ \mathbb{E} \big[ F(\eta_k) \big] – F(\theta_\ast) \leqslant \frac{1}{2} \frac{\Delta^2 + B^2 \sum_{k=1}^n \gamma_k^2}{\sum_{k=1}^n \gamma_{k}}.$$  Then, by selecting a decaying step-size \(\displaystyle \gamma_k = \frac{ \Delta}{B \sqrt{k}}\), that depends on the iteration number, we get a rate proportional to \(\displaystyle \frac{ B \Delta}{\sqrt{n}} ( 1 + \log n)\). We now have an anytime algorithm, but we have lost a logarithmic term, which is not the end of the world, but still disappointing. In [<a href="https://www2.isye.gatech.edu/~nemirovs/SIOPT_RSA_2009.pdf">9</a>], “tail-averaging” (only averaging iterates between a constant times \(n\) and \(n\)) is proposed, that removes the logarithmic term but requires to store iterates (moreover, the non-uniform averaging puts too much weight on the first iterates, slowing down convergence).</p>



<p class="justify-text"><strong>Using Abel transformation.</strong> If we start to sum inequalities from \(k=1\) to \(k=n\), we get, with \(\delta_k = \mathbb{E} \big[ \|\theta_{k} – \theta_\ast\|_2^2\big]\) (which is always between \(0\) and \(\Delta^2\)): $$ \frac{1}{n} \sum_{k=1}^n \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast)  \leqslant  \frac{1}{n} \sum_{k=1}^n \bigg( \frac{1}{2 \gamma_k} \Big( \delta_{k-1} –  \delta_k \Big)\bigg) +  \frac{1}{n} \sum_{k=1}^n \frac{\gamma_k}{2} B^2,$$ which can be transformed through Abel transformation into $$ \frac{1}{n} \sum_{k=1}^n \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast) \leqslant \frac{1}{n} \sum_{k=1}^{n-1}  {\delta_k} \bigg(\frac{1}{ 2\gamma_{k+1}}- \frac{1}{ 2\gamma_{k}} \bigg) + \frac{\delta_0}{2 \gamma_1}- \frac{\delta_t}{2 \gamma_t}+ \frac{1}{n} \sum_{k=1}^n \frac{\gamma_k}{2} B^2.$$ For decreasing step-size sequences, this leads to $$ \frac{1}{n} \sum_{k=1}^n \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast) \leqslant \frac{1}{n} \sum_{k=1}^{n-1} {\Delta^2} \bigg(\frac{1}{ 2\gamma_{k+1}}- \frac{1}{ 2\gamma_{k}} \bigg) + \frac{\Delta^2}{2 \gamma_1}+ \frac{1}{n} \sum_{k=1}^n \frac{\gamma_k}{2} B^2,$$ and thus $$ \frac{1}{n} \sum_{k=1}^n \mathbb{E} \big[ F(\theta_{k-1}) \big] – F(\theta_\ast) \leqslant \frac{\Delta^2 }{2 n \gamma_n} + \frac{1}{n} \sum_{k=1}^n \frac{\gamma_k}{2} B^2.$$ For \(\gamma_k = \frac{  \Delta}{B \sqrt{k}}\), this leads to an upper bound $$\frac{\Delta B }{2 \sqrt{n}} \big( 1+ \frac{1}{\sqrt{n}} \sum_{k=1}^n \frac{1}{\sqrt{k}}\big) \leqslant \frac{3 \Delta B }{2 \sqrt{n}},$$ which is up to a factor \(\frac{3}{2}\) exactly the same bound as with a constant step-size, but now with an anytime algorithm.</p>



<h2>Experiments</h2>



<p class="justify-text">To illustrate the behaviors above, let’s consider minimizing \(\mathbb{E}_x \| x – \theta \|_1\), with respect to \(\theta\), with \(f_k(\theta) = \| x_k- \theta\|_1\), where \(x_k\) is sampled independently from a given distribution (here independent log-normal distributions for each coordinate). The global optimum \(\theta_\ast\) is the per-coordinate median of the distribution of \(x\)’s.</p>



<p class="justify-text">When applying SGD, the chosen subgradient of \(f_k\) has components in \(\{-1,1\}\). Hence in the plots below in two dimensions, the iterates are always on a grid. With a constant step-size: if the \(\gamma\) is too large (right), there are large oscillations, while if \(\gamma\) is too small (left), optimization is too slow. Note that while the SGD iterate with a constant step-size is always oscillating, the averaged iterate converges to some point (which is not the global optimum, and is typically at distance \(O(\gamma)\) away from it [<a href="https://arxiv.org/pdf/1707.06386">11</a>]).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4339" height="230" src="https://francisbach.com/wp-content/uploads/2020/07/sgd-1.gif" width="545"/>Stochastic gradient descent (averaged or not), with constant step-size. Left: small step-size. Right: large step-size (8 times larger).</figure></div>



<p class="justify-text">With a decaying step-size (figure below), the initial conditions are forgotten reasonably fast and the iterates converge to the global optimum (and of course, we get an anytime algorithm!).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4340" height="295" src="https://francisbach.com/wp-content/uploads/2020/07/sgd_decaying.gif" width="310"/>Stochastic gradient descent (averaged or not), with decreasing step-size.</figure></div>



<p>We can now compare in terms of function values, showing that a constant step-size only works well for a specific range of iteration numbers.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4335" height="276" src="https://francisbach.com/wp-content/uploads/2020/07/convergence_proofs.png" width="371"/>Comparison of expected performance for decaying and constant-step sizes. Several constant step-sizes are tested, with uniform spacings in log-scale (hence the the uniform spacings in performance for large \(n\)).</figure></div>



<h2>Conclusion</h2>



<p class="justify-text">Being able to deal with decaying step-sizes and anytime algorithms is arguably not a major improvement, but quite a satisfactory one, at least to me! Discrete integration by parts is the key enabler here.</p>



<p class="justify-text">There is another rewarding aspect which is totally unrelated to integration by parts: when applied to supervised machine learning, we just obtained from elementary principles (convexity) and few calculations a generalization bound <em>on unseen data</em>, which is as good as regular bounds from statistics [<a href="https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf">10</a>] that use much more complex tools such as <a href="https://en.wikipedia.org/wiki/Rademacher_complexity">Rademacher complexities</a> (but typically no convexity assumptions): here, statistics considered independently from optimization is not only slower (considering the empirical risk and minimizing it using the plain non-stochastic subgradient method would lead to an \(n\) times slower algorithm) but also more difficult to analyze! </p>



<h2>References</h2>



<p class="justify-text">[1] Roger B. Nelsen, <em>Proofs without Words: Exercises in Visual Thinking</em>, Mathematical Association of America, 1997.<br/>[2] Aapo Hyvärinen, <a href="http://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Estimation of non-normalized statistical models by score matching</a>. <em>Journal of Machine Learning Research</em>, <em>6</em>(Apr), 695-709, 2005.<br/>[3] Thomas M. Stoker, <a href="https://www.jstor.org/stable/1914309">Consistent estimation of scaled coefficients</a>.  <em>Econometrica: Journal of the Econometric Society</em>, 54(6):1461-1481, 1986.<br/>[4] Tamir Hazan, George Papandreou, and Daniel Tarlow. <a href="https://mitpress.mit.edu/books/perturbations-optimization-and-statistics">Perturbation, Optimization, and Statistics</a>. MIT Press, 2016.<br/>[5] Quentin Berthet, Matthieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach, <a href="https://arxiv.org/pdf/2002.08676">Learning with differentiable perturbed optimizers</a>. Technical report arXiv 2002.08676, 2020.<br/>[6] Naum Z. Shor. An application of the method of gradient descent to the solution of the network transportation problem. <em>Notes of Scientific Seminar on Theory and Applications of Cybernetics and Operations Research</em>, <em>Ukrainian Academy of Sciences</em>, Kiev, 9–17, 1962.<br/>[7] Boris T. Polyak, <a href="http://www.mathnet.ru/links/5d71a255cae8f1a313ac599b8f20a123/dan33049.pdf">A general method for solving extremal problems</a>. <em>Doklady Akademii Nauk SSSR</em>, 174(1):33–36, 1967.<br/>[8] Martin Zinkevich. <a href="http://www.cs.cmu.edu/~maz/publications/techconvex.pdf">Online convex programming and generalized infinitesimal gradient ascent</a>. <em>Proceedings of the international conference on machine learning )(ICML)</em>, 2003.<br/>[9] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, Alexander Shapiro<em>.</em> <a href="https://www2.isye.gatech.edu/~nemirovs/SIOPT_RSA_2009.pdf">Robust stochastic approximation approach to stochastic programming</a>. <em>SIAM Journal on optimization</em>, 19(4):1574-1609, 2009.<br/>[10] Stéphane Boucheron, Olivier Bousquet, Gabor Lugosi. <a href="https://www.esaim-ps.org/articles/ps/pdf/2005/01/ps0420.pdf">Theory of classification: A survey of some recent advances</a>. <em>ESAIM: probability and statistics</em>, <em>9</em>, 323-375, 2005.<br/>[11] Aymeric Dieuleveut, Alain Durmus, and Francis Bach. <a href="https://arxiv.org/pdf/1707.06386">Bridging the gap between constant step size stochastic gradient descent and Markov chains</a>. Annals of Statistics, 48(3):1348-1382, 2020.</p></div>
    </content>
    <updated>2020-08-04T15:55:26Z</updated>
    <published>2020-08-04T15:55:26Z</published>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-08-06T05:22:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/117</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/117" rel="alternate" type="text/html"/>
    <title>TR20-117 |  New bounds on the half-duplex communication complexity | 

	Alexander Smal, 

	Yuriy Dementiev, 

	Artur Ignatiev, 

	Vyacheslav Sidelnik, 

	Mikhail Ushakov</title>
    <summary>In this work, we continue the research started in [HIMS18], where the authors suggested to study the half-duplex communication complexity. Unlike the classical model of communication complexity introduced by Yao, in the half-duplex model, Alice and Bob can speak or listen simultaneously, as if they were talking using a walkie-talkie. The motivation for such a communication model comes from the study of the KRW conjecture. Following the open questions formulated in [HIMS18], we prove lower bounds for the disjointness function in all variants of half-duplex models and an upper bound in the half-duplex model with zero, that separates disjointness from the inner product function in this setting. Next, we prove lower and upper bounds on the half-duplex complexity of the Karchmer-Wigderson games for the counting functions and for the recursive majority function, adapting the ideas used in the classical communication complexity. Finally, we define the non-deterministic half-duplex complexity and establish bounds connecting it with non-deterministic complexity in the classical model.</summary>
    <updated>2020-08-04T15:11:42Z</updated>
    <published>2020-08-04T15:11:42Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-06T05:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1371</id>
    <link href="https://ptreview.sublinear.info/?p=1371" rel="alternate" type="text/html"/>
    <title>News for July 2020</title>
    <summary>We hope you’re all staying safe and healthy! To bring you some news (and distraction?) during this… atypical summer,here are the recent papers on property testing and sublinear algorithms we saw appear this month. Graphs, probability distributions, functions… there is a something for everyone. On Testing Hamiltonicity in the Bounded Degree Graph Model, by Oded […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We hope you’re all staying safe and healthy! To bring you some news (and distraction?) during this… atypical summer,here are the recent papers on property testing and sublinear algorithms we saw appear this month. Graphs, probability distributions, functions… there is a something for everyone.</p>



<p><strong>On Testing Hamiltonicity in the Bounded Degree Graph Model,</strong> by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2020/109/">ECCC</a>). The title sort of gives it away: this relatively short paper shows that testing whether an unknown bounded-degree graph has a Hamiltonian path (or Hamiltonian cycle) in the bounded-degree model requires a number of queries linear in \(n\), the number of nodes. The results also hold for directed graphs (with respect to directed Hamiltonian path or cycle), and are shown via a local reduction to a promise problem of satisfiability of 3CNF formulae. Also included: a complete proof of the linear lower bound for another problem, Independent Set Size; and an open problem: <em>what is the query complexity of testing graph isomorphism in the bounded-degree model?</em></p>



<p><strong>Local Access to Sparse Connected Subgraphs Via Edge Sampling</strong>, by Rogers Epstein (<a href="https://arxiv.org/abs/2007.05523">arXiv</a>). Given access to a connected graph \(G=(V,E)\), can we efficiently provide access to some <em>sparse</em> connected subgraph \(G’=(V,E’)\subseteq G\) with \(|E’|\ll |E|\)? This question, well-studied in particular for the case where \(G\) had bounded degree and the goal is to achieve \(|E’|\leq (1-\varepsilon)|V|\), is the focus of this paper which provides a trade-off between the query complexity of the oracle and \(|E’|\). Specifically, for every parameter \(T\), one can give oracle access to \(G’\) with \(|E’|=O(|V|T)\), with a query complexity \(=\tilde{O}(|E|/T)\). </p>



<p>Switching gears, we move from graphs to probability distributions:</p>



<p><strong>Tolerant Distribution Testing in the Conditional Sampling Model</strong>, by Shyam Narayanan (<a href="https://arxiv.org/abs/2007.09895">arXiv</a>). In the conditional sampling model for distribution testing, which we have covered a few times on this blog, the algorithm at each step gets to specify a subset \(S\) of the domain, and observe a sample from the distribution <em>conditioned on \(S\).</em> As it turns out, this can speed things up a <strong>lot</strong>: as Canonne, Ron, and Servedio (2015) showed, even tolerant uniformity testing, which with i.i.d. samples requires a near-linear (in the domain size \(n\)) number of samples, can be done in a <em>constant</em> number of conditional queries. Well, sort of constant: no dependence on \(n\), but the dependence on the distance parameter \(\varepsilon\) was, in CRS15, quite bad: \(\tilde{O}(1/\varepsilon^{20})\).  This work gets rid of this badness, and shows the (nearly) optimal \(\tilde{O}(1/\varepsilon^{2})\) query complexity! Among other results, it also generalizes it to tolerant identity testing  (\(\tilde{O}(1/\varepsilon^{4})\)), for which previously no constant-query upper bound was known. Things have become <em>truly</em> sublinear.</p>



<p>I<strong>nteractive Inference under Information Constraints</strong>, by Jayadev Acharya, Clément Canonne, Yuhan Liu, Ziteng Sun, and Himanshu Tyagi (<a href="https://arxiv.org/abs/2007.10976">arXiv</a>). Say you want to do uniformity/identity testing (or learn, but let’s focus on testing) on a discrete distribution, but you can’t actually observe the i.i.d. samples: instead, you can only do some sort of limited, “local” measurement on each sample. How hard is the task, compared to what you’d do if you fully had the samples? This setting, which captures things like distributed testing with communication or local privacy constraints, erasure channels, etc., was well-understood from previous recent work in the <em>non-adaptive</em> setting. But what if the “measurements” could be made <em>adaptively</em>? This paper shows general lower bounds for identity testing and learning, as a function of the type of local measurement allowed: as a corollary, this gives tight bounds for communication constraints and local privacy, and shows the first separation between adaptive and non-adaptive uniformity testing, for a type of “leaky” membership query measurement.</p>



<p><strong>Efficient Parameter Estimation of</strong> <strong>Truncated Boolean Product Distributions</strong>, by Dimitris Fotakis, Alkis Kalavasis, and Christos Tzamos (<a href="https://arxiv.org/abs/2007.02392">arXiv</a>). Suppose there is a fixed and unknown subset \(S\) of the hypercube, a “truncation” set, which you can only accessible via membership query; and you receive i.i.d. samples from an unknown product distribution on the hypercube, <em>truncated</em> on that set \(S\) (for instance, because your polling strategy or experimental measurements have limitations). Can you still learn that distribution efficiently? Can you test it for various properties, as you typically really would like to? (or is it just me?) This paper identifies some natural sufficient condition on \(S\), which they call <em>fatness</em>, under which the answer is a resounding <em>yes</em>. Specifically, if \(S\) satisfies this condition, one can actually generate honest-to-goodness i.i.d. samples (non-truncated) from the true distribution, given truncated samples! </p>



<p>Leaving distribution testing, our last paper is on testing functions in the <em>distribution-free</em> model:</p>



<p><strong>Downsampling for Testing and Learning in Product Distributions,</strong> by Nathaniel Harms and Yuichi Yoshida (<a href="https://arxiv.org/abs/2007.07449">arXiv</a>). Suppose you want to test (or learn) a class of Boolean functions \(\mathcal{C}\) over some domain \(\Omega^n\), with respect to some (unknown) product distribution (i.e., in the distribution-free testing model, or PAC-learning model). This paper develops a general technique, downsampling, which allows one to reduce such distribution-free testing of \(\mathcal{C}\) under a product distribution to testing \(\mathcal{C}\) over \([r]^d\) under the <em>uniform</em> distribution, for a suitable parameter \(r=r(d,\varepsilon,\mathcal{C})\). This allows the authors, among many other things and learning results, to easily re-establish (and, in the second case, improve upon) recent results on testing of monotonicity over \([n]^d\) (uniform distribution) and over \(\mathbb{R}^d\) (distribution-free). </p></div>
    </content>
    <updated>2020-08-04T02:59:57Z</updated>
    <published>2020-08-04T02:59:57Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-08-06T02:07:22Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17379</id>
    <link href="https://rjlipton.wordpress.com/2020/08/03/cleverer-automata-exist/" rel="alternate" type="text/html"/>
    <title>Cleverer Automata Exist</title>
    <summary>A breakthrough on the separating words problem Zachary Chase is a graduate student of Ben Green at Oxford. Chase has already solved a number of interesting problems–check his site for more details. His advisor is famous for his brilliant work—especially in additive combinatorics. One example is his joint work with Terence Tao proving this amazing […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A breakthrough on the separating words problem</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/?attachment_id=17371" rel="attachment wp-att-17371"><img alt="" class="alignright wp-image-17371" src="https://rjlipton.files.wordpress.com/2020/08/chase.png?w=125" width="125"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p/><p>
Zachary Chase is a graduate student of Ben Green at Oxford. Chase has already solved a number of interesting problems–check his <a href="http://people.maths.ox.ac.uk/~chase/">site</a> for more details. His advisor is famous for his brilliant work—especially in additive combinatorics. One example is his joint work with Terence Tao <a href="https://en.wikipedia.org/wiki/Green-Tao_theorem">proving</a> this amazing statement:</p>
<blockquote><p><b>Theorem 1</b> <em> The prime numbers contain arbitrarily long arithmetic progressions. </em>
</p></blockquote>
<p>
</p><p>
Today we wish to report Chase’s new <a href="https://arxiv.org/pdf/2007.12097.pdf">paper</a> on a problem we have twice discussed before. </p>
<p>
But first Ken wants to say something about Oxford where he got his degree long before Green arrived. </p>
<p>
</p><p/><h2> Oxford Making Waves </h2><p/>
<p/><p>
Green moved to Oxford in 2013. He holds a professorship associated to Magdalen College. I (Ken) did not know him when I started at Oxford in 1981. It would have been hard, as Green was only 4 years old at the time. But I did know the preteen Ruth Lawrence when she started there and even once played a departmental croquet match including her in which Bryan Birch made some epic long shots. Lawrence had <a href="https://en.wikipedia.org/wiki/Ruth_Lawrence">joined</a> St. Hugh’s College in 1983 at the age of twelve.</p>
<p>
Oxford has been Dick’s and my mind more in the past six years than before. Both of us were guests of Joël Ouaknine in 2012–2015 when he was there. Oxford has developed a front-line group in quantum computation, which fits as David Deutsch’s role as an originator began from there—note my story in the middle of this recent <a href="https://rjlipton.wordpress.com/2020/01/15/halting-is-poly-time-quantum-provable/">post</a>.</p>
<p>
Recently Oxford has been in the <a href="https://www.statnews.com/2020/07/20/study-provides-first-glimpse-of-efficacy-of-oxford-astrazeneca-covid-19-vaccine/">news</a> for developing a promising Covid-19 vaccine. <a href="https://www.precisionvaccinations.com/vaccines/chadox1-mers-coronavirus-vaccine">ChAdOx1</a> heads Wikipedia’s <a href="https://en.wikipedia.org/wiki/COVID-19_vaccine#Vaccine_candidates">list</a> of candidate vaccines and has gone to final <a href="https://www.nationalgeographic.com/science/2020/07/oxford-vaccine-enters-final-phase-of-covid-19-trials-in-brazil-cvd/">trials</a>, though there is still a long evaluation process before approval for general use.</p>
<p>
Before that, a modeling <a href="https://nymag.com/intelligencer/2020/03/oxford-study-coronavirus-may-have-infected-half-of-u-k.html">study</a> from Oxford in March raised the question of whether many more people have had Covid-19 without symptoms or any knowledge. This kind of possibility has since been <a href="https://marginalrevolution.com/marginalrevolution/2020/06/karl-friston-on-immunological-dark-matter.html">likened</a> to a “dark matter” hypothesis, not just now regarding Covid-19 but a decade <a href="https://pubmed.ncbi.nlm.nih.gov/21839767/">ago</a> and before. </p>
<p>
A main <a href="https://theconversation.com/coronavirus-techniques-from-physics-promise-better-covid-19-models-can-they-deliver-139925">supporting</a> <a href="https://www.brunel.ac.uk/news-and-events/news/articles/Coronavirus-techniques-from-physics-promise-better-COVID-19-models-can-they-deliver">argument</a> is that a wide class of mathematical models can be fitted with higher relative likelihood if the hypothesis is true. I have wanted to take time to evaluate this argument amid the wider backdrop of <a href="https://rjlipton.wordpress.com/2018/05/19/lost-in-complexity/">controversy</a> over inference methods in physics, but online chess with unfortunately ramped-up frequency of cheating has filled up all disposable time and more.</p>
<p>
</p><p/><h2> The Problem </h2><p/>
<p/><p>
Back to Chase’s new results on the following problem: </p>
<blockquote><p><b> </b> <em> Given two distinct binary strings of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> there is always a finite state deterministic automaton (FSA) that accepts one and rejects the other. <i>How few states can such a machine have?</i> </em>
</p></blockquote>
<p/><p>
This is called the <em>separating words problem</em> (SWP). Here we consider it for binary strings only.</p>
<p>
John Robson proved <img alt="{O(n^{2/5})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B2%2F5%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^{2/5})}"/> states are enough—we suppress any log factors. Some like to write this as <img alt="{\tilde{O}(n^{2/5})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BO%7D%28n%5E%7B2%2F5%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde{O}(n^{2/5})}"/>. Chase <a href="https://arxiv.org/pdf/2007.12097.pdf">improves</a> this to <img alt="{\tilde{O}(n^{1/3})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BO%7D%28n%5E%7B1%2F3%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde{O}(n^{1/3})}"/>:</p>
<blockquote><p><b>Theorem 2</b> <em><a name="Chasethm"/> For any distinct <img alt="{x,y \in \{0,1\}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy+%5Cin+%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x,y \in \{0,1\}^{n}}"/>, there is a finite state deterministic automaton with <img alt="{O(n^{1/3} \log^{7} n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B1%2F3%7D+%5Clog%5E%7B7%7D+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^{1/3} \log^{7} n)}"/> states that accepts <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x}"/> but not <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y}"/>. </em>
</p></blockquote>
<p/><p>
We previously discussed this twice at GLL. We discussed the background and early results <a href="https://rjlipton.wordpress.com/2019/09/08/separating-words-by-automata/">here</a>. The original problem is due to Pavel Goralcik and Vaclav Koubek. They proved an upper bound that was <img alt="{o(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bo%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{o(n)}"/>. Then we went over Robson’s bound <a href="https://rjlipton.wordpress.com/2019/09/16/separating-words-decoding-a-paper/">here</a>. The best upper bound was Robson’s result until Chase came along.</p>
<p>
</p><p/><h2> The Approach </h2><p/>
<p/><p>
All the approaches to SWP seem to have a common thread. They find some family of “hash” functions <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> so that:</p>
<ol>
<li>
Any <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> in <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> can be computed by a FSA with few states. <p/>
</li><li>
For any <img alt="{x \neq y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cneq+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x \neq y}"/> binary strings of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, there is an <img alt="{h \in H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh+%5Cin+H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h \in H}"/> so that <img alt="{h(x) \neq h(y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29+%5Cneq+h%28y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h(x) \neq h(y)}"/>.
</li></ol>
<p>
The challenge is to find clever families that can do do both. Be easy to compute and also be able to tell strings apart. Actually this is only a coarse outline—Chase’s situation is a bit more complicated. </p>
<p>
</p><p/><h2> The Proof </h2><p/>
<p/><p>
We have taken the statement of Theorem <a href="https://rjlipton.wordpress.com/feed/#Chasethm">2</a> verbatim from the paper. It has a common pecadillo of beginning a sentence for a specific <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> but writing <img alt="{O(\cdots n \cdots)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Ccdots+n+%5Ccdots%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\cdots n \cdots)}"/> later. However, this is how we think intuitively: in terms of how the pieces of the formula behave. Chase declares right away his intent to ignore the power of <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log n}"/>. How he gets the power <img alt="{1/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/3}"/> of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is the real point. We can convey the intuition in brief.</p>
<p>
A length-<img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> binary string can be identified with its set <img alt="{A \subseteq [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Csubseteq+%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A \subseteq [n]}"/> of positions where the string has a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. Chase begins by showing how a power of <img alt="{1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/2}"/> on <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is obtainable by considering sets of the form </p>
<p align="center"><img alt="\displaystyle  A_{i,p} = \{j : j \in A \wedge j \equiv i \pmod{p}\}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A_%7Bi%2Cp%7D+%3D+%5C%7Bj+%3A+j+%5Cin+A+%5Cwedge+j+%5Cequiv+i+%5Cpmod%7Bp%7D%5C%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A_{i,p} = \{j : j \in A \wedge j \equiv i \pmod{p}\}, "/></p>
<p>where <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> is prime and <img alt="{i &lt; p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%3C+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i &lt; p}"/>. Suppose we know a bound <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> such that for all distinct <img alt="{A,B \subseteq n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB+%5Csubseteq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B \subseteq n}"/> (that is, all distinct binary strings of legnth <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>) there is a prime <img alt="{p &lt; k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3C+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p &lt; k}"/> and <img alt="{i &lt; p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%3C+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i &lt; p}"/> such that </p>
<p align="center"><img alt="\displaystyle  |A_{i,p}| \neq |B_{i,p}|. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA_%7Bi%2Cp%7D%7C+%5Cneq+%7CB_%7Bi%2Cp%7D%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |A_{i,p}| \neq |B_{i,p}|. "/></p>
<p>Then by the Chinese Remainder Theorem, there is a prime <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> of magnitude about <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log n}"/> such that </p>
<p align="center"><img alt="\displaystyle  |A_{i,p}| \not\equiv |B_{i,p}| \pmod{q}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA_%7Bi%2Cp%7D%7C+%5Cnot%5Cequiv+%7CB_%7Bi%2Cp%7D%7C+%5Cpmod%7Bq%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |A_{i,p}| \not\equiv |B_{i,p}| \pmod{q}. "/></p>
<p>Now we can make a finite automaton <img alt="{M_{A,B}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM_%7BA%2CB%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M_{A,B}}"/> with states <img alt="{(j,\ell)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28j%2C%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(j,\ell)}"/> that always increments <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> modulo <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and increments <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell}"/> modulo <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> each time it reads a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> when <img alt="{j \equiv i \pmod{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj+%5Cequiv+i+%5Cpmod%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j \equiv i \pmod{p}}"/>. Then <img alt="{M_{A,B}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM_%7BA%2CB%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M_{A,B}}"/> has order-of <img alt="{pq \approx k\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bpq+%5Capprox+k%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{pq \approx k\log n}"/> states. The finisher is that <img alt="{k = \tilde{O}(n^{1/2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+%5Ctilde%7BO%7D%28n%5E%7B1%2F2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = \tilde{O}(n^{1/2})}"/> suffices. Again we ignore the pecadillo but we add some redundant words to the statement in the paper between dashes:</p>
<blockquote><p><b>Lemma 3</b> <em> For any distinct <img alt="{A,B \subseteq [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB+%5Csubseteq+%5Bn%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A,B \subseteq [n]}"/>—of size at most <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>—there is a prime <img alt="{p = \tilde{O}(n^{1/2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3D+%5Ctilde%7BO%7D%28n%5E%7B1%2F2%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{p = \tilde{O}(n^{1/2})}"/> such that for some <img alt="{i \in [p]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%5Cin+%5Bp%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i \in [p]}"/>, <img alt="{|A_{i,p}| \neq |B_{i,p}|.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CA_%7Bi%2Cp%7D%7C+%5Cneq+%7CB_%7Bi%2Cp%7D%7C.%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{|A_{i,p}| \neq |B_{i,p}|.}"/> </em>
</p></blockquote>
<p/><p>
The power <img alt="{1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/2}"/> is of course weaker than Robson’s <img alt="{2/5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%2F5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2/5}"/>, but this statement conceals two “<a href="https://rjlipton.wordpress.com/2011/08/05/give-me-a-lever/">levers</a>” that enable leap-frogging <img alt="{2/5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%2F5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2/5}"/> to get <img alt="{1/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/3}"/>. The first is that we don’t have to limit attention to sets <img alt="{A,B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B}"/> that come from places where the corresponding strings <img alt="{x,y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y}"/> have a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. Consider any string <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> and take <img alt="{A_w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_w%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_w}"/> to be the set of index positions <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> in which <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> has the substring <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> beginning at place <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/>. Define <img alt="{B_w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB_w%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B_w}"/> likewise for <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>. Then we can try to prove results of the following form given <img alt="{m &lt; n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3C+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m &lt; n}"/>:</p>
<blockquote><p><b>Proposition 4</b> <em> For all distinct <img alt="{x,y \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x,y \in \{0,1\}^n}"/> there is <img alt="{w \in \{0,1\}^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw+%5Cin+%5C%7B0%2C1%5C%7D%5Em%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{w \in \{0,1\}^m}"/> such that <img alt="{A_w \neq B_w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_w+%5Cneq+B_w%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A_w \neq B_w}"/> and </em></p><em>
<p align="center"><img alt="\displaystyle  |A_w|,|B_w| = O(\frac{n}{m}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7CA_w%7C%2C%7CB_w%7C+%3D+O%28%5Cfrac%7Bn%7D%7Bm%7D%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  |A_w|,|B_w| = O(\frac{n}{m}). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
A finite automaton using this extension needs <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> states to store <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> in its finite control. The second lever is to try to prove results of this form, where now the words “of size at most <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/>” are not redundant:</p>
<blockquote><p><b>Lemma 5 (?)</b> <em><a name="conjlemma"/> For any distinct <img alt="{A,B \subseteq [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB+%5Csubseteq+%5Bn%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A,B \subseteq [n]}"/> of size at most <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{N}"/> there is a prime <img alt="{p = \tilde{O}(N^{1/2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3D+%5Ctilde%7BO%7D%28N%5E%7B1%2F2%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{p = \tilde{O}(N^{1/2})}"/> such that for some <img alt="{i \in [p]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%5Cin+%5Bp%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i \in [p]}"/>, <img alt="{|A_{i,p}| \neq |B_{i,p}|.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CA_%7Bi%2Cp%7D%7C+%5Cneq+%7CB_%7Bi%2Cp%7D%7C.%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{|A_{i,p}| \neq |B_{i,p}|.}"/>  </em>
</p></blockquote>
<p/><p>
Now we need to balance the levers using the proposition and the lemma together.  Since <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> will add order-<img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> states to the automaton, we balance it against <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> from the previous argument.  So take <img alt="{m = n^{1/3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+n%5E%7B1%2F3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m = n^{1/3}}"/>. Then <img alt="{N = \frac{n}{m} \approx n^{2/3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+%5Cfrac%7Bn%7D%7Bm%7D+%5Capprox+n%5E%7B2%2F3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N = \frac{n}{m} \approx n^{2/3}}"/>. Lemma <a href="https://rjlipton.wordpress.com/feed/#conjlemma">5</a> then gives the bound </p>
<p align="center"><img alt="\displaystyle  k = \tilde{O}(N^{1/2}) = \tilde{O}(n^{1/3}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++k+%3D+%5Ctilde%7BO%7D%28N%5E%7B1%2F2%7D%29+%3D+%5Ctilde%7BO%7D%28n%5E%7B1%2F3%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  k = \tilde{O}(N^{1/2}) = \tilde{O}(n^{1/3}) "/></p>
<p>on the magnitude of the needed primes <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/>. This yields the <img alt="{\tilde{O}(n^{1/3})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7BO%7D%28n%5E%7B1%2F3%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde{O}(n^{1/3})}"/> breakthrough on SWP.</p>
<p/><p>
Here a famous New Yorker <a href="https://www.allposters.com/-sp/Oh-if-only-it-were-so-simple-New-Yorker-Cartoon-Posters_i9168200_.htm?UPI=PGQEG50&amp;PODConfigID=8419447&amp;sOrigID=169338">cartoon</a> with the caption “If only it were so simple” comes to mind.  But there is a catch. Chase is not quite able to prove lemma <a href="https://rjlipton.wordpress.com/feed/#conjlemma">5</a>. However, the <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> lever comes with extra flexibility that enables finding <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> that make <img alt="{A_w \neq B_w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_w+%5Cneq+B_w%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_w \neq B_w}"/> and also give those sets an extra regularity property <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>. Using <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>, he is able to show the existence of good hash functions of a certain type. The modified lemma is enough to prove his new bound.  The proof still uses intricate analysis including integrals.</p>
<p>
This is classic high-power mathematics. When some idea is blocked, try to weaken the requirements. Sometimes it is possible to still proceed. It is a lesson that we sometimes forget, but a valuable one nevertheless.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We like the SWP and think Chase’s contribution is impressive. Note that it adds a third element <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> to <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and <img alt="{q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q}"/> in the automaton.  Can the argument be pushed further by finding more levers to add more elements?  Is Lemma 5 true as stated, and with what (other) tradeoffs of <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> and <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> between it and Proposition 4?</p>
<p>
We feel there could also be interesting applications for his theorem as it stands. Is the ability to tell two strings apart with a simple device—a FSA with not many states—useful? Could it solve some open problem? It does seem like a basic insight, yet we have no candidate application. Perhaps you have an idea. </p>
<p/><p><br/>
[added Q on Lemma 5 to “Open Problems”, “lower” bound –&gt; “upper” bound in third section.]</p></font></font></div>
    </content>
    <updated>2020-08-03T14:50:54Z</updated>
    <published>2020-08-03T14:50:54Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="primes"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="trick"/>
    <category term="Ben Green"/>
    <category term="estimates"/>
    <category term="finite automata"/>
    <category term="hash functions"/>
    <category term="levers"/>
    <category term="log factors"/>
    <category term="Oxford"/>
    <category term="separating words problem"/>
    <category term="SWP"/>
    <category term="Zachary Chase"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-08-06T05:20:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsmath.wordpress.com/?p=2293</id>
    <link href="https://tcsmath.wordpress.com/2020/08/02/itcs-2021-call-for-papers/" rel="alternate" type="text/html"/>
    <title>ITCS 2021 Call for Papers</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The 12th Innovations in Theoretical Computer Science (ITCS) conference will be held online from January 6-8, 2021. The submission deadline is September 7, 2020. The program committee encourages you to send your papers our way! See the call for papers for information about submitting to the conference. ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing … <a class="more-link" href="https://tcsmath.wordpress.com/2020/08/02/itcs-2021-call-for-papers/">Continue reading <span class="screen-reader-text">ITCS 2021 Call for Papers</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <strong>12th Innovations in Theoretical Computer Science (ITCS)</strong> conference will be held <strong>online</strong> from <strong>January 6-8, 2021</strong>.   The <strong>submission deadline</strong> is <strong>September 7, 2020</strong>.</p>



<p>The <a href="http://itcs-conf.org/">program committee</a> encourages you to send your papers our way!  See the <a href="http://itcs-conf.org/">call for papers</a> for information about submitting to the conference.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the greater theory community.</p>



<p/>



<h3>Important dates</h3>



<ul><li><strong>Submission deadline: </strong> September 7, 2020 (05:59PM PDT) </li><li><strong>Notification to authors:</strong> November 1, 2020</li><li><strong>Conference dates: </strong>January 6-8, 2021</li></ul>



<p/>



<p/></div>
    </content>
    <updated>2020-08-03T02:06:40Z</updated>
    <published>2020-08-03T02:06:40Z</published>
    <category term="Announcement"/>
    <category term="ITCS"/>
    <category term="theory conference"/>
    <author>
      <name>James</name>
    </author>
    <source>
      <id>https://tcsmath.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsmath.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsmath.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsmath.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsmath.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>some mathematics &amp; computation</subtitle>
      <title>tcs math</title>
      <updated>2020-08-06T05:20:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1233384953933252805</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1233384953933252805/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/the-gauss-story-is-false-yet-we-still.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1233384953933252805" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1233384953933252805" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/the-gauss-story-is-false-yet-we-still.html" rel="alternate" type="text/html"/>
    <title>The Gauss story is false yet we still tell it. Should we?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
When teaching discrete math a while back I told the following story which some had already heard in High School: <br/>
<i><br/></i>
<i>When Gauss was in 1st grade the class was being bad. So the teacher made them sit down and add up the numbers from 1 to 100. Gauss did it in 2 minutes by noting that if S was the answer then </i><br/>
<i><br/></i>
<i>2S = (100+1) +(99+2) + ... + (1 + 100) = 100*101</i><br/>
<i><br/></i>
<i>So S = 50*101.  Then he went to Google and typed in 50*101 for the answer.</i><br/>
<i><br/></i>
The class laughed because of course the last part about Google was false. But I then told them that the entire story was false and showed them the following slides:  <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/gaussstory.pdf">here</a>  Take a look at them (there are only 4 of them) before reading on.<div><br/></div><div>(ADDED LATER: here is an article by Brian Hayes that documents the history of the story.</div><div><br/></div><div><a href="http://bit-player.org/wp-content/extras/bph-publications/AmSci-2006-05-Hayes-Gauss.pdf" target="_blank">http://bit-player.org/wp-content/extras/bph-publications/AmSci-2006-05-Hayes-Gauss.pdf</a></div><div><br/></div><div>)<br/>
<br/>
<br/>
So I told them the Gauss Story was false (I am right about this) and then told them a lie- that the story's progression over time was orderly. I then told them that that was false (hmmm- actually I might not of, oh well).<br/>
<br/>
One of my students emailed me this semester<br/>
<br/>
<i>Dr Gasarch- one of my Math professors is telling the Gauss story as if its true! You should make a public service announcement and tell people its false!</i><div><i><br/></i></div><div>I do not think this is needed. I also don't know how one goes about making a public service announcement  I also  suspect the teacher knew it was false but told it anyway.<div>
<br/>
OKAY- what do you do if you have a nice story that has some good MATH in it but  its not true?<br/>
<br/><b>Options:</b></div><div><br/></div><div>Tell it and let the students think its true.</div><div><br/></div><div>Tell it and debunk it.</div><div><br/></div><div>Tell it and debunk it and tell another myth</div><div><br/></div><div>Tell it and debunk it and tell another myth and then debunk that</div><div><br/></div><div>Ask your readers what they would do. Which I do now: What do you do? <br/><br/></div></div></div></div>
    </content>
    <updated>2020-08-03T02:02:00Z</updated>
    <published>2020-08-03T02:02:00Z</published>
    <author>
      <name>Unknown</name>
      <email>noreply@blogger.com</email>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-08-04T04:09:09Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/08/02/sona-enumeration</id>
    <link href="https://11011110.github.io/blog/2020/08/02/sona-enumeration.html" rel="alternate" type="text/html"/>
    <title>Sona enumeration</title>
    <summary>The last of my CCCG 2020 papers is now on the arXiv: “New Results in Sona Drawing: Hardness and TSP Separation”, arXiv:2007.15784, with Chiu, Demaine, Diomidov, Hearn, Hesterberg, Korman, Parada, and Rudoy. (As you might infer from the long list of coauthors, it’s a Barbados workshop paper.) The paper studies a mathematical formalization of the lusona drawings of southwest Africa; in this formalization, a sona curve for a given set of points is a curve that can be drawn in a single motion, intersecting itself only at simple crossings, and surrounding each given point in a separate region of the plane, with no empty regions. The paper proves that it’s hard to find the shortest one, hard even to find whether one exists when restricted to grid edges, and gives tighter bounds for the widest possible ratio between sona curve length and TSP tour length; see the preprint or the video I already posted for more information.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The last of my CCCG 2020 papers is now on the arXiv: “New Results in Sona Drawing: Hardness and TSP Separation”, <a href="https://arxiv.org/abs/2007.15784">arXiv:2007.15784</a>, with Chiu, Demaine, Diomidov, Hearn, Hesterberg, Korman, Parada, and Rudoy. (As you might infer from the long list of coauthors, it’s a Barbados workshop paper.) The paper studies a mathematical formalization of the <a href="https://en.wikipedia.org/wiki/Lusona">lusona</a> drawings of southwest Africa; in this formalization, a sona curve for a given set of points is a curve that can be drawn in a single motion, intersecting itself only at simple crossings, and surrounding each given point in a separate region of the plane, with no empty regions. The paper proves that it’s hard to find the shortest one, hard even to find whether one exists when restricted to grid edges, and gives tighter bounds for the widest possible ratio between sona curve length and TSP tour length; see the preprint or <a href="https://11011110.github.io/blog/2020/07/22/three-cccg-videos.html">the video I already posted</a> for more information.</p>

<p>To save this post from being content-free, here’s a research question that we didn’t even state in the paper, let alone make any progress on solving: just how many of these curves can a given set of points have? A sona curve can be described as a 4-regular plane multigraph (satisfying certain extra conditions) together with an assignment of the given points to its bounded faces, so there are finitely many of these things up to some sort of topological equivalence. And because this is topological it shouldn’t matter where the points are placed in the plane: the number of curves should be a function only of the number of points. I tried hand-enumerating the curves for up to three points but it was already messy and I’m not certain I got them all. (In an earlier version of this post I definitely didn’t get them all — I had to update the figure below after finding more.) Here are the ones I found:</p>

<p style="text-align: center;"><img alt="Sona curves for up to three points" src="https://11011110.github.io/blog/assets/2020/sona-enum.svg"/></p>

<p>If this hand enumeration is correct, then the numbers of sona curves for  labeled points form an integer sequence beginning  and the numbers for unlabeled points form a sequence beginning  but I don’t really know anything more than that for this problem.</p>

<p>Another research direction I don’t know much about yet: given a topological equivalence class of sona drawings, how can we find a good layout for it as an explicit drawing? There’s lots of research on drawing plane graphs nicely but it’s not clear how much of it carries over to making nice sona curves.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104624173377453724">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-08-02T23:48:00Z</updated>
    <published>2020-08-02T23:48:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-08-03T07:36:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/116</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/116" rel="alternate" type="text/html"/>
    <title>TR20-116 |  Toward better depth lower bounds: the XOR-KRW conjecture | 

	Alexander Smal, 

	Ivan Mihajlin</title>
    <summary>In this paper, we propose a new conjecture, the XOR-KRW conjecture, which is a relaxation of the Karchmer-Raz-Wigderson conjecture [KRW95]. This relaxation is still strong enough to imply $\mathbf{P} \not\subseteq \mathbf{NC}^1$ if proven. We also present a weaker version of this conjecture that might be used for breaking $n^3$ lower bound for De~Morgan formulas. Our study of this conjecture allows us to partially answer an open question stated in [GMWW17] regarding the composition of the universal relation with a function. To be more precise, we prove that there exists a function $g$ such that the composition of the universal relation with $g$ is significantly harder than just a universal relation. The fact that we can only prove the existence of $g$ is an inherent feature of our approach.
    
The paper's main technical contribution is a method of converting lower bounds for multiplexer-type relations into lower bounds against functions. In order to do this, we develop techniques to lower bound communication complexity using reductions from non-deterministic communication complexity and non-classical models: half-duplex and partially half-duplex communication models.</summary>
    <updated>2020-08-02T06:10:06Z</updated>
    <published>2020-08-02T06:10:06Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-06T05:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=19988</id>
    <link href="https://gilkalai.wordpress.com/2020/08/01/to-cheer-you-up-in-difficult-times-8-nathan-keller-and-ohad-klein-proved-tomaszewskis-conjecture-on-randomly-signed-sums/" rel="alternate" type="text/html"/>
    <title>To Cheer you up in Difficult Times 8: Nathan Keller and Ohad Klein Proved Tomaszewski’s Conjecture on Randomly Signed Sums</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Today we talk about the paper, Proof of Tomaszewski’s Conjecture on Randomly Signed Sums, by Nathan Keller and Ohad Klein. Consider a unit vector That is latex \sum_{i=1}^n a_i^2=1$. Consider all () signed sums where each is either 1 or … <a href="https://gilkalai.wordpress.com/2020/08/01/to-cheer-you-up-in-difficult-times-8-nathan-keller-and-ohad-klein-proved-tomaszewskis-conjecture-on-randomly-signed-sums/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Today we talk about the paper, <a href="https://arxiv.org/abs/2006.16834">Proof of Tomaszewski’s Conjecture on Randomly Signed Sums</a>, by Nathan Keller and Ohad Klein.</p>
<p>Consider a unit vector <img alt="a=(a_1,a_2,\dots, a_n)." class="latex" src="https://s0.wp.com/latex.php?latex=a%3D%28a_1%2Ca_2%2C%5Cdots%2C+a_n%29.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a=(a_1,a_2,\dots, a_n)."/> That is latex \sum_{i=1}^n a_i^2=1$. Consider all (<img alt="2^n" class="latex" src="https://s0.wp.com/latex.php?latex=2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^n"/>) signed sums <img alt="\displaystyle \epsilon_1a_1+\epsilon_2a_2+\cdots +\epsilon_n a_n, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cepsilon_1a_1%2B%5Cepsilon_2a_2%2B%5Ccdots+%2B%5Cepsilon_n+a_n%2C+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \epsilon_1a_1+\epsilon_2a_2+\cdots +\epsilon_n a_n, "/> where each <img alt="\epsilon_k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\epsilon_k"/> is either 1 or -1.</p>
<p><strong>Theorem (Keller and Klein (2020) asked by Boguslav Tomaszewski (1986)): </strong>For at least <img alt="2^{n-1}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bn-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{n-1}"/> signed sums <img alt="\displaystyle |\epsilon_1a_1+\epsilon_2a_2+\cdots +\epsilon_n a_n| \le 1 ." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7C%5Cepsilon_1a_1%2B%5Cepsilon_2a_2%2B%5Ccdots+%2B%5Cepsilon_n+a_n%7C+%5Cle+1+.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle |\epsilon_1a_1+\epsilon_2a_2+\cdots +\epsilon_n a_n| \le 1 ."/></p>
<p>Another way to state the theorem is that the probability of a signed sum to be in the interval [-1, 1] is at least 1/2.</p>
<p>To see that this is best possible consider the case that <img alt="n=2" class="latex" src="https://s0.wp.com/latex.php?latex=n%3D2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n=2"/> and let <img alt="a_1,a_2" class="latex" src="https://s0.wp.com/latex.php?latex=a_1%2Ca_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_1,a_2"/> be non zero. For the sum in question to exceed one we need both summands to have the same sign which happens half of the times. There is another example of importance, the vector <img alt="(\frac{1}{2}, \frac{1}{2}, \frac {1}{2}, \frac{1}{2})" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cfrac%7B1%7D%7B2%7D%2C+%5Cfrac%7B1%7D%7B2%7D%2C+%5Cfrac+%7B1%7D%7B2%7D%2C+%5Cfrac%7B1%7D%7B2%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(\frac{1}{2}, \frac{1}{2}, \frac {1}{2}, \frac{1}{2})"/>. Here 3/8 of the absolute values of signed sums (6 out of 16) are below 1 (in fact, equal to zero), 1/2 equal to 1 and 1/8 exceed 1. Holzman and Kleitman proved in 1992 that the fraction of absolute values of signed sums below 1 is always at least 3/8.</p>
<p>Congratulations to Nathan and Ohad. I will say a little more about the problem below but before that, a few more things.</p>
<h3>A few more things</h3>
<p>Luca Trevisan posted on his blog In Theory a post “Silver linings” about two cheerful pieces of news. The first one is “Karlin, Klein, and Oveis Gharan have just <a href="https://arxiv.org/abs/2007.01409">posted a paper</a> in which, at long last, they improve over the 1.5 approximation ratio for metric TSP which was achieved, in 1974, by Christofides.”</p>
<p>The second  one is about breaking the logarithmic barrier for Roth’s theorem that we wrote about here. This was also discussed by Bill <a>Gasarch </a>on Computational Complexity. In the comment section of my post there is an interesting discussion regarding timetable for future achievements and how surprising they would be.</p>
<p>The third is about Ron Graham, a friend and a mathematical giant who passed away a few days ago. Here is a <a href="https://www.facebook.com/fan.chung.9/posts/10220086655131549">moving post</a> by Fan Chung, <a href="http://www.math.ucsd.edu/~fan/ron/">a web page for Ron</a> set by Fan, and a <a href="https://rjlipton.wordpress.com/2020/07/10/ron-graham-1935-2020/">blog post by Dick and Ken on GLL</a>.</p>
<p>The fourth is that there is a nice collection of open problems on Boolean functions that is cited in the paper of Nathan and Ohad:  Y. Filmus, H. Hatami, S. Heilman, E. Mossel, R. O’Donnell, S. Sachdeva, A. Wan, and K. Wimmer, <a href="https://simons.berkeley.edu/sites/default/files/openprobsmerged.pdf">Real analysis in computer science: A collection of open problems</a>.</p>
<p>The fifth is that both our (HUJI) combinatorics seminar and basic notions seminar are running and are recorded. Here are the links. (Hmm, the links are not yet available, I will update.)</p>
<h2>Back to the result of Keller and Klein</h2>
<h3><a href="https://gilkalai.files.wordpress.com/2020/08/dkrh2.png"><img alt="" class="alignnone size-full wp-image-20034" height="365" src="https://gilkalai.files.wordpress.com/2020/08/dkrh2.png?w=640&amp;h=365" width="640"/></a></h3>
<p><span style="color: #ff0000;">Daniel Kleitman and Ron Holzman</span></p>
<h3>A quick orientation</h3>
<p>If the <img alt="a_i" class="latex" src="https://s0.wp.com/latex.php?latex=a_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_i"/>s are all the same, or small, or random, then to compute the probability that the weighted sum is between -1 and 1, we can use some Gaussian approximation and then we will find ourselves in a clash of constants that goes our way. The probability will be close to a constant well above 1/2. So what we need to understand is the case where some <img alt="a_i" class="latex" src="https://s0.wp.com/latex.php?latex=a_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_i"/>s are large.</p>
<h3>Early papers on the problem</h3>
<p>The problem first appeared in the American Math Monthly.  Richard Guy <a href="https://www.math.wisc.edu/~miller/res/fun-problem.pdf">collected several problems  and challenged the readers Any Answers Anent These Analytical Enigmas?</a> (I don’t know what the fate of the other questions is.)  Holzman and Kleitman <a href="https://holzman.technion.ac.il/files/2012/09/combsigns.pdf">proved in 1992</a> that the fraction of absolute values of signed sums below 1 is always at least 3/8, and this is tight. For many years, 3/8 was the record for the original problem, until  the 2017 paper by Ravi Boppana and Ron Holzman: <a href="https://arxiv.org/abs/1704.00350">Tomaszewski’s problem on randomly signed sums: Breaking the 3/8 barrier</a>, where a lower bound of 0.406, was proved. The current record 0f 0.46 was proved in the paper <a href="https://arxiv.org/abs/2005.05031">Improved Bound for Tomaszewski’s Problem</a> by Vojtěch Dvořák, Peter van Hintum, and Marius Tiba. The new definite result by Nathan and Ohad used some ideas of these early papers.</p>
<h3>What is the crux of matters</h3>
<p>Let me quote what the authors kindly wrote me:</p>
<blockquote><p><em>“The crux of the matter is how to deal with the case of very large coefficients (<img alt="a_1+a_2&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=a_1%2Ba_2%3E1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_1+a_2&gt;1"/>). We gave a short semi-inductive argument covering this case (this is Section 5 of the paper). The argument is only semi-inductive, as it requires the full assertion of Tomaszewski for any n'&lt;n, and gives only the case (<img alt="a_1+a_2&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=a_1%2Ba_2%3E1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a_1+a_2&gt;1"/>) for <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>. But this means that if we can handle all other cases by other methods then we will be done.</em></p>
<p><em>The semi-inductive argument takes only 3 pages. Handling the other cases takes 72 more pages and requires several new tools, but is closer to things that were done in previous works. (Actually, after we found the 3-page argument, we were quite sure we will be able to finalize the proof; this indeed happened, but took a year).”</em></p></blockquote>
<p>Most of the paper deals with the case of small coefficients. This requires several ideas and new tools.</p>
<h3>Rademacher sums: Improved Berry-Esseen and local tail inequalities</h3>
<p>If all coefficients are “sufficiently small”, then we can<br/>
approximate X by a Gaussian and the inequality should follow. However, using the standard <a href="https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem">Berry-Esseen bound</a>, this holds only if all coefficients are less than 0.16.<br/>
Nathan and Ohad showed that for Rademacher sums, namely random variables of the form <img alt="X=\sum a_i x_i" class="latex" src="https://s0.wp.com/latex.php?latex=X%3D%5Csum+a_i+x_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X=\sum a_i x_i"/>, as discussed in the conjecture, a stronger Berry-Esseen<br/>
bound can be obtained, and this bound shows immediately that Tomaszewski’s assertion holds whenever all coefficients are less than 0.31. The stronger bound stems<br/>
from a method of Prawitz, presented in the 1972 paper. H. Prawitz, <a href="https://www.tandfonline.com/doi/abs/10.1080/03461238.1972.10404645">Limits for a distribution, if the characteristic function is given in a finite domain</a>, which appeared in the Scandinavian <span style="color: #ff0000;">Actuarial</span> journal.</p>
<p>The second tool is local tail inequalities for Rademacher sums, of the form <img alt="\Pr[a&lt;X&lt;b] \leq \Pr[c&lt;X&lt;d]," class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5Ba%3CX%3Cb%5D+%5Cleq+%5CPr%5Bc%3CX%3Cd%5D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[a&lt;X&lt;b] \leq \Pr[c&lt;X&lt;d],"/> where <em>a,b,c,d</em> satisfy certain conditions. Inequalities of this kind were obtained before by Devroye and Lugosi in the 2008 paper:  <a href="https://arxiv.org/abs/0712.1686">Local tail bounds for functions of independent random variables</a>.</p>
<p>These local tail inequalities already have some other applications, e.g., to analysis of Boolean functions. They were developed and applied in an earlier paper paper of Keller and Klein: <a href="https://arxiv.org/abs/1710.07429">Biased halfspaces, noise sensitivity, and relative Chernoff inequalities</a>. Let me mention my related MO question <a href="https://mathoverflow.net/questions/85835/a-variance-tail-description-for-continuous-probability-distributions">A variance tail description for continuous probability distributions.</a></p>
<h3>A couple more ingredients</h3>
<p><strong>A  stopping time argument.</strong> Variants of Tomaszewski’s problem appeared in various fields. The problem was stated independently in a 2002 paper by Ben-Tal, Nemirovski, Roos, <a href="https://www2.isye.gatech.edu/~nemirovs/SIOPT_RQP_2002.pdf">Robust solutions of uncertain quadratic and conic-quadratic problems.</a>  A stopping time argument introduced there (for proving a lower bound of 1/3) played a crucial role in subsequent works and the critical semi-inductive argument by Nathan and Ohad.</p>
<p><strong>Refinements of the famous <a href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality">Chebyshev’s inequality</a>.</strong>  (Did you know Chebyshev’s full name? Ans: Pafnuty Lvovich Chebyshev.)</p>
<dl>
<dd/>
</dl>
<h3>Questions and connections that come to mind</h3>
<p><strong>Q1:</strong> What can be said about families <img alt="\cal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ccal+F&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\cal F"/> of signs that can serve as those signs for which  <img alt="|\epsilon_1a_1+\epsilon_2a_2+\cdots +\epsilon_n a_n| \le 1 ," class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cepsilon_1a_1%2B%5Cepsilon_2a_2%2B%5Ccdots+%2B%5Cepsilon_n+a_n%7C+%5Cle+1+%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\epsilon_1a_1+\epsilon_2a_2+\cdots +\epsilon_n a_n| \le 1 ,"/> for some vector <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a"/>.</p>
<p><strong>Q2:</strong> What can be said about the complex version or even more generally about high dimensions?</p>
<p><strong>Q3:</strong> Are there any relations to <a href="https://www.jstor.org/stable/1971442?seq=1">Littlewood-Offord type problems</a>?</p>
<p><strong>Q4:</strong> Is there any relation to the <a href="https://ocw.mit.edu/courses/mathematics/18-s096-topics-in-mathematics-of-data-science-fall-2015/projects/MIT18_S096F15_Open0.1.pdf">Komlos Conjecture</a>?</p>
<p>See also <a href="https://mathoverflow.net/questions/53669/anti-concentration-of-bernoulli-sums/53683#53683">this MO question</a> by Luca Trevisan and <a href="https://mathoverflow.net/questions/366894/a-rademacher-root-7-anti-concentration-inequality">this one</a> by George Lowther.</p>
<h3><span style="color: #ff0000;"><strong>Is there a simpler proof?</strong></span></h3>
<p>We can ask about simpler or just different proofs for almost every result we discuss here. But here the statement is so simple…</p>
<div class="thumb tright">
<div class="thumbinner"/>
</div>
<div class="thumb tright">
<div class="thumbinner">
<div class="thumbcaption"/>
</div>
</div></div>
    </content>
    <updated>2020-08-01T18:32:23Z</updated>
    <published>2020-08-01T18:32:23Z</published>
    <category term="Analysis"/>
    <category term="Combinatorics"/>
    <category term="Probability"/>
    <category term="Boguslav Tomaszewski"/>
    <category term="Nathan Keller"/>
    <category term="Ohad Klein"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-08-06T05:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/115</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/115" rel="alternate" type="text/html"/>
    <title>TR20-115 |  The Busy Beaver Frontier | 

	Scott Aaronson</title>
    <summary>The Busy Beaver function, with its incomprehensibly rapid growth, has captivated generations of computer scientists, mathematicians, and hobbyists. In this survey, I offer a personal view of the BB function 58 years after its introduction, emphasizing lesser-known insights, recent progress, and especially favorite open problems. Examples of such problems include: when does the BB function first exceed the Ackermann function? Is the value of BB(20) independent of set theory? Can we prove that BB(n+1)&gt;2^BB(n) for large enough n? Given BB(n), how many advice bits are needed to compute BB(n+1)? Do all Busy Beavers halt on all inputs, not just the 0 input? Is it decidable, given n, whether BB(n) is even or odd?</summary>
    <updated>2020-08-01T05:13:00Z</updated>
    <published>2020-08-01T05:13:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-08-06T05:20:39Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/07/31/linkage</id>
    <link href="https://11011110.github.io/blog/2020/07/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Gil Kalai on recent developments in Roth’s theorem (, see also). Salem and Spencer and later Behrend proved in the 1940s that subsets of with no triple in arithmetic progression can have nearly linear size, and Klaus Roth proved in 1953 that they must be sublinear. The upper bounds have slowly come down, to in this new result, but they’re still far from Behrend’s lower bound.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">Gil Kalai on recent developments in Roth’s theorem</a> (<a href="https://mathstodon.xyz/@11011110/104533566344469712"/>, <a href="https://blog.computationalcomplexity.org/2020/07/erdos-turan-for-k3-is-true.html">see also</a>). Salem and Spencer and later Behrend proved in the 1940s that <a href="https://en.wikipedia.org/wiki/Salem%E2%80%93Spencer_set">subsets of  with no triple in arithmetic progression</a> can have nearly linear size, and Klaus Roth proved in 1953 that <a href="https://en.wikipedia.org/wiki/Roth%27s_theorem_on_arithmetic_progressions">they must be sublinear</a>. The upper bounds have slowly come down, to  in this new result, but they’re still far from Behrend’s  lower bound.</p>
  </li>
  <li>
    <p><a href="https://cameroncounts.wordpress.com/2020/06/27/peter-sarnaks-hardy-lecture/">Peter Cameron describes Peter Sarnak’s Hardy Lecture</a> (<a href="https://mathstodon.xyz/@11011110/104539029304056696"/>). It’s on the spectral theory of graphs. If you know about this you probably already know that regular graphs with a big gap between the largest eigenvalue (degree) and the second largest are very good expander graphs. It turns out that 3-regular graphs with gaps elsewhere in their spectrum are also important in the theories of waveguides and fullerenes, and some tight bounds on where those gaps can be are now known.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.07983">Optimal angle bounds for quadrilateral meshes</a> (<a href="https://mathstodon.xyz/@11011110/104544859698397049"/>). Christopher J. Bishop meshes any simple polygon (why simple?) with max angle 120° and min angle max(60°, min of the polygon). Nice techniques involving conformal mapping, hyperbolic tessellation, and thick/thin decompositions of hyperbolic convex hulls of ideal sets. Also amusing to see him have to disambiguate my name from David B. A. Epstein’s within a single paragraph.</p>
  </li>
  <li>
    <p><a href="https://imgur.com/gallery/72lduu6">One-dimensional diagonal cellular automata generate Sierpinski carpets and intricate branching structures</a> (<a href="https://mathstodon.xyz/@11011110/104550317577491129"/>, <a href="https://community.wolfram.com/groups/-/m/t/1890120">see also</a>). Via the June 27 update to <a href="http://www.mathpuzzle.com/">mathpuzzle.com</a> which also has plenty of other neat stuff involving tilings, drawings of symmetric graphs, graceful labeling, rectangle dissection into similar rectangles, etc.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/a/366118">Terry Tao on mathematical notation</a> (<a href="https://mathstodon.xyz/@JordiGH/104552943941946623"/>), in response to a MathOverflow question about why there’s more than one way to write inner products.</p>
  </li>
  <li>
    <p><a href="http://matroidunion.org/?p=2693">Carmesin’s 3d version of Whitney’s planarity criterion</a> (<a href="https://mathstodon.xyz/@11011110/104567191288903767"/>): a simply-connected 2-dimensional simplicial complex (meeting a technical condition, “locality”) can be topologically embedded into Euclidean space if and only if a certain ternary matroid on its faces has a graphic dual. The proof relies on Perelman’s proof of the Poincaré conjecture! Simply-connected complexes are pretty restrictive but they include e.g. the cone over a graph, which embeds if and only if the graph is planar.</p>
  </li>
  <li>
    <p><a href="https://cp4space.wordpress.com/2020/07/24/fast-growing-functions-revisited/">Fast-growing functions revisited</a> (<a href="https://mathstodon.xyz/@11011110/104573335749947907"/>). News of recent developments relating the <a href="https://en.wikipedia.org/wiki/Busy_beaver">busy beaver function</a> with <a href="https://en.wikipedia.org/wiki/Graham%27s_number">Graham’s number</a>, and proofs of some older claims.</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1007/s12109-020-09750-0">Wikipedia, the free online medical encyclopedia anyone can plagiarize: Time to address wiki‑plagiarism</a> (<a href="https://mathstodon.xyz/@11011110/104576022559029845"/>, <a href="https://retractionwatch.com/2020/07/25/weekend-reads-image-duplication-software-debuts-papers-that-plagiarize-wikipedia-time-to-get-serious-about-research-fraud/">via</a>). In this editorial in <em>Publishing Research Quarterly</em>, Michaël R. Laurent identifies five PubMed-indexed papers that copied content from Wikipedia without crediting it (noting that this is much more prevalent in predatory book and journal publishing), and argues that doing this should be treated as a form of academic misconduct.</p>
  </li>
  <li>
    <p><a href="https://andrewducker.dreamwidth.org/3861716.html">Facebook temporarily blocks posts of links to dreamwidth</a> (<a href="https://mathstodon.xyz/@11011110/104581191674329045"/>, <a href="https://news.ycombinator.com/item?id=23956640">via</a>). Maybe it was just a mistake? And I guess the decentralization of Mastodon would make doing this to Mastodon posts somewhat harder. But this continued walling-off of the open web is not a good thing.</p>
  </li>
  <li>
    <p><a href="https://nebusresearch.wordpress.com/2020/07/23/my-all-2020-mathematics-a-to-z-fibonacci/">How much we don’t know about Fibonacci</a> (<a href="https://mathstodon.xyz/@nebusj/104581720945583863"/>). Entry F in Joseph Nebus’s 2020 mathematics A-to-Z.</p>
  </li>
  <li>
    <p><a href="https://cp4space.wordpress.com/2020/07/25/rational-dodecahedron-inscribed-in-unit-sphere/">Rational dodecahedron inscribed in unit sphere</a> (<a href="https://mathstodon.xyz/@11011110/104595876537307480"/>). It’s easy to inscribe a dodecahedron in the unit sphere: just use a regular one of the appropriate size. And it’s <a href="https://johncarlosbaez.wordpress.com/2011/09/12/fools-gold/">not hard to construct a dodecahedron combinatorially equivalent to the regular dodecahedron but with integer coordinates</a>. Now Adam Goucher shows how to do both at once, in answer to <a href="https://mathoverflow.net/q/234212/440">an old MathOverflow question</a>.</p>
  </li>
  <li>
    <p><em><a href="https://doi.org/10.1007/978-1-4612-5759-2">Descartes on Polyhedra</a></em> (<a href="https://mathstodon.xyz/@11011110/104606789909959594"/>, <a href="https://en.wikipedia.org/wiki/Descartes_on_Polyhedra">see also</a>). This book is mainly on whether Descartes (circa 1630) knew Euler’s formula  (before Euler in 1752, but after Maurolico in 1537). It also covers Descartes’ invention of polyhedral figurate numbers beyond the cubes and pyramidal ones known to the Greeks. Descartes’ manuscript has an interesting history: found after his death in a desk, sunk in the Seine, copied by Leibniz, both copies lost, and Leibniz’s copy finally rediscovered in 1860.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=yY9GAyJtuJ0">Spherical geometry is stranger than hyperbolic (in how it looks from an in-universe viewpoint)</a> (<a href="https://mathstodon.xyz/@11011110/104611506183926064"/>, <a href="https://news.ycombinator.com/item?id=24011727">via</a>).</p>
  </li>
</ul></div>
    </content>
    <updated>2020-07-31T21:03:00Z</updated>
    <published>2020-07-31T21:03:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-08-03T07:36:44Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/07/29/polyhedra-convex-unfoldings</id>
    <link href="https://11011110.github.io/blog/2020/07/29/polyhedra-convex-unfoldings.html" rel="alternate" type="text/html"/>
    <title>Polyhedra with convex unfoldings</title>
    <summary>My newest arXiv preprint is “Acutely triangulated, stacked, and very ununfoldable polyhedra” with Erik and Martin Demaine (arXiv:2007.14525). It’s about polyhedra with acute-triangle faces that cannot be unfolded without cutting their surface into many separate polygons. I already posted a video for the paper so see that for more information.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My newest arXiv preprint is “Acutely triangulated, stacked, and very ununfoldable polyhedra” with Erik and Martin Demaine (<a href="https://arxiv.org/abs/2007.14525">arXiv:2007.14525</a>). It’s about polyhedra with acute-triangle faces that cannot be unfolded without cutting their surface into many separate polygons. I <a href="https://11011110.github.io/blog/2020/07/22/three-cccg-videos.html">already posted a video for the paper</a> so see that for more information.</p>

<p>Instead, I thought I’d go into a little more detail about a throwaway remark in the video and the paper (one that I already got an email query about). It says that <a href="https://en.wikipedia.org/wiki/Ideal_polyhedron">ideal hyperbolic polyhedra</a> can always be unfolded (into the hyperbolic plane). These polyhedra are the hyperbolic convex hulls of finitely many limit points of the hyperbolic space; their faces are ideal polygons, glued together along entire hyperbolic lines. More strongly, if you cut an ideal polyhedron along any spanning tree of its vertices and edges, the result always unfolds into a convex ideal hyperbolic polygon. Here, for instance, is a net for an ideal cube:</p>

<p style="text-align: center;"><img alt="Net for an ideal cube" src="https://11011110.github.io/blog/assets/2020/ideal-cube-net.svg"/></p>

<p>I don’t know of a previous reference for this result, and the paper and video state it without proof (because it’s an introductory remark and not the topic of the paper), but it’s easy to prove a stronger statement by induction: any collection of ideal hyperbolic polygons (like the faces of an ideal polyhedron), when connected edge-to-edge in a complex with the connectivity of a tree (like the faces of any convex polyhedron when you cut it along a spanning tree), unfolds to an ideal convex polygon. As a base case, when you have one polygon in your collection, it unfolds to itself. When you have more than one, find a leaf polygon of the tree structure, remove it, and unfold the rest into a convex ideal polygon. Now add back the leaf. It needs to be connected to the rest of the complex along a hyperbolic line, which (by the induction hypothesis that the rest unfolds convexly) has the rest of the complex on one side and an empty hyperbolic halfplane on the other side. Any convex ideal polygon can be placed within this halfplane so that the side on which it should be glued matches up with the boundary line of the halfplane, with enough freedom to match up the points along this line that should be matched up.</p>

<p>This caused me to wonder: which Euclidean convex polyhedra have the same property, that cutting them along any spanning tree leads to a convex unfolding? The answer is: not very many. By <a href="https://en.wikipedia.org/wiki/Descartes%27_theorem_on_total_angular_defect">Descartes’ theorem on total angular defect</a>, the angular defects at the vertices of a convex polyhedron add up to . If a polyhedron is to have all spanning trees produce a (weakly) convex unfolding, then each vertex has to have angular defect at least , because otherwise cutting along a spanning tree that has a leaf at that vertex will make an unfolding that is non-convex at that vertex. And this is the only thing that can go wrong, because if all angular defects are at least  then the unfolding will be convex at each of its vertices and cannot self-overlap.</p>

<p>So to answer the question about Euclidean polyhedra with all unfoldings convex, we need only look for ways to partition the total angular defect of  among some set of vertices so that each one gets at least . If we know the defects of all the vertices and the distances between vertices, then by <a href="https://en.wikipedia.org/wiki/Alexandrov%27s_uniqueness_theorem">Alexandrov’s uniqueness theorem</a> the shape of the polyhedron will be determined. Since we’re using Alexandrov, we should also consider a <a href="https://en.wikipedia.org/wiki/Dihedron">dihedron</a> (two mirror-image convex faces glued at their edges) to be a special case of a polyhedron. This leaves, as the only cases:</p>

<ul>
  <li>
    <p>A triangular dihedron based on a right or acute triangle.</p>
  </li>
  <li>
    <p>A rectangular dihedron.</p>
  </li>
  <li>
    <p>A tetrahedron with angular defect exactly  at each vertex.</p>
  </li>
</ul>

<p style="text-align: center;"><img alt="Convex unfoldings of dihedra and a disphenoid" src="https://11011110.github.io/blog/assets/2020/convex-unfoldings.svg"/></p>

<p>The unfoldings of the dihedra have two copies of their face, mirrored across a joining edge. The tetrahedra with all-convex unfoldings are exactly the <a href="https://en.wikipedia.org/wiki/Disphenoid">disphenoids</a>, the tetrahedra whose four faces are congruent. They unfold either to a copy of the same face shape,
expanded by a factor of two in each dimension and creased into four copies along its <a href="https://en.wikipedia.org/wiki/Medial_triangle">medial triangle</a>, or a parallelogram, creased to form a strip of four congruent triangles. Their unfoldings were discussed by Jin Akiyama in his paper “Tile-makers and semi-tile-makers” (<em>American Mathematical Monthly</em> 2007, <a href="https://doi.org/10.1080/00029890.2007.11920450">doi:10.1080/00029890.2007.11920450</a>, <a href="https://www.jstor.org/stable/27642275">jstor:27642275</a>), as part of a broader investigation of polyhedra whose every unfolding tiles the plane.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104601177683049272">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-07-29T22:18:00Z</updated>
    <published>2020-07-29T22:18:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-08-03T07:36:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/07/29/junior-fellowship-andvanced-fellowship-at-eth-institute-for-theoretical-studies-in-zurich-apply-by-september-23-2020/</id>
    <link href="https://cstheory-jobs.org/2020/07/29/junior-fellowship-andvanced-fellowship-at-eth-institute-for-theoretical-studies-in-zurich-apply-by-september-23-2020/" rel="alternate" type="text/html"/>
    <title>Junior Fellowship / Andvanced Fellowship at ETH Institute for Theoretical Studies in Zurich (apply by September 23, 2020)</title>
    <summary>Junior and Advanced Fellows of the ETH Institute for Theoretical Studies are independent postdocs of exceptional talent and promise, having achieved significant results in mathematics, theoretical computer science or the theoretical natural sciences. Junior Fellows stay at the Institute for up to three years, Advanced Fellows up to five years. Website: https://eth-its.ethz.ch/fellows/nomination-of-junior-fellows1.html Email: nominations@eth-its.ethz.ch</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Junior and Advanced Fellows of the ETH Institute for Theoretical Studies are independent postdocs of exceptional talent and promise, having achieved significant results in mathematics, theoretical computer science or the theoretical natural sciences. Junior Fellows stay at the Institute for up to three years, Advanced Fellows up to five years.</p>
<p>Website: <a href="https://eth-its.ethz.ch/fellows/nomination-of-junior-fellows1.html">https://eth-its.ethz.ch/fellows/nomination-of-junior-fellows1.html</a><br/>
Email: nominations@eth-its.ethz.ch</p></div>
    </content>
    <updated>2020-07-29T14:52:31Z</updated>
    <published>2020-07-29T14:52:31Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-08-06T05:20:59Z</updated>
    </source>
  </entry>
</feed>
