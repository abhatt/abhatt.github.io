<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-06-19T17:23:14Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-819529627944448924</id>
    <link href="http://processalgebra.blogspot.com/feeds/819529627944448924/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=819529627944448924" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/819529627944448924" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/819529627944448924" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2019/06/postdoctoral-positions-at-gssi-on.html" rel="alternate" type="text/html"/>
    <title>Postdoctoral positions at the GSSI on modelling and verification for cyber-physical and smart systems</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The <a href="http://cs.gssi.it/">Computer Science group at the Gran Sasso Science Institute</a> (GSSI, an international PhD school and centre for advanced studies located in L’Aquila, Italy) has two available postdoctoral positions on topics related to modelling and verification for cyber-physical and smart systems. The positions are in the context of two PRIN projects funded by the Italian Ministry for University and Research, namely  "Designing Spatially Distributed Cyber-Physical Systems under Uncertainty (SEDUCE)" (Catia Trubiani, PI at the GSSI) and "IT MATTERS: Methods and Tools for Trustworthy Smart Systems" (Luca Aceto, PI at the GSSI).  Each position is for two years and the salary is 36,000€ per year before taxes. The positions are renewable for a third year if this is mutually agreeable.<br/><br/>The positions are briefly described below. We foresee a close collaboration between the members of the teams working on the two projects.   The application must be submitted through the online form available at <a href="https://applications.gssi.it/postdoc/">https://applications.gssi.it/postdoc/</a> by <b>Thursday, 11 July 2019 at 6 p.m. (Central European Time)</b>. For more information, please consult the Call for Applications at <a href="https://applications.gssi.it/postdoc/">https://applications.gssi.it/postdoc/ </a>or write an email to the PIs of the respective projects.<br/><br/>Note that the Computer Science group at the GSSI also has a further position financed by the institute, which can be filled in any of the research themes within the group. See <a href="http://cs.gssi.it/">http://cs.gssi.it</a> for more information.<br/><br/>Requirements: Candidates are expected to have a PhD degree in Computer Science or related disciplines. Preference will be given to applicants who have previous research experience on topics related to the themes of the project.<br/><br/>P<b>ostdoctoral position within the project "Designing Spatially Distributed Cyber-Physical Systems under Uncertainty"</b>, supervised by Catia Trubiani: <a href="https://cs.gssi.it/catia.trubiani">https://cs.gssi.it/catia.trubiani</a>.<br/><br/>Brief description of the project: Emerging scenarios such as autonomous vehicles and the Internet-of-Things require large-scale cyber-physical systems (CPS), i.e., computing devices that interact with the physical world. To cope with their complexity, model-based design has long been advocated as a prominent approach for their rigorous development. However, the state of the art does not adequately account for two major issues: space, to capture the distribution of CPS devices as well as their mobility; and uncertainty, e.g., to reflect lack of knowledge about the environment, the accuracy of the model, or errors occurring in the real world. Our goal is to develop modelling and analysis techniques for CPS where space and uncertainty are first-class citizens. We envisage a component-based framework where digital and physical components have locality and mobility features, and where uncertainty is captured by means of probabilistically distributed activities to describe their dynamics. We devise a system to specify spatio-temporal CPS requirements, turning them into probabilistic spatio-temporal logical specifications that will be at the basis of efficient algorithms for the analysis, verification, and synthesis. We will apply our techniques to real case studies on smart buildings and crowd-navigating robots.<br/><br/>Main activities: The successful candidate will develop a component-based modelling language for the specification of cyber physical systems, where digital components (i.e., the computational devices), need to co-exist with physical components (i.e., the dynamical models of the physical world). Each component will feature an appropriate location and mobility model. Uncertainty will be specified with either probabilistically distributed activities, or nondeterministically, leading to reasoning that will take into account worst- and best-case scenarios under any possible value of such uncertain actions. The modelling framework will allow the specification of spatio-temporal requirements, which establish the behavioural guarantees to be satisfied by the CPS under analysis.   The postdoctoral researcher will work with Catia Trubiani at the GSSI, and she/he will also collaborate with the project partners at IMT Lucca (Mirco Tribastone, coordinator of the project), University of Trieste (Luca Bortolussi and Stefano Seriani), and University of Camerino (Francesco Tiezzi).<br/><br/><br/><b>Postdoctoral position within the project "IT MATTERS: Methods and Tools for Trustworthy Smart Systems"</b> supervised by Luca Aceto: <a href="http://www.gssi.it/people/professors/lectures-computer-science/item/225-aceto-luca">http://www.gssi.it/people/professors/lectures-computer-science/item/225-aceto-luca</a>.<br/><br/>Brief description of the project: The goal of the project is the development and the application of a novel methodology for the specification, implementation and validation of trustworthy smart systems based on formal methods. We envisage system development in three steps by first providing and analysing system models to find design errors, then moving from models to executable code by translation into domain-specific programming languages and, finally, monitoring runtime execution to detect anomalous behaviours and to support systems in taking context-dependent decisions autonomously. Scientifically, the research will yield new, fundamental insights on the general properties of large scale, physically located, smart systems, leading to an end-to-end, principled approach to their design, implementation and deployment. The developed software tools and the work on the case studies will show the effectiveness of our proposed approach in three practical scenarios at different application scales.<br/><br/>Main activities: The successful candidate will develop runtime-monitoring and software-model-checking techniques for “smart systems”, that is, systems that take context-dependent decisions autonomously. The research activities will involve developing frameworks that can deal with the distributed nature of smart systems and will  build on existing work on specification-based monitoring and software model checking of cyber-physical systems. The postdoctoral researcher will also devise techniques to use information derived from the runtime analysis to guide the software-model-checking effort and to refine the models of the runtime environment used in model checking.    The postdoctoral researcher will work with Luca Aceto, Omar Inverso, Ludovico Iovino and Emilio Tuosto at the GSSI. He/she will also collaborate with the project partners at IMT Lucca, CNR Pisa, University of Camerino, University of Pisa and University of Udine. Moreover, he/she will also interact with the concurrency group at ICE-TCS (<a href="http://icetcs.ru.is/">http://icetcs.ru.is/</a>), Reykjavik University, led by Luca Aceto and Anna Ingólfsdóttir.</div>
    </content>
    <updated>2019-06-19T11:58:00Z</updated>
    <published>2019-06-19T11:58:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2019-06-19T11:58:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.07631</id>
    <link href="http://arxiv.org/abs/1906.07631" rel="alternate" type="text/html"/>
    <title>Topologically robust CAD model generation for structural optimisation</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ge Yin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xiao:Xiao.html">Xiao Xiao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cirak:Fehmi.html">Fehmi Cirak</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.07631">PDF</a><br/><b>Abstract: </b>Computer-aided design (CAD) models play a crucial role in the design,
manufacturing and maintenance of products. Therefore, the mesh-based finite
element descriptions common in structural optimisation must be first translated
into CAD models. Currently, this can at best be performed semi-manually. We
propose a fully automated and topologically accurate approach to synthesise a
structurally-sound parametric CAD model from topology optimised finite element
models. Our solution is to first convert the topology optimised structure into
a spatial frame structure and then to regenerate it in a CAD system using
standard constructive solid geometry (CSG) operations. The obtained parametric
CAD models are compact, that is, have as few as possible geometric parameters,
which makes them ideal for editing and further processing within a CAD system.
The critical task of converting the topology optimised structure into an
optimal spatial frame structure is accomplished in several steps. We first
generate from the topology optimised voxel model a one-voxel-wide voxel chain
model using a topology-preserving skeletonisation algorithm from digital
topology. The undirected graph defined by the voxel chain model yields a
spatial frame structure after processing it with standard graph algorithms.
Subsequently, we optimise the cross-sections and layout of the frame members to
recover its optimality, which may have been compromised during the conversion
process. At last, we generate the obtained frame structure in a CAD system by
repeatedly combining primitive solids, like cylinders and spheres, using
boolean operations. The resulting solid model is a boundary representation
(B-Rep) consisting of trimmed non-uniform rational B-spline (NURBS) curves and
surfaces.
</p></div>
    </summary>
    <updated>2019-06-19T01:22:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-06-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.07430</id>
    <link href="http://arxiv.org/abs/1906.07430" rel="alternate" type="text/html"/>
    <title>Rooting for phylogenetic networks</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huber:Katharina_T=.html">Katharina T. Huber</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iersel:Leo_van.html">Leo van Iersel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Janssen:Remie.html">Remie Janssen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Mark.html">Mark Jones</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moulton:Vincent.html">Vincent Moulton</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Murakami:Yukihiro.html">Yukihiro Murakami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Semple:Charles.html">Charles Semple</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.07430">PDF</a><br/><b>Abstract: </b>This paper studies the relationship between undirected (unrooted) and
directed (rooted) phylogenetic networks. We describe a polynomial-time
algorithm for deciding whether an undirected binary phylogenetic network, given
the locations of the root and reticulation vertices, can be oriented as a
directed phylogenetic network. Moreover, we give a mathematical
characterization of when this is the case and show that this directed
phylogenetic network is then always unique. These results are generalized to
the nonbinary case. In addition, we describe an algorithm for deciding whether
an undirected binary phylogenetic network can be oriented as a directed
phylogenetic network of a certain class. The algorithm is fixed-parameter
tractable (FPT) when the parameter is the level of the network and is
applicable to a wide range of network classes, including tree-child,
tree-based, stack-free and orchard networks. It can also be used to decide
whether an undirected phylogenetic network is tree-based and whether a
partly-directed phylogenetic network can be oriented as a directed phylogenetic
network. Finally, we show that, in general, it is NP-hard to decide whether an
undirected network can be oriented as a tree-based network.
</p></div>
    </summary>
    <updated>2019-06-19T01:20:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.07398</id>
    <link href="http://arxiv.org/abs/1906.07398" rel="alternate" type="text/html"/>
    <title>Inner Product Oracle can Estimate and Sample</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bishnu:Arijit.html">Arijit Bishnu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Arijit.html">Arijit Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mishra:Gopinath.html">Gopinath Mishra</a>, Manaswi Paraashar <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.07398">PDF</a><br/><b>Abstract: </b>Edge estimation problem in unweighted graphs using local and sometimes global
queries is a fundamental problem in sublinear algorithms. It has been observed
by Goldreich and Ron (Random Structures &amp; Algorithms, 2008), that weighted edge
estimation for weighted graphs require $\Omega(n)$ local queries, where $n$
denotes the number of vertices in the graph. To handle this problem, we
introduce a new inner product query on matrices. Inner product query
generalizes and unifies all previously used local queries on graphs used for
estimating edges. With this new query, we show that weighted edge estimation in
graphs with particular kind of weights can be solved using sublinear queries,
in terms of the number of vertices. We also show that using this query we can
solve the problem of the bilinear form estimation, and the problem of weighted
sampling of entries of matrices induced by bilinear forms. This work is the
first step towards weighted edge estimation mentioned in Goldreich and Ron
(Random Structures &amp; Algorithms, 2008).
</p></div>
    </summary>
    <updated>2019-06-19T01:20:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.07158</id>
    <link href="http://arxiv.org/abs/1906.07158" rel="alternate" type="text/html"/>
    <title>A Note on Sequences of Lattices</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Olariu:Emanuel_Florentin.html">Emanuel Florentin Olariu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.07158">PDF</a><br/><b>Abstract: </b>We investigate the relation between the convergence of a sequence of lattices
and the set-theoretic convergence of their corresponding Voronoi cells
sequence. We prove that if a sequence of full rank lattices converges to a full
rank lattice, then the closures of the limit infimum and limit supremum of the
Voronoi cells converges to the corresponding Voronoi cell. It remains an open
question if the converse is also true.
</p></div>
    </summary>
    <updated>2019-06-19T01:23:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-06-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.07105</id>
    <link href="http://arxiv.org/abs/1906.07105" rel="alternate" type="text/html"/>
    <title>Monotonically relaxing concurrent data-structure semantics for performance: An efficient 2D design framework</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rukundo:Adones.html">Adones Rukundo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Atalar:Aras.html">Aras Atalar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsigas:Philippas.html">Philippas Tsigas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.07105">PDF</a><br/><b>Abstract: </b>There has been a significant amount of work in the literature proposing
semantic relaxation of concurrent data structures for improving scalability and
performance. By relaxing the semantics of a data structure, a bigger design
space, that allows weaker synchronization and more useful parallelism, is
unveiled. Investigating new data structure designs, capable of trading
semantics for achieving better performance in a monotonic way, is a major
challenge in the area. We algorithmically address this challenge in this paper.
We present an efficient, lock-free, concurrent data structure design framework
for out-of-order semantic relaxation. Our framework introduces a new two
dimensional algorithmic design, that uses multiple instances of a given data
structure. The first dimension of our design is the number of data structure
instances operations are spread to, in order to benefit from parallelism
through disjoint memory access. The second dimension is the number of
consecutive operations that try to use the same data structure instance in
order to benefit from data locality. Our design can flexibly explore this
two-dimensional space to achieve the property of monotonically relaxing
concurrent data structure semantics for achieving better throughput performance
within a tight deterministic relaxation bound, as we prove in the paper. We
show how our framework can instantiate lock-free out-of-order queues, stacks,
counters and dequeues. We provide implementations of these relaxed data
structures and evaluate their performance and behaviour on two parallel
architectures. Experimental evaluation shows that our two-dimensional data
structures significantly outperform the respected previous proposed ones with
respect to scalability and throughput performance. Moreover, their throughput
increases monotonically as relaxation increases.
</p></div>
    </summary>
    <updated>2019-06-19T00:00:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06965</id>
    <link href="http://arxiv.org/abs/1906.06965" rel="alternate" type="text/html"/>
    <title>Matching Patterns with Variables</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manea:Florin.html">Florin Manea</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmid:Markus_L=.html">Markus L. Schmid</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06965">PDF</a><br/><b>Abstract: </b>A pattern p (i.e., a string of variables and terminals) matches a word w, if
w can be obtained by uniformly replacing the variables of p by terminal words.
The respective matching problem, i.e., deciding whether or not a given pattern
matches a given word, is generally NP-complete, but can be solved in
polynomial-time for classes of patterns with restricted structure. In this
paper we overview a series of recent results related to efficient matching for
patterns with variables, as well as a series of extensions of this problem.
</p></div>
    </summary>
    <updated>2019-06-18T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06851</id>
    <link href="http://arxiv.org/abs/1906.06851" rel="alternate" type="text/html"/>
    <title>Near Optimal Coflow Scheduling in Networks</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chowdhury:Mosharaf.html">Mosharaf Chowdhury</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khuller:Samir.html">Samir Khuller</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Purohit:Manish.html">Manish Purohit</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Sheng.html">Sheng Yang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/You:Jie.html">Jie You</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06851">PDF</a><br/><b>Abstract: </b>The coflow scheduling problem has emerged as a popular abstraction in the
last few years to study data communication problems within a data center. In
this basic framework, each coflow has a set of communication demands and the
goal is to schedule many coflows in a manner that minimizes the total weighted
completion time. A coflow is said to complete when all its communication needs
are met. This problem has been extremely well studied for the case of complete
bipartite graphs that model a data center with full bisection bandwidth and
several approximation algorithms and effective heuristics have been proposed
recently.
</p>
<p>In this work, we study a slightly different model of coflow scheduling in
general graphs (to capture traffic between data centers) and develop practical
and efficient approximation algorithms for it. Our main result is a randomized
2 approximation algorithm for the single path and free path model,
significantly improving prior work. In addition, we demonstrate via extensive
experiments that the algorithm is practical, easy to implement and performs
well in practice.
</p></div>
    </summary>
    <updated>2019-06-19T00:08:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06751</id>
    <link href="http://arxiv.org/abs/1906.06751" rel="alternate" type="text/html"/>
    <title>Pi-surfaces: products of implicit surfaces towards constructive composition of 3D objects</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raposo:Adriano_N=.html">Adriano N. Raposo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gomes:Abel_J=_P=.html">Abel J. P. Gomes</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06751">PDF</a><br/><b>Abstract: </b>Implicit functions provide a fundamental basis to model 3D objects, no matter
they are rigid or deformable, in computer graphics and geometric modeling. This
paper introduces a new constructive scheme of implicitly-defined 3D objects
based on products of implicit functions. This scheme is in contrast with
popular approaches like blobbies, meta balls and soft objects, which rely on
the sum of specific implicit functions to fit a 3D object to a set of spheres.
</p></div>
    </summary>
    <updated>2019-06-19T00:11:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06750</id>
    <link href="http://arxiv.org/abs/1906.06750" rel="alternate" type="text/html"/>
    <title>A concise guide to existing and emerging vehicle routing problem variants</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vidal:Thibaut.html">Thibaut Vidal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Laporte:Gilbert.html">Gilbert Laporte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matl:Piotr.html">Piotr Matl</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06750">PDF</a><br/><b>Abstract: </b>Vehicle routing problems have been the focus of extensive research over the
past sixty years, driven by their economic importance and their theoretical
interest. The diversity of applications has motivated the study of a myriad of
problem variants with different attributes. In this article, we provide a brief
survey of existing and emerging problem variants. Models are typically refined
along three lines: considering more relevant objectives and performance
metrics, integrating vehicle routing evaluations with other tactical decisions,
and capturing fine-grained yet essential aspects of modern supply chains. We
organize the main problem attributes within this structured framework. We
discuss recent research directions and pinpoint current shortcomings, recent
successes, and emerging challenges.
</p></div>
    </summary>
    <updated>2019-06-19T00:01:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06595</id>
    <link href="http://arxiv.org/abs/1906.06595" rel="alternate" type="text/html"/>
    <title>Learning Restricted Boltzmann Machines with Arbitrary External Fields</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goel:Surbhi.html">Surbhi Goel</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06595">PDF</a><br/><b>Abstract: </b>We study the problem of learning graphical models with latent variables. We
give the first algorithm for learning locally consistent (ferromagnetic or
antiferromagnetic) Restricted Boltzmann Machines (or RBMs) with {\em arbitrary}
external fields. Our algorithm has optimal dependence on dimension in the
sample complexity and run time however it suffers from a sub-optimal dependency
on the underlying parameters of the RBM.
</p>
<p>Prior results have been established only for {\em ferromagnetic} RBMs with
{\em consistent} external fields (signs must be
same)\cite{bresler2018learning}. The proposed algorithm strongly relies on the
concavity of magnetization which does not hold in our setting. We show the
following key structural property: even in the presence of arbitrary external
field, for any two observed nodes that share a common latent neighbor, the
covariance is high. This enables us to design a simple greedy algorithm that
maximizes covariance to iteratively build the neighborhood of each vertex.
</p></div>
    </summary>
    <updated>2019-06-19T00:08:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06591</id>
    <link href="http://arxiv.org/abs/1906.06591" rel="alternate" type="text/html"/>
    <title>Plane Sweep Algorithms for Data Collection in Wireless Sensor Network using Mobile Sink</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dash:Dinesh.html">Dinesh Dash</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06591">PDF</a><br/><b>Abstract: </b>Usage of mobile sink(s) for data gathering in wireless sensor networks(WSNs)
improves the performance of WSNs in many respects such as power consumption,
lifetime, etc. In some applications, the mobile sink $MS$ travels along a
predefined path to collect data from the nearby sensors, which are referred as
sub-sinks. Due to the slow speed of the $MS$, the data delivery latency is
high. However, optimizing the {\em data gathering schedule}, i.e. optimizing
the transmission schedule of the sub-sinks to the $MS$ and the movement speed
of the $MS$ can reduce data gathering latency. We formulate two novel
optimization problems for data gathering in minimum time. The first problem
determines an optimal data gathering schedule of the $MS$ by controlling the
data transmission schedule and the speed of the $MS$, where the data
availabilities of the sub-sinks are given. The second problem generalizes the
first, where the data availabilities of the sub-sinks are unknown. Plane sweep
algorithms are proposed for finding optimal data gathering schedule and data
availabilities of the sub-sinks. The performances of the proposed algorithms
are evaluated through simulations. The simulation results reveal that the
optimal distribution of data among the sub-sinks together with optimal data
gathering schedule improves the data gathering time.
</p></div>
    </summary>
    <updated>2019-06-19T00:15:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06432</id>
    <link href="http://arxiv.org/abs/1906.06432" rel="alternate" type="text/html"/>
    <title>Linear-time Hierarchical Community Detection</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rossi:Ryan_A=.html">Ryan A. Rossi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmed:Nesreen_K=.html">Nesreen K. Ahmed</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koh:Eunyee.html">Eunyee Koh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Sungchul.html">Sungchul Kim</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06432">PDF</a><br/><b>Abstract: </b>Community detection in graphs has many important and fundamental applications
including in distributed systems, compression, image segmentation,
divide-and-conquer graph algorithms such as nested dissection, document and
word clustering, circuit design, among many others. Finding these densely
connected regions of graphs remains an important and challenging problem. Most
work has focused on scaling up existing methods to handle large graphs. These
methods often partition the graph into two or more communities. In this work,
we focus on the problem of hierarchical community detection (i.e., finding a
hierarchy of dense community structures going from the lowest granularity to
the largest) and describe an approach that runs in linear time with respect to
the number of edges and thus fast and efficient for large-scale networks. The
experiments demonstrate the effectiveness of the approach quantitatively.
Finally, we show an application of it for visualizing large networks with
hundreds of thousands of nodes/links.
</p></div>
    </summary>
    <updated>2019-06-19T00:05:44Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06431</id>
    <link href="http://arxiv.org/abs/1906.06431" rel="alternate" type="text/html"/>
    <title>A New Family of Tractable Ising Models</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Likhosherstov:Valerii.html">Valerii Likhosherstov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maximov:Yury.html">Yury Maximov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chertkov:Michael.html">Michael Chertkov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06431">PDF</a><br/><b>Abstract: </b>We present a new family of zero-field Ising models over N binary
variables/spins obtained by consecutive "gluing" of planar and $O(1)$-sized
components along with subsets of at most three vertices into a tree. The
polynomial time algorithm of the dynamic programming type for solving exact
inference (partition function computation) and sampling consists of a
sequential application of an efficient (for planar) or brute-force (for
$O(1)$-sized) inference and sampling to the components as a black box. To
illustrate the utility of the new family of tractable graphical models, we
first build an $O(N^{3/2})$ algorithm for inference and sampling of the
K5-minor-free zero-field Ising models - an extension of the planar zero-field
Ising models - which is neither genus- nor treewidth-bounded. Second, we
demonstrate empirically an improvement in the approximation quality of the
NP-hard problem of the square-grid Ising model (with non-zero field) inference.
</p></div>
    </summary>
    <updated>2019-06-19T00:07:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06393</id>
    <link href="http://arxiv.org/abs/1906.06393" rel="alternate" type="text/html"/>
    <title>A Unified Framework of Robust Submodular Optimization</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Rishabh Iyer <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06393">PDF</a><br/><b>Abstract: </b>In this paper, we shall study a unified framework of robust submodular
optimization. We study this problem both from a minimization and maximization
perspective (previous work has only focused on variants of robust submodular
maximization). We do this under a broad range of combinatorial constraints
including cardinality, knapsack, matroid as well as graph based constraints
such as cuts, paths, matchings and trees. Furthermore, we also study robust
submodular minimization and maximization under multiple submodular upper and
lower bound constraints. We show that all these problems are motivated by
important machine learning applications including robust data subset selection,
robust co-operative cuts and robust co-operative matchings. In each case, we
provide scalable approximation algorithms and also study hardness bounds.
Finally, we empirically demonstrate the utility of our algorithms on real world
applications.
</p></div>
    </summary>
    <updated>2019-06-19T00:05:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.05404</id>
    <link href="http://arxiv.org/abs/1906.05404" rel="alternate" type="text/html"/>
    <title>Topology-Preserving Deep Image Segmentation</title>
    <feedworld_mtime>1560902400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hu:Xiaoling.html">Xiaoling Hu</a>, Li Fuxin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Samaras:Dimitris.html">Dimitris Samaras</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Chao.html">Chao Chen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05404">PDF</a><br/><b>Abstract: </b>Segmentation algorithms are prone to make topological errors on fine-scale
structures, e.g., broken connections. We propose a novel method that learns to
segment with correct topology. In particular, we design a continuous-valued
loss function that enforces a segmentation to have the same topology as the
ground truth, i.e., having the same Betti number. The proposed
topology-preserving loss function is differentiable and we incorporate it into
end-to-end training of a deep neural network. Our method achieves much better
performance on the Betti number error, which directly accounts for the
topological correctness. It also performs superiorly on other topology-relevant
metrics, e.g., the Adjusted Rand Index and the Variation of Information. We
illustrate the effectiveness of the proposed method on a broad spectrum of
natural and biomedical datasets.
</p></div>
    </summary>
    <updated>2019-06-19T00:12:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2592965474602859939</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2592965474602859939/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/why-does-nevalina-prize-now-abacus-got.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2592965474602859939" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2592965474602859939" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/why-does-nevalina-prize-now-abacus-got.html" rel="alternate" type="text/html"/>
    <title>Why does the Nevalina Prize (now Abacus) got to Algorithms/Complexity people</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In my post about the Nevanlinna prize  name change (see <a href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html">here</a>) one of my readers raised a different question about the prize:<br/>
<br/>
BEGIN QUOTE<br/>
<br/>
<div style="text-align: start;">
<br/></div>
<span>So there's one of my main questions about the prize answered (or at least resolved). The second remains. The IMU's website(which still refers to the Nevanlinna Prize) says that it is awarded "for outstanding contributions in Mathematical Aspects of Information Sciences including:"</span><br/>
<br/>
<span>1)All mathematical aspects of computer science, including complexity theory, logic of programming languages, analysis of algorithms, cryptography, computer vision, pattern recognition, information processing and modelling of intelligence.</span><br/>
<br/>
<span>2)Scientific computing and numerical analysis. Computational aspects of optimization and control theory. Computer algebra.</span><br/>
<br/>
<span>Correct me if I'm wrong, but it seems that the recognized work of the ten winners of the award all fits into two or three of the possible research areas for which the prize may be rewarded. Why do people think that this is the case?</span><br/>
<br/>
<br/>
<div style="text-align: justify;">
<span><span style="background-color: #ccff99; font-size: 14px;">END QUOTE</span></span></div>
<div style="text-align: justify;">
<span><span style="background-color: #ccff99; font-size: 14px;"><br/></span></span></div>
<div style="text-align: justify;">
<span><span style="background-color: #ccff99; font-size: 14px;">First off, lets see if this is true. Here is a list of all the winners:</span></span></div>
<div style="text-align: justify;">
<span><span style="background-color: #ccff99; font-size: 14px;"><br/></span></span></div>
<div style="text-align: justify;">
<span><span style="background-color: #ccff99; font-size: 14px;">Tarjan, Valiant, Razborov, Wigderson, Shor, Sudan, Kleinberg, Spielman, Khot, Daskalakis </span></span><br/>
<span><span style="background-color: #ccff99; font-size: 14px;"><br/></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">Yup, they all seem to be in Algorithms or Complexity.</span></span><br/>
<span><span style="background-color: #ccff99; font-size: 14px;"><br/></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">Speculation as to why:</span></span><br/>
<span><span style="background-color: #ccff99; font-size: 14px;"><br/></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">1) Algorithms and Complexity have problems with short descriptions that can easily be understood: Tarjan proved Planarity was in O(n) time. Valiant defined Sharp-P and showed the Permanent was Sharp-P complete. Hence it is easy to see what they have done. In many of the fields listed, while people have done great work, it may be harder to tell since the questions are not as clean.  If my way to do Vision is better than your way to do Vision, that may be hard to prove. And the proof  may need to be non-rigorous.</span></span><br/>
<span><span style="background-color: #ccff99; font-size: 14px;"><br/></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">2) If someone does great work in (for example) Logic of Programming Languages, it may not be recognized until she is past 40 years old. </span></span><br/>
<span><span style="background-color: #ccff99; font-size: 14px;"><br/></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">3) This one I am less sure of (frankly I'm not that sure of any of these and invite polite commentary): areas that are more practical may take longer to build and get to work.</span></span><br/>
<span><span style="background-color: #ccff99; font-size: 14px;"><br/></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">But there is still a problem with this. Numerical Analysis and Cryptography would seem to not have these problems. </span></span><br/>
<span><span style="background-color: #ccff99; font-size: 14px;"><br/></span></span>
<span><span style="background-color: #ccff99; font-size: 14px;">I invite the reader to speculate</span></span></div></div>
    </content>
    <updated>2019-06-18T00:59:00Z</updated>
    <published>2019-06-18T00:59:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-06-19T12:47:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/06/18/robustness-in-learning-and-statistics-past-and-future/</id>
    <link href="https://cstheory-events.org/2019/06/18/robustness-in-learning-and-statistics-past-and-future/" rel="alternate" type="text/html"/>
    <title>Robustness in Learning and Statistics: Past and Future</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">August 12-15, 2019 UC San Diego http://cseweb.ucsd.edu/~slovett/workshops/robust-statistics-2019/ Registration deadline: July 31, 2019 Robust statistics and related topics offer ways to stress test estimators to the assumptions they are making. It offers insights into what makes some estimators behave well in the face of model misspecification, while others do not. In this summer school, we will … <a class="more-link" href="https://cstheory-events.org/2019/06/18/robustness-in-learning-and-statistics-past-and-future/">Continue reading <span class="screen-reader-text">Robustness in Learning and Statistics: Past and Future</span></a></div>
    </summary>
    <updated>2019-06-18T00:13:18Z</updated>
    <published>2019-06-18T00:13:18Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-06-19T17:22:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.07115</id>
    <link href="http://arxiv.org/abs/1906.07115" rel="alternate" type="text/html"/>
    <title>Time-dependent Hamiltonian simulation with $L^1$-norm scaling</title>
    <feedworld_mtime>1560816000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berry:Dominic_W=.html">Dominic W. Berry</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Childs:Andrew_M=.html">Andrew M. Childs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Su:Yuan.html">Yuan Su</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Xin.html">Xin Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiebe:Nathan.html">Nathan Wiebe</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.07115">PDF</a><br/><b>Abstract: </b>The difficulty of simulating quantum dynamics depends on the norm of the
Hamiltonian. When the Hamiltonian varies with time, the simulation complexity
should only depend on this quantity instantaneously. We develop quantum
simulation algorithms that exploit this intuition. For the case of sparse
Hamiltonian simulation, the gate complexity scales with the $L^1$ norm
$\int_{0}^{t}\mathrm{d}\tau\left\lVert H(\tau)\right\lVert_{\max}$, whereas the
best previous results scale with $t\max_{\tau\in[0,t]}\left\lVert
H(\tau)\right\lVert_{\max}$. We also show analogous results for Hamiltonians
that are linear combinations of unitaries. Our approaches thus provide an
improvement over previous simulation algorithms that can be substantial when
the Hamiltonian varies significantly. We introduce two new techniques: a
classical sampler of time-dependent Hamiltonians and a rescaling principle for
the Schr\"{o}dinger equation. The rescaled Dyson-series algorithm is nearly
optimal with respect to all parameters of interest, whereas the sampling-based
approach is easier to realize for near-term simulation. By leveraging the
$L^1$-norm information, we obtain polynomial speedups for semi-classical
simulations of scattering processes in quantum chemistry.
</p></div>
    </summary>
    <updated>2019-06-18T23:24:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.07031</id>
    <link href="http://arxiv.org/abs/1906.07031" rel="alternate" type="text/html"/>
    <title>On the Strength of Uniqueness Quantification in Primitive Positive Formulas</title>
    <feedworld_mtime>1560816000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lagerkvist:Victor.html">Victor Lagerkvist</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nordh:Gustav.html">Gustav Nordh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.07031">PDF</a><br/><b>Abstract: </b>Uniqueness quantification ($\exists !$) is a quantifier in first-order logic
where one requires that exactly one element exists satisfying a given property.
In this paper we investigate the strength of uniqueness quantification when it
is used in place of existential quantification in conjunctive formulas over a
given set of relations $\Gamma$, so-called primitive positive definitions
(pp-definitions). We fully classify the Boolean sets of relations where
uniqueness quantification has the same strength as existential quantification
in pp-definitions and give several results valid for arbitrary finite domains.
We also consider applications of $\exists !$-quantified pp-definitions in
computer science, which can be used to study the computational complexity of
problems where the number of solutions is important. Using our classification
we give a new and simplified proof of the trichotomy theorem for the unique
satisfiability problem, and prove a general result for the unique constraint
satisfaction problem.
</p></div>
    </summary>
    <updated>2019-06-18T23:22:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06873</id>
    <link href="http://arxiv.org/abs/1906.06873" rel="alternate" type="text/html"/>
    <title>Running Time Analysis of the (1+1)-EA for Robust Linear Optimization</title>
    <feedworld_mtime>1560816000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bian:Chao.html">Chao Bian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qian:Chao.html">Chao Qian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Ke.html">Ke Tang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06873">PDF</a><br/><b>Abstract: </b>Evolutionary algorithms (EAs) have found many successful real-world
applications, where the optimization problems are often subject to a wide range
of uncertainties. To understand the practical behaviors of EAs theoretically,
there are a series of efforts devoted to analyzing the running time of EAs for
optimization under uncertainties. Existing studies mainly focus on noisy and
dynamic optimization, while another common type of uncertain optimization,
i.e., robust optimization, has been rarely touched. In this paper, we analyze
the expected running time of the (1+1)-EA solving robust linear optimization
problems (i.e., linear problems under robust scenarios) with a cardinality
constraint $k$. Two common robust scenarios, i.e., deletion-robust and
worst-case, are considered. Particularly, we derive tight ranges of the robust
parameter $d$ or budget $k$ allowing the (1+1)-EA to find an optimal solution
in polynomial running time, which disclose the potential of EAs for robust
optimization.
</p></div>
    </summary>
    <updated>2019-06-18T23:20:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06732</id>
    <link href="http://arxiv.org/abs/1906.06732" rel="alternate" type="text/html"/>
    <title>The SDP value for random two-eigenvalue CSPs</title>
    <feedworld_mtime>1560816000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mohanty:Sidhanth.html">Sidhanth Mohanty</a>, Ryan O'Donnell, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paredes:Pedro.html">Pedro Paredes</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06732">PDF</a><br/><b>Abstract: </b>We precisely determine the SDP value (equivalently, quantum value) of large
random instances of certain kinds of constraint satisfaction problems,
``two-eigenvalue 2CSPs''. We show this SDP value coincides with the spectral
relaxation value, possibly indicating a computational threshold. Our analysis
extends the previously resolved cases of random regular $\mathsf{2XOR}$ and
$\textsf{NAE-3SAT}$, and includes new cases such as random $\mathsf{Sort}_4$
(equivalently, $\mathsf{CHSH}$) and $\mathsf{Forrelation}$ CSPs. Our techniques
include new generalizations of the nonbacktracking operator, the Ihara--Bass
Formula, and the Friedman/Bordenave proof of Alon's Conjecture.
</p></div>
    </summary>
    <updated>2019-06-18T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.06361</id>
    <link href="http://arxiv.org/abs/1906.06361" rel="alternate" type="text/html"/>
    <title>Online Allocation and Pricing: Constant Regret via Bellman Inequalities</title>
    <feedworld_mtime>1560816000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vera:Alberto.html">Alberto Vera</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Banerjee:Siddhartha.html">Siddhartha Banerjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gurvich:Itai.html">Itai Gurvich</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06361">PDF</a><br/><b>Abstract: </b>We develop a framework for designing tractable heuristics for Markov Decision
Processes (MDP), and use it to obtain constant regret policies for a variety of
online allocation problems, including online packing, budget-constrained
probing, dynamic pricing, and online contextual bandits with knapsacks. Our
approach is based on adaptively constructing a benchmark for the value
function, which we then use to select our actions. The centerpiece of our
framework are the Bellman Inequalities, which allow us to create benchmarks
which both have access to future information, and also, can violate the
one-step optimality equations (i.e., Bellman equations). The flexibility of
balancing these allows us to get policies which are both tractable and have
strong performance guarantees -- in particular, our constant-regret policies
only require solving an LP for selecting each action.
</p></div>
    </summary>
    <updated>2019-06-18T23:23:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-18T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/</id>
    <link href="https://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/" rel="alternate" type="text/html"/>
    <title>3rd School on Foundations of Programming and Software Systems</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">September 10-15, 2019 Warsaw, Poland https://www.mimuw.edu.pl/~fopss19/ The Summer School on Foundations of Programming and Software Systems (FoPSS) was jointly created by EATCS, ETAPS, ACM SIGLOG and ACM SIGPLAN. It was first organised in 2017. The goal is to introduce the participants to various aspects of computation theory and programming languages. The school, spread over a … <a class="more-link" href="https://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/">Continue reading <span class="screen-reader-text">3rd School on Foundations of Programming and Software Systems</span></a></div>
    </summary>
    <updated>2019-06-17T12:23:27Z</updated>
    <published>2019-06-17T12:23:27Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-06-19T17:22:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16001</id>
    <link href="https://rjlipton.wordpress.com/2019/06/17/contraction-and-explosion/" rel="alternate" type="text/html"/>
    <title>Contraction and Explosion</title>
    <summary>Different ways of recursing on graphs Bletchley Park 2017 source William Tutte was a British combinatorialist and codebreaker. He worked in a different group at Bletchley Park from that of Alan Turing. He supplied several key insights and algorithms for breaking the Lorenz cipher machine. His algorithms were implemented alongside Turing’s on Colossus code-breaking computers. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Different ways of recursing on graphs</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<img alt="TutteBletchley" class="alignright wp-image-16003" height="200" src="https://rjlipton.files.wordpress.com/2019/06/tuttebletchley.jpg?w=120&amp;h=200" width="120"/>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Bletchley Park 2017 <a href="https://bletchleypark.org.uk/news/codebreaker-bill-tutte-to-be-celebrated-in-centenary-exhibition">source</a></font></td>
</tr>
</tbody>
</table>
<p>
William Tutte was a British combinatorialist and codebreaker. He worked in a different group at Bletchley Park from that of Alan Turing. He supplied several key insights and algorithms for breaking the Lorenz cipher <a href="https://en.wikipedia.org/wiki/Lorenz_cipher">machine</a>. His algorithms were implemented alongside Turing’s on <a href="https://en.wikipedia.org/wiki/Colossus_computer">Colossus</a> code-breaking computers.</p>
<p>
Today we discuss graph recursions discovered by Tutte and Hassler Whitney.</p>
<p>
Tutte wrote a doctoral thesis after the war on graph theory and its generalization into <em>matroid theory</em>. We will follow the same arc in this and a followup post. He joined the faculty of the universities of Toronto and then Waterloo, where he was active long beyond his retirement. </p>
<p>
For more on Tutte and his work, see this <a href="https://theconversation.com/remembering-bill-tutte-another-brilliant-codebreaker-from-world-war-ii-77556">article</a> and <a href="http://thelaborastory.com/stories/william-thomas-tutte/">lecture</a> by Graham Farr, who is a professor at Monash University and a longtime friend of Ken’s from their Oxford days. We covered some of Tutte’s other work <a href="https://rjlipton.wordpress.com/2010/06/02/the-tensor-trick-and-tuttles-flow-conjectures/">here</a>.</p>
<p/><h2> Deletion and Contraction </h2><p/>
<p/><p>
The two most basic recursion operations are <em>deleting</em> and <em>contracting</em> a chosen edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> in a given graph <img alt="{G = (V,E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G = (V,E)}"/>:</p>
<p><img alt="deletion_contraction" class="aligncenter wp-image-16004" height="118" src="https://rjlipton.files.wordpress.com/2019/06/deletion_contraction.png?w=400&amp;h=118" width="400"/></p>
<p>
These operations produce graphs denoted by <img alt="{G \setminus e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G \setminus e}"/> and <img alt="{G/e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G/e}"/>, respectively. A motive for them harks back to Gustav Kirchhoff’s counting of spanning trees:</p>
<ul>
<li>
A spanning tree of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> avoids using edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> if and only if it is a spanning tree of the graph <img alt="{G \setminus e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G \setminus e}"/> with <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> deleted.<p/>
</li><li>
A spanning tree of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> uses edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> if and only if the rest of it is a spanning tree of the graph <img alt="{G/e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G/e}"/> after contracting <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/>.
</li></ul>
<p>
Well, this is not how Kirchhoff counted trees. Counting via the recursion would take exponential time. Our whole object will be telling which cases of the recursions can be computed more directly.</p>
<p>Note that contracting one edge of the triangle graph <img alt="{C_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_3}"/> produces a <em>multi-</em>graph <img alt="{C_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_2}"/> with one double-edge. Then contracting one edge of <img alt="{C_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_2}"/> yields the loop graph <img alt="{C_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_1}"/>.</p>
<p/><p><br/>
<img alt="triangle" class="aligncenter wp-image-16005" height="73" src="https://rjlipton.files.wordpress.com/2019/06/triangle.png?w=250&amp;h=73" width="250"/></p>
<p/><p><br/>
Thus contraction yields non-simple undirected graphs, but the logic of counting their spanning trees remains valid.</p>
<p>
The order of edges does not matter as long as one avoids disconnecting the graph, and the base case is a tree (ignoring any loops) which contributes <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>.</p>
<p/><h2> Tutte’s Polynomial </h2><p/>
<p/><p>
A similar recursion counts colorings <img alt="{c: V \rightarrow \{1,...,k\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%3A+V+%5Crightarrow+%5C%7B1%2C...%2Ck%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c: V \rightarrow \{1,...,k\}}"/> that are <em>proper</em>, meaning that for each edge <img alt="{e = (u,v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be+%3D+%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e = (u,v)}"/>, <img alt="{c(u) \neq c(v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%28u%29+%5Cneq+c%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c(u) \neq c(v)}"/>.</p>
<ul>
<li>
A proper coloring <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> of <img alt="{G \setminus e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G \setminus e}"/> makes <img alt="{c(u) \neq c(v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%28u%29+%5Cneq+c%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c(u) \neq c(v)}"/> iff it is a proper coloring of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>.<p/>
</li><li>
A proper coloring <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c}"/> of <img alt="{G \setminus e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G \setminus e}"/> makes <img alt="{c(u) = c(v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%28u%29+%3D+c%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c(u) = c(v)}"/> iff it induces a proper coloring of <img alt="{G/e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G/e}"/>.
</li></ul>
<p>
This leads to the recursive definition of the <em>chromatic polynomial</em>:</p>
<p align="center"><img alt="\displaystyle    P_G(x) = P_{G\setminus e}(x) - P_{G/e}(x).   " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++P_G%28x%29+%3D+P_%7BG%5Csetminus+e%7D%28x%29+-+P_%7BG%2Fe%7D%28x%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle    P_G(x) = P_{G\setminus e}(x) - P_{G/e}(x).   "/></p>
<p>
The base cases are that an isolated vertex contributes <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, whereas an isolated loop contributes <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> since its single edge is never properly colored. The final rule is that <img alt="{P_G(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_G(x)}"/> is always the product of <img alt="{P_H(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_H%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_H(x)}"/> over all connected components <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. Then <img alt="{P_G(k)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_G%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_G(k)}"/> counts the number of proper <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>-colorings.</p>
<p>
This is like the recursion for coutning spanning terees except for the minus sign. Tutte’s brilliant insight, which was anticipated by Whitney in less symbolic form, was that the features can be combined by using two variables <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>. Call an edge a <a href="https://en.wikipedia.org/wiki/Bridge_(graph_theory)">bridge</a> if it is not part of any cycle. If <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> is not a bridge, the recursion is</p>
<p align="center"><img alt="\displaystyle    T_G(x,y) = T_{G \setminus e}(x,y) + T_{G/e}(x,y).   " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++T_G%28x%2Cy%29+%3D+T_%7BG+%5Csetminus+e%7D%28x%2Cy%29+%2B+T_%7BG%2Fe%7D%28x%2Cy%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle    T_G(x,y) = T_{G \setminus e}(x,y) + T_{G/e}(x,y).   "/></p>
<p>
The base case is now a graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> with some number <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> of bridges and some number <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell}"/> of loops, which gives <img alt="{T_G(x,y) = x^k y^\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_G%28x%2Cy%29+%3D+x%5Ek+y%5E%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_G(x,y) = x^k y^\ell}"/>. An important feature is that all <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-vertex trees have the same Tutte polynomial <img alt="{x^{n-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Bn-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^{n-1}}"/>, since there are <img alt="{n-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n-1}"/> edges and they are all bridges. The following are just some of the beautiful <a href="https://en.wikipedia.org/wiki/Tutte_polynomial#Specialisations">rules</a> that <img alt="{T_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_G}"/> follows. Let <img alt="{c = c_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+c_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c = c_G}"/> stand for the number of connected components of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>.</p>
<ul>
<li>
<img alt="{T_G(1,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_G%281%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_G(1,1)}"/> counts the number of spanning trees forests. This counts the number of spanning trees if <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is connected.<p/>
</li><li>
<img alt="{T_G(1-x,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_G%281-x%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_G(1-x,0)}"/>, when multiplied by <img alt="{(-1)^{n-c} x^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28-1%29%5E%7Bn-c%7D+x%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(-1)^{n-c} x^c}"/>, yields the chromatic polynomial.<p/>
</li><li>
<img alt="{T_G(1,2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_G%281%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_G(1,2)}"/> counts the number of spanning subgraphs.<p/>
</li><li>
<img alt="{T_G(2,2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_G%282%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_G(2,2)}"/> is just <img alt="{2^{|E|}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B%7CE%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{|E|}}"/>.<p/>
</li><li>
<img alt="{T_G(x,\frac{1}{x})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_G%28x%2C%5Cfrac%7B1%7D%7Bx%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_G(x,\frac{1}{x})}"/> gives the <a href="https://en.wikipedia.org/wiki/Jones_polynomial">Jones polynomial</a> of a knot related to <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>.
</li></ul>
<p>
There are many further relations. The Jones polynomial has many applications including in quantum physics.</p>
<p/><h2> Contraction With a Twist </h2><p/>
<p>
Recall our definition of the “amplitude” <img alt="{a(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a(G)}"/> of an undirected <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-vertex graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> from the “Net-Zero Graphs” <a href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/">post</a>:</p>
<p align="center"><img alt="\displaystyle    a(G) = \frac{c_0 - c_1}{2^n},   " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+%5Cfrac%7Bc_0+-+c_1%7D%7B2%5En%7D%2C+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle    a(G) = \frac{c_0 - c_1}{2^n},   "/></p>
<p>
where <img alt="{c_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_0}"/> is the number of black-and-white 2-colorings that make an even number of edges have both nodes colored black, and <img alt="{c_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_1}"/> for an odd number. </p>
<p>
There does not seem to be a simple recursion for <img alt="{a(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a(G)}"/> from <img alt="{G \setminus e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G \setminus e}"/> and <img alt="{G/e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G/e}"/>. We can, however, obtain one by using another kind of contraction that adds a loop at the combined vertex:</p>
<p><img alt="inchworm_contraction" class="aligncenter wp-image-16006" height="128" src="https://rjlipton.files.wordpress.com/2019/06/inchworm_contraction.png?w=250&amp;h=128" width="250"/></p>
<p>
We denote this by <img alt="{G/\!/e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%2F%5C%21%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G/\!/e}"/>. We have not found a simple reference for this. We obtain the following recursive formula:</p>
<p align="center"><img alt="\displaystyle    a(G) = a(G\backslash e) + \frac{1}{2}a(G/\!/e) - \frac{1}{2}a(G/e).   " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+a%28G%5Cbackslash+e%29+%2B+%5Cfrac%7B1%7D%7B2%7Da%28G%2F%5C%21%2Fe%29+-+%5Cfrac%7B1%7D%7B2%7Da%28G%2Fe%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle    a(G) = a(G\backslash e) + \frac{1}{2}a(G/\!/e) - \frac{1}{2}a(G/e).   "/></p>
<p>
This recursion allows <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> to be a bridge, so the base cases are <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> for an isolated vertex and <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> for a loop. More generally, the basis is <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> for a node with an even number of loops, <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> for odd. Here is an example for the ‘star graph’ <img alt="{S_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_4}"/> on 4 vertices:</p>
<p/><p><br/>
<img alt="starGraph" class="aligncenter wp-image-16007" height="294" src="https://rjlipton.files.wordpress.com/2019/06/stargraph.png?w=520&amp;h=294" width="520"/></p>
<p/><p><br/>
The diagram would need another layer to get down to (products of) base cases, which we have shortcut by putting values of <img alt="{a(H)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28H%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a(H)}"/> for each graph <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> at a leaf. Adding the products over all branches gives <img alt="{a(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a(G)}"/>. For the star graph,</p>
<p align="center"><img alt="\displaystyle    a(S_4) = \frac{1}{2} + \frac{1}{4} - \frac{1}{4} + \frac{1}{4} + \frac{1}{8} - \frac{1}{8} - \frac{1}{4} - \frac{1}{8} + \frac{1}{8} = \frac{1}{2}.   " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28S_4%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%2B+%5Cfrac%7B1%7D%7B4%7D+-+%5Cfrac%7B1%7D%7B4%7D+%2B+%5Cfrac%7B1%7D%7B4%7D+%2B+%5Cfrac%7B1%7D%7B8%7D+-+%5Cfrac%7B1%7D%7B8%7D+-+%5Cfrac%7B1%7D%7B4%7D+-+%5Cfrac%7B1%7D%7B8%7D+%2B+%5Cfrac%7B1%7D%7B8%7D+%3D+%5Cfrac%7B1%7D%7B2%7D.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle    a(S_4) = \frac{1}{2} + \frac{1}{4} - \frac{1}{4} + \frac{1}{4} + \frac{1}{8} - \frac{1}{8} - \frac{1}{4} - \frac{1}{8} + \frac{1}{8} = \frac{1}{2}.   "/></p>
<p/><p><br/>
Clearly this brute-force recursion grows as <img alt="{3^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3^n}"/>. This is slower than the order-<img alt="{2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^n}"/> time of using the coloring definition directly, but what all this underscores is how singular it is to be able to compute <img alt="{a(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a(G)}"/> in polynomial time, indeed <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. The search for a more-efficient recursion, one that might apply to <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/>-hard quantities, leads us to consider a more-drastic operation on edges.</p>
<p/><h2> Exploding Edges </h2><p/>
<p>
The new recursion operation is well illustrated by this figure:</p>
<p/><p><br/>
<img alt="explosionSolo" class="aligncenter wp-image-16008" height="203" src="https://rjlipton.files.wordpress.com/2019/06/explosionsolo.png?w=300&amp;h=203" width="300"/></p>
<p/><p><br/>
Two vertices disappear, not just one. Not only does the edge <img alt="{e = (u,v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be+%3D+%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e = (u,v)}"/> disappear, but any other edge incident to <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> or <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> from a vertex <img alt="{w \neq u,v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw+%5Cneq+u%2Cv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w \neq u,v}"/> gets “recoiled” into a loop at <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/>. We denote this operation by <img alt="{G \backslash\!\backslash e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%5Cbackslash%5C%21%5Cbackslash+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G \backslash\!\backslash e}"/> to connote that <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> is not just deleted but “exploded.” </p>
<p>
Properly speaking, we need to specify what happens if there are other edges between <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> or loops at <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> or <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/>. In an upcoming post we will see that those become <em>circles</em> in a <em>graphical polymatroid</em> which generalizes the notion of a graph. For now, however, it suffices to let <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> be the total number of vaporized edges, including <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/>. Then we obtain a two-term recursive formula:</p>
<p><a name="expl"/></p><a name="expl">
<p align="center"><img alt="\displaystyle    a(G) = a(G \backslash e) + \frac{(-1)^r}{2} a(G\backslash \!\backslash e).   \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+a%28G+%5Cbackslash+e%29+%2B+%5Cfrac%7B%28-1%29%5Er%7D%7B2%7D+a%28G%5Cbackslash+%5C%21%5Cbackslash+e%29.+++%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle    a(G) = a(G \backslash e) + \frac{(-1)^r}{2} a(G\backslash \!\backslash e).   \ \ \ \ \ (1)"/></p>
</a><p><a name="expl"/></p>
<p>
The base cases for isolated vertices are the same as before, but explosion also needs a base case for pure emptiness. This contributes <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. In the following example diagram, for the path graph <img alt="{P_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_4}"/> on four nodes, we denote such base cases by `w’ for “wisp”:</p>
<p><img alt="P3explosion" class="aligncenter wp-image-16009" height="291" src="https://rjlipton.files.wordpress.com/2019/06/p3explosion.png?w=450&amp;h=291" width="450"/></p>
<p>
Note again the rule that when the recursion disconnects the graph, the component values multiply together. Thus the value is</p>
<p align="center"><img alt="\displaystyle    a(P_4) = (1 - \frac{1}{2})(1 - \frac{1}{2}) - 0 = \frac{1}{4}.   " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28P_4%29+%3D+%281+-+%5Cfrac%7B1%7D%7B2%7D%29%281+-+%5Cfrac%7B1%7D%7B2%7D%29+-+0+%3D+%5Cfrac%7B1%7D%7B4%7D.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle    a(P_4) = (1 - \frac{1}{2})(1 - \frac{1}{2}) - 0 = \frac{1}{4}.   "/></p>
<p>
This is different from the amplitude <img alt="{\frac{1}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{2}}"/> of the star graph. What this means is that <img alt="{a(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a(G)}"/> does not obey the rules of the Tutte polynomial, which is the same for both of these 4-vertex trees. </p>
<p>
To prove the recursion equation (<a href="https://rjlipton.wordpress.com/feed/#expl">1</a>), for <img alt="{e = (u,v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be+%3D+%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e = (u,v)}"/>, note that every coloring has the same odd/even parity of black-black edges for <img alt="{G\setminus e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G\setminus e}"/> as for <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> except those that color both <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> black. Let <img alt="{c_0^{uv}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_0%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_0^{uv}}"/> denote the colorings among the latter that make an even number of black-black edges (including <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/>) overall, <img alt="{c_1^{uv}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_1%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_1^{uv}}"/> for an odd number. Then</p>
<p align="center"><img alt="\displaystyle    a(G) = a(G \setminus e) + \frac{2}{2^n}(c_1^{uv} - c_0^{uv}).   " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+a%28G+%5Csetminus+e%29+%2B+%5Cfrac%7B2%7D%7B2%5En%7D%28c_1%5E%7Buv%7D+-+c_0%5E%7Buv%7D%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle    a(G) = a(G \setminus e) + \frac{2}{2^n}(c_1^{uv} - c_0^{uv}).   "/></p>
<p>
Now if there are no other edges between or loops at <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/>, then <img alt="{c_1^{uv}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_1%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_1^{uv}}"/> is the same as the number of colorings of <img alt="{G \backslash\!\backslash e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%5Cbackslash%5C%21%5Cbackslash+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G \backslash\!\backslash e}"/> that make an even number of black-black edges, and <img alt="{c_0^{uv}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_0%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_0^{uv}}"/> becomes the odd case in <img alt="{G \backslash\!\backslash e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%5Cbackslash%5C%21%5Cbackslash+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G \backslash\!\backslash e}"/> again because we subtracted <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/>. Considering the sign change from other <img alt="{(u,v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(u,v)}"/> edges or loops and <img alt="{\frac{2}{2^n} = \frac{1/2}{2^{n-2}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B2%7D%7B2%5En%7D+%3D+%5Cfrac%7B1%2F2%7D%7B2%5E%7Bn-2%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{2}{2^n} = \frac{1/2}{2^{n-2}}}"/> yields equation (<a href="https://rjlipton.wordpress.com/feed/#expl">1</a>).  It is also possible to “explode” a loop, and our readers may enjoy figuring out how to define it.</p>
<p/><h2> The Amplitude Polynomial </h2><p/>
<p/><p>
We can expand on this by defining a polynomial <img alt="{Q_G(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_G(x)}"/> such that <img alt="{a(G) = Q_G(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+Q_G%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a(G) = Q_G(1)}"/>. The base cases are <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> for an isolated vertex but still <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> for a “wisp” and <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> for a loop. The basis extends to give <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> for an isolated node with an even number of loops and <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> for odd. Another way to put it is that two edges with the same endpoints, or two loops at the same node, can be removed. The above diagram shows that for the path graph <img alt="{P_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_4}"/>,</p>
<p align="center"><img alt="\displaystyle    Q_{P_4}(x) = (x^2 - \frac{1}{2})^2 = x^4 - x^2 + \frac{1}{4}.   " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++Q_%7BP_4%7D%28x%29+%3D+%28x%5E2+-+%5Cfrac%7B1%7D%7B2%7D%29%5E2+%3D+x%5E4+-+x%5E2+%2B+%5Cfrac%7B1%7D%7B4%7D.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle    Q_{P_4}(x) = (x^2 - \frac{1}{2})^2 = x^4 - x^2 + \frac{1}{4}.   "/></p>
<p>
Whereas, the recursion for the star graph—noting that the “star” <img alt="{S_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_2}"/> on two nodes is just a single edge—gives:</p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}    Q_{S_4}(x) &amp;=&amp; xQ_{S_3}(x) - \frac{1}{2}0\\   &amp;=&amp; x^2 Q_{S_2}(x) - \frac{1}{2}0 - \frac{1}{2}0\\   &amp;=&amp; x^4 - x^2 \frac{1}{2}(1\!\cdot\! 1) - \frac{1}{2}0 - \frac{1}{2}0 = x^4 - \frac{1}{2}x^2.   \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++Q_%7BS_4%7D%28x%29+%26%3D%26+xQ_%7BS_3%7D%28x%29+-+%5Cfrac%7B1%7D%7B2%7D0%5C%5C+++%26%3D%26+x%5E2+Q_%7BS_2%7D%28x%29+-+%5Cfrac%7B1%7D%7B2%7D0+-+%5Cfrac%7B1%7D%7B2%7D0%5C%5C+++%26%3D%26+x%5E4+-+x%5E2+%5Cfrac%7B1%7D%7B2%7D%281%5C%21%5Ccdot%5C%21+1%29+-+%5Cfrac%7B1%7D%7B2%7D0+-+%5Cfrac%7B1%7D%7B2%7D0+%3D+x%5E4+-+%5Cfrac%7B1%7D%7B2%7Dx%5E2.+++%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}    Q_{S_4}(x) &amp;=&amp; xQ_{S_3}(x) - \frac{1}{2}0\\   &amp;=&amp; x^2 Q_{S_2}(x) - \frac{1}{2}0 - \frac{1}{2}0\\   &amp;=&amp; x^4 - x^2 \frac{1}{2}(1\!\cdot\! 1) - \frac{1}{2}0 - \frac{1}{2}0 = x^4 - \frac{1}{2}x^2.   \end{array} "/></p>
<p>
This is not the same polynomial as <img alt="{Q_{P_4}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_%7BP_4%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_{P_4}(x)}"/>, again implying that <img alt="{Q_G(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_G(x)}"/> is not a specialization of the Tutte polynomial. We will show in the last post in this series that <img alt="{Q_G(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_G(x)}"/> does specialize the polynomial <img alt="{S_G(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_G%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_G(x,y)}"/> introduced in this 1993 <a href="http://homepages.mcs.vuw.ac.nz/~whittle/pubs/Tutte_invariants_of_2-polymatroids.pdf">paper</a> titled, “A Characterization of Tutte Invariants of 2-Polymatroids” and covered further in this 2006 <a href="https://www.researchgate.net/publication/49399603_Evaluating_the_Rank_Generating_Function_of_a_Graphic_2-Polymatroid">paper</a>.</p>
<p/><h2> Open Problems </h2><p/>
<p>
What other rules does our “amplitude polynomial” follow?  We will explore this in the mentioned upcoming post.  What other quantities can it be made to count?</p>
<p>
What we called “explosion” is in fact attested as the natural form of <em>contraction</em> for the <b>polymatroids</b> considered in these papers. What further uses might “explosion” have in graph theory apart from polymatroids?</p></font></font></div>
    </content>
    <updated>2019-06-17T08:27:09Z</updated>
    <published>2019-06-17T08:27:09Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="algebra"/>
    <category term="graph theory"/>
    <category term="graphs"/>
    <category term="polynomials"/>
    <category term="quantum"/>
    <category term="Tutte polynomial"/>
    <category term="William Tutte"/>
    <author>
      <name>Chaowen Guan and K.W. Regan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-06-19T17:20:44Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2019/06/16/modeconnectivity/</id>
    <link href="http://offconvex.github.io/2019/06/16/modeconnectivity/" rel="alternate" type="text/html"/>
    <title>Landscape Connectivity of Low Cost Solutions for Multilayer Nets</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A big mystery about deep learning is how, in a highly nonconvex loss landscape, gradient descent often finds near-optimal solutions —those with training cost almost zero— even starting from a random initialization. This conjures an image of a landscape filled with deep pits.  Gradient descent started at a random point falls easily to the bottom of the nearest pit. In this mental image the pits are disconnected from each other, so there is no way to go from the bottom of one pit to bottom of another without going through regions of high cost.</p>

<p>The current post is about our <a href="https://arxiv.org/abs/1906.06247">new paper with Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Wei Hu, Zhiyuan Li and Sanjeev Arora</a> which provides a mathematical explanation of  the following surprising phenomenon reported last year.</p>

<blockquote>
  <p><strong>Mode connectivity</strong> (<a href="https://arxiv.org/abs/1611.01540">Freeman and Bruna, 2016</a>, <a href="https://papers.nips.cc/paper/8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf">Garipov et al. 2018</a>, <a href="https://arxiv.org/abs/1803.00885">Draxler et al. 2018</a>) All pairs of low-cost solutions found via gradient descent  can actually be connected by simple paths in the parameter space, such that every point on the path is another solution of almost the same cost. In fact the low-cost path connecting two near-optima  can be <em>piecewise linear</em> with two line-segments, or a Bezier curve.</p>
</blockquote>

<p>See Figure 1 below from <a href="https://papers.nips.cc/paper/8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf">Garipov et al. 2018</a> for an illustration. Solutions A and B have low cost but the line connecting them goes through solutions with high cost. But we can find C of low cost such that paths AC and CB only pass through low-cost region.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/modes.PNG" style="width: 300px;"/>
<br/>
<b>Figure 1</b> Mode Connectivity. Warm colors represent low loss. 
</div>

<p>Using a very simple example let us see that this phenomenon is highly counterintuitive. Suppose we’re talking about 2-layer nets with linear activations and a real-valued output. Let the two nets $\theta_A$ and 
$\theta_B$  with zero loss be
<br/>
respectively where $x, U_1, U_2 \in \Re^n$
and matrices $W_1, W_2$ are $n\times n$. Then the straight line connecting them in parameter space corresponds to nets of the type $(\alpha U_1 + (1-\alpha)U_2)^\top(\alpha W_1 + (1-\alpha)W_2)$ which can be rewritten as</p>
<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/hybridnet.jpg" style="width: 650px;"/> 
</div>

<p>Note that the middle terms correspond to putting the top layer of one net on top of the bottom of the other, which in general is a nonsensical net (reminiscent of a <em>centaur</em>, a mythical half-man half-beast) that in general would be expected to have high loss.</p>

<p>Originally we figured mode connectivity would not be mathematically understood for a long time, because of the seeming difficulty of proving any mathematical theorems  about, say, $50$-layer nets trained on ImageNet data, and in particular dealing with such “centaur-like” nets in the interpolation.</p>

<p>Several authors (<a href="https://arxiv.org/abs/1611.01540">Freeman and Bruna, 2016</a>, <a href="https://arxiv.org/abs/1802.06384">Venturi et al. 2018</a>, <a href="https://arxiv.org/abs/1803.00909">Liang et al. 2018</a>, <a href="https://arxiv.org/abs/1809.10749">Nguyen et al. 2018</a>, <a href="https://arxiv.org/abs/1901.07417">Nguyen et al. 2019</a>) did try to explain the phenomenon of mode connectivity in simple settings (the first of these demonstrated mode connectivity empirically for multi-layer nets). But these explanations only work for very unrealistic 2-layer nets (or multi-layer nets with special structure) which are highly redundant e.g., the number of neurons may have to be larger than the number of training samples.</p>

<p>Our paper starts by clarifying an important point: redundancy with respect to a ground truth neural network is  insufficient for mode connectivity, which we show via a simple counterexample sketched below.</p>

<p>Thus to explain mode connectivity for multilayer nets we  will need to leverage some stronger property of <em>typical</em> solutions discovered via gradient-based training, as we will see below.</p>

<h2 id="mode-connectivity-need-not-hold-for-2-layer-overparametrized-nets">Mode Connectivity need not hold for 2-layer overparametrized nets</h2>

<p>We show that the strongest version of mode connectivity (every two minimizers are connected) does not hold even for a simple two-layer setting, where $f(x) = W_2\sigma(W_1x)$, even where the net is vastly overparametrized than it needs to be for the dataset in question.</p>

<blockquote>
  <p><strong>Theorem</strong> For any $h&gt;1$ there exists a data set which is perfectly fitted by  a ground truth neural network with $2$ layers and only $2$ hidden neurons, but if we desire to train neural network with $h$ hidden units on this dataset then the set of global minimizers are not connected.</p>
</blockquote>

<h2 id="stability-properties-of-typical-nets">Stability properties of typical nets</h2>

<p>Since mode connectivity has been found to hold for a range of architectures and datasets, any explanation probably should only rely upon properties that <em>generically</em> seem to hold for deep net standard training. Our explanation relies upon properties that were discovered in recent years in the effort to understand the generalization properties of deep nets.  These properties say that the output of the final net is stable to various kinds of added noise.
The properties imply that the loss function does not change much when the net parameters are perturbed; this is informally described as the net being a <em>flat minimum</em> (<a href="https://www.cs.toronto.edu/~hinton/absps/colt93.html">Hinton and Van Camp 1993</a>).</p>

<p>Our explanation of mode connectivity will involve the following two properties.</p>

<h3 id="noise-stability-and-dropout-stability">Noise stability and Dropout Stability</h3>

<p><em>Dropout</em> was introduced by <a href="https://arxiv.org/abs/1207.0580">Hinton et al. 2012</a>: during gradient-based training, one  zeroes out the output of $50\%$ of the nodes, and doubles the output of the remaining nodes. The gradient used in the next update is computed for this net. While dropout may not be as popular these days, it can be added to any existing net training without loss of generality. We’ll say a net is “$\epsilon$-dropout stable” if applying dropout to $50\%$ of the nodes increases its loss by at most $\epsilon$. Note that unlike dropout training where nodes are <em>randomly</em> dropped out, in our definition a network is dropout stable as long as there <em>exists</em> a way of dropping out $50\%$ of the nodes that does not increase its loss by too much.</p>

<blockquote>
  <p><strong>Theorem 1:</strong> If two trained multilayer ReLU nets with the same architecture  are $\epsilon$-dropout stable, then they can be connected in the loss landscape via a piece-wise linear path in which the number of linear segments is linear in the number of layers, and the loss of every point on the path is at most $\epsilon$ higher than the loss of the two end points.</p>
</blockquote>

<p><em>Noise stability</em> was discovered by <a href="https://arxiv.org/abs/1802.05296">Arora et al. ICML18</a>; this   was described in a <a href="http://www.offconvex.org/2018/02/17/generalization2/">previous blog post</a>. They found that trained nets are very stable to noise injection: if one adds a fairly large Gaussian noise vector to the output of a layer, then this has only a small effect on the output of higher layers. In other words, the network <em>rejects</em> the injected noise. That paper showed that noise stability can be used to prove that the net is compressible. Thus noise stability is indeed a form of redundancy in the net.</p>

<p>In the new paper we show that a minor variant of the noise stability property (which we empirically find to still hold in trained nets) implies dropout stability. More importantly, solutions satisfying this property can be connected using a piecewise linear path with at most $10$ segments.</p>

<blockquote>
  <p><strong>Theorem 2:</strong> If two trained multilayer ReLU nets with the same architecture are $\epsilon$-noise stable, then they can be connected in the loss landscape via a piece-wise linear path with at most 10 segments, and the loss of every point on the path is at most $\epsilon$ higher than the loss of the two end points.</p>
</blockquote>

<h2 id="proving-mode-connectivity-for-dropout-stable-nets">Proving mode connectivity for dropout-stable nets</h2>
<p>We exhibit the main ideas by proving mode connectivity for  fully connected nets that are dropout-stable, meaning training loss is stable to dropping out $50\%$ of the nodes.</p>

<p>Let $W_1,W_2,…,W_p$ be the weight matrices of the neural network, so the function that is computed by the network is $f(x) = W_p\sigma(\cdots \sigma(W_2(\sigma(W_1x)))\cdots)$. Here $\sigma$ is the ReLU activation (our result in this section works for any activations). We use $\theta = (W_1,W_2,…,W_p)\in \Theta$ to denote the parameters for the neural network. Given a set of data points $(x_i,y_i)~i=1,2,…,n$, the empirical loss $L$ is just an average of the losses for the individual samples $L(\theta) = \frac{1}{n}\sum_{i=1}^n l(y_i, f_\theta(x_i))$. The function $l(y, \hat{y})$ is a loss function that is convex in the second parameter (popular loss functions such as cross-entropy or mean-squared-error are all in this category).</p>

<p>Using this notation, Theorem 1 can be restated as:</p>

<blockquote>
  <p><strong>Theorem 1 (restated)</strong> Let $\theta_A$ and $\theta_B$ be two solutions that are both $\epsilon$-dropout stable, then there exists a path $\pi:[0,1]\to \Theta$ such that $\pi(0) = \theta_A$, $\pi(1) = \theta_B$ and for any $t\in(0,1)$ the loss $L(\pi(t)) \le \max{L(\theta_A), L(\theta_B)} + \epsilon$.</p>
</blockquote>

<p>To prove this theorem, the major step is to connect a network with its dropout version where half of the neurons are not used (see next part). Then intuitively it is not too difficult to connect two dropout versions as they both have a large number of inactive neurons.</p>

<p>As we discussed before, directly interpolating between two networks may not work as it give rise to <em>centaur-like</em> networks.  A key idea in this simpler theorem is that each linear segment in the path involves varying the parameters of only one layer, which allows careful control of this issue. (Proof of Theorem 2 is more complicated because the number of layers in the net are allowed to exceed the number of path segments.)</p>

<p>As a simple example, we show how to connect a 3-layer neural network with its dropout version. (The same idea can be easily extended to more layers by a simple induction on number of layers.) Assume without loss of generality that we are going to dropout the second half of neurons for both hidden layers. For the weight matrices $W_3, W_2, W_1$, we will write them in block form: $W_3$ is a $1\times 2$ block matrix $W_3 = [L_3, R_3]$, $W_2$ is a $2\times 2$ block matrix $W_2 = \left[L_2, C_2; D_2, R_2 \right]$, and $W_1$ is a $2\times 1$ block matrix $W_1 = \left[L_1; B_1\right]$ (here ; represents the end of a row). The dropout stable property implies that the networks with weights $(W_3, W_2, W_1)$, $(2[L_3, 0], W_2, W_1)$, $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ all have low loss (these weights correspond to the cases of no dropout, dropout only applied to the top hidden layer and dropout applied to both hidden layers). Note that the final set of weights $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ is equivalent to $([2L_3, 0], [2L_2, 0; 0, 0], [L_1; 0])$ as the output from the $B_1$ part of $W_1$ has no connections. The path we construct is illustrated in Figure 2 below:
As a simple example, we show how to connect a 3-layer neural network with its dropout version. (The same idea can be easily extended to more layers by a simple induction on number of layers.) Assume without loss of generality that we are going to dropout the second half of neurons for both hidden layers. For the weight matrices $W_3, W_2, W_1$, we will write them in block form: $W_3$ is a $1\times 2$ block matrix $W_3 = [L_3, R_3]$, $W_2$ is a $2\times 2$ block matrix $W_2 = \left[L_2, C_2; D_2, R_2 \right]$, and $W_1$ is a $2\times 1$ block matrix $W_1 = \left[L_1; B_1\right]$ (here ; represents the end of a row). The dropout stable property implies that the networks with weights $(W_3, W_2, W_1)$, $(2[L_3, 0], W_2, W_1)$, $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ all have low loss (these weights correspond to the cases of no dropout, dropout only applied to the top hidden layer and dropout applied to both hidden layers). Note that the final set of weights $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ is equivalent to $([2L_3, 0], [2L_2, 0; 0, 0], [L_1; 0])$ as the output from the $B_1$ part of $W_1$ has no connections. The path we construct is illustrated in Figure 2 below:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/path.png" style="width: 400px;"/> <br/>
<b>Figure 2</b> Path from a 3-layer neural network to its dropout version.
</div>

<p>We use two types of steps to construct the path: (a) Since the loss function is convex in the weight of the top layer, we can interpolate between two different networks that only differ in top layer weights; (b) if a set of neurons already has 0 output weights, then we can set its input weights arbitrarily.</p>

<p>Figure 2 shows how to alternate between these two types of steps to connect a 3-layer network to its dropout version. The red color highlights weights that have changed. In the case of type (a) steps, the red color only appears in the top layer weights; in the case of type (b) steps, the 0 matrices highlighted by the green color are the 0 output weights, where because of these 0 matrices setting the red blocks to any matrix will not change the output of the neural network.</p>

<p>The crux of this construction appears in steps (3) and (4). When we are going from (2) to (3), we changed the bottom rows of $W_2$ from $[D_2, R_2]$ to $[2L_2, 0]$. This is a type (b) step, and because currently the top-level weight is $[2L_3, 0]$, changing the bottom row of $W_2$ has no effect on the output of the neural network. However, making this change allows us to do the interpolation between (3) and (4), as now the two networks only differ in the top layer weights. The loss is bounded because the weights in (3) are equivalent to $(2[L_3, 0], W_2, W_1)$ (weights with dropout applied to top hidden layer), and the weights in (4) are equivalent to $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ (weights with dropout applied to both hidden layers). The same procedure can be repeated if the network has more layers.</p>

<p>The number of line segments in the path is linear in the number of layers. As mentioned, the paper also gives stronger results assuming noise stability, where we can actually consruct a path with constant number of line segments.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Our results are a first-cut explanation for how mode connectivity can arise in realistic deep nets. Our methods do not answer all mysteries about mode connectivity. In particular, in many cases (especially when the number of parameters is not as large) the solutions found in practice are not as robust as we require in our theorems (either in terms of dropout stability or noise stability), yet empirically it is still possible to find simple paths connecting the solutions. Are there other properties satisfied by these solutions that allow them to be connected? Also, our results can be extended to convolutional neural networks via <em>channel-wise dropout</em>, where one randomly turn off half of the channels (this was considered before in <a href="https://arxiv.org/abs/1411.4280">Thompson et al. 2015</a>,<a href="https://arxiv.org/abs/1812.03965">Keshari et al.2018</a>). While it is possible to train networks that are robust to channel-wise dropout, standard networks or even the ones trained with standard dropout do not satisfy this property.</p>

<p>It would also be interesting to utilize the insights into the landscape given by our explanation to design better training algorithms.</p></div>
    </summary>
    <updated>2019-06-16T22:00:00Z</updated>
    <published>2019-06-16T22:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2019-06-19T00:19:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/088</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/088" rel="alternate" type="text/html"/>
    <title>TR19-088 |  On the Complexity of Estimating the Effective Support Size | 

	Oded Goldreich</title>
    <summary>Loosely speaking, the effective support size of a distribution is the size of the support of a distribution that is close to it (in totally variation distance). 
We study the complexity of estimating the effective support size of an unknown distribution when given samples of the distributions as well as an evaluation oracle (which returns the probability that the queried element appears in the distribution).
In this context, we present several algorithms that exhibit a trade-off between the quality of the approximation and the complexity of obtaining it, and leave open the question of their optimality.</summary>
    <updated>2019-06-16T15:33:01Z</updated>
    <published>2019-06-16T15:33:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-19T17:20:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:typepad.com,2003:post-6a00d83452383469e20240a466212d200c</id>
    <link href="https://3dpancakes.typepad.com/ernie/2019/06/buy-my-free-book.html" rel="alternate" type="text/html"/>
    <link href="https://3dpancakes.typepad.com/ernie/2019/06/buy-my-free-book.html" rel="replies" type="text/html"/>
    <title>Buy My Free Book!</title>
    <summary>I'm happy to finally announce the publication of an actual dead-tree paperback edition of my Algorithms textbook. The book can be purchased from Amazon (US, UK, DE, ES, FR, IT, JP), for the ludicrous price of $27.50 (or the equivalent...</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="asset-img-link" href="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a48f6b37200d-popup"><img alt="Al-khwarizmi" class="asset  asset-image at-xid-6a00d83452383469e20240a48f6b37200d img-responsive" src="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a48f6b37200d-500wi" style="display: block; margin-left: auto; margin-right: auto;" title="Al-khwarizmi"/></a></p>
<p>I'm happy to finally announce the publication of an actual dead-tree paperback edition of my <em>Algorithms</em> textbook.  The book can be purchased from Amazon (<a href="https://www.amazon.com/dp/1792644833">US</a>, <a href="https://www.amazon.co.uk/dp/1792644833">UK</a>, <a href="https://www.amazon.de/dp/1792644833">DE</a>, <a href="https://www.amazon.es/dp/1792644833">ES</a>, <a href="https://www.amazon.fr/dp/1792644833">FR</a>, <a href="https://www.amazon.it/dp/1792644833">IT</a>, <a href="https://www.amazon.co.jp/dp/1792644833">JP</a>), for the <a href="https://www.youtube.com/watch?v=ygE01sOhzz0">ludicrous</a> price of $27.50 (or the equivalent in pounds, euros, and yen).</p>
<p>I've also updated the freely available, full-color electronic version at <a href="http://algorithms.wtf/">http://algorithms.wtf</a>; this electronic version will remain free indefinitely.  The same site includes several additional lecture notes and other course materials.</p>
<p>Thanks to everyone who reported dozens of errors in the 0th and ½th edition on the <a href="https://github.com/jeffgerickson/algorithms">Github issue-tracker</a>.  Please keep the bug reports and feature requests coming!</p>
<p>Enjoy!</p></div>
    </content>
    <updated>2019-06-15T20:29:59Z</updated>
    <published>2019-06-15T20:29:59Z</published>
    <category term="Algorithms"/>
    <category term="Books"/>
    <category term="Writing"/>
    <category term="algorithms"/>
    <category term="textbook"/>
    <author>
      <name>Jeff Erickson</name>
    </author>
    <source>
      <id>tag:typepad.com,2003:weblog-6686</id>
      <link href="https://3dpancakes.typepad.com/ernie/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="https://3dpancakes.typepad.com/ernie/" rel="alternate" type="text/html"/>
      <subtitle>Let Σ be a combinatorial surface with n vertices, genus g, and b boundaries.  Amen.</subtitle>
      <title>Ernie's 3D Pancakes</title>
      <updated>2019-06-15T20:29:59Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/06/15/linkage</id>
    <link href="https://11011110.github.io/blog/2019/06/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage for the end of an academic year</title>
    <summary>The Spring term just ended at UCI (we’re on a quarter system, so we run later into June and then start up again later in September than most other US universities). I haven’t yet turned in my grades, but I can already feel summer setting in.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Spring term just ended at UCI (we’re on a quarter system, so we run later into June and then start up again later in September than most other US universities). I haven’t yet turned in my grades, but I can already feel summer setting in.</p>

<ul>
  <li>
    <p><a href="https://hackaday.com/2019/06/01/paper-strandbeest-is-strong-enough-to-walk/">Remote-controlled papercraft steerable Strandbeest</a> (<a href="https://mathstodon.xyz/@11011110/102197633382208469"/>, <a href="https://news.ycombinator.com/item?id=20068166">via</a>).</p>
  </li>
  <li>
    <p><a href="https://www.instagram.com/p/BaSVpX_nMpU/">Voronoi origami</a> (<a href="https://mathstodon.xyz/@11011110/102206289543572358"/>, <a href="http://orderinspace.blogspot.com/2015/07/voronoi.html">see also</a>).</p>
  </li>
  <li>
    <p><a href="https://archive.org/details/@mirtitles">Mir Books</a> (<a href="https://mathstodon.xyz/@11011110/102212071495683563"/>, <a href="https://mathstodon.xyz/@jarban/102209797748141088">via</a>). A big collection of interesting-looking Soviet-era mathematics and science books and booklets, translated into English and free to read.</p>
  </li>
  <li>
    <p><a href="https://mathenchant.wordpress.com/2017/09/17/how-do-you-write-one-hundred-in-base-32/">Chip-firing games and sesquinary notation</a> (<a href="https://mathstodon.xyz/@11011110/102217258990536291"/>). Jim Propp writes a monthly long-form math blog and somehow I hadn’t encountered it before; this is one of its many interesting entries. One of the oddities about base  is that you calculate it bottom-up (by starting from a ones digit that’s too big and then carrying things higher) instead of top-down (by greedily subtracting powers).</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@JordiGH/102226037720885699">A discussion on whether proofs in Wikipedia articles need references</a>, and what those references are for.</p>
  </li>
  <li>
    <p><a href="https://theinnerframe.wordpress.com/2018/07/16/death-of-proof-the-pleasures-of-failure-i/">The tale of Horgan’s surface</a> (<a href="https://mathstodon.xyz/@11011110/102228263549241096"/>, <a href="https://mathenchant.wordpress.com/2019/06/06/carnival-of-mathematics-170/">via</a>, <a href="http://www.indiana.edu/~minimal/essays/horgan/index.html">see also</a>, <a href="https://www.scottaaronson.com/blog/?p=4133">see also</a>), a nonexistent minimal surface whose existence was incorrectly predicted by numerical experiments, named sarcastically after a journalist who incautiously suggested that proof was a dead concept.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1810.02231">Compact packings of the plane with three sizes of discs</a> (<a href="https://mathstodon.xyz/@11011110/102240130646118434"/>), Thomas Fernique, Amir Hashemi, and Olga Sizova. Here, “compact packing” means interior-disjoint disks forming only 3-sided gaps. The circle packing theorem constructs these for any finite maximal planar graph, with little control over disk size. Instead this paper seeks packings of the whole plane by infinitely many disks, with few sizes. 9 pairs of sizes and 164 triples work.</p>
  </li>
  <li>
    <p>Luca Trevisan posts a series of tutorials on online convex optimization, where you want to approximately minimize a sequence of convex functions before discovering what the functions are (parts <a href="https://lucatrevisan.wordpress.com/2019/04/17/online-optimization-for-complexity-theorists/"/>, <a href="https://lucatrevisan.wordpress.com/2019/04/22/online-optimization-post-0-definitions/"/>, <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/"/>, <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/"/>, <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/"/>, <a href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/"/>, <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/"/>; <a href="https://mathstodon.xyz/@11011110/102244702633232612"/>). It’s a hot topic in TCS with connections to regularity lemmas, fast SDP approximation, and spectral sparsifiers.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Cross_sea">Squared patterns of ocean waves</a> (<a href="https://mathstodon.xyz/@11011110/102251731269228854"/>), and wave patterns in social media: search for “cross sea” and note its appearance on gizmodo in 2014, amusingplanet in 2015, azula in 2017, providr in 2018, sciencealert in 2019…all repeating the same somewhat garbled explanation of mathematical wave models and danger to shipping.</p>
  </li>
  <li>
    <p><a href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/">The SIGACT Committee for the Advancement of Theoretical Computer Science is planning a Wikipedia edit-a-thon, in Phoenix on June 24 as part of STOC</a> (<a href="https://mathstodon.xyz/@11011110/102262460312659446"/>). You can help, and you don’t even have to brave the desert heat to do so! There’s <a href="https://thmatters.wordpress.com/2017/05/02/tcs-wikipedia-project/">a shared spreadsheet</a> where CATCS is crowdsourcing TCS topics on Wikipedia that need help. Add your favorite missing algorithm, theorem, complexity class, etc, and it’s likely it’ll get some attention.</p>
  </li>
  <li>
    <p>While I’m publicizing activities associated with STOC and FCRC next week in Phoenix, here’s another: <a href="https://sigact.org/tcswomen/tcs-women-2019/">the TCS Women Spotlight Workshop</a> (<a href="https://mathstodon.xyz/@11011110/102266092255364896"/>). It features an inspirational talk from Ronitt Rubinfeld (in my experience a great speaker), four “rising star” talks by Naama Ben-David, Debarati Das, Andrea Lincoln, and Oxana Poburinnaya, a panel/lunch for women at STOC, and a poster session of recent theoretical computer science research by women.</p>
  </li>
  <li>
    <p>Two colleagues from my department, Alex Nicolau and Alex Veidenbaum, are participating in <a href="http://artdaily.com/news/114313/University-of-California--Irvine-computer-scientists-breathe-life-into-Venice-Biennale-installations">a Venice Biennale project</a> (<a href="https://mathstodon.xyz/@11011110/102272037129033816"/>) in which viewers converse with computerized simulations of poet <a href="https://en.wikipedia.org/wiki/Paul_Celan">Paul Celan</a> and politician <a href="https://en.wikipedia.org/wiki/Nicolae_Ceau%C8%99escu">Nicolae Ceaușescu</a>.
The Alexes usually work on the more technical side of CS (parallelizing compilers, computer architecture, and embedded systems) so it’s interesting to me to see this softer direction from them.</p>
  </li>
  <li>
    <p><a href="https://www.nature.com/articles/d41586-019-01796-1"><em>Nature</em> on how word processors and text editors have been shifting from roll-your-own equation editing to LaTeX</a> (<a href="https://mathstodon.xyz/@11011110/102277832022967684"/>, <a href="https://news.ycombinator.com/item?id=20191348">via</a>).</p>
  </li>
</ul></div>
    </content>
    <updated>2019-06-15T17:58:00Z</updated>
    <published>2019-06-15T17:58:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-06-17T04:41:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/</id>
    <link href="https://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/" rel="alternate" type="text/html"/>
    <title>3rd Symposium on Simplicity in Algorithms</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">January 6-7, 2020 Salt Lake City, UT https://www.siam.org/Conferences/CM/Conference/sosa20 Submission deadline: August 9, 2019 Symposium on Simplicity in Algorithms is a new conference in theoretical computer science dedicated to advancing algorithms research by promoting simplicity and elegance in the design and analysis of algorithms. The benefits of simplicity are manifold: simpler algorithms manifest a better understanding … <a class="more-link" href="https://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/">Continue reading <span class="screen-reader-text">3rd Symposium on Simplicity in Algorithms</span></a></div>
    </summary>
    <updated>2019-06-14T04:09:04Z</updated>
    <published>2019-06-14T04:09:04Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-06-19T17:22:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/</id>
    <link href="https://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/" rel="alternate" type="text/html"/>
    <title>5th Algorithmic and Enumerative Combinatorics Summer School 2019</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 29 – August 2, 2019 Linz, Austria https://www3.risc.jku.at/conferences/aec2019/local.html Registration deadline: June 16, 2019 The goal of this summer school is to put forward the interplay between the fields of Enumerative Combinatorics, Analytic Combinatorics, and Algorithmics. This is a very active research area, which, aside from the three fields fueling each other mutually, receives as … <a class="more-link" href="https://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/">Continue reading <span class="screen-reader-text">5th Algorithmic and Enumerative Combinatorics Summer School 2019</span></a></div>
    </summary>
    <updated>2019-06-13T22:21:19Z</updated>
    <published>2019-06-13T22:21:19Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-06-19T17:22:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/</id>
    <link href="https://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/" rel="alternate" type="text/html"/>
    <title>ADFOCS 2019 – Games, Brains, and Distributed Computing</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">August 19-23, 2019 Saarbrücken, Germany http://resources.mpi-inf.mpg.de/conferences/adfocs/ Registration deadline: July 19, 2019 ADFOCS is an international summer school held annually at the Max Planck Institute for Informatics (MPII). It is organized as part of the activities of the MPII, in particular the International Max-Planck Research School (IMPRS), MPII’s graduate program. The purpose of this summer school … <a class="more-link" href="https://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/">Continue reading <span class="screen-reader-text">ADFOCS 2019 – Games, Brains, and Distributed Computing</span></a></div>
    </summary>
    <updated>2019-06-13T22:20:55Z</updated>
    <published>2019-06-13T22:20:55Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-06-19T17:22:51Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentdescent.org/smoothadv</id>
    <link href="https://decentdescent.org/smoothadv.html" rel="alternate" type="text/html"/>
    <title>Provably Robust Deep Learning</title>
    <summary>Recently, several works proposed the convolution of a neural network with a Gaussian as a smoothed classifier for provably robust classification. We show that adversarially training this smoothed classifier significantly increases its provable robustness through extensive experiments, achieving state-of-the-art ℓ2\ell_2ℓ2​ provable robustness on CIFAR10 and Imagenet, as shown in the tables below.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><blockquote>
  <p>Recently, several works proposed the convolution of a neural network with a Gaussian as a smoothed classifier for provably robust classification. We show that adversarially training this <strong>smoothed</strong> classifier significantly increases its provable robustness through extensive experiments, achieving state-of-the-art <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> <strong>provable robustness</strong> on CIFAR10 and Imagenet, as shown in the tables below.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> radius (Imagenet)</th>
      <th>0.5</th>
      <th>1</th>
      <th>1.5</th>
      <th>2</th>
      <th>2.5</th>
      <th>3</th>
      <th>3.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>49</td>
      <td>37</td>
      <td>29</td>
      <td>19</td>
      <td>15</td>
      <td>12</td>
      <td>9</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>56</td>
      <td>43</td>
      <td>37</td>
      <td>27</td>
      <td>25</td>
      <td>20</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> radius (CIFAR-10)</th>
      <th>0.25</th>
      <th>0.5</th>
      <th>0.75</th>
      <th>1.0</th>
      <th>1.25</th>
      <th>1.5</th>
      <th>1.75</th>
      <th>2.0</th>
      <th>2.25</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>60</td>
      <td>43</td>
      <td>32</td>
      <td>23</td>
      <td>17</td>
      <td>14</td>
      <td>12</td>
      <td>10</td>
      <td>8</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>74</td>
      <td>57</td>
      <td>48</td>
      <td>38</td>
      <td>33</td>
      <td>29</td>
      <td>24</td>
      <td>19</td>
      <td>17</td>
    </tr>
  </tbody>
</table>

<h1 id="introduction">Introduction</h1>

<p>It is now well-known that deep neural networks suffer from the brittleness problem: A small change in an input image imperceptible to humans can cause dramatic change in a neural network’s classification of the image. Such a perturbed input is known as an <em>adversarial example</em> and is by now immortalized in the famous picture below from <a href="https://arxiv.org/abs/1412.6572">Goodfellow et al.</a></p>

<p><img alt="A small carefully crafted noise can change a panda to a gibbon --- at least to a neural network!" src="https://decentdescent.org/assets/smoothadv/panda_gibbon_adv.png"/></p>

<p>As deep neural networks enter consumer and enterprise products of various forms, this brittleness can possibly have devastating consequences (<a href="https://arxiv.org/abs/1712.09665">Brown et al. 2018</a>,
<a href="https://arxiv.org/abs/1707.07397">Athalye et al. 2017</a>,
<a href="https://arxiv.org/abs/1707.089450">Evtimov &amp; Eykholt et al. 2018</a>,
<a href="https://arxiv.org/abs/1904.00759">Li et al. 2019</a>).
Most strikingly, Tencent Keen Security Lab recently <a href="https://keenlab.tencent.com/en/2019/03/29/Tencent-Keen-Security-Lab-Experimental-Security-Research-of-Tesla-Autopilot/">demonstrated</a> that the neural network underlying Tesla Autopilot can be fooled by an adversarially crafted marker on the ground into swerving into the opposite lane.</p>

<h2 id="adversarial-attack-and-defense">Adversarial Attack and Defense</h2>

<p>Given the importance of the problem, many researchers have formulated security models of adversarial attacks, along with ways to defend against adversaries in such models. In the most popular security model in the academic circle today, the adversary is allowed to perturb an input by a small noise bounded in <span class="katex"><span class="katex-mathml">ℓp\ell_p</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.15139200000000003em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span/></span></span></span></span></span></span></span></span>-norm, in order to cause the network to misclassify it. Thus, given a loss function <span class="katex"><span class="katex-mathml">LL</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">L</span></span></span></span>, a norm bound <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">ϵ</span></span></span></span>, an input <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">x</span></span></span></span>, its label <span class="katex"><span class="katex-mathml">yy</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">y</span></span></span></span>, and a neural network <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">F</span></span></span></span>, the adversary tries to find an input <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span></span></span></span>, within <span class="katex"><span class="katex-mathml">ℓp\ell_p</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.15139200000000003em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span/></span></span></span></span></span></span></span></span>-distance <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">ϵ</span></span></span></span> of <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">x</span></span></span></span>, that maximizes the loss <span class="katex"><span class="katex-mathml">L(F(x),y)L(F(x), y)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, i.e. it solves the following optimization problem</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">x^=arg max⁡∥x′−x∥p≤ϵL(F(x′),y).\hat x = \argmax_{\|x' - x\|_p \le \epsilon} L(F(x'), y).</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.43055999999999994em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.16454285714285716em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2818857142857143em;"><span/></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"/><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.18276em;"><span/></span></span></span></span><span class="mspace"/><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span>

<p>If <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">F</span></span></span></span> has trainable parameters <span class="katex"><span class="katex-mathml">θ\theta</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord mathdefault">θ</span></span></span></span>, then the defense needs to find the parameters that minimizes <span class="katex"><span class="katex-mathml">L(F(x^),y)L(F(\hat x), y)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, for <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span> sampled from the data distribution <span class="katex"><span class="katex-mathml">DD</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">D</span></span></span></span>, i.e. it solves the following minimax problem</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">min⁡θE(x,y)∼DL(F(x^),y).\min_{\theta} \underset{(x, y) \sim D}{\mathbb{E}} L(F(\hat x), y).</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.66786em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">θ</span></span></span></span><span><span class="pstrut" style="height: 2.7em;"/><span><span class="mop">min</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.7521079999999999em;"><span/></span></span></span></span><span class="mspace"/><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.68889em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mathdefault mtight">D</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.966em;"><span/></span></span></span></span></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span>

<p>Empirically, during an attack, the adversarial input <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span></span></span></span> can be obtained approximately by solving the max problem using gradient descent, making sure to project back to the <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">ϵ</span></span></span></span>-ball after each step.
This is known as the PGD attack (<a href="https://arxiv.org/abs/1607.02533">Kurakin et al.</a>, <a href="https://arxiv.org/abs/1706.06083">Madry et al.</a>), short for “project gradient descent.”
During training by the defense, for every sample <span class="katex"><span class="katex-mathml">(x,y)∼D(x, y) \sim D</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span><span class="mspace"/><span class="mrel">∼</span><span class="mspace"/></span><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">D</span></span></span></span>, this estimate of <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span></span></span></span> can be plugged into the min problem for gradient descent of <span class="katex"><span class="katex-mathml">θ\theta</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord mathdefault">θ</span></span></span></span>. 
This is known as <strong>Adversarial Training</strong>, or <strong>PGD training</strong> specifically when PGD is used for finding <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span></span></span></span>.</p>

<h2 id="empirical-robust-accuracy">Empirical Robust Accuracy</h2>

<p>Currently, the standard benchmark for measuring the strength of a model’s adversarial defense is the model’s <em>(empirical) robust accuracy</em> on various standard datasets like CIFAR-10 and Imagenet.
This accuracy is calculated by attacking the model with a strong empirical attack (like PGD) for every sample of the test set.
The percentage of the test set that the model is still able to correctly classify is the empirical robust accuracy.</p>

<p>For example, consider an adversary allowed to perturb an input by <span class="katex"><span class="katex-mathml">ϵ=8255\epsilon = \frac{8}{255}</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">ϵ</span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mopen nulldelimiter"/><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.845108em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">5</span><span class="mord mtight">5</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span class="frac-line" style="border-bottom-width: 0.04em;"/></span><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span/></span></span></span></span><span class="mclose nulldelimiter"/></span></span></span></span> in <span class="katex"><span class="katex-mathml">ℓ∞\ell_\infty</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> norm.
On an image, this means that the adversary can change the color of each pixel by at most 8 units (out of 255 total) in each color channel — a rather imperceptible perturbation.
Currently, the state-of-the-art empirical robust accuracy against such an adversary on CIFAR-10 hovers around 55% (<a href="https://arxiv.org/abs/1901.08573">Zhang et al. 2019</a>, <a href="https://arxiv.org/abs/1901.09960">Hendrycks et al. 2019</a>), meaning that the best classifier can only withstand a strong attack on about 55% of the samples in CIFAR-10.
Contrast this with the <a href="https://paperswithcode.com/sota/image-classification-on-cifar-10">state-of-the-art nonrobust accuracy on CIFAR-10 of &gt;95%</a>.
Thus it’s clear that adversarial robustness research still has a long way to go.</p>

<h1 id="provable-robustness-via-randomized-smoothing">Provable Robustness via Randomized Smoothing</h1>

<p>Note that the empirical robust accuracy is only an upper bound on the <strong>true robust accuracy</strong>.
This is defined by hypothetically replacing the <em>strong empirical attack</em> used in empirical robust accuracy with the <em>ideal attack</em> able to find <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span></span></span></span> exactly for every <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">x</span></span></span></span>.
Thus, nothing in principle prevents a stronger empirical attack from further lowering the empirical robust accuracy of a model.
Indeed, except a few notable cases like PGD (<a href="https://arxiv.org/abs/1706.06083">Madry et al.</a>), we have seen most claims of adversarial robustness broken down by systematic and thorough attacks (as examples, see <a href="https://arxiv.org/abs/1607.04311">Carlini &amp; Wagner 2016</a>,
<a href="https://arxiv.org/pdf/1705.07263">Carlini &amp; Wagner 2017</a>,
<a href="https://arxiv.org/abs/1707.07397">Athalye et al. 2017</a>,
<a href="https://arxiv.org/abs/1802.05666">Uesato et al. 2018</a>,
<a href="https://arxiv.org/abs/1802.00420">Athalye et al. 2018</a>,
<a href="https://arxiv.org/abs/1807.10272">Engstrom et al. 2018</a>,
<a href="https://arxiv.org/abs/1902.02322">Carlini 2019</a>).</p>

<p>This has motivated researchers into developing defenses that can <em>certify the absence of adversarial examples</em> (as prominent examples, see
<a href="https://arxiv.org/abs/1711.00851">Wong &amp; Kolter 2018</a>,
<a href="https://arxiv.org/abs/1702.01135">Katz et al. 2017</a>,
and see <a href="https://arxiv.org/pdf/1902.08722">Salman et al. 2019</a> for a thorough overview of these techniques). 
Such a defense is afforded a <strong>provable (or certified) robust accuracy</strong> on each dataset, defined as the percentage of the test set that can be proved to have no adversarial examples in its neighborhood.
In contrast with empirical robust accuracy, provable robust accuracy is a <em>lower bound</em> on the true robust accuracy, and therefore cannot be lowered further by more clever attacks.
The tables in the beginning of our blog post, for example, display provable robust accuracies on CIFAR-10 and Imagenet.</p>

<p>Until recently, most such certifiable defenses have not been able to scale to large networks and datasets (<a href="https://arxiv.org/pdf/1902.08722">Salman et al. 2019</a>), but a new technique called <em>randomized smoothing</em> (<a href="https://arxiv.org/abs/1802.03471">Lecuyer et al.</a>, <a href="https://arxiv.org/abs/1809.03113">Li et al.</a>, <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>) was shown to bypass this limitation, obtaining highly-nontrival <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> certified robust accuracy on Imagenet (<a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>). We now briefly review randomized smoothing.</p>

<h2 id="definition">Definition</h2>

<p>Consider a classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span> from <span class="katex"><span class="katex-mathml">Rd\mathbb{R}^d</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"/><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span> to classes <span class="katex"><span class="katex-mathml">Y\mathcal{Y}</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathcal">Y</span></span></span></span></span>. Randomized smoothing is a method that constructs a new, <em>smoothed</em> classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> from the <em>base</em> classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span>. The smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> assigns to a query point <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">x</span></span></span></span> the class which is most likely to be returned by the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span> under isotropic Gaussian noise perturbation of <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">x</span></span></span></span>, i.e.,</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">g(x)=arg max⁡c∈Y  P(f(x+δ)=c)g(x) = \argmax_{c \in \mathcal{Y}} \; \mathbb{P}(f(x+\delta) = c)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.43055999999999983em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mathcal mtight">Y</span></span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"/><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.006825em;"><span/></span></span></span></span><span class="mspace"/><span class="mspace"/><span class="mord"><span class="mord mathbb">P</span></span><span class="mopen">(</span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord mathdefault">c</span><span class="mclose">)</span></span></span></span></span>

<p>where <span class="katex"><span class="katex-mathml">δ∼N(0,σ2I)\delta \sim \mathcal{N}(0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord mathdefault">δ</span><span class="mspace"/><span class="mrel">∼</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, and the variance <span class="katex"><span class="katex-mathml">σ2\sigma^2</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8141079999999999em; vertical-align: 0em;"/><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> is a hyperparameter of the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> (it can be thought to control a robustness/accuracy tradeoff).
In <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>, <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span> is a neural network.</p>

<h2 id="prediction">Prediction</h2>

<p>To estimate <span class="katex"><span class="katex-mathml">g(x)g(x)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>, one simply has to</p>

<ol>
  <li>Sample a collection of Gausian samples <span class="katex"><span class="katex-mathml">δi\delta_i</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span>.</li>
  <li>Predict the class <span class="katex"><span class="katex-mathml">yiy_i</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> of each <span class="katex"><span class="katex-mathml">x+δix + \delta_i</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">x</span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> using the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span>.</li>
  <li>Take the majority vote of the <span class="katex"><span class="katex-mathml">yiy_i</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span>’s as the final prediction of the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> at <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">x</span></span></span></span>.</li>
</ol>

<h2 id="certification">Certification</h2>

<p>The robustness guarantee presented by <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> is as follows: suppose that when the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span> classifies  <span class="katex"><span class="katex-mathml">N(x,σ2I)\mathcal{N}(x, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, the (most popular) class <span class="katex"><span class="katex-mathml">cAc_A</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> is returned with probability  <span class="katex"><span class="katex-mathml">pA=Pδ(f(x+δ)=cA)p_A =  \mathbb{P}_\delta(f(x+\delta) = c_A)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord"><span class="mord mathbb">P</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.33610799999999996em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">δ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mclose">)</span></span></span></span>, and the <em>runner-up</em> class <span class="katex"><span class="katex-mathml">cBc_B</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> is returned with probability <span class="katex"><span class="katex-mathml">pB=max⁡c≠cAPδ(f(x+δ)=c)p_B = \max_{c \neq c_A} \mathbb{P}_\delta(f(x+\delta) = c)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361079999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="rlap mtight"><span class="strut"/><span class="inner"><span class="mrel mtight"></span></span><span class="fix"/></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em;"><span/></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.14329285714285717em;"><span/></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span/></span></span></span></span></span><span class="mspace"/><span class="mord"><span class="mord"><span class="mord mathbb">P</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.33610799999999996em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">δ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord mathdefault">c</span><span class="mclose">)</span></span></span></span>. We estimate <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> using Monte Carlo sampling and confidence intervals<sup id="fnref:1"><a class="footnote" href="https://decentdescent.org/smoothadv.html#fn:1">1</a></sup>. Then the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> is robust around <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">x</span></span></span></span> within the radius</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">σ2(Φ−1(pA)−Φ−1(pB)),\frac{\sigma}{2} \left(\Phi^{-1}(p_A) - \Phi^{-1}(p_B)\right),</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mopen nulldelimiter"/><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.10756em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord">2</span></span></span><span><span class="pstrut" style="height: 3em;"/><span class="frac-line" style="border-bottom-width: 0.04em;"/></span><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord mathdefault">σ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span/></span></span></span></span><span class="mclose nulldelimiter"/></span><span class="mspace"/><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.864108em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mclose">)</span><span class="mspace"/><span class="mbin">−</span><span class="mspace"/><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.864108em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter"><span class="delimsizing size1">)</span></span></span><span class="mspace"/><span class="mpunct">,</span></span></span></span></span>

<p>where <span class="katex"><span class="katex-mathml">Φ−1\Phi^{-1}</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8141079999999999em; vertical-align: 0em;"/><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> is the inverse of the standard Gaussian CDF. Thus, the bigger <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> is and the smaller <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> is, the more provably robust <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> is.</p>

<h2 id="training">Training</h2>

<p><a href="https://arxiv.org/abs/1906.04584">Cohen et al.</a> simply trained the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span> under Gaussian noise data augmentation with cross entropy loss, i.e. for each data point <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, sample <span class="katex"><span class="katex-mathml">δ∼N(0,σ2I)\delta \sim \mathcal{N}(0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord mathdefault">δ</span><span class="mspace"/><span class="mrel">∼</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span> and train <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span> on the example <span class="katex"><span class="katex-mathml">(x+δ,y)(x+\delta, y)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord mathdefault">δ</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>.
With this simple training regime applied to a <a href="https://arxiv.org/abs/1512.03385">Resnet-110</a> base classifier, they were able to obtain significant certified robustness on CIFAR-10 and Imagenet, as shown in our tables.</p>

<h2 id="an-illustration">An Illustration</h2>

<p>The following figures modified from <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> illustrate randomized smoothing.
The base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span> partitions the input space into different regions with different classifications, colored differently in the left figure.
The regions’ Gaussian measures (under the Gaussian <span class="katex"><span class="katex-mathml">N(x,σ2I)\mathcal{N}(x, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span> whose level curves are shown as dashed lines) are shown as a histogram on the right.
The class <span class="katex"><span class="katex-mathml">cAc_A</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> corresponding to the blue region is the output of the smoothed classifier <span class="katex"><span class="katex-mathml">g(x)g(x)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>; the class <span class="katex"><span class="katex-mathml">cBc_B</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> corresponding to the cyan region is the runner-up class.
If <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> is large enough and <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> is small enough, then we can prove that <span class="katex"><span class="katex-mathml">g(x′)=cAg(x') = c_A</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> for all <span class="katex"><span class="katex-mathml">∥x′−x∥2≤ϵ\|x' - x\|_2 \le \epsilon</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord">∥</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"/><span class="mbin">−</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord mathdefault">x</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mspace"/><span class="mrel">≤</span><span class="mspace"/></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">ϵ</span></span></span></span>, i.e. <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> is robust at <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">x</span></span></span></span> for <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> radius <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">ϵ</span></span></span></span>.</p>

<p><img class="limit-height" src="https://decentdescent.org/assets/smoothadv/randomized_smoothing_simple_light.png"/></p>

<h1 id="adversarially-training-the-smoothed-classifier">Adversarially Training the Smoothed Classifier</h1>

<p>Intuitively, adversarial training attempts to make a classifier locally flat around input sampled from a data distribution.
Thus it would seem that adversarial training should make it easier to <em>certify</em> the lack of adversarial examples, despite having no provable guarantees itself.
Yet historically, it has been difficult to execute this idea (<a href="https://arxiv.org/pdf/1902.08722">Salman et al. 2019</a>, and folklore), with the closest being <a href="https://arxiv.org/abs/1809.03008">Xiao et al.</a></p>

<p>It is hence by no means a foregone conclusion that adversarial training should improve certified accuracy of randomized smoothing.
<em>A priori</em> there could also be many ways these two techniques can be combined, and it is not clear which one would work best:</p>

<ol>
  <li>Train the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span> to be adversarially robust, simultaneous with the Gaussian data augmentation training prescribed in <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>.</li>
  <li>Find an adversarial example of the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span>, then add Gaussian noise and train.</li>
  <li>Add Gaussian noise and find an adversarial example of <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span> in the neighborhood of this Gaussian perturbation. Train <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span> on this adversarial example.</li>
  <li>Find an adversarial example of the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span>, then train <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> on this example.</li>
</ol>

<p>It turns out that certified accuracies of these methods follow the order (1) &lt; (2) &lt; (3) &lt; (4), with (4) achieving the highest certified accuracies (see <a href="https://arxiv.org/abs/1906.04584">our paper</a>).
Indeed, in hindsight, if <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> is the classifer doing the prediction, then we should be adversarially training <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span>, and not <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span></span></span></span>.
In the rest of the blog post, we lay out the details of (4).</p>

<h2 id="randomized-smoothing-for--soft-classifiers">Randomized Smoothing for  <em>Soft</em> Classifiers</h2>
<p>Neural networks typically learn <em>soft</em> classifiers, namely, functions <span class="katex"><span class="katex-mathml">F:Rd→P(Y)F: \mathbb{R}^d \to P(\mathcal{Y})</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">F</span><span class="mspace"/><span class="mrel">:</span><span class="mspace"/></span><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"/><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span><span class="mspace"/><span class="mrel">→</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathcal">Y</span></span><span class="mclose">)</span></span></span></span>, where <span class="katex"><span class="katex-mathml">P(Y)P(\mathcal{Y})</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathcal">Y</span></span><span class="mclose">)</span></span></span></span> is the set of probability distributions over <span class="katex"><span class="katex-mathml">Y\mathcal{Y}</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathcal">Y</span></span></span></span></span>.
During prediction, the soft classifier is argmaxed to return the final hard classification.
We therefore consider a generalization of randomized smoothing to soft classifiers. Given a soft classifier <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">F</span></span></span></span>, its associated <em>smoothed</em> soft classifier  <span class="katex"><span class="katex-mathml">G:Rn→P(Y)G: \mathbb{R}^n \to P(\mathcal{Y})</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">G</span><span class="mspace"/><span class="mrel">:</span><span class="mspace"/></span><span class="base"><span class="strut" style="height: 0.68889em; vertical-align: 0em;"/><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mspace"/><span class="mrel">→</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathcal">Y</span></span><span class="mclose">)</span></span></span></span> is defined as</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">G(x)=Eδ∼N(0,σ2I)F(x+δ).G (x) = \underset{\delta \sim \mathcal{N}(0, \sigma^2 I)}{\mathbb{E}} F(x + \delta).</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.68889em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7463142857142857em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.966em;"><span/></span></span></span></span></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span>

<p>Let <span class="katex"><span class="katex-mathml">f(x)f(x)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> and <span class="katex"><span class="katex-mathml">F(x)F (x)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> denote the hard and soft classifiers learned by the neural network, respectively, and let <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> and <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">G</span></span></span></span> denote the associated smoothed hard and smoothed soft classifiers. Directly finding adversarial examples for the smoothed <em>hard</em> classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">g</span></span></span></span> is a somewhat ill-behaved problem because of the argmax, so we instead propose to <em>find adversarial examples for the smoothed soft classifier</em> <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">G</span></span></span></span>. Empirically we found that doing so will also find good adversarial examples for the smoothed hard classifier.</p>

<h2 id="finding-adverarial-examples-for-smoothed-soft-classifier">Finding Adverarial Examples for Smoothed Soft Classifier</h2>
<p>Given a labeled data point <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, we wish to find a point <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"/><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span></span></span></span> which maximizes the loss of <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">G</span></span></span></span> in an <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> ball around <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">x</span></span></span></span> for some choice of loss function.
As is canonical in the literature, we focus on the cross entropy loss <span class="katex"><span class="katex-mathml">LCEL_{\mathrm{CE}}</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span>.
Thus, given a labeled data point <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span> our (ideal) adversarial perturbation is given by the formula:</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">x^=arg max⁡∥x′−x∥2≤ϵLCE(G(x′),y)=arg max⁡∥x′−x∥2≤ϵ(−log⁡Eδ∼N(0,σ2I)F(x′+δ)y).\begin{aligned}
    \hat x &amp;= \argmax_{\|x' - x\|_2 \leq \epsilon} L_{\mathrm{CE} } (G (x'), y)\\ 
    &amp;= \argmax_{\|x' - x\|_2 \leq \epsilon}  \left( - \log \underset{\delta \sim \mathcal{N} (0, \sigma^2 I)}{\mathbb{E}}  F (x' + \delta)_y \right).
\end{aligned}</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.8554399999999998em;"><span><span class="pstrut" style="height: 3.45em;"/><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span></span></span><span><span class="pstrut" style="height: 3.45em;"/><span class="mord"/></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 2.3554399999999998em;"><span/></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.8554399999999998em;"><span><span class="pstrut" style="height: 3.45em;"/><span class="mord"><span class="mord"/><span class="mspace"/><span class="mrel">=</span><span class="mspace"/><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.43056000000000016em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31731428571428577em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span/></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"/><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.16044em;"><span/></span></span></span></span><span class="mspace"/><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span><span><span class="pstrut" style="height: 3.45em;"/><span class="mord"><span class="mord"/><span class="mspace"/><span class="mrel">=</span><span class="mspace"/><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.43056000000000016em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31731428571428577em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span/></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"/><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.16044em;"><span/></span></span></span></span><span class="mspace"/><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size3">(</span></span><span class="mord">−</span><span class="mspace"/><span class="mop">lo<span>g</span></span><span class="mspace"/><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.68889em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7463142857142857em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.966em;"><span/></span></span></span></span></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/><span class="mord mathdefault">δ</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.15139200000000003em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span/></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size3">)</span></span></span><span class="mspace"/><span class="mord">.</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 2.3554399999999998em;"><span/></span></span></span></span></span></span></span></span></span></span>

<p>We will refer to the above as the <strong>SmoothAdv</strong> objective. The <em>SmoothAdv</em> objective is highly non-convex, so as is common in the literature, we will optimize it via projected gradient descent (PGD), and variants thereof. It is hard to find exact gradients for <em>SmoothAdv</em>, so in practice we must use some estimator based on random Gaussian samples.</p>

<h2 id="estimating-the-gradient-of-smoothadv">Estimating the Gradient of <em>SmoothAdv</em></h2>

<p>If we let <span class="katex"><span class="katex-mathml">J(x′)=LCE(G(x′),y)J(x') = L_{\mathrm{CE} } (G (x'), y)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span> denote the <em>SmoothAdv</em> objective, then</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">∇x′J(x′)=∇x′(−log⁡Eδ∼N(0,σ2I)F(x′+δ)y)  .\nabla_{x'} J(x') = \nabla_{x'} \left( - \log \underset{\delta \sim \mathcal{N}(0, \sigma^2 I)}{\mathbb{E}} F (x' + \delta)_y \right) \; .</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32797999999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"/><span class="mrel">=</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32797999999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mspace"/><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size3">(</span></span><span class="mord">−</span><span class="mspace"/><span class="mop">lo<span>g</span></span><span class="mspace"/><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.68889em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7463142857142857em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.966em;"><span/></span></span></span></span></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/><span class="mord mathdefault">δ</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.15139200000000003em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span/></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size3">)</span></span></span><span class="mspace"/><span class="mspace"/><span class="mord">.</span></span></span></span></span>

<p>However, it is not clear how to evaluate the expectation inside the log exactly, as it takes the form of a complicated high dimensional integral.
Therefore, we will use Monte Carlo approximations.
We sample i.i.d. Gaussians <span class="katex"><span class="katex-mathml">δ1,…,δm∼N(0,σ2I)\delta_1, \ldots, \delta_m \sim \mathcal{N} (0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"/><span class="minner">…</span><span class="mspace"/><span class="mpunct">,</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mspace"/><span class="mrel">∼</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, and use the plug-in estimator for the expectation:</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">∇x′J(x′)≈∇x′(−log⁡(1m∑i=1mF(x′+δi)y))  .\nabla_{x'} J(x') \approx \nabla_{x'} \left( - \log \left( \frac{1}{m} \sum_{i = 1}^m  F (x' + \delta_i)_y \right) \right) \; .</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32797999999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"/><span class="mrel">≈</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32797999999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mspace"/><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size4">(</span></span><span class="mord">−</span><span class="mspace"/><span class="mop">lo<span>g</span></span><span class="mspace"/><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mopen nulldelimiter"/><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.32144em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord mathdefault">m</span></span></span><span><span class="pstrut" style="height: 3em;"/><span class="frac-line" style="border-bottom-width: 0.04em;"/></span><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span/></span></span></span></span><span class="mclose nulldelimiter"/></span><span class="mspace"/><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6513970000000002em;"><span><span class="pstrut" style="height: 3.05em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span><span class="pstrut" style="height: 3.05em;"/><span><span class="mop op-symbol large-op">∑</span></span></span><span><span class="pstrut" style="height: 3.05em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.277669em;"><span/></span></span></span></span><span class="mspace"/><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.15139200000000003em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span/></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size4">)</span></span></span><span class="mclose delimcenter"><span class="delimsizing size4">)</span></span></span><span class="mspace"/><span class="mspace"/><span class="mord">.</span></span></span></span></span>

<p>It is not hard to see that if <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">F</span></span></span></span> is smooth, this estimator will converge to <span class="katex"><span class="katex-mathml">∇x′J(x′)\nabla_{x'} J(x')</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32797999999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> as we take more samples.</p>

<h2 id="smoothadv-is-not-the-naive-objective"><em>SmoothAdv</em> is not the <em>Naive</em> Objective</h2>

<p>We note that <em>SmoothAdv</em> should not be confused with the similar-looking objective</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">=arg max⁡∥x′−x∥2≤ϵEδ∼N(0,σ2I)LCE(F(x′+δ),y)=arg max⁡∥x′−x∥2≤ϵ Eδ∼N(0,σ2I)[−log⁡F(x′+δ)y]  ,\begin{aligned}
&amp;\phantom{ {}={}} \argmax_{\|x' - x\|_2 \leq \epsilon}
    \underset{\delta \sim \mathcal{N} (0, \sigma^2 I)}{\mathbb{E}} 
        L_{\mathrm{CE} } (F (x' + \delta), y) \\
&amp;= \argmax_{\|x' - x\|_2 \leq \epsilon} 
    \ \underset{\delta \sim \mathcal{N} (0, \sigma^2 I)}{\mathbb{E}} 
        \left[-\log  F(x' + \delta)_y\right] \; ,
\end{aligned}</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.55044em;"><span><span class="pstrut" style="height: 2.84em;"/><span class="mord"/></span><span><span class="pstrut" style="height: 2.84em;"/><span class="mord"/></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 2.05044em;"><span/></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.55044em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord"/><span class="mord" style="color: transparent;"/><span class="mspace"/><span class="mrel" style="color: transparent;">=</span><span class="mspace"/><span class="mord" style="color: transparent;"/><span class="mspace"/><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.43056000000000016em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31731428571428577em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span/></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"/><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.16044em;"><span/></span></span></span></span><span class="mspace"/><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.68889em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7463142857142857em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.966em;"><span/></span></span></span></span></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"/><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord"/><span class="mspace"/><span class="mrel">=</span><span class="mspace"/><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.43056000000000016em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31731428571428577em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span/></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"/><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.16044em;"><span/></span></span></span></span><span class="mspace"/><span class="mspace"> </span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.68889em;"><span><span class="pstrut" style="height: 3em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7463142857142857em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.966em;"><span/></span></span></span></span></span><span class="mspace"/><span class="minner"><span class="mopen delimcenter">[</span><span class="mord">−</span><span class="mspace"/><span class="mop">lo<span>g</span></span><span class="mspace"/><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/><span class="mord mathdefault">δ</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.15139200000000003em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span/></span></span></span></span></span><span class="mclose delimcenter">]</span></span><span class="mspace"/><span class="mspace"/><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 2.05044em;"><span/></span></span></span></span></span></span></span></span></span></span>

<p>where the <span class="katex"><span class="katex-mathml">log⁡\log</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mop">lo<span>g</span></span></span></span></span> and <span class="katex"><span class="katex-mathml">E\mathbb{E}</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68889em; vertical-align: 0em;"/><span class="mord"><span class="mord mathbb">E</span></span></span></span></span> have been swapped compared to <em>SmoothAdv</em>, as suggested in section G.3 of <a href="https://arxiv.org/abs/1902.02918">Cohen et al</a>.
This objective, which we shall call <strong>naive</strong>, is the one that corresponds to finding an adversarial example of <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">F</span></span></span></span> that is robust to Gaussian noise.
In contrast, <em>SmoothAdv</em> directly corresponds to finding an adversarial example of <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">G</span></span></span></span>.
From this point of view, <em>SmoothAdv</em> is the right optimization problem that should be used to find adversarial examples of <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">G</span></span></span></span>. 
This distinction turns out to be crucial in practice: empirically,  <a href="https://arxiv.org/abs/1902.02918">Cohen et al</a> found attacks based on the <em>naive</em> objective not to be effective.
In <a href="https://arxiv.org/abs/1906.04584">our paper</a>, we perform <em>SmoothAdv</em>-attack on <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>’s smoothed model and find, indeed, that it works better than the <em>Naive</em> objective, and it performs better with more Gaussian noise samples used to estimate its gradient.</p>

<h2 id="adversarially-training-smoothed-classifiers">Adversarially Training Smoothed Classifiers</h2>

<p>We now wish to use our new <em>SmoothAdv</em> attack to boost the adversarial robustness of smoothed classifiers.
As described in the beginning of this blog post, in (ordinary) adversarial training, given a current set of model parameters <span class="katex"><span class="katex-mathml">θt\theta_t</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> and a labeled data point <span class="katex"><span class="katex-mathml">(xt,yt)(x_t, y_t)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mclose">)</span></span></span></span>, one finds an adversarial perturbation <span class="katex"><span class="katex-mathml">x^t\hat x_t</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> of <span class="katex"><span class="katex-mathml">xtx_t</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> for the current model, and then takes a gradient step for the model parameters <span class="katex"><span class="katex-mathml">θt\theta_t</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span>, evaluated at the point <span class="katex"><span class="katex-mathml">(x^t,yt)(\hat x_t, y_t)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mclose">)</span></span></span></span>.
Intuitively, this encourages the network to learn to minimize the worst-case loss over a neighborhood around the input.</p>

<p>What is different in our proposed algorithm is that <em>we are finding the adversarial example <span class="katex"><span class="katex-mathml">x^t\hat x_t</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> with respect to the smoothed classifier <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">G</span></span></span></span> using the <strong>SmoothAdv</strong> objective</em>, and <em>we are training <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">G</span></span></span></span> at this adversarial example <span class="katex"><span class="katex-mathml">x^t\hat x_t</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> with respect to the <strong>SmoothAdv</strong> objective, estimated by the plug-in estimator.</em></p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">θt+1=θt+η∇θlog⁡(1m′∑i=1m′F(x^t+δi)y),\begin{aligned}
\theta_{t+1} &amp;= \theta_t + \eta \nabla_\theta \log\left(\frac{1}{m'} \sum_{i=1}^{m'} F(\hat x_t + \delta_i)_y\right),
\end{aligned}</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.2000199999999994em;"><span><span class="pstrut" style="height: 4.05002em;"/><span class="mord"><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.208331em;"><span/></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.70002em;"><span/></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.2000199999999994em;"><span><span class="pstrut" style="height: 4.05002em;"/><span class="mord"><span class="mord"/><span class="mspace"/><span class="mrel">=</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/><span class="mord mathdefault">η</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.33610799999999996em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mspace"/><span class="mop">lo<span>g</span></span><span class="mspace"/><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.05002em;"><span><span class="pstrut" style="height: 3.1550000000000002em;"/><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span><span class="pstrut" style="height: 3.1550000000000002em;"/><span class="delimsizinginner delim-size4"><span>⎛</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.55002em;"><span/></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"/><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.32144em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6778919999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span class="frac-line" style="border-bottom-width: 0.04em;"/></span><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span/></span></span></span></span><span class="mclose nulldelimiter"/></span><span class="mspace"/><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.9294850000000003em;"><span><span class="pstrut" style="height: 3.05em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span><span class="pstrut" style="height: 3.05em;"/><span><span class="mop op-symbol large-op">∑</span></span></span><span><span class="pstrut" style="height: 3.05em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8278285714285715em;"><span><span class="pstrut" style="height: 2.5em;"/><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.277669em;"><span/></span></span></span></span><span class="mspace"/><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord mathdefault">x</span></span><span><span class="pstrut" style="height: 3em;"/><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mspace"/><span class="mbin">+</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.15139200000000003em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span/></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.05002em;"><span><span class="pstrut" style="height: 3.1550000000000002em;"/><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span><span class="pstrut" style="height: 3.1550000000000002em;"/><span class="delimsizinginner delim-size4"><span>⎞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.55002em;"><span/></span></span></span></span></span></span><span class="mspace"/><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.70002em;"><span/></span></span></span></span></span></span></span></span></span></span>

<p>where <span class="katex"><span class="katex-mathml">θt\theta_t</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2805559999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> are the parameters of <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">F</span></span></span></span> at time <span class="katex"><span class="katex-mathml">tt</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.61508em; vertical-align: 0em;"/><span class="mord mathdefault">t</span></span></span></span>,  <span class="katex"><span class="katex-mathml">δi∼N(0,σ2I)\delta_i \sim \mathcal{N}(0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.31166399999999994em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span><span class="mspace"/><span class="mrel">∼</span><span class="mspace"/></span><span class="base"><span class="strut"/><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"/><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, and <span class="katex"><span class="katex-mathml">η\eta</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord mathdefault">η</span></span></span></span> is a learning rate.</p>

<h2 id="results">Results</h2>

<p>Over the course of the blog post, we have introduced several hyperparameters, such as 1) <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">ϵ</span></span></span></span>, the radius of perturbation used for adversarial training, 2) <span class="katex"><span class="katex-mathml">mm</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">m</span></span></span></span>, the number of Gaussian noise samples, 3) <span class="katex"><span class="katex-mathml">σ\sigma</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"/><span class="mord mathdefault">σ</span></span></span></span>, the standard deviation of the Gaussian noise.
We also did not mention other hyperparameters like <span class="katex"><span class="katex-mathml">TT</span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"/><span class="mord mathdefault">T</span></span></span></span>, the number of iterations used for PGD iterations, or the usage of DDN, an alternative attack to PGD that has been shown to be effective for <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span>-perturbations (<a href="https://arxiv.org/abs/1811.09600">Rony et al.</a>).
In <a href="https://arxiv.org/abs/1906.04584">our paper</a> we do extensive analysis of the effects of these hyperparameters, to which we refer interested readers.</p>

<p>Taking the max over all such hyperparameter combinations for each <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> perturbation radius, we obtain the <em>upper envelopes</em> of the certified accuracies of our method vs the <em>upper envelopes</em> of <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> in the tables in the beginning of this post, which we also replicate here for convenience.</p>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> radius (Imagenet)</th>
      <th>0.5</th>
      <th>1</th>
      <th>1.5</th>
      <th>2</th>
      <th>2.5</th>
      <th>3</th>
      <th>3.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>49</td>
      <td>37</td>
      <td>29</td>
      <td>19</td>
      <td>15</td>
      <td>12</td>
      <td>9</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>56</td>
      <td>43</td>
      <td>37</td>
      <td>27</td>
      <td>25</td>
      <td>20</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> radius (CIFAR-10)</th>
      <th>0.25</th>
      <th>0.5</th>
      <th>0.75</th>
      <th>1.0</th>
      <th>1.25</th>
      <th>1.5</th>
      <th>1.75</th>
      <th>2.0</th>
      <th>2.25</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>60</td>
      <td>43</td>
      <td>32</td>
      <td>23</td>
      <td>17</td>
      <td>14</td>
      <td>12</td>
      <td>10</td>
      <td>8</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>74</td>
      <td>57</td>
      <td>48</td>
      <td>38</td>
      <td>33</td>
      <td>29</td>
      <td>24</td>
      <td>19</td>
      <td>17</td>
    </tr>
  </tbody>
</table>

<h1 id="conclusion">Conclusion</h1>

<p>In this blog post, we reviewed adversarial training and randomized smoothing, a recently proposed provable defense.
By adversarially training the smoothed classifier — and carefully getting all the details right — we obtained the state-of-the-art <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.30110799999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> provable robustness on CIFAR-10 and Imagenet, demonstrating significant improvement over randomized smoothing alone.</p>

<h1 id="acknowledgements">Acknowledgements</h1>

<p>This blog post presented work done by Hadi Salman, Greg Yang, Jerry Li, Huan Zhang, Pengchuan Zhang, Ilya Razenshteyn, and Sebastien Bubeck.
We would like to thank Zico Kolter, Jeremy Cohen, Elan Rosenfeld, Aleksander Madry, Andrew Ilyas, Dimitris Tsipras, Shibani Santurkar, Jacob Steinhardt for comments and discussions during the making of this paper.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>We actually estimate a lower bound <span class="katex"><span class="katex-mathml">pA‾\underline{p_A}</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.43056em;"><span><span class="pstrut" style="height: 3em;"/><span class="underline-line" style="border-bottom-width: 0.04em;"/></span><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.39443999999999996em;"><span/></span></span></span></span></span></span></span> of <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> and an upper bound <span class="katex"><span class="katex-mathml">pB‾\overline{p_B}</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.63056em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span class="overline-line" style="border-bottom-width: 0.04em;"/></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em;"><span/></span></span></span></span></span></span></span> of <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> with high probability, and substitute <span class="katex"><span class="katex-mathml">pA‾\underline{p_A}</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.43056em;"><span><span class="pstrut" style="height: 3em;"/><span class="underline-line" style="border-bottom-width: 0.04em;"/></span><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.39443999999999996em;"><span/></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml">pB‾\overline{p_B}</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.63056em;"><span><span class="pstrut" style="height: 3em;"/><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span><span><span class="pstrut" style="height: 3em;"/><span class="overline-line" style="border-bottom-width: 0.04em;"/></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em;"><span/></span></span></span></span></span></span></span> for <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"/><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.32833099999999993em;"><span><span class="pstrut" style="height: 2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span/></span></span></span></span></span></span></span></span> everywhere. This is an overestimate, so our guarantee holds except for a small probability that the estimates are wrong. See <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> or <a href="https://arxiv.org/abs/1906.04584">our paper</a> for more details. <a class="reversefootnote" href="https://decentdescent.org/smoothadv.html#fnref:1">↩</a></p>
    </li>
  </ol>
</div></div>
    </content>
    <updated>2019-06-13T14:00:00Z</updated>
    <published>2019-06-13T14:00:00Z</published>
    <author>
      <name/>
    </author>
    <source>
      <id>https://decentdescent.org/feed.xml</id>
      <link href="https://decentdescent.org/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://decentdescent.org/" rel="alternate" type="text/html"/>
      <subtitle>A slow descent into the AIbyss, by  [Greg Yang](https://twitter.com/thegregyang) and friends.</subtitle>
      <title>Decent Descent</title>
      <updated>2019-06-13T16:11:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15989</id>
    <link href="https://rjlipton.wordpress.com/2019/06/12/how-to-make-a-polynomial-map-nicer/" rel="alternate" type="text/html"/>
    <title>How To Make A Polynomial Map Nicer</title>
    <summary>Stability theory and polynomials [ Essen ] Arno van den Essen is the author of the book on the Jacobian Conjecture. Today I want to highlight one of the ideas he presents in his book. The theory is sometimes called stabilization methods. Or K-theory methods. It is often used in connection with the famous Jacobian […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Stability theory and polynomials</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/12/how-to-make-a-polynomial-map-nicer/essen/" rel="attachment wp-att-15990"><img alt="" class="alignright size-full wp-image-15990" src="https://rjlipton.files.wordpress.com/2019/06/essen.jpeg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Essen ]</font></td>
</tr>
</tbody>
</table>
<p>
Arno van den Essen is the author of <b>the</b> book on the Jacobian Conjecture.</p>
<p>
Today I want to highlight one of the ideas he presents in his <a href="https://www.springer.com/gp/book/9783764363505">book</a>.<br/>
<span id="more-15989"/></p>
<p>
The theory is sometimes called stabilization methods. Or K-theory methods. It is often used in connection with the famous Jacobian conjecture (JC). I will not say any more about JC now—see <a href="https://rjlipton.wordpress.com/2014/01/29/progress-on-the-jacobian-conjecture/">this</a> for some comments we made a while ago. </p>
<p>
</p><p/><h2> The Stability Philosophy </h2><p/>
<p/><p>
Essen states that the philosophy of stability theory is: </p>
<blockquote><p><b> </b> <em> <i>It is possible to change a map <img alt="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}"/> and make it “nicer” provided we allow <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> to be increased</i>.<img alt="{^{\dagger}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5E%7B%5Cdagger%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{^{\dagger}}"/> </em>
</p></blockquote>
<p/><p>
<img alt="\rule{0.4\textwidth}{0.4pt}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crule%7B0.4%5Ctextwidth%7D%7B0.4pt%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rule{0.4\textwidth}{0.4pt}"/></p>
<p>
<img alt="{(\dagger)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cdagger%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\dagger)}"/> Not a direct quote.</p>
<p>
That is provided we can change <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> to 	</p>
<p align="center"><img alt="\displaystyle  \tilde{F} : \mathbb{K}^{N} \rightarrow \mathbb{K}^{N} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctilde%7BF%7D+%3A+%5Cmathbb%7BK%7D%5E%7BN%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7BN%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \tilde{F} : \mathbb{K}^{N} \rightarrow \mathbb{K}^{N} "/></p>
<p>where <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> is larger than <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. The whole method is based on a simple observation. Suppose that <img alt="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}"/> and define <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> by 	</p>
<p align="center"><img alt="\displaystyle  G(x_{1},\dots,x_{n},u_{1},\dots,u_{N-n}) = (F(x_{1},\dots,x_{n}), u_{1},\dots,u_{N-n}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++G%28x_%7B1%7D%2C%5Cdots%2Cx_%7Bn%7D%2Cu_%7B1%7D%2C%5Cdots%2Cu_%7BN-n%7D%29+%3D+%28F%28x_%7B1%7D%2C%5Cdots%2Cx_%7Bn%7D%29%2C+u_%7B1%7D%2C%5Cdots%2Cu_%7BN-n%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  G(x_{1},\dots,x_{n},u_{1},\dots,u_{N-n}) = (F(x_{1},\dots,x_{n}), u_{1},\dots,u_{N-n}). "/></p>
<p>Then <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> is injective if and only if <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is injective. This is trivial—really trivial. From trivial observations sometimes important methods are created. </p>
<p>
I will now explain how why this is useful by presenting an example. </p>
<p>
</p><p/><h2> The Method: By Example </h2><p/>
<p/><p>
Suppose that 	</p>
<p align="center"><img alt="\displaystyle  F: \mathbb{K}^{2} \rightarrow \mathbb{K}^{2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%3A+%5Cmathbb%7BK%7D%5E%7B2%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F: \mathbb{K}^{2} \rightarrow \mathbb{K}^{2} "/></p>
<p>is a polynomial mapping where 	</p>
<p align="center"><img alt="\displaystyle  F(x,y) = (f(x,y),g(x,y)). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28x%2Cy%29+%3D+%28f%28x%2Cy%29%2Cg%28x%2Cy%29%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(x,y) = (f(x,y),g(x,y)). "/></p>
<p>We wish to show that we can replace <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> by another polynomial map <img alt="{\tilde F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde F}"/> that has degree at most <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/>. Moreover, the new polynomial map is injective if and only if the original polynomial map is injective. The method can be used to preserve other properties of the polynomial mapping, but being injective is a important example. </p>
<p>
There seems to be no way to lower the degree of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> without destroying its structure. But if we use the stability philosophy and allow extra dimensions we can succeed. That is we replace <img alt="{F(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(x,y)}"/> by the function 	</p>
<p align="center"><img alt="\displaystyle  \tilde{F}(x,y,u,v) = (f(x,y),g(x,y),u,v). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctilde%7BF%7D%28x%2Cy%2Cu%2Cv%29+%3D+%28f%28x%2Cy%29%2Cg%28x%2Cy%29%2Cu%2Cv%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \tilde{F}(x,y,u,v) = (f(x,y),g(x,y),u,v). "/></p>
<p>Why does this work? The idea is that the extra two dimensions can be used as extra <i>registers</i>. These registers can be used to simplify the computation, and reduce the degree of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. </p>
<p>
Let <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x,y)}"/> have one term that we wish to remove. To be concrete, let’s assume the term is 	</p>
<p align="center"><img alt="\displaystyle  x^{3}y. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7B3%7Dy.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^{3}y. "/></p>
<p>Start with the input 	</p>
<p align="center"><img alt="\displaystyle  (x,y,u,v). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x%2Cy%2Cu%2Cv%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (x,y,u,v). "/></p>
<p>Now change this to 	</p>
<p align="center"><img alt="\displaystyle  (x,y,u+P,v+Q), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x%2Cy%2Cu%2BP%2Cv%2BQ%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (x,y,u+P,v+Q), "/></p>
<p>where 	</p>
<p align="center"><img alt="\displaystyle  P=x^{2} \text{ and } Q=xy. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%3Dx%5E%7B2%7D+%5Ctext%7B+and+%7D+Q%3Dxy.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  P=x^{2} \text{ and } Q=xy. "/></p>
<p>This is an invertible transformation. Note this is only possible because we have two extra dimensions or registers. Otherwise, we could not compute <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> and <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> without messing up the rest of the computation. Now map this to 	</p>
<p align="center"><img alt="\displaystyle  (f(x,y),g(x,y),u+P,v+Q). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28f%28x%2Cy%29%2Cg%28x%2Cy%29%2Cu%2BP%2Cv%2BQ%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (f(x,y),g(x,y),u+P,v+Q). "/></p>
<p>This is nothing more than computing the original function and ignoring the new registers. </p>
<p>
The next step is to go to 	</p>
<p align="center"><img alt="\displaystyle  (f-(u+P)(v+Q),g,u+Pv+Q). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28f-%28u%2BP%29%28v%2BQ%29%2Cg%2Cu%2BPv%2BQ%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (f-(u+P)(v+Q),g,u+Pv+Q). "/></p>
<p>The last point is that 	</p>
<p align="center"><img alt="\displaystyle  f- (u+P)(v+Q), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+%28u%2BP%29%28v%2BQ%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f- (u+P)(v+Q), "/></p>
<p>cancels the term <img alt="{x^{3}y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B3%7Dy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^{3}y}"/> we wished to remove. 	</p>
<p align="center"><img alt="\displaystyle  f- PQ -uQ -vP-uv = f-x^{2}xy -uxy - vx^{2}-uv. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+PQ+-uQ+-vP-uv+%3D+f-x%5E%7B2%7Dxy+-uxy+-+vx%5E%7B2%7D-uv.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f- PQ -uQ -vP-uv = f-x^{2}xy -uxy - vx^{2}-uv. "/></p>
<p>The price we pay is that new terms have been added, but they have at most degree <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/>. </p>
<p>
</p><p/><h2> The Method: General Case </h2><p/>
<p/><p>
We can prove by induction the following general theorem: </p>
<blockquote><p><b>Theorem 1</b> <em> Suppose <img alt="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}"/> is polynomial map where <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{K}"/> is a field. Then we can construct a polynomial map of degree at most <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{3}"/> denoted by <img alt="{\tilde F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+F%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\tilde F}"/> so that it is injective precisely when <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{F}"/> is injective. </em>
</p></blockquote>
<p/><p>
Even stronger theorems are possible. For example, the polynomial map <img alt="{\tilde F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde F}"/> can be required to be cubic linear: </p>
<blockquote><p><b>Definition 2</b> <em> Suppose that <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is in <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n \times n}"/> matrix over the field <img alt="{\mathbb{K}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BK%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathbb{K}}"/>. The <img alt="{F_{A}(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF_%7BA%7D%28X%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{F_{A}(X)}"/> is the <b>cubic linear</b> map for the matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is defined to be the map <img alt="{F_{A}: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF_%7BA%7D%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{F_{A}: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}"/> 	</em></p><em>
<p align="center"><img alt="\displaystyle  X \rightarrow X + (AX)^{*3}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%5Crightarrow+X+%2B+%28AX%29%5E%7B%2A3%7D%2C+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  X \rightarrow X + (AX)^{*3}, "/></p>
</em><p><em>where <img alt="{Y^{*3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%5E%7B%2A3%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{Y^{*3}}"/> is defined to be the vector <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{Z}"/> so that <img alt="{Z_{k} = Y_{k}^{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ_%7Bk%7D+%3D+Y_%7Bk%7D%5E%7B3%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{Z_{k} = Y_{k}^{3}}"/> for all coordinates <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{k}"/>. </em>
</p></blockquote>
<p/><p>
See Essen’s book for more details. Note a cubic linear map when <img alt="{n=2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=2}"/> is of the form: 	</p>
<p align="center"><img alt="\displaystyle  ( x + ( ax + by)^{3}, y + (cx + dy)^{3} ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28+x+%2B+%28+ax+%2B+by%29%5E%7B3%7D%2C+y+%2B+%28cx+%2B+dy%29%5E%7B3%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  ( x + ( ax + by)^{3}, y + (cx + dy)^{3} ) "/></p>
<p>where <img alt="{a,b,c,d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%2Cd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a,b,c,d}"/> are constants. This reduction to cubic linear maps is quite pretty, and requires a clever application of the stabilization method.</p>
<p>
</p><p/><h2> A Limit of The Method </h2><p/>
<p/><p>
The reduction in degree is possible only to degree <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/>. It cannot be reduced to degree <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> in general. Let’s look at the intuition why this is true. The last step is 	</p>
<p align="center"><img alt="\displaystyle  f- (u+P)(v+Q) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+%28u%2BP%29%28v%2BQ%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f- (u+P)(v+Q) "/></p>
<p>which is 	</p>
<p align="center"><img alt="\displaystyle  f- PQ -uQ -vP. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+PQ+-uQ+-vP.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f- PQ -uQ -vP. "/></p>
<p>Suppose <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> has a leading term of degree <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d}"/>. Also suppose that <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> has degree <img alt="{d_{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{P}}"/> and <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> has degree <img alt="{d_{Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7BQ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{Q}}"/>. Then 	</p>
<p align="center"><img alt="\displaystyle  d = d_{P} + d_{Q} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++d+%3D+d_%7BP%7D+%2B+d_%7BQ%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  d = d_{P} + d_{Q} "/></p>
<p>since the leading term goes away. But <img alt="{uQ}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BuQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{uQ}"/> and <img alt="{vP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BvP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{vP}"/> have degrees <img alt="{d_{P}+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7BP%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{P}+1}"/> and <img alt="{d_{Q}+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7BQ%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{Q}+1}"/> respectively. So to keep <img alt="{d_{P}+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7BP%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{P}+1}"/> and <img alt="{d_{Q}+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7BQ%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{Q}+1}"/> both <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> or less, it follows that <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d}"/> can be at most <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. However, in this case a term of degree <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> is removed and other terms of degree <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> are added. This is not a formal proof that the method cannot reduce the degree to <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. I do believe that formalized properly it is a theorem that reduction to degree <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> is in general impossible. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I like this technology. I wonder if it might be possible to use it on some of our favorite problems. I do like that it conserves invertibility. This seems like it could be related to quantum computing, because of the reversible nature of quantum computing. </p>
<p/></font></font></div>
    </content>
    <updated>2019-06-12T20:31:41Z</updated>
    <published>2019-06-12T20:31:41Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Proofs"/>
    <category term="cubic"/>
    <category term="cubic linear"/>
    <category term="jacobian conjecture"/>
    <category term="polynomial maps"/>
    <category term="redcution"/>
    <category term="stable"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-06-19T17:20:46Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8019500166163846173</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8019500166163846173/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/compressing-in-moscow.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8019500166163846173" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8019500166163846173" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/compressing-in-moscow.html" rel="alternate" type="text/html"/>
    <title>Compressing in Moscow</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="separator" style="clear: both; text-align: center;">
</div>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://1.bp.blogspot.com/-Bisdr756vxM/XQDylk3YrYI/AAAAAAABozI/M1FgKQZyX08bscqjoy9wmF8BxtXG86RLACLcBGAs/s1600/Vereshchagin.jpg" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="https://1.bp.blogspot.com/-Bisdr756vxM/XQDylk3YrYI/AAAAAAABozI/M1FgKQZyX08bscqjoy9wmF8BxtXG86RLACLcBGAs/s1600/Vereshchagin.jpg"/></a><a href="https://1.bp.blogspot.com/-G6yjxH8R1pU/XQDylvxG2aI/AAAAAAABozE/P54lQPWWEa46pTgyDEi6QWQOsaWLxP5zwCLcBGAs/s1600/Shen.jpg" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" src="https://1.bp.blogspot.com/-G6yjxH8R1pU/XQDylvxG2aI/AAAAAAABozE/P54lQPWWEa46pTgyDEi6QWQOsaWLxP5zwCLcBGAs/s1600/Shen.jpg"/></a></div>
<br/>
This week finds me in Moscow for a pair of workshops, the <a href="https://mipt.ru/education/chairs/dm/conferences/workshop-june-9-11-moscow-2019.php">Russian Workshop on Complexity and Model Theory</a> and a workshop on <a href="https://www.poncelet.ru/conference/ric">Randomness, Information and Complexity</a>. The latter celebrates the lives of Alexander Shen and Nikolay Vereshchagin on their 60th birthdays.<br/>
<br/>
Alexander Shen might be best known in computational complexity for his <a href="https://doi.org/10.1145/146585.146613">alternate proof</a> of IP = PSPACE. In 1989, Lund, Fortnow, Karloff and Nisan gave an interactive proof for the permanent, which got the entire polynomial-time hierarchy by Toda's theorem. But we didn't know how to push the protocol to PSPACE, we had a problem keeping degrees of polynomials low. Shamir had the first proof by looking at a specific protocol for PSPACE. Shen had the brilliant but simple idea to use a degree reducing operator, taking the polynomial modulo x<sup>2</sup>-x. The three papers appeared <a href="https://dl.acm.org/citation.cfm?id=146585#prox">back-to-back-to-back</a> in JACM.<br/>
<br/>
Shen and Vereshchagin though made their careers with their extensive work on Kolmogorov complexity and entropy, often together. Vereshchagin and I have co-authored some papers together during our mutual trips to Amsterdam, including on <a href="http://doi.org/10.1007/11672142_10">Kolmogorov Complexity with Errors</a> and how to <a href="http://doi.org/10.1007/b106485">increase Kolmogorov Complexity</a>. My <a href="https://doi.org/10.1006/jcss.1999.1677">favorite work</a> of Shen and Vereshchagin, which they did with Daniel Hammer and Andrei Romashchenko showed that every linear inequality that holds for entropy also holds for Kolmogorov complexity and vice-versa, the best argument that the two notions of information, one based on distributions, the other based on strings, share strong connections.<br/>
<br/>
Today is <a href="https://en.wikipedia.org/wiki/Russia_Day">Russia Day</a> that celebrates the reestablishment of Russia out of the Soviet Union in 1990. Just like how the British celebrate their succession from the US in 1776 I guess. But I'm celebrating Russia day by honoring these two great Russians. Congrats Sasha and Kolya!</div>
    </content>
    <updated>2019-06-12T16:27:00Z</updated>
    <published>2019-06-12T16:27:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-06-19T12:47:18Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/06/11/dancing-arc-quadrilaterals</id>
    <link href="https://11011110.github.io/blog/2019/06/11/dancing-arc-quadrilaterals.html" rel="alternate" type="text/html"/>
    <title>Dancing arc-quadrilaterals</title>
    <summary>Several of my past papers concern Lombardi drawing, which I and my coauthors named after conspiracy-theory artist Mark Lombardi. In this style of drawing, edges are drawn as circular arcs, and must meet at equal angles around every vertex. Not every graph has such a drawing, but many symmetrical graphs do (example below: the smallest zero-symmetric graph with only two edge orbits).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Several of my past papers concern Lombardi drawing, which I and my coauthors named after conspiracy-theory artist <a href="https://en.wikipedia.org/wiki/Mark_Lombardi">Mark Lombardi</a>. In this style of drawing, edges are drawn as circular arcs, and must meet at equal angles around every vertex. Not every graph has such a drawing, but many symmetrical graphs do (example below: the smallest <a href="https://en.wikipedia.org/wiki/Zero-symmetric_graph">zero-symmetric graph</a> with only two edge orbits).</p>

<p style="text-align: center;"><img alt="The smallest zero-symmetric graph with only two edge orbits" src="https://11011110.github.io/blog/assets/2019/Two-edge-orbit_GRR.svg"/></p>

<p>All 2-degenerate graphs do as well; these are the graphs that can be reduced to nothing by removing vertices of degree at most two. And all 3-regular planar graphs have planar drawings in this style; I drew the one below to illustrate <a href="https://en.wikipedia.org/wiki/Grinberg%27s_theorem">Grinberg’s theorem</a>, a necessary condition for Hamiltonicity of planar graphs.</p>

<p style="text-align: center;"><img alt="Grinberg's non-Hamiltonian planar cubic graph with cyclic edge-connectivity five" src="https://11011110.github.io/blog/assets/2019/Grinberg_5CEC_Nonhamiltonian_graph.svg"/></p>

<p>So anyway, my newest arXiv preprint is “Bipartite and series-parallel graphs without planar Lombardi drawings” (<a href="https://arxiv.org/abs/1906.04401">arXiv:1906.04401</a>, to appear at <a href="https://sites.ualberta.ca/~cccg2019/">CCCG</a>). It is about some families of planar graphs that have Lombardi drawings (because they are 2-degenerate) but do not have planar Lombardi drawings. They include planar bipartite graphs like the one below (but with more edges and vertices):</p>

<p style="text-align: center;"><img alt="Construction for a family of planar bipartite graphs with no planar Lombardi drawing " src="https://11011110.github.io/blog/assets/2019/nested-K2n.svg"/></p>

<p>and embedded series-parallel graphs like the one below (again, with more edges and vertices):</p>

<p style="text-align: center;"><img alt="Construction for a family of embedded series-parallel graphs with no planar Lombardi drawing" src="https://11011110.github.io/blog/assets/2019/nonlom-serpar.svg"/></p>

<p>The key structures that makes both of these graphs hard to draw in Lombardi style are their yellow-blue quadrilateral faces.
The red parts of the graph are just filler to make these faces have the right angles. The yellow-blue quadrilaterals all share the same two yellow vertices,
and I like to think of them as forming a ring dancing hand-to-hand around these two yellow vertices like <a href="https://en.wikipedia.org/wiki/Dance_(Matisse)">Matisse’s dancers</a>.</p>

<p style="text-align: center;"><img alt="_La Danse_, Henri Matisse, 1910, from https://en.wikipedia.org/wiki/File:Matissedance.jpg" src="https://11011110.github.io/blog/assets/2019/Matisse-Dance.jpg"/></p>

<p>When I wrote about <a href="https://11011110.github.io/blog/2018/12/22/circles-crossing-equal.html">quadrilaterals with circular-arc edges meeting at equal angles at each vertex</a> last December, it was these rings of quadrilaterals I was thinking about. As I wrote in my previous post, each of these quadrilaterals has a circumscribing circle. It’s not hard to make one arc polygon with four equal angles, as sharp as you like. But when the angles get sharp and two of these polygons share two opposite vertices and are packed at a tight angle next to each other (as all the yellow-blue quadrilateral faces in these graphs do) each of the two adjacent quadrilaterals must have a deep pocket into which its neighbor reaches to touch its circumcircle, and this forces them to become quite distorted looking.</p>

<p style="text-align: center;"><img alt="Two sharp circular-arc quadrilaterals reach into each others' pockets to touch their circumscribing circles" src="https://11011110.github.io/blog/assets/2019/reacharound.svg"/></p>

<p>If we think of these two quadrilaterals as dancers, with their heads and toes at the shared points and with their hands free, then both of them have their right hands (from the perspective of the viewer) held high and their left hands held low. Extending the same picture to a complete ring of dancers, each dancer in the ring must be holding their right hand higher than their neighbor to the left. But this obviously can’t extend all the way around the ring, because when you came back to the dancer you started with their hand should be the same height as it was when you started.</p>

<p>This is all very metaphorical but fortunately this intuition can be turned into a mathematical proof that the drawing doesn’t exist. The correct tools for measuring the heights of the quadrilateral vertices turn out to be <a href="https://en.wikipedia.org/wiki/M%C3%B6bius_transformation">Möbius transformations</a> and <a href="https://en.wikipedia.org/wiki/Bipolar_coordinates">bipolar coordinates</a>, a system for assigning coordinates to points in the plane by the angle they make with two fixed points (the two yellow shared vertices of all the quadrilateral faces of our graph) and the ratio of their distances to the fixed points.</p>

<p style="text-align: center;"><img alt="Level sets for bipolar coordinates" src="https://11011110.github.io/blog/assets/2019/apollo.svg"/></p>

<p>Möbius transformations preserve the circular-arc shape of our quadrilateral sides, and the angles at which they meet. When we restrict them to the transformations that leave the two poles of the bipolar coordinate system fixed, they act very nicely on the two coordinates, essentially adding fixed amounts to each coordinate. We can use them to transform our quadrilaterals into a more canonical shape and prove that the radius-ratio coordinate increases from one quadrilateral to the next around our ring of quadrilaterals, getting the same contradiction described above and proving that no drawing exists.</p>

<p>The most obvious questions in the same direction that this paper leaves unsolved are: what about series-parallel graphs where you do not have a fixed planar embedding for the graph? And what about outerplanar graphs (either with the outerplanar embedding or without fixing an embedding)? We neither have a method for finding planar Lombardi drawings of all graphs of these types, nor a proof that these drawings do not exist.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102256946045372078">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-06-11T21:16:00Z</updated>
    <published>2019-06-11T21:16:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-06-17T04:41:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamdsmith.wordpress.com/?p=602</id>
    <link href="https://adamdsmith.wordpress.com/2019/06/11/tpdp-2019/" rel="alternate" type="text/html"/>
    <title>TPDP 2019</title>
    <summary>The call for submissions for the latest edition of TPDP (Theory and Practice of Differential Privacy) is out! https://tpdp.cse.buffalo.edu/2019/  The workshop covers work on differential privacy (of course), and more generally on rigorous modeling of, and attacks on, statistical data privacy. The intention is to be inclusive. Submissions are due June 21, 2019. Advertisements</summary>
    <updated>2019-06-11T20:52:53Z</updated>
    <published>2019-06-11T20:52:53Z</published>
    <category term="Conferences"/>
    <category term="theory"/>
    <author>
      <name>adamdsmith</name>
    </author>
    <source>
      <id>https://adamdsmith.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamdsmith.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamdsmith.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamdsmith.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamdsmith.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>An inquiry into the Nature and Causes of Stuff</subtitle>
      <title>Oddly Shaped Pegs</title>
      <updated>2019-06-19T17:21:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/087</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/087" rel="alternate" type="text/html"/>
    <title>TR19-087 |  Coin Theorems and the Fourier Expansion | 

	Rohit Agrawal</title>
    <summary>In this note we compare two measures of the complexity of a class $\mathcal F$ of Boolean functions studied in (unconditional) pseudorandomness: $\mathcal F$'s ability to distinguish between biased and uniform coins (the coin problem), and the norms of the different levels of the Fourier expansion of functions in $\mathcal F$ (the Fourier growth). We show that for coins with low bias $\varepsilon = o(1/n)$, a function's distinguishing advantage in the coin problem is essentially equivalent to $\varepsilon$ times the sum of its level $1$ Fourier coefficients, which in particular shows that known level $1$ and total influence bounds for some classes of interest (such as constant-width read-once branching programs) in fact follow as a black-box from the corresponding coin theorems, thereby simplifying the proofs of some known results in the literature. For higher levels, it is well-known that Fourier growth bounds on all levels of the Fourier spectrum imply coin theorems, even for large $\varepsilon$, and we discuss here the possibility of a converse.</summary>
    <updated>2019-06-11T14:34:45Z</updated>
    <published>2019-06-11T14:34:45Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-19T17:20:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1267</id>
    <link href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/" rel="alternate" type="text/html"/>
    <title>Wikipedia edit-a-thon at STOC’19</title>
    <summary>CATCS is organizing a Wikipedia edit-a-thon at STOC in Phoenix this year. The goal is to create/edit Wikipedia articles on TCS topics that need improvement. (A crowdsourced list of such topics is maintained here.) The event will be held on June 24th, 2019 in West 104A, starting right after the STOC business meeting around 9-9:30 […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>CATCS is organizing a Wikipedia edit-a-thon at <a href="http://acm-stoc.org/stoc2019/">STOC</a> in Phoenix this year. The goal is to create/edit Wikipedia articles on TCS topics that need improvement. (A crowdsourced list of such topics is maintained <a href="https://docs.google.com/spreadsheets/d/1zVUdxKk9nqR5Itwc37v26aRHMqgV9qI8XexssoFq_CE/edit">here</a>.) The event will be held on June 24th, 2019 in West 104A, starting right after the STOC business meeting around 9-9:30 pm.</p>
<p>We invite members of the community to participate. Prior experience with Wikipedia is a plus, but is not necessary. If you are interested in participating, please fill out <a href="https://forms.gle/ZnPkHN1Wc2edAWhB6">this form</a>. Participants are asked to bring their own laptop or other device. Power outlets will be available. Light refreshments will be provided.</p>
<p>If you are interested in helping improve TCS coverage on Wikipedia but are unable to attend this event, please see <a href="https://thmatters.wordpress.com/2017/05/02/tcs-wikipedia-project/">this post</a> for how you can help.</p>
<p> </p></div>
    </content>
    <updated>2019-06-11T13:35:08Z</updated>
    <published>2019-06-11T13:35:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2019-06-19T17:21:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15977</id>
    <link href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/" rel="alternate" type="text/html"/>
    <title>Net-Zero Graphs</title>
    <summary>A new class of undirected graphs with quantum relevance Cropped from source Gustav Kirchhoff was a German physicist active in the mid-1800s. He is known for many things, especially for his “Laws” governing voltage and current in electrical circuits. Today we ask whether anything akin to Kirchhoff’s laws can be formulated for quantum circuits. What […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc">
<em>A new class of undirected graphs with quantum relevance</em>
<font color="#000000">





</font></font></p><table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/345px-gustav_robert_kirchhoff/" rel="attachment wp-att-15978"><img alt="" class="alignright wp-image-15978" height="156" src="https://rjlipton.files.wordpress.com/2019/06/345px-gustav_robert_kirchhoff.jpg?w=102&amp;h=156" width="102"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="https://en.wikipedia.org/wiki/Gustav_Kirchhoff">source</a></font></td>
</tr>
</tbody>
</table><font color="#0044cc"><font color="#000000">



<p>
Gustav Kirchhoff was a German physicist active in the mid-1800s. He is known for many things, especially for his “Laws” governing voltage and current in electrical circuits. Today we ask whether anything akin to Kirchhoff’s laws can be formulated for quantum circuits.

</p><p>
What may be less known about Kirchhoff is that he was a pioneer in graph theory. He proved Kirchhoff’s <a href="https://en.wikipedia.org/wiki/Kirchhoff's_theorem">Theorem</a> that the number of spanning trees equals the determinant of an associated matrix. This shows that the trees can be counted in polynomial time—a cool result. Here we present a class of graphs arising from quantum circuits and associated operations more complex than determinants.

</p><p>
Our search for new graph-based laws was driven by our work on simulations of stabilizer circuits. We have discussed this recently <a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/">here</a> and <a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/">here</a>. The bottom line is this:

</p><p>

</p><blockquote><b> </b> <em> <i>A new class of graphs arises in a natural way and holds a key to improving certain quantum simulations.</i> </em>
</blockquote>


<p>



</p><p>
We call these <i>net-zero graphs</i>. We like to imagine that Kirchhoff would have been interested. We will say more about why after we present the graphs.<span id="more-15977"/>



</p><p>


</p><p>



</p><p>

</p><h2> Net-Zero Graphs </h2>


<p>



</p><p>
The new class of graphs comes from a natural counting problem. Consider black/white two colorings (not necessarily proper) of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> vertices of a graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>, and count the number of edges whose two nodes are both colored black, being called B-B edges. Let <img alt="{c_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_0}"/> be the count of colorings that make an even number of B-B edges and <img alt="{c_1 = 2^n - c_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_1+%3D+2%5En+-+c_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_1 = 2^n - c_0}"/> be the count of colorings that make an odd number of B-B edges. Then <img alt="{a(G) = c_0 - c_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+c_0+-+c_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a(G) = c_0 - c_1}"/> divided by <img alt="{2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^n}"/>. Now we are good to define “Net-Zero” graphs as follows:

</p><p>

</p><blockquote><b> </b> <em> An undirected graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/> is net-zero if <img alt="{a(G) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{a(G) = 0}"/>. </em>
</blockquote>


<p>



</p><p>
Furthermore, we can call <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> <em>net-positive</em> if <img alt="{a &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a &gt; 0}"/> and <em>net-negative</em> if <img alt="{a &lt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3C+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a &lt; 0}"/>. By simple trial and error, the smallest net-zero graph is the triangle graph and the graph made by two triangles sharing an edge is net-zero as well. Here are some connected net-zero graphs of small size:

</p><p>



</p><p>
You might ask why study such labelings of graphs? Why is net-zero an interesting property? An equivalent formulation of <img alt="{a(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a(G)}"/> was given as “<img alt="{Z_{H_2}(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ_%7BH_2%7D%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z_{H_2}(G)}"/>” (divided by <img alt="{2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^n}"/>) in a 2009 <a href="https://arxiv.org/abs/0804.1932">paper</a> by Leslie Goldberg, Martin Grohe, Mark Jerrum, and Marc Thurley, as part of a larger enumeration of polynomial-time cases. Their proof works by reduction to the problem of counting solutions to quadratic polynomials modulo 2, whose time we just <a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/">improved</a> from <img alt="{O(n^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^3)}"/> to <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/>. Their paper does not mention quantum but does involve Hadamard-type matrices. Thus the short answer is that net-zero captures a type of balancing property that is related to understanding quantum circuits. 

</p><p>
<a href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/netzerographs/" rel="attachment wp-att-15981"><img alt="" class="aligncenter wp-image-15981" height="70" src="https://rjlipton.files.wordpress.com/2019/06/netzerographs.png?w=550&amp;h=70" width="550"/></a>


</p><p>

</p><h2> Some Examples and Facts </h2>


<p>



</p><p>
The following elementary facts show how the theory of our graphs takes shape.

</p><p>

</p><blockquote><b>Proposition 1</b> <em> Every cycle graph <img alt="{C_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C_n}"/> with <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> odd is net-zero. </em>
</blockquote>


<p>



</p><p>
This follows because every coloring of <img alt="{C_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_n}"/> has an even number of B-W edges. Hence the number of monochrome edges is odd, and so complementing the coloring flips the parity between B-B and W-W edges. However, having an odd cycle as an induced graph does not make a graph net-zero. An example of this would be a 4-clique graph. Any subset of vertices of size 3 gives an triangle which is net-zero, but the 4-clique itself is net-negative.

</p><p>

</p><blockquote><b>Proposition 2</b> <em> A graph is net-zero if and only if one of its connected components is net-zero. </em>
</blockquote>


<p>



</p><p>
This fact is intuitive when we look at the graph as a tensor product over the connected components, so the colorings to each component are independent. Every coloring <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> on nodes outside the net-zero connected component can be easily extended to one coloring <img alt="{R'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R'}"/> for the entire graph by coloring nodes on the net-zero component, so the difference <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> restricted by <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> is zero.

</p><p>
Now let’s deviate from net-zero graphs. What are some typical net-positive graphs?

</p><p>

</p><blockquote><b>Proposition 3</b> <em> Bipartite graphs are net-positive. </em>
</blockquote>


<p>



</p><p>
To prove this, let <img alt="{V_1, V_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV_1%2C+V_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V_1, V_2}"/> be the two disjoint vertex sets such that each edge connects one node from <img alt="{V_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V_1}"/> and one from <img alt="{V_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V_2}"/>. If all nodes in <img alt="{V_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V_1}"/> are set to be white, then there will be zero B-B edges regardless of how <img alt="{V_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V_2}"/> is acolored, and <img alt="{a &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a &gt; 0}"/> in this case. Now if any of the nodes, say <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/>, in <img alt="{V_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V_1}"/> is colored black, then the number of B-B blacks equals the number of nodes connected to <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> that are colored black, and <img alt="{a = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a = 0}"/> in this situation by straightforward combination calculation. Hence bipartite graphs are net-positive. 

</p><p>
As a consequence, since all trees are bipartite, all trees are net-positive. So is <img alt="{C_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_n}"/> for <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> even. Net-negative graphs may seem to be rarer. We invite readers to work out from Pascal’s triangle when the <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>-clique is net-negative, net-zero, and net-positive. Congruence modulo <img alt="{8}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{8}"/> is involved.

</p><p>
Other interesting examples come from allowing self-loops. The smallest net-zero graph of this kind is a single self-loop. But a 2-node graph with an edge connecting them and two self-loops is net-negative, and so is a graph of two triangles connected by one edge. Pictorially, these two graphs are: 



</p><p>
<a href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/localequiv/" rel="attachment wp-att-15982"><img alt="" class="aligncenter wp-image-15982" height="120" src="https://rjlipton.files.wordpress.com/2019/06/localequiv.png?w=136&amp;h=120" width="136"/></a>

</p><p>
There is a “local equivalence” between a single self-loop and a triangle: Any self-loop in a graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> can be replaced by a triangle using two new vertices, and the resulting graph <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/> will be net-zero if and only if <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is.

</p><p>



</p><p>

</p><h2> The Quantum Connection </h2>


<p>

There is a special class of quantum circuits that relate closely to graphs. They use just two kinds of quantum gates: Hadamard gate and the <img alt="{\mathsf{CZ}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{CZ}}"/> gate. For more on quantum circuits see this elementary <a href="https://rjlipton.wordpress.com/2015/04/08/a-quantum-two-finger-exercise/">post</a> and this more involved <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">post</a>.

</p><p>

</p><blockquote><b>Definition 4</b> <em> Given a graph <img alt="{G = (V,E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G = (V,E)}"/>, the corresponding <em>graph state circuit</em> <img alt="{C_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_G%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C_G}"/> involves <img alt="{n = |V|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+%7CV%7C%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n = |V|}"/> qubits and consists of: 

<ul> 
<li>
An initial Hadamard gate on each qubit line <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i}"/>. 
</li><li>
For every edge <img alt="{(i,j) \in E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28i%2Cj%29+%5Cin+E%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{(i,j) \in E}"/>, a <img alt="{\mathsf{CZ}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{CZ}}"/> gate connecting lines <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i}"/> and <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{j}"/>. The order of placing the <img alt="{\mathsf{CZ}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{CZ}}"/> gates does not matter. 
</li><li>
A closing Hadamard gate on each line <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i}"/>. 
</li></ul>

</em>
</blockquote>


<p>



</p><p>
These circuits are a subset of <em>stabilizer circuits</em>, which we <a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/">have</a> been <a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/">discussing</a>. They become equivalent to stabilizer circuits if we also allow so-called phase gates <img alt="{\mathsf{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{S}}"/> on single qubits, where they are analogous to a loop or “half-loop” at the corresponding vertex. We will stay with the simpler circuits here. The connection to graphs is expressed by:

</p><p>

</p><blockquote><b>Theorem 5</b> <em><a name="amplitude"/> For any graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/>, <img alt="{a(G) = \langle 0^n | C_G | 0^n\rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+%5Clangle+0%5En+%7C+C_G+%7C+0%5En%5Crangle%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{a(G) = \langle 0^n | C_G | 0^n\rangle}"/>, that is, the amplitude of measuring an all-zero output given an all-zero input. In particular, <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/> is net-zero if and only if <img alt="{\langle 0^n | C_G | 0^n\rangle = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7C+C_G+%7C+0%5En%5Crangle+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\langle 0^n | C_G | 0^n\rangle = 0}"/>. </em>
</blockquote>


<p>



</p><p>



</p><p>

</p><h2> Recognizing Net-Zero Graphs </h2>


<p>



</p><p>
Theorem <a href="https://rjlipton.wordpress.com/feed/#amplitude">5</a> implies that whether a graph is net-zero can be decided in <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. The question is, can we improve the time to <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/>, which for dense graphs means linear in the number of edges? The reason why we want to do so is the following further theorem:

</p><p>


</p><p>

</p><blockquote><b>Theorem 6</b> <em> If net-zero graphs of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> nodes with self-loops allowed are recognizable in <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time, then computing the strong simulation probability <img alt="{|\langle 0^n | C | 0^n\rangle|^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+0%5En+%7C+C+%7C+0%5En%5Crangle%7C%5E2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{|\langle 0^n | C | 0^n\rangle|^2}"/> for quantum stabilizer circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/> is <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^2)}"/>-time equivalent to computing <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n \times n}"/> matrix rank over <img alt="{\mathbb{Z}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_2}"/>. </em>
</blockquote>


<p>



</p><p>
This is proved in section 5 of our <a href="https://arxiv.org/abs/1904.00101">paper</a>, which has a duality technique for eliminating the self-loops from phase gates that works for the probability but possibly not for the amplitude. Another way of stating our theorem is:

</p><p>

</p><blockquote><b>Theorem 7</b> <em> Given any <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>-vertex graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/>, we can compute <img alt="{p(G) = a(G)^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%28G%29+%3D+a%28G%29%5E2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{p(G) = a(G)^2}"/> in <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time given only the rank of the adjacency matrix of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/> and the yes/no answer about whether <img alt="{a(G) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{a(G) = 0}"/>. </em>
</blockquote>


<p>



</p><p>
This result extends to computing the probability of any output of a stabilizer circuit given a standard-basis input. This is why the decision problem for recognizing net-zero graphs is important.

</p><p>
In upcoming posts we will connect net-zero graphs further to ideas of circuit “laws” by defining recursions for <img alt="{a(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a(G)}"/>. These recursions do not give efficient algorithms by themselves, but they connect to a wide theory involving graph polynomials and matroids. That theory includes Kirchhoff’s counting of spanning trees as a special case, and we will be interested in which other cases are polynomial-time feasible. This may position quantum computing as a meeting point for closer connections between work such as this 1997 <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.2071">paper</a> by Andrei Broder and Ernst Mayr on counting minimum-weight spanning trees in <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time and the paper by Goldberg et al. mentioned above.

</p><p>


</p><p>



</p><p>

</p><h2> Open Problems </h2>


<p>



</p><p>
What is the complexity of deciding whether a given <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-vertex graph is net-zero? We know it is at worst order-<img alt="{n^\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^\omega}"/>. If it is <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/>, then we obtain a really tight connection between computing matrix rank and computing a quantum simulation probability.

</p><p>
Are there further applications of net-zero graphs?

</p><p>
[gave Kirchhoff his second “h”]



</p><ul class="wp-block-gallery columns-0 is-cropped"/></font></font></div>
    </content>
    <updated>2019-06-10T22:23:56Z</updated>
    <published>2019-06-10T22:23:56Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="Results"/>
    <category term="graph theory"/>
    <category term="graphs"/>
    <category term="Gustav Kirchoff"/>
    <category term="quantum"/>
    <category term="quantum circuits"/>
    <category term="stabilizer circuits"/>
    <author>
      <name>Chaowen Guan and K.W. Regan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-06-19T17:20:46Z</updated>
    </source>
  </entry>
</feed>
