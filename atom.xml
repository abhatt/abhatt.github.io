<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-10-23T04:21:54Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11788</id>
    <link href="http://arxiv.org/abs/2010.11788" rel="alternate" type="text/html"/>
    <title>Equation satisfiability in solvable groups</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Paweł Idziak, Piotr Kawałek, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krzaczkowski:Jacek.html">Jacek Krzaczkowski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei=szlig=:Armin.html">Armin Weiß</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11788">PDF</a><br/><b>Abstract: </b>The study of the complexity of the equation satisfiability problem in finite
groups had been initiated by Goldmann and Russell (2002) where they showed that
this problem is in polynomial time for nilpotent groups while it is NP-complete
for non-solvable groups. Since then, several results have appeared showing that
the problem can be solved in polynomial time in certain solvable groups $G$
having a nilpotent normal subgroup $H$ with nilpotent factor $G/H$. This paper
shows that such normal subgroup must exist in each finite group with equation
satisfiability solvable in polynomial time, unless the Exponential Time
Hypothesis fails.
</p></div>
    </summary>
    <updated>2020-10-23T01:22:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11754</id>
    <link href="http://arxiv.org/abs/2010.11754" rel="alternate" type="text/html"/>
    <title>Separation Results for Boolean Function Classes</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Aniruddha Biswas, Palash Sarkar <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11754">PDF</a><br/><b>Abstract: </b>We show (almost) separation between certain important classes of Boolean
functions. The technique that we use is to show that the total influence of
functions in one class is less than the total influence of functions in the
other class. In particular, we show (almost) separation of several classes of
Boolean functions which have been studied in the coding theory and cryptography
from classes which have been studied in combinatorics and complexity theory.
</p></div>
    </summary>
    <updated>2020-10-23T01:22:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11658</id>
    <link href="http://arxiv.org/abs/2010.11658" rel="alternate" type="text/html"/>
    <title>On the Compressed-Oracle Technique, and Post-Quantum Security of Proofs of Sequential Work</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chung:Kai=Min.html">Kai-Min Chung</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fehr:Serge.html">Serge Fehr</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Yu=Hsuan.html">Yu-Hsuan Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liao:Tai=Ning.html">Tai-Ning Liao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11658">PDF</a><br/><b>Abstract: </b>We revisit the so-called compressed oracle technique, introduced by Zhandry
for analyzing quantum algorithms in the quantum random oracle model (QROM). To
start off with, we offer a concise exposition of the technique, which easily
extends to the parallel-query QROM, where in each query-round the considered
algorithm may make several queries to the QROM in parallel. This variant of the
QROM allows for a more fine-grained query-complexity analysis.
</p>
<p>Our main technical contribution is a framework that simplifies the use of
(the parallel-query generalization of) the compressed oracle technique for
proving query complexity results. With our framework in place, whenever
applicable, it is possible to prove quantum query complexity lower bounds by
means of purely classical reasoning. More than that, for typical examples the
crucial classical observations that give rise to the classical bounds are
sufficient to conclude the corresponding quantum bounds.
</p>
<p>We demonstrate this on a few examples, recovering known results (like the
optimality of parallel Grover), but also obtaining new results (like the
optimality of parallel BHT collision search). Our main target is the hardness
of finding a $q$-chain with fewer than $q$ parallel queries, i.e., a sequence
$x_0, x_1,\ldots, x_q$ with $x_i = H(x_{i-1})$ for all $1 \leq i \leq q$.
</p>
<p>The above problem of finding a hash chain is of fundamental importance in the
context of proofs of sequential work. Indeed, as a concrete cryptographic
application of our techniques, we prove that the "Simple Proofs of Sequential
Work" proposed by Cohen and Pietrzak remains secure against quantum attacks.
Such an analysis is not simply a matter of plugging in our new bound; the
entire protocol needs to be analyzed in the light of a quantum attack. Thanks
to our framework, this can now be done with purely classical reasoning.
</p></div>
    </summary>
    <updated>2020-10-23T01:20:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11632</id>
    <link href="http://arxiv.org/abs/2010.11632" rel="alternate" type="text/html"/>
    <title>The Primal-Dual method for Learning Augmented Algorithms</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bamas:=Eacute=tienne.html">Étienne Bamas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maggiori:Andreas.html">Andreas Maggiori</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svensson:Ola.html">Ola Svensson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11632">PDF</a><br/><b>Abstract: </b>The extension of classical online algorithms when provided with predictions
is a new and active research area. In this paper, we extend the primal-dual
method for online algorithms in order to incorporate predictions that advise
the online algorithm about the next action to take. We use this framework to
obtain novel algorithms for a variety of online covering problems. We compare
our algorithms to the cost of the true and predicted offline optimal solutions
and show that these algorithms outperform any online algorithm when the
prediction is accurate while maintaining good guarantees when the prediction is
misleading.
</p></div>
    </summary>
    <updated>2020-10-23T01:31:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11629</id>
    <link href="http://arxiv.org/abs/2010.11629" rel="alternate" type="text/html"/>
    <title>Learning Augmented Energy Minimization via Speed Scaling</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bamas:=Eacute=tienne.html">Étienne Bamas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maggiori:Andreas.html">Andreas Maggiori</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohwedder:Lars.html">Lars Rohwedder</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svensson:Ola.html">Ola Svensson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11629">PDF</a><br/><b>Abstract: </b>As power management has become a primary concern in modern data centers,
computing resources are being scaled dynamically to minimize energy
consumption. We initiate the study of a variant of the classic online speed
scaling problem, in which machine learning predictions about the future can be
integrated naturally. Inspired by recent work on learning-augmented online
algorithms, we propose an algorithm which incorporates predictions in a
black-box manner and outperforms any online algorithm if the accuracy is high,
yet maintains provable guarantees if the prediction is very inaccurate. We
provide both theoretical and experimental evidence to support our claims.
</p></div>
    </summary>
    <updated>2020-10-23T01:35:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11620</id>
    <link href="http://arxiv.org/abs/2010.11620" rel="alternate" type="text/html"/>
    <title>From trees to barcodes and back again: theoretical and statistical perspectives</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kanari:Lida.html">Lida Kanari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garin:Ad=eacute=lie.html">Adélie Garin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hess:Kathryn.html">Kathryn Hess</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11620">PDF</a><br/><b>Abstract: </b>Methods of topological data analysis have been successfully applied in a wide
range of fields to provide useful summaries of the structure of complex data
sets in terms of topological descriptors, such as persistence diagrams. While
there are many powerful techniques for computing topological descriptors, the
inverse problem, i.e., recovering the input data from topological descriptors,
has proved to be challenging. In this article we study in detail the
Topological Morphology Descriptor (TMD), which assigns a persistence diagram to
any tree embedded in Euclidean space, and a sort of stochastic inverse to the
TMD, the Topological Neuron Synthesis (TNS) algorithm, gaining both theoretical
and computational insights into the relation between the two. We propose a new
approach to classify barcodes using symmetric groups, which provides a concrete
language to formulate our results. We investigate to what extent the TNS
recovers a geometric tree from its TMD and describe the effect of different
types of noise on the process of tree generation from persistence diagrams. We
prove moreover that the TNS algorithm is stable with respect to specific types
of noise.
</p></div>
    </summary>
    <updated>2020-10-23T01:31:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11571</id>
    <link href="http://arxiv.org/abs/2010.11571" rel="alternate" type="text/html"/>
    <title>A 4-Approximation of the $\frac{2\pi}{3}$-MST</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ashur:Stav.html">Stav Ashur</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Katz:Matthew_J=.html">Matthew J. Katz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11571">PDF</a><br/><b>Abstract: </b>Bounded-angle (minimum) spanning trees were first introduced in the context
of wireless networks with directional antennas. They are reminiscent of
bounded-degree spanning trees, which have received significant attention. Let
$P = \{p_1,\ldots,p_n\}$ be a set of $n$ points in the plane, let $\Pi$ be the
polygonal path $(p_1,\ldots,p_n)$, and let $0 &lt; \alpha &lt; 2\pi$ be an angle. An
$\alpha$-spanning tree ($\alpha$-ST) of $P$ is a spanning tree of the complete
Euclidean graph over $P$, with the following property: For each vertex $p_i \in
P$, the (smallest) angle that is spanned by all the edges incident to $p_i$ is
at most $\alpha$. An $\alpha$-minimum spanning tree ($\alpha$-MST) is an
$\alpha$-ST of $P$ of minimum weight, where the weight of an $\alpha$-ST is the
sum of the lengths of its edges. In this paper, we consider the problem of
computing an $\alpha$-MST, for the important case where $\alpha =
\frac{2\pi}{3}$. We present a simple 4-approximation algorithm, thus improving
upon the previous results of Aschner and Katz and Biniaz et al., who presented
algorithms with approximation ratios 6 and $\frac{16}{3}$, respectively.
</p>
<p>In order to obtain this result, we devise a simple $O(n)$-time algorithm for
constructing a $\frac{2\pi}{3}$-ST\, ${\cal T}$ of $P$, such that ${\cal T}$'s
weight is at most twice that of $\Pi$ and, moreover, ${\cal T}$ is a 3-hop
spanner of $\Pi$. This latter result is optimal in the sense that for any
$\varepsilon &gt; 0$ there exists a polygonal path for which every
$\frac{2\pi}{3}$-ST has weight greater than $2-\varepsilon$ times the weight of
the path.
</p></div>
    </summary>
    <updated>2020-10-23T01:37:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11497</id>
    <link href="http://arxiv.org/abs/2010.11497" rel="alternate" type="text/html"/>
    <title>Cluster-and-Conquer: When Randomness Meets Graph Locality</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giakkoupis:George.html">George Giakkoupis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kermarrec:Anne=Marie.html">Anne-Marie Kermarrec</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ruas:Olivier.html">Olivier Ruas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Ta=iuml=ani:Fran=ccedil=ois.html">François Taïani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11497">PDF</a><br/><b>Abstract: </b>K-Nearest-Neighbors (KNN) graphs are central to many emblematic data mining
and machine-learning applications. Some of the most efficient KNN graph
algorithms are incremental and local: they start from a random graph, which
they incrementally improve by traversing neighbors-of-neighbors links.
Paradoxically, this random start is also one of the key weaknesses of these
algorithms: nodes are initially connected to dissimilar neighbors, that lie far
away according to the similarity metric. As a result, incremental algorithms
must first laboriously explore spurious potential neighbors before they can
identify similar nodes, and start converging. In this paper, we remove this
drawback with Cluster-and-Conquer (C 2 for short). Cluster-and-Conquer boosts
the starting configuration of greedy algorithms thanks to a novel lightweight
clustering mechanism, dubbed FastRandomHash. FastRandomHash leverages
random-ness and recursion to pre-cluster similar nodes at a very low cost. Our
extensive evaluation on real datasets shows that Cluster-and-Conquer
significantly outperforms existing approaches, including LSH, yielding
speed-ups of up to x4.42 while incurring only a negligible loss in terms of KNN
quality.
</p></div>
    </summary>
    <updated>2020-10-23T01:33:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11462</id>
    <link href="http://arxiv.org/abs/2010.11462" rel="alternate" type="text/html"/>
    <title>Polynomial Delay Enumeration for Minimal Steiner Problems</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kobayashi:Yasuaki.html">Yasuaki Kobayashi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurita:Kazuhiro.html">Kazuhiro Kurita</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wasa:Kunihiro.html">Kunihiro Wasa</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11462">PDF</a><br/><b>Abstract: </b>Let $G = (V, E)$ be a undirected graph and let $W \subseteq V$ be a set of
terminals. A \emph{Steiner subgraph} of $(G, W)$ is a subgraph of $G$ that
contains all vertices of $W$ and there is a path between every pair of vertices
of $W$ in the subgraph. We say that a Steiner subgraph is minimal if it has no
proper Steiner subgraph. It is easy to observe that every minimal Steiner
subgraph forms a tree, which is called a minimal Steiner tree. We propose a
linear delay and polynomial space algorithm for enumerating all minimal Steiner
trees of $(G, W)$, which improves a previously known polynomial delay
enumeration algorithm in [Kimelfeld and Sagiv, Inf. Syst., 2008]. Our
enumeration algorithm can be extended to other Steiner problems: minimal
Steiner forests, minimal terminal Steiner trees, minimal directed Steiner
trees. As another variant of the minimal Steiner subgraph problem, we study the
problem of enumerating minimal induced Steiner subgraphs. We propose a
polynomial delay and exponential space enumeration algorithm of minimal induced
Steiner subgraphs for claw-free graphs, whereas the problem on general graphs
is shown to be at least as hard as the problem of enumerating minimal
transversals in hypergraphs. Contrary to these tractable results, we show that
the problem of enumerating minimal group Steiner trees is at least as hard as
the minimal transversal enumeration problem on hypergraphs.
</p></div>
    </summary>
    <updated>2020-10-23T01:36:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11456</id>
    <link href="http://arxiv.org/abs/2010.11456" rel="alternate" type="text/html"/>
    <title>An Investigation of the Recoverable Robust Assignment Problem</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Dennis.html">Dennis Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hartmann:Tim_A=.html">Tim A. Hartmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lendl:Stefan.html">Stefan Lendl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woeginger:Gerhard_J=.html">Gerhard J. Woeginger</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11456">PDF</a><br/><b>Abstract: </b>We investigate the so-called recoverable robust assignment problem on
balanced bipartite graphs with $2n$ vertices, a mainstream problem in robust
optimization: For two given linear cost functions $c_1$ and $c_2$ on the edges
and a given integer $k$, the goal is to find two perfect matchings $M_1$ and
$M_2$ that minimize the objective value $c_1(M_1)+c_2(M_2)$, subject to the
constraint that $M_1$ and $M_2$ have at least $k$ edges in common.
</p>
<p>We derive a variety of results on this problem. First, we show that the
problem is W[1]-hard with respect to the parameter $k$, and also with respect
to the recoverability parameter $k'=n-k$. This hardness result holds even in
the highly restricted special case where both cost functions $c_1$ and $c_2$
only take the values $0$ and $1$. (On the other hand, containment of the
problem in XP is straightforward to see.) Next, as a positive result we
construct a polynomial time algorithm for the special case where one cost
function is Monge, whereas the other one is Anti-Monge. Finally, we study the
variant where matching $M_1$ is frozen, and where the optimization goal is to
compute the best corresponding matching $M_2$, the second stage recoverable
assignment problem. We show that this problem variant is contained in the
randomized parallel complexity class $\text{RNC}_2$, and that it is at least as
hard as the infamous problem \probl{Exact Matching in Red-Blue Bipartite
Graphs} whose computational complexity is a long-standing open problem
</p></div>
    </summary>
    <updated>2020-10-23T01:22:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11450</id>
    <link href="http://arxiv.org/abs/2010.11450" rel="alternate" type="text/html"/>
    <title>Optimal Approximation -- Smoothness Tradeoffs for Soft-Max Functions</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Epasto:Alessandro.html">Alessandro Epasto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahdian:Mohammad.html">Mohammad Mahdian</a>, Vahab Mirrokni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zampetakis:Manolis.html">Manolis Zampetakis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11450">PDF</a><br/><b>Abstract: </b>A soft-max function has two main efficiency measures: (1) approximation -
which corresponds to how well it approximates the maximum function, (2)
smoothness - which shows how sensitive it is to changes of its input. Our goal
is to identify the optimal approximation-smoothness tradeoffs for different
measures of approximation and smoothness. This leads to novel soft-max
functions, each of which is optimal for a different application. The most
commonly used soft-max function, called exponential mechanism, has optimal
tradeoff between approximation measured in terms of expected additive
approximation and smoothness measured with respect to R\'enyi Divergence. We
introduce a soft-max function, called "piecewise linear soft-max", with optimal
tradeoff between approximation, measured in terms of worst-case additive
approximation and smoothness, measured with respect to $\ell_q$-norm. The
worst-case approximation guarantee of the piecewise linear mechanism enforces
sparsity in the output of our soft-max function, a property that is known to be
important in Machine Learning applications [Martins et al. '16, Laha et al.
'18] and is not satisfied by the exponential mechanism. Moreover, the
$\ell_q$-smoothness is suitable for applications in Mechanism Design and Game
Theory where the piecewise linear mechanism outperforms the exponential
mechanism. Finally, we investigate another soft-max function, called power
mechanism, with optimal tradeoff between expected \textit{multiplicative}
approximation and smoothness with respect to the R\'enyi Divergence, which
provides improved theoretical and practical results in differentially private
submodular optimization.
</p></div>
    </summary>
    <updated>2020-10-23T01:29:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11443</id>
    <link href="http://arxiv.org/abs/2010.11443" rel="alternate" type="text/html"/>
    <title>Optimal Robustness-Consistency Trade-offs for Learning-Augmented Online Algorithms</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei:Alexander.html">Alexander Wei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Fred.html">Fred Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11443">PDF</a><br/><b>Abstract: </b>We study the problem of improving the performance of online algorithms by
incorporating machine-learned predictions. The goal is to design algorithms
that are both consistent and robust, meaning that the algorithm performs well
when predictions are accurate and maintains worst-case guarantees. Such
algorithms have been studied in a recent line of works due to Lykouris and
Vassilvitskii (ICML '18) and Purohit et al (NeurIPS '18). They provide
robustness-consistency trade-offs for a variety of online problems. However,
they leave open the question of whether these trade-offs are tight, i.e., to
what extent to such trade-offs are necessary. In this paper, we provide the
first set of non-trivial lower bounds for competitive analysis using
machine-learned predictions. We focus on the classic problems of ski-rental and
non-clairvoyant scheduling and provide optimal trade-offs in various settings.
</p></div>
    </summary>
    <updated>2020-10-23T01:30:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11440</id>
    <link href="http://arxiv.org/abs/2010.11440" rel="alternate" type="text/html"/>
    <title>Vertex deletion into bipartite permutation graphs</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Łukasz Bożyk, Jan Derbisz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krawczyk:Tomasz.html">Tomasz Krawczyk</a>, Jana Novotná, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Okrasa:Karolina.html">Karolina Okrasa</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11440">PDF</a><br/><b>Abstract: </b>A permutation graph can be defined as an intersection graph of segments whose
endpoints lie on two parallel lines $l_1$ and $l_2$, one on each. A bipartite
permutation graph is a permutation graph which is bipartite. In this paper we
study the parameterized complexity of the bipartite permutation vertex deletion
problem, which asks, for a given n-vertex graph, whether we can remove at most
k vertices to obtain a bipartite permutation graph. This problem is NP-complete
by the classical result of Lewis and Yannakakis. We analyze the structure of
the so-called almost bipartite permutation graphs which may contain holes
(large induced cycles) in contrast to bipartite permutation graphs. We exploit
the structural properties of the shortest hole in a such graph. We use it to
obtain an algorithm for the bipartite permutation vertex deletion problem with
running time $O(9^k\cdot n^9)$, and also give a polynomial-time 9-approximation
algorithm.
</p></div>
    </summary>
    <updated>2020-10-23T01:29:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11420</id>
    <link href="http://arxiv.org/abs/2010.11420" rel="alternate" type="text/html"/>
    <title>Deterministic Approximation for Submodular Maximization over a Matroid in Nearly Linear Time</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Han:Kai.html">Kai Han</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cao:Zongmai.html">Zongmai Cao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cui:Shuang.html">Shuang Cui</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Benwei.html">Benwei Wu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11420">PDF</a><br/><b>Abstract: </b>We study the problem of maximizing a non-monotone, non-negative submodular
function subject to a matroid constraint. The prior best-known deterministic
approximation ratio for this problem is $\frac{1}{4}-\epsilon$ under
$\mathcal{O}(({n^4}/{\epsilon})\log n)$ time complexity. We show that this
deterministic ratio can be improved to $\frac{1}{4}$ under $\mathcal{O}(nr)$
time complexity, and then present a more practical algorithm dubbed
TwinGreedyFast which achieves $\frac{1}{4}-\epsilon$ deterministic ratio in
nearly-linear running time of
$\mathcal{O}(\frac{n}{\epsilon}\log\frac{r}{\epsilon})$. Our approach is based
on a novel algorithmic framework of simultaneously constructing two candidate
solution sets through greedy search, which enables us to get improved
performance bounds by fully exploiting the properties of independence systems.
As a byproduct of this framework, we also show that TwinGreedyFast achieves
$\frac{1}{2p+2}-\epsilon$ deterministic ratio under a $p$-set system constraint
with the same time complexity. To showcase the practicality of our approach, we
empirically evaluated the performance of TwinGreedyFast on two network
applications, and observed that it outperforms the state-of-the-art
deterministic and randomized algorithms with efficient implementations for our
problem.
</p></div>
    </summary>
    <updated>2020-10-23T01:32:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11381</id>
    <link href="http://arxiv.org/abs/2010.11381" rel="alternate" type="text/html"/>
    <title>Query strategies for priced information, revisited</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lange:Jane.html">Jane Lange</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11381">PDF</a><br/><b>Abstract: </b>We consider the problem of designing query strategies for priced information,
introduced by Charikar et al. In this problem the algorithm designer is given a
function $f : \{0,1\}^n \to \{-1,1\}$ and a price associated with each of the
$n$ coordinates. The goal is to design a query strategy for determining $f$'s
value on unknown inputs for minimum cost.
</p>
<p>Prior works on this problem have focused on specific classes of functions. We
analyze a simple and natural strategy that applies to all functions $f$, and
show that its performance relative to the optimal strategy can be expressed in
terms of a basic complexity measure of $f$, its influence. For $\varepsilon \in
(0,\frac1{2})$, writing $\mathsf{opt}$ to denote the expected cost of the
optimal strategy that errs on at most an $\varepsilon$-fraction of inputs, our
strategy has expected cost $\mathsf{opt} \cdot \mathrm{Inf}(f)/\varepsilon^2$
and also errs on at most an $O(\varepsilon)$-fraction of inputs. This
connection yields new guarantees that complement existing ones for a number of
function classes that have been studied in this context, as well as new
guarantees for new classes.
</p>
<p>Finally, we show that improving on the parameters that we achieve will
require making progress on the longstanding open problem of properly learning
decision trees.
</p></div>
    </summary>
    <updated>2020-10-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11252</id>
    <link href="http://arxiv.org/abs/2010.11252" rel="alternate" type="text/html"/>
    <title>On Adaptive Distance Estimation</title>
    <feedworld_mtime>1603411200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cherapanamjeri:Yeshwanth.html">Yeshwanth Cherapanamjeri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nelson:Jelani.html">Jelani Nelson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11252">PDF</a><br/><b>Abstract: </b>We provide a static data structure for distance estimation which supports
{\it adaptive} queries. Concretely, given a dataset $X = \{x_i\}_{i = 1}^n$ of
$n$ points in $\mathbb{R}^d$ and $0 &lt; p \leq 2$, we construct a randomized data
structure with low memory consumption and query time which, when later given
any query point $q \in \mathbb{R}^d$, outputs a $(1+\epsilon)$-approximation of
$\lVert q - x_i \rVert_p$ with high probability for all $i\in[n]$. The main
novelty is our data structure's correctness guarantee holds even when the
sequence of queries can be chosen adaptively: an adversary is allowed to choose
the $j$th query point $q_j$ in a way that depends on the answers reported by
the data structure for $q_1,\ldots,q_{j-1}$. Previous randomized Monte Carlo
methods do not provide error guarantees in the setting of adaptively chosen
queries. Our memory consumption is $\tilde O((n+d)d/\epsilon^2)$, slightly more
than the $O(nd)$ required to store $X$ in memory explicitly, but with the
benefit that our time to answer queries is only $\tilde O(\epsilon^{-2}(n +
d))$, much faster than the naive $\Theta(nd)$ time obtained from a linear scan
in the case of $n$ and $d$ very large. Here $\tilde O$ hides
$\log(nd/\epsilon)$ factors. We discuss applications to nearest neighbor search
and nonparametric estimation.
</p>
<p>Our method is simple and likely to be applicable to other domains: we
describe a generic approach for transforming randomized Monte Carlo data
structures which do not support adaptive queries to ones that do, and show that
for the problem at hand, it can be applied to standard nonadaptive solutions to
$\ell_p$ norm estimation with negligible overhead in query time and a factor
$d$ overhead in memory.
</p></div>
    </summary>
    <updated>2020-10-23T01:33:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-23T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/</id>
    <link href="https://cstheory-jobs.org/2020/10/22/tenure-track-assistant-professor-at-rutgers-university-apply-by-january-15-2021/" rel="alternate" type="text/html"/>
    <title>Tenure-Track Assistant Professor at Rutgers University (apply by January 15, 2021)</title>
    <summary>The Computer Science Department at Rutgers University invites applications for a Tenure-Track Assistant Professor position in Theoretical Computer Science. We welcome candidates working on computational complexity theory but outstanding applicants in all areas of TCS will be considered. Website: http://jobs.rutgers.edu/postings/120527 Email: martin@farach-colton.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science Department at Rutgers University invites applications for a Tenure-Track Assistant Professor position in Theoretical Computer Science. We welcome candidates working on computational complexity theory but outstanding applicants in all areas of TCS will be considered.</p>
<p>Website: <a href="http://jobs.rutgers.edu/postings/120527">http://jobs.rutgers.edu/postings/120527</a><br/>
Email: martin@farach-colton.com</p></div>
    </content>
    <updated>2020-10-22T18:10:24Z</updated>
    <published>2020-10-22T18:10:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-23T04:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/22/professorship-chair-at-tu-hamburg-apply-by-november-29-2020/" rel="alternate" type="text/html"/>
    <title>Professorship/Chair at TU Hamburg (apply by November 29, 2020)</title>
    <summary>TU Hamburg invites applications for a full professorship (chair) in Hardware-aware Combinatorial Optimization, at its School of Electrical Engineering, Computer Science and Mathematics. Our goal is to establish a group that excels at developing and implementing state-of-the-art optimization techniques on modern computing architectures at hardware level. The chair is endowed by Fujitsu. Website: https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1 Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>TU Hamburg invites applications for a full professorship (chair) in Hardware-aware Combinatorial Optimization, at its School of Electrical Engineering, Computer Science and Mathematics. Our goal is to establish a group that excels at developing and implementing state-of-the-art optimization techniques on modern computing architectures at hardware level. The chair is endowed by Fujitsu.</p>
<p>Website: <a href="https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1">https://stellenportal.tuhh.de/jobposting/4e98d49c7dee223203482d1e1316990eac777fe1</a><br/>
Email: berufungen@tuhh.de</p></div>
    </content>
    <updated>2020-10-22T16:13:33Z</updated>
    <published>2020-10-22T16:13:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-23T04:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17694</id>
    <link href="https://rjlipton.wordpress.com/2020/10/22/vaccines-are-not-developing/" rel="alternate" type="text/html"/>
    <title>Vaccines are Not Developing</title>
    <summary>The search for a vaccine—is not a development. Edward Jenner was an English physician who created the first vaccine, one for smallpox. In 1798 he used the weak cox-pox to fool our immune system to create a protection against the deadly small-pox. Jenner is said to have saved more lives than any other human. Today […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="color: #0044cc;"><br/>
<em>The search for a vaccine—is not a development.</em><br/>
</span></p>
<p>Edward Jenner was an English physician who created the first vaccine, one for smallpox.<br/>
<a href="https://rjlipton.wordpress.com/jenner/"><img alt="" class="alignright size-full wp-image-17696" src="https://rjlipton.files.wordpress.com/2020/10/jenner.jpg?w=600"/></a><br/>
In 1798 he used the weak cox-pox to fool our immune system to create a protection against the deadly small-pox. Jenner is said to have <i>saved more lives than any other human.</i></p>
<p>Today there is an attempt to create a vaccine against our small-pox of the 21st century.</p>
<p>In his day small-pox killed 10% or more of populations. In our day there is a similar threat. and thus the immense interest in the development of a vaccine. However, there is a misunderstanding about vaccines for COVID-19 that is pervasive. Read the New York Times or watch cable news—CNN, FOX, MSNBC—where “experts” explain how AstraZeneca, Johnson &amp; Johnson, Novavax, and other drug companies are developing a vaccine. What developing means could potentially affect all of us, and a better understanding could save millions of lives.</p>
<p>They are not currently <i>developing</i> the vaccines, they are <i>testing</i> them.  The point we want to emphasize is:</p>
<blockquote>
<p><b> </b> <em> <i>The development of a vaccine does not change the vaccine. The vaccine is the same at the start of its testing trials, and remains the same throughout.</i> </em></p>
</blockquote>
<p>The Oxford vaccine AZD1222 is the same today as it was months ago when it was created. The same is true for the other vaccines currently being tested around the world.</p>
<p>A vaccine is <b>not</b> developed in the usual sense. Drug companies can modify: how the drug is made, how it is stored, how it is given, how many doses are needed, and so on. Drug companies cannot modify the vaccine without starting over—the vaccine must remain the same. Trials can lead to a vaccine being adopted, or it can cause the vaccine to be abandoned. In the later case the drug company can try again, but with a different vaccine.</p>
<h2>Not Development</h2>
<p>Think of the what development means elsewhere.</p>
<ul>
<li>In programming an app: We build a version and try it out. We find bugs and fix them. We use version numbers. Note, there is no AZD1222 version 3.</li>
<li>In writing a book: We make a draft. We have people read the draft. We fix typos and inaccuracies. Our quantum book’s <img alt="{2^{nd}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bnd%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2^{nd}}"/> edition is on version 11.</li>
<li>In engineering a product: You get the idea.</li>
</ul>
<p>Here is a sample explaining vaccine <a href="https://www.rivm.nl/en/novel-coronavirus-covid-19/vaccine-against-covid-19">development</a>:</p>
<ul>
<li>Phase I: The vaccine is given to healthy volunteers to see whether it is safe and what dosage (quantity) is most effective.</li>
<li>Phase II: The vaccine is given to target groups that would be vaccinated to see what dosage (quantity) is most effective and how well the immune system responds to it.</li>
<li>Phase III: The vaccine is given to an even larger group, consisting of thousands of people, to see how well the vaccine works to prevent COVID-19. People who do receive the vaccine are then compared with people who did not receive the vaccine.Note: there is no step that modifies the vaccine.
</li></ul>
<h2>Consequences</h2>
<p>There are several consequences from this insight about vaccines. For one it makes sense to order millions of doses of a vaccine, even one that has not yet been proved to be safe and effective. For example,</p>
<blockquote>
<p><b> </b> <em> The European Commission has placed its first advance order for a coronavirus vaccine, snapping up 300 million doses of AstraZeneca’s AZD1222 candidate developed by the University of Oxford, with an option on another 100 million. </em></p>
</blockquote>
<p>Note we would never order a large number of copies of a book before all editing and typos were fixed. This is a “proof” that the vaccine is the same.</p>
<p>Actually it may make sense to even begin to take the vaccine. Especially for high risk people. In the past inventors of vaccines have often taken their own new vaccine, even before they were sure they worked.</p>
<h2>Open Problems</h2>
<p>I am a computer scientist with no experience in vaccines. In 1954 I did help test the Jonas Salk polio vaccine. My help was in the form supplying an arm that got a shot of the Salk polio vaccine, I was nine years old then. But I have a math view of vaccines—a viewpoint that sheds light on this misunderstanding.</p>

</div>
    </content>
    <updated>2020-10-22T15:50:23Z</updated>
    <published>2020-10-22T15:50:23Z</published>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-10-23T04:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=499</id>
    <link href="https://tcsplus.wordpress.com/2020/10/22/tcs-talk-wednesday-october-28-omar-montasser-ttic/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 28 — Omar Montasser, TTIC</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Omar Montasser from TTIC will speak about “Adversarially Robust Learnability: Characterization and Reductions” (abstract below). You can reserve a spot as an individual or a group to join […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 28th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong>Omar Montasser</strong> from TTIC will speak about “<em>Adversarially Robust Learnability: Characterization and Reductions</em>” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We study the question of learning an adversarially robust predictor from uncorrupted samples. We show that any VC class <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="H"/> is robustly PAC learnable, but we also show that such learning must sometimes be improper (i.e. use predictors from outside the class), as some VC classes are not robustly properly learnable.  In particular, the popular robust empirical risk minimization approach (also known as adversarial training), which is proper, cannot robustly learn all VC classes.  After establishing learnability, we turn to ask whether having a tractable non-robust learning algorithm is sufficient for tractable robust learnability and give a reduction algorithm for robustly learning any hypothesis class <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="H"/> using a non-robust PAC learner for <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="H"/>, with nearly-optimal oracle complexity.<br/>This is based on joint work with Steve Hanneke and Nati Srebro, available at <a href="https://arxiv.org/abs/1902.04217">https://arxiv.org/abs/1902.04217</a>.</p></div>
    </content>
    <updated>2020-10-22T14:48:14Z</updated>
    <published>2020-10-22T14:48:14Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-10-23T04:21:20Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/22/computer-science-tenured-tenure-track-at-nyu-shanghai-apply-by-february-1-2021/</id>
    <link href="https://cstheory-jobs.org/2020/10/22/computer-science-tenured-tenure-track-at-nyu-shanghai-apply-by-february-1-2021/" rel="alternate" type="text/html"/>
    <title>Computer Science, Tenured/Tenure-track at NYU Shanghai (apply by February 1, 2021)</title>
    <summary>NYU Shanghai is currently inviting applications for a Tenured or Tenure-track position in Computer Science Theory. The search is not restricted to any rank. We invite candidates with a strong research record in CS theory to apply, including research in algorithms, data structures, computational complexity, cryptography, learning theory, and so on. Website: https://apply.interfolio.com/80168 Email: shanghai.faculty.recruitment@nyu.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>NYU Shanghai is currently inviting applications for a Tenured or Tenure-track position in Computer Science Theory. The search is not restricted to any rank. We invite candidates with a strong research record in CS theory to apply, including research in algorithms, data structures, computational complexity, cryptography, learning theory, and so on.</p>
<p>Website: <a href="https://apply.interfolio.com/80168">https://apply.interfolio.com/80168</a><br/>
Email: shanghai.faculty.recruitment@nyu.edu</p></div>
    </content>
    <updated>2020-10-22T13:41:49Z</updated>
    <published>2020-10-22T13:41:49Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-23T04:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/156</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/156" rel="alternate" type="text/html"/>
    <title>TR20-156 |  Codes over integers, and the singularity of random matrices with large entries | 

	Sankeerth Rao Karingula, 

	Shachar Lovett</title>
    <summary>The prototypical construction of error correcting codes is based on linear codes over finite fields. In this work, we make first steps in the study of codes defined over integers. We focus on Maximum Distance Separable (MDS) codes, and show that MDS codes with linear rate and distance can be realized over the integers with a constant alphabet size. This is in contrast to the situation over finite fields, where a linear size finite field is needed.

The core of this paper is a new result on the singularity probability of random matrices. We show that for a random $n \times n$ matrix with entries chosen independently from the range $\{-m,\ldots,m\}$, the probability that it is singular is at most $m^{-cn}$ for some absolute constant $c&gt;0$.</summary>
    <updated>2020-10-22T05:36:16Z</updated>
    <published>2020-10-22T05:36:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-23T04:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.11064</id>
    <link href="http://arxiv.org/abs/2010.11064" rel="alternate" type="text/html"/>
    <title>Smoothed Analysis of Pareto Curves in Multiobjective Optimization</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/R=ouml=glin:Heiko.html">Heiko Röglin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.11064">PDF</a><br/><b>Abstract: </b>In a multiobjective optimization problem a solution is called Pareto-optimal
if no criterion can be improved without deteriorating at least one of the other
criteria. Computing the set of all Pareto-optimal solutions is a common task in
multiobjective optimization to filter out unreasonable trade-offs.
</p>
<p>For most problems the number of Pareto-optimal solutions increases only
moderately with the input size in applications. However, for virtually every
multiobjective optimization problem there exist worst-case instances with an
exponential number of Pareto-optimal solutions. In order to explain this
discrepancy, we analyze a large class of multiobjective optimization problems
in the model of smoothed analysis and prove a polynomial bound on the expected
number of Pareto-optimal solutions.
</p>
<p>We also present algorithms for computing the set of Pareto-optimal solutions
for different optimization problems and discuss related results on the smoothed
complexity of optimization problems.
</p></div>
    </summary>
    <updated>2020-10-22T23:25:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10997</id>
    <link href="http://arxiv.org/abs/2010.10997" rel="alternate" type="text/html"/>
    <title>Rigid continuation paths II. Structured polynomial systems</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=uuml=rgisser:Peter.html">Peter Bürgisser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cucker:Felipe.html">Felipe Cucker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lairez:Pierre.html">Pierre Lairez</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10997">PDF</a><br/><b>Abstract: </b>We design a probabilistic algorithm that, given $\epsilon&gt;0$ and a polynomial
system $F$ given by black-box evaluation functions, outputs an approximate zero
of $F$, in the sense of Smale, with probability at least $1-\epsilon$. When
applying this algorithm to $u \cdot F$, where $u$ is uniformly random in the
product of unitary groups, the algorithm performs $\operatorname{poly}(n,
\delta) \cdot L(F) \cdot \left( \Gamma(F) \log \Gamma(F) + \log \log
\epsilon^{-1} \right)$ operations on average. Here $n$ is the number of
variables, $\delta$ the maximum degree, $L(F)$ denotes the evaluation cost of
$F$, and $\Gamma(F)$ reflects an aspect of the numerical condition of $F$.
Moreover, we prove that for inputs given by random Gaussian algebraic branching
programs of size $\operatorname{poly}(n,\delta)$, the algorithm runs on average
in time polynomial in $n$ and $\delta$. Our result may be interpreted as an
affirmative answer to a refined version of Smale's 17th question, concerned
with systems of \emph{structured} polynomial equations.
</p></div>
    </summary>
    <updated>2020-10-22T23:20:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10881</id>
    <link href="http://arxiv.org/abs/2010.10881" rel="alternate" type="text/html"/>
    <title>Multi-Dimensional Randomized Response</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Domingo=Ferrer:Josep.html">Josep Domingo-Ferrer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soria=Comas:Jordi.html">Jordi Soria-Comas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10881">PDF</a><br/><b>Abstract: </b>In our data world, a host of not necessarily trusted controllers gather data
on individual subjects. To preserve her privacy and, more generally, her
informational self-determination, the individual has to be empowered by giving
her agency on her own data. Maximum agency is afforded by local anonymization,
that allows each individual to anonymize her own data before handing them to
the data controller. Randomized response (RR) is a local anonymization approach
able to yield multi-dimensional full sets of anonymized microdata that are
valid for exploratory analysis and machine learning. This is so because an
unbiased estimate of the distribution of the true data of individuals can be
obtained from their pooled randomized data. Furthermore, RR offers rigorous
privacy guarantees. The main weakness of RR is the curse of dimensionality when
applied to several attributes: as the number of attributes grows, the accuracy
of the estimated true data distribution quickly degrades. We propose several
complementary approaches to mitigate the dimensionality problem. First, we
present two basic protocols, separate RR on each attribute and joint RR for all
attributes, and discuss their limitations. Then we introduce an algorithm to
form clusters of attributes so that attributes in different clusters can be
viewed as independent and joint RR can be performed within each cluster. After
that, we introduce an adjustment algorithm for the randomized data set that
repairs some of the accuracy loss due to assuming independence between
attributes when using RR separately on each attribute or due to assuming
independence between clusters in cluster-wise RR. We also present empirical
work to illustrate the proposed methods.
</p></div>
    </summary>
    <updated>2020-10-22T23:21:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10879</id>
    <link href="http://arxiv.org/abs/2010.10879" rel="alternate" type="text/html"/>
    <title>Tangent Quadrics in Real 3-Space</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brysiewicz:Taylor.html">Taylor Brysiewicz</a>, Claudia Fevola, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sturmfels:Bernd.html">Bernd Sturmfels</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10879">PDF</a><br/><b>Abstract: </b>We examine quadratic surfaces in 3-space that are tangent to nine given
figures. These figures can be points, lines, planes or quadrics. The numbers of
tangent quadrics were determined by Hermann Schubert in 1879. We study the
associated systems of polynomial equations, also in the space of complete
quadrics, and we solve them using certified numerical methods. Our aim is to
show that Schubert's problems are fully real.
</p></div>
    </summary>
    <updated>2020-10-22T23:26:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10809</id>
    <link href="http://arxiv.org/abs/2010.10809" rel="alternate" type="text/html"/>
    <title>A Note on the Approximability of Deepest-Descent Circuit Steps</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Borgwardt:Steffen.html">Steffen Borgwardt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brand:Cornelius.html">Cornelius Brand</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldmann:Andreas_Emil.html">Andreas Emil Feldmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kouteck=yacute=:Martin.html">Martin Koutecký</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10809">PDF</a><br/><b>Abstract: </b>Linear programs (LPs) can be solved through a polynomial number of so-called
deepest-descent circuit steps, each step following a circuit direction to a new
feasible solution of most-improved objective function value. A computation of
deepest-descent steps has recently been shown to be NP-hard [De Loera et al.,
arXiv, 2019]. This is a consequence of the hardness of what we call the optimal
circuit-neighbor problem (OCNP) for LPs with non-unique optima. However, the
non-uniqueness assumption is crucial to the hardness of OCNP, because we show
that OCNP for LPs with a unique optimum is solvable in polynomial time.
Moreover, in practical applications one is usually only interested in finding
some optimum of an LP, in which case a simple perturbation of the objective
yields an instance with a unique optimum. It is thus natural to ask whether
deepest-descent steps are also easy to compute for LPs with unique optima, or
whether this problem is hard despite OCNP being easy.
</p>
<p>We show that deepest-descent steps can be efficiently approximated within a
factor of $n$, where $n$ is the dimension of the polyhedron at hand, but not
within a factor of $O(n^{1-\epsilon})$ for any $\epsilon &gt; 0$. While we prove
that OCNP can be solved efficiently for LPs with a unique optimum, our
different hardness approach allows us to show strong negative results:
computing deepest-descent steps is NP-hard and inapproximable even for 0/1
linear programs with a unique optimum which are defined by a totally unimodular
constraint matrix.
</p></div>
    </summary>
    <updated>2020-10-22T23:26:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.10726</id>
    <link href="http://arxiv.org/abs/2010.10726" rel="alternate" type="text/html"/>
    <title>MINVO Basis: Finding Simplexes with Minimum Volume Enclosing Polynomial Curves</title>
    <feedworld_mtime>1603324800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tordesillas:Jesus.html">Jesus Tordesillas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/How:Jonathan_P=.html">Jonathan P. How</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.10726">PDF</a><br/><b>Abstract: </b>Outer polyhedral representations of a given polynomial curve are extensively
exploited in computer graphics rendering, computer gaming, path planning for
robots, and finite element simulations. B\'ezier curves (which use the
Bernstein basis) or B-Splines are a very common choice for these polyhedral
representations because their non-negativity and partition-of-unity properties
guarantee that each interval of the curve is contained inside the convex hull
of its control points. However, the convex hull provided by these bases is not
the one with smallest volume, producing therefore undesirable levels of
conservatism in all of the applications mentioned above. This paper presents
the MINVO basis, a polynomial basis that generates the smallest $n$-simplex
that encloses any given $n^\text{th}$-order polynomial curve. The results
obtained for $n=3$ show that, for any given $3^{\text{rd}}$-order polynomial
curve, the MINVO basis is able to obtain an enclosing simplex whose volume is
$2.36$ and $254.9$ times smaller than the ones obtained by the Bernstein and
B-Spline bases, respectively. When $n=7$, these ratios increase to $902.7$ and
$2.997\cdot10^{21}$, respectively.
</p></div>
    </summary>
    <updated>2020-10-22T23:27:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-10-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2020/10/21/intrinsicLR/</id>
    <link href="http://offconvex.github.io/2020/10/21/intrinsicLR/" rel="alternate" type="text/html"/>
    <title>Mismatches between Traditional Optimization Analyses and Modern Deep Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>You may remember our <a href="http://www.offconvex.org/2020/04/24/ExpLR1/">previous blog post</a> showing that it is possible to do state-of-the-art deep learning with learning rate that increases exponentially during training.  It was meant to be a dramatic illustration that what we learned in optimization classes and books isn’t always a good fit for modern deep learning, specifically, <em>normalized nets</em>, which is our term for nets that use any one of popular normalization schemes,e.g. <a href="https://arxiv.org/abs/1502.03167">BatchNorm (BN)</a>, <a href="https://arxiv.org/abs/1803.08494">GroupNorm (GN)</a>, <a href="https://arxiv.org/abs/1602.07868">WeightNorm (WN)</a>. Today’s post (based upon <a href="https://arxiv.org/abs/2010.02916">our paper</a> with Kaifeng Lyu at NeurIPS20)  identifies other surprising incompatibilities between normalized nets and traditional analyses. We hope this will change the way you teach and think about deep learning!</p>

<p>Before diving into the results, we recall that normalized nets  are typically trained with weight decay (aka $\ell_2$ regularization). Thus the $t$th iteration of Stochastic Gradient Descent (SGD) is:</p>

\[w_{t+1} \gets (1-\eta_t\lambda)w_t - \eta_t \nabla \mathcal{L}(w_t; \mathcal{B}_t),\]

<p>where $\lambda$ is the weight decay (WD) factor (or $\ell_2$-regularization coefficient),  $\eta_t$ the learning rate, $\mathcal{B}_t$ the batch, and $\nabla \mathcal{L}(w_t,\mathcal{B}_t)$ the batch gradient.</p>

<p>As sketched in our previous blog post, under fairly mild assumptions (namely, fixing the top layer during random initialization —which empirically does not hurt final accuracy) the loss function for training such normalized nets is <em>scale invariant</em>, which means $\mathcal{L}(w _ t; \mathcal{B}_ t)=\mathcal{L}(cw _ t; \mathcal{B} _ t)$, $\forall c&gt;0$.</p>

<p>A consequence of scale invariance is that the $ \nabla _ w \mathcal{L} \vert _ {w = w _ 0} = c \nabla _ w \mathcal{L}\vert _  {w = cw _ 0}$ and $\nabla ^ 2 _ w \mathcal{L} \vert _ {w = w _ 0} = c ^ 2 \nabla ^ 2 _ w \mathcal{L} \vert _  {w = cw _ 0}$, for any $c&gt;0$.</p>

<h2 id="some-conventional-wisdoms-cws">Some Conventional Wisdoms (CWs)</h2>

<p>Now we briefly describe some conventional wisdoms. Needless to say, by the end of this post these will turn out to be very very suspect! Possibly they were OK in earlier days of deep learning, and with shallower nets.</p>

<blockquote>
  <p>CW 1) As we reduce LR to zero, optimization dynamic converges to a deterministic path (Gradient Flow) along which training loss strictly decreases.</p>
</blockquote>

<p>Recall that in traditional explanation of (deterministic) gradient descent, if LR is smaller than roughly the inverse of the smoothness of the loss function, then each step reduces the loss. SGD, being stochastic, has a distribution over possible paths. But very tiny LR can be thought of as full-batch Gradient Descent (GD), which in the limit of infinitesimal step size approaches Gradient Flow (GF).</p>

<p>The above reasoning shows very small LR is guaranteed to decrease the loss at least, as well as any higher LR, can. Of course, in deep learning, we care not only about optimization but also generalization. Here small LR is believed to hurt.</p>

<blockquote>
  <p>CW 2) To achieve the best generalization the LR must be large initially for quite a few epochs.</p>
</blockquote>

<p>This is primarily an empirical finding: using too-small learning rates or too-large batch sizes from the start (all other hyper-parameters being fixed) is known to lead to worse generalization (<a href="https://arxiv.org/pdf/1206.5533.pdf">Bengio, 2012</a>; <a href="https://arxiv.org/abs/1609.04836">Keskar et al., 2017</a>).</p>

<p>A popular explanation for this phenomenon is  that the noise in gradient estimation during SGD is beneficial for generalization. (As noted, this noise tends to average out when LR is very small.)  Many authors have suggested that the noise helps becauses it keeps the trajectory away from sharp minima which are believed to generalize worse, although there is some difference of opinion here (<a href="http://www.bioinf.jku.at/publications/older/3304.pdf">Hochreiter&amp;Schmidhuber, 1997</a>; <a href="https://arxiv.org/abs/1609.04836">Keskar et al., 2017</a>; <a href="https://arxiv.org/abs/1712.09913">Li et al., 2018</a>; <a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>; <a href="https://arxiv.org/pdf/1902.00744.pdf">He et al., 2019</a>). <a href="https://arxiv.org/abs/1907.04595">Li et al., 2019</a> also gave an example (a simple two-layer net) where this observation of worse generalization due to small LR is mathematically proved and also experimentally verified.</p>

<blockquote>
  <p>CW 3) Modeling SGD via a Stochastic Differential Equation (SDE) in the continuous-time limit with a fixed Gaussian noise. Namely, think of SGD as a diffusion process that <strong>mixes</strong>  to some Gibbs-like distribution on trained nets.</p>
</blockquote>

<p>This is the usual approach to  formal understanding of CW 2 (<a href="https://arxiv.org/abs/1710.06451">Smith&amp;Le, 2018</a>; <a href="https://arxiv.org/abs/1710.11029">Chaudhari&amp;Soatto, 2018</a>; <a href="https://arxiv.org/abs/2004.06977">Shi et al., 2020</a>). The idea is that SGD is gradient descent with a noise term, which has a continuous-time approximation as a diffusion process described as</p>

\[dW_t = - \eta_t \lambda W_t dt - \eta_t \nabla \mathcal{L}(W_t) dt + \eta_t \Sigma_{W_t}^{1/2} dB_t,\]

<p>where �$\sigma_{W_t}$ is the covariance of stochastic gradient $ \nabla \mathcal{L}(w_t; \mathcal{B}_t)$,  and $B_t$ denotes Brownian motion of the appropriate dimension. Several works have adopted this SDE view and given some rigorous analysis of the effect of noise.</p>

<p>In this story, SGD turns into a geometric random walk in the landscape, which can in principle explore the landscape more thoroughly, for instance by occasionally making loss-increasing steps. While an appealing view, rigorous analysis is difficult because we lack a mathematical description of the loss landscape.  Various papers assume the noise in SDE is isotropic Gaussian, and then derive an expression for the stationary distribution of the random walk in terms of the familiar Gibbs distribution. This view gives intuitively appealing explanation of some deep learning phenomena since the magnitude of noise (which is related to LR and batch size) controls the convergence speed and other properties. For instance it’s well-known that this SDE approximation implies the well-known <em>linear scaling rule</em> (Goyal et. al., 2017](https://arxiv.org/pdf/1706.02677.pdf)).</p>

<p>Which raises the question: <em>does SGD really behave like a diffusion process that mixes in the loss landscape?</em></p>

<!--[A few lines explaining for why noise term has this form? e.g., show one step discretization]!-->

<h2 id="conventional-wisdom-challenged">Conventional Wisdom challenged</h2>

<p>We now describe the actual discoveries for normalized nets, which show that the above CW’s are quite off.</p>

<blockquote>
  <p>(Against CW1): Full batch gradient descent $\neq$ gradient flow.</p>
</blockquote>

<p>It’s well known that if LR is smaller than the inverse of the smoothness, then the trajectory of gradient descent will be close to that of gradient flow. But for normalized networks, the loss function is scale-invariant and thus provably non-smooth (i.e., smoothness becomes unbounded)  around the origin (<a href="https://arxiv.org/abs/1910.07454">Li&amp;Arora, 2019</a>). We show that this non-smoothness issue is very real and makes training unstable and even chaotic for full batch SGD with any nonzero learning rate. This occurs both empirically and provably so with some toy losses.</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/gd_not_gf.png" style="width: 60%;"/>
</div>

<p><strong>Figure 1.</strong> WD makes GD on scale-invariant loss unstable and chaotic.
(a) Toy model with scale-invariant loss $L(x,y) = \frac{x^2}{x^2+y^2}$  (b)(c) Convergence never truly happens for  ResNet trained on sub-sampled
CIFAR10 containing 1000 images with full-batch GD (without momentum).  ResNet
can easily get to 100% training accuracy but then veers off.  When WD is turned off at epoch 30000 it converges.</p>

<p>Note that WD plays a crucial role in this effect since without WD the parameter norm increases monotonically 
 (<a href="https://arxiv.org/abs/1812.03981">Arora et al., 2018</a>) which implies SGD moves away from the origin at all times.</p>

<p>Savvy readers might wonder whether using a smaller LR could fix this issue. Unfortunately, getting close to the origin is unavoidable because once the gradient gets small,  WD will dominate the dynamics and decrease the norm at a geometric rate, causing the gradient to rise again due to the scale invariance! (This happens so long as the gradient gets arbitrarily small, but not actually zero, as is the case in practice.)</p>

<p>In fact, this is an excellent (and rare) place where early stopping is necessary even for correct optimization of the loss.</p>

<blockquote>
  <p>(Against CW 2) Small LR can generalize equally well as large LR.</p>
</blockquote>

<p>This actually was a prediction of the new theoretical analysis we came up with. We ran extensive experiments to test this prediction and found that initial large LR is <strong>not necessary</strong> to match the best performance, even when <em>all the other hyperparameters are fixed</em>. See Figure 2.</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/blog_sgd_8000_test_acc.png" style="width: 300px;"/>
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/blog_sgd_8000_train_acc.png" style="width: 300px;"/>
</div>

<p><strong>Figure 2</strong>. ResNet trained on CIFAR10 with SGD with normal LR schedule (baseline) as well as a schedule with 100 times smaller initial LR.  The latter matches performance of baseline after one more LR decay!  Note it needs  5000 epochs which is 10x higher! See our paper for details. (Batch size is 128, WD is 0.0005, and LR is divided by 10 for each decay.)</p>

<p>Note the  surprise here is that generalization was not hurt from drastically smaller LR even  <em>when no other hyperparameter changes</em>.  It is known empirically as well as rigorously (Lemma 2.4 in <a href="https://arxiv.org/abs/1910.07454">Li&amp;Arora, 2019</a>)  that it is possible to compensate for small LR by other hyperparameter changes.</p>

<blockquote>
  <p>(Against Wisdom 3) Random walk/SDE view of SGD is way off. There is no evidence of mixing as  traditionally understood, at least within normal training times.</p>
</blockquote>

<p>Actually the evidence against global mixing exists already via the phenomenon of Stochastic Weight Averaging (SWA) (<a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>). Along the trajectory of SGD, if  the network parameters from two different epochs are averaged, then the average has test loss lower than either.  Improvement via averaging continues to  work for run times 10X longer  than usual as shown in Figure 3. However, the accuracy improvement doesn’t happen for SWA between two solutions obtained from different initialization.  Thus checking whether SWA holds distinguishes between  pairs of solutions drawn from the same trajectory and pairs drawn from different trajectories, which  shows the diffusion process hasn’t mixed to stationary distribution within normal training times. (This is not surprising, since the theoretical analysis of mixing does not suggest it happens rapidly at all.)</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/swa_sgd_test_acc.png" style="width: 300px;"/>
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/swa_sgd_dist.png" style="width: 300px;"/>
</div>

<p><strong>Figure 3</strong>. Stochastic Weight Averaging improves the test accuracy of ResNet trained with
SGD on CIFAR10. <strong>Left:</strong> Test accuracy. <strong>Right:</strong> Pairwise distance between parameters from different epochs.</p>

<p>Actually <a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a> already noticed the implication that SWA rules out that SGD is a diffusion process which mixes to a unique global equilibrium. They suggested instead that perhaps the trajectory of SGD could be well-approximated by a multivariate Ornstein-Uhlenbeck (OU) process around the <em>local minimizer</em> $W^ * $, assuming the loss surface is locally strongly convex. As the corresponding stationary is multi-dimensional Gaussian, $N(W^ *, \Sigma)$, around the local minimizer, $W^ *$, this explains why SWA helps to reduce the training loss.</p>

<p>However, we note that (<a href="https://arxiv.org/abs/1803.05407">Izmailov et al., 2018</a>)’s suggestion is also refuted by the fact that we can show $\ell_2$ distance between weights from epochs $T$ and $T+\Delta$ monotonically increases with $\Delta$ for every $T$ (See Figure 3), while $ \mathbf{E} [ | W_ T-W_ {T+\Delta} |^2]$ should converge to the constant $2Tr[\Sigma]$ as $T, \Delta \to +\infty$ in the OU process. This suggests that all these weights are correlated, unlike the hypothesized OU process.</p>

<h2 id="so-whats-really-going-on">So what’s really going on?</h2>

<p>We develop a new theory (some parts rigorously proved and others supported by experiments) suggesting that <strong>LR doesn’t play the role assumed in most discussions.</strong></p>

<p>It’s widely believed that LR $\eta$ controls the convergence rate of SGD and affects the generalization via changing the magnitude of noise because LR $\eta$ adjusts the magnitude of gradient update per step. 
 <!--It's also worth noting that for vanilla SGD, changing LR is equivalent to rescaling the loss function. -->
 However, for normalized networks trained with SGD + WD, the effect of LR is more subtle as now it has two roles: (1). the multiplier before the gradient of the loss. (2). the multiplier before WD. Intuitively, one imagines the WD part is  useless since the loss function is scale-invariant, and thus the first role must be more important. But surprisingly, this intuition is completely wrong and it turns out that the second role is way more important than the first one. 
Further analysis shows that a better measure of speed of learning is   $\eta \lambda$, which we call the <em>intrinsic learning rate</em> or <em>intrinsic LR</em>, denoted $\lambda_e$.</p>

<p>While previous papers have noticed qualitatively that LR and WD have a close interaction, our ExpLR paper   <a href="https://arxiv.org/abs/1910.07454">Li&amp;Arora, 2019</a>)  gave mathematical proof that <em>if WD* LR, i.e., $\lambda\eta$ is fixed, then the effect of changing LR on the dynamics is equivalent to rescaling the initial parameters</em>.  As far as we can tell, performance of SGD on modern architectures is quite robust to (indeed usually independent of) scale of the initialization, so the effect of changing initial LR while keeping intrinsic LR fixed is also negligible.</p>

<p>Our paper gives insight into the role of intrinsic LR $\lambda_e$ by giving a new SDE-style analysis of SGD for normalized nets, leading to the following conclusion (which rests in part on experiments):</p>

<blockquote>
  <p>In normalized nets SGD does indeed lead to rapid mixing, but in <strong>function space</strong> (i.e., input-output behavior of the net). Mixing happens after $O(1/\lambda_e)$ iterations, in contrast to the exponentially slow mixing guaranteed in the parameter space by traditional analysis of diffusion walks.</p>
</blockquote>

<p>To explain the meaning of mixing in function space, let’s view SGD (carried out for a fixed number of steps) as a way to sample a trained net from a  distribution over trained nets. Thus the end result of SGD from a fixed initialization can be viewed as a probabilistic classifier whose output on any datapoint is the $K$-dimenstional vector whose $i$th coordinate is the probability of outputting label $i$. (Here $K$ is the total number of labels.) Now if two different initializations both cause SGD to produce classifiers with error $5$ percent on heldout datapoints, then  <em>a priori</em> one would imagine that  on a given held-out datapoint the classifier from the first distribution <strong>disagrees</strong>  with the classifier from the second distribution with roughly $2 * 5 =10$ percent probability. (More precisely, $2 * 5 * (1-0.05) = 9.5$ percent.) However, convergence to an equilibrium distribution in function space means that the probability of disagreement is almost $0$, i.e., the distribution is almost the same regardless of the initialization! This is indeed what we experimentally find, to our big surprise. Our theory is built around this new phenomenon.</p>

<div style="text-align: center;">
<img src="https://www.cs.princeton.edu/~zl4/small_lr_blog_images/additional_blog_image/conjecture.png" style="width: 500px;"/>
</div>
<p><strong>Figure 4</strong>: A simple 4-layer normalized CNN trained on MNIST with three schedules converge to the same equilibrium after intrinsic LRs become equal at epoch 81. We use Monte Carlo ($500$ trials) to estimate $\ell_1$ distances between distributions.</p>

<p>In the next post, we will explain our new theory and the partial new analysis of SDEs arising from SGD in normalized nets.</p></div>
    </summary>
    <updated>2020-10-21T22:00:00Z</updated>
    <published>2020-10-21T22:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2020-10-22T23:32:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/21/tenure-track-faculty-at-portland-state-university-apply-by-december-31-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/21/tenure-track-faculty-at-portland-state-university-apply-by-december-31-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track faculty at Portland State University (apply by December 31, 2020)</title>
    <summary>The Department of Computer Science at Portland State University invites applications for several Assistant Professor positions. Exceptional candidates will also be considered for appointment at the rank of Associate Professor. Candidates in all areas of Computer Science will be considered including theoretical computer science. Website: https://www.pdx.edu/computer-science/open-faculty-position Email: cssearch@pdx.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at Portland State University invites applications for several Assistant Professor positions. Exceptional candidates will also be considered for appointment at the rank of<br/>
Associate Professor. Candidates in all areas of Computer Science will be considered including theoretical computer science.</p>
<p>Website: <a href="https://www.pdx.edu/computer-science/open-faculty-position">https://www.pdx.edu/computer-science/open-faculty-position</a><br/>
Email: cssearch@pdx.edu</p></div>
    </content>
    <updated>2020-10-21T16:42:54Z</updated>
    <published>2020-10-21T16:42:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-23T04:20:41Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/reconstruction-theory/</id>
    <link href="https://differentialprivacy.org/reconstruction-theory/" rel="alternate" type="text/html"/>
    <title>The Theory of Reconstruction Attacks</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We often see people asking whether or not differential privacy might be overkill.  Why do we need strong privacy protections like differential privacy when we’re only releasing approximate, aggregate statistical information about a dataset?  Is it really possible to extract information about specific users from releasing these statistics?  The answer turns out to be a resounding yes!  The textbook by Dwork and Roth <a href="https://www.cis.upenn.edu/~aaroth/privacybook.html">[DR14]</a> calls this phenomenon the Fundamental Law of Information Recovery:</p>

<blockquote>
  <p>Giving overly accurate answers to too many questions will inevitably destroy privacy.</p>
</blockquote>

<p>So what exactly does this fundamental law mean precisely, and how can we prove it?  We can formalize and prove the law via <em>reconstruction attacks</em>, where an attacker can recover secret information from nearly every user in the dataset, simply by observing noisy answers to a modestly large number of (surprisingly simple) queries on the dataset. Reconstruction attacks were introduced in a seminal paper by Dinur and Nissim in 2003 <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>.  Although this paper predates differential privacy by a few years, the discovery of reconstruction attacks directly led to the definition of differential privacy, and shaped a lot of the early research on the topic. We now know that differentially private algorithms can, in some cases, match the limitations on accuracy implied by reconstruction attacks. When this is the case, we have a remarkably sharp transition from a blatant privacy violation when the accuracy is high enough to enable a reconstruction attack, to the strong protection given by differential privacy at the cost of only slightly lower accuracy.</p>

<p>Aside from the theoretical importance of reconstruction attacks, one may wonder if they can be carried out in practice, or if the attack model is unrealistic and can be avoided with some simple workarounds?  In this series of posts, we argue that reconstruction attacks can be quite practical.  In particular, we describe successful attacks by some of this post’s authors on a family of systems called <em>Diffix</em>, that attempt to prevent reconstruction without introducing as much noise as the reconstruction attacks suggest is necessary. To the best of our knowledge, these attacks represent the first successful attempt to reconstruct data from a commercial statistical-database system that is specifically designed to protect the privacy of the underlying data.  A larger and much more significant demonstration of the practical power of reconstruction attacks was carried out by the US Census Bureau in 2018, motivating the Bureau’s adoption of differential privacy for data products derived from the 2020 decennial census <a href="https://queue.acm.org/detail.cfm?ref=rss&amp;id=3295691">[GAM18]</a>.</p>

<p>This series will come in two parts: In this post, we will review the theory of reconstruction attacks, and present a model for reconstruction attacks that corresponds more directly to real attacks than the one that is typically presented.  In the second post, we will describe attacks that were launched against various iterations of the <em>Diffix</em> system. \(
\newcommand{\uni}{\mathcal{X}} % The universe
\newcommand{\usize}{T} % Universe size
\newcommand{\elem}{x} % Generic universe element. 
\newcommand{\pbs}{z} %Non-secret bits
\newcommand{\pbsuni}{\mathcal{Z}}
\renewcommand{\sb}{b} % Secret bit
\newcommand{\pds}{Z} %non-secret part of the data set
\newcommand{\ddim}{d} % Data dimension
\newcommand{\queries}{Q} % A set/workload of queries
\newcommand{\qmat}{\mat{Q}} % Query matrix
\newcommand{\qent}{w} % Entry of the query matrix
\newcommand{\hist}{h} % Histogram vector
\newcommand{\mech}{\mathcal{M}} % Generic Mechanism
\newcommand{\query}{q}
\newcommand{\queryfunc}{\varphi}
\newcommand{\ans}{a} % query answer
\newcommand{\qsize}{k}
\newcommand{\ds}{X}
\newcommand{\dsrow}{\elem} % same as elem above
\newcommand{\dsize}{n}
\newcommand{\priv}{\eps}
\newcommand{\privd}{\delta}
\newcommand{\acc}{\alpha}
\newcommand{\from}{:}
\newcommand{\set}[1]{\left{#1\right}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\eps}{\varepsilon}
\newcommand{\pmass}{\mathbbm{1}}
\newcommand{\zo}{\{0,1\}}
\newcommand{\mat}[1]{#1} % matrix notation: for now nothing
\)</p>
<h3 id="a-model-of-reconstruction-attacks">A Model of Reconstruction Attacks</h3>

<p>This part presents the basic theory of reconstruction attacks.  We’ll introduce a model of reconstruction attacks that is a little different from what you would see if you read the papers, and then describe the main results of Dinur and Nissim.  At the end we will briefly mention some variations that have been considered in the nearly two decades since.</p>

<p>Let us fix a dataset model, so that we can describe the attack precisely. (These attacks are very flexible and the ideas can usually be adapted to new models, as we’ll see at the end of this part.) We take the dataset to be a collection of \(\dsize\) records \(\ds = \{\elem_1,\dots,\elem_n\}\), each corresponding to the data of a single person.  The attacker’s goal is to learn some piece of secret information about as many individuals as possible, so we think of each record as having the form \(\elem_i = (\pbs_i,\sb_i)\) where \(\pbs_i\) is some identifying information, and \(\sb_i \in \zo\) is some secret. We assume that the secret is binary, although this aspect of the model can be generalized. We can visualize such a dataset as a matrix \([\pds \mid \sb]\) with two blocks as follows:
\[ \left[ \begin{array}{c|c} \pbs_1 &amp; \sb_1 \\ \vdots &amp; \vdots \\ \pbs_n &amp; \sb_n \end{array} \right] \]
For a concrete example, suppose each element in the dataset contains \(d\) binary attributes, and the attacker’s goal is to learn the last attribute of each user.  In this case we would write each element as a pair \((\pbs_i, \sb_i)\) where \(\pbs_i \in \zo^{d-1}\) and \(\sb_i \in \zo\).</p>

<p>Note that this distinction between \(\pbs_i\) and \(\sb_i\) is only in the mind of the attacker, who has some prior information about the users, but is trying to learn some specific secret information.  In order to make the attack simpler to describe, we will also assume that the attacker knows \(\pbs_1,\dots,\pbs_\dsize\), which is everything about the dataset except the secret bits, although this assumption can also be relaxed to a large extent. As a shorthand, we will refer to \(\pbs_1, \ldots, \pbs_\dsize\) as the prior information, and to \(\sb_1, \ldots,\sb_\dsize\) as the secret bits.</p>

<p>Our goal is to understand whether asking aggregate queries defined by the prior information can allow an attacker to learn non-trivial information about the secret bits.  Perhaps the most basic type of aggregate query we can ask is a <em>counting query</em>, which is a query that asks what number of the data points satisfy a given property. The Dinur-Nissim attacks assume that the attacker can get approximate answers to a type of counting queries that ask how many data points satisfy some property defined in terms of the prior information, and also have the sensitive bit set to \(1\).  Let us use the notation \(\pbsuni\) for the set of all possible values that the prior information can take. For the purposes of the attack, each query \(\query\) will be specified by a function \(\queryfunc \from \pbsuni \to \zo\) and have the specific form
\[
\query(\ds) = \sum_{j=1}^{\dsize} \queryfunc(\pbs_j) \cdot \sb_j.
\]
This is a good time to make one absolutely crucial point about this model, which is that</p>
<blockquote>
  <p>all the users are treated completely symmetrically by the queries, and the attacker cannot issue a query that targets a specific user \(x_i\) by name or a specific subset of users.  The different users are distinguished only by their data.  Nonetheless, we will see how to learn information about specific users from the answers to these queries.</p>
</blockquote>

<p>Returning to our example with binary attributes, consider the very natural set of queries that asks for the inner product of the secret bits with each attribute in the prior information, which is a measure of the correlation between these two attributes.  Then each query takes the form \(\query_i(\ds) = \sum_{j=1}^{n} \pbs_{j,i} \cdot \sb_{j}\).</p>

<p>The nice thing about this type of query is that we can express the answers to a set of queries \({\query_1,\dots,\query_\qsize}\) defined by \(\queryfunc_1, \ldots, \queryfunc_\qsize\) as the following matrix-vector product \(\qmat_{\pds}\cdot \mat{b}\):
\[ \left[ \begin{array}{c}\query_1(\ds) \\ \vdots  \\ \query_\qsize(\ds) \end{array} \right] = \left[ \begin{array}{ccc} \queryfunc_1(\pbs_1) &amp; \dots &amp; \queryfunc_1(\pbs_\dsize) \\ \vdots &amp;  \ddots  &amp; \vdots \\ \queryfunc_\qsize(\pbs_1) &amp;   \dots &amp; \queryfunc_k(\pbs_\dsize)  \end{array} \right] \left[ \begin{array}{c} \sb_1 \\ \vdots  \\ \sb_n  \end{array} \right]
\]
so we can study this model using tools from linear algebra.</p>

<h3 id="an-inefficient-attack">An Inefficient Attack</h3>

<p>Exact answers to such queries are clearly revealing, because, the attacker can use the predicates \[ \queryfunc_i(z) = \begin{cases} 1 &amp; \textrm{if } \pbs = \pbs_i  \\ 0 &amp;  \textrm{otherwise} \end{cases} \] to single out a specific user and receive their bit \(\sb_i\).  It is less obvious, however, that an attacker can learn a lot about the private bits even given noisy answers to the queries.</p>

<p>The first Dinur-Nissim attack shows that this is indeed possible—if the attacker can ask an unbounded number of counting queries, and each query is answered with, for example, 5% error, then the attacker can reconstruct 80% of the secret bits.  This attack requires exponentially many queries to run, making it somewhat impractical, but it is a proof of concept that an attack can reconstruct a large amount of private information even from very noisy statistics. Later we will see how to scale down the attack to use fewer queries at the cost of requiring more accurate answers.</p>

<p>The attack itself is quite simple:</p>

<ul>
  <li>
    <p>For simplicity, assume all the \(\pbs_1, \ldots, \pbs_\dsize\) are distinct so that each user is uniquely identified by the prior information.</p>
  </li>
  <li>
    <p>The attacker chooses the queries \(\query_1, \ldots, \query_\qsize\) so that the matrix \(\qmat_\pds\) has as its rows all of \(\zo^\dsize\). Namely, \(\qsize=2^\dsize\) and the functions \(\queryfunc_1, \ldots, \queryfunc_\qsize\) defining the queries take all possible values on \(\pbs_1, \ldots, \pbs_\dsize\).</p>
  </li>
  <li>
    <p>The attacker receives a vector \(\ans\) of noisy answers to the queries, where \( |\query_{i}(\ds) - \ans_{i}| &lt; \acc \dsize \) for each query \( \query_i \).  In matrix notation, this means \[ \max_{i = 1}^\qsize  |(\qmat_\pds\cdot {\sb})_i -\ans_i|= \| \qmat_\pds \cdot \sb -\ans\|_\infty  \leq \alpha \dsize. \]
  Note that, for \(\{0,1\}\)-valued queries, the answers range from \(0\) to \(\dsize\), so answers with additive error \(\pm 5\%\) corresponds to \(\acc = 0.05\).</p>
  </li>
  <li>
    <p>Finally, the attacker outputs any guess \(\hat{\sb} = (\hat{\sb}_{1}, \ldots, \hat{\sb}_{n})\) of the private bits vector that is consistent with the answers and the additive error bound \(\acc\). In other words, \(\hat{\sb}\) just needs to satisfy \[\max_{i = 1}^\qsize |\ans_i - (\qmat_\pds\cdot \hat{\sb})_i|= \| \qmat_\pds \cdot \hat\sb - a \|_{\infty} \leq \alpha \dsize \]
  Note that a solution always exists, since the true private bits \(\sb\) will do.</p>
  </li>
</ul>

<p>Our claim is that any such guess \(\hat{b}\) in fact agrees with the true private bits \(b\) for all but \(4\acc \dsize\) of the users. The reason is that if \(\hat{\sb}\) disagreed with more than \(4\acc \dsize\) of the secret bits, then the answer to some query would have eliminated \(\hat{\sb}\) from contention.  To see this, fix some \(\hat{\sb}\in \zo^\dsize\), and let \[ S_{01} = \{j: \hat{\sb}_j = 0,  \sb_j = 1\} \textrm{ and } S_{10} = \{j: \hat{\sb}_j = 1,  \sb_j = 0\}\] 
If \(\hat{\sb}\) and \(\sb\) disagree on more than \(4\acc \dsize\) bits, then at least one of these two sets has size larger than \(2\acc \dsize\). Let us assume that this set is \(S_{01}\), and we’ll deal with the other case by symmetry.  Suppose that the \(i\)-th row of \(\qmat_\pds\) is the indicator vector of \(S_{01}\), i.e., \[(\qmat_\pds)_{i,j} = 1 \iff j \in S_{01}.\] We then have
\[
|(\qmat_{\pds}\cdot {\sb})_i - (\qmat_{\pds}\cdot \hat{\sb})_i|= |S_{01}| &gt; 2 \acc \dsize,
\]
but, at the same time, if \(\hat{\sb}\) were output by the attacker, we would have
\[
|(\qmat_{\pds}\cdot {\sb})_i - (\qmat_{\pds}\cdot \hat{\sb})_i| \le |\ans_i - (\qmat_\pds\cdot \hat{\sb})_i| + |(\qmat_\pds \cdot \sb)_i - \ans_{i}| \le 2\acc \dsize, \]
which is a contradiction. An important point to note is that the attacker does not need to know the set \(S_{10}\), or the corresponding \(i\)-th row of \(\qmat_\pds\) and query \(\query_i\). Since the attacker asks all possible queries determined by the prior information, we can be sure \(\query_i\) is one of these queries, and an accurate answer to it rules out this particular bad choice of \(\hat{\sb}\).  To give you something concrete to cherish, we can summarize this discussion in the following theorem.</p>

<blockquote>
  <p><strong>Theorem <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>:</strong> There is a reconstruction attack that issues \(2^n\) queries to a dataset of \(n\) users, obtains answers with error \(\alpha n\), and reconstructs the secret bits of all but \(4 \alpha n\) users.</p>
</blockquote>

<h3 id="an-efficient-attack">An Efficient Attack</h3>

<p>The exponential Dinur-Nissim attack is quite powerful, as it recovers 80% of the secret bits even from answers with 5% error, but it has the drawback that it requires asking \(2^\dsize\) queries to a dataset with \(\dsize\) users.  Note that this is inherent to some extent.  Suppose we randomly subsample 50% of the dataset and answer the queries using only this subset by rescaling appropriately.  Although this random subsampling does not guarantee any meaningful privacy, clearly no attacker can reconstruct 75% of the secret bits, since some of them are effectively deleted.  However, the guarantees of random sampling tell us that any set of \(\qsize\) queries will be answered with maximum error \( \acc n =  O(\sqrt{n \log \qsize})\), so we can answer \( 2^{\Omega(n)} \) queries with \(5\%\) error while provably preventing this sort of reconstruction.</p>

<p>However, Dinur and Nissim showed that if we obtain <em>highly accurate</em> answers—still noisy, but with error smaller than the sampling error—then we can reconstruct the dataset to high accuracy.  We can also make the reconstruction process computationally efficient by using linear programming to replace the exhaustive search over all \(2^\dsize\) possible vectors of secrets.  Specifically, we change the attack as follows:</p>

<ul>
  <li>
    <p>The attacker now chooses \(\qsize\) <em>randomly chosen</em> functions \( \varphi_i \from \pbsuni \to \{0,1\} \) for a much smaller \(\qsize = O(\dsize) \).</p>
  </li>
  <li>
    <p>Upon receiving an answer vector \(\ans\), the attacker now searches for a <em>real-valued</em> \( \tilde{b} \in [0,1]^{\dsize} \) such that \( \| \ans - \qmat_\pds \cdot \tilde{b} \|_{\infty} \leq \acc n \).  Note that this vector can be found efficiently via linear programming.  The attacker then rounds each \( \tilde{b}_{i} \) to the nearest \( \hat{b}_{i} \in \{0,1\}\).</p>
  </li>
</ul>

<p>It’s now much trickier to analyze this attack and show that it achieves low reconstruction error, and we won’t go into details in this post.  However, the key idea is that, because the queries are chosen randomly, \( \qmat_\pds \) is a random matrix with entries in \( \{0,1\} \), and we can use the statistical properties of this random matrix to argue that, with high probability,
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty^2 \gtrsim |{i: \sb_i \neq \hat{\sb}_i}|.
\]
By the way we chose \(\tilde{\sb}\), we have 
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty \le \|\qmat_\pds \cdot \sb - \ans\|_\infty + \| \ans - \qmat_\pds \cdot \tilde{b} \|_{\infty} \leq 2\acc n,
\]
so, by combining the inequalities we get that the reconstruction error is about \( O(\alpha^2 n^2) \). Note that, in order to reconstruct 80% of the secret bits using this attack, we now need the error to be \( \alpha n  \ll \sqrt{n} \), but as long as this condition on the error is satisfied, we will have a highly accurate reconstruction.  Let’s add this theorem to your goodie bags:</p>

<blockquote>
  <p><strong>Theorem <a href="https://dl.acm.org/doi/10.1145/773153.773173">[DN03]</a>:</strong> There is an efficient reconstruction attack that issues \(O(n)\) random queries to a dataset of \(n\) users, obtains answer with error \(\alpha n\), and, with high probability, reconstructs the secret bits of all but \( O(\alpha^2 n^2)\) users.</p>
</blockquote>

<p>Although we modeled the queries, and thus the matrix \(\qmat_\pds\) as uniformly random, it’s important to note that we really only relied on the fact that 
\[
\|\qmat_\pds \cdot \sb - \qmat_\pds \cdot \tilde{\sb}\|_\infty^2 \gtrsim
|\{i: \sb_i \neq \hat{\sb}_i\}|,
\]
and we can reconstruct while tolerating the same \(\Omega(\sqrt{n})\) error for any family of queries that gives rise to a matrix with this property.  Intuitively, any <em>random-enough</em> family of queries will have this property.  More specifically, the property is satisfied by any matrix with no small singular values <a href="https://dl.acm.org/doi/10.1007/978-3-540-85174-5_26">[DY08]</a> or with large discrepancy <a href="https://arxiv.org/abs/1203.5453">[MN12]</a>.  There is a large body of work showing that many specific families of queries lead to reconstruction. For example, we can perform reconstruction using <em>conjunction queries</em> that ask for the marginal distribution of small subsets of the attributes <a href="https://dl.acm.org/doi/abs/10.1145/1806689.1806795">[KLSU10]</a>.  That is, queries of the form “count the number of people with blue eyes and brown hair and a birthday in August.”  In fairness, there are also families of queries that do not satisfy the property, or only satisfy quantitatively weaker versions of it, such as histograms and threshold queries, and for these queries it is indeed possible to achieve differential privacy with \( \ll \sqrt{n} \) error.</p>

<h3 id="conclusion">Conclusion</h3>

<p>This is going to be the end of our technical discussion, but before signing off, let’s mention some of the important extensions of this theorem that have been developed over the years:</p>

<ul>
  <li>
    <p>We can allow the secret information \(\sb\) to be integers or real numbers, rather than bits. The queries still return \(\qmat_\pds\cdot \sb\). The exponential attack then guarantees that, given answers with error \(\acc n\), the reconstruction \(\hat{\sb}\) satisfies \(\|\hat{\sb}-\sb\|_1 \le 4\acc n\). This means, for example, that the reconstructed secrets of all but \(4\alpha n\) users are within \(\pm 1\) of the true secrets. The efficient attack guarantees that \(\|\hat{\sb}-\sb\|_2^2 \le O(\acc^2 n^2)\), which means that the reconstructed secrets are within \(\pm 1\) for all but \(O(\acc^2 n^2)\) users.</p>
  </li>
  <li>
    <p>It’s not crucial that <em>every</em> query be answered with error \( \ll \sqrt{n} \).  If we are willing to settle 
  for an inefficient attack, then we can reconstruct even if only 51% of the queries have small error.  If at least 75% have small error, then we can reconstruct efficiently <a href="https://dl.acm.org/doi/10.1145/1250790.1250804">[DMT07]</a>.</p>
  </li>
  <li>
    <p>The reconstruction attacks still apply to the seemingly more general data model in which the private 
  dataset \(\ds\) is a subset of some arbitrary (but public) data universe \(\uni\).  To see this, note that we can take \(\uni = \{\pbs_1, \ldots, \pbs_\dsize\}\), and we can interpret the secret bits \(\sb_i\) to indicate whether \(\pbs_i\) is an element of \(\ds\). Then the reconstruction attacks allow us to determine, up to some error, which elements of \(\uni\) are contained in \(\ds\). In the setting, the attack is sometimes called <em>membership inference</em>.</p>
  </li>
  <li>
    <p>The fact that the efficient Dinur-Nissim reconstruction attack fails when the error is \( \gg \sqrt{n} \) 
  does not mean it’s easy to achieve privacy with error of that magnitude.  As we mentioned earlier, we can achieve non-trivial error guarantees for a large number of queries simply by using a random subsample of half of the dataset, which is not a private algorithm in any reasonable sense of the word, as it can reveal everything about the chosen subset.  As this example shows,</p>

    <blockquote>
      <p>preventing reconstruction attacks does not mean preserving privacy.</p>
    </blockquote>

    <p>In particular, there are membership-inference attacks that succeed in violating privacy even when the queries are answered with \( \gg \sqrt{n}\) error. We refer the reader to the survey <a href="https://privacytools.seas.harvard.edu/publications/exposed-survey-attacks-private-data">[DSSU17]</a> for a somewhat more in-depth survey of reconstruction and membership-inference attacks.</p>
  </li>
</ul>

<p>Many types of queries give rise to the conditions under which reconstruction is possible.  Stay tuned for our next post, where we show how to generate those types of queries in practice against a family of systems known as <em>Diffix</em> that are specifically designed to thwart reconstruction.</p></div>
    </summary>
    <updated>2020-10-21T16:30:00Z</updated>
    <published>2020-10-21T16:30:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-10-22T23:32:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/21/assistant-and-associate-professors-to-contribute-to-the-future-of-the-department-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-january-11-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/21/assistant-and-associate-professors-to-contribute-to-the-future-of-the-department-of-computer-science-at-department-of-computer-science-aarhus-university-apply-by-january-11-2020/" rel="alternate" type="text/html"/>
    <title>Assistant and Associate Professors to contribute to the future of the Department of Computer Science at Department of Computer Science, Aarhus University (apply by January 11, 2020)</title>
    <summary>Aarhus University – an international top-100 University – has made an ambitious strategic investment in a 5-year recruitment plan to radically expand the Department of Computer Science. We expect to hire four candidates this year. Therefore, we invite applications from candidates that are driven by excellence in research and teaching as well as external collaboration […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Aarhus University – an international top-100 University – has made an ambitious strategic investment in a 5-year recruitment plan to radically expand the Department of Computer Science. We expect to hire four candidates this year. Therefore, we invite applications from candidates that are driven by excellence in research and teaching as well as external collaboration on societal challenges.</p>
<p>Website: <a href="https://au.career.emply.com/ad/aarhus-university-is-hiring-assistant-and-associate-professors-to-contribute-to-t/lqwlj1/en">https://au.career.emply.com/ad/aarhus-university-is-hiring-assistant-and-associate-professors-to-contribute-to-t/lqwlj1/en</a><br/>
Email: kgronbak@cs.au.dk</p></div>
    </content>
    <updated>2020-10-21T07:02:53Z</updated>
    <published>2020-10-21T07:02:53Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-23T04:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/20/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-17-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/20/tenure-track-faculty-positions-at-simon-fraser-university-apply-by-december-17-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track Faculty Positions at Simon Fraser University (apply by December 17, 2020)</title>
    <summary>The School of Computing Science at Simon Fraser University (SFU) invites applications for tenure-track faculty positions. The School has multiple openings and will consider applications at all ranks, including assistant, associate and full professor. Excellent applicants in all areas of computer science will be considered. Website: https://www.sfu.ca/computing/job-opportunities.html#tenure Email: cs_faculty_affairs@sfu.ca</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The School of Computing Science at Simon Fraser University (SFU) invites applications for tenure-track faculty positions. The School has multiple openings and will consider applications at all ranks, including assistant, associate and full professor. Excellent applicants in all areas of computer science will be considered.</p>
<p>Website: <a href="https://www.sfu.ca/computing/job-opportunities.html#tenure">https://www.sfu.ca/computing/job-opportunities.html#tenure</a><br/>
Email: cs_faculty_affairs@sfu.ca</p></div>
    </content>
    <updated>2020-10-20T20:36:40Z</updated>
    <published>2020-10-20T20:36:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-23T04:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/10/20/three-year-and-tenure-track-positions-at-toyota-technological-institute-at-chicago-apply-by-december-1-2020/</id>
    <link href="https://cstheory-jobs.org/2020/10/20/three-year-and-tenure-track-positions-at-toyota-technological-institute-at-chicago-apply-by-december-1-2020/" rel="alternate" type="text/html"/>
    <title>Three-year and tenure-track positions at Toyota Technological Institute at Chicago (apply by December 1, 2020)</title>
    <summary>TTIC invites applications for the following faculty positions: research assistant professor (3-year term), tenure-track assistant professor, full or associate professor, and visiting professor. Applicants for research assistant professor positions (RAPs) are encouraged to simultaneously apply for the TTIC RAP program and the Simons-Berkeley Research Fellowship. Website: https://www.ttic.edu/faculty-hiring/ Email: recruiting@ttic.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>TTIC invites applications for the following faculty positions: research assistant professor (3-year term), tenure-track assistant professor, full or associate professor, and visiting professor. Applicants for research assistant professor positions (RAPs) are encouraged to simultaneously apply for the TTIC RAP program and the Simons-Berkeley Research Fellowship.</p>
<p>Website: <a href="https://www.ttic.edu/faculty-hiring/">https://www.ttic.edu/faculty-hiring/</a><br/>
Email: recruiting@ttic.edu</p></div>
    </content>
    <updated>2020-10-20T17:06:50Z</updated>
    <published>2020-10-20T17:06:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-10-23T04:20:41Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable</id>
    <link href="https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable.html" rel="alternate" type="text/html"/>
    <title>The graphs of stably matchable pairs</title>
    <summary>The stable matching problem takes as input the preferences from two groups of agents (most famously medical students and supervisors of internships), and pairs up agents from each group in a way that encourages everyone to play along: no pair of agents would rather go their own way together than take the pairings they were both given. A solution can always be found by the Gale–Shapley algorithm, but there are generally many solutions, described by the lattice of stable matchings. Some pairs of agents are included in at least one stable matching, while some other pairs are never matched. In this way, each instance of stable matchings gives rise to a graph, the graph of stably matchable pairs. This graph is the subject and title of my latest preprint, arXiv:2010.09230, which asks: Which graphs can arise this way? How hard is it to recognize these graphs, and infer a stable matching instance that might have generated them? How does the graph structure relate to the lattice structure?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://en.wikipedia.org/wiki/Stable_marriage_problem">stable matching problem</a> takes as input the preferences from two groups of agents (most famously medical students and supervisors of internships), and pairs up agents from each group in a way that encourages everyone to play along: no pair of agents would rather go their own way together than take the pairings they were both given. A solution can always be found by the <a href="https://en.wikipedia.org/wiki/Gale%E2%80%93Shapley_algorithm">Gale–Shapley algorithm</a>, but there are generally many solutions, described by the <a href="https://en.wikipedia.org/wiki/Lattice_of_stable_matchings">lattice of stable matchings</a>. Some pairs of agents are included in at least one stable matching, while some other pairs are never matched. In this way, each instance of stable matchings gives rise to a graph, the <em>graph of stably matchable pairs</em>. This graph is the subject and title of my latest preprint, <a href="https://arxiv.org/abs/2010.09230">arXiv:2010.09230</a>, which asks: Which graphs can arise this way? How hard is it to recognize these graphs, and infer a stable matching instance that might have generated them? How does the graph structure relate to the lattice structure?</p>

<p>For some answers, see the preprint. One detail is connected to <a href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html">my previous post, on polyhedra with no two disjoint faces</a> (even though there are no polyhedra in the new preprint): the (prism,\(K_{3,3}\))-minor-free graphs discussed there come up in proving an equivalence between outerplanar graphs of stably matchable pairs and lattices of <a href="https://en.wikipedia.org/wiki/Closure_problem">closures</a> of <a href="https://en.wikipedia.org/wiki/Polytree">oriented trees</a>. Instead of providing any technical details of any the other results in the paper, though, I thought it would be more fun to show a few visual highlights.</p>

<p>The following figure shows a cute mirror-inversion trick (probably already known, although I don’t know where or by whom) for embedding an arbitrary bipartite graph as an induced subgraph of a regular bipartite graph. I use it to show that graphs of stably matchable pairs have no forbidden induced subgraphs:</p>

<p style="text-align: center;"><img alt="Embedding a bipartite graph as an induced subgraph of a regular bipartite graph" src="https://11011110.github.io/blog/assets/2020/regularize.svg" width="60%"/></p>

<p>This next one depicts a combinatorial description of a stable matching instance having a \(6\times 5\) grid as its graph, in terms of the top and bottom matchings in the lattice of matchings, the “rotations” that can be used to move between matchings in this lattice, and a partial order on the rotations. For what I was doing in this paper, these rotation systems were much more convenient to work with than preferences.</p>

<p style="text-align: center;"><img alt="Rotation system describing a system of stable matchings having a 6x5 grid as its graph" src="https://11011110.github.io/blog/assets/2020/5x6.svg" width="80%"/></p>

<p>All the main ideas for a proof of NP-completeness of recognizing these graphs, by reduction from <a href="https://en.wikipedia.org/wiki/Not-all-equal_3-satisfiability">not-all-equal 3-satisfiability</a>, are visible in the next picture. The proof now in the paper is significantly more complicated, though, because the construction in this image produces nonplanar graphs but I wanted a proof that would also apply in the planar case.</p>

<p style="text-align: center;"><img alt="NP-completeness reduction from NAE3SAT to recognizing graphs of stably matchable pairs" src="https://11011110.github.io/blog/assets/2020/nae3sat-to-matching.svg"/></p>

<p>The last one shows a sparse graph that can be represented as a graph of stably-matching pairs (because it’s outerplanar, bipartite, and biconnected) but has a high-degree vertex. If we tried to test whether it could be realized by doing a brute-force search over preference systems, the time would be factorial in the degree, but my preprint provides faster algorithms that are only singly exponential in the number of edges.</p>

<p style="text-align: center;"><img alt="Outerplanar graph of stably matchable pairs with a factorial number of potential preference systems" src="https://11011110.github.io/blog/assets/2020/factorial.svg"/></p>

<p>(<a href="https://mathstodon.xyz/@11011110/105065476283424319">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-10-19T20:29:00Z</updated>
    <published>2020-10-19T20:29:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-10-20T05:33:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6623846086903606036</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6623846086903606036/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6623846086903606036" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6623846086903606036" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html" rel="alternate" type="text/html"/>
    <title>Nature vs Nurture close to my birthday</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Since I was born on Oct 1, 1960 (that's not true---if I posted  my real birthday I might get my  identity stolen), I will do a nature vs nurture post based on my life, which seems less likely to offend then doing it on someone else's life. I'll just rattle off some random points on Nature vs Nurture.</p><p>1) Is it plausible that I was born with some math talent? Is plausible that I was born with some talent to understand the polynomial van der Warden theorem? What is the granularity of nature-given or nurture-given abilities?</p><p>2) My dad was a HS English teacher and later Vice-Principal. My mom taught English at a Community college. Readers of the blog might think, given my spelling and grammar, that I was switched at birth. My mom says (jokingly?) that I was switched at birth since she thinks I am good at math.</p><p>a) I am not THAT good at math. Also see next point.</p><p>b) While there are some math families, there are not many. See my post <a href="https://blog.computationalcomplexity.org/2009/02/baseball-families-and-math-families.html">here</a>.</p><p>c) I think being raised in an intellectual atmosphere by two EDUCATORS who had the money to send me to college and allowed me the freedom to study what I wanted to  is far more important than the rather incidental matter of what field I studied.</p><p>d) Since my parents never went into math or the sciences it is very hard to tell  if they were `good at math' or even what that means.</p><p>3) There were early signs I was INTERESTED in math, though not that I was good at it.</p><p>a) In fourth grade I wanted to know how many SECONDS were in a century so I spend some time figuring it out on paper. Did I get the right answer?  I forgot about leap years.</p><p>b) I was either a beneficiary of, or a victim of, THE NEW MATH. So I learned about comm. and assoc. operations in 8th grade. We were asked to come up with our own operations. I wanted to come up with an operation that was comm. but not assoc. I did! Today I would write it as f(x,y) = |x-y|. This is the earliest I can think of where I made up a nice math problem. Might have been the last time I made up a nice math problem AND solved it without help. </p><p>c) In 10th grade I took some Martin Gardner books out of the library. The first theorem I learned not-in-school was that a graph is Eulerian iff every vertex has even degree. I read the chapter on Soma cubes and bought a set. (Soma cubes are explained <a href="https://en.wikipedia.org/wiki/Soma_cube">here</a>.) </p><p>d) I had a talent (nature?) at Soma Cubes.  I did every puzzle in the book in a week, diagrammed them, and even understood (on some level) the proofs that some could not be done. Oddly I am NOT good at 3-dim geom. Or even 2-dim geom.  For 1-dim I hold my own!</p><p>e) Throughout my childhood I noticed odd logic and odd math things that were said: </p><p>``Here at WCOZ (a radio station) we have an AXIOM, that's like a saying man, that weekends should be SEVEN DAYS LONG'' (Unless that axiom resolves CH, I don't think it should be assumed.) </p><p>``To PROVE we have the lowest prices in town we will give you a free camera!'' (how does that prove anything?) </p><p>``This margarine tastes JUST LIKE BUTTER'' (Okay-- so why not just buy butter?)</p><p>e) In 9th grade when I learned the quadratic formula I re-derived it once-a-month since I though it was important that one can prove such things.  I heard (not sure from where) that there was no 5th degree equation. At that very moment I told my parents:</p><p><i>I am going to major in math so I can find out why there is no 5th degree equation.</i></p><p>There are worse things for parents to hear from their children. See <a href="https://blog.computationalcomplexity.org/2019/06/a-proof-that-227-pi-0-and-more.html">here</a> for dad's reaction. </p><p>f) When I learned that the earth's orbit around the sun is an ellipse and that the earth was one of the foci, I wondered where the other foci is and if its important. I still wonder about this one. Google has not helped me here, though perhaps I have not phrased the question properly. If you know the answer please leave a comment. </p><p>g) I also thought about The Ketchup problem and other problems, that I won't go into since I already blogged about them  <a href="https://blog.computationalcomplexity.org/2012/06/ketchup-problem.html">here</a></p><p>4) I was on the math team in high school, but wasn't very good at it. I WAS good at making up math team problems. I am now on the committee that makes up the Univ of MD HS math competition. I am still not good at solving the problems but good at making them up. </p><p>5) From 9th grade on before I would study for an exam by making up what I thought would be a good exam and doing that. Often my exam was a better test of knowledge than the exam given. In college I helped people in Math and Physics by making up exams for them to work on as practice. </p><p>6) I was good at reading, understanding, and explaining papers. </p><p>7) I was never shy about asking for help. My curiosity exeeded by ego... by a lot!</p><p>8) Note that items 5,6, and 7 above do not mention SOLVING problems. The papers I have written are of three (overlapping) types:</p><p>a) I come up with the problem, make some inroads on it based on knowledge, and then have people cleverer (this is often) or with more knowledge (this is rarer) help me solve the problems.</p><p>b) I come up with the problem, and combine two things I know from other papers to solve it. </p><p>c) Someone else asks for my help on something and I have the knowledge needed. I can only recall one time where this lead to a paper. </p><p>NOTE- I do not think I have ever had a clever or new technique. CAVEAT: the diff between combining  known knowledge in new ways and having a clever or new technique is murky. </p><p>8) Over time these strengths and weaknesses have gotten more extreme. It has become a self-fulfilling prophecy where I spend A LOT of time making up problems without asking for help, but when I am trying to solve a problem I early on ask for help. Earlier than I should? Hard to know. </p><p>9) One aspect is `how good am I at math' But a diff angle is that I like to work on things that I KNOW are going to work out, so reading an article is better than trying to create new work. This could be a psychological thing. But is that nature or nurture?  </p><p>10) Could I be a better problem solver? Probably. Could I be a MUCH better problem solver? NO. Could I have been a better problem solver  I did more work on that angle when I was younger? Who knows? </p><p>11) Back to the Quintic: I had the following thought in ninth grade, though I could not possibly have expressed it: The question of, given a problem, how hard is it, upper and lower bounds, is a fundamental one that is worth a lifetime of study. As such my interest in complexity theory and recursion theory goes back to ninth grade or even further. My interest in Ramsey Theory for its own sake (and not in the service of complexity theory) is much more recent and does not quite fit into my narrative. But HEY- real life does not have as well defined narratives as fiction does. </p><p>12) Timing and Luck: IF I had been in grad student at a slight diff time I can imagine doing work on  algorithmic  Galois theory. <a href="https://singer.math.ncsu.edu/Algorithmic_slides.pdf">Here</a>  is a talk on Algorithmic  Galois theory. Note that one of the earliest results is by Landau and Miller from 1985---I had a course from Miller on Alg. Group Theory in 1982. This is NOT a wistful `What might have been' thought. Maybe I would have sucked at it, so its just as well I ended up doing recursion theory, then Ramsey theory, then recursion-theoretic Ramsey Theory, then muffins. </p><p><br/></p><p><br/></p><div><br/></div></div>
    </content>
    <updated>2020-10-19T15:55:00Z</updated>
    <published>2020-10-19T15:55:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-22T11:46:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7803</id>
    <link href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/" rel="alternate" type="text/html"/>
    <title>Understanding generalization requires rethinking deep learning?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Yamini Bansal, Gal Kaplun, and Boaz Barak (See also paper on arxiv, code on gitlab, upcoming talk by Yamini&amp;Boaz, video of past talk) A central puzzle of deep learning is the question of generalization. In other words, what can we deduce from the training performance of a neural network about its test performance on fresh … <a class="more-link" href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/">Continue reading <span class="screen-reader-text">Understanding generalization requires rethinking deep learning?</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em><a href="https://yaminibansal.com/about/">Yamini Bansal</a>, <a href="https://www.galkaplun.com/">Gal Kaplun</a>, and Boaz Barak</em></p>



<p>(See also <em><a href="https://arxiv.org/abs/2010.08508">paper on arxiv</a></em>,  <a href="https://gitlab.com/harvard-machine-learning/rationality-generalization">code on gitlab</a>,  <a href="https://cmsa.fas.harvard.edu/10-28-2020-new-technologies-in-mathematics-seminar/">upcoming talk by Yamini&amp;Boaz</a>,  <a href="https://youtu.be/89ixhju1hJ0">video of past talk</a>)</p>



<p>A central <a href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/">puzzle</a> of deep learning is the question of <em>generalization</em>. In other words, what can we deduce from the <em>training performance</em> of a neural network about its <em>test performance</em> on <em>fresh unseen examples</em>. An <a href="https://arxiv.org/abs/1611.03530">influential paper</a> of Zhang, Bengio, Hardt, Recht, and Vinyals showed that the answer could be “nothing at all.” </p>



<p>Zhang et al. gave examples where modern deep neural networks achieve 100% accuracy on classifying their training data, but their performance on unseen data may be no better than chance. Therefore we cannot give meaningful guarantees for deep learning using traditional “generalization bounds” that bound the difference between test and train performance by some quantity that tends to zero as the number of datapoints <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> increases. This is why (to quote their title), Zhang et al. claimed that <strong>“understanding deep learning requires rethinking generalization”</strong>.</p>



<p>But what if the issue isn’t that we’ve been doing generalization bounds wrong, but rather that we’ve been doing deep learning (or more accurately, supervised deep learning) wrong?</p>



<h3>Self Supervised + Simple fit (SSS) learning</h3>



<p>To explain what we mean, let’s take a small detour to contrast “traditional” or “end-to-end” supervised learning with a different approach to supervised learning, which we’ll call here “Self-Supervised + Simple fit” or “SSS algorithms.” (While the name “SSS algorithms” is new, the approach itself has a <a href="http://people.idsia.ch/~juergen/FKI-126-90_%28revised%29bw_ocr.pdf">long history</a> and has recently been used with great success in practice; our work gives no new methods—only new analysis.)</p>



<p>The classical or “end-to-end” approach for supervised learning can be phrased as <em>“ask and you shall receive”</em>. Given labeled data, you ask (i.e., run an optimizer) for a complex classifier (e.g., a deep neural net) that fits the data (i.e., outputs the given labels on the given data points) and hope that it will be successful on future, unseen, data points as well. End-to-end supervised learning achieves state-of-art results for many classification problems, particularly for computer vision datasets ImageNet and CIFAR-10.</p>



<p>However, end-to-end learning does not directly correspond to the way humans learn to recognize objects (see also <a href="https://youtu.be/7I0Qt7GALVk?t=2475">this talk of LeCun</a>). A baby may see millions of images in the first year of her life, but most of them do not come with explicit labels. After seeing those images, a baby can make future classifications using very few labeled examples. For example, it might be enough to show her once what is a dog and what is a cat for her to correctly classify future dogs and cats, even if they look quite different from these examples.</p>



<figure class="wp-block-image"><img alt="End-to-end learning vs SSS algorithms." src="https://i.imgur.com/LXRaTQq.png"/><strong>Figure 1:</strong> Cartoon of end-to-end vs SSS learning </figure>



<p>In recent years, practitioners have proposed algorithms that are more similar to human learning than supervised learning. Such methods separate the process into two stages. In the <em>first stage</em>, we do <strong>representation learning</strong> whereby we use <em>unlabeled</em> data to learn a <em>representation</em>: a complex map (e.g., a deep neural net) mapping the inputs into some “representation space.” In the <em>second stage</em>, we fit a simple classifier (e.g., a linear threshold function) to the representation of the datapoints and the given labels. We call such algorithms <strong>“Self-Supervision + Simple fit”</strong> or <strong>SSS algorithms</strong>. (Note that, unlike other representation-learning based classifiers, the complex representation is “frozen” and not “fine-tuned” in the second stage, where only a simple classifier is used on top of it.)</p>



<p>While we don’t have a formal definition, a “good representation” should make downstream tasks easier, in the sense of allowing for fewer examples or simpler classifiers. We typically learn a representation via <strong>self supervision</strong> , whereby one finds a representation minimizing an objective function that intuitively requires some “insight” into the data. Approaches for self-supervision include reconstruction, where the objective involves recovering data points from partial information (e.g., recover missing <a href="https://arxiv.org/abs/1810.04805">words</a> or <a href="https://arxiv.org/abs/1604.07379">pixels</a>), and <em><a href="https://arxiv.org/abs/2002.05709">contrastive learning</a></em>, where the objective is to find a representation that make similar points close and dissimilar points far (e.g., in Euclidean space).</p>



<p>SSS algorithms have been traditionally used in natural language processing, where unlabeled data is plentiful, but labeled data for a particular task is often scarce. But recently SSS algorithms were also <a href="https://arxiv.org/abs/1902.06162">used with great success</a> even for vision tasks such as ImageNet and CIFAR10 where <em>all data is labeled!</em> While SSS algorithms do not yet beat the state-of-art supervised learning algorithms, they do get <a href="https://arxiv.org/abs/2006.10029">pretty close</a>. SSS algorithms also have other practical advantages over “end-to-end supervised learning”: they can make use of unlabeled data, the representation could be useful for non-classification tasks, and may have improved out of distribution performance. There has also been recent theoretical analysis of contrastive and reconstruction learning under certain statistical assumptions (see <a href="https://arxiv.org/abs/1902.09229">Arora et al</a> and  <a href="https://arxiv.org/abs/2008.01064">Lee et al</a>).</p>



<h3>The generalization gap of SSS algorithms</h3>



<p>In <a href="https://arxiv.org/abs/2010.08508">a recent paper</a>, we show that SSS algorithms not only work in practice, but work in theory too.</p>



<p>Specifically, we show that such algorithms have <strong>(1)</strong> small generalization gap and <strong>(2)</strong> we can <strong>prove</strong> (under reasonable assumptions) that their generalization gap tends to zero with the number of samples, with bounds that are meaningful for many modern classifiers on the CIFAR-10 and ImageNet datasets. We consider the setting where all data is labeled, and the <em>same dataset</em> is used for both learning the representation and fitting a simple classifier. The resulting classifier includes the overparameterized representation, and so we cannot simply apply “off the shelf” generalization bounds. Indeed, a priori it’s not at all clear that the generalization gap for SSS algorithms should be small.</p>



<p>To get some intuition for the generalization gap of SSS algorithms, consider the experiment where we inject some <em>label noise</em> into our distribution. That is, we corrupt an <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/> fraction of the labels in both the train and test set, replacing them with random labels. Already in the noiseless case (<img alt="\eta=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=0"/>), the generalization gap of SSS algorithms is noticeably smaller than that of end-to-end supervised learning. As we increase the noise, the difference becomes starker. End-to-end supervised learning algorithms can always achieve 100% training accuracy, even as the test accuracy deteriorates, since they can “memorize” all the training labels they are given. In contrast, for SSS algorithms, both training and testing accuracy decrease together as we increase the noise, with training accuracy correlating with test performance. </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif"><img alt="" class="wp-image-7826" src="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif?w=812"/></a><strong>Figure 2:</strong> Generalization gap of end-to-end and SSS algorithms on CIFAR 10 as a function of noise (since there are 10 classes, 90% noisy samples corresponds to the Zhang et al experiment). See also <a href="https://plotly.com/~yaminibansal/1.embed" rel="noreferrer noopener" target="_blank">interactive version</a>.</figure>



<p/>



<p>Our main theoretical result is a formal proof of the above statement. To do so, we consider training with a small amount of label noise (say <img alt="\eta=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D5%5C%25&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=5\%"/>) and define the following quantities:</p>



<ul><li>The <strong>robustness gap</strong> is the amount by which training accuracy degrades between the “clean” (<img alt="\eta=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=0"/>) experiment and the noisy one. (In this and all other quantities, the training accuracy is measured with respect to the original uncorrupted labels.)</li><li>The <strong>memorization gap</strong> considers the noisy experiment (<img alt="\eta=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D5%5C%25&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=5\%"/>) and measures the amount by which performance on the corrupted data samples (where we received the wrong label) is worse than performance on the overall training set. If the algorithm can memorize all given labels, it will be perfectly wrong on the corrupted data samples, leading to a large memorization gap.</li><li>The <strong>rationality gap</strong> is the difference between the performance on the corrupted data samples and performance on unseen test examples. For example, if <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is an image of a dog, then it measures the difference between the probability that <img alt="f(x)=\text{&quot;dog&quot;}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Ctext%7B%22dog%22%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=\text{&quot;dog&quot;}"/> when <img alt="(x,\text{&quot;cat&quot;})" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C%5Ctext%7B%22cat%22%7D%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,\text{&quot;cat&quot;})"/> is in the training set and the probability that <img alt="f(x)=\text{&quot;dog&quot;}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Ctext%7B%22dog%22%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=\text{&quot;dog&quot;}"/> when <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is not in the training set at all. Since intuitively, getting the wrong label should be worse than getting no label at all, we typically expect the rationality gap to be around zero or negative. Formally we define the rationality gap to the maximum between <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> and the difference above, so it is always non-negative. We think of an algorithm with a significant positive rationality gap as “irrational.”</li></ul>



<p>By summing up the quantities above, we get the following inequality, which we call the <strong>RRM bound</strong></p>



<p><em><span class="has-inline-color" style="color: #0693e3;">generalization gap</span></em> <img alt="\leq" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\leq"/> <em><span class="has-inline-color" style="color: #00d084;">robustness gap</span></em> <img alt="+" class="latex" src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="+"/> <em><span class="has-inline-color" style="color: #fcb900;">rationality gap</span></em> <img alt="+" class="latex" src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="+"/> <em><span class="has-inline-color" style="color: #cf2e2e;">memorization gap</span></em></p>



<p>In practice, the <strong>robustness</strong> and <strong>rationality</strong> gaps are always small, both for end-to-end supervised algorithms (which have a large generalization gap), and for SSS algorithms (which have a small generalization gap). Thus the main contribution to the generalization gap comes from the <strong>memorization gap</strong>. Roughly speaking, our main result is the following:</p>



<p><em>If the complexity of the second-stage classifier of an SSS algorithm is smaller than the number of samples then the generalization gap is small.</em></p>



<p>See the <a href="https://arxiv.org/abs/2010.08508">paper</a> for the precise definition of “complexity,” but it is bounded by the number of bits that it takes to describe the simple classifier (no matter how complex is the representation used in the first stage). Our bound yields non-vacuous results in various practical settings; see the figures below or their <a href="https://share.streamlit.io/yaminibansal/streamlit-apps/main/figure1.py" rel="noreferrer noopener" target="_blank">interactive version</a>. </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/intro-cifar.png"><img alt="" class="wp-image-7815" src="https://windowsontheory.files.wordpress.com/2020/10/intro-cifar.png?w=1024"/></a><strong>Figure 3:</strong> Empirical study of the generalization gap of a variety of of SSS algorithms on CIFAR-10. Each vertical line corresponds to one model, sorted by generalization gap. The RRM bound is typically near-tight, and our complexity upper bound is often non vacuous. Use <a href="https://share.streamlit.io/yaminibansal/streamlit-apps/main/figure1.py" rel="noreferrer noopener" target="_blank">this webpage</a> to interact with figures 3 and 4.</figure>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/imagenet_gaps.png"><img alt="" class="wp-image-7817" src="https://windowsontheory.files.wordpress.com/2020/10/imagenet_gaps.png?w=1024"/></a><strong>Figure 4:</strong> Empirical study of gaps for the ImageNet dataset. Because of limited computational resources, we only evaluated the theoretical bound for two models in this dataset.</figure>



<h3>What’s next</h3>



<p>There are still many open questions. Can we prove rigorous bounds on robustness and rationality? We have some preliminary results in the paper, but there is much room for improvement. Similarly, our complexity-based upper bound is far from tight at the moment, though the RRM bound itself is often surprisingly tight. Our work only applies to SSS algorithms, but people have the intuition that even end-to-end supervised learning algorithms implicitly learn a representation. So perhaps these tools can apply to such algorithms as well. As mentioned, we don’t yet have formal definitions for “good representations,” and the choice of the self-supervision task is still somewhat of a “black art” – can we find a more principled approach?</p></div>
    </content>
    <updated>2020-10-19T00:30:40Z</updated>
    <published>2020-10-19T00:30:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-10-23T04:20:56Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint</id>
    <link href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html" rel="alternate" type="text/html"/>
    <title>Polyhedra without disjoint faces</title>
    <summary>Some research I’ve been doing led me to consider the (prism,\(K_{3,3}\))-minor-free graphs. It’s not always easy to go from forbidden minors to the graphs that forbid them, or vice versa, but in this case I think there’s a nice characterization, which I’m posting here because it doesn’t fit into the research writeup: these are the graphs whose nontrivial triconnected components are \(K_5\), wheel graphs, or the graph \(K_5-e\) of the triangular bipyramid. The illustration below shows an example of a graph with this structure, with its nontrivial triconnected components colored red and yellow. There’s a simpler and more geometric way to say almost the same thing: the only convex polyhedra that do not have two vertex-disjoint faces are the pyramids and the triangular bipyramid.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Some research I’ve been doing led me to consider the (prism,\(K_{3,3}\))-minor-free graphs. It’s not always easy to go from <a href="https://en.wikipedia.org/wiki/Forbidden_graph_characterization">forbidden minors</a> to the graphs that forbid them, or vice versa, but in this case I think there’s a nice characterization, which I’m posting here because it doesn’t fit into the research writeup: these are the graphs whose nontrivial triconnected components are \(K_5\), <a href="https://en.wikipedia.org/wiki/Wheel_graph">wheel graphs</a>, or the graph \(K_5-e\) of the <a href="https://en.wikipedia.org/wiki/Triangular_bipyramid">triangular bipyramid</a>. The illustration below shows an example of a graph with this structure, with its nontrivial triconnected components colored red and yellow. There’s a simpler and more geometric way to say almost the same thing: the only convex polyhedra that do not have two vertex-disjoint faces are the pyramids and the triangular bipyramid.</p>

<p style="text-align: center;"><img alt="A (prism, K_{3,3})-minor-free graph, with its nontrivial triconnected components colored red and yellow" src="https://11011110.github.io/blog/assets/2020/prism-k33-free.svg"/></p>

<p>Some definitions:</p>

<ul>
  <li>
    <p>Here by the prism graph I mean the graph of the triangular prism. Any other prism has this one as a minor, and so is irrelevant as a forbidden minor. However, the pyramids in this structure can have any polygon as their base, corresponding to wheel graphs with arbitrarily many vertices.</p>
  </li>
  <li>
    <p>\(K_{3,3}\) is a complete bipartite graph with three vertices on each side of its bipartition, famous as the <a href="https://en.wikipedia.org/wiki/Three_utilities_problem">utility graph</a>, one of the two forbidden minors for planar graphs. The triangular prism graph and \(K_{3,3}\) are the only two <a href="https://en.wikipedia.org/wiki/Cubic_graph">3-regular graphs</a> with six vertices.</p>
  </li>
</ul>

<p style="text-align: center;"><img alt="The prism graph and K_{3,3}" src="https://11011110.github.io/blog/assets/2020/prism-k33.svg"/></p>

<ul>
  <li>
    <p>The triconnected components of a graph are the graphs associated with the nodes of its <a href="https://en.wikipedia.org/wiki/SPQR_tree">SPQR tree</a>, or of the SPQR trees of its biconnected components. These are cycle graphs, dipole multigraphs, or 3-connected graphs, and by “nontrivial” I mean the ones that are not cycles or dipoles. A triconnected component might not be a subgraph of the given graph, because it can have additional edges that correspond to paths in the given graph. For instance, subdividing the edges of any graph into paths, or more generally replacing edges by arbitrary series-parallel graphs, does not change its set of nontrivial triconnected components.</p>
  </li>
  <li>
    <p>I’m using “face” in the usual three-dimensional meaning, a two-dimensional subset of the boundary of the polyhedron. For higher-dimensional polytopes, “face” has a different meaning that also includes vertices and edges, and “facet” would be used to refer to the \((d-1)\)-dimensional faces, but using that terminology seems overly pedantic here.</p>
  </li>
</ul>

<p>Sketch of proof of the characterization of polyhedra without two disjoint faces: Consider any polyhedron without disjoint faces. If one face shares an edge with all the others, it’s a <a href="https://en.wikipedia.org/wiki/Halin_graph">Halin graph</a>, a graph formed by linking the leaves of a tree into a cycle; if the tree is a star, it’s a pyramid, and otherwise contracting all but one of the interior edges of the tree, and then all but four of the cycle edges, will produce a prism minor. In the remaining case, some two faces share only a vertex \(v\), which must have degree four or more. Each face that is disjoint from \(v\) must touch all that faces incident to \(v\), which can only happen when there is one face disjoint from \(v\) (a pyramid) or two faces disjoint from \(v\), neither of which has an edge disjoint from the other one (a bipyramid).</p>

<p>Sketch of a lemma that every convex polyhedron with two disjoint faces has a prism minor: glue a pyramidal cap into each of the two faces, producing a larger convex polyhedron which by either <a href="https://en.wikipedia.org/wiki/Steinitz%27s_theorem">Steinitz’s theorem</a> or <a href="https://en.wikipedia.org/wiki/Balinski%27s_theorem">Balinski’s theorem</a> is necessarily 3-connected, and find three vertex-disjoint paths between the apexes of the attached pyramids. The parts of these paths outside the two glued pyramids, together with the boundaries of the two faces, form a subdivision of a prism.</p>

<p>Sketch of proof of the characterization of (prism,\(K_{3,3}\))-minor-free graphs: The nontrivial triconnected components are exactly the maximal triconnected minors of the given graph, so if either of the two triconnected forbidden minors is to be found in the given graph, it will be found in one of the triconnected components. \(K_5\) and the triangular bipyramid are too small to have one of the forbidden minors. The only 3-connected minors of the pyramid graphs are smaller pyramids, obtained by contracting one of the cycle edges of the pyramid, so these also do not have a forbidden minor. Therefore the graphs of the stated form are all (prism,\(K_{3,3}\))-minor-free.</p>

<p>In the other direction, suppose that a graph is (prism,\(K_{3,3}\))-minor-free.
Each triconnected component is a minor, so it must also be (prism,\(K_{3,3}\))-minor-free. What can these components look like? Forbidding \(K_{3,3}\) as a minor rules out nonplanar components other than \(K_5\), by a theorem of Wagner<sup id="fnref:wagner"><a class="footnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fn:wagner">1</a></sup> and Hall.<sup id="fnref:hall"><a class="footnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fn:hall">2</a></sup> So the remaining components that we need to consider are triconnected planar graphs with no prism minor. These cannot have two disjoint faces by the lemma, and so they can only be pyramids or the triangular bipyramid.</p>

<div class="footnotes">
  <ol>
    <li id="fn:wagner">
      <p>K. Wagner. Über eine Erweiterung des Satzes von Kuratowski. <em>Deutsche Mathematik</em>, 2:280–285, 1937. <a class="reversefootnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fnref:wagner">↩</a></p>
    </li>
    <li id="fn:hall">
      <p>D. W. Hall. A note on primitive skew curves. <em>Bulletin of the American Mathematical Society</em>, 49(12):935–936, 1943. <a href="https://doi.org/10.1090/ S0002-9904-1943-08065-2">doi:10.1090/ S0002-9904-1943-08065-2</a>. <a class="reversefootnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fnref:hall">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/105058649830809584">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-10-18T17:06:00Z</updated>
    <published>2020-10-18T17:06:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-10-20T05:33:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/155</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/155" rel="alternate" type="text/html"/>
    <title>TR20-155 |  Log-rank and lifting for AND-functions | 

	Sam McGuire, 

	Shachar Lovett, 

	Alexander Knop, 

	Weiqiang Yuan</title>
    <summary>Let $f: \{0,1\}^n \to \{0, 1\}$ be a boolean function, and let $f_\land (x, y) = f(x \land y)$ denote the AND-function of $f$, where $x \land y$ denotes bit-wise AND. We study the deterministic communication complexity of $f_\land$ and show that, up to a $\log n$ factor, it is bounded by a polynomial in the logarithm of the real rank of the communication matrix of $f_\land$. This comes within a $\log n$ factor of establishing the log-rank conjecture for AND-functions with no assumptions on $f$. Our result stands in contrast with previous results on special cases of the log-rank 
conjecture, which needed significant restrictions on $f$ such as monotonicity or low $\mathbb{F}_2$-degree. Our techniques can also be used to prove (within a $\log n$ factor) a lifting theorem for AND-functions, stating that the deterministic communication complexity of $f_\land$ is polynomially-related to the AND-decision tree complexity of $f$.

The results rely on a new structural result regarding boolean functions $f:\{0, 1\}^n \to \{0, 1\}$ with a sparse polynomial representation, which may be of independent interest. We show that if the polynomial computing $f$ has few monomials then the set system of the monomials has a small hitting set, of size poly-logarithmic in its sparsity. We also establish extensions of this result to multi-linear polynomials $f:\{0,1\}^n \to \mathbb{R}$ with a larger range.</summary>
    <updated>2020-10-18T14:44:03Z</updated>
    <published>2020-10-18T14:44:03Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-23T04:20:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=493</id>
    <link href="https://tcsplus.wordpress.com/2020/10/16/tcs-talk-wednesday-october-21-aayush-jain-ucla/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 21 — Aayush Jain, UCLA</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Aayush Jain from UCLA will speak about “Indistinguishability Obfuscation from Well-Founded Assumptions” (abstract below). You can reserve a spot as an individual or a group to join us […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Aayush Jain</strong> from UCLA will speak about “<em>Indistinguishability Obfuscation from Well-Founded Assumptions</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We present a construction of an indistinguishability obfuscation scheme, whose security rests on the subexponential hardness of four well-founded assumptions. We show the existence of an indistinguishability Obfuscation scheme for all circuits assuming sub-exponential security of the following assumptions:</p>



<ul class="wp-block-quote"><li>The Learning with Errors (LWE) assumption with arbitrarily small subexponential modulus-to-noise ratio,</li><li>The SXDH assumption with respect to bilinear groups of prime order <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="p"/>,</li><li>Existence of a Boolean Pseudorandom Generator (PRG) in <img alt="\mathsf{NC}^0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathsf{NC}^0"/> with arbitrary polynomial stretch, that is, mapping <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n"/> bits to <img alt="n^{1+\tau}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2B%5Ctau%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n^{1+\tau}"/> bits, for any constant \tau&gt;0.</li><li>The Learning Parity with Noise (LPN) assumption over <img alt="\mathbb{Z}_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathbb{Z}_p"/> with error-rate <img alt="\ell^{-\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell%5E%7B-%5Cdelta%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\ell^{-\delta}"/>, where <img alt="\ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\ell"/> is the dimension of the secret and <img alt="\delta&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%3E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\delta&gt;0"/> is an arbitrarily small constant.<br/>Further, assuming only polynomial security of these assumptions, there exists a compact public-key functional encryption scheme for all circuits.</li></ul>



<p class="wp-block-quote">The main technical novelty is the introduction and construction of a cryptographic pseudorandom generator that we call a Structured-Seed PRG (sPRG), assuming LPN over <img alt="\mathbb{Z}_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathbb{Z}_p"/> and PRGs in <img alt="\mathsf{NC}^0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathsf{NC}^0"/>. During the talk, I will discuss how structured seed PRGs have evolved from different notions of novel pseudorandom generators proposed in the past few years, and how an interplay between different areas of theoretical computer science played a major role in providing valuable insights leading to this work. Time permitting, I will go into the details of how to construct sPRGs. <br/><br/>Joint work with Huijia (Rachel) Lin and Amit Sahai</p></div>
    </content>
    <updated>2020-10-16T06:33:00Z</updated>
    <published>2020-10-16T06:33:00Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-10-23T04:21:20Z</updated>
    </source>
  </entry>
</feed>
