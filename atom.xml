<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-03-12T10:23:30Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>https://differentialprivacy.org/flavoursofdelta/</id>
    <link href="https://differentialprivacy.org/flavoursofdelta/" rel="alternate" type="text/html"/>
    <title>What is Œ¥, and what Œ¥ifference does it make?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>There are many variants or flavours of differential privacy (DP) some weaker than others: often, a given variant comes with own guarantees and ‚Äúconversion theorems‚Äù to the others. As an example, ‚Äúpure‚Äù DP has a single parameter \(\varepsilon\), and corresponds to a very stringent notion of DP:</p>

<blockquote>
  <p>An algorithm \(M\) is \(\varepsilon\)-DP if, for all neighbouring inputs \(D,D'\) and all measurable \(S\), \( \Pr[ M(D) \in S ] \leq e^\varepsilon\Pr[ M(D‚Äô) \in S ] \).</p>
</blockquote>

<p>By relaxing this a little, one obtains the standard definition of approximate DP, a.k.a. \((\varepsilon,\delta)\)-DP:</p>

<blockquote>
  <p>An algorithm \(M\) is \((\varepsilon,\delta)\)-DP if, for all neighbouring inputs \(D,D'\) and all measurable \(S\), \( \Pr[ M(D) \in S ] \leq e^\varepsilon\Pr[ M(D‚Äô) \in S ]+\delta \).</p>
</blockquote>

<p>This definition is very useful, as in many settings achieving the stronger \(\varepsilon\)-DP guarantee (i.e., \(\delta=0\)) is impossible, or comes at a very high utility cost. But how to interpret it? The above definition, on its face, doesn‚Äôt preclude what one may call ‚Äú<em>catastrophic failures of privacy</em> üí•:‚Äù most of the time, things are great, but with some small probability \(\delta\) all hell breaks loose. For instance, the following algorithm is \((\varepsilon,\delta)\)-DP:</p>

<ul>
  <li>Get a sensitive database \(D\) of \(n\) records</li>
  <li>Select uniformly at random a fraction \(\delta\) of the database (\(\delta n\) records)</li>
  <li>Output that subset of records in the clear üí•</li>
</ul>

<p>(actually, this is even \((0,\delta)\)-DP!). This sounds preposterous, and obviously something that one would want to avoid in practice (lest one wants to face very angry customers or constituents). This is one of the rules of thumb for picking \(\delta\) small enough (or even ‚Äúcryptographically small‚Äù), typically \(\delta \ll 1/n\), so that the records are safe (hard to disclose \(\delta n \ll 1\) records).</p>

<p>So: good privacy most of the time, but with probably \(\delta\) then all bets are off.</p>

<p>However, those catastrophic failure of privacy, while technically allowed by the definition of \((\varepsilon,\delta)\)-DP, <strong>are not something that can really happen with the DP algorithms and techniques used both in practice and in theoretical work.</strong> Before explaining why, let‚Äôs see what is the kind of desirable behaviour one would expect: a <em>‚Äúsmooth, manageable tradeoff of privacy parameters.‚Äù</em> For that discussion, let‚Äôs introduce the <em>privacy loss random variable</em>: given an algorithm M and two neighbouring inputs D,D‚Äô, let \(f(y)\) be defined as
\[
	f(y) = \log\frac{\Pr[M(D)=y]}{\Pr[M(D‚Äô)=y]}
\]
for every possible output \(y\in\Omega\). Now, define the random variable \(Z := f(M(D))\) (implicitly, \(Z\) depends on \(D,D',M\)). This random variable quantifies how much observing the output of the algorithm \(M\) helps distinguishing between \(D\) and \(D'\).</p>

<p>Now, going a little bit fast, you can check that saying that \(M\) is \(\varepsilon\)-DP corresponds to the guarantee ‚Äú<em>\(\Pr[Z &gt; \varepsilon] = 0\) for all neighbouring inputs \(D,D'\).</em>‚Äù
Similarly, \(M\) being \((\varepsilon,\delta)\)-DP is the guarantee \(\Pr[Z &gt; \varepsilon] \leq \delta\).\({}^{(\dagger)}\) For instance, the ‚Äúcatastrophic failure of privacy‚Äù corresponds to the scenario below, which depicts a possible distribution for \(Z\): \(Z\leq \varepsilon\) with probability \(1-\delta\), but then with probability \(\delta\) we have \(Z\gg 1\).</p>

<p><img alt="The type of (bad) distribution of Z corresponding to 'our catastrophic failure of privacy'" src="https://differentialprivacy.org/images/flavours-delta-fig1.png" style="margin: auto; display: block;" width="600"/></p>

<p>What we would like is a smoother thing, where even when \(Z&gt;\varepsilon\) is still remains reasonable and doesn‚Äôt immediately become large. A nice behaviour of the tails, ideally something like this:</p>

<p><img alt="A distribution for Z with nice tails, leading to smooth tradeoffs between &#x3B5; and &#x3B4;" src="https://differentialprivacy.org/images/flavours-delta-fig2.png" style="margin: auto; display: block;" width="600"/></p>

<p>For instance, if we had a bound on \(\mathbb{E}[|Z|]\), we could use Markov‚Äôs inequality to get, well, <em>something</em>. For instance, imagine we had \(\mathbb{E}[|Z|]\leq \varepsilon\delta\): then 
\[
	\Pr[ |Z| &gt; \varepsilon ] \leq \frac{\mathbb{E}[|Z|]}{\varepsilon }\leq \delta
\]
<em>(great! We have \((\varepsilon,\delta)\)-DP)</em>; but also  \(\Pr[ |Z| &gt; 10\varepsilon ] \leq \frac{\delta}{10}\). Privacy violations do not blow up out of proporxtion immediately, we can trade \(\varepsilon\) for \(\delta\). That seems like the type of behaviour we would like our algorithms to exhibit.</p>

<p><img alt="The type of privacy guarantees a Markov-type tail bound would give" src="https://differentialprivacy.org/images/flavours-delta-fig3.png" style="margin: auto; display: block;" width="600"/></p>

<p>But why stop at Markov‚Äôs inequality then, which gives some nice but still weak tail bounds? Why not ask for <em>stronger</em>: Chebyshev‚Äôs inequality? Subexponential tail bounds? Hell, <em>subgaussian</em> tail bounds? This is, basically, what some stronger notions of differential privacy than approximate DP give.</p>

<ul>
  <li>
    <p><strong>R√©nyi DP</strong> <a href="https://arxiv.org/abs/1702.07476" title="Ilya Mironov. Renyi Differential Privacy. CSF 2017"><strong>[Mironov17]</strong></a>, for instance, is a guarantee on the moment-generating function (MGF) of the privacy random variable \(Z\): it has two parameters, \(\alpha&gt;1\) and \(\tau\), and requires that \(\mathbb{E}[e^{(\alpha-1)Z}] \leq e^{(\alpha-1)\tau}\) for all neighbouring \(D,D'\). In turn, by applying for instance Markov‚Äôs inequality to the MGF of \(Z\), we can control the tail bounds, and get a nice, smooth tradeoff in terms of \((\varepsilon,\delta)\)-DP.</p>
  </li>
  <li>
    <p><strong>Concentrated DP</strong> (CDP)  <a href="https://arxiv.org/abs/1605.02065" title="Mark Bun and Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. TCC 2016"><strong>[BS16]</strong></a> is an even stronger requirement, which roughly speaking requires the algorithm to be R√©nyi DP <em>simultaneously</em> for all \(1&lt; \alpha \leq \infty\). More simply, this is ‚Äúmorally‚Äù a requirement on the MGF of \(Z\) which asks it to be subgaussian.</p>
  </li>
</ul>

<p>The above two examples are not just fun but weird variants of DP: they actually capture the behaviour of many well-known differentially private algorithms, and in particular that of the Gaussian mechanism. While the guarantees they provide are less easy to state and interpret than \(\varepsilon\)-DP or \((\varepsilon,\delta)\)-DP, they are incredibly useful to analyze those algorithms, and enjoy very nice composition properties‚Ä¶ and, of course, lead to that smooth tradeoff between \(\varepsilon\) and \(\delta\) for \((\varepsilon,\delta)\)-DP.</p>

<p><strong>To summarize:</strong></p>
<ul>
  <li>\(\varepsilon\)-DP gives great guarantees, but is a very stringent requirement. Corresponds to the privacy loss random variable supported on \([-\varepsilon,\varepsilon]\) (no tails!)</li>
  <li>\((\varepsilon,\delta)\)-DP gives guarantees easy to parse, but on its face allows for very bad behaviours. Corresponds to the privacy loss random variable in \([-\varepsilon,\varepsilon]\) with probability \(1-\delta\) (but outside, all bets are off!)</li>
  <li>R√©nyi DP and Concentrated DP correspond to something in between, controlling the tails of the privacy loss random variable by a guarantee on its MGF. A bit harder to interpret, but capture the behaviour of many DP building blocks can be converted to \((\varepsilon,\delta)\)-DP (with nice trade-offs between \(\varepsilon\) and \(\delta\).</li>
</ul>

<hr/>
<p>\({}^{(\dagger)}\) The astute reader may notice that this is not <em>quite</em> true. Namely, the guarantee \(\Pr[Z &gt; \varepsilon] \leq \delta\) on the privacy loss random variable (PLRV) does imply \((\varepsilon,\delta)\)-differential privacy, but the converse does not hold. See, for instance, Lemma 9 of <a href="https://arxiv.org/abs/2004.00010" title="Cl&#xE9;ment L. Canonne, Gautam Kamath, Thomas Steinke. The Discrete Gaussian for Differential Privacy. NeurIPS 2020"><strong>[CKS20]</strong></a> for an exact characterization of \((\varepsilon,\delta)\)-DP in terms of the PLRV.</p></div>
    </summary>
    <updated>2021-03-12T01:00:00Z</updated>
    <published>2021-03-12T01:00:00Z</published>
    <author>
      <name>Cl√©ment Canonne</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-03-11T22:43:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=535</id>
    <link href="https://tcsplus.wordpress.com/2021/03/11/tcs-talk-wednesday-march-17-avishay-tal-uc-berkeley/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, March 17 ‚Äî Avishay Tal, UC Berkeley</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, March 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Avishay Tal from UC Berkeley will speak about ‚ÄúJunta Distance Approximation with Sub-Exponential Queries‚Äù (abstract below). You can reserve a spot as an individual or a group to [‚Ä¶]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, March 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <a href="http://www.avishaytal.org/"><strong>Avishay Tal</strong></a> from UC Berkeley will speak about ‚Äú<em>Junta Distance Approximation with Sub-Exponential Queries</em>‚Äù (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Joint Work with Vishnu Iyer and Michael Whitmeyer.</p>
<p>A Boolean function <img alt="f\colon \{0,1\}^n \to \{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccolon+%5C%7B0%2C1%5C%7D%5En+%5Cto+%5C%7B0%2C1%5C%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> is called a <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>-junta if it depends only on <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> out of the <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> input bits. Junta testing is the task of distinguishing between <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>-juntas and functions that are far from <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>-juntas. A long line of work settled the query complexity of testing <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>-juntas, which is <img alt="O(k log(k))" class="latex" src="https://s0.wp.com/latex.php?latex=O%28k+log%28k%29%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> [Blais, STOC 2009; Saglam, FOCS 2018]. Suppose, however, that <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> is not a perfect <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>-junta but rather correlated with a <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>-junta. How well can we estimate this correlation? This question was asked by De, Mossel, and Neeman [FOCS 2019], who gave an algorithm for the task making <img alt="\sim\exp(k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csim%5Cexp%28k%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> queries. We present an improved algorithm that makes <img alt="\sim\exp(\sqrt{k})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csim%5Cexp%28%5Csqrt%7Bk%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> many queries. Along the way, we also give an algorithm, making <img alt="\mathrm{poly}(k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bpoly%7D%28k%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/> queries, that provides ‚Äúimplicit oracle access‚Äù to the underlying <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002"/>-junta. Our techniques are Fourier analytical and introduce the notion of ‚Äúnormalized influences‚Äù that might be of independent interest.</p>
<p>Paper: <a href="https://eccc.weizmann.ac.il/report/2021/004/" rel="nofollow">https://eccc.weizmann.ac.il/report/2021/004/</a></p></blockquote></div>
    </content>
    <updated>2021-03-11T23:11:23Z</updated>
    <published>2021-03-11T23:11:23Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-03-12T10:21:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5382</id>
    <link href="https://www.scottaaronson.com/blog/?p=5382" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5382#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5382" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Long-delayed UT Austin Quantum Complexity Theory Student Project Showcase!</title>
    <summary xml:lang="en-US">Back at MIT, whenever I taught my graduate course on Quantum Complexity Theory (see here for lecture notes), I had a tradition of showcasing the student projects on this blog: see here (Fall 2010), here (Fall 2012), here (Fall 2014). I was incredibly proud that, each time I taught, at least some of the projects [‚Ä¶]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Back at MIT, whenever I taught my graduate course on Quantum Complexity Theory (<a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-845-quantum-complexity-theory-fall-2010/lecture-notes/">see here</a> for lecture notes), I had a tradition of showcasing the student projects on this blog: see <a href="https://www.scottaaronson.com/blog/?p=515">here (Fall 2010)</a>, <a href="https://www.scottaaronson.com/blog/?p=1181">here (Fall 2012)</a>, <a href="https://www.scottaaronson.com/blog/?p=2109">here (Fall 2014)</a>.  I was incredibly proud that, each time I taught, at least some of the projects led to publishable original research‚Äîsometimes highly significant research, like Paul Christiano‚Äôs work on quantum money (which led to my later <a href="https://arxiv.org/abs/1203.4740">paper with him</a>), Shelby Kimmel‚Äôs <a href="https://arxiv.org/abs/1101.0797">work</a> on quantum query complexity, Jenny Barry‚Äôs <a href="https://arxiv.org/abs/1406.2858">work</a> on quantum partially observable Markov decision processes (‚ÄúQOMDPs‚Äù), or Matt Coudron and Henry Yuen‚Äôs work on randomness expansion (which led to their later <a href="https://arxiv.org/abs/1310.6755">breakthrough</a> in the subject).</p>



<p>Alas, after I moved to UT Austin, for some reason I discontinued the tradition of these blog-showcases‚Äîand inexcusably, I did this even though the wonderful new research results continued!  Now that I‚Äôm teaching Quantum Complexity Theory at UT for the third time (via Zoom, of course), I decided that it was finally time to remedy this.  To keep things manageable, this time I‚Äôm going to limit myself to research projects that began their lives in my course <em>and that are already public on the arXiv</em> (or in one case, that will soon be).</p>



<p>So please enjoy the following smorgasbord, from 2016 and 2019 iterations of my course!  And if you have any questions about any of the projects‚Äîwell, I‚Äôll try to get the students to answer in the comments section!  Thanks so much and congratulations to the students for their work.</p>



<h2>From the Fall 2016 iteration of the course</h2>



<p>William Hoza (project turned into a joint paper with Cole Graham), <strong><a href="https://arxiv.org/abs/1612.05680">Universal Bell Correlations Do Not Exist</a></strong>.</p>



<blockquote class="wp-block-quote"><p>We prove that there is no finite-alphabet nonlocal box that generates exactly those correlations that can be generated using a maximally entangled pair of qubits. More generally, we prove that if some finite-alphabet nonlocal box is strong enough to simulate arbitrary local projective measurements of a maximally entangled pair of qubits, then that nonlocal box cannot itself be simulated using any finite amount of entanglement. We also give a quantitative version of this theorem for approximate simulations, along with a corresponding upper bound.</p></blockquote>



<p>Patrick Rall, <strong><a href="https://arxiv.org/abs/1702.06990">Signed quantum weight enumerators characterize qubit magic state distillation</a></strong>.</p>



<blockquote class="wp-block-quote"><p>Many proposals for fault-tolerant quantum computation require injection of ‚Äòmagic states‚Äô to achieve a universal set of operations. Some qubit states are above a threshold fidelity, allowing them to be converted into magic states via ‚Äòmagic state distillation‚Äô, a process based on stabilizer codes from quantum error correction.<br/>We define quantum weight enumerators that take into account the sign of the stabilizer operators. These enumerators completely describe the magic state distillation behavior when distilling T-type magic states. While it is straightforward to calculate them directly by counting exponentially many operator weights, it is also an NP-hard problem to compute them in general. This suggests that finding a family of distillation schemes with desired threshold properties is at least as hard as finding the weight distributions of a family of classical codes.<br/>Additionally, we develop search algorithms fast enough to analyze all useful 5 qubit codes and some 7 qubit codes, finding no codes that surpass the best known threshold.</p></blockquote>



<h2>From the Spring 2019 iteration of the course</h2>



<p>Ying-Hao Chen, <strong><a href="https://arxiv.org/abs/1909.03787">2-Local Hamiltonian with Low Complexity is QCMA-complete</a></strong>.</p>



<blockquote class="wp-block-quote"><p>We prove that 2-Local Hamiltonian (2-LH) with Low Complexity problem is QCMA-complete by combining the results from the QMA-completeness of 2-LH and QCMA-completeness of 3-LH with Low Complexity. The idea is straightforward. It has been known that 2-LH is QMA-complete. By putting a low complexity constraint on the input state, we make the problem QCMA. Finally, we use similar arguments as in [Kempe, Kitaev, Regev] to show that all QCMA problems can be reduced to our proposed problem.</p></blockquote>



<p>Jeremy Cook, <strong><a href="https://arxiv.org/abs/1907.11368">On the relationships between Z-, C-, and H-local unitaries</a></strong>.</p>



<blockquote class="wp-block-quote"><p>Quantum walk algorithms can speed up search of physical regions of space in both the discrete-time [<a href="https://arxiv.org/abs/quant-ph/0402107">arXiv:quant-ph/0402107</a>] and continuous-time setting [<a href="https://arxiv.org/abs/quant-ph/0306054">arXiv:quant-ph/0306054</a>], where the physical region of space being searched is modeled as a connected graph. In such a model, Aaronson and Ambainis [<a href="https://arxiv.org/abs/quant-ph/0303041">arXiv:quant-ph/0303041</a>] provide three different criteria for a unitary matrix to act locally with respect to a graph, called¬†<em>Z</em>-local,¬†<em>C</em>-local, and¬†<em>H</em>-local unitaries, and left the open question of relating these three locality criteria. Using a correspondence between continuous- and discrete-time quantum walks by Childs [<a href="https://arxiv.org/abs/0810.0312">arXiv:0810.0312</a>], we provide a way to approximate¬†<em>N</em>√ó<em>N H</em>-local unitaries with error¬†<em>Œ¥</em>¬†using¬†<em>O</em>(1/<em>‚àöŒ¥,‚àöN</em>)¬†<em>C</em>-local unitaries, where the comma denotes the maximum of the two terms.</p></blockquote>



<p>Joshua A. Cook, <strong><a href="https://arxiv.org/abs/1906.10495">Approximating Unitary Preparations of Orthogonal Black Box States</a></strong>.</p>



<blockquote class="wp-block-quote"><p>In this paper, I take a step toward answering the following question: for m different small circuits that compute m orthogonal n qubit states, is there a small circuit that will map m computational basis states to these m states without any input leaving any auxiliary bits changed. While this may seem simple, the constraint that auxiliary bits always be returned to 0 on any input (even ones besides the m we care about) led me to use sophisticated techniques. I give an approximation of such a unitary in the m = 2 case that has size polynomial in the approximation error, and the number of qubits n.</p></blockquote>



<p>Sabee Grewal (project turned into a joint paper with me), <strong><a href="https://arxiv.org/abs/2102.10458">Efficient Learning of Non-Interacting Fermion Distributions</a></strong>.</p>



<blockquote class="wp-block-quote"><p>We give an efficient classical algorithm that recovers the distribution of a non-interacting fermion state over the computational basis. For a system of¬†<em>n</em>¬†non-interacting fermions and¬†<em>m</em>¬†modes, we show that¬†<em>O</em>(<em>m</em><sup>2</sup><em>n</em><sup>4</sup>log(<em>m</em>/<em>Œ¥</em>)/<em>Œµ</em><sup>4</sup>)¬†samples and¬†<em>O</em>(<em>m</em><sup>4</sup><em>n</em><sup>4</sup>log(<em>m</em>/<em>Œ¥</em>)/<em>Œµ</em><sup>4</sup>)¬†time are sufficient to learn the original distribution to total variation distance¬†<em>Œµ</em>¬†with probability¬†1‚àí<em>Œ¥</em>. Our algorithm empirically estimates the one- and two-mode correlations and uses them to reconstruct a succinct description of the entire distribution efficiently.</p></blockquote>



<p>Sam Gunn and Niels Kornerup, <strong><a href="https://arxiv.org/abs/1906.07673">Review of a Quantum Algorithm for Betti Numbers</a></strong>.</p>



<blockquote class="wp-block-quote"><p>We looked into the algorithm for calculating Betti numbers presented by Lloyd, Garnerone, and Zanardi (LGZ). We present a new algorithm in the same spirit as LGZ with the intent of clarifying quantum algorithms for computing Betti numbers. Our algorithm is simpler and slightly more efficient than that presented by LGZ. We present a thorough analysis of our algorithm, pointing out reasons that both our algorithm and that presented by LGZ do not run in polynomial time for most inputs. However, the algorithms do run in polynomial time for calculating an approximation of the Betti number to polynomial multiplicative error, when applied to some class of graphs for which the Betti number is exponentially large.</p></blockquote>



<p>William Kretschmer, <strong><a href="https://arxiv.org/abs/1907.06731">Lower Bounding the AND-OR Tree via Symmetrization</a></strong>.</p>



<blockquote class="wp-block-quote"><p>We prove a simple, nearly tight lower bound on the approximate degree of the two-level¬†AND-OR¬†tree using symmetrization arguments. Specifically, we show that¬†~deg(AND<em>m</em>‚àòOR<em>n</em>)=Œ©(~<em>‚àö</em>(<em>mn</em>)). To our knowledge, this is the first proof of this fact that relies on symmetrization exclusively; most other proofs involve the more complicated formulation of approximate degree as a linear program [BT13, She13, BDBGK18]. Our proof also demonstrates the power of a symmetrization technique involving Laurent polynomials (polynomials with negative exponents) that was previously introduced by Aaronson, Kothari, Kretschmer, and Thaler [AKKT19].</p></blockquote>



<p>Jiahui Liu and Ruizhe Zhang (project turned into a joint paper with me, Mark Zhandry, and Qipeng Liu), <br/><strong><a href="https://arxiv.org/abs/2004.09674">New Approaches for Quantum Copy-Protection</a></strong>.</p>



<blockquote class="wp-block-quote"><p>Quantum copy protection uses the unclonability of quantum states to construct quantum software that provably cannot be pirated. Copy protection would be immensely useful, but unfortunately little is known about how to achieve it in general. In this work, we make progress on this goal, by giving the following results:<br/>‚Äì We show how to copy protect any program that cannot be learned from its input/output behavior, relative to a classical oracle. This improves on Aaronson [CCC‚Äô09], which achieves the same relative to a quantum oracle. By instantiating the oracle with post-quantum candidate obfuscation schemes, we obtain a heuristic construction of copy protection.<br/>‚Äì We show, roughly, that any program which can be watermarked can be copy detected, a weaker version of copy protection that does not prevent copying, but guarantees that any copying can be detected. Our scheme relies on the security of the assumed watermarking, plus the assumed existence of public key quantum money. Our construction is general, applicable to many recent watermarking schemes.</p></blockquote>



<p>John Kallaugher, <strong>Triangle Counting in the Quantum Streaming Model</strong>.  Not yet available but coming soon to an arXiv near you!</p>



<blockquote class="wp-block-quote"><p>We give a quantum algorithm for counting triangles in graph streams that uses less space than the best possible classical algorithm.</p></blockquote></div>
    </content>
    <updated>2021-03-11T20:31:03Z</updated>
    <published>2021-03-11T20:31:03Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-03-11T20:31:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.06264</id>
    <link href="http://arxiv.org/abs/2103.06264" rel="alternate" type="text/html"/>
    <title>A Lattice Linear Predicate Parallel Algorithm for the Dynamic Programming Problems</title>
    <feedworld_mtime>1615420800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Vijay_K=.html">Vijay K. Garg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06264">PDF</a><br/><b>Abstract: </b>It has been shown that the parallel Lattice Linear Predicate (LLP) algorithm
solves many combinatorial optimization problems such as the shortest path
problem, the stable marriage problem and the market clearing price problem. In
this paper, we give the parallel LLP algorithm for many dynamic programming
problems. In particular, we show that the LLP algorithm solves the longest
subsequence problem, the optimal binary search tree problem, and the knapsack
problem. Furthermore, the algorithm can be used to solve the constrained
versions of these problems so long as the constraints are lattice linear. The
parallel LLP algorithm requires only read-write atomicity and no higher-level
atomic instructions.
</p></div>
    </summary>
    <updated>2021-03-11T22:38:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.06218</id>
    <link href="http://arxiv.org/abs/2103.06218" rel="alternate" type="text/html"/>
    <title>Bounds on half graph orders in powers of sparse graphs</title>
    <feedworld_mtime>1615420800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Marek Soko≈Çowski <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06218">PDF</a><br/><b>Abstract: </b>Half graphs and their variants, such as ladders, semi-ladders and
co-matchings, are combinatorial objects that encode total orders in graphs.
Works by Adler and Adler (Eur. J. Comb.; 2014) and Fabia\'nski et al. (STACS;
2019) prove that in the powers of sparse graphs, one cannot find arbitrarily
large objects of this kind. However, these proofs either are non-constructive,
or provide only loose upper bounds on the orders of half graphs and
semi-ladders. In this work we provide nearly tight asymptotic lower and upper
bounds on the maximum order of half graphs, parameterized on the distance, in
the following classes of sparse graphs: planar graphs, graphs with bounded
maximum degree, graphs with bounded pathwidth or treewidth, and graphs
excluding a fixed clique as a minor.
</p>
<p>The most significant part of our work is the upper bound for planar graphs.
Here, we employ techniques of structural graph theory to analyze semi-ladders
in planar graphs through the notion of cages, which expose a topological
structure in semi-ladders. As an essential building block of this proof, we
also state and prove a new structural result, yielding a fully polynomial bound
on the neighborhood complexity in the class of planar graphs.
</p></div>
    </summary>
    <updated>2021-03-11T22:38:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.06139</id>
    <link href="http://arxiv.org/abs/2103.06139" rel="alternate" type="text/html"/>
    <title>On the Complexity of the CSG Tree Extraction Problem</title>
    <feedworld_mtime>1615420800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Friedrich:Markus.html">Markus Friedrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fayolle:Pierre=Alain.html">Pierre-Alain Fayolle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06139">PDF</a><br/><b>Abstract: </b>In this short note, we discuss the complexity of the search space for the
problem of finding a CSG expression (or CSG tree) corresponding to an input
point-cloud and a list of fitted solid primitives.
</p></div>
    </summary>
    <updated>2021-03-11T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-03-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.06102</id>
    <link href="http://arxiv.org/abs/2103.06102" rel="alternate" type="text/html"/>
    <title>Effectively Counting s-t Simple Paths in Directed Graphs</title>
    <feedworld_mtime>1615420800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chehreghani:Mostafa_Haghir.html">Mostafa Haghir Chehreghani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06102">PDF</a><br/><b>Abstract: </b>An important tool in analyzing complex social and information networks is s-t
simple path counting, which is known to be #P-complete. In this paper, we study
efficient s-t simple path counting in directed graphs. For a given pair of
vertices s and t in a directed graph, first we propose a pruning technique that
can efficiently and considerably reduce the search space. Then, we discuss how
this technique can be adjusted with exact and approximate algorithms, to
improve their efficiency. In the end, by performing extensive experiments over
several networks from different domains, we show high empirical efficiency of
our proposed technique. Our algorithm is not a competitor of existing methods,
rather, it is a friend that can be used as a fast pre-processing step, before
applying any existing algorithm.
</p></div>
    </summary>
    <updated>2021-03-11T22:38:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.06040</id>
    <link href="http://arxiv.org/abs/2103.06040" rel="alternate" type="text/html"/>
    <title>Covering a Curve with Subtrajectories</title>
    <feedworld_mtime>1615420800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akitaya:Hugo_A=.html">Hugo A. Akitaya</a>, Frederik Br√ºning, Erin Chambers, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Driemel:Anne.html">Anne Driemel</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.06040">PDF</a><br/><b>Abstract: </b>We study subtrajectory clustering under the Fr\'echet distance. Given a
polygonal curve $P$ with $n$ vertices, and parameters $k$ and $\ell$, the goal
is to find $k$ center curves of complexity at most $\ell$ such that every point
on $P$ is covered by a subtrajectory that has small Fr\'echet distance to one
of the $k$ center curves. We suggest a new approach to solving this problem
based on a set cover formulation leading to polynomial-time approximation
algorithms. Our solutions rely on carefully designed set system oracles for
systems of subtrajectories.
</p></div>
    </summary>
    <updated>2021-03-11T22:39:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-03-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.05960</id>
    <link href="http://arxiv.org/abs/2103.05960" rel="alternate" type="text/html"/>
    <title>Delaunay triangulations of generalized Bolza surfaces</title>
    <feedworld_mtime>1615420800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ebbens:Matthijs.html">Matthijs Ebbens</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iordanov:Iordan.html">Iordan Iordanov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Teillaud:Monique.html">Monique Teillaud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vegter:Gert.html">Gert Vegter</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.05960">PDF</a><br/><b>Abstract: </b>The Bolza surface can be seen as the quotient of the hyperbolic plane,
represented by the Poincar\'e disk model, under the action of the group
generated by the hyperbolic isometries identifying opposite sides of a regular
octagon centered at the origin. We consider generalized Bolza surfaces
$\mathbb{M}_g$, where the octagon is replaced by a regular $4g$-gon, leading to
a genus $g$ surface. We propose an extension of Bowyer's algorithm to these
surfaces. In particular, we compute the value of the systole of $\mathbb{M}_g$.
We also propose algorithms computing small sets of points on $\mathbb{M}_g$
that are used to initialize Bowyer's algorithm.
</p></div>
    </summary>
    <updated>2021-03-11T22:40:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-03-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.05952</id>
    <link href="http://arxiv.org/abs/2103.05952" rel="alternate" type="text/html"/>
    <title>Quantum Algorithms in Cybernetics</title>
    <feedworld_mtime>1615420800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nikolov:Petar.html">Petar Nikolov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.05952">PDF</a><br/><b>Abstract: </b>A new method for simulation of a binary homogeneous Markov process using a
quantum computer was proposed. This new method allows using the distinguished
properties of the quantum mechanical systems -- superposition, entanglement and
probability calculations. Implementation of an algorithm based on this method
requires the creation of a new quantum logic gate, which creates entangled
state between two qubits. This is a two-qubit logic gate and it must perform a
predefined rotation over the X-axis for the qubit that acts as a target, where
the rotation accurately represents the transient probabilities for a given
Markov process. This gate fires only when the control qubit is in state &lt;0|. It
is necessary to develop an algorithm, which uses the distribution for the
transient probabilities of the process in a simple and intuitive way and then
transform those into X-axis offsets. The creation of a quantum controlled n-th
root of X gate using only the existing basic quantum logic gates at the
available cloud platforms is possible, although the hardware devices are still
too noisy, which results in a significant measurement error increase. The IBM's
Yorktown 'bow-tie' back-end performs quite better than the 5-qubit T-shaped and
the 14-qubit Melbourne quantum processors in terms of quantum fidelity. The
simulation of the binary homogeneous Markov process on a real quantum processor
gives best results on the Vigo and Yorktown (both 5-qubit) back-ends with
Hellinger fidelity of near 0.82. The choice of the right quantum circuit, based
on the available hardware (topology, size, timing properties), would be the
approach for maximizing the fidelity.
</p></div>
    </summary>
    <updated>2021-03-11T22:37:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.05933</id>
    <link href="http://arxiv.org/abs/2103.05933" rel="alternate" type="text/html"/>
    <title>Pebble Guided Near Optimal Treasure Hunt in Anonymous Graphs</title>
    <feedworld_mtime>1615420800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gorain:Barun.html">Barun Gorain</a>, Kaushik Mondal, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nayak:Himadri.html">Himadri Nayak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pandit:Supantha.html">Supantha Pandit</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.05933">PDF</a><br/><b>Abstract: </b>We study the problem of treasure hunt in a graph by a mobile agent. The nodes
in the graph are anonymous and the edges at any node $v$ of degree $deg(v)$ are
labeled arbitrarily as $0,1,\ldots, deg(v)-1$. A mobile agent, starting from a
node, must find a stationary object, called {\it treasure} that is located on
an unknown node at a distance $D$ from its initial position. The agent finds
the treasure when it reaches the node where the treasure is present. The {\it
time} of treasure hunt is defined as the number of edges the agent visits
before it finds the treasure. The agent does not have any prior knowledge about
the graph or the position of the treasure. An Oracle, that knows the graph, the
initial position of the agent, and the position of the treasure, places some
pebbles on the nodes, at most one per node, of the graph to guide the agent
towards the treasure.
</p>
<p>We target to answer the question: what is the fastest possible treasure hunt
algorithm regardless of the number of pebbles are placed?
</p>
<p>We show an algorithm that uses $O(D \log \Delta)$ pebbles to find the
treasure in a graph $G$ in time $O(D \log \Delta + \log^3 \Delta)$, where
$\Delta$ is the maximum degree of a node in $G$ and $D$ is the distance from
the initial position of the agent to the treasure. We show an almost matching
lower bound of $\Omega(D \log \Delta)$ on time of the treasure hunt using any
number of pebbles.
</p></div>
    </summary>
    <updated>2021-03-11T22:37:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-03-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.05931</id>
    <link href="http://arxiv.org/abs/2103.05931" rel="alternate" type="text/html"/>
    <title>Vertex Fault-Tolerant Spanners for Weighted Points in Polygonal Domains</title>
    <feedworld_mtime>1615420800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inkulu:R=.html">R. Inkulu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:A=.html">A. Singh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.05931">PDF</a><br/><b>Abstract: </b>Given a set $S$ of $n$ points, a weight function $w$ to associate a
non-negative weight to each point in $S$, a positive integer $k \ge 1$, and a
real number $\epsilon &gt; 0$, we devise the following algorithms to compute a
$k$-vertex fault-tolerant spanner network $G(S, E)$ for the metric space
induced by the weighted points in $S$: (1) When the points in $S$ are located
in a simple polygon, we present an algorithm to compute $G$ with multiplicative
stretch $\sqrt{10}+\epsilon$, and the number of edges in $G$ (size of $G$) is
$O(k n (\lg{n})^2)$. (2) When the points in $S$ are located in the free space
of a polygonal domain $\cal P$ with $h$ number of obstacles, we present an
algorithm to compute $G$ with multiplicative stretch $6+\epsilon$ and size
$O(\sqrt{h} k n(\lg{n})^2)$. (3) When the points in $S$ are located on a
polyhedral terrain, we devise an algorithm to compute $G$ with multiplicative
stretch $6+\epsilon$ and size $O(k n (\lg{n})^2)$.
</p></div>
    </summary>
    <updated>2021-03-11T22:39:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-03-11T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5376</id>
    <link href="https://www.scottaaronson.com/blog/?p=5376" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5376#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5376" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Sayonara Majorana?</title>
    <summary xml:lang="en-US">Many of you have surely already seen the news that the Kouwenhoven group in Delft‚Äîwhich in 2018 published a paper in Nature claiming to have detected Majorana fermions, a type of nonabelian anyon‚Äîhave retracted the paper and apologized for ‚Äúinsufficient scientific rigour.‚Äù This work was considered one of the linchpins of Microsoft‚Äôs experimental effort toward [‚Ä¶]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Many of you have surely <a href="https://www.wired.com/story/microsoft-retracts-disputed-quantum-computing-paper/">already</a> <a href="https://www.nature.com/articles/d41586-021-00612-z">seen</a> the news that the Kouwenhoven group in Delft‚Äîwhich in 2018 published a paper in <em>Nature</em> claiming to have detected <a href="https://en.wikipedia.org/wiki/Majorana_fermion">Majorana fermions</a>, a type of nonabelian <a href="https://en.wikipedia.org/wiki/Anyon">anyon</a>‚Äîhave <a href="https://www.nature.com/articles/s41586-021-03373-x?utm_medium=affiliate&amp;utm_source=commission_junction&amp;utm_campaign=3_nsn6445_deeplink_PID100095187&amp;utm_content=deeplink">retracted the paper</a> and apologized for ‚Äúinsufficient scientific rigour.‚Äù  This work was considered one of the linchpins of Microsoft‚Äôs experimental effort toward building topological quantum computers.</p>



<p>Like most quantum computing theorists, I guess, I‚Äôm thrilled if Majorana fermions can be created using existing technology, I‚Äôm sad if they can‚Äôt be, but I don‚Äôt have any special investment in or knowledge of the topic, beyond what I read in the news or hear from colleagues.  Certainly Majorana fermions seem neither necessary nor sufficient for building a scalable quantum computer, although they‚Äôd be a step forward for the topological approach to QC.</p>



<p>The purpose of this post is to invite <em>informed scientific discussion</em> of the relevant issues‚Äîfirst and foremost so that I can learn something, and second so that my readers can!  I‚Äôd be especially interested to understand:</p>



<ol><li>Weren‚Äôt there, like, several <em>other</em> claims to have produced Majorana fermions?  What of those then?</li><li>If, today, no one has convincingly demonstrated the existence of Majoranas, then do people think it more likely that they were produced but not detected, or that they weren‚Äôt even produced?</li><li>How credible are the explanations as to what went wrong?</li><li>Are there any broader implications for the prospects of topological QC, or Microsoft‚Äôs path to topological QC, or was this just an isolated mistake?</li></ol></div>
    </content>
    <updated>2021-03-10T22:29:51Z</updated>
    <published>2021-03-10T22:29:51Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-03-11T20:31:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21347</id>
    <link href="https://gilkalai.wordpress.com/2021/03/10/amazing-feng-pan-and-pan-zhang-announced-a-way-to-spoof-classically-simulate-the-googles-quantum-supremacy-circuit/" rel="alternate" type="text/html"/>
    <title>Amazing: Feng Pan and Pan Zhang Announced a Way to ‚ÄúSpoof‚Äù (Classically Simulate) the Google‚Äôs Quantum Supremacy Circuit!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Feng Pan and Pan Zhang uploaded a new paper on the arXive ¬†‚ÄúSimulating the Sycamore supremacy circuits.‚Äù with an amazing announcement. Abstract: We propose a general tensor network method for simulating quantum circuits. The method is massively more efficient in ‚Ä¶ <a href="https://gilkalai.wordpress.com/2021/03/10/amazing-feng-pan-and-pan-zhang-announced-a-way-to-spoof-classically-simulate-the-googles-quantum-supremacy-circuit/">Continue reading <span class="meta-nav">‚Üí</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Feng Pan and Pan Zhang uploaded a new paper on the arXive ¬†<a href="https://arxiv.org/abs/2103.03074">‚ÄúSimulating the Sycamore supremacy circuits.‚Äù</a> with an amazing announcement.</p>
<blockquote><p><span style="color: #0000ff;"><em><strong> Abstract:</strong> We propose a general tensor network method for simulating quantum circuits. The method is massively more efficient in computing a large number of correlated bitstring amplitudes and probabilities than existing methods. As an application, we study the sampling problem of Google‚Äôs Sycamore circuits, which are believed to be beyond the reach of classical supercomputers and have been used to demonstrate quantum supremacy. Using our method, employing a small computational cluster containing 60 graphical processing units (GPUs), we have generated one million correlated bitstrings with some entries fixed, from the Sycamore circuit with 53 qubits and 20 cycles, with linear cross-entropy benchmark (XEB) fidelity equals 0.739, which is much higher than those in Google‚Äôs quantum supremacy experiments.</em></span></p></blockquote>
<p><strong><span style="color: #ff0000;">Congratulations to Feng Pan and Pan Zhang for this remarkable breakthrough!</span></strong></p>
<p>Of course, we can expect that in the weeks and months to come, the community will learn, carefully check, and digest this surprising result and will ponder about its meaning and interpretation. Stay tuned!</p>
<p><span style="color: #993366;">Here is a technical matter I am puzzled about: the paper claims the ability to compute precisely the amplitudes for a large number of bitstrings. (Apparently computing the amplitudes is even more difficult computational task than sampling.) But then, it is not clear to me where the upper bound of 0.739 comes from? If you have the precise amplitudes it seems that you can sample with close to perfect fidelity. (And, if you wish, you can get a F_XEB score larger than 1.)<br/>
</span></p>
<p><span style="color: #993366;">Update: This is explained just before the discussion part of the paper. The crucial thing is that the probabilities for the 2^21 strings are distributed close to Porter-Thomas (exponentials). If you take samples for them indeed you can get samples with F_XEB between -1 and 15. Picking the highest 10^6¬† strings from 2^21 get you 0.739 (so this value has no special meaning.) Probably by using Metropolis sampling you can get (smaller, unless you enlarge 2^21 to 2^25, say) samples with F_XEB close to 1 and size-biased distribution (the distribution of probabilities of sampled strings) that fits the theoretical size biased distribution.¬† And you can also use metropolis sampling to get a sample of size 10^6 with the correct distribution of probabilities for somewhat smaller fidelity.¬†</span></p>
<p>The paper mentions several earlier papers in this direction, including an earlier result by Johnnie Gray and Stefanos Kourtis in the paper <a href="https://arxiv.org/abs/2002.01935">Hyper-optimized tensor network contraction</a> and another earlier result in the paper <a href="https://arxiv.org/abs/2005.06787">Classical Simulation of Quantum Supremacy Circuits</a> by a group of researchers Cupjin Huang, Fang Zhang, Michael Newman, Junjie Cai, Xun Gao, Zhengxiong Tian, Junyin Wu, Haihong Xu, Huanjun Yu, Bo Yuan, Mario Szegedy, Yaoyun Shi, and Jianxin Chen, from Alibaba co.¬† Congratulations to them as well.</p>
<p>I am thankful to colleagues who told me about this paper.</p>
<h3>Some links:</h3>
<p><a href="https://thequantumdaily.com/2021/03/05/scientists-say-they-used-classical-computers-to-outperform-googles-sycamore-qc/"><span dir="ltr">Scientists Say They Used Classical Approach to outperform Google‚Äôs Sycamore QC</span></a> (‚ÄúThe Quantum‚Äù Written by Matt Swayne with interesting quotes from the paper. )</p>
<p><a href="https://www.scottaaronson.com/blog/?p=5371" rel="bookmark" title="Permanent Link: Another axe swung at the Sycamore">Another axe swung at the Sycamore</a> (Shtetl-Optimized; with interesting preliminary thoughts by Scott;¬† )</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/03/pan-zhang.png"><img alt="" class="alignnone size-full wp-image-21373" height="396" src="https://gilkalai.files.wordpress.com/2021/03/pan-zhang.png?w=640&amp;h=396" width="640"/></a></p></div>
    </content>
    <updated>2021-03-10T13:07:14Z</updated>
    <published>2021-03-10T13:07:14Z</published>
    <category term="Computer Science and Optimization"/>
    <category term="Physics"/>
    <category term="Quantum"/>
    <category term="Feng Pan"/>
    <category term="Pan Zhang"/>
    <category term="Quantum computation"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-03-12T10:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18282</id>
    <link href="https://rjlipton.wordpress.com/2021/03/10/making-algorithms-fair/" rel="alternate" type="text/html"/>
    <title>Making Algorithms Fair</title>
    <summary>We don‚Äôt need to be good. But let‚Äôs try to be fair. ‚ÄîHolly Black FairVis source/personal website Jamie Morgenstern is an assistant professor in Computer Science and Engineering at the University of Washington. Previously she was at the School of Computer Science at Georgia Tech‚Äîa place where I, Dick, spent some time too. We were [‚Ä¶]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>We don‚Äôt need to be good. But let‚Äôs try to be fair. ‚ÄîHolly Black</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/03/jamie-morgenstern.jpg"><img alt="" class="alignright wp-image-18297" height="150" src="https://rjlipton.files.wordpress.com/2021/03/jamie-morgenstern.jpg?w=150&amp;h=150" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">FairVis <a href="https://fredhohman.com/papers/fairvis">source</a>/personal <a href="http://jamiemorgenstern.com">website</a> </font></td>
</tr>
</tbody>
</table>
<p>
Jamie Morgenstern is an assistant professor in Computer Science and Engineering at the University of Washington. Previously she was at the School of Computer Science at Georgia Tech‚Äîa place where I, Dick, spent some time too. We were sad when Jamie left to go west.</p>
<p>
Today we thought we would talk about making algorithms fair.  </p>
<p>
We will focus on an aspect that first seems like something you would never think of.  Then, when you think of it, getting fairness seems impossible.  So when I thought of it, Ken and I thought maybe no one else had, and Ken came up only with one maybe-relevant paper that used <i>blockchain</i>.  Then I found that some people have thought of it‚Äîincluding Jamie.</p>
<p>
You can kind-of just make it out in the ‚Äúelevator pitch‚Äù description of her work at the very top of her personal <a href="https://jamiemorgenstern.com">website</a>:</p>
<blockquote><p><b> </b> <em> How should machine learning be made robust to behavior of the people generating training or test data for it? How should ensure that the models we design do not exacerbate inequalities already present in society? </em>
</p></blockquote>
<p>
It‚Äôs in the word ‚Äúgenerating.‚Äù  Often we generate or select test data <i>randomly</i>.  Or at least we say we do.  How can we assure that our use of randomness is <i>fair</i>?  Is that even a question?  After all, randomness doesn‚Äôt intend bias.  The epitome of randomness is something we call ‚Äúflipping a <i>fair</i> coin‚Äù to begin with.</p>
<p>
</p><h2> Never Thought of It </h2><p/>
<p>
I believe that one of the issues that delayed the rise of fairness as a property of algorithms must be that it is not an obvious property. </p>
<p>
First, what makes a Turing machine <i>efficient?</i> Complexity theory is based on time and space which was first defined for Turing machines. This started with the famous 1965  <a href="https://en.wikipedia.org/wiki/Juris_Hartmanis">paper</a>, ‚ÄúOn the Computational Complexity of Algorithms‚Äù by Juris Hartmanis and Richard Stearns. The notion of <i>time</i> is clear‚Äîjust count each step of the Turing machine. <i>Space</i> is more complicated‚Äîjust counting squares of the tapes is too simple, one must allow the input tape to be different from the work tape.  A workable definition is counting the tape cells that are ever <i>changed</i>.</p>
<p>
Next, what makes a Turing machine <i>fair?</i> It seems impossible to imagine a universal definition like the above paper‚Äîfairness requires domain-specific information. There is no definition that looks just at the structure of a Turing machine.</p>
<p>
So we went past the turn of the millennium without considering fairness, and missed a concept that has been around for millennia more.  As stated in last June‚Äôs <a href="https://www.simonsfoundation.org/2020/06/18/foundation-announces-simons-collaboration-on-the-theory-of-algorithmic-fairness/">announcement</a> of a Simons collaboration directed by Omer Reingold:</p>
<blockquote><p><b> </b> <em> The study of fairness is ancient and multidisciplinary: philosophers, legal experts, economists, statisticians, social scientists and others have been concerned with fairness for as long as these fields have existed. Nevertheless, the scale of decision-making in the age of big data, the computational complexities of algorithmic decision-making and simple professional responsibility mandate that computer scientists contribute to this research endeavor. </em></p></blockquote>
<p>
Indeed we could quote many others on why fairness is important. </p>
<p>
One of the embarrassments is that computer scientists did not study fairness earlier. I must admit I never did too. I was concerned about being ‚Äúgood‚Äù: about making algorithms faster and making them use less space. But not being fair.</p>
<p>
</p><h2> In Search of Fairness </h2><p/>
<p>
Okay, fairness requires domain-specific information. But are there some cases that could be defined in a way that avoids specialized knowledge? A more general definition would allow results to have more applications. And one domain that should naturally be more generic is the use of randomness.</p>
<p>
The <a href="https://arxiv.org/pdf/1906.03284.pdf">paper</a> that caught my eye is titled, ‚ÄúEqualized odds post processing under imperfect group information,‚Äù by Morgenstern with Pranjal Awasthi and Matthaus Kleindessner. They say: </p>
<blockquote><p><b> </b> <em> Most approaches aiming to ensure a model‚Äôs fairness with respect to a protected attribute (such as gender or race) assume to know the true value of the attribute for every data point. In this paper, we ask to what extent fairness interventions can be effective even when only imperfect information about the protected attribute is available. </em>
</p></blockquote>
<p>
At the high level the result is about how the ability to achieveness depends on the amount of information one has about values of the attribute <img alt="{A};" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D%3B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that one is trying to protect against bias.  The result states that even having imperfect information yields nontrivial improvements in fairness. But at the low level are techniques for equalizing conditional probabilities on values of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and a long technical appendix employing random variables.</p>
<p>
Another area that yields a reasonably generic definition of fairness is algorithms for games of chance. The insight is: These all invoke randomness and the algorithms must not cheat. That is the generation and use of randomness must be fair‚Äîno cheating. 	</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<p>
<a href="https://rjlipton.files.wordpress.com/2021/03/blockchainbullets.jpg"><img alt="" class="aligncenter wp-image-18298" height="290" src="https://rjlipton.files.wordpress.com/2021/03/blockchainbullets.jpg?w=550&amp;h=290" width="550"/></a>
</p></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from CasinosBlockchain.io <a href="https://casinosblockchain.io/provably-fair-gambling/">source</a></font>
</td>
</tr>
</tbody></table>
<p>
There are two different types of approaches to making the randomness fair. One uses blockchain technology, as presented in the <a href="https://casinosblockchain.io/provably-fair-gambling/">article</a> accompanying the above picture.</p>
<blockquote><p><b> </b> <em> In a contest system on hybrid blockchain Dragonchain, a future hash of Bitcoin and Ethereum that has not been created yet, combined with an algorithm anyone can execute themselves, a provably fair random selection and revealing occurs, which is near impossible to profitably manipulate, as it is backed by hundreds of millions of dollars worth of proof from decentralized blockchains that Interchain with Dragonchain. </em>
</p></blockquote>
<p>
Another uses a more standard way to make the randomness secure. Here is one such <a href="https://en.wikipedia.org/wiki/Provably_fair_algorithm#Benefits_And_Drawbacks_of_Provable_Fairness">paper</a>.</p>
<blockquote><p><b> </b> <em> In a provably fair gambling system, a player places bets on games offered by the service operator. The service operator will publish a method for verifying each transaction in the game. This is usually done by using open source algorithms for random seed generation, hashing, and for the random number generator.</em></p><em>
<p>
Once a game has been played, the player can use these algorithms to test the game‚Äôs response to their in-game decisions, and evaluate the outcome by only using the published algorithms, the seeds, hashes, and the events which transpired during the game.</p>
</em><p><em>
In a simplified way, players can always check that the outcome of every game round is fair and wasn‚Äôt tampered with, and so can the game‚Äôs operators. As such, cheating is arguably impossible. </em>
</p></blockquote>
<p>
</p><h2> In Search of Correctness </h2><p/>
<p>
Let‚Äôs look at fairness from a complexity point of view. We are interested in seeing if NP gives us some insight to how to define fairness, at lest for games.</p>
<p>
Consider an algorithm <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that given a graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> must output a coloring of the vertices with <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> colors. The algorithm outputs a map from <img alt="{V(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to <img alt="{\{1,2,3\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C2%2C3%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: from vertices of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to the colors <img alt="{1,2,3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2C2%2C3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The algorithm may output a legal coloring or not. We are left with two choices: </p>
<ol>
<li>Check for each edge <img alt="{(a, b)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28a%2C+b%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> have different colors.
</li><li>Prove that the algorithm <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is always correct.
</li></ol>
<p>
We argue that there is a similar situation with an algorithm <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that claims that it is fair. The algorithm leaves us with one choice: Prove that the algorithm <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> always satisfies our definition of fair. Recall the famous <a href="https://en.wikipedia.org/wiki/Trust,_but_verify">saying</a>: </p>
<blockquote><p><b> </b> <em> Trust, but verify. </em>
</p></blockquote>
<p>
In the above we can either trust the algorithm <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or it can check that its output is a three coloring. What we want to know is can we also avoid trusting that the algorithm <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is fair? We believe that it is possible in some situations to have the fairness of an algorithm be checkable. That is we will not have to prove the algorithm is fair, but can look at the output of the algorithm and conclude that it is fair.</p>
<p>
Let‚Äôs look at this next.</p>
<p>
</p><h2> A Fairness Problem </h2><p/>
<p>
The question we consider is this: Imagine that <img alt="{Total}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BTotal%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> people apply to our department of Computer Science for the PhD program. We have some reasonable criterion that will select the eligible applicants and yields <img alt="{{\mathbb N} \le Total}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+N%7D+%5Cle+Total%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Among the <img alt="{{\mathbb N}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> we must choose randomly say <img alt="{Accept}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAccept%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>‚Äîno other criterion can be used. How can we be sure that this actually is what happens?</p>
<p>
One possible solution is to have an algorithm that operates like this: </p>
<ol>
<li>Let us input the <img alt="{Total}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BTotal%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> people with their properties.
</li><li>Then use the acceptance criterion to reduce this to a set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> people that meet all the criteria.
</li><li>Finally randomly select <img alt="{Accept}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAccept%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from the set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.
</li></ol>
<p>
Note: Any choice of <img alt="{Accept}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAccept%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from the <img alt="{{\mathbb N}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> could be fair. But what if it was based on some decision that was biased? What if we select further from the <img alt="{{\mathbb N}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> based on race or gender or some other criterion? This is the problem. The algorithm could claim that it is fair, but it could cheat. If the criteria are complex the code that checks them could be messy. We could hide the fact that we then ‚Äúrandomly‚Äù selected as required. </p>
<p>
</p><h2> A Solution </h2><p/>
<p>
Here is a solution that uses a protocol between the selection algorithm and <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> other players. </p>
<ol>
<li>It is a protocol that uses several players <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.
</li><li>Only by having all <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> cheat can the protocol be defeated.
</li><li>It is public and can be checked after the fact.
</li><li>Its correctness assumes standard crypto methods are safe.
</li></ol>
<p>
The protocol assume that <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> object must be selected from <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> total objects. Note the general case follows the same method. For each <img alt="{j=1,\dots,P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%3D1%2C%5Cdots%2CP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> objects a bit commitment <img alt="{B_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is sent to the <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> player. This encodes either a <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or a <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as usual. The <img alt="{j^{th}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%5E%7Bth%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> player then randomly flips a coin and sends back <img alt="{r_j \in \{0,1\}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_j+%5Cin+%5C%7B0%2C1%5C%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Let <img alt="{s_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be equal to <img alt="{r_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if the bit committed <img alt="{B_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{1-r_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-r_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> otherwise. Then compute 	</p>
<p align="center"><img alt="\displaystyle  s_1 + \dots + s_P \bmod 2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++s_1+%2B+%5Cdots+%2B+s_P+%5Cbmod+2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>If it is <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then pick object <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> else pick object <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
The claim is that unless the bit method is unsafe or all the players are cheating this is correct.</p>
<p>
</p><h2> Open Problems </h2><p/>
<p>
Does making coin flips fair solve any real problem? Does it help make some algorithms easier to show that they are fair?</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/03/10/making-algorithms-fair/pd/" rel="attachment wp-att-18287"><img alt="" class="alignright size-full wp-image-18287" height="405" src="https://rjlipton.files.wordpress.com/2021/03/pd.jpg?w=600&amp;h=405" width="600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table></font></font></div>
    </content>
    <updated>2021-03-10T12:38:09Z</updated>
    <published>2021-03-10T12:38:09Z</published>
    <category term="Ideas"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="domain specific"/>
    <category term="fair"/>
    <category term="fairness"/>
    <category term="games"/>
    <category term="random"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>G√∂del‚Äôs Lost Letter and P=NP</title>
      <updated>2021-03-12T10:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=86</id>
    <link href="https://dstheory.wordpress.com/2021/03/09/thursday-march-18-tim-roughgarden-from-columbia-university/" rel="alternate" type="text/html"/>
    <title>Thursday March 18 ‚Äî Tim Roughgarden  from Columbia University</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Thursday, March 18th at 11:00 AM Pacific Time (14:00 Eastern Time, 20:00 Central European Time, 19:00 UTC).¬†¬†Tim Roughgarden from Columbia Univeristy will speak about ‚ÄúData-Driven Algorithm Design.‚Äù Please register here to join the virtual talk. Abstract: The best algorithm for a computational problem<a class="more-link" href="https://dstheory.wordpress.com/2021/03/09/thursday-march-18-tim-roughgarden-from-columbia-university/">Continue reading <span class="screen-reader-text">"Thursday March 18 ‚Äî Tim Roughgarden  from Columbia¬†University"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next <a href="https://sites.google.com/view/dstheory/home" rel="noreferrer noopener" target="_blank">Foundations of Data Science</a> virtual talk will take place on <strong>Thursday, March 18</strong>th at <strong>11:00 AM Pacific Time</strong> (14:00 Eastern Time, 20:00 Central European Time, 19:00 UTC).¬†¬†<strong>Tim Roughgarden</strong> from <strong>Columbia Univeristy</strong> will speak about ‚Äú<strong>Data-Driven Algorithm Design</strong>.‚Äù</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: The best algorithm for a computational problem generally depends on the ‚Äúrelevant inputs‚Äù, a concept that depends on the application domain and often defies formal articulation. While there is a large literature on empirical approaches to selecting the best algorithm for a given application domain, there has been surprisingly little theoretical analysis of the problem.</p>



<p class="has-text-align-justify">We adapt concepts from statistical and online learning theory to reason about application-specific algorithm selection. Our models are straightforward to understand, but also expressive enough to capture several existing approaches in the theoretical computer science and AI communities, ranging from self-improving algorithms to empirical performance models. We present one framework that models algorithm selection as a statistical learning problem, and our work here shows that dimension notions from statistical learning theory, historically used to measure the complexity of classes of binary- and real-valued functions, are relevant in a much broader algorithmic context. We also study the online version of the algorithm selection problem, and give possibility and impossibility results for the existence of no-regret learning algorithms.</p>



<p>Joint work with Rishi Gupta.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2021-03-09T20:00:23Z</updated>
    <published>2021-03-09T20:00:23Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science ‚Äì Virtual Talk Series</title>
      <updated>2021-03-12T10:23:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1823</id>
    <link href="https://theorydish.blog/2021/03/09/automated-design-of-error-correcting-codes-part-2/" rel="alternate" type="text/html"/>
    <title>Automated Design of Error-Correcting Codes, Part 2</title>
    <summary>In our previous post, we discussed the automation of error correcting codes and how formal methods are quite helpful toward this goal. In this post, we will discuss machine learning techniques as well as possible directions for future research. Machine Learning. The use of machine-learning techniques to study ECCs has increased exponentially in recent years. The main strength of machine learning methods is that they can adapt well to a variety of conditions (c.f., this paper), unlike the codes produced by formal methods which are often designed for a rigid application. On the other hand, it is much more difficult to give any formal guarantees for error-correcting codes designed with machine learning, although such an obstacle may be overcome in the future (see the conclusion).¬† Error-correcting output codes. One interface between machine learning and error correcting codes which has been studied for decades is the construction of error correcting codes for multiclass learning. Multiclass learning is classifying a group of objects into a number of categories. Instead of trying to distinguish the categories all up front, one can instead try to do a binary classification of some subset of the categories from another subset of the categories.‚Äù If an error [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In our <a href="https://theorydish.blog/2021/03/02/automated-design-of-error-correcting-codes-part-1/">previous post</a>, we discussed the automation of error correcting codes and how formal methods are quite helpful toward this goal. In this post, we will discuss machine learning techniques as well as possible directions for future research.</p>



<p><strong>Machine Learning. </strong>The use of machine-learning techniques to study ECCs has increased exponentially in recent years. The main strength of machine learning methods is that they can adapt well to a variety of conditions (c.f., <a href="https://arxiv.org/pdf/1911.03038.pdf">this paper</a>), unlike the codes produced by formal methods which are often designed for a rigid application. On the other hand, it is much more difficult to give any formal guarantees for error-correcting codes designed with machine learning, although such an obstacle may be overcome in the future (see the conclusion).¬†</p>



<p><strong>Error-correcting output codes. </strong>One interface between machine learning and error correcting codes which has been studied for decades is the construction of error correcting codes for multiclass learning. Multiclass learning is classifying a group of objects into a number of categories. Instead of trying to distinguish the categories all up front, one can instead try to do a binary classification of some subset of the categories from another subset of the categories.‚Äù If an error correcting code (called an error-correcting output code‚ÄìECOC) is used for the distinguishing, it can provide a more robust classification.</p>



<p>This technique was studied theoretically for a preset error-correcting code (e.g., <a href="https://dl.acm.org/doi/pdf/10.1145/307400.307429">[Guruswami, Sahai, 1999]</a>). There are also works which synthesize an ECOC for multiclass learning. For example, the work of <a href="https://ieeexplore.ieee.org/abstract/document/1624364">Pujol, Radeva, and Vitria [2006]</a> used a heuristic which tries to find tests which reveal the highest amount of information (subject to a prior that each class is a <a href="https://en.wikipedia.org/wiki/Mixture_of_gaussians#Gaussian_mixture_model">mixture of gaussians</a>).</p>



<p>A related recent work <a href="https://arxiv.org/pdf/1808.01942.pdf">[Xiang, Wang, Kitani, 2018]</a> uses the technique of <em>deep hashing</em> to hash images so that images of a similar class have hashes which are close in Hamming distance, while images of different classes have hashes which are far in edit distance.</p>



<p><strong>Deep Learning. </strong>The bulk of recent machine learning research on the automation of ECCs has been using <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> techniques; that is, training a constructed deep learning network to either encode‚Äìgiven a message as input, output a robust encoding‚Äìor decode‚Äìgiven a noisy transmission, recover the original message. As deep neural networks pass messages from earlier neurons to deeper neurons, the commonly referred to benchmark is that of <em>belief propagation</em>.</p>



<p>In brief (see also <a href="https://arxiv.org/pdf/1607.04793.pdf">Nachmani, et. al, 2016</a>), the <a href="https://en.wikipedia.org/wiki/Belief_propagation">belief propagation</a> (BP) algorithm starts by taking the parity-check matrix defining a given linear code and converts it into a bipartite graph‚Äìone side of the vertices are the symbols of the code and the other vertices are the parity checks. Each vertex starts with a prior (i.e., probability that the vertex should be a 0 or 1) based on the received transmission, and then messages are passed back and forth. In odd-numbered rounds, the symbols give their confidence levels to the parity checks, which then update probabilities based on being satisfied or not. In even-numbered rounds the parity checks inform the symbols if they should change their prior to better satisfy the parity checks. After some number of rounds, a decoding is deduced.</p>



<p>Some of the earlier papers using deep learning to automate ECCs [<a href="https://arxiv.org/pdf/1607.04793.pdf">Nachmani, et. al, 2016</a>, <a href="https://arxiv.org/pdf/1706.07043.pdf">2018</a>; <a href="https://arxiv.org/pdf/1702.06901.pdf">Crammerer, et.al., 2017</a>; <a href="https://arxiv.org/pdf/1701.07738.pdf">Gruber, et.al., 2017</a>] tried to generalize a belief propagation algorithm for decoding ECCs. At a high level, the works of Nachmani, et.al. ‚Äúunroll‚Äù the belief propagation into a deep neural network with many layers. Unlike BP which has a fixed weight for the value of each message in each stage, they have trainable weights which allow one to obtain a lower BER than plain BP. Both Nachmani, et.al., and the works of Crammerer, et.al., and Gruber, et.al., consider more complex neural architectures which use BP as a subroutine. The latter two works show success in these methods for <a href="https://en.wikipedia.org/wiki/Polar_code_(coding_theory)">polar codes</a> and random linear codes. Note that all of these works are only training a decoder, as the encodings are fixed.</p>



<p>A later group of papers [<a href="https://arxiv.org/pdf/1807.00801.pdf">Kim, et.al. 2020</a>; <a href="https://arxiv.org/pdf/1903.02295.pdf">Jiang, et.al., 2019a</a>, <a href="https://arxiv.org/pdf/1911.03038.pdf">2019b</a>, <a href="https://ieeexplore.ieee.org/abstract/document/9053254">2020</a>] (see also <a href="https://deepcomm.github.io/">their blog</a>) introduce what is known as DeepCode. Unlike previous work, the initial DeepCode paper [Kim, et.al., 2020] builds both a custom encoder and decoder. A particular tool which the authors utilized is that of <em>feedback</em>‚Äìafter each bit is transmitted the noisy received bit is sent back (possibly with more noise added). Intuitively, feedback allows for the encoder to have an approximate understanding of what the decoder doesn‚Äôt know, allowing for on-the-fly adjustments to the encoding. The encoder sends the original message in plain-text and then incorporates the feedback with <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN</a> units to add two redundant bits per message bit (and thus the rate is one-third). The decoder incorporates the noisy plain text and these redundant bits using bidirectional-<a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">GRU</a>s. Their methods can get a BER of as low as <img alt="10^{-7}" class="latex" src="https://s0.wp.com/latex.php?latex=10%5E%7B-7%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for a block length of 50 bits.</p>



<p>Follow-up work [Jiang, et.al., 2019a, 2019b, 2020] consider variants of the model. In [Jiang, et.al., 2019a], the encoder is fixed to be a <a href="https://en.wikipedia.org/wiki/Turbo_code">turbo code</a> (a type of <a href="https://en.wikipedia.org/wiki/Convolutional_code">convolutional code</a>), a ‚Äústandard‚Äù code in many applications, but the goal is to train a deep net to beat the <a href="https://en.wikipedia.org/wiki/BCJR_algorithm">standard decoder</a> in a ‚Äúnon-Gaussian noise‚Äù model. The work [Jiang, et.al., 2019b] tries to beat the turbo code at its own game by constructing a turbo code-like deep net where a commonly used finite automata is replaced by a deep net (and a corresponding decoder is constructed as well). Finally, the work [Jiang, et.al, 2020] generalized [Jiang, et.al., 2019b] by also using feedback, like in DeepCode.</p>



<p>Outside of deep learning, <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> techniques have also been used to construct a decoder (e.g., <a href="https://arxiv.org/pdf/2009.09277.pdf">[Liao, et.al., 2020]</a>).</p>



<p><strong>Conclusion and Future Directions</strong>. Although the technology is still in its infancy, the automation of error correction codes is an exciting domain which could have a variety of applications in the future. As we have seen in this post, both formal methods and machine learning allow for powerful methods for obtaining automating various aspects of ECCs, whether it be constructing error correcting codes with provable guarantees or designing non-standard decodes which are robust to a variety of conditions. Looking ahead, there are a number of exciting directions which could be ripe for theoretical and practical contributions.</p>



<ol><li>A very active field of research currently is DNA storage‚Äìthat is, synthesizing DNA molecules which encode data (rather than biological life). The primary theoretical challenge of DNA storage is that data loss is no longer a ‚Äúbit flip,‚Äù rather nucleotides can be inserted, deleted, replicated, etc. Very recently, convolutional codes have proved successful in this setting <a href="https://www.biorxiv.org/content/10.1101/2019.12.20.871939v2.full">[Chandak, et.al., 2020]</a>, but an analogue to DeepCode does not currently exist in this setting. One barrier is that it is difficult for a fixed-architecture neural network to easily accommodate ‚Äúindex shifting‚Äù taking place during insertions and deletions. Automated techniques have been used to give lower bounds on how much redundancy is needed for such codes <a href="https://publish.illinois.edu/kiyavash/files/2015/06/Kulkarni_it_trans_2013.pdf">[Kulkarni, Kiyavash, 2013]</a>.</li><li>One potential theoretical contribution in this space is understanding how these automation techniques relate to <a href="https://en.wikipedia.org/wiki/Augmented_learning"><em>augmented learning</em></a>. An example of augmented learning is the problem of picking the ‚Äúbest‚Äù algorithm from a class of algorithms for a given problem. A theoretical framework for understanding such questions has recently been developed by <a href="https://theory.stanford.edu/~tim/papers/features.pdf">Gupta and Roughgarden</a>. Such a framework seems natural in the context of error-correcting codes, as there are many different kinds of codes, and even within one family of codes, such as Reed-Solomon codes, there are many parameter choices whose optimal values can vary significantly from application to application.¬†</li><li>As a final thought, the use of ‚ÄúFormal Methods‚Äù in contrast to ‚ÄúMachine Learning‚Äù for automating ECCs has been largely separate. Recently, outside the context of ECCs, there have been a number of works (e.g., <a href="https://arxiv.org/pdf/1705.01320.pdf">[Ehlers, 2017]</a>, <a href="https://arxiv.org/abs/1811.01057">[Raghunathan, Steinhardt, Liang, 2018]</a>) which use formal methods to <em>provably verify</em> that a machine learning model such as a deepnet runs correctly under given conditions. It would be exciting if such methods could be extended to ECCs by showing that machine learning encoders/decoders like DeepCode can be utilized correctly.¬†</li></ol>



<p>Are there other directions for which you would like to see the automation of ECCs extended? If so, please leave a comment.</p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Aviad Rubinstein, Moses Charikar, and Mary Wootters for valuable feedback. I would also like to thank Sivakanth Gopi and Sergey Yekhanin for insightful discussion on the relationship between automation of ECCs and DNA storage as well as Ofir Geri for discussion on augmented learning.</p></div>
    </content>
    <updated>2021-03-09T16:00:00Z</updated>
    <published>2021-03-09T16:00:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Joshua Brakensiek</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-03-12T10:23:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2021-03-09-good-case-latency-of-byzantine-broadcast-the-synchronous-case/</id>
    <link href="https://decentralizedthoughts.github.io/2021-03-09-good-case-latency-of-byzantine-broadcast-the-synchronous-case/" rel="alternate" type="text/html"/>
    <title>Good-case Latency of Byzantine Broadcast: the Synchronous Case</title>
    <summary>In our first post, we presented a summary of our good-case latency results for Byzantine broadcast (BB) and state machine replication (SMR), where the good case measures the latency to commit given that the broadcaster or leader is honest. In our second post, we discussed our results for partial synchrony,...</summary>
    <updated>2021-03-09T16:00:00Z</updated>
    <published>2021-03-09T16:00:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2021-03-11T22:43:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/034</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/034" rel="alternate" type="text/html"/>
    <title>TR21-034 |  Robust Self-Ordering versus Local Self-Ordering | 

	Oded Goldreich</title>
    <summary>We study two notions that refers to asymmetric graphs, which we view as graphs having a unique ordering that can be reconstructed by looking at an unlabeled version of the graph.

A {\em local self-ordering} procedure for a graph $G$ is given oracle access to an arbitrary isomorphic copy of $G$, denoted $G'$, and a vertex $v$ in $G'$, and is required to identify the name (or location) of $v$ in $G$, while making few (i.e., polylogarithmically many) queries to $G'$.
A graph $G=(V,E)$ is {\em robustly self-ordered} if the size of the symmetric difference between $E$ and the edge-set of the graph obtained by permuting $V$ using any permutation $\pi:V\to V$ is proportional to the number of non-fixed-points of $\pi$ and to the maximal degree of $G$; that is, any permutation of the vertices that displaces $t$ vertices must ``displace'' $\Omega(t\cdot d)$ edges, where $d$ is the maximal degree of the graph. 

We consider the relation between these two notions in two regimes: The bounded-degree graph regime, where oracle access to a graph means oracle access to its incidence function, and the dense graph regime, where oracle access to the graph means access to its adjacency predicate. 

We show that, {\em in the bounded-degree regime}, robustly self-ordering and local self-ordering are almost orthogonal; that is, even extremely strong versions of one notion do not imply very weak versions of the other notion. 
Specifically, we present very efficient local self-ordering procedures for graphs that possess derangements that are almost automorphisms (i.e., a single incidence is violated).  
One the other hand, we show robustly self-ordered graphs having no local self-ordering procedures even when allowing a number of queries that is a square root of the graph's size. 

{\em In the dense graph regime}, local self-ordering procedures are shown to yield a quantitatively weaker version of the robust self-ordering condition, in which the said proportion is off by a factor that is related to the query complexity of the local self-ordering procedure. Furthermore, we show that this quantitatively loss is inherent.
On the other hand, we show how to transform any robustly self-ordered graph 
into one having a local self-ordering procedure, while preserving the robustness condition. Combined with prior work, this yields explicit constructions of graphs that are both robustly and locally self-ordered, and an application to property testing.</summary>
    <updated>2021-03-09T09:13:43Z</updated>
    <published>2021-03-09T09:13:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-12T10:20:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/03/08/more-mathematics-books</id>
    <link href="https://11011110.github.io/blog/2021/03/08/more-mathematics-books.html" rel="alternate" type="text/html"/>
    <title>More mathematics books by women</title>
    <summary>A year ago, for International Women‚Äôs Day, I made a list of mathematics books by women covered by then-new Wikipedia articles. I thought it would be worthwhile to revisit the same topic and list several more mathematics books with at least one female author, at many different levels of audience, and again covered by new Wikipedia articles. They are (alphabetical by title):</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A year ago, for International Women‚Äôs Day, I made <a href="https://11011110.github.io/blog/2020/03/08/mathematics-books-women.html">a list of mathematics books by women covered by then-new Wikipedia articles</a>. I thought it would be worthwhile to revisit the same topic and list several more mathematics books with at least one female author, at many different levels of audience, and again covered by new Wikipedia articles. They are (alphabetical by title):</p>

<ul>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Algorithmic_Combinatorics_on_Partial_Words">Algorithmic Combinatorics on Partial Words</a></em> (2008), Francine Blanchet-Sadri. Partial words are strings with ‚Äúdon‚Äôt care‚Äù symbols; Blanchet-Sadri looks at the combinatorics of repeated patterns within these strings.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Algorithmic_Geometry">Algorithmic Geometry</a></em> (1995), Jean-Daniel Boissonnat and Mariette Yvinec. One of several standard computational geometry textbooks; this is the French one, but it has also been published in translation into English.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Algorithmic_Puzzles">Algorithmic Puzzles</a></em> (2011), Anany and Maria Levitin. A nice collection of classic logic puzzles involving algorithmic thinking.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Braids,_Links,_and_Mapping_Class_Groups">Braids, Links, and Mapping Class Groups</a></em> (1975), Joan Birman. A classic research monograph on the topology of braid groups.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Code_of_the_Quipu">Code of the Quipu</a></em> (1981), Marcia and Robert Ascher. A general-audience book on how the Inca used knotted strings to record numbers and other information.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Combinatorics:_The_Rota_Way">Combinatorics: The Rota Way</a></em> (2009), Joseph P. S. Kung, Catherine Yan, and (posthumously) Gian-Carlo Rota. A graduate textbook on algebraic combinatorics.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Combinatorics_of_Experimental_Design">Combinatorics of Experimental Design</a></em> (1987), Anne Penfold Street and her daughter Deborah Street. A textbook on the design of experiments, an area that crosses between statistics and combinatorics.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Computability_in_Analysis_and_Physics">Computability in Analysis and Physics</a></em> (1989), Marian Pour-El and J. Ian Richards. A research monograph on problems involving differential equations including the wave equation whose initial conditions are continuous and computable, but that evolve to states whose values cannot be computed.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Diophantus_and_Diophantine_Equations">Diophantus and Diophantine Equations</a></em> (1972), Isabella Bashmakova. A somewhat idiosyncratic history based on the idea that Diophantus knew some very general techniques for finding rational-number solutions to equations, that can be inferred from the much more specific solutions to individual equations that have survived to us.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Elementary_Number_Theory,_Group_Theory_and_Ramanujan_Graphs">Elementary Number Theory, Group Theory, and Ramanujan Graphs</a></em> (2003), Giuliana Davidoff, Peter Sarnak, and Alain Valette. An attempt to make the construction of expander graphs accessible to undergraduate mathematics students.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Equivalents_of_the_Axiom_of_Choice">Equivalents of the Axiom of Choice</a></em> (1963, updated 1985), Herman and Jean Rubin. A large catalog of problems in mathematics whose solution is equivalent to the axiom of choice, from a time when the independence of choice from ZF set theory had not been proven.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Erd%C5%91s_on_Graphs">Erd≈ës on Graphs: His Legacy of Unsolved Problems</a></em> (1998), 
Fan Chung and Ronald Graham. The open problems in graph theory from this book have been further collected and updated on a web site, <a href="http://www.math.ucsd.edu/~erdosproblems/">Erd≈ës‚Äôs Problems on Graphs</a>, maintained by Chung.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Extensions_of_First_Order_Logic">Extensions of First Order Logic</a></em> (1996), Mar√≠a Manzano. Attempts to unify second-order logic, modal logic, and dynamic logic, by translating them all into many-sorted logic.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Fat_Chance:_Probability_from_0_to_1">Fat Chance: Probability from 0 to 1</a></em> (2019), Benedict Gross, Joe Harris, and Emily Riehl. A general-audience undergraduate textbook on probability theory based on a metaphor of games of chance.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_Fractal_Dimension_of_Architecture">The Fractal Dimension of Architecture</a></em> (2016), Michael J. Ostwald and Josephine Vaughan. Studies the fractal dimension of floor plans as a way to model the changing demands on the complexity of housing structures and to classify buildings by architect and style.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_Geometry_of_Numbers">The Geometry of Numbers</a></em> (2000), Carl D. Olds, Anneli Cahn Lax, and Giuliana Davidoff. A textbook on connections between number theory and integer grids, rescued twice from the posthumous works of its first two coauthors.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_History_of_Mathematical_Tables">The History of Mathematical Tables: from Sumer to Spreadsheets</a></em> (2003), Martin Campbell-Kelly, Mary Croarken, Raymond Flood, and Eleanor Robson. An edited volume with chapters on tables from many different periods in mathematical history.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Incidence_and_Symmetry_in_Design_and_Architecture">Incidence and Symmetry in Design and Architecture</a></em> (1983), Jenny Baglivo and Jack E. Graver. A textbook on graph theory and symmetry aimed at architecture students, also including interesting material on structural rigidity.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Introduction_to_the_Theory_of_Error-Correcting_Codes">Introduction to the Theory of Error-Correcting Codes</a></em> (1982, updated 1989 and 1998), Vera Pless. An advanced undergraduate textbook centered on algebraic constructions of linear block codes.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Introduction_to_3-Manifolds">Introduction to 3-Manifolds</a></em> (2014), Jennifer Schultens. An introductory graduate textbook on low-dimensional topology, leading up to the use of normal surfaces and Heegard splittings.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Journey_into_Geometries">Journey into Geometries</a></em> (1991), M√°rta Sv√©d. A conversational Alice-in-wonderland-inspired tour of non-Euclidean geometry.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Knots_Unravelled">Knots Unravelled: From String to Mathematics</a></em> (2011), Meike Akveld and Andrew Jobbings. Knot theory for schoolchildren, centered on knot invariants.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Lectures_in_Geometric_Combinatorics">Lectures in Geometric Combinatorics</a></em> (2006), Rekha R. Thomas. An advanced undergraduate or introductory graduate textbook on the combinatorics of convex polytopes and their connections to abstract algebra through secondary polytopes and toric varieties.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Making_Mathematics_with_Needlework">Making Mathematics with Needlework: Ten Papers and Ten Projects</a></em> (2008), sarah-marie belcastro and Carolyn Yackel. The projects come from eight different contributors and include photos, instructions, mathematical analyses, and teaching activities.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Mathematical_Excursions">Mathematical Excursions: Side Trips along Paths Not Generally Traveled in Elementary Courses in Mathematics</a></em> (1933), Helen Abbot Merrill. An early book on recreational mathematics, aimed at getting high school students interested in mathematics.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Mathematics_in_India">Mathematics in India: 500 BCE‚Äì1800 CE</a></em> (2009), Kim Plofker. Organized chronologically, this has become the standard overview of this large topic. It also includes material on the history of astronomy in India, which was often tied to the mathematics of its era.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/The_Mathematics_of_Chip-Firing">The Mathematics of Chip-Firing</a></em> (2018), Caroline Klivans. A textbook on chip-firing games and abelian sandpile models.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Markov_Chains_and_Mixing_Times">Markov Chains and Mixing Times</a></em> (2009, 2017), David A. Levin and Yuval Peres, with contributions by Elizabeth Wilmer. A graduate-level text and research reference on how quickly random walks converge to their stable distributions.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Mirrors_and_Reflections">Mirrors and Reflections: The Geometry of Finite Reflection Groups</a></em> (2009), Alexandre V. and Anna Borovik. An undergraduate textbook on the classification of finite reflection groups and their associated root systems.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Pioneering_Women_in_American_Mathematics">Pioneering Women in American Mathematics: The Pre-1940 PhD‚Äôs</a></em> (2009), Judy Green and Jeanne LaDuke. Biographical profiles of over 200 women who earned doctorates in mathematics in the US before 1940, with some background material on what it was like for women to work in mathematics in those times.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Playing_with_Infinity">Playing with Infinity: Mathematical Explorations and Excursions</a></em> (1955, translated into English 1961), R√≥zsa P√©ter. An attempt to explain the nature of mathematics and of the infinite in mathematics to non-mathematicians, based on a series of letters from P√©ter to a literary friend.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Point_Processes">Point Processes</a></em> (1980), David Cox and Valerie Isham. A research reference on processes that randomly place points on the real line or other geometric spaces.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Power_in_Numbers:_The_Rebel_Women_of_Mathematics">Power in Numbers: The Rebel Women of Mathematics</a></em> (2018), Talithia Williams. A selection of profiles of famous women mathematicians, aimed at motivating young women to become mathematicians.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Primality_Testing_for_Beginners">Primality Testing for Beginners</a></em> (2009, translated into English 2014), Lasse Rempe-Gillen and Rebecca Waldecker. An undergraduate text on primality testing algorithms, based on a course from a summer research program for undergraduates.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Quantum_Computing:_A_Gentle_Introduction">Quantum Computing: A Gentle Introduction</a></em> (2011), Eleanor Rieffel and Wolfgang Polak. One of many texts on this fast-moving subject.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Robust_Regression_and_Outlier_Detection">Robust Regression and Outlier Detection</a></em> (1987), Peter Rousseeuw and Annick M. Leroy. A monograph on statistical methods that can tolerate the total corruption of a large fraction of the data points that they analyze, and still produce meaningful results.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Two-Sided_Matching">Two-Sided Matching: A Study in Game-Theoretic Modeling and Analysis</a></em> (1990), Alvin E. Roth and Marilda Sotomayor. A survey of methods related to stable matching, aimed at economics practitioners and focused on applications.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/When_Topology_Meets_Chemistry">When Topology Meets Chemistry: A Topological Look At Molecular Chirality</a></em> (2000), Erica Flapan. Many biomolecules are different than their mirror images; classical examples include sugars, whose mirrored molecules may taste different and have different effects. This undergraduate-level text studies how to model this effect using a combination of graph theory and knot theory.</p>
  </li>
  <li>
    <p><em><a href="https://en.wikipedia.org/wiki/Women_in_Mathematics">Women in Mathematics</a></em> (1974), Lynn Osen. This is the one that based its coverage of Hypatia on an early-20th-century children‚Äôs book that gave her a made-up backstory and attributed made-up modern rationalist quotes to her. Not recommended, and included mainly as a warning not to use this as a reference.</p>
  </li>
</ul>

<p>To keep from ending on a sour note, I‚Äôll add one more, that I found recently on Wikipedia (although the article there is very old) and I think is worthy of expansion: <em><a href="https://en.wikipedia.org/wiki/Logic_Made_Easy">Logic Made Easy: How to Know When Language Deceives You</a></em> (2004), Deborah J. Bennett, a popular-audience book on how to translate English phrases into logical formalisms and use that translation to understand more clearly what they mean.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105857580884627445">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-03-08T18:28:00Z</updated>
    <published>2021-03-08T18:28:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-03-09T02:55:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/033</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/033" rel="alternate" type="text/html"/>
    <title>TR21-033 |  Automating Tree-Like Resolution in Time $n^{o(\log n)}$ Is ETH-Hard | 

	Susanna de Rezende</title>
    <summary>We show that tree-like resolution is not automatable in time $n^{o(\log n)}$ unless ETH is false. This implies that, under ETH, the algorithm given by Beame and Pitassi (FOCS 1996) that automates tree-like resolution in time $n^{O(\log n)}$ is optimal. We also provide a simpler proof of the result of Alekhnovich and Razborov (FOCS 2001) that unless the fixed parameter hierarchy collapses, tree-like resolution is not automatable in polynomial time. The proof of our results builds on a joint work with G√∂√∂s, Nordstr√∂m, Pitassi, Robere and Sokolov (STOC 2021), which presents a simplification of the recent breakthrough of Atserias and M√ºller (FOCS 2019).</summary>
    <updated>2021-03-08T05:13:48Z</updated>
    <published>2021-03-08T05:13:48Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-12T10:20:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-61276742347305278</id>
    <link href="https://blog.computationalcomplexity.org/feeds/61276742347305278/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/03/when-do-i-need-to-warn-about-spoilers.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/61276742347305278" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/61276742347305278" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/03/when-do-i-need-to-warn-about-spoilers.html" rel="alternate" type="text/html"/>
    <title>When do I need to warn about Spoilers?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In a recent post¬†<a href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html">here</a>¬†I mentioned in passing a plot point from the last season of The Big Bang Theory. Note that the last season was in 2019.¬† WARNING- do not read that post if you are watching The Big Bang Theory and do not want a plot point revealed.¬†</p><p>Someone who perhaps thinks Lance and I are the same person (are we? See¬†<a href="https://blog.computationalcomplexity.org/2014/04/i-am-bill-gasarch.html">here</a>) left Lance a tweet complaining about the spoiler. At least I think they are complaining. The tweet is in Spanish and its¬†<a href="https://twitter.com/deoxyt2/status/1366120364070338560">here</a>.</p><p>Either</p><p>1) Some country is two years behind America on showing The Big Bang Theory.¬†</p><p>2) The person who tweeted has them on DVR (or something like that) and is watching them a few years after they air (I watched Firefly on a DVD I borrowed from a friend 10 years after it went off he air. Ask your grandparents what a DVD used to be.)¬†</p><p>3) They are kidding us and making fun of the notion of spoilers.</p><p>This raises the question: When is it okay to post spoilers without warning? A few random thoughts:</p><p>1) ``Don't tell me who won the superb owl! I have it on tape and want to watch it without knowing who won!''¬† This always seemed odd to me.¬† Routing for events to happen that have already happened seems weird to me. When I was 10 years old I was in New York listening to a Knicks-Celtics Basketball game on the radio and during halftime I accidentally found a Boston radio station that had the game 30 minutes later (I did not realize that the channel I was on originally was 30 minutes behind). So I heard how the game ended, then switched back <i>listening to a game knowing how it would end. </i>I didn't route for my team (the Knicks, who lost) but it just felt very weird listening to it. If I had thought of it I might have noticed how the different broadcasts differ and got a paper out of the data, but as a 10 year old I was not thinking about how to pad my resume quite yet.¬†</p><p>2) I like seeing a mystery twice- first time I don't know who did it, second time I do but can look for clues I missed the first time.</p><p>3) I would have thought 2 years after a show is off the air its fine to spoil. But... maybe not.</p><p>4) It also matters how important the plot point is. I didn't think the plot point I revealed was that important.¬†</p><p>5) Many TV shows are predictable so I am not sure what `spoiler' even means. If I said to Darling:</p><p><i>¬†The bad guy is an unimportant character we meet in the first 10 minutes.</i></p><p>that does not show I've seen it before. It shows that I am a master of TV-logic.</p><p>6) With Arc TV shows this is more of a problem. While it was possible to spoil an episode (Captain Kir will survive but Ensign Red Shirt will bite the dust) it was impossible to spoil a long-term arc. TV has gotten to complicated. And I say that without having watched Game of Thrones.¬†</p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-03-08T02:35:00Z</updated>
    <published>2021-03-08T02:35:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-03-12T08:28:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5371</id>
    <link href="https://www.scottaaronson.com/blog/?p=5371" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5371#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5371" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Another axe swung at the Sycamore</title>
    <summary xml:lang="en-US">So there‚Äôs an interesting new paper on the arXiv by Feng Pan and Pan Zhang, entitled ‚ÄúSimulating the Sycamore supremacy circuits.‚Äù It‚Äôs about a new tensor contraction strategy for classically simulating Google‚Äôs 53-qubit quantum supremacy experiment from Fall 2019. Using their approach, and using just 60 GPUs running for a few days, the authors say [‚Ä¶]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>So there‚Äôs an interesting new paper on the arXiv by Feng Pan and Pan Zhang, entitled <a href="https://arxiv.org/abs/2103.03074">‚ÄúSimulating the Sycamore supremacy circuits.‚Äù</a>  It‚Äôs about a new tensor contraction strategy for classically simulating Google‚Äôs 53-qubit quantum supremacy experiment from Fall 2019.  Using their approach, and using just 60 GPUs running for a few days, the authors say they managed to generate a million <em>correlated</em> 53-bit strings‚Äîmeaning, strings that all agree on a specific subset of 20 or so bits‚Äîthat achieve a high linear cross-entropy score.</p>



<p>Alas, I haven‚Äôt had time this weekend to write a ‚Äúproper‚Äù blog post about this, but several people have by now emailed to ask my opinion, so I thought I‚Äôd share the brief response I sent to a journalist.</p>



<p>This does look like a significant advance on simulating Sycamore-like random quantum circuits!  Since it‚Äôs based on tensor networks, you don‚Äôt need the literally largest supercomputer on the planet filling up tens of petabytes of hard disk space with amplitudes, as in the brute-force strategy <a href="https://arxiv.org/abs/1910.09534">proposed by IBM</a>.  Pan and Zhang‚Äôs strategy seems most similar to the strategy previously <a href="https://arxiv.org/pdf/2005.06787.pdf">proposed by Alibaba</a>, with the key difference being that the new approach generates millions of correlated samples rather than just one.</p>



<p>I guess my main thoughts for now are:</p>



<ol><li>Once you knew about this particular attack, you could evade it and get back to where we were before by switching to a more sophisticated verification test ‚Äî namely, one where you not only computed a Linear XEB score for the observed samples, you <em>also</em> made sure that the samples didn‚Äôt share too many bits in common. ¬†(Strangely, though, the paper never mentions this point.)</li><li>The other response, of course, would just be to redo random circuit sampling with a slightly bigger quantum computer, like the ~70-qubit devices that Google, IBM, and others are now building!</li></ol>



<p>Anyway, very happy for thoughts from anyone who knows more.</p></div>
    </content>
    <updated>2021-03-07T19:15:53Z</updated>
    <published>2021-03-07T19:15:53Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-03-11T20:31:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18263</id>
    <link href="https://rjlipton.wordpress.com/2021/03/07/advancing-and-counting/" rel="alternate" type="text/html"/>
    <title>Advancing and Counting</title>
    <summary>Announcing tomorrow‚Äôs Women in Data Science workshop (global start tonight 8pm ET), plus a US State Department event for International Women‚Äôs Day (also March 8) Santa Fe Inst. external faculty src Dana Randall is an ADVANCE Professor of Computing and also is an Adjunct Professor of Mathematics at the Georgia Institute of Technology. She is [‚Ä¶]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Announcing tomorrow‚Äôs Women in Data Science workshop (global start tonight 8pm ET), plus a US State Department event for International Women‚Äôs Day (also March 8)</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/03/dr.png"><img alt="" class="alignright wp-image-18265" height="175" src="https://rjlipton.files.wordpress.com/2021/03/dr.png?w=145&amp;h=175" width="145"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Santa Fe Inst. external faculty <a href="https://www.scs.gatech.edu/news/610412/dana-randall-named-external-faculty-santa-fe-institute">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Dana Randall is an ADVANCE Professor of Computing and also is an Adjunct Professor of Mathematics at the Georgia Institute of Technology. She is a terrific <a href="https://www.youtube.com/watch?v=MhYpfBUjQFQ">speaker</a> and <a href="https://www.ratemyprofessors.com/ShowRatings.jsp?tid=1579140">teacher</a> and leader. Her class ratings are off the charts. See also <a href="http://www.ams.org/publicoutreach/students/mathgame/arl2009">AMS</a> for her past special talks.</p>
<p>
Today I thought we would discuss her research and its connections to complexity theory and to physics and to math in general.<br/>
<span id="more-18263"/></p>
<p>
Dana does research into the boundary between math and physics. At the highest level Dana seeks to understand random processes, especially those connected to physical systems. The difficulty, in my opinion, is that sometimes the random system is not artificial. This means that we have no control over the system, and this makes the analysis of its behavior that much harder. </p>
<p>
Another way to say this is: we often fare better when we can control the exact random process. When someone else gets to decide on what the process is, we are often in trouble. The system might behave badly, or even worse, might be hard to understand. Nature is often that way‚Äînot always thinking about making the analysis of a system easy.</p>
<p>
</p><h2> Shuffling </h2><p/>
<p>
Let‚Äôs make this concrete. Suppose that you or Dana were presented with three methods for shuffling a deck of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> cards‚Äîwhen we play cards <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{52}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Imagine the methods are: </p>
<ol>
<li>This method selects <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> numbers in the range <img alt="{[1,n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5B1%2Cn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and checks for repeats. If there are, then try again. Use the permutation to shuffle.
</li><li>This method takes a deck of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> numbers and repeatedly shuffles them.
</li><li>This method executes the following code:
</li></ol>
<p><a href="https://rjlipton.files.wordpress.com/2021/03/qscode.png"><img alt="" class="aligncenter wp-image-18266" height="100" src="https://rjlipton.files.wordpress.com/2021/03/qscode.png?w=550&amp;h=100" width="550"/></a></p>
<p>The task of understanding these methods is before us. The first (1) is slow, even for modest size <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The chance of getting a permutation on a given trial is <img alt="{\frac{n!}{n^n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bn%21%7D%7Bn%5En%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which for <img alt="{n= 52}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D+52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{4.7257911 \times 10^{-22}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4.7257911+%5Ctimes+10%5E%7B-22%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which equals </p>
<p align="center"><img alt="\displaystyle  0.00000000000000000000047257911. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0.00000000000000000000047257911.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>But it does generate a fair shuffle‚Äîall possible ones are equally likely. And the proof of this is easy. The second (2) is more complicated. The final shuffle depends on the number and manner we use to shuffle the deck. The final analysis is messy.</p>
<p>
The third one (3) is due to Ronald Fisher and Frank Yates, who discovered it in 1938. It has an elegant, but nontrivial, analysis. It is both exact in that all orderings are equally likely, and it takes time linear in <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The <a href="https://en.wikipedia.org/wiki/Fisher-Yates_shuffle">history</a> of it is:</p>
<blockquote><p><b> </b> <em> The modern version of the Fisher-Yates shuffle, designed for computer use, was introduced by Richard Durstenfeld in 1964 and popularized by Donald Knuth in The Art of Computer Programming as ‚ÄúAlgorithm P (Shuffling)‚Äù [‚Ä¶apparently unawares; Fisher and Yates were acknowledged in later editions of Knuth‚Äôs text‚Ä¶]</em>
</p></blockquote>
<p>
My guess is that Dana, if given these methods, would not be interested in (1): too slow on one hand and trivial on the other. Nor interested in (2): not elegant and messy. Perhaps (3) would accord with her work: it has a nice analysis and runs in linear time. </p>
<p>
</p><h2> Advancing </h2><p/>
<p>
As we said earlier, Dana is part of <a href="http://www.advance.gatech.edu/team/gt-advance-professors">ADVANCE</a> at Georgia Tech: </p>
<blockquote><p><b> </b> <em> Georgia Tech‚Äôs ADVANCE Program seeks to develop systemic and institutional approaches that increase the representation, full participation, and advancement of women and minorities in academic STEM careers‚Äîthus contributing to a more diverse workforce, locally and nationally. </em>
</p></blockquote>
<p>
Dana has been and is a leader in helping advance these goals. It is especially relevant since this Monday, March 8th is special. It is <i>Celebrate International Women‚Äôs Day with WiDS</i>. See <a href="https://www.widsconference.org/conference.html">this</a> for details:</p>
<blockquote><p><b> </b> <em> Join us for the 24-hour virtual WiDS Worldwide Conference. We‚Äôll follow the sun, bringing you speakers from around the world on International Women‚Äôs Day beginning at 1:00 am GMT March 8 (5:00 pm PST March 7). </em>
</p></blockquote>
<p>
In making a collage of their speaker <a href="https://www.widsconference.org/speakers.html">page</a>, we have compressed and rearranged it somewhat. And we have added the logo for the <a href="https://www.widsconference.org/regional-events-2021.html">regional events</a> happening around the globe (some already past) and one for their sponsors.</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2021/03/widsspeakers.png"><img alt="" class="aligncenter size-large wp-image-18268" height="590" src="https://rjlipton.files.wordpress.com/2021/03/widsspeakers.png?w=600&amp;h=590" width="600"/></a></p>
<p/><p><br/>
If you could not take time to follow all the talks, you might take a few for a sample. Maybe you would figure that taking the first four speakers in alphabetical order, or the last four, would be as random a sample as any‚Äîafter all, what‚Äôs in a name?  Well, if you took the last four, you would actually get three of the four <a href="https://www.widsconference.org/conference.html">keynote</a> speakers. Sometimes procedures that we hope would give ‚Äúrandom‚Äù samples in fact give special ones. That takes us back to one more topic in Randall‚Äôs work.</p>
<p>
</p><h2> Counting </h2><p/>
<p>
One benefit of a random process is that under good conditions it can give us an accurate small sampling of a large and complex system. We have <a href="https://rjlipton.wordpress.com/2016/08/14/a-surprise-for-big-data-analytics/">mentioned</a> dimension reduction in this context. A simpler task is just to get an approximate count of entities in the system. Sometimes one can control the system, but sometimes not.</p>
<p>
One success is represented by a <a href="https://www.sciencedirect.com/science/article/pii/S0166218X15002255">paper</a> with Sarah Miracle of the University of St. Thomas. It is about counting colorings in multigraphs <img alt="{G = (V,E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that do not violate simple constraints on the edges <img alt="{(u,v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Each edge has a forbidden pair <img alt="{(c,c')}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28c%2Cc%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of colors, and a coloring <img alt="{\chi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> defined on <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is legal provided it does not have both <img alt="{\chi(u) = c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%28u%29+%3D+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{\chi(v) = c'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi%28v%29+%3D+c%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Multiple edges between <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can enforce multiple such constraints. Several natural problems can be represented via this one. The paper is one where their Markov Chain methods do well.</p>
<p>
A second recent <a href="https://arxiv.org/pdf/1611.03385.pdf">paper</a> uses Markov chains to count elements of a given rank in finite partially ordered sets. The chains should be biased according to the structure of the Hasse diagram of the poset. The trick in the paper is a way to balance the bias so as to prevent states that need to be counted from have too low frequency. This enables a direct analysis of the mixing time. The notable application was the first provably efficient ways to sample uniformly from certain kinds of partitions of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, for <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> quite large. </p>
<p>
The flip side‚Äîa phase change being a kind of a flip‚Äîis represented by other work with myriad collaborators that is summarized in a wonderful <a href="http://dimacs.rutgers.edu/events/details?eID=1409">talk</a> she gave at a <a href="http://dimacs.rutgers.edu/events/details?eID=1226&amp;loc=1409#1409">workshop</a> marking the 30th anniversary of DIMACS. As with an earlier <a href="https://drops.dagstuhl.de/opus/volltexte/2017/8021/pdf/LIPIcs-DISC-2017-3.pdf">version</a> at a Schloss Dagstuhl workshop, the talk was titled, ‚ÄúPhase Transitions and Emergent Phenomena in Algorithms and Applications‚Äù: </p>
<blockquote><p><b> </b> <em> Markov chain Monte Carlo methods have become ubiquitous across science and engineering as a means of exploring large configuration spaces. The idea is to walk among the configurations so that even though you explore a very small part of the space, samples will be drawn from a desirable distribution. Over the last 30 years there have been tremendous advances in the design and analysis of efficient sampling algorithms for this purpose, largely building on insights from statistical physics. One of the striking discoveries has been the realization that many natural Markov chains undergo a phase transition where they change from being efficient to inefficient as some parameter of the system is varied. </em>
</p></blockquote>
<p>
Here are a <a href="https://www.youtube.com/watch?v=IYkikVLkpoU">video</a> and <a href="http://dimacs.rutgers.edu/tools/fileman/Uploads/Documents/DIMACS-30/Randall_DIMACS30.pdf">slides</a>. The first main slide is about <em>programmable active matter</em> and it interests me especially to see DNA computing included. These systems can have <em>emergent behavior</em>, and while that can ruin randomized procedures that would bank on the system staying stable, it opens other opportunities. </p>
<p>
I, Dick, have run into this type of issue before. I have been on the wrong side, with theorems that were weak because we assumed the process could not be changed. Others who followed us changed the process‚Äîgot stronger results with easier proofs. Is there a name for this?</p>
<p>
</p><h2> Open Problems </h2><p/>
<p>
Take a look at the <a href="https://www.widsconference.org/conference.html">talks</a> for the WiDS Worldwide Conference this Monday. </p>
<p>
Ken also notes another event happening tomorrow: a 10am <a href="https://www.state.gov/2021-international-women-of-courage-award-recipients-announced/">ceremony</a> for the International Women of Courage Award. His friend the Iranian chess arbiter Shohreh Bayat is among the honorees, after a story told in her own words <a href="https://www.washingtonpost.com/opinions/i-loosened-my-hijab-at-a-chess-championship-now-im-afraid-to-return-to-iran/2020/02/17/1a670f66-5194-11ea-9e47-59804be1dcfb_story.html">here</a> in the Washington Post. Ken was working with her, on statistical assurance against cheating in the championship match, at the time. The ceremony starts at 10am ET hosted by the US Department of State, with opening remarks by Dr. Jill Biden.</p>
<p>
We must add that there is something that is hard about the type of random processes that Dana studies. We tried to explain what makes her work deep, but perhaps we did not properly explain it. </p>
<p/><p><br/>
[added global start time of workshop to subtitle]</p></font></font></div>
    </content>
    <updated>2021-03-07T17:34:01Z</updated>
    <published>2021-03-07T17:34:01Z</published>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="People"/>
    <category term="approximation"/>
    <category term="conference"/>
    <category term="Dana Randall"/>
    <category term="emergence"/>
    <category term="GaTech ADVANCE"/>
    <category term="International Women's Day"/>
    <category term="Markov chains"/>
    <category term="molecular computing"/>
    <category term="Monte Carlo"/>
    <category term="random walks"/>
    <category term="sampling"/>
    <category term="Shohreh Bayat"/>
    <category term="structures"/>
    <category term="Women in Data Science"/>
    <category term="Women of Courage Award"/>
    <category term="workshop"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>G√∂del‚Äôs Lost Letter and P=NP</title>
      <updated>2021-03-12T10:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=5711</id>
    <link href="https://francisbach.com/self-concordant-analysis-for-logistic-regression/" rel="alternate" type="text/html"/>
    <title>Going beyond least-squares ‚Äì II : Self-concordant analysis for logistic regression</title>
    <summary>Last month, we saw that self-concordance is a key property in optimization, to use local quadratic approximations in the sharpest possible way. In particular it was an affine-invariant quantity leading to a simple and elegant analysis of Newton method. The key assumption was a link between third and second-order derivatives, which took the following form...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text"><a href="https://francisbach.com/self-concordant-analysis-newton/">Last month</a>, we saw that self-concordance is a key property in optimization, to use local quadratic approximations in the sharpest possible way. In particular it was an affine-invariant quantity leading to a simple and elegant analysis of Newton method. The key assumption was a link between third and second-order derivatives, which took the following form for one-dimensional functions, $$|f^{\prime\prime\prime}(x)| \leqslant 2 f^{\prime\prime}(x)^{3/2}.$$ Alas, some of the most classical smooth functions appearing in machine learning are not self-concordant with this particular link between derivatives. The main example is the logistic loss, which is widely used across machine learning.</p>



<p class="justify-text">Indeed, if we take this logistic loss function \(f(x) = \log ( 1 + \exp(-x))\), it satisfies $$ f^\prime(x) =  \frac{ -\exp(-x)}{1+\exp(-x)} =\  ‚Äì \frac{1}{1+\exp(x)} = \ ‚Äì \sigma(-x),$$ where \(\sigma\) is the usual <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a> defined as \(\sigma(x) = \frac{1}{1+\exp(-x)}\), and which is increasing from \(0\) to \(1\). We then have \(f^{\prime\prime}(x) = \sigma(x) ( 1- \sigma(x) )\) and \(f^{\prime \prime \prime}(x) = \sigma(x) ( 1- \sigma(x) )( 1 ‚Äì 2 \sigma(x) )\) leading to $$|f^{\prime\prime\prime}(x)| \leqslant f^{\prime\prime}(x).$$ See below for plots of the logistic loss (left) and its derivatives (right).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-5795" height="267" src="https://francisbach.com/wp-content/uploads/2021/03/losses_logistic.png" width="639"/></figure></div>



<p class="justify-text">There is thus a link between third and second-order derivatives, but <em>without the power \(3/2\)</em>. Does this difference really matter? In this post, I will show how some properties from classical self-concordance can be extended to this slightly different notion. We will then present applications to stochastic gradient descent as well as the statistical analysis of generalized linear models, and in particular logistic regression.</p>



<p class="justify-text">I will describe applications to Newton method for large-scale logistic regression [<a href="https://papers.nips.cc/paper/8980-globally-convergent-newton-methods-for-ill-conditioned-generalized-self-concordant-losses.pdf">8</a>] in later posts (you read well: Newton method for large-scale machine learning can be useful, in particular for severely ill-conditioned problems).</p>



<h2>\(\nu\)-self-concordance</h2>



<p class="justify-text">A function \(f: C \subset \mathbb{R} \to \mathbb{R}\) is said \(\nu\)-self-concordant on the open interval \(C\) if and only if it is convex, three-times differentiable on \(C\), and there exists \(R &gt; 0\), such that $$\tag{1}\forall x \in C, \  |f^{\prime\prime\prime}(x)| \leqslant R f^{\prime\prime}(x)^{\nu\: \!  /2}.$$ </p>



<p class="justify-text">Note the difference with classical self-concordance (which corresponds to \(\nu=3\) and \(R=2\)). All positive powers are possible (see [<a href="https://link.springer.com/content/pdf/10.1007/s10107-018-1282-4.pdf">1</a>]), but we will focus primarily on \(\nu=2\), for which most of the properties below were derived in [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">2</a>].</p>



<p class="justify-text">Note that the definition above in one dimension is still ‚Äúaffine-invariant‚Äù if the constant \(R\) is allowed to change (that is, if it is true for \(f\), it is true for \(x \mapsto f(ax)\) for any \(a\)). However, unless \(\nu = 3\), this will not be true in higher dimension, and therefore, the analysis of Newton method will be more complicated.</p>



<p class="justify-text">For a convex function defined on a convex subset \(C\) of \(\mathbb{R}\), we need the same property along all rays, or equivalently, if \(f^{\prime\prime\prime}(x)[h,h^\prime,h^{\prime\prime}]= \sum_{i,j,k=1}^d h_i h_j^\prime h^{\prime\prime}_k \frac{\partial^3 f}{\partial x_i \partial x_j \partial x_k}(x)\) is the third-order tensor (with three different arguments, as needed below) and \(f^{\prime\prime}(x)[h,h] = \sum_{i,j=1}^d h_i h_j  \frac{\partial^2 f}{\partial x_i \partial x_j}(x)\) the symmetric second-order one, then there exists \(R\) such that $$\tag{2} \forall x \in C, \ \forall h \in \mathbb{R}^d , \ |f^{\prime\prime\prime}(x)[h,h^\prime,h^{\prime}]| \leqslant R \| h\| \cdot f^{\prime\prime}(x)[h^\prime,h^{\prime}],$$ where \(\| h\|\) is the standard Euclidean norm of \(h\). Note here the difference with classical self-concordance where we could consider the symmetric third-order tensor (that is, no need for \(h^\prime\) and \(h^{\prime\prime}\)), and only the Euclidean norm based on the Hessian \(f^{\prime\prime}(x)\) was used.</p>



<p class="justify-text"><strong>Examples. </strong>One can check that if \(f\) and \(g\) are \(2\)-self-concordant, then so is their average \(\frac{1}{2} ( f+g ) \) with the same constant \(R\) (this is one key advantage over \(3\)-self-concordance). Moreover, if \(f\) is \(2\)-self-concordant with constant \(R\), then \(g(x) = f(Ax)\) is also \(2\)-self concordant, with constant \(R \| A\|_{\rm op}\).</p>



<p class="justify-text">Classical examples are all linear and quadratic functions (with constant \(R = 0\)), the exponential function and the logistic loss \(f(x) = \log(1+\exp(-x))\), both with constant \(R=1\). This extends to the ‚Äúlog-sum-exp‚Äù function \(f(x) = \log\big( \sum_{i=1}^d \exp(x_i)\big)\), which is \(2\)-self-concordant with constant \(R = \sqrt{2}\). More generally, as shown at the end of the post, any log-partition function of the form $$ f(x) = \log \Big( \int_\mathcal{A} \exp( \varphi(a)^\top x) d\mu(a) \Big) $$ arising from <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear models</a> with bounded features, will be \(2\)-self-concordant, with constant the diameter of the set of features. Thus, self-concordance applies to all generalized linear models with the canonical link function. This includes <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">softmax regression</a> (for multiple classses), <a href="https://en.wikipedia.org/wiki/Conditional_random_field">conditional random fields</a>, and of course logistic regression which I will focus on below.</p>



<p class="justify-text"><strong>Logistic regression.</strong> The most classical example is thus logistic regression, with $$f(x) = \frac{1}{n} \sum_{i=1}^n \log(1 + \exp( ‚Äì x^\top a_i b_i ) ),$$ for observations \((a_i,b_i) \in \mathbb{R}^d \times \{-1,1\}\). See an example below in \(d=2\) dimensions.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full"><img alt="" class="wp-image-5814" height="261" src="https://francisbach.com/wp-content/uploads/2021/03/video_log_reg.gif" width="660"/>Logistic regression in two dimensions: data space with \(a_i \in \mathbb{R}^2\) represented with a different color/mark depending on the label \(b_i\) (left), parameter space (right) with level sets of the objective function \(f\) and its minimizer (purple asterisk).</figure></div>



<p class="justify-text"><strong>Properties in one dimension.</strong>  Mimicking what was done <a href="https://francisbach.com/self-concordant-analysis-newton/">last month</a>, a nice reformulation of Eq. (1) (which is one-dimensional) is $$ \big| \frac{d}{dx} \big( \! \log( f^{\prime\prime}(x)) \big) \big| = \big|   f^{\prime\prime\prime}(x)  f^{\prime \prime}(x)^{-1} \big| \leqslant R,$$ which allows to define upper and lower bounds on \(f^{\prime \prime}(x)\) by integration, as, for \(x &gt; 0\), $$ ‚Äì Rx \leqslant \log( f^{\prime\prime}(x))  \, ‚Äì \log(f^{\prime\prime}(0)) \leqslant Rx,$$ which can be transformed into (by isolating \(f^{\prime\prime}(x)\)): $$ \tag{3}  f^{\prime\prime}(0) \exp(\  ‚Äì R x ) \leqslant f^{\prime\prime}(x) \leqslant f^{\prime\prime}(0) \exp( R x ).$$ We thus obtain global upper and lower bounds on \(f^{\prime\prime}(x)\).</p>



<p class="justify-text">We can then integrate Eq. (3) twice between \(0\) and \(x\) to obtain lower and upper bounds on \(f^\prime\) and then \(f\): $$  f^{\prime\prime}(0) \frac{1-\exp( \ ‚Äì R x )}{R} \leqslant f^\prime(x)-f^\prime(0) \leqslant f^{\prime\prime}(0) \frac{\exp( R x )\  ‚Äì 1}{R},$$ and  $$ \tag{4} \!\!\!\!\!\! f^{\prime\prime}(0) \frac{\exp( \ ‚Äì R x ) + Rx \ ‚Äì 1}{R^2}\leqslant f(x) \ ‚Äì f(0) \ ‚Äì f^\prime(0) x \leqslant  f^{\prime\prime}(0) \frac{\exp( R x ) \ ‚Äì Rx \ ‚Äì 1}{R^2}.$$ We thus get a bound $$f(x) \ ‚Äì f(0) \ ‚Äì f^\prime(0) x \in f^{\prime\prime}(0) \frac{x^2}{2} \cdot [ \rho(-Rx), \rho(Rx) ],$$ with \(\displaystyle \rho(u) =\ \frac{\exp( u ) \ ‚Äì u\  ‚Äì 1}{u^2 / 2 } \sim 1 \) when \(u\to 0\), that is, the second-order expansion is tight at \(x =0\), but leads to global lower and upper bounds. These upper and lower Taylor expansions are illustrated below.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5792" height="227" src="https://francisbach.com/wp-content/uploads/2021/03/rho_log.png" width="292"/></figure></div>



<p class="justify-text"><strong>Properties in multiple dimensions.</strong> The properties above in Eq. (3) and (4) directly extend to multiple dimensions. For any \(x \in C\), then for any \(\Delta \in \mathbb{R}^d\), we have upper and lower bounds for the Hessian, the gradient (not presented below) and the functions value at \(x + \Delta\), that is, denoting by \(\| \cdot \|\) the standard Euclidean norm $$\tag{5}\exp(\ ‚Äì R\|\Delta\|) f^{\prime \prime}(x) \preccurlyeq  f^{\prime \prime}(x+\Delta) \preccurlyeq \exp( R\|\Delta\|)  f^{\prime \prime}(x),$$ and $$\tag{6} \!\!\!\!\!\!\!\!\! \frac{ \Delta^\top f^{\prime \prime}(x) \Delta}{2} \rho(-R \|\Delta\|_2) \leqslant f(x+\Delta)\ -f(x) \ ‚Äì f^\prime(x)^\top \Delta \leqslant \frac{ \Delta^\top f^{\prime \prime}(x) \Delta}{2} \rho(R \|\Delta\|_2).\! $$ These approximations are ‚Äúsecond-order tight‚Äù at \(\Delta=0\), that is, the term in \(f^{\prime\prime}(x)\) in Taylor expansion around \(x\) is exact. These can be derived by considering \(g(t) = f(x+ t\Delta)\), which is \(2\)-self-concordant with constant \(R \|\Delta\|_2\), and applying the one-dimensional properties above in Eqs. (2) and (3) between \(0\) and \(1\).</p>



<p class="justify-text"><strong>Avoiding exponentially decaying constants.</strong> In this post, I will focus primarily on the use self-concordant functions in optimization (stochastic gradient descent in this post and Newton method in another post) as well as in statistics.</p>



<p class="justify-text">The main benefit of using self-concordance is to avoid exponential constants traditionally associated with the analysis of logistic regression. Indeed, for the logistic loss, the second-derivative at \(x\) is is equal to \(\sigma(x) ( 1 ‚Äì \sigma(x) )\) and is equivalent to \(\exp(-|x|)\) when  \(| x| \) is large. Thus, if we are willing to only apply the logistic loss to small values of \(|x|\), let‚Äôs say less than \(M\), then the logistic loss is strongly-convex with constant greater than \(\exp(-M)\). Therefore, we can apply many results in optimization and statistics that apply to such losses. However, all of these results will be impacted by the constant \(\exp(-M)\), which is strictly positive but can be very small. With self-concordance, the analysis will get rid of these annoying constants and replace them by eigenvalues of Hessian matrices at the optimum, which are typically much larger (note that in the worst case, the exponential constants are unavoidable [<a href="http://proceedings.mlr.press/v35/hazan14a.pdf">3</a>]).</p>



<h2>Adaptivity of stochastic gradient descent</h2>



<p class="justify-text">I have not written about stochastic gradient descent for quite a while. Self-concordance gives me the occasion to talk about <em>adaptivity</em>.</p>



<p class="justify-text">It is well known that for smooth convex functions, gradient descent will converge exponentially fast if the function is also strongly-convex (essentially all eigenvalues of all Hessians being strictly positive). If the problem is ill-conditioned, then the exponential convergence rate turns into a rate of \(O(1/t)\) where \(t\) is the number of iterations. Gradient descent is <em>adaptive</em> as the exact same algorithm (with constant step-size or line-search) can be applied without the need to know the strong-convexity parameter. Moreover, if locally around the global optimum, the Hessians are better conditioned, gradient descent will also benefit from it. Therefore, gradient descent is great! What about stochastic gradient descent (SGD)?</p>



<p class="justify-text">It turns out that similar adaptivity exists for a well-defined version of SGD, and that self-concordance is one way to achieve simple non-asymptotic bounds (asymptotic bounds exist more generally [4]).</p>



<p class="justify-text"><strong>Logistic regression. </strong>We consider the logistic regression problem where we aim to minimize the expectation $$f(x) = \mathbb{E}_{a,b} \log( 1 + \exp(-b a^\top x) ) = \mathbb{E}_{a,b} g(x|a,b) ,$$ where \((a,b) \in \mathbb{R}^d \times \{-1,1\}\) is a pair of input \(a\) and output \(b\) (hopefully, the machine learning police will excuse my use of \(x\) as the parameter and not the input). We are given \(n\) independent and identically distributed observations \((a_1,b_1),\dots, (a_n,b_n)\) and we aim at finding the minimizer \(x_\ast\) of \(f\) (which is the logistic loss on unseen data), which we assume to exist. We assume that the feature norms \(\|a\|\) are almost surely bounded by \(R\).</p>



<p>Note here that we are not trying to minimize the empirical risk and by using a single pass, we obtain bounds on the generalization performance. This is one of the classical benefits of SGD.</p>



<p class="justify-text"><strong>Averaged stochastic gradient descent.</strong> We consider the stochastic gradient recursion: $$x_i = x_{i-1} ‚Äì \gamma_i g^\prime(x_{i-1}|a_i,b_i),$$ for \(i=1,\dots,n\), with a single pass over the data. We also consider the average iterate \(\bar{x}_n = \frac{1}{n+1} \sum_{i=0}^{n} x_i\). </p>



<p class="justify-text">Standard results from the stochastic gradient descent literature [<a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">5</a>, <a href="https://papers.nips.cc/paper/2011/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf">6</a>] show that if \(\gamma_i = \frac{1}{R^2 \sqrt{i}}\), then, up to universal (small) constants, $$ \mathbb{E} f(\bar{x}_i)\  ‚Äì f(x_\ast) \leqslant (1 + R^2 \| x_0 ‚Äì x_\ast\|^2) \frac{\log n}{\sqrt{n}}.$$ If in addition, the function \(f\) is \(\mu\)-strongly-convex, then with the step-size \(\gamma_i = \frac{1}{\mu i}\), up to universal (small) constants, $$ \mathbb{E} f(\bar{x}_i) \ ‚Äì f(x_\ast) \leqslant   \frac{R^2 \log n}{n\mu}.$$ The strongly convex result seems beneficial as we get a rate in \(O(( \log n) / n)\) instead of \(O(( \log n) / \sqrt{n})\), <em>but</em>, (1) it depends on \(\mu\), which can be very small in problems in high dimension \(d\), (2) it depends on this global strong-convexity constant \(\mu\), that is a lower bound on all Hessians, which is zero for logistic regression unless a projection step is used (and with exponentially small constant as explained above), and (3) the step-size has to be adapted. </p>



<p class="justify-text"><strong>Adaptivity. </strong>Wouldn‚Äôt it be great if these three problems could be solved at once? This is what I worked on a few years ago [<a href="http://">7</a>], where I showed that for constant step-size \(\gamma\) proportional to \(\frac{1}{R^2 \sqrt{n}}\) (thus dependent on the total number of gradient steps), we have. up to constants: $$ \mathbb{E} f(\bar{x}_i)\ ‚Äì f(x_\ast) \leqslant (1 + R^2 \| x_0 ‚Äì x_\ast\|^2) \frac{1}{\sqrt{n}},$$ <em>and</em> $$ \mathbb{E} f(\bar{x}_i)\ ‚Äì f(x_\ast) \leqslant (1 + R^4 \| x_0 ‚Äì x_\ast\|^4) \frac{R^2 }{\mu_\ast n},$$ where \(\mu_\ast\) is the smallest eigenvalue of the Hessian \(f^{\prime\prime}(x_\ast)\) <em>at the optimum</em>. The two bounds are always satisfied and one can be bigger than the other depending on \(n\) and the condition number \(R^2 / \mu_\ast\).</p>



<p class="justify-text">We thus get (almost) the best of all worlds! The proof relies strongly on self-concordance and applies to all generalized linear models. Note that (a) no new algorithm is proposed here, I am simply providing partial theoretical justifications why a classical algorithm works so well, (b) this is <em>only an upper-bound</em> on performance (more on this below).</p>



<h2>Generalization bounds for generalized linear models</h2>



<p class="justify-text">Beyond optimization, the use of self-concordant can make the non-asymptotic <em>statistical</em> analysis of logistic regression, and more generally all generalized linear models, sharper in the regularized unregularized setting [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">2</a>, <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">10</a>], with \(\ell_1\)-norm [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">10</a>], or with non-parametric kernel-based models [<a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">2</a>, <a href="http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf">9</a>]. The first benefit is to avoid exponential constants associated with usual strong-convexity arguments of the loss (which can also be achieved with other tools, see [<a href="http://proceedings.mlr.press/v9/kakade10a/kakade10a.pdf">11</a>]). But there is another important benefit that requires some digression.</p>



<p class="justify-text"><strong>Asymptotic statistics is great‚Ä¶</strong> Supervised learning through empirical risk minimization is the workhorse of machine learning. It can be analyzed from different perspectives and with different tools. As shown in the great book by Aad Van der Vaart [12], asymptotics statistics is a very clean way of understanding the behavior of statistical estimators when the number of observations \(n\) goes to infinity. </p>



<p class="justify-text">Empirical risk minimization is indeed an example of M-estimation problems (estimators based on minimizing the empirical average of some loss functions), and it is known that under general conditions, the estimator has a known asymptotic mean and variance (with the traditional <a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher information matrices</a>), which leads to an asymptotic equivalent of the unseen population risk (e.g., the ‚Äútest error‚Äù). We recover the usual \(d/n\) bound for unregularized problems as well as dimension-independent results when using regularization with squared Euclidean norms (then with worse dependence in \(n\)).</p>



<p class="justify-text">Because we deal with <em>limits</em>, one can formally compare two methods by favoring the one with the smallest asymptotic risk. This is not possible when non-asymptotic <em>upper bounds</em> are available: the fact that they are true for all \(n\) is a strong benefit, but since they are only bounds, they don‚Äôt say anything about which method is best.</p>



<p class="justify-text"><strong>‚Ä¶ But it is only asymptotic.</strong> Of course, these comparisons are only true in the limit of large \(n\), and, in particular for high-dimensional problems (e.g., data and/or parameters in large dimensions), we are unlikely to be in the asymptotic regime. So we cannot really rely only on letting \(n\) go to infinity. Moreover, these asymptotic limits typically depend on some information which is not available at training time. But does this mean that we have to throw away all asymptotic results?</p>



<p class="justify-text"><strong>Self-concordance to the rescue.</strong> Since many of the asymptotic results are obtained by second-order Taylor expansions, we need non-asymptotic ways of dealing with these expansions, which is exactly what self-concordance allows you to do (with some extra effort of course). Therefore the best of both worlds can be achieved with such tools; see, e.g., [<a href="http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf">9</a>, <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">10</a>], for examples of analysis, and [<a href="https://projecteuclid.org/download/pdfview_1/euclid.aos/1360332187">13</a>, <a href="https://papers.nips.cc/paper/2015/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf">14</a>] for other tools that can achieve similar results. It is then possible to prove that some asymptotic expansions are valid non-asymptotically.</p>



<h2>Conclusion</h2>



<p class="justify-text">In this post I focused on two aspects of self-concordant analysis for logistic regression and its extensions, namely adaptivity of stochastic gradient descent and statistical generalization bounds.</p>



<p class="justify-text">In a later post, I will go back to Newton‚Äôs method, where the lack of affine invariance of \(2\)-self-concordance makes the analysis more complicated. However it will come with some interesting benefits for large-scale severely ill-conditioned problems [<a href="https://papers.nips.cc/paper/8980-globally-convergent-newton-methods-for-ill-conditioned-generalized-self-concordant-losses.pdf">8</a>]. You may wonder why we should bother with Newton method for large-scale machine learning when stochastic gradient descent, with or without variance reduction, seems largely enough. Stay tuned!</p>



<h2>References</h2>



<p class="justify-text">[1] Tianxiao Sun, and Quoc Tran-Dinh. <a href="https://link.springer.com/content/pdf/10.1007/s10107-018-1282-4.pdf">Generalized self-concordant functions: a recipe for Newton-type methods</a>.¬†<em>Mathematical Programming</em>¬†178(1): 145-213, 2019.<br/>[2] Francis Bach. <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F09-EJS521">Self-Concordant Analysis for Logistic Regression</a>. Electronic Journal of Statistics, 4, 384-414, 2010.<br/>[3] Elad Hazan, Tomer Koren, and Kfir Y. Levy. <a href="http://proceedings.mlr.press/v35/hazan14a.pdf">Logistic regression: Tight bounds for stochastic and online optimization</a>.¬†Proceedings of the International Conference on Learning Theory (COLT), 2014.<br/>[4] Boris T. Polyak, and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging.¬†SIAM journal on control and optimization,¬†30(4):838-855, 1992.<br/>[5] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. <a href="https://epubs.siam.org/doi/pdf/10.1137/070704277">Robust stochastic approximation approach to stochastic programming</a>. SIAM Journal on optimization, 19(4), 1574-1609, 2009.<br/>[6] Francis Bach, and Eric Moulines. <a href="https://papers.nips.cc/paper/2011/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf">Non-asymptotic analysis of stochastic approximation algorithms for machine learning</a>. Advances in Neural Information Processing Systems (NIPS), 2011.<br/>[7] Francis Bach. <a href="http://jmlr.org/papers/volume15/bach14a/bach14a.pdf">Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression</a>. Journal of Machine Learning Research, 15(Feb):595‚àí627, 2014.<br/>[8] Ulysse Marteau-Ferey, Francis Bach, Alessandro Rudi. <a href="https://papers.nips.cc/paper/8980-globally-convergent-newton-methods-for-ill-conditioned-generalized-self-concordant-losses.pdf">Globally convergent Newton methods for ill-conditioned generalized self-concordant Losses</a>. Advances in Neural Information Processing Systems (NeurIPS), 2019.<br/>[9] Ulysse Marteau-Ferey, Dmitrii Ostrovskii, Francis Bach, Alessandro Rudi. <a href="http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf">Beyond Least-Squares: Fast Rates for Regularized Empirical Risk Minimization through Self-Concordance</a>. Proceedings of the International Conference on Learning Theory (COLT), 2019<br/>[10] Dmitrii Ostrovskii, <a href="https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2F20-EJS1780">Francis Bach. Finite-sample Analysis of M-estimators using Self-concordance</a>. Electronic Journal of Statistics, 15(1):326-391, 2021.<br/>[11] Sham Kakade, Ohad Shamir, Karthik Sridharan, and Ambuj Tewari. <a href="http://proceedings.mlr.press/v9/kakade10a/kakade10a.pdf">Learning exponential families in high-dimensions: Strong convexity and sparsity</a>. In Proceedings of the international conference on artificial intelligence and statistics (AISTATS), 2010.<br/>[12] Aad W. Van der Vaart. Asymptotic Statistics. Cambridge University Press, 2000.<br/>[13] Vladimir Spokoiny. <a href="https://projecteuclid.org/download/pdfview_1/euclid.aos/1360332187">Parametric estimation. Finite sample theory</a>. The Annals of Statistics, 40(6), 2877-2909, 2012.<br/>[14] Tomer Koren, and Kfir Y. Levy. <a href="https://papers.nips.cc/paper/2015/file/acf4b89d3d503d8252c9c4ba75ddbf6d-Paper.pdf">Fast Rates for Exp-concave Empirical Risk Minimization</a>. Advances in Neural Information Processing Systems (NIPS), 2015.<br/></p>



<h2>Self-concordance for generalized linear models</h2>



<p class="justify-text">We consider a probability distribution on some set \(\mathcal{A}\), with density $$\exp\big( \varphi(a)^\top x) \ ‚Äì f(x) \big)$$ with respect to the positive measure \(d\mu\), with \(f(x)\) the log-partition function, defined so that the total mass is one, that is, $$ f(x) = \log \Big( \int_\mathcal{A} \exp( \varphi(a)^\top x) d\mu(a) \Big). $$ We assume the feature vector \(\varphi(a)\) and the parameter \(x\) are in \(\mathbb{R}^d\).</p>



<p class="justify-text">The theory of <a href="https://en.wikipedia.org/wiki/Exponential_family">exponential families</a> tells us that the function \(f(x)\) is the ‚Äúcumulant generating‚Äù function. That is, the cumulants of \(\varphi(a)\) for the probability distribution defined by \(x\), are exactly the derivatives of \(f\) taken at \(x\). More precisely, for the usual mean and covariance matrix, we get $$ \mathbb{E}_{a|x} \varphi(a) = f^\prime (x),$$ $$ \mathbb{E}_{a|x} \big(\varphi(a) \ ‚Äì  f^\prime (x)\big) \otimes \big(\varphi(a) \ ‚Äì f^\prime (x)\big) = f^{\prime\prime}(x).$$ For the third order cumulant, we get: $$ \mathbb{E}_{a|x} \big(\varphi(a)\  ‚Äì f^\prime (x)\big)\otimes \big(\varphi(a)\  ‚Äì f^\prime (x)\big) \otimes \big(\varphi(a) \ ‚Äì f^\prime (x)\big) = f^{\prime\prime\prime}(x).$$ Thus, for any \(h \in \mathbb{R}^d\), \(\big(\varphi(a) \ ‚Äì f^\prime (x)\big)^\top h \leqslant \| h\| D\), where \(D\) is the diameter of the set \(\{ \varphi(a), \ a \in \mathcal{A} \}\), which leads to the desired \(2\)-self-concordance property.</p></div>
    </content>
    <updated>2021-03-07T16:06:50Z</updated>
    <published>2021-03-07T16:06:50Z</published>
    <category term="Machine learning"/>
    <category term="Optimization"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2021-03-12T10:23:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=848</id>
    <link href="https://emanueleviola.wordpress.com/2021/03/07/questions-on-the-future-of-lower-bounds/" rel="alternate" type="text/html"/>
    <title>Questions on the future of lower bounds</title>
    <summary>Will any of the yellow books be useful? Book 576 (not pictured) was saved just in time from the paper mill. It was rumored that Lemma 76.7.(ii) could have applications to lower bounds. Upon closer inspection, that lemma has a one-line proof by linearity of expectation if you change the constant 17 to 19. This [‚Ä¶]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Will any of the yellow books be useful?</p>



<figure class="wp-block-image"><img alt="https://media.springernature.com/w153/springer-static/cover/book/9783030276447.jpg" src="https://media.springernature.com/w153/springer-static/cover/book/9783030276447.jpg"/></figure>



<p>Book 576 (not pictured) was saved just in time from the paper mill.  It was rumored that Lemma 76.7.(ii) could have applications to lower bounds.  Upon closer inspection, that lemma has a one-line proof by linearity of expectation if you change the constant 17 to 19.  This change does not affect the big-Oh.</p>



<p>Will the study of randomness lead to the answer to any of the questions that are open since before randomness became popular? I think it‚Äôs a coin-toss.</p>



<p>Will there be any substance to the belief that algebraic lower bounds must be proved <em>first</em>?</p>



<p>Will the people who were mocked for working on DLOGTIME uniformity, top fan-in k circuits, or ZFC independence have the last laugh?</p>



<p>Will someone switch the circuit breaker and lit up CRYPT, DERAND, and PCPOT, or will they remain unplugged amusement parks where you sit in the roller coaster, buckle up, and pretend?</p>



<p>Will diagonalization be forgotten, or will it continue to frustrate combinatorialists with lower bounds they can‚Äôt match for functions they don‚Äôt care about?</p>



<p>Will decisive progress be made tonight, or will it take centuries?</p>



<p>Only Ketan Mulmuley knows for sure.</p></div>
    </content>
    <updated>2021-03-07T11:52:21Z</updated>
    <published>2021-03-07T11:52:21Z</published>
    <category term="Uncategorized"/>
    <category term="lower bounds"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2021-03-12T10:21:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1485</id>
    <link href="https://ptreview.sublinear.info/?p=1485" rel="alternate" type="text/html"/>
    <title>News for February 2021</title>
    <summary>We got quite some action last month. We saw five papers. A lot of action in graph world and some action in quantum property testing which we hope you will find appetizing. Also included is a result on sampling uniformly random graphlets. Testing Hamiltonicity (and other problems) in Minor-Free Graphs, by Reut Levi and Nadav [‚Ä¶]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We got quite some action last month. We saw five papers. A lot of action in graph world and some action in quantum property testing which we hope you will find appetizing. Also included is a result on sampling uniformly random <em>graphlets</em>. </p>



<p><strong>Testing Hamiltonicity (and other problems) in Minor-Free Graphs</strong>, by Reut Levi and Nadav Shoshan (<a href="https://arxiv.org/abs/2102.11728">arXiv</a>). Graph Property Testing has been explored pretty well for dense graphs (and reasonably well for bounded degree graphs). However, testing properties in the general case still remains an elusive goal. This paper makes contributions in this direction and as a first result it gives an algorithm for testing Hamiltonicity <em>in minor free graphs</em> (with two sided error) with running time \(poly(1/\varepsilon)\). Let me begin by pointing out that Hamiltonicity is an irksome property to test in the following senses.</p>



<ul><li>It is neither monotone nor additive. So the partition oracle based algorithms do not immediately imply a tester (with running time depending only on \(\varepsilon\) for Hamiltonicity. This annoyance bugs you even in the bounded degree case.</li><li> Czumaj and Sohler characterized what graph properties are testable with one-sided error in general planar graphs. In particular, they show a property of general planar graphs is testable <em>iff</em> this property can be reduced to testing for a finite family of finite forbidden subgraphs. Again, Hamiltonicity does not budge to this result. </li><li>There are (concurrent) results by Goldreich and Adler-Kohler which show that with one-sided error, Hamiltonicity cannot be tested with \(o(n)\) queries. </li></ul>



<p>The paper shows that distance to Hamiltonicity can be exactly captured in terms of a certain combinatorial parameter. Thereafter, the paper tries to estimate this parameter after cleaning up the graph a little. This allows them to estimate the distance to Hamiltonicity and thus also implies a tolerant tester (restricted to mino-free graphs).</p>



<p><strong>Testing properties of signed graphs</strong>, by Florian Adriaens, Simon Apers (<a href="https://arxiv.org/abs/2102.07587">arXiv</a>). Suppose I give you a graph \(G=(V,E)\) where all edges come with a label: which is either ‚Äúpositive‚Äù or ‚Äúnegative‚Äù. Such signed graphs are used to model various scientific phenomena. Eg, you can use these to model interactions between individuals in social networks into two categories like friendly or antagonistic.</p>



<p>This paper considers property testing problems on signed graphs. The notion of farness from the property extends naturally to these graphs (both in the dense graph model and the bounded degree model). The paper contains explores three problems in both of these models: signed triangle freeness, balance and clusterability. Below I will zoom into the tester for clusterability in the bounded degree setting developed In the paper. A signed graph is considered clusterable if you can partition the vertex set into some number of components such that the edges within any component are all positive and the edges running across components are all negative.</p>



<p>The paper exploits a forbidden subgraph characterization of clusterability which shows that any cycle with exactly one negative edge is a certificate of non-clusterability of \(G\). The tester runs multiple random walks from a handful of start vertices to search for these ‚Äúbad cycles‚Äù by building up on ideas in the seminal work of Goldreich and Ron for testing bipariteness. The authors put all of these ideas together and give a \(\widetilde{O}(\sqrt n)\) time one-sided tester for clusterability in signed graphs.</p>



<p/>



<p><strong>Local Access to Random Walks</strong>, by Amartya Shankha Biswas, Edward Pyne, Ronitt Rubinfeld (<a href="https://arxiv.org/abs/2102.07740">arXiv</a>). Suppose I give you a gigantic graph (with bounded degree) which does not fit in your main memory and I want you to solve some computational problem which requires you to solve longish random walks of length \(t\). And lots of them. It would be convenient to not spend \(\Omega(t)\) units of time performing every single walk. Perhaps it would work just as well for you to have an oracle which provides query access to a \(Position(G,s,t)\) oracle which returns the position of a walk from \(s\) at time \(t\) of your choice. Of course, you would want the sequence of vertices returned to behave consistently with some actual random walk sampled from the distribution of random walks starting at \(s\). Question is: Can I build you this primitive? This paper answers this question in affirmative ¬†and shows that for graphs with spectral gap \(\Delta\), this can be achieved with running time \(\widetilde{O}(\sqrt n/\Delta)\) per query. And you get the guarantee that the joint distribution of the vertices you return at queried times is \(1/poly(n)\) close to the uniform distribution over such walks in \(\ell_1\). ¬†Thus, for a random \(d\)-regular graph, you get running times of the order \(\widetilde{O}(\sqrt n)\) per query. The authors also show tightness of this result by showing to get subconstant error in \(\ell_1\), you necessarily need \(\Omega(\sqrt n/\log n)\) queries in expectation.</p>



<p/>



<p><strong>Efficient and near-optimal algorithms for sampling connected subgraphs</strong>, by Marco Bressan (<a href="https://arxiv.org/abs/2007.12102">arXiv</a>). As the title suggests, this paper considers efficient algorithms for sampling a uniformly random \(k\)-graphlet from a given graph \(G\) (for \(k \geq 3\)). Recall, a \(k\)-graphlet refers to a collection of \(k\)-vertices which induce a connected graph in \(G\). The algorithm considered in the paper is pretty simple. You just define a Markov Chain \(\mathcal{G}_k\) with all \(k\)-graphlets as its state space. Two states in \(\mathcal{G}_k\) are adjacent <em>iff</em> their intersection is a \((k-1)\)-graphlet. To obtain a uniformly random sample, a classical idea is to just run this Markov Chain and obtain an \(\varepsilon\)-uniform sample. However, the gap between upper and lower bounds on the mixing time of this walk is of the order \(\rho^{k-1}\) where \(\rho = \Delta/\delta\) (that is the ratio of maximum and minimum degrees to the power \(k-1\)). The paper closes this gap up to logarithmic factors and shows that the mixing time of the walk is at most \(t_{mix}(G) \rho^{k-1} \log(n/\varepsilon)\). It also proves an almost matching lower bound. Further, the paper also presents an algorithm with event better running time to return an almost uniform \(k\)-graphlet. This exploits a previous observation: sampling a uniformly random \(k\)-graphlet is equivalent to sampling a uniformly random edge in \(\mathcal{G}_{k-1}\). The paper then proves a lemma which upperbounds the relaxation time of walks in \(\mathcal{G}_k\) to walks in \(\mathcal{G}_{k-1}\). And then you upperbound the mixing time in terms of the relaxation time to get an improved expected running time of the order \(O(t_{mix}(G) \cdot \rho^{k-2} \cdot \log(n/\varepsilon)\).</p>



<p/>



<p><strong>Toward Instance-Optimal State Certification With Incoherent Measurements</strong>, by Sitan Chen, Jerry Li, Ryan O‚ÄôDonnell (<a href="https://arxiv.org/abs/2102.13098">arXiv</a>). The problem of quantum state certification has gathered interest over the last few years. Here is the setup: you are given a quantum state \(\sigma \in \mathbb{C}^{d \times d}\) and you are also given \(N\) copies of an unknown state \(\rho\). You want to distinguish between the following two cases: Does \(\rho = \sigma\) or is \(\sigma\) at least \(\varepsilon\)-far from \(\rho\) in trace norm? Badescu et al showed in a recent work that if entangled measurements are allowed, you can do this with a mere \(O(d/\varepsilon^2)\) copies of \(\rho\). But using entangled states comes with its own share of problems. On the other hand if you disallow entanglement, as Bubeck et al show, you need \(\Omega(d^{3/2}/\varepsilon^2)\) measurements. This paper asks: for which states \(\sigma\) can you improve upon this bound. The work takes inspirations from <em>a la</em> ‚Äúinstance optimal‚Äù bounds for identity testing. Authors show a fairly general result which (yet again) confirms that the quantum world is indeed weird. In particular, the main result of the paper implies that the copy complexity of (the quantum analog of) identity testing in the quantum world (with non-adaptive queries) grows as \(\Theta(d^{1/2}/\varepsilon^2)\). That is, the number of quantum measurements you need increases with \(d\) (which is the stark opposite of the behavior you get in the classical world).</p></div>
    </content>
    <updated>2021-03-06T17:50:44Z</updated>
    <published>2021-03-06T17:50:44Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Akash</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-03-11T22:42:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/032</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/032" rel="alternate" type="text/html"/>
    <title>TR21-032 |  Fiat-Shamir via List-Recoverable Codes (or: Parallel Repetition of GMW is not Zero-Knowledge) | 

	Ron Rothblum, 

	Justin Holmgren, 

	Alex Lombardi</title>
    <summary>Shortly after the introduction of zero-knowledge proofs, Goldreich, Micali and Wigderson (CRYPTO '86) demonstrated their wide applicability by constructing  zero-knowledge proofs for the NP-complete problem of graph 3-coloring. A long-standing open question has been whether parallel repetition of their protocol preserves zero knowledge. In this work, we answer this question in the negative, assuming a a standard cryptographic assumption (i.e., the hardness of learning with errors (LWE)).

Leveraging a connection observed by Dwork, Naor, Reingold, and Stockmeyer (FOCS '99), our negative result is obtained by making positive progress on a related fundamental problem in cryptography: securely instantiating the Fiat-Shamir heuristic for eliminating interaction in public-coin interactive protocols. A recent line of works has shown how to instantiate the heuristic securely, albeit only for a limited class of protocols.

Our main result shows how to instantiate Fiat-Shamir for parallel repetitions of much more general interactive proofs. In particular, we construct hash functions that, assuming LWE, securely realize the Fiat-Shamir transform for the following rich classes of protocols:

- The parallel repetition of any ``commit-and-open'' protocol (such as the GMW protocol mentioned above), when a specific (natural) commitment scheme is used.  Commit-and-open protocols are a ubiquitous paradigm for constructing general purpose public-coin zero knowledge proofs.

- The parallel repetition of any base protocol that (1) satisfies a stronger notion of soundness called round-by-round soundness, and (2) has an efficient procedure, using a suitable trapdoor, for recognizing ``bad verifier randomness'' that would allow the prover to cheat.

Our results are obtained by establishing a new connection between the Fiat-Shamir transform and  list-recoverable codes.  In contrast to the usual focus in coding theory, we focus on a parameter regime in which the input lists are extremely large, but the rate can be small.  We give a (probabilistic) construction based on Parvaresh-Vardy codes (FOCS '05) that suffices for our applications.</summary>
    <updated>2021-03-05T23:22:09Z</updated>
    <published>2021-03-05T23:22:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-12T10:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/031</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/031" rel="alternate" type="text/html"/>
    <title>TR21-031 |  Upper Bound for Torus Polynomials | 

	Vaibhav Krishan</title>
    <summary>We prove that all functions that have low degree torus polynomials approximating them with small error also have $MidBit^+$ circuits computing them. This serves as a partial converse to the result that all $ACC$ functions have low degree torus polynomials approximating them with small error, by Bhrushundi, Hosseini, Lovett and Rao (ITCS 2019).</summary>
    <updated>2021-03-05T14:14:09Z</updated>
    <published>2021-03-05T14:14:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-12T10:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18238</id>
    <link href="https://rjlipton.wordpress.com/2021/03/04/wsj-meets-group-algorithms/" rel="alternate" type="text/html"/>
    <title>WSJ Meets Group Algorithms</title>
    <summary>Our whole life is solving puzzles. ‚Äî Ern≈ë Rubik Cropped from source Jessica Fridrich is a Distinguished Professor of Electrical and Computer Engineering at Binghamton University. She is an expert on data hiding, that is, steganography. She has over 34,000 citations‚Äîimpressive. A lot more than most of us. She also has worked on the famous [‚Ä¶]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Our whole life is solving puzzles. ‚Äî Ern≈ë Rubik</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/03/jf-1.png"><img alt="" class="alignright wp-image-18244" height="120" src="https://rjlipton.files.wordpress.com/2021/03/jf-1.png?w=163&amp;h=120" width="163"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="http://www.ws.binghamton.edu/fridrich/pressconnects_com%20%2009-11-03%20%20News%20Story.htm">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Jessica Fridrich is a Distinguished Professor of Electrical and Computer Engineering at Binghamton University. She is an expert on data hiding, that is, <a href="https://en.wikipedia.org/wiki/Steganography">steganography</a>. She has over 34,000 citations‚Äîimpressive. A lot more than most of us. She also has <a href="http://www.ws.binghamton.edu/fridrich/cube.html">worked</a> on the famous Rubik‚Äôs cube.</p>
<p>
Today we look at her work on Rubik‚Äôs cube, the WSJ‚Äôs interest in Rubik‚Äôs cube, and what both say‚Äîand don‚Äôt say‚Äîabout fundamental algorithms.<br/>
<span id="more-18238"/></p>
<p>
By the way, WSJ stands for the Wall Street Journal‚Äîthe American <a href="https://en.wikipedia.org/wiki/The_Wall_Street_Journal">newspaper</a> of business. The WSJ has shown great interest in the Rubik‚Äôs cube puzzle and has run many articles over the years on it.</p>
<p>
Recall the cube puzzle was invented‚Ä¶of course you know all about Rubik‚Äôs cube. You probably have owned one at one time. Right. Just for a <a href="https://en.wikipedia.org/wiki/Rubik%27s_Cube">refresher</a>: </p>
<blockquote><p><b> </b> <em> The Rubik‚Äôs Cube is a 3-D combination puzzle invented in 1974 by Hungarian sculptor and professor of architecture Ern≈ë Rubik. As of January 2009, 350 million cubes had been sold worldwide, making it the world‚Äôs top-selling puzzle game. It is widely considered to be the world‚Äôs best-selling toy. </em>
</p></blockquote>
<p>
But you may not know all about Fridrich.</p>
<p/><h2> Speed Solving </h2><p/>
<p>
Fridrich was one of the progenitors of <em>speed cubing</em>. She took part in the First World Championship in 1982 in Budapest, next-door to her native Czechoslovakia. She finished in the middle of the pack with a time of <b>29.11</b> seconds from a randomly well-mixed starting cube position. Her thoughts on how the cubes could be better prepared for speed are recorded on her <a href="http://www.ws.binghamton.edu/fridrich/cubewrld.html">page</a> about the tournament.</p>
<p>
At the Second World Championship, she improved her average time to <b>20.48</b> seconds and placed <a href="https://www.worldcubeassociation.org/competitions/WC2003">2nd</a>. She had the two fastest solves in the finals but lost on average-of-median-three-of-five.  That championship took place in Toronto‚Äîin <b>2003</b>. She is at a loss to explain why there was such a gap. Usually an athlete‚Äîin this case a mathlete?‚Äîis on the downswing nearing age 40, but even as a self-described ‚Äú<a href="http://ws2.binghamton.edu/fridrich/history.html">old-timer</a>,‚Äù she fended off all but one of a whole next generation. </p>
<p>
Much of the credit goes to her solving method. She originated the ‚ÄúO‚Äù and ‚ÄúP‚Äù parts of the <a href="https://en.wikipedia.org/wiki/CFOP_method">CFOP</a> method. CFOP stands for: Cross, First 2 Layers, Orient Last Layer, Permute Last Layer. Versions of this are used my most top ‚Äúcubers‚Äù to this day, and her name is often affixed to the method. In a 2008 profile of her, the New York Times <a href="https://www.nytimes.com/2008/12/16/science/16prof.html?_r=1&amp;em">quoted</a> the 2003 winner as saying that Fridrich found the route up the mountain while the rest of the cubers optimize traversing ledges along it. And in 2012, the NYT <a href="https://london2012.blogs.nytimes.com/2012/06/25/master-of-the-shot-put-and-the-cube/?searchResultPosition=5">quoted</a> Olympic shot-putter Reese Hoffa as wanting ‚Äúto learn the Fridrich Method of solving the puzzle, ‚Äòwhich is what all of the best cubers use.'‚Äù </p>
<p>
At this point, knowing our interest in chess, you might expect a <a href="https://en.wikipedia.org/wiki/The_Queen's_Gambit_(novel)"><i>Queen‚Äôs</i></a> <a href="https://en.wikipedia.org/wiki/The_Queen's_Gambit_(miniseries)"><i>Gambit</i></a> reference. But what we have here is not a story of Beth Harmon coming back from a life <a href="https://www.thereviewgeek.com/thequeensgambit-e6review/">adjournment</a> or Roy Hobbs in <a href="https://en.wikipedia.org/wiki/The_Natural"><i>The</i></a> <a href="https://en.wikipedia.org/wiki/The_Natural_(film)"><i>Natural</i></a> rejoining baseball almost 20 years after being shot. It‚Äôs about going overseas, earning a PhD, getting two research positions, writing early papers (under the <a href="https://dblp.org/pid/29/4038.html">name</a> Jiri Fridrich), transitioning, then getting a faculty position leading to tenure while developing mathematical formulas and writing tons of code for systems to <a href="https://www.nytimes.com/2004/07/22/technology/what-s-next-for-doctored-photos-a-new-flavor-of-digital-truth-serum.html?searchResultPosition=16">source</a> photos and catch digital pirates and pornographers and other image fraudsters, then coming back to light up an <img alt="{8 \times 8}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or <img alt="{3 \times 3 \times 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> universe. Not to mention doing her own stunning <a href="https://www.jessicafridrich.com/">photo art</a> of the American Southwest.</p>
<p/><h2> Quicker Times and Cubes </h2><p/>
<p>
Since 2003, the <a href="https://en.wikipedia.org/wiki/Speedcubing#Competitions">championships</a> have been held every other year, thought the 2021 championships set for the Netherlands are uncertain owing to the pandemic. The youngsters soon broke through en-masse, and it strikes me that the cube technology improved so that the cubes are springier and lighter. The winning time fell almost 5 seconds to <b>15.10</b> in 2005 and hit <b>6.74</b> in 2019. That was not the world record, however‚Äîan incredible <b>3.47</b> seconds in 2018 by Yusheng Du, beating the previous record of Feliks Zemdegs by a whopping 3/4 of a second.</p>
<p>
Fridrich, however, must claim a distinction no one may ever match. She learned how to solve the cube and traced out the performance of methods of doing so in 1981, months before she saw a cube, let alone owned one. Despite the ‚ÄúB≈±v√∂s Kocka‚Äù (‚ÄúMagic Cube,‚Äù as Rubik called it) having been on shelves in neighboring Hungary for four years, with worldwide marketing by early 1980, they were hard to come by in her home city, Ostrava. </p>
<p>
She found an article on solving the cube in a Russian magazine. It laid out the concept of group theory and the role of group commutators, which she learned to apply creatively in order to streamline actions. The first time she touched a cube was to help a friend put his back the way it was. A family visiting from France let her keep one, and later in 1981 she was finally able to purchase a few more. This invites analogy to working out chess without a board on a bedroom ceiling as depicted in <em>The Queen‚Äôs Gambit</em>.</p>
<p>
We‚ÄîDick and Ken‚Äîmust admit that neither of us has ever done this with the cubes we own, not fast, not slow. Yet we do understand the theory behind it. We believe we do. </p>
<p/><h2> Group Theory of the Cube </h2><p/>
<p>
I (Dick) plan on explaining the theory by using a new toy that I have invented: The <img alt="{\mathit{slider}^{TM}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathit%7Bslider%7D%5E%7BTM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. 	</p>
<p><a href="https://rjlipton.files.wordpress.com/2021/03/slidercubes.png"><img alt="" class="aligncenter wp-image-18247" height="45" src="https://rjlipton.files.wordpress.com/2021/03/slidercubes.png?w=96&amp;h=45" width="96"/></a></p>
<p>
We will write the state as <img alt="{xyz}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bxyz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where each of <img alt="{x,y,z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is one of <font color="red">1</font>, <font color="green">2</font>, or <font color="blue">3</font>. The operations allowed are the <i>cyclic shift</i> <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which does 	</p>
<p align="center"><img alt="\displaystyle  xyz \rightarrow zxy, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++xyz+%5Crightarrow+zxy%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>and the <i>flip</i> <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of the initial two elements: 	</p>
<p align="center"><img alt="\displaystyle  xyz \rightarrow yxz. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++xyz+%5Crightarrow+yxz.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
Note there are 6 possible states. For the real Rubik‚Äôs cube, the number of states is just a little bit larger: <b>43,252,003,274,489,856,000</b>. But the basic concept is the same. Suppose we are given the state 	</p>
<p align="center"><img alt="\displaystyle  132. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++132.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>How fast can you get the initial state <img alt="{123}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B123%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? Apply <img alt="{FCC}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BFCC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: 	</p>
<p align="center"><img alt="\displaystyle  132 \rightarrow 312 \rightarrow 231 \rightarrow 123 . " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++132+%5Crightarrow+312+%5Crightarrow+231+%5Crightarrow+123+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
This is a special case of the general <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/genset.pdf">result</a> that any symmetric group is <a href="https://groupprops.subwiki.org/wiki/Symmetric_group_on_a_finite_set_is_2-generated">generated</a> by two operations: a full cycle and a single flip. The key with the actual Rubik‚Äôs cube is since the group is larger and it has more operations that can be applied finding the group operations may be more difficult. But there are algorithms that can find them. See <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/gpaction.pdf">this</a> for another article by Keith Conrad. </p>
<p>
There are many more pages like that on the cube. But Fridrich still shows the <a href="http://www.ws.binghamton.edu/fridrich/system.html">seminal page</a> she posted in ‚ÄúWinter 1996/97.‚Äù It links to other pages, ones that also credit other people, such as <a href="http://www.ws.binghamton.edu/fridrich/Mike/middle.html">this</a> explaining the algorithms in great pictorial detail. This was in the infancy of the Internet. Her pages are often credited with spurring the turn-of-the-millennium boom in Rubik‚Äôs cube which led to the revival of the championships in 2003. A 2016 New York Post <a href="https://nypost.com/2016/10/31/how-the-internet-brought-the-rubiks-cube-back-to-life/">article</a> whose URL is titled, ‚Äúhow the Internet brought the Rubik‚Äôs cube back to life,‚Äù says: </p>
<blockquote><p><b> </b> <em> The seeds for Rubik‚Äôs Cube‚Äôs rediscovery were sown on the internet. In the mid-1990s, a Rubik‚Äôs Cube champion-turned-computer-science professor at SUNY Binghamton posted her secrets of the Cube on a primitive Web 1.0 site on the university‚Äôs servers. Jessica Fridrich‚Äôs method spread and is today the most widely used technique to solve the puzzle. </em>
</p></blockquote>
<p>
See also this <a href="https://uncletyson.wordpress.com/tag/dan-knights/">telling</a> by the 2003 winner, Dan Knights. This shows how one person using spare time on the Internet can power up business.</p>
<p/><h2> Enter the WSJ </h2><p/>
<p>
The WSJ has had an interest in Rubik‚Äôs cube for years. They had a long feature <a href="https://www.wsj.com/articles/how-to-teach-professors-humility-hand-them-a-rubiks-cube-11614352261">article</a> last week titled, ‚ÄúHow to Teach Professors Humility? Hand Them a Rubik‚Äôs Cube,‚Äù by Melissa Korn. It describes a faculty development challenge among several small colleges in which professors became students again. Last month they also had an <a href="https://www.wsj.com/articles/seeing-things-with-the-power-of-symmetry-11612461325">article</a> on symmetry by the mathematician Eugenia Cheng that mentioned the cube.</p>
<p>
I recall several features the WSJ has run on the cube and its solvers. The 2011 <a href="https://www.wsj.com/articles/SB10001424052970204319004577088513615125328">article</a>, ‚ÄúOne Cube, Many Knockoffs, Quintillions of Possibilities,‚Äù led off with the Polish teenager Michal Pleskowicz winning the 2011 world championship with a time of <b>8.65</b> seconds, then discussed the performance of pirated cubes: ‚ÄúOne reason Mr. Pleskowicz and a new generation of Rubik‚Äôs fanatics can solve the notoriously difficult puzzle in record time: They don‚Äôt use Rubik‚Äôs Cubes at all, instead substituting souped-up Chinese knockoffs engineered for speed‚Ä¶‚Äù Their 2014 <a href="https://www.wsj.com/articles/SB10001424052702304518704579523513594900696">article</a>, ‚ÄúRubik‚Äôs Cube Proves It‚Äôs Hip to Be Square,‚Äù profiled both Rubik and speed-solvers. </p>
<p>
The <a href="https://www.wsj.com/articles/a-thinking-persons-guide-to-the-rubiks-cube-1517586702">feature</a> I recall best was in 2018. It was titled, ‚ÄúA Thinking Person‚Äôs Guide to the Rubik‚Äôs Cube,‚Äù and subtitled, ‚ÄúWhat‚Äôs the best solution method‚Äîtheory, algorithms or chance?‚Äù It was also by Eugenia Cheng. She begins by confessing, ‚ÄúI have always loved playing with a Rubik‚Äôs Cube, which combines logic with a satisfying tactile activity. I can solve it‚Äîgetting each of the six sides to be one color‚Äîbut not particularly quickly or cleverly.‚Äù </p>
<p>
They also like its use for analogies. Scrolling through their advanced search‚Äîboth Ken and I subscribe to the WSJ‚Äîwe find:</p>
<ul>
<li><a href="https://www.wsj.com/articles/close-reopen-repeat-restaurants-dont-know-what-covid-19-will-dish-out-next-11613138412">2/12/21</a>: ‚ÄúRunning restaurants is now ‚Äòa bit of a Rubik‚Äôs Cube,‚Äô said Mr. Mosier, who reopened his casual cafes in late January.‚Äù
</li><li><a href="https://www.wsj.com/articles/reopening-schools-is-so-complicated-new-york-struggles-to-schedule-classes-11597939473">8/20/20</a>, headline: ‚ÄúReopening Schools Is So Complicated, New York Is Struggling to Schedule Classes Nation‚Äôs largest district is still hashing out basic details about the school day; ‚Äòa multidimensional Rubik‚Äôs Cube‚Äô of challenges.‚Äù
</li><li><a href="https://www.wsj.com/articles/new-u-s-rules-on-foreign-students-put-universitiesin-dilemma-11594149280">7/7/20</a>: ‚ÄúThe new [pandemic] rules have created a Rubik‚Äôs Cube of decisions for schools, which face unique challenges with each of their international student populations.‚Äù
</li><li><a href="https://www.wsj.com/articles/an-l-a-home-asking-62-million-includes-a-playful-perk-a-model-racetrack-11590523214">5/26/20</a>, about a home selling for $62 million: ‚ÄúDesigned by Seattle-based architecture firm Olson Kundig, the house has interlocking boxes and planes resembling a Rubik‚Äôs cube‚Ä¶‚Äù
</li><li><a href="https://www.wsj.com/articles/president-trump-announces-19-billion-relief-program-for-farmers-11587165759">4/17/20</a>, quoting Agriculture Secretary Sonny Perdue on the coronavirus relief program for farmers: ‚ÄúIt will be a logistical Rubik‚Äôs Cube.‚Äù
</li><li><a href="https://www.wsj.com/articles/he-wanted-something-more-from-retirement-so-he-got-three-jobs-11573743922">11/14/19</a>, about a retiree who started teaching business classes, keeping books for a non-profit business, and working on a ferry dock: ‚ÄúMy society consists of able-bodied seamen, boat captains, truckers hauling bait and lobsters, fishermen, islanders and wide-eyed vacationers,‚Äù says Mr. Marshall. It‚Äôs ‚Äúa constant Rubik‚Äôs cube. You never know what you‚Äôll find.‚Äù
</li></ul>
<p>
In all, using the WSJ advanced search, we find 239 hits for ‚ÄúRubik‚Äù going back to 1980. We should mention in-passing that one of them is their 7/17/20 <a href="https://www.wsj.com/articles/ron-graham-dazzled-admirers-with-math-and-juggling-feats-11594994403">obituary</a> for Ron Graham. We also find 7 hits for ‚ÄúFridrich‚Äù over the same span. But they are all about the housing market, involving the Nashville-based realty Fridrich and Clarke.</p>
<p/><h2> Open Problems </h2><p/>
<p>
I am happy to see that the WSJ has published multiple articles on a particular algorithmic task. I like that algorithms have been the center of articles. I wish they would talk more about important algorithms. Solving a Rubik‚Äôs cube is not an algorithm that is used every day: What about: </p>
<ul>
<li>Sorting
</li><li>Searching
</li><li>Dynamic Programming
</li><li>Fast Arithmetic
</li></ul>
<p>They do have Eugenia Cheng, who wrote a <a href="https://www.wsj.com/articles/algorithms-arent-just-for-computers-11557407055">column</a> comparing sorting algorithms. And they have written on algorithms used in <a href="https://www.wsj.com/graphics/journey-inside-a-real-life-trading-algorithm/">trading</a> and on <a href="https://www.wsj.com/articles/social-media-algorithms-rule-how-we-see-the-world-good-luck-trying-to-stop-them-11610884800">social</a>‚Äì<a href="https://www.wsj.com/articles/how-google-interferes-with-its-search-algorithms-and-changes-your-results-11573823753">media</a> <a href="https://www.wsj.com/articles/how-to-win-friends-and-influence-algorithms-11555246800">platforms</a> and for <a href="https://www.wsj.com/articles/algorithms-used-in-policing-face-policy-review-11591003801">policing</a> and <a href="https://www.wsj.com/articles/SB10001424052702304626104579121251595240852">parole</a> and <a href="https://www.wsj.com/articles/algorithm-helps-new-york-decide-who-goes-free-before-trial-11600610400">bail</a> decisions. But that tends away from <em>fundamental algorithms</em> where the math is the matter.</p>
<p>
A 2018 WSJ <a href="https://www.wsj.com/articles/dont-believe-the-algorithm-1536157620">article</a> by Hannah Fry titled ‚ÄúDon‚Äôt Believe the Algorithm,‚Äù which begins with flaws in using facial recognition to find wanted suspects, brings us back toward Fridrich‚Äôs research. Might this all also raise discussion of ‚Äúalgorithms‚Äù for what and whom to cover?</p>
<p>
[fixed name at end]</p></font></font></div>
    </content>
    <updated>2021-03-05T00:37:41Z</updated>
    <published>2021-03-05T00:37:41Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Results"/>
    <category term="Algorithms"/>
    <category term="competitions"/>
    <category term="Jessica Fridrich"/>
    <category term="photography"/>
    <category term="puzzles"/>
    <category term="Rubik's Cube"/>
    <category term="science and society"/>
    <category term="speed cubing"/>
    <category term="steganography"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>G√∂del‚Äôs Lost Letter and P=NP</title>
      <updated>2021-03-12T10:20:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5359</id>
    <link href="https://www.scottaaronson.com/blog/?p=5359" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5359#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5359" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Zen Anti-Interpretation of Quantum Mechanics</title>
    <summary xml:lang="en-US">As I lay bedridden this week, knocked out by my second dose of the Moderna vaccine, I decided I should blog some more half-baked ideas because what the hell? It feels therapeutic, I have tenure, and anyone who doesn‚Äôt like it can close their broswer tab. So: although I‚Äôve written tens of thousands of words, [‚Ä¶]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>As I lay bedridden this week, knocked out by my second dose of the Moderna vaccine, I decided I should blog some more half-baked ideas because what the hell?  It feels therapeutic, I have tenure, and anyone who doesn‚Äôt like it can close their broswer tab.</p>



<p>So: although I‚Äôve written tens of thousands <a href="https://www.pbs.org/wgbh/nova/article/can-quantum-computing-reveal-the-true-meaning-of-quantum-mechanics/">of</a> <a href="https://arxiv.org/abs/1306.0159">words</a>, <a href="https://www.scottaaronson.com/papers/philos.pdf">on</a> <a href="https://www.scottaaronson.com/blog/?p=1103">this</a> <a href="https://www.scottaaronson.com/blog/?p=3628">blog</a> <a href="https://www.scottaaronson.com/democritus/">and</a> <a href="https://www.scottaaronson.com/qclec.pdf">elsewhere</a>, about interpretations of quantum mechanics, again and again I‚Äôve dodged the question of which interpretation (if any) I <em>really believe myself</em>.  Today, at last, I‚Äôll emerge from the shadows and tell you precisely where I stand.</p>



<p>I hold that all interpretations of QM are just crutches that are better or worse at helping you along to the Zen realization that <strong>QM is what it is and doesn‚Äôt need an interpretation</strong>.¬† As Sidney Coleman <a href="https://arxiv.org/abs/2011.12671">famously argued</a>, what needs reinterpretation is not QM itself, but all our <em>pre</em>-quantum philosophical baggage‚Äîthe baggage that leads us to demand, for example, that a wavefunction |œà‚ü© either be ‚Äúreal‚Äù like a stubbed toe or else ‚Äúunreal‚Äù like a dream.  Crucially, because this philosophical baggage differs somewhat from person to person, the ‚Äúbest‚Äù interpretation‚Äîmeaning, the one that leads most quickly to the desired Zen state‚Äîcan also differ from person to person.  Meanwhile, though, thousands of physicists (and chemists, mathematicians, quantum computer scientists, etc.) have approached the Zen state merely by spending decades working with QM, never worrying much about interpretations at all.  This is probably the truest path; it‚Äôs just that most people lack the inclination, ability, or time.</p>



<p>Greg Kuperberg, one of the smartest people I know, once told me that the problem with the Many-Worlds Interpretation is not that it says anything wrong, but only that it‚Äôs ‚Äúmelodramatic‚Äù and ‚Äúoverwritten.‚Äù  Greg is far along the Zen path, probably further than me.</p>



<p>You shouldn‚Äôt confuse the Zen Anti-Interpretation with ‚ÄúShut Up And Calculate.‚Äù  The latter phrase, mistakenly attributed to Feynman but really due to David Mermin, is something one might say at the <em>beginning</em> of the path, when one is as a baby.  I‚Äôm talking here only about the <em>endpoint</em> of path, which one can approach but never reach‚Äîthe endpoint where you intuitively understand exactly what a Many-Worlder, Copenhagenist, or Bohmian would say about any given issue, and also how they‚Äôd respond to each other, and how they‚Äôd respond to the responses, etc. but after years of study and effort you‚Äôve <em>returned</em> to the situation of the baby, who just sees the thing for what it is.</p>



<p>I don‚Äôt mean to say that the interpretations are all interchangeable, or equally good or bad.  If you had to, you could call even me a ‚ÄúMany-Worlder,‚Äù but <em>only</em> in the following limited sense: that in fifteen years of teaching quantum information, my experience has consistently been that for <em>most</em> students, <a href="https://en.wikipedia.org/wiki/Many-worlds_interpretation">Everett‚Äôs crutch</a> is the best one currently on the market.  At any rate, it‚Äôs the one that‚Äôs the most like a straightforward <em>picture</em> of the equations, and the least like a wobbly tower of words that might collapse if you utter any wrong ones.¬† Unlike Bohr, Everett will never make you feel stupid for asking the questions an inquisitive child would ask; he‚Äôll simply give you answers that are as clear, logical, and internally consistent as they are metaphysically extravagant.  That‚Äôs a start.</p>



<p>The <a href="https://en.wikipedia.org/wiki/Copenhagen_interpretation">Copenhagen Interpretation</a> retains a place of honor as the <em>first</em> crutch, for decades the <em>only</em> crutch, and the one closest to the spirit of positivism.  Unfortunately, <em>wielding</em> the Copenhagen crutch requires mad philosophical skillz‚Äîwhich parts of the universe should you temporarily regard as ‚Äúclassical‚Äù?  which questions should be answered, and which deflected?‚Äîto the point where, if you‚Äôre capable of all that verbal footwork, then why do you even <em>need</em> a crutch in the first place?  In the hands of amateurs‚Äîmeaning, alas, nearly everyone‚ÄîCopenhagen often leads <em>away</em> <em>from</em> rather than toward the Zen state, as one sees with the generations of New-Age bastardizations about ‚Äúobservations creating reality.‚Äù</p>



<p>As for <a href="https://en.wikipedia.org/wiki/De_Broglie%E2%80%93Bohm_theory">deBroglie-Bohm</a>‚Äîwell, that‚Äôs a weird, interesting, baroque crutch, one whose actual details (the preferred basis and the guiding equation) are historically contingent and tied to specific physical systems.  It‚Äôs probably the right crutch for <em>someone</em>‚Äîit gets eternal credit for having led Bell to discover the Bell inequality‚Äîbut its quirks definitely need to be discarded along the way.</p>



<p>Note that, among those who approach the Zen state, many might still call themselves Many-Worlders or Copenhagenists or Bohmians or whatever‚Äîjust as those far along in spiritual enlightenment might still call themselves Buddhists or Catholics or Muslims or Jews (or atheists or agnostics)‚Äîeven though, by that point, they might have more in common with each other than they do with their supposed coreligionists or co-irreligionists.</p>



<p>Alright, but isn‚Äôt all this Zen stuff just a way to dodge the <em>actual, substantive</em> questions about QM, by cheaply claiming to have transcended them?  If that‚Äôs your charge, then please help yourself to the following FAQ about the details of the Zen Anti-Interpretation.</p>



<ol><li><strong>What is a quantum state?</strong>  It‚Äôs a unit vector of complex numbers (or if we‚Äôre talking about mixed states, then a trace-1, Hermitian, positive semidefinite matrix), which encodes everything there is to know about a physical system.<br/></li><li><strong>OK, but are the quantum states ‚Äúontic‚Äù (really out in the world), or ‚Äúepistemic‚Äù (only in our heads)?</strong>  Dude.  Do ‚Äúbasketball games‚Äù really exist, or is that just a phrase we use to summarize our knowledge about certain large agglomerations of interacting quarks and leptons?  Do even the ‚Äúquarks‚Äù and ‚Äúleptons‚Äù exist, or are those just words for excitations of the more fundamental fields?  Does ‚Äújealousy‚Äù exist?  Pretty much<em> all</em> our concepts are complicated grab bags of ‚Äúontic‚Äù and ‚Äúepistemic,‚Äù so it shouldn‚Äôt surprise us if quantum states are too.  Bad dichotomy.<br/></li><li><strong>Why are there probabilities in QM?</strong>  Because QM <em>is</em> a (the?) generalization of probability theory to involve complex numbers, whose squared absolute values are probabilities.  It <em>includes</em> probability as a special case.<br/></li><li><strong>But why do the probabilities obey the Born rule?</strong>  Because, once the unitary part of QM has picked out the 2-norm as being special, for the probabilities <em>also</em> to be governed by the 2-norm is pretty much the only possibility that makes mathematical sense; there are many nice theorems formalizing that intuition under reasonable assumptions.<br/></li><li><strong>What is an ‚Äúobserver‚Äù?</strong>  It‚Äôs exactly what modern decoherence theory says it is: a particular kind of quantum system that interacts with other quantum systems, becomes entangled with them, and thereby records information about them‚Äîreversibly in principle but irreversibly in practice.<br/></li><li><strong>Can observers be manipulated in coherent superposition, as in the <a href="https://en.wikipedia.org/wiki/Wigner%27s_friend">Wigner‚Äôs Friend</a> scenario?</strong>  If so, they‚Äôd be radically unlike any physical system we‚Äôve ever had direct experience with.  So, are you asking whether such ‚Äúobservers‚Äù would be <em>conscious</em>, or if so what they‚Äôd be conscious of?  Who the hell knows?<br/></li><li><strong>Do ‚Äúother‚Äù branches of the wavefunction‚Äîones, for example, where my life took a different course‚Äîexist in the same sense this one does?</strong>  If you start with a quantum state for the early universe and then time-evolve it forward, then yes, you‚Äôll get not only ‚Äúour‚Äù branch but also a proliferation of other branches, in the overwhelming majority of which Donald Trump was never president and civilization didn‚Äôt grind to a halt because of a bat near Wuhan.¬† But how could we possibly know whether anything ‚Äúbreathes fire‚Äù into the other branches and makes them real, when we have no idea what breathes fire into <em>this</em> branch and makes <em>it</em> real?  This is not a dodge‚Äîit‚Äôs just that a simple ‚Äúyes‚Äù or ‚Äúno‚Äù would fail to do justice to the enormity of such a question, which is above the pay grade of physics as it currently exists.¬†<br/></li><li><strong>Is this it?  Have you brought me to the end of the path of understanding QM?</strong>  No, I‚Äôve just pointed the way toward the <em>beginning</em> of the path.  The most fundamental tenet of the Zen Anti-Interpretation is that there‚Äôs no shortcut to actually <a href="https://www.scottaaronson.com/qclec.pdf">working</a> <a href="https://www.amazon.com/Quantum-Mechanics-Theoretical-Leonard-Susskind/dp/0465062903/ref=asc_df_0465062903/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=312014159412&amp;hvpos=&amp;hvnetw=g&amp;hvrand=7852945785672685485&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9028280&amp;hvtargid=pla-435140302691&amp;psc=1">through</a> the Bell inequality, quantum teleportation, Shor‚Äôs algorithm, the Kochen-Specker and PBR theorems, possibly even a ‚Ä¶ <em>photon</em> or a <em>hydrogen atom</em>, so you can see quantum probability in action and be enlightened.  I‚Äôm further along the path than I was twenty years ago, but not as far along as some of my colleagues.  Even the greatest quantum Zen masters will be able to get further when new quantum phenomena and protocols are discovered in the future.  All the same, though‚Äîand this is another major teaching of the Zen Anti-Interpretation‚Äîthere‚Äôs more to life than achieving greater and greater clarity about the foundations of QM.  And on that note‚Ä¶</li></ol>



<p>To those who asked me about Claus Peter Schnorr‚Äôs <a href="https://eprint.iacr.org/2021/232">claim</a> to have discovered a fast <em>classical</em> factoring algorithm, thereby ‚Äúdestroying‚Äù (in his words) the RSA cryptosystem, see (e.g.) <a href="https://twitter.com/inf_0_/status/1367376526300172288?fbclid=IwAR19Ip7XyoPjHfm9WBzqiUkQpxUVLGfVTgLGQmmncgrkUsvcLIrkzbOPw_U">this Twitter thread by Keegan Ryan</a>, which explains what certainly <em>looks</em> like a fatal error in Schnorr‚Äôs paper.</p></div>
    </content>
    <updated>2021-03-04T23:26:29Z</updated>
    <published>2021-03-04T23:26:29Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Metaphysical Spouting"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-03-11T20:31:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/</id>
    <link href="https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/" rel="alternate" type="text/html"/>
    <title>2-round BFT SMR with n=4, f=1</title>
    <summary>Guest post by Zhuolun Xiang In the previous post, we presented a summary of our good-case latency results for Byzantine broadcast and Byzantine fault tolerant state machine replication (BFT SMR), where the good case measures the latency to commit given that the leader/broadcaster is honest. In this post, we describe...</summary>
    <updated>2021-03-03T11:37:00Z</updated>
    <published>2021-03-03T11:37:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2021-03-11T22:43:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/030</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/030" rel="alternate" type="text/html"/>
    <title>TR21-030 |  Hardness of Constant-round Communication Complexity | 

	Rahul Ilango, 

	Shuichi Hirahara, 

	Bruno Loff</title>
    <summary>How difficult is it to compute the communication complexity of a two-argument total Boolean function $f:[N]\times[N]\to\{0,1\}$, when it is given as an $N\times N$ binary matrix? In 2009, Kushilevitz and Weinreb showed that this problem is cryptographically hard, but it is still open whether it is NP-hard. 

In this work, we show that it is NP-hard to approximate the size (number of leaves) of the smallest constant-round protocol for a two-argument total Boolean function $f:[N]\times[N]\to\{0,1\}$, when it is given as an $N\times N$ binary matrix. Along the way to proving this, we show a new *deterministic* variant of the round elimination lemma, which may be of independent interest.</summary>
    <updated>2021-03-02T21:31:00Z</updated>
    <published>2021-03-02T21:31:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-12T10:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/</id>
    <link href="https://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/" rel="alternate" type="text/html"/>
    <title>Faculty at Universidad Cat√≥lica de Chile (apply by April 10, 2021)</title>
    <summary>The Institute for Mathematical and Computational Engineering at Universidad Cat√≥lica de Chile offers one or more full-time positions. We invite applications from candidates in the areas of Data Science, Machine Learning, Optimization, Statistics and Stochastic, although other areas from Computational Science and Engineering, Optimization and Applied Mathematics will also be considered. Website: http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc Email: pbarcelo@uc.cl</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Institute for Mathematical and Computational Engineering at Universidad Cat√≥lica de Chile offers one or more full-time positions. We invite applications from candidates in the areas of Data Science, Machine Learning, Optimization, Statistics and Stochastic, although other areas from Computational Science and Engineering, Optimization and Applied Mathematics will also be considered.</p>
<p>Website: <a href="http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc">http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc</a><br/>
Email: pbarcelo@uc.cl</p></div>
    </content>
    <updated>2021-03-02T15:50:54Z</updated>
    <published>2021-03-02T15:50:54Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-12T10:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1842</id>
    <link href="https://theorydish.blog/2021/03/02/automated-design-of-error-correcting-codes-part-1/" rel="alternate" type="text/html"/>
    <title>Automated Design of Error-Correcting Codes, Part 1</title>
    <summary>Introduction. For nearly a century, error-correcting codes (ECCs) have been used for allowing communication even when the used communication channel is corrupted by noise. Beyond communication, error-correcting codes have found a variety of other uses, from multiclass learning to even showing hardness of approximation. As such, understanding the rich world of error-correcting codes is essential for progress in all of these domains. In our setting, imagine Alice wants to send a message to Bob of length , but the channel between them is corrupted by noise. To overcome this, Alice uses an encoder to turn her message into a longer, redundant string of length . Then, Bob receives this transmission and uses a decoder to (hopefully) recover the original message of length . Two important properties are the rate of the code (essentially what fraction of the transmission is ‚Äúinformation‚Äù) and the bit error rate (BER) which is the (expected) number of decoding errors divided by . Desirable properties are to make the rate as large as possible and the BER as small as possible. Basic ECC paradigm. Since the days of Claude Shannon, many error-correcting codes have been discovered, such as Reed-Solomon codes and BCH codes. Each error-correcting code [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Introduction. </strong>For nearly a century, error-correcting codes (ECCs) have been used for allowing communication even when the used communication channel is corrupted by noise. Beyond communication, error-correcting codes have found a variety of other uses, from<a href="https://en.wikipedia.org/wiki/Multiclass_classification"> multiclass learning</a> to even <a href="https://arxiv.org/pdf/1002.3864.pdf">showing hardness of approximation</a>. As such, understanding the rich world of error-correcting codes is essential for progress in all of these domains.</p>



<p>In our setting, imagine Alice wants to send a message to Bob of length <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, but the channel between them is corrupted by noise. To overcome this, Alice uses an <em>encoder</em> to turn her message into a longer, redundant string of length <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then, Bob receives this transmission and uses a <em>decoder</em> to (hopefully) recover the original message of length <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Two important properties are the <em>rate</em> <img alt="k/n" class="latex" src="https://s0.wp.com/latex.php?latex=k%2Fn&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of the code (essentially what fraction of the transmission is ‚Äúinformation‚Äù) and the <em>bit error rate (BER)</em> which is the (expected) number of decoding errors divided by <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Desirable properties are to make the rate as large as possible and the BER as small as possible.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img alt="" height="353" src="https://lh3.googleusercontent.com/9y96GkfFf6FBCUKoufmsqUGfXt7VmrDqAuCQI1IaOfy4DB-VmJWEvSwL9c1mj9QP9gVYq49haRBI96eNMx5qPpr3BFhhGuWvTv4wixNbLLuTuSnI3xv36xaVa64D3DvshMs_XwmT" width="526"/></figure></div>



<p class="has-text-align-center">Basic ECC paradigm.</p>



<p>Since the days of <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>, many error-correcting codes have been discovered, such as <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction">Reed-Solomon codes</a> and <a href="https://en.wikipedia.org/wiki/BCH_code">BCH codes</a>. Each error-correcting code has its own tradeoffs (e.g., some have higher rate, some are more resistant to special kinds of channel corruptions, etc.). With the large number of ECCs which have been discovered, it can sometimes be overwhelming what the proper error correcting code is for a given application. Further, if the application is sufficiently specialized there may be <em>no </em>known ECC which meets your needs. Such concerns motivate the <em>automation</em> <em>of error correcting codes</em>, which is the main topic of this blog post.¬†</p>



<p>I‚Äôm using the word ‚Äúautomation‚Äù to cover a variety of tasks which various computational methods could assist with in the study of ECCs:</p>



<ol><li>Existence ‚Äî Does the code I want even exist?</li><li>Encoding ‚Äî What is the ‚Äúbest‚Äù way to convert my messages into a code?</li><li>Decoding ‚Äî How do I recover from noisy transmissions?</li><li>Verification ‚Äî Is the proposed ECC design provably correct?</li><li>Selection ‚Äî Which ECC from a given class should I use for a given application?</li></ol>



<p>Each of these facets of the automation of ECCs is a whole field of research! In this and the subsequent post, I will discuss at a high level two types of techniques which have been used to approach these questions: ‚ÄúFormal Methods‚Äù and ‚ÄúMachine Learning.‚Äù We‚Äôll cover formal methods in this post, and in the next post we will cover machine learning methods.</p>



<p><strong>Formal Methods. </strong>The field of Formal Methods strives to give <em>provable guarantees</em> for various computational questions by reducing them to formal logic. Although formal methods are mostly used for software and hardware verification (that is, making sure they are ‚Äúbug free‚Äù), such tools are also used by mathematicians to show the validity of mathematical statements that would be difficult to prove by hand. For example, the <a href="https://en.wikipedia.org/wiki/Kepler%27s_conjecture">Kepler conjecture</a>, a question of what is the best way to pack spheres in three dimensions‚Äìessentially finding an optimal error-correcting code in Euclidean space‚Äìwas only firmly proved by <a href="https://github.com/flyspeck/flyspeck">Thomas Hales and his team</a> through the use of automated theorem-proving tools.</p>



<p>A line of work for using formal methods to directly construct practical ECCs was initiated by <a href="https://ieeexplore.ieee.org/abstract/document/5699220">Shamshiri and Cheng</a> in 2010. In their work, they are motivated by designing error-correcting codes for static random-access memory (SRAM), the kind that is often used for CPU caches. When using SRAM (pictured below), a worry is that cosmic rays could hit some of the bits, causing them to flip. Further, it is not uncommon for a group of consecutive bits to flip. As such, it is desirable to esure that the error correcting code can correct either <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <em>global </em>errors or <img alt="l" class="latex" src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <em>local</em> errors. Correcting global errors is a common property of error correcting codes, such as BCH codes or the <a href="https://en.wikipedia.org/wiki/Binary_Golay_code">Golay code</a>. However, local error correction is a much less common property to guarantee. Thus, the authors use a <em>SAT solver</em> to construct error correcting codes with the properties they desire.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img alt="" height="318" src="https://lh4.googleusercontent.com/qQRzeu2F2XzrjAl6DyPPWAnBVOKOSODIBX4RPBuz0aAMqErYxC2ZyAbtWy3z8ORU4rvMT9UEMI8-C7boHeKEijFwrKY2pRaneEm1lsAoKBUHpQQ1rcMpaLBZs8zAwr2yviMGJPyG" width="529"/></figure></div>



<p class="has-text-align-center">Static random-access memory (source: <a href="https://en.wikipedia.org/wiki/File:Hyundai_RAM_HY6116AP-10.jpg" rel="prettyphoto">Wikipedia</a>)</p>



<p>Assuming that the code to be construction is linear (the encoding map is a linear function over the field <img alt="\mathbb F_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+F_2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>), then the error correcting code can be described by a <a href="https://en.wikipedia.org/wiki/Parity-check_matrix">parity check matrix</a> M in <img alt="{0,1}^{(n-k) \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2C1%7D%5E%7B%28n-k%29+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The key observation the authors make is that for M to be a proper error correcting code, every error pattern <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (i.e., vectors with hamming weight at most <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or consecutive errors in a block of length <img alt="l" class="latex" src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>) must have <img alt="Mp" class="latex" src="https://s0.wp.com/latex.php?latex=Mp&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a distinct vector. For instance¬† <img alt="M(0, 1, 0, 0, 1) \neq M(1,1,1,0,0)" class="latex" src="https://s0.wp.com/latex.php?latex=M%280%2C+1%2C+0%2C+0%2C+1%29+%5Cneq+M%281%2C1%2C1%2C0%2C0%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if <img alt="g = 2" class="latex" src="https://s0.wp.com/latex.php?latex=g+%3D+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="l = 3" class="latex" src="https://s0.wp.com/latex.php?latex=l+%3D+3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>These Boolean constraints can be expressed in conjunctive normal form, i.e., a SAT instance. As such a SAT-solver can be used to determine if there exists a matrix M with the given properties for a given k and n. For instance, they are able to find an error correcting code with parameters <img alt="k = 16, n = 26, g = 2" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3D+16%2C+n+%3D+26%2C+g+%3D+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="l = 4" class="latex" src="https://s0.wp.com/latex.php?latex=l+%3D+4&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. In their follow-up work [<a href="https://ieeexplore.ieee.org/abstract/document/6139156">Shamshi, Ghofrani, Cheng, 2011]</a>, they use this error-correcting code for modeling an ‚Äúon-chip network‚Äù between CPU cores in a multi-core processor<strong>.</strong></p>



<p>Another line of work led by Ben Curtis (see the <a href="https://cs.uwaterloo.ca/~cbright/reports/cacm-preprint.pdf">survey by Curtis, Kotsireas, and Ganesh</a>) has been seeking to construct ECC-like combinatorial objects. An example of such an object is a Hadamard matrix: a square matrix with <img alt="\pm 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpm+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> entries such that every row and column is orthogonal in <img alt="\mathbb R^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. In fact, the authors search for a special type of Hadamard matrix made up of a quartet Williamson matrices which have an intricate algebraic structure. They find these objects by using an algorithm which goes back-and-forth between a SAT solver with a CAS (computer algebraic system) to help narrow the search space.¬†</p>



<p>Formal Methods have further applications in error-correcting codes for <a href="https://ieeexplore.ieee.org/abstract/document/6649704">distributed cloud storage</a> and <a href="https://arxiv.org/pdf/1804.02317.pdf">value-deviation-bounded codes</a>.</p>



<p>This concludes our first post. In the <a href="https://wordpress.com/post/theorydish.blog/1823">next post</a>, we discuss machine learning methods. </p>



<p>Are you aware of other examples or applications of automation to error-correcting codes? If so, please leave a comment.</p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Aviad Rubinstein, Moses Charikar, and Mary Wootters for valuable feedback.¬†</p></div>
    </content>
    <updated>2021-03-02T15:00:00Z</updated>
    <published>2021-03-02T15:00:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Joshua Brakensiek</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-03-12T10:23:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/" rel="alternate" type="text/html"/>
    <title>Associate professor at KTH Royal Institute of Technology (apply by April 15, 2021)</title>
    <summary>KTH Royal Institute of Technology, School of Electrical Engineering and Computer Science has a vacancy for one Associate Professor with specialization in Foundations of Data Science. The position will be permanent and full time, to start as soon as possible. Website: https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/ Email: tenuretrack@eecs.kth.se</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>KTH Royal Institute of Technology, School of Electrical Engineering and Computer Science has a vacancy for one Associate Professor with specialization in Foundations of Data Science. The position will be permanent and full time, to start as soon as possible.</p>
<p>Website: <a href="https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/">https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/</a><br/>
Email: tenuretrack@eecs.kth.se</p></div>
    </content>
    <updated>2021-03-01T21:54:01Z</updated>
    <published>2021-03-01T21:54:01Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-12T10:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/029</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/029" rel="alternate" type="text/html"/>
    <title>TR21-029 |  Public-Coin Statistical Zero-Knowledge Batch Verification against Malicious Verifiers | 

	Inbar Kaslasi, 

	Ron Rothblum, 

	Prashant Nalini Vasudevan</title>
    <summary>Suppose that a problem $\Pi$ has a statistical zero-knowledge (SZK) proof with communication complexity $m$. The question of batch verification for SZK asks whether one can prove that $k$ instances $x_1,\ldots,x_k$ all belong to $\Pi$ with a statistical zero-knowledge proof whose communication complexity is better than $k \cdot m$ (which is the complexity of the trivial solution of executing the original protocol independently on each input).

In a recent work, Kaslasi et al. (TCC, 2020) constructed such a batch verification protocol for any problem having a non-interactive SZK (NISZK) proof-system. Two drawbacks of their result are that their protocol is private-coin and is only zero-knowledge with respect to the honest verifier.

In this work, we eliminate these two drawbacks by constructing a public-coin malicious-verifier SZK protocol for batch verification of NISZK. Similarly to the aforementioned prior work, the communication complexity of our protocol is $\big(k+poly(m) \big) \cdot polylog(k,m)$</summary>
    <updated>2021-03-01T16:38:36Z</updated>
    <published>2021-03-01T16:38:36Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-03-12T10:20:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2021/03/01/beyondlogconcave2/</id>
    <link href="http://offconvex.github.io/2021/03/01/beyondlogconcave2/" rel="alternate" type="text/html"/>
    <title>Beyond log-concave sampling (Part 2)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In our previous <a href="http://www.offconvex.org/2020/09/19/beyondlogconvavesampling">blog post</a>, we introduced the challenges of sampling distributions beyond log-concavity. 
We first introduced the problem of sampling from a distibution $p(x) \propto e^{-f(x)}$ given value or gradient oracle access to $f$, as an analogous problem to black-box optimization with oracle access. We introduced the natural algorithm for sampling in this setup: Langevin Monte Carlo, a Markov Chain reminiscent of noisy gradient descent,</p>

\[x_{t+\eta} = x_t - \eta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I).\]

<p>Finally, we laid out the challenges when $f$ is not convex; in particular, LMC can suffer from slow mixing.</p>

<p>In this and the coming post, we describe two of our recent works tackling this problem. We identify two kinds of structure beyond log-concavity under which we can design provably efficient algorithms:  <em>multi-modality</em> and <em>manifold structure in the level sets</em>. These structures commonly occur in practice, especially in problems involving statistical inference and posterior sampling in generative models.</p>

<p>In this post, we will focus on multimodality, covered by the paper <a href="https://arxiv.org/abs/1812.00793">Simulated tempering Langevin Monte Carlo</a> by Rong Ge, Holden Lee, and Andrej Risteski.</p>

<h1 id="sampling-multimodal-distributions-with-simulated-tempering">Sampling multimodal distributions with simulated tempering</h1>

<p>The classical scenario in which Langevin takes exponentially long to mix is when $p$ is a mixture of two well-separated gaussians. In broadest generality, this was considered by <a href="http://www.ems-ph.org/journals/show_abstract.php?issn=1435-9855%20&amp;vol=6&amp;iss=4&amp;rank=1">Bovier et al. 2004</a> who used tools from metastable processes to show that transitioning from one peak to another can take exponential time. Roughly speaking, they show the transition time is proportional to the ‚Äúenergy barrier‚Äù a particle has to cross. If the gaussians have unit variance and means at distance $2r$, then the probability density at a point midway in between is $\propto e^{-r^2/2}$, and this energy barrier is $\propto e^{r^2/2}$. Thus, the mixing time is exponential. Qualitatively, the intuition for this phenomenon is simple to describe: if started at point A, the drift (i.e. gradient) term will push the walk towards A, so long as it‚Äôs close to the basin around A; hence, to transition from A to B (through C) the Gaussian noise must persistenly counteract the gradient term.</p>

<center>
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_bovier.gif" width="500"/>
</center>

<p>Hence Langevin on its own will not work even in very simple multimodal settings.</p>

<p>In <a href="https://arxiv.org/abs/1812.00793">our paper</a>, we show that combining Langevin Monte Carlo with a temperature-based heuristic called <em>simulated tempering</em> can significantly speed up mixing for multimodal distributions, where the number of modes is not too large, and the modes ‚Äúlook similar.‚Äù</p>

<p>More precisely, we show:</p>

<blockquote>
  <p><strong>Theorem (Ge, Lee, Risteski ‚Äò18, informal)</strong>: If $p(x)$ is a mixture of $k$ shifts of a strongly log-concave distribution in $d$ dimensions (e.g. Gaussian), an algorithm based on simulated tempering and Langevin Monte Carlo that runs in time poly($d,k, 1/\varepsilon$) produces samples from a distribution $\varepsilon$-close to $p$ in total variation distance.</p>
</blockquote>

<p>The main idea is to create a meta-Markov chain (the simulated tempering chain) which has two types of moves: change the current ‚Äútemperature‚Äù of the sample, or move ‚Äúwithin‚Äù a temperature. The main intuition behind this is that at higher temperatures, the distribution is flatter, so the chain explores the landscape faster (see the figure below).</p>

<center> 
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_tempering.gif"/>
</center>

<p>More formally, the distribution at inverse temperature $\beta$ is given by $p_\beta(x) \propto e^{-\beta f(x)}$. The Langevin chain which corresponds to $\beta$ is given by</p>

\[x_{t+\eta} = x_t - \eta \beta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I).\]

<p>As in the figure above, a high temperature (low $\beta&lt;1$) flattens out the distribution and causes the chain to mix faster (top distribution in figure). However, we can‚Äôt merely run Langevin at a higher temperature, because the stationary distribution of the high-temperature chain is wrong: it‚Äôs $p_\beta(x)$. The idea behind simulated tempering is to run Langevin chains at different temperatures, sometimes swapping to another temperature to help lower-temperature chains explore. To maintain the right stationary distributions at each temperature, we use a Metropolis-Hastings filtering step.</p>

<p>More formally, choosing a suitable sequence $0&lt; \beta_1&lt; \cdots &lt;\beta_L=1$, we define the simulated tempering chain as follows.</p>

<p><img src="http://holdenlee.github.io/pics/stl.png" style="float: right;" width="300"/></p>

<ul>
  <li>The <em>state space</em> is a pair of a temperature and location in space $(i, x), i \in [L], x \in \mathbb{R}^d$.<br/>
<!--$L$ copies of the state space (in our case $\mathbb R^d$), one copy for each temperature.--></li>
  <li>The <em>transitions</em> are defined as follows.
    <ul>
      <li>If the current point is $(i,x)$, then <em>evolve</em> $x$ according to Langevin diffusion with inverse temperature $\beta_i$.</li>
      <li>Propose swaps with some rate $\lambda &gt;0$. Proposing a swap means attempting to move to a neighboring chain, i.e. change $i$ to $i‚Äô=i\pm 1$. With probability $\min{p_{i‚Äô}(x)/p_i(x), 1}$, the transition is accepted. Otherwise, stay at the same point. This is a <em>Metropolis-Hastings step</em>; its purpose is to preserve the stationary distribution.</li>
    </ul>
  </li>
</ul>

<p>Finally, it‚Äôs not too hard to see that at the stationary distribution, the samples at the $L$th level ($\beta_L=1$) are the desired samples.</p>

<h2 id="proof-idea-decomposition-theorem">Proof idea: decomposition theorem</h2>

<p>The main strategy is inspired by Madras and Randall‚Äôs <a href="https://www.jstor.org/stable/2699896">Markov chain decomposition theorem</a>, which gives a criterion for a Markov chain to mix rapidly: partition the state space into sets, and show that</p>

<ol>
  <li>The Markov chain mixes rapidly when restricted to each set of the partition.</li>
  <li>The <em>projected</em> Markov Chain, which we define momentarily, mixes rapidly. If there are $m$ sets, the projected chain $\overline M$ is defined on the state space ${1,\ldots, m}$, and transition probabilities are given by average probability flows between the corresponding sets.</li>
</ol>

<p>To implement this strategy, we first have to specify the partition. In fact, we roughly show that there is a partition of $[L] \times \mathbb{R}^d$ in which:</p>

<ol>
  <li>The simulated tempering Langevin chain mixes fast within each of the sets.</li>
  <li>The ‚Äúvolume‚Äù of the sets (under the stationary distribution of the tempering chain) is not too small.
<!-- [HL: alt.] There is no set at high temperature that has much larger volume at low temperature.
 --></li>
</ol>

<p>In applying the Madras-Randall framework with this partition, it‚Äôs clear that point (1) above satisfies requirement (1) for the framework; point (2) ensures that the projected Markov chain has no ‚Äúbottlenecks‚Äù and hence that it mixes rapidly (requirement (2)). More precisely, we can show rapid mixing either through the method of canonical paths or Cheeger‚Äôs inequality. To do this, we exhibit a ‚Äúgood-probability‚Äù path between any two sets in the partition, going through the highest temperature.</p>

<p>The intuition for why this path works is illustrated in the figure below: when transitioning from the set corresponding to the left mode at level $L$ to the right mode at level $L$, each of the steps up/down the temperatures are accepted with good probability if the neighboring temperatures are not too different; at the highest temperature, the chain mixes fast by point (1), and since each of the sets are not too small by point (2), there is a reasonable probability to end at the right mode at the highest temperature.</p>

<center>
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_conductance.gif"/>
</center>

<!--(rework this picture?) This is a Markov chain with a small state space, so its spectral gap is easy to lower-bound (e.g., with Cheeger's inequality). The one thing we need to check is that there is no "bottleneck," i.e., one set in the partition that has low probability at high temperature and high probability at low temperature. -->

<p>Intuitively, the partition should track the ‚Äúmodes‚Äù of the distribution, but a technical hurdle in implementing this plan is in defining the partition when the modes overlap. One can either do this spectrally (i.e. showing that the Langevin chain has a spectral gap, and use theorems about <a href="https://arxiv.org/abs/1309.3223">spectral graph partitioning</a>, as we did in the <a href="https://arxiv.org/abs/1710.02736">first version</a> of the paper), or use a functional ‚Äúsoft decomposition theorem‚Äù which is a more flexible version of the classical decomposition theorem, which we use in a <a href="https://arxiv.org/abs/1812.00793">later version</a> of the paper.</p>

<!-- ![](http://holdenlee.github.io/pics/proj_chain.png)--></div>
    </summary>
    <updated>2021-03-01T14:00:00Z</updated>
    <published>2021-03-01T14:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2021-03-11T22:42:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/</id>
    <link href="https://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/" rel="alternate" type="text/html"/>
    <title>PhD. Thesis at LAMSADE (Paris Dauphine) (apply by April 30, 2021)</title>
    <summary>PhD. Thesis offer in Paris Dauphine University ‚ÄúAlgorithmic aspects of intersection graphs‚Äù Website: https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf Email: florian.sikora@dauphine.fr</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>PhD. Thesis offer in Paris Dauphine University ‚ÄúAlgorithmic aspects of intersection graphs‚Äù</p>
<p>Website: <a href="https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf">https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf</a><br/>
Email: florian.sikora@dauphine.fr</p></div>
    </content>
    <updated>2021-03-01T10:35:30Z</updated>
    <published>2021-03-01T10:35:30Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-03-12T10:20:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/</id>
    <link href="https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/" rel="alternate" type="text/html"/>
    <title>Good-case Latency of Byzantine Broadcast: a Complete Categorization</title>
    <summary>Guest post by Zhuolun Xiang State Machine Replication and Broadcast Many existing permission blockchains are built using Byzantine fault-tolerant state machine replication (BFT SMR), which ensures all honest replicas agree on the same sequence of client inputs. Most of the practical solutions for BFT SMR are based on the Primary-Backup...</summary>
    <updated>2021-02-28T18:07:00Z</updated>
    <published>2021-02-28T18:07:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2021-03-11T22:43:02Z</updated>
    </source>
  </entry>
</feed>
