<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-07-31T11:22:30Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18993</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/" rel="alternate" type="text/html"/>
    <title>Pandemic Lag</title>
    <summary>In chess ratings and what other measures of cognitive development? src Henri Didon was a French priest and promoter of youth sports in the late 1800s. He coined the phrase Citius, Altius, Fortius, meaning faster-higher-stronger, which became the motto of the Olympic Games between their reinception in 1896 and its proclamation when the Games were […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>In chess ratings and what other measures of cognitive development?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/didonbook/" rel="attachment wp-att-18995"><img alt="" class="alignright wp-image-18995" height="222" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/DidonBook.jpg?resize=160%2C222&amp;ssl=1" width="160"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.amazon.com/Influence-morale-sports-athl%C3%A9tiques-French-ebook/dp/B00YOIOKQY">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Henri Didon was a French priest and promoter of youth sports in the late 1800s. He coined the phrase <em>Citius, Altius, Fortius</em>, meaning <em>faster-higher-stronger</em>, which became the motto of the Olympic Games between their reinception in 1896 and its proclamation when the Games were held in Paris in 1924. For the 2020 Games being held now in 2021 they have added the word <em>Communiter</em>, meaning <em>together</em>, which is said to express solidarity during the pandemic. </p>
<p>
Today we review how the official measure of being faster, higher, and stronger at chess has been impacted by the pandemic. </p>
<p>
Didon spoke of his words as “the foundation and <em>raison d’être</em> of athletics” amid the progress of humanity. They have been borne out by the steady progression of athletic records over the Games’ 125-year history. Whether the Tokyo Games will continue that trend is open. Besides the year delay and the pandemic’s impact on qualifying competitions and athletic conditioning in general, there has emerged a question of mental effects amid the lack of spectators and straitened atmosphere. The one example I’ll quote is the <a href="https://www.huffpost.com/entry/kristof-milak-tokyo-olympics_n_61013f01e4b00fa7af7db9fb">claim</a> by the Hungarian swimmer Kristof Milak that a pre-race mishap with his favorite swimming trunks cost him a record in an event he still won:</p>
<blockquote><p><b> </b> <em> “They split 10 minutes before I entered the pool and in that moment I knew the world record was gone. I lost my focus and knew I couldn’t do it.” </em>
</p></blockquote>
<p>
At least the means of measuring athletic performances have not been disrupted. For <b>psychometrics</b>—a word <a href="https://www.morgan.edu/psychology/psychometrics">meaning</a> <em>the science of measuring mental capacities and processes</em>—the standardized tests most often used to measure aptitude have themselves been curtailed. This makes all the more open the question of how our youth have progressed during the pandemic in education on the whole. We will examine the special case of chess, where the official instrument has been almost entirely frozen for 15 months, but my own work carries both the ability and the responsibility to make up the difference.</p>
<p>
</p><p/><h2> Chess Ratings and Lag </h2><p/>
<p/><p>
The <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo</a> rating system is simple but accurate enough for use by sporting federations besides chess. In chess, 1000 is a typical rating for a novice player, 1600 means a good club player, 2200 is the threshold for “master,” and 2800 is world championship standard. A player’s rating measures skill in a way that the <em>difference</em> to the opponent’s rating yields probabilities by which to predict the outcomes of games between them. Elo is the main prediction engine of <a href="https://fivethirtyeight.com/">FiveThirtyEight</a> for <a href="https://fivethirtyeight.com/features/how-we-calculate-nba-elo-ratings/">basketball</a>, <a href="https://projects.fivethirtyeight.com/complete-history-of-mlb/">baseball</a>, and <a href="https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/">football</a> (but not <a href="https://fivethirtyeight.com/methodology/how-our-club-soccer-predictions-work/">soccer</a>). </p>
<p>
Although the prediction formula uses only differences, so that an additive shift in all ratings would not affect the chances, I have shown that the ratings administered by the International Chess Federation (FIDE) have stayed stable in absolute regard to the objective quality of moves played as measured by my own predictive model, via my Intrinsic Performance Ratings (IPRs) geared to the FIDE rating scale. Having stable numbers is vital not only to my cheating tests but to the public understanding of the system on the whole.  This goes for FIDE, for Internet gaming federations, and even for the use of Elo by <a href="https://web.archive.org/web/20170819190821/https://killscreen.com/articles/tinder-matchmaking-is-more-like-warcraft-than-you-might-think/">Tinder</a>. </p>
<p>
Thus it is all the more sad for me to see things like this happen not only to FIDE’s Elo ratings but also those of the US Chess Federation (USCF), who adopted Arpad Elo’s formulas in the 1950s:</p>
<p/><p><br/>
<a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/anniewangratinggraphann2/" rel="attachment wp-att-19013"><img alt="" class="aligncenter wp-image-19013" height="256" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AnnieWangRatingGraphann2.jpg?resize=527%2C256&amp;ssl=1" width="527"/></a></p>
<p/><p><br/>
This is the FIDE Rating Progress <a href="https://ratings.fide.com/profile/2053900/chart">Chart</a> of Annie Wang, who just won the US Junior Women’s Championship played in-person at the <a href="https://saintlouischessclub.org/">Saint Louis Chess Club</a> last week. Her FIDE rating has been stuck at <b>2384</b> ever since the April 2020 rating list. One glance at the chart suffices to project her rating into the neighborhood of 2500 by now. Her USCF rating is closer at 2457, but this is offset by a long-known inflation of USCF ratings relative to FIDE, <a href="https://chessgoals.com/rating-comparison-old/">measured</a> about 75 points at that level in May 2020. Wang’s USCF rating has been similarly frozen. You can find the same for a plethora of young players down to aspiring kids of single-digit age blasting out of three-digit ratings, as Wang did.  They have a flat line like the ones circled in blue, but located where she had a sharp rise (circled in green):</p>
<p/><p><br/>
<a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/anniewangratinguscfann/" rel="attachment wp-att-18998"><img alt="" class="aligncenter wp-image-18998" height="280" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AnnieWangRatingUSCFann.jpg?resize=460%2C280&amp;ssl=1" width="460"/></a></p>
<p/><h2> The Need to Adjust </h2><p/>
<p/><p>
The lag mattered immediately for me as I gave daily statistical reports to the tournament’s chief arbiter last week. Using Wang’s official rating would have underestimated her true strength and biased my reports in the direction of false positives. Instead, having developed a formula that I won’t claim is anything more than <a href="https://en.wikipedia.org/wiki/Fermi_problem">Fermi</a>–<a href="https://brilliant.org/wiki/fermi-estimate/">estimated</a>, I calculated her effective FIDE rating as <b>2482</b>, adding almost 100 points. I would have upped her USCF rating to 2543 by the same formula. </p>
<p>
Wang was both the highest rated among the ten competitors and the oldest, with a long enough record of international play to have her FIDE <a href="https://en.wikipedia.org/wiki/Elo_rating_system#Most_accurate_K-factor">K-factor</a> reduced from 40 to 20. My formula adds more points for lower ratings, higher K-factor, and younger age—all reflecting the arc of many improving junior players. My average increase to the women’s ratings was <b>199.1</b> points, versus <b>57.4</b> to the ten players in the junior men’s/mixed championship, who had mostly higher ratings to begin with. </p>
<p>
Also playing in St. Louis were ten in the US Senior Championship, including last year’s winner Joel Benjamin, whom I knew and played in the 1970s when we were kids. Their ratings have been likewise frozen. Rating points in chess are zero-sum, so the triple-digit gains I have credited to the young would in normal reality have been taken out of other players—most plausibly, us geezers. There are more of us than keen juniors, so the presumed individual losses would be less. </p>
<p>
Did that prove out? My IPRs furnish a way to verify. They differ from other deployed quality metrics by organically involving the difficulty of the positions a player faces, in several ways besides the complexity and temptation factors I <a href="https://rjlipton.wpcomstaging.com/2019/08/15/predicting-chess-and-horses/">incorporated</a> two years ago. Here are the results—but bear in mind that these three 10-player tournaments are small data: their two-sigma error bars on the average IPRs are about <img alt="{\pm 80}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+80%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Elo points.</p>
<ul>
<li>
<b>US Jr. W</b>: Avg. rating 2101, adjusted <b>2300</b>, avg. IPR <b>2337</b> (+37). <p/>
</li><li>
<b>US Jr. M</b>: Avg. rating 2492, adjusted <b>2550</b>, avg. IPR <b>2527</b> (-23). <p/>
</li><li>
<b>US Sr. M</b>: Avg. rating <b>2494</b> (no adjustment), avg. IPR <b>2459</b> (-35).
</li></ul>
<p>
The truly significant result is that the women performed much closer to my adjustment than to their official ratings. The men were only slightly closer amid general insignificance, which applies also to the seniors. The juniors combined were highly close to my projections. </p>
<p>
Right now I am gathering data from larger Open tournaments in this first month of widespread in-person play. There have been some hits and misses, and I have not yet evaluated all (un-)controllable factors. But gathering the original large data for my adjustment formula required coping with a major factor: the 100–200x higher evident cheating rate I’ve observed in online chess.</p>
<p>
</p><p/><h2> How To Be Not Very Wrong </h2><p/>
<p/><p>
I first perceived the phenomenon when monitoring the European Youth Online Rapid Chess Championship last September. I compiled full analysis on all 689 competitors in women’s and men’s/mixed sections ranging from Under-12 to Under-18. Besides four particular cases, my results said that probably at least four of another five were cheating, but without the confidence needed to flag any one. Removing the high outliers did not, however, equate either the IPRs or my sharper test of conformance to the bell curve to my projections. The Under-12 M and W and Under-14 M sections had IPRs averaging 83, 235, and 125 higher, respectively. The Under-14 W and U16 and U18 sections were close to my projections, so I did not suspect general modeling issues. </p>
<p>
The online World Youth Rapid Championships in November-December, which added an under-10 division, brought the lag phenomenon out in force, on all continents. The correction I postulated even before that tournament finished was:</p>
<blockquote><p><b> </b> <em> 15 Elo <img alt="{\times}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctimes%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> (months since April 2020), higher for those under 13 (50% to 2x higher). </em>
</p></blockquote>
<p/><p>
There are several reasons I have not tried to be more precise. There is uncertainty about how many high outliers to remove, about faster time controls, and about <a href="https://en.chessbase.com/post/why-do-some-countries-always-gain-and-other-always-lose-rating-points">geographical</a> drifts in ratings. The effect depends on how much a junior player is disposed to improve in the first place; I found it absent in the lower divisions of the UK’s junior leagues played online last winter. In an individual cheating case I take a more-particular fix on the appropriate rating. What the equation is for is <em>to show the fairness of my baseline relative to the field on the whole</em>. There are also non-cheating purposes, which should come to the fore as FIDE and other federations emerge from the pandemic, and which I discuss next. </p>
<p>
I have been using essentially this formula ever since. From large scholastic tournaments across the globe this spring, I settled on fixing the adjustment for those with birth year 2008 or later as 25 Elo <img alt="{\times}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctimes%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (months since April 2020). For players with official rating <img alt="{R &gt; 2000}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%3E+2000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> I apply the rough multiplier <img alt="{(3000 - R)/1000}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%283000+-+R%29%2F1000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and for those with <img alt="{K &lt; 40}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK+%3C+40%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> I (also) multiply by <img alt="{\sqrt{K/40}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7BK%2F40%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
I won’t claim the ’15’ and ’25’ are right, compared to multipliers that are 1 or 2 higher or lower. But the results I have been getting all year say that my 15 and 25 are most often closer than factors of 10 or 20 or 30 would be. In almost all cases, like for the US Jr. W above, my pre-set rating calibration has come an order of magnitude closer to the IPR verification than the adjustments themselves. Taking a cue from the title of Jordan Ellenberg’s <a href="https://en.wikipedia.org/wiki/How_Not_to_Be_Wrong">predecessor</a> to his book I <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">previewed</a> last month, my main concern is to be not very wrong.</p>
<p>
</p><p/><h2> A Dilemma Moving Forward </h2><p/>
<p/><p>
Providing an accurate and stable rating system has long been recognized as a prime service of FIDE. A legal dimension has been added insofar as evaluating cheating allegations requires a prior assessment of the natural skill of the accused player. The pandemic has made me take over much of the latter responsibility, but the former presents a wider dilemma doubtless faced in some form by other impacted sporting federations and educational assessment agencies on the whole:</p>
<blockquote><p><b> </b> <em> Is it a higher responsibility to provide the most accurate assessment of current ability obtainable now, or to maintain continuity of the official assessment mechanism? </em>
</p></blockquote>
<p/><p>
I could go even wider to analogize this to the US Census debate over whether estimations, presuming demonstration of their greater accuracy, should be used in preference to the conducted count. The latter is enshrined in the US Constitution, while the principle that chess rating points should be won or lost only in actual combat is similarly <a href="https://handbook.fide.com/chapter/B022017">hallowed</a>. But I have certainly “demonstrated the obvious”: that the current official ratings of almost all the keenest young players are very wrong.</p>
<p>
Mathematically, the rating system <em>will</em> re-establish equilibrium if the current discrepancy is left alone. The trouble is that the mathematical nature of the update and the relative paucity of chess games also guarantees that the process will be <em>slow</em>, measured in years. FiveThirtyEight has remarked in <a href="https://fivethirtyeight.com/features/60-games-arent-enough-to-crown-the-best-mlb-team-but-neither-are-162-games/">several</a> <a href="https://fivethirtyeight.com/features/no-mlb-team-is-great-and-fewer-are-awful-is-this-the-parity-we-wanted/">recent</a> <a href="https://fivethirtyeight.com/features/bad-teams-may-be-posing-as-good-teams-in-a-60-game-baseball-season/">article</a> about the long update times in baseball as measured by Elo ratings. My cheating tests often cannot wait a day.  I have to use my cross-check and validation features to detect and remove a huge amount of mathematically the same kind of bias believed to afflict other currently-deployed predictive models less transparently.</p>
<p>
There is precedent for a large-scale adjustment of ratings by FIDE. Women’s chess used to be even more segregated from men than today. In 1986, Arpad Elo himself—as secretary of FIDE’s Qualifications Commission—<a href="http://www.anusha.com/elo.htm">reported</a> that women’s ratings had drifted down by about “one half of a class interval.” FIDE added 100 points to the rating of every active female player except Susan Polgar, whose rating was already ‘well-mixed’ according to the report, since she had faced many more male players than the others.</p>
<p>
Attempting to resolve that historical controversy by computing IPRs for Polgar and the other players in Elo’s study has never reached my front burner. But the point remains that my work is uniquely capable of informing the state of ratings in a radical manner. The pandemic has created both a need and an opportunity for a reset that could also solve other issues previously noted—while ensuring that ratings on all continents are on a common scale.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
How pronounced is the lag of assessment in education and other competitive arenas, both physical and in mind-sports?</p>
<p>
I had not noticed that Tyler Cowen had already used the term “psychometric test” in a <a href="https://marginalrevolution.com/marginalrevolution/2020/03/the-world-is-running-a-disturbing-psychometric-test.html">post</a> on the <em>Marginal Revolution</em> blog at the beginning of the pandemic, until he <a href="https://marginalrevolution.com/marginalrevolution/2021/07/the-great-psychometric-test-continues.html">repeated</a> it just today.</p>
<p>
I have hinted at some other issues in chess but stopped short of addressing them. One is whether online play—where play at 5-minute “Blitz” down to 1-minute “Bullet” time controls predominates even over “Rapid” beginning at 10 minutes—has a similar effect on development in the absence of any in-person “Classical” chess. Another is whether the observed increase in the ranks of players with 2700+ elite ratings is really <em>Fortius</em> or merely rating <em>inflation</em>. A third is whether the current conditions for in-person chess will last long enough to get a good fix on the ‘post-pandemic’ state of skill, and a fourth—coming back to what I quoted about the current Olympics—is whether they are truly “normal” enough even now.</p>
<p/><p><br/>
[changed first figure to show the March 2020 pandemic start accurately; some minor word changes.]</p></font></font></div>
    </content>
    <updated>2021-07-30T20:35:00Z</updated>
    <published>2021-07-30T20:35:00Z</published>
    <category term="All Posts"/>
    <category term="chess"/>
    <category term="detection"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Annie Wang"/>
    <category term="cheating"/>
    <category term="cognitive assessment"/>
    <category term="computational bias"/>
    <category term="education"/>
    <category term="Elo ratings"/>
    <category term="Henri Didon"/>
    <category term="Olympics"/>
    <category term="pandemic"/>
    <category term="predictive modeling"/>
    <category term="psychometrics"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-07-31T11:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2324</id>
    <link href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/" rel="alternate" type="text/html"/>
    <title>Average-Case Fine-Grained Hardness, Part II</title>
    <summary>In the previous post, we did two examples of proving average-case fine-grained hardness via worst-case to average-case reductions. In this post, I want to continue the discussion of counting -cliques (in particular, on Erdős–Rényi graphs) to showcase a new technique, which builds on the general recipe and the example of counting -cliques described in the previous post. In the next post, I will discuss how this new technique can be applied to some other combinatorial problems. Counting -cliques in Erdős–Rényi graph. A strong follow-up [BBB19] of the result [GR18] we discussed in the previous post shows that there is an -time reduction from counting -cliques in any -vertex graph to counting -cliques with error probability in Erdős–Rényi graph (whereas the sampable distribution of the random graph in [GR18] is somewhat unnatural). The key idea is a decomposition lemma which says for sufficiently large prime and , for any constants (for our application, these constants will be equal), given independent Bernoulli random variables , the distribution of is close to the uniform distribution over , i.e., the statistical distance is less than (later when we apply this lemma, we want to be , and therefore ). We skip the proof of [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, we did two examples of proving average-case fine-grained hardness via worst-case to average-case reductions. In this post, I want to continue the discussion of counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques (in particular, on <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">Erdős–Rényi graphs</a>) to showcase a new technique, which builds on the general recipe and the example of counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques described in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>. In the next post, I will discuss how this new technique can be applied to some other combinatorial problems.</p>



<p><strong>Counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in Erdős–Rényi graph.</strong> A strong follow-up <a href="https://arxiv.org/abs/1903.08247">[BBB19]</a> of the result <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> we discussed in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> shows that there is an <img alt="\widetilde{O}(n^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-time reduction from counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in any <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-vertex graph to counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques with error probability <img alt="&lt;\frac{1}{\log^{O(1)} n}" class="latex" src="https://s0.wp.com/latex.php?latex=%3C%5Cfrac%7B1%7D%7B%5Clog%5E%7BO%281%29%7D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in Erdős–Rényi graph (whereas the sampable distribution of the random graph in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> is somewhat unnatural). The key idea is a decomposition lemma which says for sufficiently large prime <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="k=O(\log(p)\log(p/\varepsilon))" class="latex" src="https://s0.wp.com/latex.php?latex=k%3DO%28%5Clog%28p%29%5Clog%28p%2F%5Cvarepsilon%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, for any constants <img alt="0&lt;p^{(1)},\dots,p^{(k)}&lt;1" class="latex" src="https://s0.wp.com/latex.php?latex=0%3Cp%5E%7B%281%29%7D%2C%5Cdots%2Cp%5E%7B%28k%29%7D%3C1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (for our application, these constants will be equal), given independent Bernoulli random variables <img alt="y_{\ell}\sim\textrm{Bern}(p^{(\ell)})" class="latex" src="https://s0.wp.com/latex.php?latex=y_%7B%5Cell%7D%5Csim%5Ctextrm%7BBern%7D%28p%5E%7B%28%5Cell%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, the distribution of <img alt="\sum_{\ell=0}^k 2^{\ell}y_{\ell}\mod p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7B%5Cell%7D%5Cmod+p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is close to the uniform distribution over <img alt="\mathbf{F}_{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, i.e., the statistical distance is less than <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (later when we apply this lemma, we want <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be <img alt="1/\textrm{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and therefore <img alt="k=O(\log p\cdot(\log p+\log n))" class="latex" src="https://s0.wp.com/latex.php?latex=k%3DO%28%5Clog+p%5Ccdot%28%5Clog+p%2B%5Clog+n%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> ). We skip the proof of this lemma which is a nice application of basic Fourier analysis.</p>



<p>As in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> denotes a constructed polynomial that computes the number of <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques when the input is an adjacency matrix of a graph. The step 3 of the general recipe from the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> reduces counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques for the worst-case graph to evaluating <img alt="f_{t\textrm{-clique}}(Y)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28Y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on <img alt="d+1" class="latex" src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> many uniformly random <img alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (recall in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the degree of the polynomial <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="O(t\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> after the Chinese remaindering trick). Based on the decomposition lemma, using standard sampling scheme (this is essentially <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a>, which I will not go into the details, but I just want to mention that we would like the <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the decomposition lemma to be <img alt="1/\textrm{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that the sampling scheme succeeds w.h.p. by a few attempts), we can further reduce evaluating <img alt="f_{t\textrm{-clique}}(Y)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28Y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on uniformly random <img alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to evaluating <img alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where each <img alt="Y_{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a random 0-1 valued matrix that is statistically close to the adjacency matrix of an Erdős–Rényi graph. Now, if we can “pull out” the weighted sum in <img alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then we are done, because evaluating <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on the adjacency matrix of an Erdős–Rényi graph is precisely counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques for Erdős–Rényi graph.</p>



<p>When can we “pull out” the weighted sum for a polynomial <img alt="f(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? One answer is when the polynomial is <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite.</p>



<p>An <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-variate polynomial <img alt="f(x_1,\dots,x_m)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x_1%2C%5Cdots%2Cx_m%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite if there is a partition of the set of variables <img alt="\dot{\bigcup}_{j\in[d]} S_j=[m]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdot%7B%5Cbigcup%7D_%7Bj%5Cin%5Bd%5D%7D+S_j%3D%5Bm%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the sum of monomials in which each monomial contains exactly one variable from each part <img alt="S_j" class="latex" src="https://s0.wp.com/latex.php?latex=S_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (more formally, <img alt="f(x_1,\dots,x_m)=\sum_{(i_1,i_2,\dots,i_d)\in S}\prod_{j\in[d]}x_{i_j}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x_1%2C%5Cdots%2Cx_m%29%3D%5Csum_%7B%28i_1%2Ci_2%2C%5Cdots%2Ci_d%29%5Cin+S%7D%5Cprod_%7Bj%5Cin%5Bd%5D%7Dx_%7Bi_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="S\subseteq S_1\times S_2\times\dots\times S_d" class="latex" src="https://s0.wp.com/latex.php?latex=S%5Csubseteq+S_1%5Ctimes+S_2%5Ctimes%5Cdots%5Ctimes+S_d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>).</p>



<p>For <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite polynomial <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, it is not hard to show that<br/><img alt="f(\sum_{\ell=0}^k 2^{\ell}y_{1,\ell},\dots,\sum_{\ell=0}^k 2^{\ell}y_{m,\ell})=\sum_{\ell_1=0}^k\sum_{\ell_2=0}^k\dots\sum_{\ell_d=0}^k 2^{\ell_1+\dots+\ell_d}\cdot f(y_{1,\ell_1},\dots,y_{m,\ell_m})." class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7B1%2C%5Cell%7D%2C%5Cdots%2C%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7Bm%2C%5Cell%7D%29%3D%5Csum_%7B%5Cell_1%3D0%7D%5Ek%5Csum_%7B%5Cell_2%3D0%7D%5Ek%5Cdots%5Csum_%7B%5Cell_d%3D0%7D%5Ek+2%5E%7B%5Cell_1%2B%5Cdots%2B%5Cell_d%7D%5Ccdot+f%28y_%7B1%2C%5Cell_1%7D%2C%5Cdots%2Cy_%7Bm%2C%5Cell_m%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/>(Essentially, because two variables from the same <img alt="S_i" class="latex" src="https://s0.wp.com/latex.php?latex=S_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> never appear in the same monomial, we can enumerate variables from the same <img alt="S_i" class="latex" src="https://s0.wp.com/latex.php?latex=S_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the same order.)</p>



<p>Let us think of each <img alt="y_{i,\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=y_%7Bi%2C%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as a coordinate of Erdős–Rényi adjacency matrix <img alt="Y_{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then <img alt="f(y_{1,\ell_1},\dots,y_{m,\ell_m})" class="latex" src="https://s0.wp.com/latex.php?latex=f%28y_%7B1%2C%5Cell_1%7D%2C%5Cdots%2Cy_%7Bm%2C%5Cell_m%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is evaluating <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on an ensemble of distinct coordinates of <img alt="Y_{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>‘s, and such ensemble is obviously Erdős–Rényi as well. Therefore, we have managed to decompose <img alt="f(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> into sum of <img alt="k^d" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> many <img alt="f(Y^{(\ell_1,\dots,\ell_d)})" class="latex" src="https://s0.wp.com/latex.php?latex=f%28Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>‘s where each <img alt="Y^{(\ell_1,\dots,\ell_d)}" class="latex" src="https://s0.wp.com/latex.php?latex=Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> denotes an Erdős–Rényi adjacency matrix. In the next paragraph, we will construct a <img alt="d=\binom{t}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite polynomial <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques, and therefore, we have reduced computing <img alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to computing <img alt="k^{\binom{t}{2}}=\log^{O(1)} n" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7B%5Cbinom%7Bt%7D%7B2%7D%7D%3D%5Clog%5E%7BO%281%29%7D+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> many <img alt="f_{t\textrm{-clique}} (Y^{(\ell_1,\dots,\ell_d)})" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D+%28Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>‘s, which is a mild blow-up of the number of the random instances which the reduction needs to solve. (In general, we consider the reduction to be efficient when the number of random instances it needs to solve is <img alt="n^{o(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and hence, as long as <img alt="d=o(\log n/\log \log n)" class="latex" src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog+n%2F%5Clog+%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we are good to go.)</p>



<p>Unfortunately, the <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> given in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> does not work. Instead, we first reduce counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in any graph to counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in a <a href="https://en.wikipedia.org/wiki/Multipartite_graph"><img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite graph</a>, and then we construct a <img alt="\binom{t}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite polynomial <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in a <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite graph. Reduction from counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in any graph to counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in a <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite graph is standard. Simply consider the <a href="https://en.wikipedia.org/wiki/Tensor_product_of_graphs">tensor product</a> between the original graph and another <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-clique. The number of <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in the tensor product graph is exactly <img alt="t!" class="latex" src="https://s0.wp.com/latex.php?latex=t%21&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> times that in the original graph. Now given the <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-paritite graph, let <img alt="V_i" class="latex" src="https://s0.wp.com/latex.php?latex=V_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> denote the <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-th part of vertices, and let <img alt="X^{(i,j)}" class="latex" src="https://s0.wp.com/latex.php?latex=X%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (for <img alt="i&lt;j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Cj&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>) denote the adjacency matrix between <img alt="V_i" class="latex" src="https://s0.wp.com/latex.php?latex=V_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="V_j" class="latex" src="https://s0.wp.com/latex.php?latex=V_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Consider the new polynomial <img alt="f_{t\textrm{-clique}}(X^{(1,2)},X^{(1,3)},\dots,X^{(t-1,t)}):=\sum_{v_1\in V_1}\sum_{v_2\in V_2}\dots\sum_{v_t\in V_t}\prod_{(i,j)\in\binom{[t]}{2}} X_{v_i,v_j}^{(i,j)}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%5E%7B%281%2C2%29%7D%2CX%5E%7B%281%2C3%29%7D%2C%5Cdots%2CX%5E%7B%28t-1%2Ct%29%7D%29%3A%3D%5Csum_%7Bv_1%5Cin+V_1%7D%5Csum_%7Bv_2%5Cin+V_2%7D%5Cdots%5Csum_%7Bv_t%5Cin+V_t%7D%5Cprod_%7B%28i%2Cj%29%5Cin%5Cbinom%7B%5Bt%5D%7D%7B2%7D%7D+X_%7Bv_i%2Cv_j%7D%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="X_{v_i,v_j}^{(i,j)}" class="latex" src="https://s0.wp.com/latex.php?latex=X_%7Bv_i%2Cv_j%7D%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> denotes the coordinate that indicates if there is an edge between <img alt="v_i,v_j" class="latex" src="https://s0.wp.com/latex.php?latex=v_i%2Cv_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Observe that <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> counts <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques by picking one vertex for each part and checking if these <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> vertices form a clique. It is indeed <img alt="\binom{t}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite as each <img alt="X^{(i,j)}" class="latex" src="https://s0.wp.com/latex.php?latex=X%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> corresponds to a part of variables.</p>



<p><strong>New recipe for worst-case to average-case reductions.</strong> Let us take a minute to think about what structural properties of counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques we have used in the entire reduction except for constructing <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>The answer is none! The only part specific to counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques was cooking up a <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-partite polynomial for <img alt="d=o(\log n/\log \log n)" class="latex" src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog+n%2F%5Clog+%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (let us call such polynomial “good”) that encodes this problem. The reduction we showed above works as long as we can construct such “good” polynomial for a problem. Therefore, a new recipe, which was explicitly formulated in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a> for proving average-case (here “average-case” means Erdős–Rényi random input model) fine-grained hardness for a problem <img alt="L:\{0,1\}^n\to\mathbf{Z}_{\ge 0}\cap \textrm{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=L%3A%5C%7B0%2C1%5C%7D%5En%5Cto%5Cmathbf%7BZ%7D_%7B%5Cge+0%7D%5Ccap+%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in P, is </p>



<ol><li>Construct a “good” polynomial <img alt="f_{L}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on <img alt="\mathbf{F}_p^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_p%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="p=\textrm{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3D%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, such that <img alt="f_{L}(x)=L(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for all <img alt="x\in\{0,1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</li></ol>



<p>Short and sweet.</p>



<p>In the final post, I will present another instantiation of this new recipe.</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk.</p></div>
    </content>
    <updated>2021-07-30T14:50:15Z</updated>
    <published>2021-07-30T14:50:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Junyao Zhao</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-07-31T11:21:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/07/30/postdoc-at-ben-gurion-university-apply-by-december-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/07/30/postdoc-at-ben-gurion-university-apply-by-december-31-2021/" rel="alternate" type="text/html"/>
    <title>POSTDOC at Ben-Gurion University (apply by December 31, 2021)</title>
    <summary>My group at Ben-Gurion University has an open postdoctoral position, part of an ERC Starting Grant project. Starting date, as well as duration, are flexible. The position includes a generous salary, as well as funding for equipment and travel. Website: https://www.cs.bgu.ac.il/~klim/Links/Call Email: klim@bgu.ac.il</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My group at Ben-Gurion University has an open postdoctoral position, part of an ERC Starting Grant project. Starting date, as well as duration, are flexible.<br/>
The position includes a generous salary, as well as funding for equipment and travel.</p>
<p>Website: <a href="https://www.cs.bgu.ac.il/~klim/Links/Call">https://www.cs.bgu.ac.il/~klim/Links/Call</a><br/>
Email: klim@bgu.ac.il</p></div>
    </content>
    <updated>2021-07-30T12:42:00Z</updated>
    <published>2021-07-30T12:42:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-07-31T11:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/112</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/112" rel="alternate" type="text/html"/>
    <title>TR21-112 |  CNF Satisfiability in a Subspace and Related Problems | 

	Vikraman Arvind, 

	Venkatesan Guruswami</title>
    <summary>We introduce the problem of finding a satisfying assignment to a CNF formula that must further belong to a prescribed input subspace. Equivalent formulations of the problem include finding a point outside a union of subspaces (the Union-of-Subspace Avoidance (USA) problem), and finding a common zero of a system of polynomials over the field of two elements each of which is a product of affine forms.
    
We focus on the case of $k$-CNF formulas (the $k$-SUB-SAT problem). Clearly, $k$-SUB-SAT is no easier than $k$-SAT, and might be harder. Indeed, via simple reductions we show that 2-SUB-SAT is NP-hard, and W[1]-hard when parameterized by the co-dimension of the subspace. We also prove that the optimization version Max-2-SUB-SAT is NP-hard to approximate better than the trivial $3/4$ ratio even on satisfiable instances.
    
On the algorithmic front, we investigate fast exponential algorithms which give non-trivial savings over brute-force algorithms. We give a simple branching algorithm with runtime $(1.5)^r$ for 2-SUB-SAT, where $r$ is the subspace dimension, as well as a $(1.4312)^n$ time algorithm where $n$ is the number of variables.

Turning to $k$-SUB-SAT for $k \ge 3$, while known algorithms for solving a system of degree $k$ polynomial equations already imply a solution with runtime $\approx 2^{r(1-1/2k)}$, we explore a more combinatorial approach.  Based on an analysis of critical variables (a key notion underlying the randomized $k$-SAT algorithm of Paturi, Pudlak, and Zane), we give an algorithm with runtime $\approx {n\choose {\le t}} 2^{n-n/k}$ where $n$ is the number of variables and $t$ is the co-dimension of the subspace. This improves upon the runtime of the polynomial equations approach for small co-dimension. Our combinatorial approach also achieves polynomial space in contrast to the algebraic approach that uses exponential space. We also give a PPZ-style algorithm for $k$-SUB-SAT with runtime $\approx 2^{n-n/2k}$. This algorithm is in fact oblivious to the structure of the subspace, and extends when the subspace-membership constraint is replaced by any constraint for which partial satisfying assignments can be efficiently completed to a full satisfying assignment.  Finally, for systems of $O(n)$ polynomial equations in $n$ variables over the field of two elements, we give a fast exponential algorithm when each polynomial has bounded degree irreducible factors (but can otherwise have large degree) using a degree reduction trick.</summary>
    <updated>2021-07-30T02:11:20Z</updated>
    <published>2021-07-30T02:11:20Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-31T11:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.14221</id>
    <link href="http://arxiv.org/abs/2107.14221" rel="alternate" type="text/html"/>
    <title>Can't See The Forest for the Trees: Navigating Metric Spaces by Bounded Hop-Diameter Spanners</title>
    <feedworld_mtime>1627603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Omri Kahalon, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Hung.html">Hung Le</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Milenkovic:Lazar.html">Lazar Milenkovic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomon:Shay.html">Shay Solomon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14221">PDF</a><br/><b>Abstract: </b>Spanners for metric spaces have been extensively studied, both in general
metrics and in restricted classes, perhaps most notably in low-dimensional
Euclidean spaces -- due to their numerous applications. Euclidean spanners can
be viewed as means of compressing the $\binom{n}{2}$ pairwise distances of a
$d$-dimensional Euclidean space into $O(n) = O_{\epsilon,d}(n)$ spanner edges,
so that the spanner distances preserve the original distances to within a
factor of $1+\epsilon$, for any $\epsilon &gt; 0$. Moreover, one can compute such
spanners in optimal $O(n \log n)$ time. Once the spanner has been computed, it
serves as a "proxy" overlay network, on which the computation can proceed,
which gives rise to huge savings in space and other important quality measures.
</p>
<p>On the negative side, by working on the spanner rather than the original
metric, one loses the key property of being able to efficiently "navigate"
between pairs of points. While in the original metric, one can go from any
point to any other via a direct edge, it is unclear how to efficiently navigate
in the spanner: How can we translate the existence of a "good" path into an
efficient algorithm finding it? Moreover, usually by "good" path we mean a path
whose weight approximates the original distance between its endpoints -- but a
priori the number of edges (or "hops") in the path could be huge. To control
the hop-length of paths, one can try to upper bound the spanner's hop-diameter,
but naturally bounded hop-diameter spanners are more complex than spanners with
unbounded hop-diameter, which might render the algorithmic task of efficiently
finding good paths more challenging.
</p>
<p>The original metric enables us to navigate optimally -- a single hop (for any
two points) with the exact distance, but the price is high -- $\Theta(n^2)$
edges. [...]
</p></div>
    </summary>
    <updated>2021-07-30T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.14126</id>
    <link href="http://arxiv.org/abs/2107.14126" rel="alternate" type="text/html"/>
    <title>The Complexity of Growing a Graph</title>
    <feedworld_mtime>1627603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mertzios:George_B=.html">George B. Mertzios</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Michail:Othon.html">Othon Michail</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Skretas:George.html">George Skretas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Spirakis:Paul_G=.html">Paul G. Spirakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Theofilatos:Michail.html">Michail Theofilatos</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14126">PDF</a><br/><b>Abstract: </b>Motivated by biological processes, we introduce here the model of growing
graphs, a new model of highly dynamic networks. Such networks have as nodes
entities that can self-replicate and thus can expand the size of the network.
This gives rise to the problem of creating a target network $G$ starting from a
single entity (node). To properly model this, we assume that every node $u$ can
generate at most one node $v$ at every round (or time slot), and every
generated node $v$ can activate edges with other nodes only at the time of its
birth, provided that these nodes are up to a small distance $d$ away from $v$.
We show that the most interesting case is when the distance is $d=2$. Edge
deletions are allowed at any time slot. This creates a natural balance between
how fast (time) and how efficiently (number of deleted edges) a target network
can be generated. A central question here is, given a target network $G$ of $n$
nodes, can $G$ be constructed in the model of growing graphs in at most $k$
time slots and with at most $\ell$ excess edges (i.e., auxiliary edges $\notin
E(G)$ that are activated and later deleted)? We consider here both centralized
and distributed algorithms for such questions (and also their computational
complexity). Our results include lower bounds based on properties of the target
network and algorithms for general graph classes that try to balance speed and
efficiency. We then show that the optimal number of time slots to construct an
input target graph with zero-waste (i.e., no edge deletions allowed), is hard
even to approximate within $n^{1-\varepsilon}$, for any $\varepsilon&gt;0$, unless
P=NP. On the contrary, the question of the feasibility of constructing a given
target graph in $\log n$ time slots and zero-waste, can be answered in
polynomial time. Finally, we initiate a discussion on possible extensions for
this model for a distributed setting.
</p></div>
    </summary>
    <updated>2021-07-30T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.14079</id>
    <link href="http://arxiv.org/abs/2107.14079" rel="alternate" type="text/html"/>
    <title>Density of Binary Disc Packings:Lower and Upper Bounds</title>
    <feedworld_mtime>1627603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fernique:Thomas.html">Thomas Fernique</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14079">PDF</a><br/><b>Abstract: </b>We provide, for any $r\in (0,1)$, lower and upper bounds on the maximal
density of a packing in the Euclidean plane of discs of radius $1$ and $r$. The
lower bounds are mostly folk, but the upper bounds improve the best previously
known ones for any $r\in[0.11,0.74]$. For many values of $r$, this gives a
fairly good idea of the exact maximum density. In particular, we get new
intervals for $r$ which does not allow any packing more dense that the
hexagonal packing of equal discs.
</p></div>
    </summary>
    <updated>2021-07-30T23:09:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.13809</id>
    <link href="http://arxiv.org/abs/2107.13809" rel="alternate" type="text/html"/>
    <title>Generalisations of Matrix Partitions : Complexity and Obstructions</title>
    <feedworld_mtime>1627603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alexey Barsukov, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kant=eacute=:Mamadou_Moustapha.html">Mamadou Moustapha Kanté</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.13809">PDF</a><br/><b>Abstract: </b>A trigraph is a graph where each pair of vertices is labelled either 0 (a
non-edge), 1 (an edge) or $\star$ (both an edge and a non-edge). In a series of
papers, Hell et al. proposed to study the complexity of homomorphisms from
graphs to trigraphs, called Matrix Partition Problems, where edges and
non-edges can be both mapped to $\star$-edges, while a non-edge cannot be
mapped to an edge, and vice-versa. Even though, Matrix Partition Problems are
generalisations of CSPs, they share with them the property of being
"intrinsically" combinatorial. So, the question of a possible dichotomy, i.e.
P-time vs NP-complete, is a very natural one and raised in Hell et al.'s
papers. We propose in this paper to study Matrix Partition Problems on
relational structures, wrt a dichotomy question, and, in particular,
homomorphisms between trigraphs. We first show that trigraph homomorphisms and
Matrix Partition Problems are P-time equivalent, and then prove that one can
also restrict (wrt dichotomy) to relational structures with one relation.
Failing in proving that Matrix Partition Problems on directed graphs are not
P-time equivalent to Matrix Partitions on relational structures, we give some
evidence that it is unlikely by showing that reductions used in the case of
CSPs cannot work. We turn then our attention to Matrix Partitions with finite
sets of obstructions. We show that, for a fixed trigraph, the set of
inclusion-wise minimal obstructions is finite for directed graphs if and only
if it is finite for trigraphs. We also prove similar results for relational
structures. We conclude by showing that on trees (seen as trigraphs) it is
NP-complete to decide whether a given tree has a trigraph homomorphism to
another input trigraph. The latter shows a notable difference on tractability
between CSP and Matrix Partition as it is well-known that CSP is tractable on
the class of trees.
</p></div>
    </summary>
    <updated>2021-07-30T22:49:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.13801</id>
    <link href="http://arxiv.org/abs/2107.13801" rel="alternate" type="text/html"/>
    <title>A New Lossless Data Compression Algorithm Exploiting Positional Redundancy</title>
    <feedworld_mtime>1627603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Pranav Venkatram <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.13801">PDF</a><br/><b>Abstract: </b>A new run length encoding algorithm for lossless data compression that
exploits positional redundancy by representing data in a two-dimensional model
of concentric circles is presented. This visual transform enables detection of
runs (each of a different character) in which runs need not be contiguous and
hence, is a generalization of run length encoding. Its advantages and drawbacks
are characterized by comparing its performance with TurboRLE.
</p></div>
    </summary>
    <updated>2021-07-30T23:08:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.13658</id>
    <link href="http://arxiv.org/abs/2107.13658" rel="alternate" type="text/html"/>
    <title>On Families of Planar DAGs with Constant Stack Number</title>
    <feedworld_mtime>1627603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/N=ouml=llenburg:Martin.html">Martin Nöllenburg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pupyrev:Sergey.html">Sergey Pupyrev</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.13658">PDF</a><br/><b>Abstract: </b>A $k$-stack layout (or $k$-page book embedding) of a graph consists of a
total order of the vertices, and a partition of the edges into $k$ sets of
non-crossing edges with respect to the vertex order. The stack number of a
graph is the minimum $k$ such that it admits a $k$-stack layout.
</p>
<p>In this paper we study a long-standing problem regarding the stack number of
planar directed acyclic graphs (DAGs), for which the vertex order has to
respect the orientation of the edges. We investigate upper and lower bounds on
the stack number of several families of planar graphs: We prove constant upper
bounds on the stack number of single-source and monotone outerplanar DAGs and
of outerpath DAGs, and improve the constant upper bound for upward planar
3-trees. Further, we provide computer-aided lower bounds for upward (outer-)
planar DAGs.
</p></div>
    </summary>
    <updated>2021-07-30T22:55:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.13638</id>
    <link href="http://arxiv.org/abs/2107.13638" rel="alternate" type="text/html"/>
    <title>Load Balancing: The Long Road from Theory to Practice</title>
    <feedworld_mtime>1627603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berndt:Sebastian.html">Sebastian Berndt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deppert:Max_A=.html">Max A. Deppert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jansen:Klaus.html">Klaus Jansen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohwedder:Lars.html">Lars Rohwedder</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.13638">PDF</a><br/><b>Abstract: </b>There is a long history of approximation schemes for the problem of
scheduling jobs on identical machines to minimize the makespan. Such a scheme
grants a $(1+\epsilon)$-approximation solution for every $\epsilon &gt; 0$, but
the running time grows exponentially in $1/\epsilon$. For a long time, these
schemes seemed like a purely theoretical concept. Even solving instances for
moderate values of $\epsilon$ seemed completely illusional. In an effort to
bridge theory and practice, we refine recent ILP techniques to develop the
fastest known approximation scheme for this problem. An implementation of this
algorithm reaches values of $\epsilon$ lower than $2/11\approx 18.2\%$ within a
reasonable timespan. This is the approximation guarantee of MULTIFIT, which, to
the best of our knowledge, has the best proven guarantee of any non-scheme
algorithm.
</p></div>
    </summary>
    <updated>2021-07-30T22:50:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5413385044107468315</id>
    <link href="http://blog.computationalcomplexity.org/feeds/5413385044107468315/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/07/covid-stats.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/5413385044107468315" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/5413385044107468315" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/07/covid-stats.html" rel="alternate" type="text/html"/>
    <title>Covid Stats</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>A stat often quoted: <a href="https://www.nytimes.com/2021/07/22/health/coronavirus-breakthrough-infections-delta.html">About 97% of hospitalized coronavirus patients have not been vaccinated. </a> </div><div><br/></div><div>People take this as proof that once vaccinated no worries. But I have so many challenges with this statistic.</div><div><ul style="text-align: left;"><li>This statistic is down from 100% a year ago. Are vaccines working poorer now?</li><li>There are likely correlations to those vaccinated and those who take precautions like mask wearing and social distancing, though I'm sure which way those correlations go.</li><li>Those unvaccinated are more likely to be near others unvaccinated so more likely get infected and hospitalized.</li></ul><div>Even though one shouldn't draw the conclusion from the statistic, that doesn't mean the conclusion is false. The gold standard are the double-blind vaccine trials which clearly showed the vaccines more efficient and safe. So get the vaccine, not that this blog post will convince those who have been making the conscious choice not to vaccinate to change their minds.</div><div><br/></div></div><div>The other stat I found odd was that the life expectancy dropped 1.5 years in 2020. This doesn't mean on average we'll live 1.5 year less. Rather it means that someone who lives their whole life in 2020 conditions, widespread Covid without vaccines, would on average live 1.5 years less than someone who live their entire life in 2019 conditions pre-Covid.</div><div><br/></div><div>Oddly enough if you made it to 2021 your life expectancy will probably increase (assuming you've been vaccinated). We made tremendous progress in understanding vaccine technology in 2020. Also people with conditions that could have limited their later life span were more likely to be fatal Covid victims, meaning those who are left would live longer.</div></div>
    </content>
    <updated>2021-07-29T19:39:00Z</updated>
    <published>2021-07-29T19:39:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-31T09:19:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=891</id>
    <link href="https://emanueleviola.wordpress.com/2021/07/28/chess-com-55-1000/" rel="alternate" type="text/html"/>
    <title>Chess.com: 5|5 &gt; 1000</title>
    <summary>Today for the first time, I surpassed score 1000 on Chess.com playing 5|5, which means you start with a 5-minute budget, and every move you get 5 more seconds. For a while I also played 3|2 (2-second increments), but it takes me about 2 seconds to move a piece, which means I lost games in […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Today for the first time, I surpassed score 1000 on Chess.com playing 5|5, which means you start with a 5-minute budget, and every move you get 5 more seconds.  For a while I also played 3|2 (2-second increments), but it takes me about 2 seconds to move a piece, which means I lost games in which I knew exactly what to do, but simply couldn’t move the pieces fast enough, which I found frustrating.  Longer games I tried but I don’t seem to have the patience for.</p>



<p>I won’t reveal my id, because I feel bad about how much time I am spending losing at chess (and I think you could see all my games with my id, but I am not sure).  My self-imposed limit is losing no more than one game a day, which means on average playing 2 games per day.  (I had to stop and Google <img alt="\sum_i i/2^i = 2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_i+i%2F2%5Ei+%3D+2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/>; there’s a neat calculation-free proof of it which hopefully will make me remember this fact next time.)</p>



<p>However not being a robot, I sometimes get upset at the way I lose.  Most of my games are classified as <em>giveaway</em>, which means I was winning according to the computer (and myself), but then because of some stupid mistake I end up losing the game.  And so what the heck, I am better than this!, I break the rule and start another match — only to lose again, chess seems not to forgive hot heads.</p>



<p>The main reason why I play seems to be that fast-paced chess has the ability to completely absorb my mind, so it’s a good quick escape.  Of course, there are also the little feel-good voices reminding me that it’s better than watching TV and that by playing I sharpen my mind.</p>



<p>While 1000 can of course be a ridiculously low bar by some standard, I found reaching it more difficult than I expected, and I like to think that the 5|5 format attracts stronger players, so that the competition is tougher, even though it may not be true.   (But it does seem true that a certain score in a certain format does not correspond to the same score in a different format.)  For one thing, I had to familiarize myself with several basic openings.  I bought a little cute book <em>Chess openings for kids</em> which is good for people like me whose knowledge of chess openings was “e4 e5.”  I don’t do anything fancy, but it was fun to read about common openings.  I think I also wouldn’t mind playing random chess, but it seems harder to find opponents.</p>



<p>So why don’t you try and see what is your 5|5 score?  And if you want to play sometimes, drop me a line.</p>



<p/></div>
    </content>
    <updated>2021-07-28T18:24:53Z</updated>
    <published>2021-07-28T18:24:53Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2021-07-31T11:21:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5661</id>
    <link href="https://www.scottaaronson.com/blog/?p=5661" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5661#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5661" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Striking new Beeping Busy Beaver champion</title>
    <summary xml:lang="en-US">For the past few days, I was bummed about the sooner-than-expected loss of Steven Weinberg. Even after putting up my post, I spent hours just watching old interviews with Steve on YouTube and reading his old essays for gems of insight that I’d missed. (Someday, I’ll tackle Steve’s celebrated quantum field theory and general relativity […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>For the past few days, I was bummed about the sooner-than-expected loss of Steven Weinberg.  Even after putting up my <a href="https://www.scottaaronson.com/blog/?p=5566">post</a>, I spent hours just watching old interviews with Steve on YouTube and reading his old essays for gems of insight that I’d missed.  (Someday, I’ll tackle Steve’s celebrated quantum field theory and general relativity textbooks … but that day is not today.)</p>



<p>Looking for something to cheer me up, I was delighted when <em>Shtetl-Optimized</em> reader Nick Drozd reported a significant new discovery in BusyBeaverology—one that, I’m proud to say, was directly inspired by my <a href="https://www.scottaaronson.com/papers/bb.pdf">Busy Beaver survey article</a> from last summer (<a href="https://www.scottaaronson.com/blog/?p=4916">see here for blog post</a>).</p>



<p>Recall that BB(n), the n<sup>th</sup> Busy Beaver number (technically, “Busy Beaver shift number”), is defined as the maximum number of steps that an n-state Turing machine, with 1 tape and 2 symbols, can make on an initially all-0 tape before it invokes a Halt transition.  Famously, BB(n) is not only uncomputable, it grows faster than any computable function of n—indeed, computing anything that grows as quickly as Busy Beaver is equivalent to solving the halting problem.</p>



<p>As of 2021, here is the extent of human knowledge about concrete values of this function:</p>



<ul><li>BB(1) = 1 (trivial)</li><li>BB(2) = 6 (Lin 1963)</li><li>BB(3) = 21 (Lin 1963)</li><li>BB(4) = 107 (Brady 1983)</li><li>BB(5) ≥ 47,176,870 (Marxen and Buntrock 1990)</li><li>BB(6) &gt; 7.4 × 10<sup>36,534</sup> (Kropitz 2010)</li><li>BB(7) &gt; 10<sup>2×10^10^10^18,705,352</sup> (“Wythagoras” 2014)</li></ul>



<p>As you can see, the function is reasonably under control for n≤4, then “achieves liftoff” at n=5.</p>



<p>In my survey, inspired by a suggestion of Harvey Friedman, I defined a variant called Beeping Busy Beaver, or BBB.  Define a <em>beeping Turing machine</em> to be a TM that has a single designated state where it emits a “beep.”  The <em>beeping number</em> of such a machine M, denoted b(M), is the largest t such that M beeps on step t, or ∞ if there’s no finite maximum.  Then BBB(n) is the largest finite value of b(M), among all n-state machines M.</p>



<p>I noted that the BBB function grows uncomputably <em>even given an oracle for the ordinary BB function</em>.  In fact, computing anything that grows as quickly as BBB is equivalent to solving any problem in the second level of the <a href="https://en.wikipedia.org/wiki/Arithmetical_hierarchy">arithmetical hierarchy</a> (where the computable functions are in the zeroth level, and the halting problem is in the first level).  Which means that pinning down the first few values of BBB should be <em>even</em> <em>more</em> breathtakingly fun than doing the same for BB!</p>



<p>In my survey, I noted the following four concrete results:</p>



<ul><li>BBB(1) = 1 = BB(1)</li><li>BBB(2) = 6 = BB(2)</li><li>BBB(3) ≥ 55 &gt; 21 = BB(3)</li><li>BBB(4) ≥ 2,819 &gt; 107 = BB(4)</li></ul>



<p>The first three of these, I managed to get on my own, with the help of a little program I wrote.  The fourth one was communicated to me by Nick Drozd even before I finished my survey.</p>



<p>So as of last summer, we knew that BBB coincides with the ordinary Busy Beaver function for n=1 and n=2, then breaks away starting at n=3.  We didn’t know how quickly BBB “achieves liftoff.”</p>



<p>But Nick continued plugging away at the problem all year, and he now claims to have resolved the question.  More concretely, he claims the following two results:</p>



<ul><li>BBB(3) = 55 (via exhaustive enumeration of cases)</li><li>BBB(4) ≥ 32,779,478 (via a newly-discovered machine)</li></ul>



<p>For more, see Nick’s <a href="https://cs.nyu.edu/pipermail/fom/2021-July/022743.html">announcement on the Foundations of Mathematics email list</a>, or his own <a href="https://nickdrozd.github.io/2021/07/11/self-cleaning-turing-machine.html">blog post</a>.</p>



<p>Nick actually writes in terms of yet another Busy Beaver variant, which he calls BLB, or “Blanking Beaver.”  He defines BLB(n) to be the maximum finite number of steps that an n-state Turing machine can take before it first “wipes its tape clean”—that is, sets all the tape squares to 0, as they were at the very beginning of the computation, but as they were <em>not</em> at intermediate times.  Nick has discovered a 4-state machine that takes 32,779,477 steps to blank out its tape, thereby proving that</p>



<ul><li>BLB(4) ≥ 32,779,477.</li></ul>



<p>Nick’s construction, when investigated, turns out to be based on a “Collatz-like” iterative process—exactly like the BB(5) champion and most of the other strong Busy Beaver contenders currently known.  A simple modification of his construction yields the lower bound on BBB.</p>



<p>Note that the Blanking Beaver function does <em>not</em> have the same sort of super-uncomputable growth that Beeping Busy Beaver has: it merely grows “normally” uncomputably fast, like the original BB function did.  Yet we see that BLB, just like BBB, already “achieves liftoff” by n=4, rather than n=5.  So the real lesson here is that <em>4-state Turing machines can already do fantastically complicated things on blank tapes</em>.  It’s just that the usual definitions of the BB function artificially prevent us from seeing that; they hide the uncomputable insanity until we get to 5 states.</p></div>
    </content>
    <updated>2021-07-27T22:43:15Z</updated>
    <published>2021-07-27T22:43:15Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-07-30T17:56:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/07/26/postdoc-at-ist-austria-apply-by-august-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/07/26/postdoc-at-ist-austria-apply-by-august-31-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at IST Austria (apply by August 31, 2021)</title>
    <summary>My group at IST Austria has an open postdoctoral position, part of an ERC Starting Grant project, whose goal is to develop new theory, and algorithms for scalable machine learning. For questions, please contact dan.alistarh@ist.ac.at. The application should contain a CV, publication list, and a 1-page statement describing motivation and research interests. Website: https://scaleml.pages.ist.ac.at/ Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My group at IST Austria has an open postdoctoral position, part of an ERC Starting Grant project, whose goal is to develop new theory, and algorithms for scalable machine learning.</p>
<p>For questions, please contact dan.alistarh@ist.ac.at. The application should contain a CV, publication list, and a 1-page statement describing motivation and research interests.</p>
<p>Website: <a href="https://scaleml.pages.ist.ac.at/">https://scaleml.pages.ist.ac.at/</a><br/>
Email: dan.alistarh@ist.ac.at</p></div>
    </content>
    <updated>2021-07-26T05:51:55Z</updated>
    <published>2021-07-26T05:51:55Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-07-31T11:20:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-161863856324090968</id>
    <link href="http://blog.computationalcomplexity.org/feeds/161863856324090968/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/07/i-wish-problems-i-have-with-computers.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/161863856324090968" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/161863856324090968" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/07/i-wish-problems-i-have-with-computers.html" rel="alternate" type="text/html"/>
    <title>I wish problems I have with computers really were my fault</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> As you know, the website for out for a few days, as Lance explained <a href="https://blog.computationalcomplexity.org/2021/07/technical-difficulties.html">here</a>.</p><p>When I first could not get to the this blog  my thought was</p><p><i>OH, I must have changed some setting by accident. When Lance gets back (he was on vacation) he'll know how to fix it. Bad timing that it happened when he was gone, though prob not an accident- with him on vacation I was at the site more often and had more of a chance to screw things up. AND Lance will tell me what I did and I'll know to not do it again. And I will learn more about how this all works which will help me in the future!</i></p><p>When Lance got back we found out that NO Bill didn't do anything wrong. The blog site company  that we work with did an update and BLAH BLAH BLAH.  Reminds me of the theme behind the TV show Seinfeld: <i>No Hugs, No Learning.</i> At least no learning. I am not in the slightest more enlightened. </p><p>Lance worked with them and YADA YADA YADA the problem is fixed, so I am very happy about that. </p><p>On the one hand I wish it had been my fault so I would learn something.  On he other hand, if it was my fault would it have been as easy to fix? Would I really have learned something? </p><p>When something does not work my protocol is</p><p>1) Turn the machine off and on again (e.g., log out and log in again). I want to say </p><p><i>this works surprisingly often</i></p><p>but I doubt this surprises any of my readers, or is even news to them.</p><p>2) Spend at most 5 minutes <i>trying to fix it myself . </i>You will soon see that 5 minutes is a good choice for me.</p><p>3) Ask staff or Lance or Darling or my TA  (depending on the problem). </p><p>4) They tell me to log off and log on again. When I tell them I already have they do something magical and it works again. I then ask them:</p><p>a) Could I have fixed this myself. 2/3 of the time the answer is no. They don't mean intellectually. They mean that I do not have access to what I need to fix it.</p><p>b) Did I do something wrong? I want to know so I won't do it again. about 99/100 times the answer is that I did nothing wrong (I don't recall that last time that I did).</p><p>Given a and b, I think 5 minutes is all the time I want to spend to try to fix it myself. </p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-07-26T04:12:00Z</updated>
    <published>2021-07-26T04:12:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-31T09:19:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/111</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/111" rel="alternate" type="text/html"/>
    <title>TR21-111 |  Influence of a Set of Variables on a Boolean Function | 

	Aniruddha  Biswas, 

	Palash Sarkar</title>
    <summary>The influence of a set of variables on a Boolean function has three separate definitions in the literature, the first due to Ben-Or and Linial (1989), the second due to Fischer et al. (2002) and Blais (2009) and the third due to Tal (2017). The goal of the present work is to carry out a comprehensive study of the notion of influence of a set of variables on a Boolean function. To this end, we introduce a           definition of this notion using the auto-correlation function. A modification of the definition leads to the notion of pseudo-influence. Somewhat surprisingly, it turns out that the auto-correlation based definition of influence is equivalent to the definition introduced by Fischer et al. (2002) and Blais (2009) and the notion of pseudo-influence is equivalent to the definition of influence considered by Tal (2017). Extensive analysis of influence and pseduo-influence as well as the Ben-Or and Linial notion of influence is carried out and the relations between these notions are established.</summary>
    <updated>2021-07-25T16:37:04Z</updated>
    <published>2021-07-25T16:37:04Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-31T11:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/110</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/110" rel="alternate" type="text/html"/>
    <title>TR21-110 |  Fourier growth of structured $\mathbb{F}_2$-polynomials and applications | 

	Jaroslaw Blasiok, 

	Peter Ivanov, 

	Yaonan Jin, 

	Chin Ho Lee, 

	Rocco Servedio, 

	Emanuele Viola</title>
    <summary>We analyze the Fourier growth, i.e. the $L_1$ Fourier weight at level $k$ (denoted $L_{1,k}$), of various well-studied classes of "structured" $\mathbb{F}_2$-polynomials. This study is motivated by applications in pseudorandomness, in particular recent results and conjectures due to [CHHL19,CHLT19,CGLSS20] which show that upper bounds on Fourier growth (even at level $k=2$) give unconditional pseudorandom generators.

  Our main structural results on Fourier growth are as follows:

  - We show that any symmetric degree-$d$ $\mathbb{F}_2$-polynomial $p$ has $L_{1,k}(p) \le \Pr[p=1] \cdot O(d)^k$, and this is tight for any constant $k$. This quadratically strengthens an earlier bound that was implicit in [RSV13].

  - We show that any read-$\Delta$ degree-$d$ $\mathbb{F}_2$-polynomial $p$ has $L_{1,k}(p) \le \Pr[p=1] \cdot (k \Delta d)^{O(k)}$.

  - We establish a composition theorem which gives $L_{1,k}$ bounds on disjoint compositions of functions that are closed under restrictions and admit $L_{1,k}$ bounds.

  Finally, we apply the above structural results to obtain new unconditional pseudorandom generators and new correlation bounds for various classes of $\mathbb{F}_2$-polynomials.</summary>
    <updated>2021-07-25T11:28:58Z</updated>
    <published>2021-07-25T11:28:58Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-31T11:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5566</id>
    <link href="https://www.scottaaronson.com/blog/?p=5566" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5566#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5566" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Steven Weinberg (1933-2021): a personal view</title>
    <summary xml:lang="en-US">Steven Weinberg was, perhaps, the last truly towering figure of 20th-century physics. In 1967, he wrote a 3-page paper saying in effect that as far as he could see, two of the four fundamental forces of the universe—namely, electromagnetism and the weak nuclear force—had actually been the same force until a tiny fraction of a […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p/>



<p/>



<div class="wp-block-image"><figure class="aligncenter"><img alt="Steven Weinberg sitting in front of a chalkboard covered in equations" src="https://www.sciencenews.org/wp-content/uploads/2021/07/072421_weinberg_closecrop-1030x580.jpg"/></figure></div>



<p>Steven Weinberg was, perhaps, the last truly towering figure of 20th-century physics.  In 1967, he wrote a <a href="https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.19.1264">3-page paper</a> saying in effect that as far as he could see, two of the four fundamental forces of the universe—namely, electromagnetism and the weak nuclear force—had actually been the same force until a tiny fraction of a second after the Big Bang, when a broken symmetry caused them to decouple.  Strangely, he had developed the math underlying this idea for the strong nuclear force, and it didn’t work there, but it <em>did</em> seem to work for the weak force and electromagnetism.  Steve noted that, if true, this would require the existence of two force-carrying particles that hadn’t yet been seen — the W and Z bosons — and would <em>also </em>require the existence of the famous Higgs boson.</p>



<p>By 1979, enough of this picture had been confirmed by experiment that Steve shared the Nobel Prize in Physics with Sheldon Glashow—Steve’s former high-school classmate—as well as with Abdus Salam, both of whom had separately developed pieces of the same puzzle.  As arguably the central architect of what we now call the Standard Model of elementary particles, Steve was in the ultra-rarefied class where, had he <em>not</em> won the Nobel Prize, it would’ve been a stain on the prize rather than on him.</p>



<p>Steve once recounted in my hearing that Richard Feynman initially heaped scorn on the electroweak proposal.  Late one night, however, Steve was woken up by a phone call.  It was Feynman.  “I believe your theory now,” Feynman announced.  “Why?” Steve asked.  Feynman, being Feynman, gave some idiosyncratic reason that he’d worked out for himself.</p>



<p>It used to happen more often that someone would put forward a bold new proposal about the most fundamental laws of nature … and then the experimentalists would <em>actually go out and confirm it</em>.  Besides with the Standard Model, though, there’s approximately <em>one</em> other time that that’s happened in the living memory of most of today’s physicists.  Namely, when astronomers discovered in 1998 that the expansion of the universe was accelerating, apparently due to a dark energy that behaved like Einstein’s long-ago-rejected cosmological constant.  Very few had expected such a result.  There was one prominent exception, though: Steve Weinberg had <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.59.2607">written in 1987</a> that he saw no reason why the cosmological constant shouldn’t take a nonzero value that was still tiny enough to be consistent with galaxy formation and so forth.</p>



<hr class="wp-block-separator"/>



<p>In his long and illustrious career, one of the <em>least</em> important things Steve did, six years ago, was to play a major role in recruiting me and my wife Dana to UT Austin.  The first time I met Steve, his first question to me was “have we met before?  you look familiar.”  It turns out that he’d met my dad, Steve Aaronson, way back in the 1970s, when my dad (then a young science writer) had interviewed Weinberg for a magazine article.  I was astonished that Weinberg would remember such a thing across decades.</p>



<p>Steve was then gracious enough to take me, Dana, and both of my parents out to dinner in Austin as part of my and Dana’s recruiting trip.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><a href="https://www.scottaaronson.com/blog/wp-content/uploads/2021/07/weinberg3.jpg"><img alt="" class="wp-image-5618" height="317" src="https://www.scottaaronson.com/blog/wp-content/uploads/2021/07/weinberg3.jpg" width="422"/></a></figure></div>



<p>We talked, among other things, about Telluride House at Cornell, where Steve had lived as an undergrad in the early 1950s and where I’d lived as an undergrad almost half a century later.  Steve said that, while he loved the intellectual atmosphere at Telluride, he tried to have as little to do as possible with the “self-government” aspect, since he found the political squabbles that convulsed many of the humanities majors there to be a waste of time.  I burst out laughing, because … well, imagine you got to have dinner with James Clerk Maxwell, and he opened up about some ridiculously specific pet peeve from his college years, and it was <em>your</em> ridiculously specific pet peeve from <em>your</em> college years.</p>



<p>(Steve claimed to us, not entirely convincingly, that he was a mediocre student at Cornell, more interesting in “necking” with his fellow student and future wife Louise than in studying physics.)</p>



<p>After Dana and I came to Austin, Steve was kind enough to invite me to the high-energy theoretical physics lunches, where I chatted with him and the other members of his group every week (or better yet, simply listened).  I’d usually walk to the faculty club ten minutes early.  Steve, having arrived by car, would be sitting alone in an armchair, reading a newspaper, while he waited for the other physicists to arrive by foot.  No matter how scorching the Texas sun, Steve would <em>always</em> be wearing a suit (usually a tan one) and a necktie, his walking-cane by his side.  I, typically in ratty shorts and t-shirt, would sit in the armchair next to him, and we’d talk—about the latest developments in quantum computing and information (Steve, a perpetual student, would pepper me with questions), or his recent work on nonlinear modifications of quantum mechanics, or his memories of Cambridge, MA, or climate change or the anti-Israel protests in Austin or whatever else.  These conversations, brief and inconsequential as they probably were to him, were highlights of my week. </p>



<p>There was, of course, something a little melancholy about getting to know such a great man only in the twilight of his life.  To be clear, Steve Weinberg in his mid-to-late 80s was <em>far</em> more cogent, articulate, and quick to understand what was said to him than just about anyone you’d ever met in their prime.  But then, after a short conversation, he’d have to leave for a nap.  Steve was as clear-eyed and direct about his age and impending mortality as he was about everything else.  “Scott!” he once greeted me.  “I just saw the announcement for your physics colloquium about quantum supremacy.  I hope I’m still alive next month to attend it.”</p>



<p>(As it happens, the colloquium in question was on November 9, 2016, the day we learned that Trump would become president.  I offered to postpone the talk, since no one could concentrate on physics on such a day.  While several of the physicists agreed that that was the right call, Steve convinced me to go ahead with the following message: “I sympathize, but I do want to hear you … There is some virtue in just plowing on.”)</p>



<p>I sometimes felt, as well, like I was speaking with Steve across a cultural chasm even greater than the half-century that separated us in age.  Steve enjoyed nothing more than to discourse at length, in his booming New-York-accented baritone, about opera, or ballet, or obscure corners of 18th-century history.  It would be easy to feel like a total philistine by comparison … and I did.  Steve also told me that he never reads blogs or other social media, since he’s unable believe any written work is “real” unless it’s published, ideally on paper.  I could only envy such an attitude.</p>



<hr class="wp-block-separator"/>



<p>If you <em>did</em> try to judge by the social media that he never read, you might conclude that Steve would be remembered by the wider world less for any of his epochal contributions to physics than for a single viral quote of his:</p>



<blockquote class="wp-block-quote"><p>With or without religion, good people can behave well and bad people can do evil; but for good people to do evil — that takes religion.</p></blockquote>



<p>I can testify that Steve fully lived his atheism.  Four years ago, I invited him (along with many other UT colleagues) to the <em>brit milah</em> of my newborn son Daniel.  Steve said he’d be happy to come over our house another time (and I’m happy to say that he did a year later), but not to witness any body parts being cut.</p>



<p>Despite his hostility to Judaism—along with every other religion—Steve was a vociferous supporter of the state of Israel, almost to the point of making me look like Edward Said or Noam Chomsky.  For Steve, Zionism was not in spite of his liberal, universalist Enlightenment ideals but because of them.</p>



<p>Anyway, there’s no need even to wonder whether Steve had any sort of deathbed conversion.  He’d laugh at the thought.</p>



<hr class="wp-block-separator"/>



<p>In 2016, Steve published <a href="https://www.amazon.com/Explain-World-Discovery-Modern-Science/dp/0062346660"><em>To Explain the World</em></a>, a history of human progress in physics and astronomy from the ancient Greeks to Newton (when, Steve says, the scientific ethos reached the form that it still basically has today).  It’s unlike any other history-of-science book that I’ve read.  Of course I’d read other books about Aristarchus and Ptolemy and so forth, but I’d never read a modern writer treating them not as historical subjects, but as <em>professional colleagues merely separated in time.</em>  Again and again, Steve would redo ancient calculations, finding errors that had escaped historical notice; he’d remark on how Eratosthenes or Kepler could’ve done better with the data available to them; he’d grade the ancients by how much of modern physics and cosmology they’d correctly anticipated.</p>



<p><em>To Explain the World</em> was savaged in reviews by professional science historians.  Apparently, Steve had committed the unforgivable sin of “Whig history”: that is, judging past natural philosophers by the standards of today.  Steve clung to the naïve, debunked, scientistic notions that there’s such a thing as “actual right answers” about how the universe works; that we today are, at any rate, much closer to those right answers than the ancients were; and that we can <em>judge</em> the ancients by how close they got to the right answers that we now know.</p>



<p>As I read the sneering reviews, I kept thinking: so suppose Archimedes, Copernicus, and all the rest were brought back from the dead.  Who would they rather talk to: historians seeking to explore every facet of their misconceptions, like anthropologists with a paleolithic tribe; or Steve Weinberg, who’d want to bring them up to speed as quickly as possible so they could continue the joint quest?</p>



<hr class="wp-block-separator"/>



<p>When it comes to the foundations of quantum mechanics, Steve <a href="https://www.nybooks.com/articles/2017/01/19/trouble-with-quantum-mechanics/">took the view</a> that <em>no</em> existing interpretation is satisfactory, although the Many-Worlds Interpretation is perhaps the least bad of the bunch.  Steve felt that our reaction to this state of affairs should be to <em>test quantum mechanics more precisely</em>—for example, by looking for tiny nonlinearities in the Schrödinger equation, or other signs that QM itself is only a limit of some more all-encompassing theory.  This is, to put it mildly, not a widely-held view among high-energy physicists—but it provided a fascinating glimpse into how Steve’s mind works.</p>



<p>Here was, empirically, the most successful theoretical physicist alive, and again and again, his response to conceptual confusion was not to ruminate more about basic principles but to <em>ask for more data</em> or <em>do a more detailed calculation</em>.  He never, ever let go of a short tether to the actual testable consequences of whatever was being talked about, or future experiments that might change the situation.</p>



<p>(Steve worked on string theory in the early 1980s, and he remained engaged with it for the rest of his life, for example by recruiting the string theorists Jacques Distler and Willy Fischler to UT Austin.  But he later soured on the prospects for getting testable consequences out of string theory within a reasonable timeframe.  And he once complained to me that the papers he’d read about “It from Qubit,” AdS/CFT, and the black hole information problem had had “too many words and not enough equations.”)</p>



<hr class="wp-block-separator"/>



<p>Steve was, famously, about as hardcore a reductionist as has ever existed on earth.  He was a reductionist not just in the usual sense that he believed there <em>are</em> fundamental laws of physics, from which, together with the initial conditions, everything that happens in our universe can be calculated in principle (if not in practice), at least probabilistically.  He was a reductionist in the stronger sense that he thought the quest to discover the fundamental laws of the universe had a special pride of place among all human endeavors—a place not shared by the many sciences devoted to the study of complex emergent behavior, interesting and important though they might be.</p>



<p>This came through clearly in Steve’s <a href="https://www.nybooks.com/articles/2002/10/24/is-the-universe-a-computer/">critical review</a> of Stephen Wolfram’s <em>A New Kind of Science</em>, where Steve (Weinberg, that is) articulated his views of why “free-floating” theories of complex behavior can’t take the place of a reductionistic description of our actual universe.  (Of course, I was <em>also</em> highly critical of <em>A New Kind of Science</em> in <a href="https://arxiv.org/abs/quant-ph/0206089">my review</a>, but for somewhat different reasons than Steve was.)  Steve’s reductionism was also clearly expressed in his testimony to Congress in support of continued funding for the Superconducting Supercollider.  (Famously, Phil Anderson testified <em>against</em> the SSC, arguing that the money would better be spent on condensed-matter physics and other sciences of emergent behavior.  The result: Congress did cancel the SSC, and it redirected precisely zero of the money to other sciences.  But at least Steve lived to see the LHC dramatically confirm the existence of the Higgs boson, as the SSC would have.)</p>



<p>I, of course, have devoted my career to theoretical computer science, which you might broadly call a “science of emergent behavior”: it tries to figure out the ultimate possibilities and limits of computation, taking the underlying laws of physics as given.  Quantum computing, in particular, takes as its input a physical theory that was already known by 1926, and studies what can be done with it.  So you might expect me to disagree passionately with Weinberg on reductionism versus holism.</p>



<p>In reality, I have a hard time pinpointing any substantive difference.  Mostly I see a difference in <em>opportunities</em>: Steve saw a golden chance to contribute something to the millennia-old quest to discover the fundamental laws of nature, at the tail end of the heroic era of particle physics that culminated in what we now call the Standard Model.  He was brilliant enough to seize that chance.  I didn’t see a similar chance: possibly because it no longer existed; almost certainly because, even if it did, I wouldn’t have had the right mind for it.  I found a different chance, to work at the intersection of physics and computer science that was finally kicking into high gear at the end of the 20th century.  Interestingly, while I came to that intersection from the CS side, quite a few who were originally trained as high-energy physicists ended up there as well—including a star PhD student of Steve Weinberg’s named John Preskill.</p>



<p>Despite his reductionism, Steve was as curious and enthusiastic about quantum computation as he was about a hundred other topics beyond particle physics—he even ended his <a href="https://www.amazon.com/Lectures-Quantum-Mechanics-Steven-Weinberg/dp/1107028728">quantum mechanics textbook</a> with a chapter about Shor’s factoring algorithm.  Having said that, a central <em>reason</em> for his enthusiasm about QC was that he clearly saw how demanding a test it would be of quantum mechanics itself—and as I mentioned earlier, Steve was open to the possibility that quantum mechanics might not be exactly true.</p>



<hr class="wp-block-separator"/>



<p>It would be an understatement to call Steve “left-of-center.”  He believed in higher taxes on rich people like himself to service a robust social safety net.  When Trump won, Steve remarked to me that most of the disgusting and outrageous things Trump would do could be reversed in a generation or so—but not the aggressive climate change denial; that actually <em>could</em> matter on the scale of centuries.  Steve made the news in Austin for <a href="https://www.texastribune.org/2016/01/27/the-brief/">openly defying</a> the Texas law forcing public universities to allow concealed carry on campus: he said that, regardless of what the law said, firearms would not be welcome in <em>his</em> classroom.  (Louise, Steve’s wife for 67 years and a professor at UT Austin’s law school, also wrote perhaps the <a href="https://law.utexas.edu/faculty/publications/2002-When-Courts-Decide-Elections-The-Constitutionality-of-Bush-v-Gore">definitive scholarly takedown</a> of the shameful <em>Bush vs. Gore</em> Supreme Court decision, which installed George W. Bush as president.)</p>



<p>All the same, during the “science wars” of the 1990s, Steve was <a href="https://www.nybooks.com/articles/1996/08/08/sokals-hoax/">scathing</a> about the academic left’s postmodernist streak and deeply sympathetic to what Alan Sokal had done with his <em>Social Text </em>hoax.  Steve also once told me that, when he (like other UT faculty) was required to write a statement about what he would do to advance Diversity, Equity, and Inclusion, he submitted just a single sentence: “I will seek the best candidates, without regard to race or sex.”  I remarked that he might be one of the only academics who could get away with that.</p>



<p>I confess that, for the past five years, knowing Steve was a greater source of psychological strength for me than, from a rational standpoint, it probably should have been.  Regular readers will know that I’ve spent months of my life agonizing over various nasty things people have said me about on Twitter and Reddit—that I’m a sexist white male douchebag, a clueless techbro STEMlord, a neoliberal Zionist shill, and I forget what else.</p>



<p>But I lately <em>have</em> had a secret emotional weapon that helped somewhat: namely, the certainty that Steven Weinberg had more intellectual power in a single toenail clipping than these Twitter-attackers had collectively experienced over the course of their lives.  It’s like, have you heard the joke where two rabbis are arguing some point of Talmud, and then God speaks from a booming thundercloud to declare that the first rabbi is right, and then the second rabbi says “OK fine, now it’s 2 against 1?”  For the W and Z bosons and Higgs boson that <em>you</em> predicted to turn up at the particle accelerator is not <em>exactly</em> God declaring from a thundercloud that the way your mind works is aligned with the way the world actually is—Steve, of course, would wince at the suggestion—but it’s about the closest thing available in this universe.  My secret emotional weapon was that I knew the man who’d experienced this, arguably more than any of the 7.6 billion other living humans, and not only did that man not sneer at me, but by some freakish coincidence, he seemed to have reached roughly the same views as I had on &gt;95% of controversial questions where we both had strong opinions.</p>



<hr class="wp-block-separator"/>



<p>My final conversations with Steve Weinberg were about a laptop.  When covid started in March 2020, Steve and Louise, being in their late 80s, naturally didn’t want to take chances, and rigorously sheltered at home.  But an issue emerged: Steve couldn’t install Zoom on his Bronze Age computer, and so couldn’t participate in the virtual meetings of his own group, nor could he do Zoom calls with his daughter and granddaughter.  While as a <em>theoretical</em> computer scientist, I don’t normally volunteer myself as tech support staff, I decided that an exception was more than warranted in this case.  The quickest solution was to configure one of my own old laptops with everything Steve needed and bring it over to his house.</p>



<p>Later, Steve emailed me to say that, while the laptop had worked great and been a lifesaver, he’d finally bought his own laptop, so I should come by to pick mine up.  I delayed and delayed with that, but finally decided I should do it before leaving Austin at the beginning of this summer.  So I emailed Steve to tell him I’d be coming.  He replied to me asking Louise to leave the laptop on the porch — but the email was addressed only to me, not her.</p>



<p>At that moment, I knew something had changed: only a year before, incredibly, <em>I’d</em> been more senile and out-of-it as a 39-year-old than Steve had been as an 87-year-old.  What I didn’t know at the time was that Steve had sent that email from the hospital when he was close to death.  It was the last I heard from him.</p>



<p>(Once I learned what was going on, I did send a get-well note, which I hope Steve saw, saying that I hoped he appreciated that I <em>wasn’t</em> praying for him.)</p>



<hr class="wp-block-separator"/>



<p>Besides the quote about good people, bad people, and religion, the other quote of Steve’s that he never managed to live down came from the last pages of <a href="https://www.amazon.com/First-Three-Minutes-Modern-Universe/dp/0465024378"><em>The First Three Minutes</em></a>, his classic 1970s popularization of big-bang cosmology:</p>



<blockquote class="wp-block-quote"><p>The more the universe seems comprehensible, the more it also seems pointless.</p></blockquote>



<p>In the 1993 epilogue, Steve tempered this with some more hopeful words, nearly as famous:</p>



<blockquote class="wp-block-quote"><p>The effort to understand the universe is one of the very few things which lifts human life a little above the level of farce and gives it some of the grace of tragedy.</p></blockquote>



<p>It’s not my purpose here to resolve the question of whether life or the universe have a point.  What I can say is that, even in his last years, Steve never for a nanosecond <em>acted</em> as if life was pointless.  He already had all the material comforts and academic renown anyone could possibly want.  He could have spent all day in his swimming pool, or listening to operas.  Instead, he continued publishing textbooks—a <a href="https://www.amazon.com/Lectures-Quantum-Mechanics-Steven-Weinberg/dp/1107028728">quantum mechanics textbook</a> in 2012, an <a href="https://www.amazon.com/Lectures-Astrophysics-Steven-Weinberg/dp/1108415075">astrophysics textbook</a> in 2019, and a <a href="https://www.amazon.com/dp/1108841767/?tag=pfamazon01-20">“Foundations of Modern Physics” textbook</a> in 2021 (!).  As recently as this year, he continued <a href="https://arxiv.org/search/hep-th?searchtype=author&amp;query=Weinberg%2C+Steven">writing papers</a>—and not just “great man reminiscing” papers, but hardcore technical papers.  He continued writing with nearly unmatched lucidity for a general audience, in the <em>New York Review of Books</em> and elsewhere.  And I can attest that he continued peppering visiting speakers with questions about stellar evolution or whatever else they were experts on—because, more likely than not, he had redone some calculation himself and gotten a subtly different result from what was in the textbooks.</p>



<p>If God exists, I can’t believe He or She would find nothing more interesting to do with Steve than to torture him for his unbelief.  More likely, I think, God is right now talking to Steve the same way Steve talked to Aristarchus in <em>To Explain the World</em>: “yes, you were close about the origin of neutrino masses, but here’s the part you were missing…”  While, of course, Steve is redoing God’s calculation to be sure.</p>



<hr class="wp-block-separator"/>



<p>Feel free to use the comments as a place to share your own memories.</p>



<hr class="wp-block-separator"/>



<p><strong>More Steven Weinberg memorial links (I’ll continue adding to this over the next few days):</strong></p>



<ul><li><a href="https://www.math.columbia.edu/~woit/wordpress/?p=12413">Peter Woit</a></li><li><a href="https://motls.blogspot.com/2021/07/steven-weinberg-1933-2021.html?m=1">He-Who-Must-Not-Be-Named</a></li><li><a href="https://mobile.twitter.com/johncarlosbaez/status/1418804320611364867?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet">John Baez</a> </li><li><a href="https://www.sciencenews.org/article/steven-weinberg-death-physics-electromagnetism-standard-model">Tom Siegfried</a></li><li><a href="https://mobile.twitter.com/seanmcarroll/status/1418804903871356930">Sean Carroll</a></li><li><a href="https://whyevolutionistrue.com/2021/07/24/steven-weinberg-died/">Jerry Coyne</a></li><li><a href="https://grahamfarmelo.com/remembering-steven-weinberg/">Graham Farmelo</a></li><li><a href="https://www.nytimes.com/2021/07/25/science/steven-weinberg-groundbreaking-nobelist-in-physics-dies-at-88.html">NYT</a></li><li><a href="https://thebulletin.org/2021/07/steven-weinberg-nobel-laureate-in-physics-and-bulletin-board-member-died-at-88/">Bulletin of the Atomic Scientists</a></li></ul>



<hr class="wp-block-separator"/>



<p><strong>Miscellaneous Steven Weinberg links</strong></p>



<ul><li><a href="https://mobile.twitter.com/dashunwang/status/1419267004905754624">Steve’s advice to young scientists</a></li><li><a href="https://physicstoday.scitation.org/doi/full/10.1063/1.3397044">Lenny Susskind’s 2010 review</a> of Steve’s book <em>Lake Views</em></li><li><a href="https://www.closertotruth.com/contributor/steven-weinberg/profile">Steve’s interviews on “Closer to Truth”</a></li><li><a href="https://cerncourier.com/a/model-physicist/">Interview in <em>CERN Courier</em></a></li><li>Steve <a href="https://www.youtube.com/watch?v=ebuve4INdAU&amp;t=1454s">spars with another of my greatest friends and heroes</a>, Rebecca Goldstein, about the basis of morality—also featuring Richard Dawkins, Daniel Dennett, Sean Carroll, and more (YouTube video from a 2012 conference)</li></ul>



<p/></div>
    </content>
    <updated>2021-07-25T04:19:37Z</updated>
    <published>2021-07-25T04:19:37Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-07-30T17:56:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/109</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/109" rel="alternate" type="text/html"/>
    <title>TR21-109 |  QRAT Polynomially Simulates Merge Resolution. | 

	Sravanthi Chede, 

	Anil Shukla</title>
    <summary>Merge Resolution (MRes [Beyersdorff et al. J. Autom. Reason.'2021] ) is a refutational proof system for quantified Boolean formulas (QBF). Each line of MRes consists of clauses with only existential literals, together with information of countermodels stored as merge maps. As a result, MRes has strategy extraction by design. The QRAT [Heule et al. J. Autom. Reason.'2017] proof system was designed to capture QBF preprocessing. QRAT can simulate both the expansion-based proof system $\forall$Exp+Res and CDCL-based QBF proof system LD-Q-Res. 

A family of false QBFs called SquaredEquality formulas were introduced in [Beyersdorff et al. J. Autom. Reason.'2021] and shown to be easy for MRes but need exponential size proofs in Q-Res, QU-Res, CP+$\forall$red, $\forall$Exp+Res, IR-calc and reductionless LD-Q-Res. As a result none of these systems can simulate MRes. In this paper, we show a short QRAT refutation of the SquaredEquality formulas. We further show that QRAT strictly p-simulates MRes. 
Besides highlighting the power of QRAT system, this work also presents the first simulation result for MRes.</summary>
    <updated>2021-07-23T20:10:36Z</updated>
    <published>2021-07-23T20:10:36Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-31T11:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Vienna (apply by August 31, 2021)</title>
    <summary>Two full-time 3-year postdoc positions in algorithms are available starting Jan 1, 2022 as part of the ERC Advanced Grant “Modern Dynamic Data Structures” to joint the algorithms research group headed by Monika Henzinger. Prior knowledge in fair algorithms and differential privacy is a plus. Please send your CV, a letter of motivation, and the […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Two full-time 3-year postdoc positions in algorithms are available starting Jan 1, 2022 as part of the ERC Advanced Grant “Modern Dynamic Data Structures” to joint the algorithms research group headed by Monika Henzinger. Prior knowledge in fair algorithms and differential privacy is a plus. Please send your CV, a letter of motivation, and the names of 3 references to applications.taa@univie.ac.at</p>
<p>Website: <a href="https://taa.cs.univie.ac.at/">https://taa.cs.univie.ac.at/</a><br/>
Email: applications.taa@univie.ac.at</p></div>
    </content>
    <updated>2021-07-23T18:04:04Z</updated>
    <published>2021-07-23T18:04:04Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-07-31T11:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5690</id>
    <link href="https://adamsheffer.wordpress.com/2021/07/23/a-basic-question-about-multiplicative-energy/" rel="alternate" type="text/html"/>
    <title>A Basic Question About Multiplicative Energy</title>
    <summary>As part of some research project, I got to a basic question about multiplicative energy. Embarrassingly , I wasn’t able to get any non-trivial bound for it. Here is the problem. Any information about it would be highly appreciated. Problem. Let . Let be a set of real numbers. How large can the multiplicative energy […]</summary>
    <updated>2021-07-23T16:27:45Z</updated>
    <published>2021-07-23T16:27:45Z</published>
    <category term="Additive combinatorics"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2021-07-31T11:21:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6975981189321037535</id>
    <link href="http://blog.computationalcomplexity.org/feeds/6975981189321037535/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/07/technical-difficulties.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6975981189321037535" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6975981189321037535" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/07/technical-difficulties.html" rel="alternate" type="text/html"/>
    <title>Technical Difficulties</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-racThFBUDl0/YPrj-7Dmi4I/AAAAAAAB9KA/St1lqT9-U5YLf_j03KkzPZKvO3wdk4csgCPcBGAsYHg/s4032/PXL_20210711_224807389.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="https://1.bp.blogspot.com/-racThFBUDl0/YPrj-7Dmi4I/AAAAAAAB9KA/St1lqT9-U5YLf_j03KkzPZKvO3wdk4csgCPcBGAsYHg/s320/PXL_20210711_224807389.jpg" width="320"/></a></div><br/><p>After returning from vacation last weekend (hello North Dakota--my 49th state visited), all sorts of odd problems arose. This blog stopped working, a P v NP paper was published on the ACM Transactions of Computing website and my personal emails were getting marked as spam. All is better, I hope.</p><p>Years ago I donated the URL computationalcomplexity.org to the <a href="https://www.computationalcomplexity.org/">Computational Complexity Conference</a>, coincidentally held this past week, with the condition that I could continue to use the "blog" subdomain for this blog. Organizations continue but the people in them change, and when the website was "upgraded" on Saturday the pointers to make this blog work were left out. Thanks to Ashwin Nayak for getting it all straightened out and we're back online.</p><p>For the ToCT paper, a paper claiming to reduce 3-SAT to 2-SAT, and thus show P = NP, was originally rejected by the journal but a "disposition field" got inadvertently set to accept and wasn't caught until it showed up online. Editor-in-Chief Ryan O'Donnell quickly got on the case and ACM has <a href="https://dl-acm-org.ezproxy.gl.iit.edu/doi/10.1145/3460950">removed the paper</a>. P v NP remains as open as ever.</p><p>Fixing the email required me to learn far more about <a href="https://en.wikipedia.org/wiki/Sender_Policy_Framework">SPFs</a> than I ever wanted to know.</p><p>By the way if anyone in Idaho wants to invite me to give a talk, I might be interested.</p></div>
    </content>
    <updated>2021-07-23T15:46:00Z</updated>
    <published>2021-07-23T15:46:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-31T09:19:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2256</id>
    <link href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/" rel="alternate" type="text/html"/>
    <title>Average-Case Fine-Grained Hardness, Part I</title>
    <summary>I recently finished my qualifying exam for which I surveyed worst-case to average-case reductions for problems in NP. As part of my survey, I reviewed recent results on average-case hardness in P (a.k.a. average-case fine-grained hardness). I would like to give an overview of some of these results in a series of blog posts, and I want to start by giving some motivations. Why do we care about average-case fine-grained hardness? (i) We hope to explain why we are struggling to find faster algorithms for problems in P such as DNA sequencing even for some random large datasets. (ii) Average-case hard problems in P is useful to cryptography. For example, constructing one-way functions (i.e., functions that are easy to compute but hard to invert) based on worst-case complexity assumptions such as NP BPP has been a long-standing open question. In this context, “easy” means polynomial-time computable. If we consider “easy” to be computable in time like instead, then a natural alternative question is whether we can construct such one-way functions based on worst-case fine-grained complexity assumptions. Another example is proof of work [DN92]. When a miner tries to mine the cryptocurrency, the miner is asked to solve a random puzzle, [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I recently finished my qualifying exam for which I surveyed worst-case to average-case reductions for problems in NP. As part of my survey, I reviewed recent results on average-case hardness in P (a.k.a. average-case fine-grained hardness). I would like to give an overview of some of these results in a series of blog posts, and I want to start by giving some motivations.</p>



<p><strong>Why do we care about average-case fine-grained hardness?</strong></p>



<p>(i) We hope to explain why we are struggling to find faster algorithms for problems in P such as DNA sequencing even for some random large datasets.</p>



<p>(ii) Average-case hard problems in P is useful to cryptography. For example, constructing <a href="https://en.wikipedia.org/wiki/One-way_function">one-way functions</a> (i.e., functions that are easy to compute but hard to invert) based on worst-case complexity assumptions such as NP <img alt="\not\subseteq" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnot%5Csubseteq&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> BPP has been a long-standing open question. In this context, “easy” means polynomial-time computable. If we consider “easy” to be computable in time like <img alt="O(n^{100})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B100%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> instead, then a natural alternative question is whether we can construct such one-way functions based on worst-case fine-grained complexity assumptions. Another example is <a href="https://en.wikipedia.org/wiki/Proof_of_work">proof of work</a> <a href="https://link.springer.com/chapter/10.1007/3-540-48071-4_10">[DN92]</a>. When a miner tries to mine the <a href="https://en.wikipedia.org/wiki/Cryptocurrency">cryptocurrency</a>, the miner is asked to solve a random puzzle, that is average-case hard but still tractable, and then the miner needs to prove they have indeed solved the puzzle through efficient protocols. An interesting and more recent follow-up is proof of useful work <a href="https://eprint.iacr.org/2017/203.pdf">[BRSV17b]</a>, which proposes that instead of wasting computing power on a random puzzle that comes from nowhere, we can first reduce a computational task of practical interest to multiple random puzzles and then ask the miner to solve those puzzles.</p>



<p>The most common approach for proving average-case fine-grained hardness is arguably worst-case to average-case reduction, i.e., reducing an arbitrary instance to a number of random instances of which the distribution is polynomial-time sampable. Before I give concrete examples, I want to describe a general recipe for designing such worst-case to average-case reductions. (Some reader might notice that the step 3 of the recipe is same as the argument for proving <a href="https://en.wikipedia.org/wiki/Random_self-reducibility#Permanent_of_a_matrix">computing permanent is self-reducible</a> <a href="https://www.semanticscholar.org/paper/New-Directions-In-Testing-Lipton/fb2eba4d69bdab34c2d240380d4370be2feeacb9">[L91]</a>, which essentially uses <a href="https://en.wikipedia.org/wiki/Locally_decodable_code#The_Reed%E2%80%93Muller_code">local decoding for Reed-Muller codes</a>.)</p>



<p><strong>General recipe for worst-case to average-case reductions:</strong></p>



<ol><li>Choose our favorite hard problem <img alt="L:\{0,1\}^n\to\mathbf{Z}_{\ge 0}" class="latex" src="https://s0.wp.com/latex.php?latex=L%3A%5C%7B0%2C1%5C%7D%5En%5Cto%5Cmathbf%7BZ%7D_%7B%5Cge+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in P.</li><li>Construct a low-degree (degree-<img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>) polynomial <img alt="f_{L}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on <img alt="\mathbf{F}_{p}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that <img alt="f_{L}(x)=L(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for all <img alt="x\in\{0,1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</li><li>To solve <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for worst-case <img alt="x\in\{0,1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: sample a uniformly random <img alt="y\in\mathbf{F}_{p}^n" class="latex" src="https://s0.wp.com/latex.php?latex=y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, compute <img alt="f_{L}(x+t_1 y),\dots,f_{L}(x+t_{d+1} y)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%2Bt_1+y%29%2C%5Cdots%2Cf_%7BL%7D%28x%2Bt_%7Bd%2B1%7D+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for distinct nonzero <img alt="t_1,\dots,t_{d+1}" class="latex" src="https://s0.wp.com/latex.php?latex=t_1%2C%5Cdots%2Ct_%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> using average-case solver (note each <img alt="x+t_{i} y" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Bt_%7Bi%7D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is uniformly random), interpolate univariate polynomial <img alt="g(t):=f_{L}(x+t y)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28t%29%3A%3Df_%7BL%7D%28x%2Bt+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> using these points, and output <img alt="g(0)" class="latex" src="https://s0.wp.com/latex.php?latex=g%280%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> which is <img alt="f_{L}(x)=L(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. (This step can be replaced by decoding algorithms for <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction">Reed-Solomon codes</a> to handle larger errors of average-case solver.)</li><li>(The above steps already show that evaluating <img alt="f_{L}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="d+1" class="latex" src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> uniformly random inputs is as hard as solving <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for worst-case input.) If we want to show <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> itself is average-case fine-grained hard, it suffices to give a reduction from computing <img alt="f_{L}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="x\in\mathbf{F}_{p}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> back to solving <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</li></ol>



<p>Notice that the above general recipe reduces a worst-case instance to <img alt="d+1" class="latex" src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> random instances at the step 3, and thus, we want <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be small (like <img alt="n^{o(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>) such that it does not blow up the total runtime significantly. Typically, the step 4 would also blow up the runtime, and sometimes it depends on <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. For all the problems (or the techniques) in this series, I will explicitly quantify the runtime blow-up in the step 4 (if there is any) and explain how small we want <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be (if the blow-up depends on <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>).</p>



<p>Now let us go through a concrete example. Consider one of the flagship problems in fine-grained complexity — orthogonal vector problem (OV): Given <img alt="X=\{x_1,\dots,x_n\}" class="latex" src="https://s0.wp.com/latex.php?latex=X%3D%5C%7Bx_1%2C%5Cdots%2Cx_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="Y=\{y_1,\dots,y_n\}" class="latex" src="https://s0.wp.com/latex.php?latex=Y%3D%5C%7By_1%2C%5Cdots%2Cy_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where each <img alt="x_i,y_i\in\{0,1\}^{d'}" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%2Cy_i%5Cin%5C%7B0%2C1%5C%7D%5E%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="d'\in\omega(\log n)\cap n^{o(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=d%27%5Cin%5Comega%28%5Clog+n%29%5Ccap+n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, decide if there are <img alt="x_i,y_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%2Cy_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that <img alt="\langle x_i,y_j\rangle=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i%2Cy_j%5Crangle%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. It is known OV has no sub-quadratic algorithm assuming strong <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">exponential-time hypothesis</a> (SETH) <a href="https://link.springer.com/chapter/10.1007/978-3-540-27836-8_101">[W05]</a>, and there is a generalization of OV called <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-OV problem that has no <img alt="O(n^{k-o(1)})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7Bk-o%281%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> algorithm under the same assumption <a href="https://dl.acm.org/doi/10.1145/3300150.3300158">[W18]</a>.</p>



<p><strong>Polynomial evaluation.</strong> Given an OV instance <img alt="X,Y" class="latex" src="https://s0.wp.com/latex.php?latex=X%2CY&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with <img alt="d'=n^{o(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=d%27%3Dn%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we construct a degree-<img alt="2d'" class="latex" src="https://s0.wp.com/latex.php?latex=2d%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> polynomial <img alt="f_{\textrm{OV}}(X,Y):=\sum_{i,j\in[n]}\prod_{t\in[d']}(1-x_{i,t}y_{i,t})" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D%28X%2CY%29%3A%3D%5Csum_%7Bi%2Cj%5Cin%5Bn%5D%7D%5Cprod_%7Bt%5Cin%5Bd%27%5D%7D%281-x_%7Bi%2Ct%7Dy_%7Bi%2Ct%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> over <img alt="\mathbf{F}_{p}^{2nd'}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5E%7B2nd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="p&gt;n^2" class="latex" src="https://s0.wp.com/latex.php?latex=p%3En%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="x_{i,t}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bi%2Ct%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> denotes the <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-th coordinate of <img alt="x_i\in X" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%5Cin+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Observe that <img alt="f_{\textrm{OV}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> simply enumerates all the pairs of vectors and counts the number of orthogonal pairs for the OV instance, and obviously counting is at least as hard as decision. Using the aforementioned general recipe, it follows immediately that evaluating <img alt="f_{\textrm{OV}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> requires <img alt="O(n^{2-o(1)})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2-o%281%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> time for average case assuming randomized version of SETH. This result was shown in <a href="https://eprint.iacr.org/2017/202.pdf">[BRSV17a]</a>, and analogously, they constructed a polynomial for <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-OV, which implies an average-case time hierarchy assuming randomized SETH.</p>



<p>However, the polynomial evaluation problem is algebraic. Next, let us consider a natural combinatorial problem — counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in an <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-vertices graph (for simplicity, think of <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as a large constant). This problem has worst-case complexity <img alt="n^{\Theta(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B%5CTheta%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> assuming ETH <a href="https://core.ac.uk/download/pdf/82508832.pdf">[CHKX06]</a>.</p>



<p><strong>Counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques.</strong> It was first shown in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> that there is an <img alt="\widetilde{O}(n^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-time reduction from counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in any graph to counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques with error probability <img alt="&lt;\frac{1}{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%3C%5Cfrac%7B1%7D%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in some polynomial-time sampable random graph. The proof also follows our general recipe. First, we construct a degree-<img alt="\binom{t}{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> polynomial <img alt="f_{t\textrm{-clique}}(X):=\sum_{\textrm{size-}t\, T\subseteq [n]}\prod_{i&lt;j\in T} X_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29%3A%3D%5Csum_%7B%5Ctextrm%7Bsize-%7Dt%5C%2C+T%5Csubseteq+%5Bn%5D%7D%5Cprod_%7Bi%3Cj%5Cin+T%7D+X_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> over <img alt="\mathbf{F}_{p}^{n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="p&gt;n^t" class="latex" src="https://s0.wp.com/latex.php?latex=p%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the adjacency matrix. Observe that <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> counts <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques by enumerating each size-<img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> subset of vertices and checking whether it is a <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-clique. It remains to work out the step 4 of the general recipe. This was done by a gadget reduction, that runs in <img alt="\widetilde{O}(p^{t^2}n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> time, from evaluating <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on <img alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to counting <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques in an <img alt="\widetilde{O}(p^{t^2}n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-vertices graph <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a>.</p>



<p>Although this gadget reduction is nice, I will not explain it here, because later works <a href="https://arxiv.org/abs/1903.08247">[BBB19,</a> <a href="https://arxiv.org/abs/2008.06591">DLW20]</a> show that if the polynomial constructed at the step 2 has certain structure, then there is a general technique to reduce evaluating this polynomial back to the original problem (at the cost of requiring smaller error probability of average-case solver), which I will discuss in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">next post</a>. Finally, let me point out an issue in the previous paragraph — <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is too large for the gadget reduction to be useful! We need <img alt="p&gt;n^t" class="latex" src="https://s0.wp.com/latex.php?latex=p%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (note <img alt="n^t" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a trivial upper bound of the number of <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques) such that <img alt="f_{t\textrm{-clique}}" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> indeed outputs the number of <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-cliques, but the gadget reduction takes <img alt="\widetilde{O}(p^{t^2}n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> time, and moreover, we do not know how to find a prime <img alt="&gt;n^t" class="latex" src="https://s0.wp.com/latex.php?latex=%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in <img alt="\widetilde{O}(n^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> time. This issue was handled in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> using <a href="https://en.wikipedia.org/wiki/Chinese_remainder_theorem">Chinese remainder theorem</a>. Specifically, we choose <img alt="O(t\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> many distinct primes <img alt="p_i" class="latex" src="https://s0.wp.com/latex.php?latex=p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>‘s upper bounded by <img alt="O(t\log n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> whose product is <img alt="&gt;n^t" class="latex" src="https://s0.wp.com/latex.php?latex=%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (the existence of such primes follows from <a href="https://en.wikipedia.org/wiki/Prime_number_theorem">asymptotic distribution of the prime numbers</a>). Then, we apply the whole reduction described so far to evaluating <img alt="f_{t\textrm{-clique}}(X)\mod p_i" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29%5Cmod+p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for each <img alt="p_i" class="latex" src="https://s0.wp.com/latex.php?latex=p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and then use Chinese remainder theorem to recover <img alt="f_{t\textrm{-clique}}(X)" class="latex" src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. (We can use Chinese remaindering with errors <a href="https://dl.acm.org/doi/10.1109/18.850672">[GRS99]</a> to handle larger error probability.)</p>



<p>Hopefully, the above examples give you a flavor of worst-case to average-case reductions for fine-grained hard problems. As promised, in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">next post</a>, I will continue to discuss the new technique in  <a href="https://arxiv.org/abs/1903.08247">[BBB19,</a> <a href="https://arxiv.org/abs/2008.06591">DLW20]</a>, which automatizes the step 4 of the general recipe by requiring more structural properties for the polynomial constructed in the step 2 of the general recipe.</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk.</p>



<p><br/></p></div>
    </content>
    <updated>2021-07-23T13:50:44Z</updated>
    <published>2021-07-23T13:50:44Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Junyao Zhao</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-07-31T11:21:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=6127</id>
    <link href="https://francisbach.com/laplace-method/" rel="alternate" type="text/html"/>
    <title>Approximating integrals with Laplace’s method</title>
    <summary>Integrals appear everywhere in all scientific fields, and their numerical computation is an active area of research. In the playbook of approximation techniques, my personal favorite is “la méthode de Laplace”, a must-know for students that like to cut integrals into pieces, that comes with lots of applications. We will be concerned with integrals of...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Integrals appear everywhere in all scientific fields, and their numerical computation is an active area of research. In the playbook of approximation techniques, my personal favorite is “la méthode de Laplace”, a must-know for students that like to cut integrals into pieces, that comes with lots of applications.</p>



<p class="justify-text">We will be concerned with integrals of the form $$I(t) =  \int_K h(x) e^{ \, – \, t f(x) } dx, $$ where \(K\) is compact (bounded and closed) subset of \(\mathbb{R}^d\), and \(h\) and \(f\) are two real-valued functions defined on \(K\) such that the integral is well defined for large enough \(t \in \mathbb{R}\). The goal is to obtain an asymptotic equivalent when \(t\) tends to infinity.</p>



<p class="justify-text">Within machine learning, as explained below, this is useful where computing and approximating integrals is important. This thus comes up naturally in Bayesian inference where \(t\) will be the number of observations in a statistical model. But let’s start with presenting this neat asymptotic result that Laplace discovered for a particular one-dimensional case in 1774 [<a href="http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32">1</a>].</p>



<h2>Laplace approximation</h2>



<p>Let’s first state the main result. Assume that:</p>



<ul class="justify-text"><li>\(h\) is continuous on \(K\) and that \(f\) is twice continuously differentiable on \(K\).</li><li>\(f\) has a strict global minimizer \(x_\ast\) on \(K\), which is in the interior of \(K\), where the gradient \(f^\prime(x_\ast)\) is thus equal to zero, and where the Hessian \(f^{\prime \prime}(x_\ast)\) is a positive definite matrix (it is always positive semi-definite because \(x_\ast\) is a local minimizer of \(f\)); moreover, \(h(x_\ast) \neq 0\).</li></ul>



<p class="justify-text">Then, as \(t\) tends to infinity, we have the following asymptotic equivalent: $$ I(t) \sim \frac{h(x_\ast)}{\sqrt{ \det f^{\prime \prime}(x_\ast) }} \Big( \frac{2\pi}{t}\Big)^{d/2}  e^{ \, – \, t f(x_\ast) }.$$</p>



<p class="justify-text"><strong>Where does it come from?</strong> The idea is quite simple: for \(t&gt;0\), the exponential term \(e^{ \, – \, t f(x) }\) is largest when \(x\) is equal to the minimizer \(x_\ast\). Hence only contributions close to \(x_\ast\) will count in the integral. Then we can do Taylor expansions of the two functions around \(x_\ast\), as \(h(x) \approx h(x_\ast)\) and \(f(x) \approx f(x_\ast) + \frac{1}{2} ( x – x_\ast)^\top f^{\prime \prime}(x_\ast) (x-x_\ast)\), and approximate \(I(t)\) as $$ I(t) \approx \int_K h(x_\ast) \exp\Big[ – t f(x_\ast) \ – \frac{t}{2} ( x – x_\ast)^\top f^{\prime \prime}(x_\ast) (x-x_\ast)\Big] dx.$$ We can then make a change of variable \(y = \sqrt{t} f^{\prime \prime}(x_\ast)^{1/2}( x – x_\ast)\) (where \(f^{\prime \prime}(x_\ast)^{1/2}\) is the positive square root of \(f^{\prime \prime}(x_\ast)\)), to get, with the Jacobian of the transformation leading to the term \(  (\det f^{\prime \prime}(x_\ast) )^{1/2} t^{d/2} \): $$I(t) \approx \frac{ h(x_\ast) e^{ \, – \, t f(x_\ast) }}{( \det f^{\prime \prime}(x_\ast) )^{1/2}t^{d/2} } \int_{\sqrt{t}f^{\prime \prime}(x_\ast)^{1/2}( K \ – x_\ast)} \!\!\! \exp\big[ – \frac{1}{2} y^\top  y  \big] dy.$$ Since \(x_\ast\) is in the interior of \(K\), when \(t\) tends to infinity, the set \(\sqrt{t} f^{\prime \prime}(x_\ast)^{1/2} ( K \ – x_\ast)\) tends to \(\mathbb{R}^d\) (see illustration below), and we obtain the usual Gaussian integral that leads to the normalizing constant of the Gaussian distribution, which is equal to \((2\pi)^{d/2} \).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-6263" height="153" src="https://francisbach.com/wp-content/uploads/2021/07/Klaplace-1024x288.png" width="544"/>Left: set \(K\) with \(x_\ast\) in its interior. Right: set \(\sqrt{t} f^{\prime \prime}(x_\ast)^{1/2} ( K \ – x_\ast)\), which is a translated (by \(x_\ast\)), tilted (by \(f^{\prime\prime}(x_\ast)\)) and scaled (by \(\sqrt{t}\)) version of \(K\).</figure></div>



<p class="justify-text"><strong>Formal proof.</strong> Let’s first make our life simpler: without loss of generality, we may assume that \(f(x_\ast) = 0\) (by subtracting the minimal value), that \(f^{\prime \prime}(x_\ast) = I\) (by a change of variable whose Jacobian is the square root of \(f^{\prime \prime}(x_\ast)\), leading to the determinant term), and that \(x_\ast = 0\). With the dominated convergence theorem (which was unfortunately unknown to me when I first learned about the method in high school, forcing students to cut integrals into multiple pieces), the proof sketch above is almost exact. We simply need a simple argument, based on the existence of a continuous function \(g: K \to \mathbb{R}\) such that $$ g(x) = \frac{f(x) } {\|  x  \|^2}$$ for \(x \neq 0\) and \(g(0) = \frac{1}{2}\) (here \(\| \! \cdot \! \|\) denotes the standard Euclidean norm). The function \(g\) is trivially defined and continuous on \(K \backslash \{0\}\), the value and continuity at zero is a simple consequence of the Taylor expansion $$ f(x) = f(0) + f^\prime(0)^\top x + \frac{1}{2} x^\top f^{\prime\prime}(0)x + o( \| x\|^2) = \frac{1}{2} \| x\|^2 + o( \| x\|^2). $$</p>



<p class="justify-text">We may then use the same change of variable as above to get the <em>equality</em>: $$I(t) =  \frac{ 1 }{ t^{d/2}  }  \int_{\sqrt{t}  K } h\big( \frac{1}{\sqrt{t}}  y\big) \exp\big[ – \frac{1}{2} y^\top y \cdot g\big(  \frac{1}{\sqrt{t}} y\big) \big] dy.$$ We can write the integral part of the expression above as $$J(t) = \int_{\mathbb{R}^d} a(y,t) dy, $$ with $$a(y,t) = 1_{ y \in \sqrt{t} K } h\big(  \frac{1}{\sqrt{t}}  y\big)\exp\big[ – \frac{1}{2} y^\top y \cdot g\big( \frac{1}{\sqrt{t}} y\big) \big].$$ We have for all \(t&gt;0\) and \(y \in \mathbb{R}^d\), \(|a(y,t)| \leqslant \max_{z \in K} | h(z)| \exp\Big[ – \| y\|^2 \cdot \min_{ z \in K } g(z) \Big]\), which is integrable because \(h\) is continuous on the compact set \(K\) and thus bounded, and \(g\) is strictly positive on \(K\) (since \(f\) is strictly positive except at zero as \(0\) is a strict global minimum), and by continuity, its minimal value is strictly positive. Thus by the <a href="https://en.wikipedia.org/wiki/Dominated_convergence_theorem">dominated convergence theorem</a>: $$\lim_{t \to +\infty} J(t) = \int_{\mathbb{R}^d} \big( \lim_{t \to +\infty} a(y,t) \big) dy = \int_{\mathbb{R}^d}\exp\big[ – \frac{1}{2} y^\top y \big] dy = ( 2\pi)^{d/2}.$$ This leads to the desired result since \(I(t) = J(t) / t^{d/2}\).</p>



<h2>Classical examples</h2>



<p>Two cute examples are often mentioned as applications (adapted from [2]).</p>



<p><strong><a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s formula</a></strong>. We have, by definition of the <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma function</a> \(\Gamma\), and the change of variable \(u = tx\):<br/>$$\Gamma(1+t) = \int_0^\infty \!\! e^{-u} u^{t} du = \int_0^\infty \!\! e^{-tx}t^t x^t t dx = t^{t+1} \int_0^\infty \!\! e^{-t(x-\log x)} dx.$$ Since \(x \mapsto x\, – \log x\) is minimized at \(x=1\) with second derivative equal to \(1\), we get: $$\Gamma(1+t) \sim t^{t+1} e^{-t} \sqrt{2\pi / t} = \big( \frac{t}{e} \big)^t \sqrt{ 2\pi t}.,$$ which is exactly Stirling’s formula, often used when \(t\) is an integer, and then, \(\Gamma(1+t) = t!\).</p>



<p class="justify-text"><strong>Convergence of \(L_p\)-norms to the \(L_\infty\)-norm.</strong> We consider the \(L_p\)-norm of a positive twice continuously differentiable function on the compact set \(K\), with a unique global maximizer at \(x_\ast\) in the interior of \(K\). Then we can write its \(L_p\)-norm \(\|g\|_p\) through $$\| g\|_p^p = \int_K g(x)^p dx = \int_K e^{ p \log g(x)} dx.$$ The function \(f: x \mapsto\  – \log g(x)\) has gradient \(f^\prime(x) = \ – \frac{1}{g(x)}g^\prime(x)\) and Hessian \(f^{\prime\prime}(x)=\  – \frac{1}{g(x)} g^{\prime\prime}(x) + \frac{1}{g(x)^2} g^\prime(x) g^\prime(x)^\top .\) At \(x_\ast\), we get \(f^{\prime\prime}(x_\ast) = \ – \frac{1}{g(x_\ast)} g^{\prime \prime}(x_\ast)\). Thus, using Laplace’s method, we have: $$ \|g\|_p^p =  \frac{A}{p^{d/2}} g(x_\ast)^p (1 + o(1) ),$$ with \(\displaystyle A = \frac{(2\pi g(x_\ast))^{d/2}}{(\det (-g^{\prime\prime}(x_\ast)))^{1/2}}\).</p>



<p class="justify-text">Taking the power \(1/p\), we get: $$ \|g\|_p = \exp \big( \frac{1}{p} \log \|g\|_p^p\big) =  \exp \Big( \frac{1}{p} \log A \ – \frac{d}{2p} \log p +   \log g(x_\ast) + o(1/p) \Big).$$ This leads to, using \(\exp(u) = 1+u + o(u)\) around zero: $$ \| g\|_p = g(x_\ast) \Big( 1 – \frac{d}{2p} \log p + \frac{1}{p} \log A + o(1/p) \Big) = g(x_\ast) \Big( 1 – \frac{d}{2p} \log p + O(1/p) \Big).$$ A surprising fact is that the second-order term does not depend on anything but \(g(x_\ast)\) (beyond \(p\) and \(d\)). Note that this applies also to continuous log-sum-exp functions.</p>



<h2>Applications in Bayesian inference</h2>



<p class="justify-text">It turns out that Laplace’s motivation in deriving this general technique for approximating integrals was exactly Bayesian inference, which he in fact essentially re-discovered and extended (see an interesting account <a href="https://ebrary.net/118879/history/laplaces_bayesian_analysis_1774_1781">here</a>). Let me now explain how Laplace’s method applies.</p>



<p class="justify-text">We consider a parameterized family of probability distributions with density \(p(x|\theta)\) with respect to some base measure \(\mu\) on \(x \in \mathcal{X}\), with \(\theta \in \Theta \subset \mathbb{R}^d\) a set of parameters. </p>



<p class="justify-text">As a running example (the one from Laplace in 1774), we consider the classical Bernoulli distribution for \(x \in \mathcal{X} = \{0,1\}\), and densities with respect to the counting measure, that is, the parameter \(\theta \in [0,1]\) is the probability that \(x=1\).</p>



<p class="justify-text"><strong>From frequentist to Bayesian inference. </strong>We are given \(n\) independent and identically distributed observations \(x_1,\dots,x_n \in \mathcal{X}\), from an unknown probability distribution \(q(x)\). One classical goal of statistical inference is to find the parameter \(\theta \in \Theta\) so that \(p(\cdot| \theta)\) is as close to \(q\) as possible.</p>



<p class="justify-text">In the frequentist framework, <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a> amounts to maximizing $$ \sum_{i=1}^n \log p(x_i | \theta)$$ with respect to \(\theta \in \Theta\). It comes with a series of guarantees, in particular (but not only) when \(q = p(\cdot | \theta)\) for a certain \(\theta \in \Theta\). For our running example, the maximum likelihood estimator \(\hat{\theta}_{\rm ML} = \frac{1}{n} \sum_{i=1}^n x_i\) is the frequency of non-zero outcomes.</p>



<p class="justify-text">In the Bayesian framework, the data are assumed to be generated by a certain \(\theta\), but now with \(\theta\) itself random with some prior probability density \(p(\theta)\) (with respect to the Lebesgue measure). Statistical inference typically does not lead to a point estimator (like the maximum likelihood estimator), but to the full posterior distribution of the parameter \(\theta\) given the observations \(x_1,\dots,x_n\).</p>



<p class="justify-text">The celebrated <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ rule</a> states that the posterior density \(p(\theta | x_1,\dots,x_n)\) can be written as: $$p(\theta | x_1,\dots,x_n) = \frac{ p(\theta) \prod_{i=1}^n p(x_i|\theta) }{p(x_1,\dots,x_n)},$$ where \(p(x_1,\dots,x_n)\) is the marginal density of the data (once the parameter has been marginalized out).</p>



<p class="justify-text">If pressured, a Bayesian will end up giving you a point estimate, but a true Bayesian will not give away the maximum a posteriori estimate (see <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation#Limitations">here</a> for some reasons why), but if you give him or her a loss function (e.g., the square loss), he or she will give away (reluctantly) the estimate that minimizes the posterior risk (e.g., the posterior mean for the square loss).</p>



<p class="justify-text"><strong>Bernoulli and Beta distributions. </strong>In our running Bernoulli example for coin tossing, it is standard to put a <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a> on \(\theta \in [0,1]\), which is here a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> with parameters \(\alpha\) and \(\beta\), that is, $$p(\theta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha-1} ( 1- \theta)^{\beta-1}.$$ The posterior distribution is then also a Beta distribution with parameters \(\alpha + \sum_{i=1}^n x_i\) and \(\beta + \sum_{i=1}^n ( 1- x_i)\). The posterior mean is $$\mathbb{E} [ \theta | x_1,\dots,x_n ] = \frac{\alpha+ \sum_{i=1}^n x_i}{n + \alpha + \beta},$$ and corresponds to having pre-observed \(\alpha\) ones and \(\beta\) zeroes (this is sometimes referred to as Laplace smoothing). However, it is rarely possible to compute the posterior distribution in closed form, hence the need for approximations.</p>



<p class="justify-text"><strong>Laplace approximation. </strong>Thus, in Bayesian inference, integrals of the form $$ \int_{\Theta} h(\theta) p(\theta) \prod_{i=1}^n p(x_i|\theta) d\theta$$ for some function \(h: \Theta \to \mathbb{R}\), are needed. For example, computing the marginal likelihood corresponds to \(h=1\).</p>



<p class="justify-text">By taking logarithms, we can write $$\int_{\Theta} h(\theta) p(\theta) \prod_{i=1}^n p(x_i|\theta) d\theta = \int_{\Theta} h(\theta) \exp\Big(  \log p(\theta) + \sum_{i=1}^n \log p(x_i|\theta) \Big) d\theta, $$ and with \(f_n(\theta) = \ – \frac{1}{n} \log p(\theta) \ – \frac{1}{n} \sum_{i=1}^n \log p(x_i|\theta),\) we have an integral in Laplace form, that is, $$\int_{\Theta} h(\theta) \exp( -n f_n(\theta))d\theta,$$ with a function \(f_n\) that now varies with \(n\). This simple variation does not matter as because of the law of large numbers, when \(n\) is large, \(f_n(\theta)\) tends to a fixed function \(\mathbb{E} \big[ \log p(x|\theta) \big]\). </p>



<p class="justify-text">The Laplace approximation thus requires to compute the minimizer of \(f_n(\theta)\), which is exactly the maximum a posteriori estimate \(\hat{\theta}_{\rm MAP}\), and use the approximation: $$ \int_{\Theta} h(\theta) \exp( -n f_n(\theta))d\theta \approx (2\pi / n)^{d/2} \frac{h( \hat{\theta}_{\rm MAP})}{(\det f_n^{\prime \prime}( \hat{\theta}_{\rm MAP}))^{1/2}} \exp( – n f_n( \hat{\theta}_{\rm MAP})).$$</p>



<p class="justify-text"><strong>Gaussian posterior approximation.</strong> Note that the Laplace approximation exactly corresponds to approximating the log-posterior density by a quadratic form and thus approximating the posterior by a Gaussian distribution with mean \(\hat{\theta}_{\rm MAP}\) and covariance matrix \(\frac{1}{n} f_n^{\prime \prime}( \hat{\theta}_{\rm MAP})^{-1}\). Note that Laplace’s method gives one natural example of such Gaussian approximation and that <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational inference</a> can be used to find better ones.</p>



<p class="justify-text"><strong>Example.</strong> We consider the Laplace approximation of a Beta random variable, that is a Gaussian with mean at the mode of the original density and variance equal to the inverse of the second derivative of the log-density. Below, the mean \(\alpha / (\alpha +\beta)\) is set to a constant, while the variance shrinks due to an increasing \(\alpha+\beta\) (which corresponds to the number of observations in the Bayesian interpretation above).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-6335" height="248" src="https://francisbach.com/wp-content/uploads/2021/07/plot_beta-3.gif" width="598"/>Left: densities. Right: negative log-densities translated so that they have matched first two derivatives at their minimum.</figure></div>



<p class="justify-text">We see above, that for large variances (small \(\alpha +\beta\)), the Gaussian approximation is not tight, while it becomes tighter as the mass gets concentrated around the mode.</p>



<h2>Extensions</h2>



<p class="justify-text"><strong>High-order expansion.</strong> The approximation is based on Taylor expansions of the functions \(h\) (order \(0\)) and \(f\) (order \(2\)). In order to obtain extra terms of the form \(t^{d/2+\nu}\), for \(\nu\) a positive integer, we need higher-order derivatives of \(h\) and \(f\). In more than one dimension, that quickly gets complicated (see, e.g., [3, 4]).</p>



<p class="justify-text">One particular case which is interesting in one dimension is \(h(x) \sim A ( x- x_\ast)^\alpha\), and \(f(x)-f(x_\ast) = B|x-x_\ast|^{\beta}\). Note that \(\alpha=0\) and \(\beta=2\) corresponds to the regular case. A short calculation gives the equivalent \(I(t) \sim \frac{2}{2+\beta}  \frac{A \Gamma\big( \frac{\alpha+1}{\beta} \big)}{(tB)^{\frac{\alpha+1}{\beta}}}\).</p>



<p class="justify-text"><strong><a href="https://en.wikipedia.org/wiki/Stationary_phase_approximation">Stationary phase approximation</a>.</strong> We can also consider integrals of the form $$I(t) = \int_K h(x) \exp( – i t f(x) ) dx,$$ where \(i\) is the usual square root of \(-1\). Here, the main contribution also comes from vectors \(x\) where the gradient of \(f\) is zero. This can be further extended to more general <a href="https://en.wikipedia.org/wiki/Method_of_steepest_descent">complex integrals</a>.</p>



<h2>When Napoléon meets Laplace and Lagrange</h2>



<p class="justify-text">As a conclusion, I cannot resist mentioning a <a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace#I_had_no_need_of_that_hypothesis">classical (potentially not totally authentic) anecdote</a> about encounters between Laplace and Lagrange, two mathematical heroes of mine, and Napoléon, as described in [<a href="https://www.gutenberg.org/files/31246/31246-pdf.pdf">5</a>, page 343]:</p>



<blockquote class="wp-block-quote justify-text"><p>Laplace went in state to beg Napoleon to accept a copy of his work, and the following account of the interview is well authenticated, and so characteristic of all the parties concerned that I quote it in full. Someone had told Napoleon that the book contained no mention of the name of God; Napoleon, who was fond of putting embarrassing questions, received it with the remark, “M. Laplace, they tell me you have written this large book on the system of the universe, and have never even mentioned its Creator.” Laplace, who, though the most supple of politicians, was as stiff as a martyr on every point of his philosophy, drew himself up and answered bluntly, “Je n’avais pas besoin de cette hypothèse-là.” Napoleon, greatly amused, told this reply to Lagrange, who exclaimed, “Ah! c’est une belle hypothèse; ça explique beaucoup de choses.”</p><cite>W. W. Rouse Ball, A Short Account of the History of Mathematics, 1888.</cite></blockquote>



<h2>References</h2>



<p class="justify-text">[1] Pierre-Simon Laplace. <a href="http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32">Mémoire sur la probabilité des causes par les événements</a>, Mémoires de l’Académie royale des sciences de Paris (Savants étrangers), t. VI. p. 621, 27-65, 1774.<br/>[2] Norman Bleistein, Richard A. Handelsman. Asymptotic Expansions of Integrals. Dover<br/>Publications, 1986.[3] Stephen M. Stigler. <a href="https://www.jstor.org/stable/pdf/2245475.pdf">Laplace’s 1774 memoir on inverse probability</a>. <em>Statistical Science</em>, 1(3):359-378, 1986.<br/>[3] Luke Tierney, Robert E. Kass, Joseph B. Kadane. <a href="https://www.jstor.org/stable/pdf/2289652.pdf">Fully exponential Laplace approximations to expectations and variances of nonpositive functions</a>. <em>Journal of the American Statistical Association</em>, 84(407): 710-716, 1989.<br/>[4] Zhenming Shun, Peter McCullagh. <a href="https://www.jstor.org/stable/pdf/2345941.pdf">Laplace approximation of high dimensional integrals</a>. <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 57(4): 749-760, 1995.<br/>[5] W. W. Rouse Ball. <a href="https://www.gutenberg.org/files/31246/31246-pdf.pdf">A Short Account of the History of Mathematics</a>, 1888.</p></div>
    </content>
    <updated>2021-07-23T06:37:20Z</updated>
    <published>2021-07-23T06:37:20Z</published>
    <category term="Machine learning"/>
    <category term="Optimization"/>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2021-07-31T11:22:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/108</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/108" rel="alternate" type="text/html"/>
    <title>TR21-108 |  Limitations of the Impagliazzo--Nisan--Wigderson Pseudorandom Generator against Permutation Branching Programs | 

	Edward Pyne, 

	Salil Vadhan</title>
    <summary>The classic Impagliazzo--Nisan--Wigderson (INW) psesudorandom generator (PRG) (STOC `94) for space-bounded computation uses a seed of length $O(\log n \cdot \log(nwd/\varepsilon))$ to fool ordered branching programs of length $n$, width $w$, and alphabet size $d$ to within error $\varepsilon$. A series of works have shown that the analysis of the INW generator can be improved for the class of $\textit{permutation}$ branching programs or the more general $\textit{regular}$ branching programs, improving the $O(\log^2 n)$ dependence on the length $n$ to $O(\log n)$ or $\tilde{O}(\log n)$.  However, when also considering the dependence on the other parameters, these analyses still fall short of the optimal PRG seed length $O(\log(nwd/\varepsilon))$.
    
    In this paper, we prove that any ``spectral analysis'' of the INW generator requires seed length $$\Omega\left(\log n\cdot \log\log(\min\{n,d\})+\log n\cdot \log(w/\varepsilon)+\log d\right)$$ to fool ordered permutation branching programs of length $n$, width $w$, and alphabet size $d$ to within error $\varepsilon$.  By ``spectral analysis'' we mean an analysis of the INW generator that relies only on the spectral expansion of the graphs used to construct the generator; this encompasses all prior analyses of the INW generator.  Our lower bound matches the upper bound of Braverman--Rao--Raz--Yehudayoff (FOCS 2010, SICOMP 2014) for regular branching programs of alphabet size $d=2$ except for a gap between their $O(\log n \cdot \log\log n)$ term and our $O(\log n \cdot \log\log \min\{n,d\})$ term.  It also matches the upper bounds of Koucky--Nimbhorkar--Pudlak (STOC 2011), De (CCC 2011), and Steinke (ECCC 2012) for constant-width ($w=O(1)$) permutation branching programs of alphabet size $d=2$ to within a constant factor.  
    
    To fool permutation branching programs in the stronger measure of $\textit{spectral norm}$, we prove that any spectral analysis of the INW generator requires a seed of length $\Omega(\log n\cdot \log\log n+\log n\cdot\log(1/\varepsilon)+\log d)$ when the width is at least polynomial in $n$ ($w=n^{\Omega(1)}$), matching the recent upper bound of Hoza--Pyne--Vadhan (ITCS `21) to within a constant factor.</summary>
    <updated>2021-07-23T06:10:53Z</updated>
    <published>2021-07-23T06:10:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-31T11:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/107</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/107" rel="alternate" type="text/html"/>
    <title>TR21-107 |  Classification of OBDD size for monotone 2-CNFs | 

	igor razgon</title>
    <summary>We introduce a new graph parameter called linear upper maximum induced
matching width \textsc{lu-mim width}, denoted for a graph $G$ by $lu(G)$.
We prove that the smallest size of the \textsc{obdd} for $\varphi$,
the monotone 2-\textsc{cnf} corresponding to $G$, is sandwiched 
between $2^{lu(G)}$ and $n^{O(lu(G))}$. 
The upper bound is based on a combinatorial statement that might 
be of an independent interest.
We show that the bounds in terms of this parameter are best possible.

The new parameter is closely related to two existing parameters:
linear maximum induced matching width (\textsc{lmim width}) and linear symmetric induced matching width 
(\textsc{lsim width}). We prove that \textsc{lu-mim width} lies strictly in between these two
parameters, being dominated by \textsc{lsim width} and dominating \textsc{lmim width}.
We conclude that neither of the two existing parameters can be used instead of \textsc{lu-mim width}
to characterize the size of \textsc{obdd}s for monotone $2$-\textsc{cnf}s and this justifies introduction
of the new parameter.</summary>
    <updated>2021-07-23T06:08:12Z</updated>
    <published>2021-07-23T06:08:12Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-31T11:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/106</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/106" rel="alternate" type="text/html"/>
    <title>TR21-106 |  The Space Complexity of Sampling | 

	Eshan Chattopadhyay, 

	Jesse Goodman, 

	David Zuckerman</title>
    <summary>Recently, there has been exciting progress in understanding the complexity of distributions. Here, the goal is to quantify the resources required to generate (or sample) a distribution. Proving lower bounds in this new setting is more challenging than in the classical setting, and has yielded interesting new techniques and surprising applications. In this work, we initiate a study of the complexity of sampling with limited memory, and prove small-space analogs of several results previously known only for sampling in AC$^0$.

1. We exhibit an explicit boolean function $b:\{0,1\}^n\to\{0,1\}$ that cannot be computed by width $2^{\Omega(n)}$ read-once branching programs (ROBPs), even on average, but such that $(\mathbf{U}_n,b(\mathbf{U}_n))$ can be exactly sampled by ROBPs of width $O(n)$.

2. We exhibit an explicit boolean function $b:\{0,1\}^n\to\{0,1\}$ such that any distribution sampled by an ROBP of width $2^{\Omega(n)}$ has statistical distance $\frac{1}{2}-2^{-\Omega(n)}$ from $(\mathbf{U}_n,b(\mathbf{U}_n))$. We show that any such $b$ witnesses exponentially small correlation bounds against ROBPs, and we extend these results to hold for the unknown-order setting.

3. We show that any distribution sampled by an ROBP of width $2^{\Omega(n)}$ has statistical distance $1-2^{-\Omega(n)}$ from any distribution that is uniform over a good code. More generally, we obtain sampling lower bounds for any list decodable code, which are nearly tight. Using a known connection, we also obtain data structure lower bounds for storing codewords.

Along the way, we prove a direct product theorem and several equivalence theorems. These tools offer a generic method to construct distributions with strong sampling lower bounds, and translate these lower bounds into correlation bounds against ROBPs. As an application of our direct product theorem, we show a strong complexity separation between sampling with AC$^0$ circuits and sampling with ROBPs.</summary>
    <updated>2021-07-23T05:15:13Z</updated>
    <published>2021-07-23T05:15:13Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-31T11:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18973</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/07/22/the-reach-of-dichotomy/" rel="alternate" type="text/html"/>
    <title>The Reach of Dichotomy</title>
    <summary>Congratulations on the 2021 Gödel Prize Composite including Richerby’s talk at STOC 2021 Andrey Bulatov, Martin Dyer and David Richerby, and Jin-Yi Cai and Xi Chen are the authors of three papers awarded the 2021 Gödel Prize. The citations by the ACM and EATCS say clearly that the papers not the people win the prize, […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Congratulations on the 2021 Gödel Prize</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/22/the-reach-of-dichotomy/godelprize2021/" rel="attachment wp-att-18975"><img alt="" class="alignright wp-image-18975" height="148" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/GodelPrize2021.png?resize=222%2C148&amp;ssl=1" width="222"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite including Richerby’s <a href="https://www.youtube.com/watch?v=4vLdQ-vkVtg">talk</a> at STOC 2021</font></td>
</tr>
</tbody>
</table>
<p>
Andrey Bulatov, Martin Dyer and David Richerby, and Jin-Yi Cai and Xi Chen are the authors of three papers awarded the 2021 Gödel Prize. The citations by the <a href="https://sigact.org/prizes/g&#xF6;del/citation2021.html">ACM</a> and <a href="https://eatcs.org/index.php/component/content/article/1-news/2885-2021-05-07-21-20-13">EATCS</a> say clearly that the papers not the people win the prize, but our “blog invariant” is to show photos of people at the top, so there they are.</p>
<p>
Today we congratulate the people—not the papers—and tell some of the technical story behind this richly deserved award.<br/>
<span id="more-18973"/></p>
<p>
Dick and I have intended to write about this since the prize was announced in early May. Travel and personal events have slowed the blog, plus in my case, June was by far the most pressing month of the “chess cheating pandemic” since chess moved online, and that carried into July. The award to our friend Jin-Yi—who has been a faculty colleague of both of us—is the “something else about UW Madison on our mind” that was mentioned in the June 20 <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">post</a>, during which chess cases arrived in real time. I am thankful that this month, to judge by the weekly roundups of games by <a href="https://en.chessbase.com/">ChessBase</a> and <a href="https://theweekinchess.com/">The Week in Chess</a>, most major tournaments are back to being played in-person—where the yearly volume of cases coming to my attention has been on either side of ten, rather than in middle triple digits.</p>
<p>
That the papers win the award calls something else to mind: We can no longer point to an “original manuscript” with papers being electronic. Perhaps the continued rise of <em>non-fungible tokens</em> (<a href="https://en.wikipedia.org/wiki/Non-fungible_token">NFTs</a>) will restore this distinction. Tim Berners-Lee recently <a href="https://www.cnn.com/style/article/tim-berners-lee-nft-auction/index.html">created</a> and auctioned off an NFT of his original code for the World Wide Web. This may be done to prize-winning papers in the future. Then the prize money could create an inherent base value, such as we discussed for “semi-fungible tokens” at the end of this <a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/">post</a>, so that the prize really would go to the papers. </p>
<p>
</p><p/><h2> The Beginning of Dichotomy </h2><p/>
<p/><p>
The term “dichotomy” in complexity theory first became prominent with Thomas Schaefer’s 1978 <a href="http://aleteya.cs.buap.mx/~jlavalle/papers/complexity/p216-schaefer.pdf">paper</a> about <img alt="{\mathrm{SAT}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BSAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-like problems. For a large and natural class <img alt="{\mathcal{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of such problems, he proved that either the problem is <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-complete like <img alt="{\mathrm{SAT}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BSAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> itself or it is in <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>—never <a href="https://en.wikipedia.org/wiki/NP-intermediate">intermediate</a> between them. The famous 1975 <a href="https://en-academic.com/dic.nsf/enwiki/2272652">theorem</a> of Richard Ladner showed that if <img alt="{\mathsf{NP \neq P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP+%5Cneq+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then plenty of intermediate languages exist, but no problem in <img alt="{\mathcal{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can be among them. That is the dichotomy: everything in <img alt="{\mathcal{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is either easy or maximally hard. A similar project was taken up for <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> by Nadia Creignou and Nicolas (Miki) Hermann in a 1996 <a href="https://www.sciencedirect.com/science/article/pii/S0890540196900164">paper</a> showing that every <img alt="{\mathrm{\#SAT}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7B%5C%23SAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-like counting problem is either in <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or is <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-complete. </p>
<p>
What does “<img alt="{\mathrm{SAT}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BSAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-like” mean? Consider the clause <img alt="{(\bar{x}_i \vee \bar{x}_j \vee x_k)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cbar%7Bx%7D_i+%5Cvee+%5Cbar%7Bx%7D_j+%5Cvee+x_k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. It is satisfied if and only if the values of the variables belong to the relation </p>
<p align="center"><img alt="\displaystyle  R = \{(0,0,0),(0,0,1),(0,1,0),(0,1,1),(1,0,0),(1,0,1),(1,1,1)\}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R+%3D+%5C%7B%280%2C0%2C0%29%2C%280%2C0%2C1%29%2C%280%2C1%2C0%29%2C%280%2C1%2C1%29%2C%281%2C0%2C0%29%2C%281%2C0%2C1%29%2C%281%2C1%2C1%29%5C%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
which excludes only the non-satisfying assignment <img alt="{(1,1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2C1%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Note that if we had the clause <img alt="{(\bar{x}_i \vee x_j \vee \bar{x}_k)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cbar%7Bx%7D_i+%5Cvee+x_j+%5Cvee+%5Cbar%7Bx%7D_k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> instead, we could re-use <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> by switching <img alt="{x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{x_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the given sequence of variables. Putting negated literals first, if we instead want to require exactly one literal to be true we could apply one of the following relations to each clause, depending on how many negated variables it has: </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  R^0 &amp;=&amp; \{(1,0,0),(0,1,0),(0,0,1)\}\\ R^1 &amp;=&amp; \{(0,0,0),(1,1,0),(1,0,1)\}\\ R^2 &amp;=&amp; \{(0,1,0),(1,0,0),(1,1,1)\}\\ R^3 &amp;=&amp; \{(0,1,1),(1,0,1),(1,1,0)\} \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++R%5E0+%26%3D%26+%5C%7B%281%2C0%2C0%29%2C%280%2C1%2C0%29%2C%280%2C0%2C1%29%5C%7D%5C%5C+R%5E1+%26%3D%26+%5C%7B%280%2C0%2C0%29%2C%281%2C1%2C0%29%2C%281%2C0%2C1%29%5C%7D%5C%5C+R%5E2+%26%3D%26+%5C%7B%280%2C1%2C0%29%2C%281%2C0%2C0%29%2C%281%2C1%2C1%29%5C%7D%5C%5C+R%5E3+%26%3D%26+%5C%7B%280%2C1%2C1%29%2C%281%2C0%2C1%29%2C%281%2C1%2C0%29%5C%7D+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
Then we can say that both the “Exactly One 3SAT” decision problem in <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and the associated counting problem in <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are <em>defined by</em> the collection <img alt="{\Gamma = \{R^0,R^1,R^2,R^3\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma+%3D+%5C%7BR%5E0%2CR%5E1%2CR%5E2%2CR%5E3%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of relations. The following shows how they are subcases of the general constraint satisfaction problem (<a href="https://en.wikipedia.org/wiki/Constraint_satisfaction_problem">CSP</a>):</p>
<p/><p/>
<ul>
<li>
Parameter: A set <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of relations over some domain <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. <p/>
</li><li>
Instance: A set <img alt="{\Phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of tuples <img alt="{C_1,\dots,C_m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_1%2C%5Cdots%2CC_m%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> over variables <img alt="{X = \{x_1,\dots,x_n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5C%7Bx_1%2C%5Cdots%2Cx_n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, with each <img alt="{C_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> associated to some relation <img alt="{R_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. <p/>
</li><li>
Question: Is there an assignment <img alt="{\alpha: X \rightarrow D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%3A+X+%5Crightarrow+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that makes <img alt="{\alpha(C_j) \in R_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28C_j%29+%5Cin+R_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for each <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? For the counting version, called <img alt="{\mathsf{\#CSP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23CSP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, how many such assignments are there?
</li></ul>
<p/><p><br/>
Here <img alt="{\alpha(C_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28C_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> means the ordered tuple of values <img alt="{\alpha(x_i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28x_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of the variables <img alt="{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in <img alt="{C_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. If <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are considered part of the instance—where they must be finite—the problem is <em>uniform</em>, but if they are fixed (and possibly infinite), it is <em>nonuniform</em>. For a simple analogy: for every fixed context-free grammar <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, its language <img alt="{L(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> belongs to <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, but a much stronger fact is that the uniform acceptance problem <img alt="{A_{CFG} = \{\langle G,x\rangle: x \in L(G)\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7BCFG%7D+%3D+%5C%7B%5Clangle+G%2Cx%5Crangle%3A+x+%5Cin+L%28G%29%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> also belongs to <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Another relaxation is to allow different domains <img alt="{D_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for different variables <img alt="{x_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
In the Boolean case, <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is fixed to be <img alt="{\{0,1\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We can now say what Schaefer and Creignou-Hermann proved. Schaefer showed that every nonuniform Boolean CSP with finite <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is either in <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-complete. He showed the former holds only when <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> falls into one of six explicit cases, where which case is also decidable in polynomial time given <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Creignou and Hermann gave a similar result for Boolean #CSP but with a smaller list of cases—because there are cases where the decision problem is in <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> but the counting problem remains <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-complete. A simple such case is where <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has only the relation <img alt="{R = \{(1,0),(0,1),(1,1)\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%3D+%5C%7B%281%2C0%29%2C%280%2C1%29%2C%281%2C1%29%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which defines monotone <img alt="{\mathrm{2SAT}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7B2SAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
</p><p/><h2> Extending the Reach </h2><p/>
<p/><p>
Also in the 1990s, Tomás Feder and Moshe Vardi wrote <a href="https://dl.acm.org/doi/10.1145/167088.167245">two</a> <a href="https://epubs.siam.org/doi/abs/10.1137/S0097539794266766?journalCode=smjcat">papers</a> on extending Schaefer’s dichotomy to a wider class of problems. To quote the first sentence of the latter’s abstract:</p>
<blockquote><p><b> </b> <em> This paper starts with the project of finding a large subclass of <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> which exhibits a dichotomy. </em>
</p></blockquote>
<p/><p>
The first step was to go to larger domains <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. For example, consider <img alt="{D = \{1,2,3\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%3D+%5C%7B1%2C2%2C3%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Take <img alt="{R = \{(1,2),(2,3),(1,3)\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%3D+%5C%7B%281%2C2%29%2C%282%2C3%29%2C%281%2C3%29%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is simply the relation <img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Given an undirected graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, its edges become the tuples. The resulting CSP is satisfiable if and only if <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is 3-colorable. Of course, this is <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-complete. Feder and Vardi arrived at this by going down from large subclasses of <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> defined by logic in which Ladner-type constructions can still be expressed, so that dichotomy does <em>not</em> hold. When they imposed all of their conditions, they were left with nonuniform CSPs, and conjectured dichotomy for their decision problems.</p>
<p>
The corresponding question for counting problems was solved <a href="https://www.cs.sfu.ca/~abulatov/papers/counting-acm.pdf">first</a>, by Bulatov. Dyer and Richerby <a href="https://arxiv.org/pdf/1003.3879.pdf">followed</a> by simplifying Bulatov’s proof in a way that also allows deciding whether a given finite <img alt="{(\Gamma,D)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5CGamma%2CD%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> falls into one of the polynomial-time cases (assuming <img alt="{\mathsf{\#P \neq P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P+%5Cneq+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to begin with). The latter task runs in polynomial time with an oracle for testing what they call <em>strong balance</em>, which in turn polynomial-time mapping-reduces to Graph Isomorphism—so by Laci Babai’s celebrated theorem they can decide in time quasi-polynomial in <img alt="{|\Gamma|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5CGamma%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> whether it is a hard or easy case.</p>
<p>
Finally—and not awarded any of this prize money—a 2017 <a href="https://arxiv.org/abs/1703.03021">paper</a> by Bulatov proved dichotomy for the decision case of nonuniform CSPs. Independently, so did a <a href="https://arxiv.org/abs/1704.01914">paper</a> by Dmitriy Zhuk. We will explore just a few of the underlying ideas in the next section. </p>
<p align="center"><img alt="\displaystyle  {\S} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5CS%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Here we continue with the further extension made by Jin-Yi and Xi Chen. This starts by writing the counting problem as <a name="partitionfn"/></p><a name="partitionfn">
<p align="center"><img alt="\displaystyle  \#\Phi = \sum_{\alpha: X \rightarrow D} \prod_{j=1}^m w(\alpha(C_j),R_j), \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%23%5CPhi+%3D+%5Csum_%7B%5Calpha%3A+X+%5Crightarrow+D%7D+%5Cprod_%7Bj%3D1%7D%5Em+w%28%5Calpha%28C_j%29%2CR_j%29%2C+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
</a><p><a name="partitionfn"/> where for simple #CSP, <img alt="{w(\alpha(C_j),R_j) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%28%5Calpha%28C_j%29%2CR_j%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if the values assigned by <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to the variables in <img alt="{C_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> satisfy the associated relation <img alt="{R_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="{w(\alpha(C_j),R_j) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%28%5Calpha%28C_j%29%2CR_j%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> otherwise. However, one need not score an unsatisfied constraint as zero. Giving a nonzero partial credit <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for a wrong answer <img alt="{\alpha(C_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28C_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to <img alt="{R_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> might make you more magnetic as a teacher. It would literally be magnetism: the simple case where <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> consists of just the binary equality relation on <img alt="{\{0,1\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, so that for all binary clauses (edges) <img alt="{(u,v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> we have </p>
<p align="center"><img alt="\displaystyle  w(0,0) = w(1,1) = 1, \qquad w(0,1) = w(1,0) = \lambda, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++w%280%2C0%29+%3D+w%281%2C1%29+%3D+1%2C+%5Cqquad+w%280%2C1%29+%3D+w%281%2C0%29+%3D+%5Clambda%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>yields the simplest case of the <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a> of magnetism. But we can also take a quantum leap and make you complex: we can allow the <b>weights</b> <img alt="{w(\alpha(C_j),R_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%28%5Calpha%28C_j%29%2CR_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to have complex values. In particular, we can have functions of the form </p>
<p align="center"><img alt="\displaystyle  \sum_{\alpha: X \rightarrow D} \prod_{j=1}^m \omega^{g(\alpha(C_j),R_j)}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7B%5Calpha%3A+X+%5Crightarrow+D%7D+%5Cprod_%7Bj%3D1%7D%5Em+%5Comega%5E%7Bg%28%5Calpha%28C_j%29%2CR_j%29%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>where <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a complex root of unity. Writing the product of powers of <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as just <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> raised to a sum of terms ranging over all the clauses <img alt="{C_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the instance <img alt="{\Phi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and inserting a normalizing constant <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we obtain functions of the form </p>
<p align="center"><img alt="\displaystyle  Z(f_\Phi) = r\sum_{\alpha: X \rightarrow D} \omega^{f_\Phi(\alpha)}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Z%28f_%5CPhi%29+%3D+r%5Csum_%7B%5Calpha%3A+X+%5Crightarrow+D%7D+%5Comega%5E%7Bf_%5CPhi%28%5Calpha%29%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>which are generally called <a href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)">partition functions</a>. Indeed, we long ago <a href="https://rjlipton.wpcomstaging.com/2012/07/08/grilling-quantum-circuits/">discussed</a> various efficient ways to convert quantum circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> into polynomials <img alt="{f_C(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> so that, with <img alt="{D = \{0,1\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%3D+%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> depending only on <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{Z(f_C)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%28f_C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> gives the acceptance amplitude of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The acceptance probability can be represented in like manner. See also this <a href="https://arxiv.org/pdf/1409.5627.pdf">paper</a> for another view of the bridge from the Ising model to quantum circuits.</p>
<p>
Thus, the dichotomy theorems that Jin-Yi and Xi have obtained for complex weighted CSPs take matters into the quantum realm. What this may say about the class <img alt="{\mathsf{BQP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (bounded-error quantum polynomial time) we leave to the end.</p>
<p>
</p><p/><h2> Some of the Ideas </h2><p/>
<p/><p>
One key idea can be approached as generalizing a familiar feature of linear algebra. Suppose we take some number <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of vectors of length <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that belong to a linear subspace <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We can arrange them into an <img alt="{\ell \times k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%5Ctimes+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> matrix <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Now let us multiply <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on the left by a row vector <img alt="{a = (a_1,\dots,a_\ell)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+%28a_1%2C%5Cdots%2Ca_%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then <img alt="{aM}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BaM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is another vector of length <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This vector belongs to <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> because it is just a linear combination of our <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-many vectors in <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
The insight for abstraction is that we can regard <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as having been applied individually to each of the <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> columns of <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We got one new <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-tuple from this application, but it still belonged to <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. So now, let us consider “multiplying” <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on the left by an arbitrary <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-ary function <img alt="{h(x_1,\dots,x_\ell)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x_1%2C%5Cdots%2Cx_%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, so that <img alt="{hM}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BhM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is always a <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-tuple. And let us relax <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be any (finite) set <img alt="{R \subseteq D^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%5Csubseteq+D%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is our (finite) domain. </p>
<blockquote><p><b>Definition 1</b> <em> <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> has <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> as a <b>polymorphism</b> if for any <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> made from <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> members of <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>, allowing repeats, <img alt="{hM}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BhM%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> belongs to <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. A set <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> of relations <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> has <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> as a polymorphism if every <img alt="{R \in \Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%5Cin+%5CGamma%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> has <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> as a polymorphism. </em>
</p></blockquote>
<p/><p>
With <img alt="{\ell = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, the binary <em>and</em> function is a polymorphism of the relation <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> above that represented 3-clauses with two negated literals. This is because the excluded triple <img alt="{(1,1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2C1%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> cannot be written as the <em>and</em> of the other triples. This extends to the relation <img alt="{R'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for satisfiability with three negated literals, which excludes <img alt="{(1,1,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2C1%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, so <em>and</em> is a polymorphism of <img alt="{\Gamma = \{R,R'\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma+%3D+%5C%7BR%2CR%27%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and also for <img alt="{k &gt; 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3E+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to pertain to <a href="https://en.wikipedia.org/wiki/Horn_clause">Horn clauses</a> in general. Dually, <em>or</em> is a polymorphism for satisfiability of dual-Horn clauses. </p>
<p>
The case of <img alt="{\mathrm{2SAT}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7B2SAT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is trickier. Its <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has three constituent relations, depending on the number of negated literals: </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  R^0 &amp;=&amp; \{(1,0),(0,1),(1,1)\}\\ R^1 &amp;=&amp; \{(0,0),(0,1),(1,1)\}\\ R^2 &amp;=&amp; \{(0,0),(0,1),(1,0)\}. \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++R%5E0+%26%3D%26+%5C%7B%281%2C0%29%2C%280%2C1%29%2C%281%2C1%29%5C%7D%5C%5C+R%5E1+%26%3D%26+%5C%7B%280%2C0%29%2C%280%2C1%29%2C%281%2C1%29%5C%7D%5C%5C+R%5E2+%26%3D%26+%5C%7B%280%2C0%29%2C%280%2C1%29%2C%281%2C0%29%5C%7D.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Is there an <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that is simultaneously a polymorphism of each one? Neither <em>and</em> nor <em>or</em> works: the former fails on <img alt="{R^0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%5E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> because the component-wise <em>and</em> of <img alt="{(1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{(0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%280%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the excluded <img alt="{(0,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%280%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <em>or</em> similarly fails on <img alt="{R^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. In fact, <em>none</em> of the sixteen binary Boolean functions is a homomorphism of all three. But when we go up to <img alt="{\ell = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, however, it turns out that the <em>majority-of-3</em> function is a polymorphism: any duplicated tuple rules, so only the three instances of three separate tuples need to be checked.</p>
<p>
Going back to the linear dependence idea, the ternary function <img alt="{h(x_1,x_2,x_3) = x_1 + x_2 + x_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x_1%2Cx_2%2Cx_3%29+%3D+x_1+%2B+x_2+%2B+x_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a polymorphism of <em>affine</em> relations. For <img alt="{D = \{0,1\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%3D+%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where <img alt="{+}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the same as exclusive-or, it has the property that <img alt="{h(x,y,y) = h(y,y,x) = x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x%2Cy%2Cy%29+%3D+h%28y%2Cy%2Cx%29+%3D+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This property is named for Anatoly Maltsev, who used it long before any of this complexity work began. It turned out to be a key to demarcating the easy cases for the dichotomy in higher dimensions. Affine relations were already the lone easy case in Creignou-Hermann for Boolean #CSP, while we can now crisply state Schaefer’s original dichotomy for decision CSP: affine, majority-of-3, <em>and</em>, <em>or</em>, plus two trivial cases where the all-zero or all-one assignment always works, are the polymorphisms that characterize the SAT-like problems in <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
A second basic idea is to re-cast the counting problems as ones of counting <em>homomorphisms</em> between relational structures. For instance, let <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be any graph and <img alt="{K_r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> the complete graph on <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> nodes. A homomorphism <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has the property that when <img alt="{(u,v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an edge of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{(h(u),h(v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28h%28u%29%2Ch%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an edge of <img alt="{K_r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>—in particular, <img alt="{h(u) \neq h(v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28u%29+%5Cneq+h%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This is possible if and only if <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-colorable. This approach was greatly enhanced in a 1998 <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.40.2693">paper</a> by Peter Jeavons and its follow-ups with other authors. It was particularly useful to the decidability part of Dyer and Richerby’s paper.</p>
<p>
A third idea used by Cai and Chen originated in a 2010 <a href="https://arxiv.org/pdf/1012.5659.pdf">paper</a> of theirs with Pinyan Lu for the case of non-negative weights. It is to stratify the sum in (<a href="https://rjlipton.wpcomstaging.com/feed/#partitionfn">1</a>) over all <img alt="{t \leq n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> by mapping </p>
<p align="center"><img alt="\displaystyle  f(a_1,\dots,a_t) = \sum_{\alpha: \alpha(x_1)=a_1,\dots,\alpha(x_t)=a_t} \;\;\; \prod_{j=1}^m w(\alpha(C_j),R_j). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28a_1%2C%5Cdots%2Ca_t%29+%3D+%5Csum_%7B%5Calpha%3A+%5Calpha%28x_1%29%3Da_1%2C%5Cdots%2C%5Calpha%28x_t%29%3Da_t%7D+%5C%3B%5C%3B%5C%3B+%5Cprod_%7Bj%3D1%7D%5Em+w%28%5Calpha%28C_j%29%2CR_j%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>With <img alt="{t = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> this is just a single evaluation; with <img alt="{t=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> this is the original function. They create an oracle that either evaluates the whole sum or allows progressing from <img alt="{t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. They create a matrix <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with <img alt="{d^{t-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%5E%7Bt-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> rows for the possible fixed parts <img alt="{\vec{a} = (a_1,\dots,a_{t-1})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Ba%7D+%3D+%28a_1%2C%5Cdots%2Ca_%7Bt-1%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> columns for the possible choices of <img alt="{a_t = \alpha(x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_t+%3D+%5Calpha%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="{d = |D|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+%7CD%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The entries are <img alt="{M[\vec{a},c] = f(a_1,\dots,a_{t-1},c)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%5B%5Cvec%7Ba%7D%2Cc%5D+%3D+f%28a_1%2C%5Cdots%2Ca_%7Bt-1%7D%2Cc%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="{M[\vec{a},\cdot]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%5B%5Cvec%7Ba%7D%2C%5Ccdot%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> stands for the row vector over all <img alt="{c \in D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cin+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The oracle, given <img alt="{\vec{a}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Ba%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, returns the multiple of <img alt="{M[\vec{a},\cdot]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%5B%5Cvec%7Ba%7D%2C%5Ccdot%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that normalizes the first non-zero entry to <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, or the all-zero vector if <img alt="{M[\vec{a},\cdot]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%5B%5Cvec%7Ba%7D%2C%5Ccdot%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is all zero. </p>
<p>
A big extra challenge in the complex-weights case is how to handle cancellations, as begun in this <a href="https://arxiv.org/abs/0903.4728">paper</a> by Cai, Chen, and Lu. Implementing the oracle efficiently requires the Maltsev property, a kind of “proto-unitarity” condition on <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (under which <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has at most <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> pairwise linearly independent rows and every two such rows are orthogonal), and a third property they call the “Type Partition” condition to avert blowup in the number of queries. Whether the classification they get is decidable remains open.</p>
<p>
Here is where we say to go to the papers for details, but we hope we have expressed ideas and their motivation as a guide. We have a little more to say about interpretation and making further progress on the dichotomy.</p>
<p>
</p><p/><h2> Taking Dichotomy Further </h2><p/>
<p/><p>
Progress on the counting dichotomy increases the “internal evidence” for <img alt="{\mathsf{\#P \neq P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P+%5Cneq+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which of course is implied by <img alt="{\mathsf{NP \neq P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP+%5Cneq+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. To paraphrase how our blogging friends Scott and Lance and Bill have often put it:</p>
<blockquote><p><b> </b> <em> Why would all this finely filigreed structure with sharp demarcation between hard and easy cases exist if <img alt="{\mathsf{P \neq NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+NP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> were a false illusion? </em>
</p></blockquote>
<p/><p>
At the very least, this puts the onus on a believer in <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to give an alternative explanation of how such structure could arise. </p>
<p>
It strikes us, however, that this also puts a squeeze on <img alt="{\mathsf{BQP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> under the conventional wisdom that it is intermediate between <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, without being <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-hard in particular. This is not in technical conflict with the Cai-Chen dichotomy because exactly computing the acceptance probabilities of a class of quantum circuits (on inputs built into the circuits) can be <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-complete while the circuits use only a gap in probabilities to solve problems that are easy or intermediate. But we suspect that this still limits possible ways to capture <img alt="{\mathsf{BQP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> “naturally” by descriptive logic.</p>
<p>
The prize paper with Chen is just one part of a long-term project by Jin-Yi and co-workers on classifying counting problems and showing where dichotomy applies. A 2010 <a href="https://arxiv.org/abs/1004.0803">paper</a> of Jin-Yi with Lu and Sangxia Huang bridged from #CSP and a subcase of counting homomorphisms between graphs to a wider kind of sum-over-products formula originally called a <b>holant</b> <a href="http://pages.cs.wisc.edu/~jyc/papers/LAA-journalversion.pdf">here</a>. Recent progress on dichotomy for holants is represented by this FOCS 2020 <a href="https://arxiv.org/abs/2005.07906">paper</a> with Shuai Shao. This is also both directly relevant to quantum computation and maps out progress that is still out there to make. The paper has a one-page prologue that summons characters from legendary fantasy (and <em>Star Wars</em>) to conquer quantum issues.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Again, we congratulate the winners—the human winners. What will the full implications of this work become in the years ahead?</p>
<p/><p><br/>
[some minor word changes]</p></font></font></div>
    </content>
    <updated>2021-07-23T01:32:50Z</updated>
    <published>2021-07-23T01:32:50Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="#P"/>
    <category term="algebra"/>
    <category term="Andrey Bulatov"/>
    <category term="complexity theory"/>
    <category term="constraint satisfaction problems"/>
    <category term="counting complexity"/>
    <category term="David Richerby"/>
    <category term="dichotomy"/>
    <category term="godel prize"/>
    <category term="Jin-Yi Cai"/>
    <category term="Martin Dyer"/>
    <category term="P&#x2260;NP"/>
    <category term="quantum"/>
    <category term="Xi Chen"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-07-31T11:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/105</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/105" rel="alternate" type="text/html"/>
    <title>TR21-105 |  Functional lower bounds for restricted arithmetic circuits of depth four | 

	Suryajith Chillara</title>
    <summary>Recently, Forbes, Kumar and Saptharishi [CCC, 2016] proved that there exists an explicit $d^{O(1)}$-variate and degree $d$ polynomial $P_{d} \in VNP$ such that if any depth four circuit $C$ of bounded formal degree $d$ which computes a polynomial of bounded individual degree $O(1)$, that is functionally equivalent to $P_d$, then $C$ must have size $2^{\Omega(\sqrt{d}\log{d})}$.

The motivation for their work comes from Boolean Circuit Complexity. Based on a characterization for $ACC^0$ circuits by Yao [FOCS, 1985] and Beigel and Tarui [CC, 1994], Forbes, Kumar and Saptharishi [CCC, 2016] observed that functions in $ACC^0$ can also be computed by algebraic $\Sigma\mathord{\wedge}\Sigma\Pi$ circuits (i.e., circuits of the form -- sums of powers of polynomials) of $2^{\log^{O(1)}n}$ size. Thus they argued that a $2^{\omega(\log^{O(1)}{n})}$ "functional" lower bound for an explicit polynomial $Q$ against $\Sigma\mathord{\wedge}\Sigma\Pi$ circuits would imply a lower bound for the "corresponding Boolean function" of $Q$ against non-uniform $ACC^0$. In their work, they ask if their lower bound be extended to $\Sigma\mathord{\wedge}\Sigma\Pi$ circuits.

In this paper, for large integers $n$ and $d$ such that $\Omega(\log^2{n})\leq d\leq n^{0.01}$, we show that any $\Sigma\mathord{\wedge}\Sigma\Pi$ circuit of bounded individual degree at most $O(\frac{d}{k^2})$ that functionally computes Iterated Matrix Multiplication polynomial $IMM_{n,d}$ ($\in VP$) over $\{0,1\}^{n^2d}$ must have size $n^{\Omega(k)}$. Since Iterated Matrix Multiplication $IMM_{n,d}$ over $\{0,1\}^{n^2d}$ is functionally in $GapL$, improvement of the afore mentioned lower bound to hold for quasipolynomially large values of individual degree would imply a fine-grained separation of $ACC^0$ from $GapL$.

For the sake of completeness, we also show a syntactic size lower bound against any $\Sigma\mathord{\wedge}\Sigma\Pi$ circuit computing $IMM_{n,d}$ (for the same regime of $d$) which is tight over large fields. Like Forbes, Kumar and Saptharishi [CCC, 2016], we too prove lower bounds against circuits of bounded formal degree which functionally compute $IMM_{n,d}$, for a slightly larger range of individual degree.</summary>
    <updated>2021-07-22T13:09:16Z</updated>
    <published>2021-07-22T13:09:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-31T11:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5561</id>
    <link href="https://www.scottaaronson.com/blog/?p=5561" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5561#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5561" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Slowly emerging from blog-hibervacation</title>
    <summary xml:lang="en-US">Alright everyone: Victor Galitski has an impassioned rant against out-of-control quantum computing hype, which I enjoyed and enthusiastically recommend, although I wished Galitski had engaged a bit more with the strongest arguments for optimism (e.g., the recent sampling-based supremacy experiments, the extrapolations that show gate fidelities crossing the fault-tolerance threshold within the next decade). Even […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Alright everyone:</p>



<ol><li>Victor Galitski has an <a href="https://www.linkedin.com/pulse/quantum-computing-hype-bad-science-victor-galitski-1c?trk=public_post-content_share-article_title">impassioned rant against out-of-control quantum computing hype</a>, which I enjoyed and enthusiastically recommend, although I wished Galitski had engaged a bit more with the strongest arguments for optimism (e.g., the recent sampling-based supremacy experiments, the extrapolations that show gate fidelities crossing the fault-tolerance threshold within the next decade).  Even if I’ve been saying similar things on this blog for 15 years, I clearly haven’t been doing so in a style that works for everyone.  Quantum information needs as many people as possible who will tell the truth as best they see it, unencumbered by any competing interests, and has nothing legitimate to fear from that.  The modern intersection of quantum theory and computer science has raised profound scientific questions that will be with us for decades to come.  It’s a lily that need not be gilded with hype.<br/></li><li>Last month Limaye, Srinivasan, and Tavenas posted <a href="https://eccc.weizmann.ac.il/report/2021/081/">an exciting preprint to ECCC</a>, which apparently proves the first (slightly) superpolynomial lower bound on the size of constant-depth arithmetic circuits, over fields of characteristic 0.  Assuming it’s correct, this is another small victory in the generations-long war against the P vs. NP problem.<br/></li><li>I’m grateful to the Texas Democratic legislators who fled the state to prevent the legislature, a couple miles from my house, having a quorum to enact new voting restrictions, and who thereby drew national attention to the enormity of what’s at stake.  It should go without saying that, if a minority gets to rule indefinitely by forcing through laws to suppress the votes of a majority that would otherwise unseat it, thereby giving itself the power to force through more such laws, etc., then we no longer live in a democracy but in a banana republic.  And there’s no symmetry to the situation: no matter how terrified you (or I) might feel about wokeists and their denunciation campaigns, the Democrats have no comparable effort to suppress Republican votes.  Alas, I don’t know of any solutions beyond the obvious one, of trying to deal the conspiracy-addled grievance party crushing defeats in 2022 and 2024.<br/></li><li><strong><span class="has-inline-color has-vivid-red-color">Added:</span></strong> <a href="https://www.youtube.com/watch?v=DEYUt1tJlck">Here’s the video</a> of my recent Astral Codex Ten ask-me-anything session.</li></ol>



<p/></div>
    </content>
    <updated>2021-07-21T20:57:08Z</updated>
    <published>2021-07-21T20:57:08Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Speaking Truth to Parallelism"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-07-30T17:56:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blog.simons.berkeley.edu/?p=304</id>
    <link href="https://blog.simons.berkeley.edu/2021/07/trends-in-machine-learning-theory/" rel="alternate" type="text/html"/>
    <title>Trends in Machine Learning Theory</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference ALT 2021, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series is disseminated as guest posts on … <a href="https://blog.simons.berkeley.edu/2021/07/trends-in-machine-learning-theory/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Welcome to ALT Highlights, a series of blog posts spotlighting various happenings at the recent conference <a href="http://algorithmiclearningtheory.org/alt2021/" rel="noreferrer noopener" target="_blank">ALT 2021</a>, including plenary talks, tutorials, trends in learning theory, and more! To reach a broad audience, the series is disseminated as guest posts on different blogs in machine learning and theoretical computer science. This initiative is organized by the <a href="https://www.let-all.com/" rel="noreferrer noopener" target="_blank">Learning Theory Alliance</a> and is overseen by <a href="http://www.gautamkamath.com/" rel="noreferrer noopener" target="_blank">Gautam Kamath</a>. All posts in ALT Highlights are indexed on the official <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/" rel="noreferrer noopener" target="_blank">Learning Theory Alliance blog</a>.</p>



<p>This is the sixth and final post in the series, on trends in machine learning theory, written by <a href="https://web.stanford.edu/~mglasgow/" rel="noreferrer noopener" target="_blank">Margalit Glasgow</a>, <a href="https://sites.google.com/view/michal-moshkovitz" rel="noreferrer noopener" target="_blank">Michal Moshkovitz</a>, and <a href="https://sites.google.com/site/cyrusrashtchian/" rel="noreferrer noopener" target="_blank">Cyrus Rashtchian</a>.</p>



<p><strong>Introduction</strong><br/>Throughout the last few decades, we have witnessed unprecedented growth of machine learning. Originally a topic formalized by a small group of computer scientists, machine learning now impacts many areas: the physical sciences, medicine, commerce, finance, urban planning, and more. The rapid growth of machine learning can be partially attributed to the availability of large amounts of data and the development of powerful computing devices. Another important factor is that machine learning has foundations in many other fields, such as theoretical computer science, algorithms, applied mathematics, statistics, and optimization. </p>



<p>If machine learning is already mathematically rooted in many existing research areas, why do we need a field solely dedicated to learning theory? According to <a href="https://www.cs.columbia.edu/~djhsu/" rel="noreferrer noopener" target="_blank">Daniel Hsu</a>, “Learning theory serves (at least) two purposes: to help make sense of machine learning, and also to explore the capabilities and limitations of learning algorithms.” Besides finding innovative applications for existing tools, learning theorists also provide answers to long-standing problems and ask new fundamental questions. </p>



<p>Modern learning theory goes beyond classical statistical and computer science paradigms by: </p>



<ul><li>developing insights about specific computational models (e.g., neural networks) </li><li>analyzing popular learning algorithms (e.g., stochastic gradient descent)</li><li>taking into account data distributions (e.g., margin bounds or manifold assumptions)</li><li>adding auxiliary goals (e.g., robustness or privacy), and </li><li>rethinking how algorithms interact with and access data (e.g., online or reinforcement learning).</li></ul>



<p>By digging deep into the basic questions, researchers generate new concepts and models that change the way we solve problems and help us understand emerging phenomena.</p>



<p>This article provides a brief overview of three key areas in machine learning theory: new learning paradigms, trustworthy machine learning, and reinforcement learning. We describe the main thrust of each of these areas, as well as point to a few papers from <a href="http://algorithmiclearningtheory.org/alt2021/" rel="noreferrer noopener" target="_blank">ALT 2021</a> (the 32nd International Conference on Algorithmic Learning Theory) that touch each of these topics. To share a broader view, we also asked experts in the areas to comment on the field and on their recent papers. Needless to say, this article only scratches the surface. At the end, we point to places to learn more about learning theory.</p>



<span id="more-304"/>



<p><strong>New Machine Learning Paradigms</strong><br/>The traditional learning theory framework, probably approximately correct (PAC) learning, defines what it means to learn a ground-truth classifier from a candidate class of possible classifiers. Alongside PAC learning is Vapnik-Chervonenkis (VC) theory, which characterizes the number of samples needed and sufficient to learn a classifier from a given class. The generalization analysis from VC theory is restricted to guarantees that hold independently of the data distribution — that is, even for worst-case distributions. Additionally, the VC/PAC learning paradigm suggests that whenever learning is possible, it can be accomplished by choosing the classifier that minimizes loss on the training data, called the empirical risk minimizer (ERM). </p>



<p>This classical framework unfortunately fails to explain the empirical success of machine learning (ML). “The distribution-free setting, while it comes with the elegant VC theory, turned out to be unsatisfactory,” says Csaba Szepesvári. “Due to the oversimplified setting, the theory could not contribute meaningfully to understanding all kinds of learning methods such as learning with trees, boosting, neural networks, SVMs, or using any other nonparametric methods.” Researchers posit that stronger guarantees should be possible if we leverage natural assumptions about the data distribution, though identifying the right “natural assumptions” is a challenging task. Similarly, understanding which of many possible ERM solutions a learning algorithm chooses may yield better generalization results than those yielded by VC theory. </p>



<p>Methods that provide <em>distribution-specific </em>guarantees aren’t new to learning theory. A canonical example is known as a margin bound, where the test error of a classifier is analyzed in terms of the margin that separates the different prediction categories. In <a href="http://proceedings.mlr.press/v132/hanneke21a.html" rel="noreferrer noopener" target="_blank">one</a> of the ALT best papers, Steve Hanneke and Aryeh Kontorovich prove generalization guarantees in terms of the size of the margin for two popular classification algorithms: <a href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noreferrer noopener" target="_blank">support vector machines</a> (SVMs) and the <a href="https://en.wikipedia.org/wiki/Perceptron" rel="noreferrer noopener" target="_blank">perceptron</a> algorithm. The authors answer a core open question, showing that SVMs achieve the optimal margin bound!</p>



<p>Further work at ALT uses an assumption that data lies on a low-dimensional manifold to prove guarantees for generative models. Generative models synthesize <em>original</em> samples, such as images or text, that resemble training data, but without copying the data directly. While so-called <em>generative adversarial networks</em> work well in practice, few guarantees exist because it is challenging to statistically formulate the requirement of generating original samples. Nicolas Schreuder, Victor-Emmanuel Brunel, and Arnak S. Dalalyan consider a new framework in their <a href="http://proceedings.mlr.press/v132/schreuder21a.html" rel="noreferrer noopener" target="_blank">paper</a>, in which they guarantee originality by outputting a continuous distribution, which ensures that it is very unlikely to output training examples. If the training data is generated from a low-dimensional manifold, they show that it is possible to learn a good generator, which outputs a smooth transformation of a random point.</p>



<p>In another <a href="http://proceedings.mlr.press/v132/tosh21a.html" rel="noreferrer noopener" target="_blank">paper</a>, Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu use distributional assumptions to show when unsupervised methods can use unlabeled data to learn useful <em>representations </em>of data. A linear function trained on these representations and some labeled data can then be used for downstream prediction of the labels. The key idea is to look at when data has <em>multiview redundancy (MVR), </em>which arises, for instance, when data is augmented: Under MVR, each data point can be viewed as a pair (<em>X,</em> <em>Z</em>), and the label <em>Y</em> can be predicted almost as well from <em>X</em> or <em>Z</em> as from the full pair. For instance, each pair might be two halves of an article, or two rotations of an image. The authors show how a theoretical approach called <em>landmark embedding </em>can produce a representation that enables low-error linear classification. Additionally, they analyze when the representations are learned implicitly while training a model to predict whether two views <em>X</em> and <em>Z</em> correspond to the same example, which is close to what is done in practice. </p>



<p>Another new paradigm considers how the specific<em> training algorithm</em> affects which of many candidate ERM solutions are chosen. This is called the <em>implicit bias </em>of a training algorithm: If there are multiple equally good solutions, then why does an algorithm choose one over the other? This is particularly relevant when studying neural networks, which are typically<em> overparameterized</em> and can be trained to find many solutions with zero empirical risk. In one <a href="http://proceedings.mlr.press/v132/ji21a/ji21a.pdf" rel="noreferrer noopener" target="_blank">paper</a>, Ziwei Ji and Matus Telgarsky characterize the implicit bias of using gradient descent to train a linear classifier with a general loss function. They show that the solution relates to the optimizer of a particular smoothed margin function. While this paper does not yield generalization guarantees, this type of implicit bias analysis can sometimes lead to generalization guarantees via margin bounds.</p>



<p>The goal of this area is to go beyond traditional learning theory paradigms by leveraging distributional and algorithmic properties. But a major open question is designing a more general mathematical theory that exploits such properties. <a href="http://www.cs.technion.ac.il/~shaymrn/" rel="noreferrer noopener" target="_blank">Shay Moran</a> offers a standard for new theory: “I hope that in the next 10 years we will develop more realistic models of learning, but I will insist that they still be mathematically clean.”</p>



<p><strong>Trustworthy ML</strong><br/>Machine learning has inspired many new areas and technologies: personalized health care, drug discovery, advertising, résumé screening, credit loans, and more. However, these critical and user-centered applications require a higher standard of testing and verification because mistakes may deeply affect many people. Addressing these challenges has inspired a new field of research centered on making machine learning more trustworthy and reliable, which is the motivation for many of the ALT papers this year as well. An expert in the area, <a href="http://cseweb.ucsd.edu/~kamalika/" rel="noreferrer noopener" target="_blank">Kamalika Chaudhuri</a>, says, “For my field, which is trustworthy ML, the theoretical goal and challenge remains modeling and frameworks.” She elaborates: “Coming up with new conceptual frameworks for learning has always been one of the core challenges in learning theory since its early days, and it is doubly important now.” Researchers have been exploring this direction in many areas, including privacy, data deletion, robustness, fairness, interpretability, and causality. </p>



<p>Several ALT 2021 papers cover questions in privacy. The general goal is to understand how to modify existing learning methods to take into account privacy constraints. One of the <a href="http://proceedings.mlr.press/v132/wang21a.html" rel="noreferrer noopener" target="_blank">papers</a>, by Di Wang, Huanyu Zhang, Marco Gaboardi, and Jinhui Xu, considers generalized linear models in a differential privacy model. A central motivation is to understand the role of public, unlabeled data in improving the learnability of these problems in a private setting. Summarizing another direction, <a href="http://www.gautamkamath.com" rel="noreferrer noopener" target="_blank">Gautam Kamath</a> comments on his <a href="http://proceedings.mlr.press/v132/aden-ali21a.html" rel="noreferrer noopener" target="_blank">paper</a> with co-authors Ishaq Aden-Ali and Hassan Ashtiani, “This paper focuses on a very simple but surprisingly challenging question: Can we learn a general multivariate Gaussian under the constraint of differential privacy? Prior works focused on restricted settings — for example, with bounded parameters or known covariance. We gave the first finite sample complexity bound for this problem, which evidence suggests is near optimal. The next question is to design a computationally efficient algorithm for this problem.” Another <a href="http://proceedings.mlr.press/v132/cesar21a.html" rel="noreferrer noopener" target="_blank">paper</a> on privacy, by Mark Cesar and Ryan Rogers, studies the <a href="https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf" rel="noreferrer noopener" target="_blank">composition</a> of various privacy mechanisms in the context of real-world data analytics pipelines.</p>



<p>Another aspect of respecting user privacy is allowing people to choose to stop sharing their data. Concretely, this means removing their data from data sets and ensuring that existing and future models do not make use of their data in any way. One name for this process is <em>machine unlearning</em>, and the main challenge is removing the data efficiently without retraining all models from scratch. One <a href="http://proceedings.mlr.press/v132/neel21a.html" rel="noreferrer noopener" target="_blank">paper</a>, by Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi, addresses this challenge. They propose ways to strategically update the model by using modified gradient descent methods. They also analyze this approach, and prove new upper and lower bounds for updating models after data deletion with their new optimization algorithm. </p>



<p>Robust methods for machine learning and statistics aim to provide rigorous guarantees in the presence of outliers or adversarially modified data points. The field of robustness has been steadily growing as researchers uncover more and more models where deviations in the data can lead to unexpected and dramatic changes in model behavior. In ALT 2021, a <a href="http://proceedings.mlr.press/v132/shen21a.html" rel="noreferrer noopener" target="_blank">paper</a> by Jie Shen and Chicheng Zhang covers learning half-spaces nearly optimally even in the presence of malicious noise.</p>



<p>As a final and thought-provoking direction in trustworthy ML, <a href="https://omereingold.wordpress.com" rel="noreferrer noopener" target="_blank">Omer Reingold</a> points out that we need to better understand “the meaning of individual probabilities/risk scores,” which are common ways that ML systems summarize or justify decisions. Ideally, the output of a model should be something that people can interpret directly and use to potentially modify their future actions. He elaborates that it is important to think about “the individual quantities (which imply important decisions) that ML is trying to approximate” and to answer “what does fitting the parameters of a model on the entire population imply for individuals and subcommunities?” This question brings to the forefront the fact that ML systems affect both individuals and groups of people, which is an important consideration when formulating rigorous definitions of fairness (e.g., see <a href="https://global.oup.com/academic/product/the-ethical-algorithm-9780190948207" rel="noreferrer noopener" target="_blank">this book</a> or <a href="https://fairmlbook.org/" rel="noreferrer noopener" target="_blank">this one</a>).</p>



<p><strong>Reinforcement Learning</strong><br/>Reinforcement learning (RL) is a framework for interactive learning where an agent interacts with an environment, and the agent’s actions govern the rewards it receives from the environment. Part of the motivation for studying RL is that relevant problems are everywhere. Sometimes the agents are autonomous vehicles. Other times, they are programs playing games like chess or go. People interact more and more with ML models, and hence, living our lives is actually being a part of a multiagent game where we, humans, and the ML models are the agents. “The most exciting direction in learning theory of recent years,” says <a href="https://www.ehazan.com" rel="noreferrer noopener" target="_blank">Elad Hazan</a>, “is adding rigorous theoretical guarantees to reinforcement learning.” </p>



<p>The RL environment is typically modeled as a <em>Markov decision process (MDP)</em>: a set of states, actions, and transition probabilities that determine the next state and reward given the agent’s current state and action. The agent uses a <em>policy</em> to choose its action from each state with the goal of maximizing its cumulative reward over time. A central challenge is balancing<em> exploration</em> (learning about the environment) and <em>exploitation</em> (spending time choosing actions in states where they can collect high rewards).</p>



<p>In the most basic setting, <em>multi-armed bandits</em>, there is a single state, and each action (or “arm”) leads to a stochastic reward. “Here, the theory is quite mature, though interesting problems remain in connection to the limits of how structure can be exploited,” <a href="https://sites.ualberta.ca/~szepesva/" rel="noreferrer noopener" target="_blank">Csaba Szepesvári</a> says. In two works at ALT, by <a href="http://proceedings.mlr.press/v132/jourdan21a.html" rel="noreferrer noopener" target="_blank">Marc Jourdan, Mojmír Mutný, Johannes Kirschner, and Andreas Krause</a> and by <a href="http://proceedings.mlr.press/v132/cuvelier21a.html" rel="noreferrer noopener" target="_blank">Thibaut Cuvelier, Richard Combes, and Eric Gourdin</a>, the authors show that efficient exploration is possible in a <em>combinatorial semi-bandit </em>setting. Here, the agent can choose an allowed <em>set</em> of arms in each step, and it receives a distinct reward for each chosen arm. While the number of action choices for the agent is larger, this more detailed feedback makes the problem tractable.</p>



<p>Beyond the stateless bandit setting, ML theorists are still figuring out how fast agents can learn how to play optimally in MDPs with <em>finite</em> state spaces and action spaces. Recent progress on this front has given lower bounds on the sample complexity required for agents to learn the best policy. In one ALT <a href="http://proceedings.mlr.press/v132/domingues21a.html" rel="noreferrer noopener" target="_blank">paper</a>, the authors give a unified view of lower bounds for three distinct, but related, problems in RL. The hard MDP instances they construct to show lower bounds are based on hard instances for multi-armed bandit problems.</p>



<p>In the more challenging setting where the state space is infinite, a central question is whether the agent can learn from exploring a finite number of states, and generalize<em> </em>to perform well on unknown areas of the environment. For certain MDPs, generalizing is impossible, but some assumptions on the structure of the MDP may enable generalization. “While algorithm independent problem formulations existed and have been studied in the finite case, a quite recent development is to extend these to the case of ‘large’ environments where the use of <em>function approximation </em>techniques becomes crucial for achieving nontrivial results,” explains Csaba Szepesvári. </p>



<p>Function approximation has to do with the optimal <em>action-value function, </em>which captures the long-term reward of playing a certain action from a given state. This function can sometimes be approximated by some simple class of functions. One of the strongest such assumptions is <em>linear realizability, </em>where the optimal action-value function<em> </em>is a linear function of some representation of the action and state. In one of the <a href="http://proceedings.mlr.press/v132/weisz21a.html" rel="noreferrer noopener" target="_blank">papers</a> receiving a best paper award in ALT, Gellert Weisz, Philip Amortila, and Csaba Szepesvári show that even under this strong assumption of linear realizability, the agent needs a number of samples exponential in the length of the episode or the dimension of the representation in order to generalize. Looking forward, the goal is to follow the lead of these papers and better understand the landscape of sample complexity: When can we learn models with a polynomial number of samples, and when is an exponential number necessary?</p>



<p>Nearly every offline learning problem can be studied in an interactive setting, where inputs arrive in an online fashion and need to be processed immediately, which is common in many real-world settings. Models for interactive machine learning provide a framework for studying problems and algorithms in this more challenging setting. Beyond the MDP setting, interactive learning spans online learning (e.g., <a href="http://proceedings.mlr.press/v132/moshkovitz21a.html" rel="noreferrer noopener" target="_blank">no-substitution</a> <a href="http://proceedings.mlr.press/v132/bhattacharjee21a.html" rel="noreferrer noopener" target="_blank">clustering</a>), nonstochastic control theory (e.g., <a href="https://arxiv.org/abs/1911.12178" rel="noreferrer noopener" target="_blank">robust controllers for dynamical systems</a>), <a href="https://arxiv.org/abs/1909.05207" rel="noreferrer noopener" target="_blank">online convex optimization</a>, and many more domains. </p>



<p><strong>Conclusion</strong><br/>We hope this provides a fairly broad view on some of the topics that people are researching right now in learning theory. Of course, there are many more areas that we don’t have space to describe: theory of deep neural networks, quantum algorithms for machine learning problems, human-centered considerations, learning with strategic agents and multiplayer games, convex/nonconvex optimization, federated and distributed learning algorithms, and many more. In general, as Gautam Kamath observes, “A lot of important questions in learning theory arise through interplay between the theoretical and applied machine learning communities.” To have a greater impact, it is important to collaborate with people doing empirical research, and to learn from the front lines about the most interesting phenomena to explain, or the challenges that do not seem surmountable by combining existing tools.</p>



<p>To learn more and to get more involved, we have listed a variety of resources (blogs, workshops, videos, etc.) that can help you get started in this area. As a final motivation for writing this article, we remark that people in the area are keenly aware that we need more young talent to help uncover truth and contribute groundbreaking ideas. As Gautam Kamath puts it, “There are far more interesting questions in learning theory than there are researchers to solve them.”</p>



<h2>Places to learn more</h2>



<p><strong>Blogs:</strong> <a href="https://ucsdml.github.io" rel="noreferrer noopener" target="_blank">UCSD ML blog</a>, <a href="http://www.offconvex.org" rel="noreferrer noopener" target="_blank"><em>Off the Convex Path</em></a>, <a href="https://windowsontheory.org" rel="noreferrer noopener" target="_blank"><em>Windows On Theory</em></a>, <a href="https://blogs.princeton.edu/imabandit/" rel="noreferrer noopener" target="_blank"><em>I’m a bandit</em></a>, <a href="https://francisbach.com" rel="noreferrer noopener" target="_blank">Francis Bach’s blog</a>, <a href="https://differentialprivacy.org" rel="noreferrer noopener" target="_blank">Differential Privacy blog</a>, <a href="https://distill.pub" rel="noreferrer noopener" target="_blank"><em>Distill</em></a>, <a href="http://thegradient.pub" rel="noreferrer noopener" target="_blank"><em>The Gradient</em></a></p>



<p><strong>Conferences:</strong> <a href="http://algorithmiclearningtheory.org/alt2021/" rel="noreferrer noopener" target="_blank">ALT</a>, <a href="http://learningtheory.org/colt2021/" rel="noreferrer noopener" target="_blank">COLT</a>, <a href="https://icml.cc" rel="noreferrer noopener" target="_blank">ICML</a>, <a href="https://nips.cc" rel="noreferrer noopener" target="_blank">NeurIPS</a>, <a href="https://aistats.org/aistats2021/" rel="noreferrer noopener" target="_blank">AISTATS</a>, <a href="https://www.auai.org/uai2021/" rel="noreferrer noopener" target="_blank">UAI</a>, <a href="https://responsiblecomputing.org" rel="noreferrer noopener" target="_blank">FORC</a>, <a href="http://acm-stoc.org/stoc2021/" rel="noreferrer noopener" target="_blank">STOC</a>, <a href="http://ieee-focs.org" rel="noreferrer noopener" target="_blank">FOCS</a>, <a href="http://itcs-conf.org" rel="noreferrer noopener" target="_blank">ITCS</a>, <a href="https://iclr.cc/virtual_2020/index.html" rel="noreferrer noopener" target="_blank">ICLR</a>, <a href="https://dl.acm.org/conference/soda" rel="noreferrer noopener" target="_blank">SODA</a></p>



<p><strong>Podcasts:</strong> <a href="https://twimlai.com" rel="noreferrer noopener" target="_blank"><em>TWIML</em></a>, <a href="https://wandb.ai/site/podcast" rel="noreferrer noopener" target="_blank"><em>Gradient Dissent</em></a>, <a href="https://www.quantamagazine.org/tag/the-joy-of-x/" rel="noreferrer noopener" target="_blank"><em>Joy of x</em></a>, <a href="https://shows.acast.com/the-robot-brains" rel="noreferrer noopener" target="_blank"><em>The Robot Brains</em></a>, <a href="https://www.thetalkingmachines.com/home" rel="noreferrer noopener" target="_blank"><em>Talking Machines</em></a>, <a href="https://www.talkrl.com/episodes/marc-bellemare" rel="noreferrer noopener" target="_blank"><em>TalkRL</em></a>, <a href="https://www.listennotes.com/podcasts/underrated-ml-sara-hooker-sean-hooker-nG4owP2C8s3/" rel="noreferrer noopener" target="_blank"><em>Underrated ML</em></a></p>



<p><strong>Videos:</strong> <a href="https://simons.berkeley.edu" rel="noreferrer noopener" target="_blank">Simons Institute</a>, <a href="https://www.youtube.com/watch?v=IXotICNx2qk&amp;list=PLdDZb3TwJPZ5dqqg_S-rgJqSFeH4DQqFQ" rel="noreferrer noopener" target="_blank">IAS deep learning workshop</a>, <a href="https://www.oneworldml.org/home" rel="noreferrer noopener" target="_blank">One World ML</a>, <a href="https://www.trustworthyml.org" rel="noreferrer noopener" target="_blank">Trustworthy ML</a>, <a href="https://sites.google.com/view/dstheory/home" rel="noreferrer noopener" target="_blank"><em>Foundations of Data Science</em>,</a> <a href="https://sites.google.com/view/rltheoryseminars/home" rel="noreferrer noopener" target="_blank">RL Theory Virtual Seminars</a>, <a href="https://www.imsi.institute/the-multifaceted-complexity-of-machine-learning/" rel="noreferrer noopener" target="_blank">iMSi: The Multifaceted Complexity of Machine Learning</a>, <em><a href="https://sites.google.com/view/control-meets-learning/home" rel="noreferrer noopener" target="_blank">Control Meets Learning</a></em> </p>



<p><strong>Acknowledgments:</strong> We thank Kamalika Chaudhuri, Elad Hazan, Daniel Hsu, Gautam Kamath, Shay Moran, Omer Reingold, Csaba Szepesvári, and Claire Vernade for helpful comments and thoughtful quotes. We thank Kush Bhatia, Lee Cohen, Neha Gupta, Nika Haghtalab, Max Hopkins, Gautam Kamath, Gaurav Mahajan, and Uri Sherman for helpful feedback on initial drafts.</p>



<div class="wp-block-image"><figure class="alignleft is-resized"><img alt="" class="wp-image-339" height="150" src="https://blog.simons.berkeley.edu/wp-content/uploads/2021/07/GlasgowMargalit-2-edited-1.jpg" width="150"/></figure></div>



<p class="has-text-align-left"><a href="https://web.stanford.edu/~mglasgow/" rel="noreferrer noopener" target="_blank"><strong>Margalit Glasgow</strong></a> is a PhD student in Stanford’s Computer Science Department, advised by Mary Wootters. Her research focuses on theoretical machine learning and random matrices.</p>



<div class="wp-block-spacer" style="height: 20px;"/>



<div class="wp-block-image"><figure class="alignright size-large is-resized"><img alt="" class="wp-image-327" height="150" src="https://blog.simons.berkeley.edu/wp-content/uploads/2021/07/MoshkovitzMichal.jpg" width="150"/></figure></div>



<p><a href="https://sites.google.com/view/michal-moshkovitz" rel="noreferrer noopener" target="_blank"><strong>Michal Moshkovitz</strong></a> is a postdoc at the Qualcomm Institute at UC San Diego. She received her PhD and MSc in computational neuroscience from the Hebrew University of Jerusalem, and her MSc in computer science from Tel Aviv University. Her research focuses on the foundations of AI, exploring how different constraints affect learning. She has worked on bounded-memory learning, explainable machine learning, and online decision-making in unsupervised learning. </p>



<div class="wp-block-image"><figure class="alignleft size-thumbnail"><img alt="" class="wp-image-332" height="150" src="https://blog.simons.berkeley.edu/wp-content/uploads/2021/07/RashtchianCyrus-150x150.jpg" width="150"/></figure></div>



<p><strong><a href="https://sites.google.com/site/cyrusrashtchian/" rel="noreferrer noopener" target="_blank">Cyrus Rashtchian</a></strong> is a postdoc at UC San Diego in computer science and engineering, and he received his PhD from the University of Washington. His research focuses on trustworthy machine learning, algorithms for big data, statistical reconstruction, and DNA data storage.</p></div>
    </content>
    <updated>2021-07-19T21:20:18Z</updated>
    <published>2021-07-19T21:20:18Z</published>
    <category term="General"/>
    <author>
      <name>1737780</name>
    </author>
    <source>
      <id>https://blog.simons.berkeley.edu</id>
      <link href="https://blog.simons.berkeley.edu/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blog.simons.berkeley.edu" rel="alternate" type="text/html"/>
      <subtitle>What's New at the Simons Institute for the Theory of Computing.</subtitle>
      <title>Calvin Café: The Simons Institute Blog</title>
      <updated>2021-07-31T11:21:31Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2541777959928040133</id>
    <link href="http://blog.computationalcomplexity.org/feeds/2541777959928040133/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/07/political-intersections-trump-honors.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/2541777959928040133" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/2541777959928040133" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/07/political-intersections-trump-honors.html" rel="alternate" type="text/html"/>
    <title>Political Intersections: Trump honors Antifa member who was shot dead by police</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> 1) Trump and other reps have said the following about the Jan 6 event at various times:</p><p>a) The Jan 6 event was freedom fighters who were fighting the noble fight to overturn a fraudulent election. Rah Rah!</p><p>b) The Jan 6 event was a peaceful protest to overturn a fraudulent election. </p><p>c) The Jan 6 event was democrats trying to make republicans look bad.</p><p>d) The Jan 6 event was Antifa. (See <a href="https://www.reuters.com/article/us-usa-election-trump/trump-privately-blamed-antifa-people-for-storming-u-s-capitol-axios-idUSKBN29H0DR">here</a>)</p><p>e) Ashli Babbitt (a protestor who was shot by police at the Jan 6 event) should have been honored by flying the flag at half-mast (see <a href="https://www.mediaite.com/trump/trump-reportedly-regrets-not-lowering-american-flag-at-the-white-house-to-honor-ashli-babbitt/">here</a>) </p><p>If we take the intersection of d and e we find that Trump wants to honor a member of Antifa who was shot by police. </p><p>To be fair, Trump is entitled to change his mind. But I wonder- did he ever really think it was Antifa or was that a talking point? If he really thought so then  when did he change his mind? I ask non rhetorically---  NOT a `gotcha question'</p><p>(NOTE: I wrote this post a while back. Since then Trump and some other republicans are tending towards the Freedom Fighters narrative.) </p><p>2) Vaccines:</p><p>a) Some people think that the vaccines are bad to take. I suspect they would give some (incorrect) health reasons, while the real reason may be political. (One reason is that the vaccine make you magnetic. That sounds awesome! Others think that there is a microchip in the vaccine so that Bill Gates can track our movements. Gee, Mark Zuckerberg can already do that. Some thing it will rewrite our DNA. A bio major I know  tells me that such people are confusing messenger RNA with DNA. Great- we can now have a nice conversation and point out where they are wrong.) </p><p>b) Some people think we should NOT give vaccines to poor countries that need them, or to people in prison,  since Americans should have priority. (NOTE- from a purely health-viewpoint this is not correct since a pandemic does not respect boundaries- if there is an outbreak in a diff country or in a prison it will affect people not in those countries and not in prison.) </p><p>Are there people who believe a and b? I ask non-rhetorically. </p><p><br/></p><p>I am not surprised when people hold contradictory thoughts in their heads, but these two cases just struck me as particularly strange. Not sure why.</p><p> </p><p><br/></p></div>
    </content>
    <updated>2021-07-18T18:58:00Z</updated>
    <published>2021-07-18T18:58:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-31T09:19:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/104</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/104" rel="alternate" type="text/html"/>
    <title>TR21-104 |  Does QRAT simulate IR-calc? QRAT simulation algorithm for $\forall$Exp+Res cannot be lifted to IR-calc | 

	Sravanthi Chede, 

	Anil Shukla</title>
    <summary>We show that the QRAT simulation algorithm of $\forall$Exp+Res from [B. Kiesl and M. Seidl, 2019] cannot be lifted to IR-calc.</summary>
    <updated>2021-07-18T16:15:18Z</updated>
    <published>2021-07-18T16:15:18Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-31T11:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/103</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/103" rel="alternate" type="text/html"/>
    <title>TR21-103 |  Elliptic Curve Fast Fourier Transform (ECFFT) Part I: Fast Polynomial Algorithms over all Finite Fields | 

	Eli Ben-Sasson, 

	Dan Carmon, 

	Swastik Kopparty, 

	David Levit</title>
    <summary>Over finite fields $F_q$ containing a root of unity of smooth order $n$ (smoothness means $n$ is the product of small primes), the Fast Fourier Transform (FFT) leads to the fastest known algebraic algorithms for many basic polynomial operations, such as multiplication, division, interpolation and multi-point evaluation. These operations can be computed by constant fan-in arithmetic circuits over $F_q$ of quasi-linear size; specifically, $O(n \log n)$ for multiplication and division, and $O(n \log^2 n)$ for interpolation and evaluation.

However, the same operations over fields with no smooth order root of unity suffer from an asymptotic slowdown, typically due to the need to introduce “synthetic” roots of unity to enable the FFT. The classical algorithm of Schönhage and Strassen incurred a multiplicative slowdown factor of $\log \log n$ on top of the smooth case. Recent remarkable results of Harvey, van der Hoeven and Lecerf dramatically reduced this multiplicative overhead to $\exp(\log^* (n))$.

We introduce a new approach to fast algorithms for polynomial operations over all large finite fields. The key idea is to replace the group of roots of unity with a set of points $L \subset F_q$ suitably related to a well-chosen elliptic curve group over $F_q$ (the set L itself is not a group). The key advantage of this approach is that elliptic curve groups can be of any size in the Hasse–Weil interval $[q + 1 \pm 2\sqrt{q}]$ and thus can have subgroups of large, smooth order, which an FFT-like divide and conquer algorithm can exploit. Compare this with multiplicative subgroups over $F_q$ whose order must divide $q-1$. By analogy, our method extends the standard, multiplicative FFT in a similar way to how Lenstra’s elliptic curve method extended Pollard’s $p-1$ algorithm for factoring integers.

For polynomials represented by their evaluation over subsets of $L$, we show that  multiplication, division, degree-computation, interpolation, evaluation and Reed–Solomon encoding (also known as low-degree extension) with fixed evaluation points can all be computed with arithmetic circuits of size similar to what is achievable with the classical FFTs when the field size $q$ is special. For several problems, this yields the asymptotically smallest known arithmetic circuits even in the standard monomial representation of polynomials.

The efficiency of the classical FFT follows from using the 2-to-1 squaring map to reduce the evaluation set of roots of unity of order $2^k$ to similar groups of size $2^{k?i}$, $i &gt; 0$. Our algorithms operate similarly, using isogenies of elliptic curves with kernel size 2 as 2-to-1 maps to reduce $L$ of size $2^k$ to sets of size $2^{k?i}$ that are, like $L$, suitably related to elliptic curves, albeit different ones.</summary>
    <updated>2021-07-18T14:07:56Z</updated>
    <published>2021-07-18T14:07:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-31T11:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/</id>
    <link href="https://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/" rel="alternate" type="text/html"/>
    <title>5th SIAM Symposium on Simplicity in Algorithms</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">January 10-11, 2022 Alexandria, Virginia, U.S. https://www.siam.org/conferences/cm/conference/sosa22 Submission deadline: August 9, 2021 Symposium on Simplicity in Algorithms is a conference in theoretical computer science dedicated to advancing algorithms research by promoting simplicity and elegance in the design and analysis of algorithms. The benefits of simplicity are manifold: simpler algorithms manifest a better understanding of the … <a class="more-link" href="https://cstheory-events.org/2021/07/18/5th-siam-symposium-on-simplicity-in-algorithms/">Continue reading <span class="screen-reader-text">5th SIAM Symposium on Simplicity in Algorithms</span></a></div>
    </summary>
    <updated>2021-07-18T13:15:17Z</updated>
    <published>2021-07-18T13:15:17Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-07-31T11:21:57Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2021-07-17-simplifying-raft-with-chaining/</id>
    <link href="https://decentralizedthoughts.github.io/2021-07-17-simplifying-raft-with-chaining/" rel="alternate" type="text/html"/>
    <title>Simplifying Raft with Chaining</title>
    <summary>Raft is a consensus algorithm for deciding a sequence of commands to execute on a replicated state machine. Raft is famed for its understandability (relative to other consensus algorithms such as Paxos) yet some aspects of the protocol still require careful treatment. For instance, determining when it is safe for...</summary>
    <updated>2021-07-17T15:25:00Z</updated>
    <published>2021-07-17T15:25:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2021-07-30T23:12:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/07/16/assistant-associate-full-professorship-in-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-september-12-2021/</id>
    <link href="https://cstheory-jobs.org/2021/07/16/assistant-associate-full-professorship-in-machine-learning-at-department-of-computer-science-faculty-of-science-university-of-copenhagen-apply-by-september-12-2021/" rel="alternate" type="text/html"/>
    <title>Assistant/Associate/Full Professorship in Machine Learning at Department of Computer Science, Faculty of Science, University of Copenhagen (apply by September 12, 2021)</title>
    <summary>The researcher will join a rapidly growing department, with strong research sections in the areas of Algorithms and Complexity, Machine Learning, Natural Language Processing, Human-Centered Computing, Software Engineering and Data Management, Programming Languages, and Image Analysis. Website: https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=154462&amp;DepartmentId=18971&amp;MediaId=4642 Email: simonsen@di.ku.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The researcher will join a rapidly growing department, with strong research sections in the areas of Algorithms and Complexity, Machine Learning, Natural Language Processing, Human-Centered Computing, Software Engineering and Data Management, Programming Languages, and Image Analysis.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=154462&amp;DepartmentId=18971&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=154462&amp;DepartmentId=18971&amp;MediaId=4642</a><br/>
Email: simonsen@di.ku.dk</p></div>
    </content>
    <updated>2021-07-16T08:41:28Z</updated>
    <published>2021-07-16T08:41:28Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-07-31T11:20:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/07/15/linkage-many-wikipedia</id>
    <link href="https://11011110.github.io/blog/2021/07/15/linkage-many-wikipedia.html" rel="alternate" type="text/html"/>
    <title>Linkage with many Wikipedia Good Articles</title>
    <summary>There are two reasons for the large number of Good Articles in this set. First, I had previously been trying to keep my nominations and reviews in balance, but there were too few nominations to review on topics of interest to me, and the inability to find things to review was preventing me from nominating other articles when they were ready. So I started nominating more often. And second, the Wikipedia Good Articles editors are having a drive this month to clean out old nominations, as they tend to do a couple of times per year.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>There are two reasons for the large number of Good Articles in this set. First, I had previously been trying to keep my nominations and reviews in balance, but there were too few nominations to review on topics of interest to me, and the inability to find things to review was preventing me from nominating other articles when they were ready. So I started nominating more often. And second, the Wikipedia Good Articles editors are having a drive this month to clean out old nominations, as they tend to do a couple of times per year.</p>

<ul>
  <li>
    <p><a href="https://www.thisiscolossal.com/2021/07/steve-lindsay-fractal-vise/">Morphing fractal engraving vice jaws</a> (<a href="https://mathstodon.xyz/@11011110/106506737270461558">\(\mathbb{M}\)</a>). Circular arcs nested within circular arcs rotate to conform to whatever shape is being gripped.</p>
  </li>
  <li>
    <p>Christian Lawson-Perfect sets up a new wiki, <a href="https://whystartat.xyz">Why start at \(x,y,z\)?</a> (<a href="https://mathstodon.xyz/@christianp/106500170446647463">\(\mathbb{M}\)</a>). Its aim is to collect ambiguous, inconsistent, or just plain unpleasant conventions in mathematical notation. My contribution: <a href="https://whystartat.xyz/wiki/Big_O_notation">big O notation</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Pick%27s_theorem">Pick’s theorem </a> (<a href="https://mathstodon.xyz/@11011110/106523554824974108">\(\mathbb{M}\)</a>). The area of a grid polygon equals its number of interior grid points, plus half the boundary points, minus one. Good Article #1.</p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2021/07/03/the-problem-with-combinatorics-textbooks/">The problem with combinatorics textbooks</a> (<a href="https://mathstodon.xyz/@11011110/106526891677914307">\(\mathbb{M}\)</a>). Igor Pak on the difficulty of teaching combinatorics in a comprehensive way in a single term. Instead, he suggests teaching courses on narrower subtopics: “the more specific you make the combinatorics course the more interesting it is to the students”.</p>
  </li>
  <li>
    <p>I recently returned from a relaxing early-long-weekend mini-vacation to Avila Beach (<a href="https://mathstodon.xyz/@11011110/106532239057852428">\(\mathbb{M}\)</a>), with seafood sunset beach dinners (the coast faces south so the sun sets over land), wine tasting (near the setting of Sideways), and sulphur springs hot tub soaks. The photo below is a garden in a field of rusted pylons in the flood basin of San Luis Obispo Creek. I liked its contrast of natural growth and regular artificial forms.</p>

    <p style="text-align: center;"><img alt="Secret garden, Sycamore Springs Resort, Avila Beach" src="https://www.ics.uci.edu/~eppstein/pix/sycsprings/SecretGarden-m.jpg" style="border-style: solid; border-color: black;"/></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Vi%C3%A8te%27s_formula">Viète’s formula</a> (<a href="https://mathstodon.xyz/@11011110/106535094271822431">\(\mathbb{M}\)</a>), an infinite product of nested roots evaluating to \(2/\pi\), “the first formula of European mathematics to represent an infinite process”. Good Article #2.</p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2021/06/25/science/puzzles-fonts-math-demaine.html">The <em>New York Times</em> on Erik and Marty Demaine’s mathematical typefaces</a> (<a href="https://mathstodon.xyz/@11011110/106545874983076062">\(\mathbb{M}\)</a>, <a href="https://archive.ph/oJ8xG">also</a>, <a href="http://stormbear.com/carnival-of-mathematics-195-july-2021/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Euclid%E2%80%93Euler_theorem">Euclid–Euler theorem</a> (<a href="https://mathstodon.xyz/@11011110/106554476459806645">\(\mathbb{M}\)</a>). A 2-millenium-long collab in which Euclid proved that all Mersenne primes produce even perfect numbers, and Euler proved that all even perfect numbers come from Mersenne primes. But let’s not forget <a href="https://en.wikipedia.org/wiki/Ibn_al-Haytham">Ibn al-Haytham</a> (Alhazen), halfway between them in time, who conjectured Euler’s result but couldn’t prove it. Good Article #3.</p>
  </li>
  <li>
    <p><a href="https://mirtitles.org/2021/07/07/convex-figures-and-polyhedra-lyusternik/">Lyusternik’s book <em>Convex Figures and Polyhedra</em></a> (<a href="https://mathstodon.xyz/@jarban/106551202604578180">\(\mathbb{M}\)</a>), one of the Mir translations from Russian to English, <a href="https://archive.org/details/lyusternik-convex-figures-and-polyhedra">available without restrictions on archive.org</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Bucket_queue">Bucket queue</a> (<a href="https://mathstodon.xyz/@11011110/106570282017522543">\(\mathbb{M}\)</a>). This priority queue is a bit out of fashion, but good for small integer priorities or for shortest paths when the ratio of longest to shortest edge is small. Good Article #4, despite a reviewer who had <a href="https://en.wikipedia.org/wiki/WP:CHEESE">somehow become convinced that deletion from doubly linked lists is nonconstant</a>. The issue is off-topic but real: updating objects in data structures often needs the objects to track their location in the structure; for instance, changing priorities in a binary heap requires each object to know its index. Most introductory material on these topics and even some standard library implementations (like Python’s heapq) fail to address this complication.</p>
  </li>
  <li>
    <p><a href="https://www.cs.ru.nl/~freek/100/">Formalizing 100 theorems</a> (<a href="https://mathstodon.xyz/@11011110/106579903443188306">\(\mathbb{M}\)</a>). Freek Wiedijk uses a rather arbitrary collection of 100 favorite theorems from some 1999 web page as a benchmark set for the progress of automatic proof assistants. I’m sad that Pick’s theorem has seen so little love.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.06490">Greedy spanners in Euclidean spaces admit sublinear separators</a> (<a href="https://mathstodon.xyz/@11011110/106583477388316730">\(\mathbb{M}\)</a>). My <a href="https://11011110.github.io/blog/2020/02/17/spanners-have-sparse.html">SoCG’21 result with Hadi Khodabandeh that 2d greedy spanners have separators of size \(O(\sqrt{n})\)</a> used crossing-based methods heavily dependent on planarity. This new preprint by Hung Le and Cuong Than uses different ideas to find separators of size \(O(n^{1-1/d})\), optimal in any dimension \(d\). Their work also extends from Euclidean spaces to doubling spaces.</p>
  </li>
  <li>
    <p><a href="https://oscarcunningham.com/670/unique-distancing-problem/">Unique distancing</a> (<a href="https://mathstodon.xyz/@11011110/106587574216525670">\(\mathbb{M}\)</a>). How many points can you place in an \(n\times n\) grid so that all pairwise distances are distinct? The linked post concerns whether \(n\) points are possible (no for all but finitely many cases because there are too many pairs and too few sums of squares) but it also looks interesting to maximize the number of points.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-07-15T18:02:00Z</updated>
    <published>2021-07-15T18:02:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-07-16T01:38:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18952</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/" rel="alternate" type="text/html"/>
    <title>Socially Reproduced Experiments</title>
    <summary>We must avoid becoming one Cropped from USA Today source José Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseball’s most dramatic cheating accusation of 2019. Today, at baseball’s All-Star break, we review this and other social […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>We must avoid becoming one</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/altuvehr/" rel="attachment wp-att-18954"><img alt="" class="alignright size-full wp-image-18954" height="151" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AltuveHR.jpg?resize=151%2C151&amp;ssl=1" width="151"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from USA Today <a href="https://www.usatoday.com/story/sports/mlb/2021/07/11/astros-jose-altuve-walk-off-hr-shirt-ripped-off-yankees/7933018002/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
José Altuve hit a game-winning home run in the bottom of the ninth against the Yankees on Sunday. He thereby reproduced the conditions and the outcome of baseball’s most dramatic cheating accusation of 2019.</p>
<p>
Today, at baseball’s All-Star break, we review this and other social experiments that have quite a bit more data.<br/>
<span id="more-18952"/></p>
<p>
Altuve won the 2019 American League Championship series with a pinch-hit homer in the bottom of the ninth against the Yankees’ closer, Aroldis Chapman. As he approached home plate, he was <a href="https://www.youtube.com/watch?v=MTNBnk1dz6g">seen</a> telling his teammates waiting to mob him at the plate not to rip off his jersey in celebration. He subsequently <a href="https://youtu.be/-ryvOya4PoE?t=247">scooted</a> into the corridor behind the dugout, then re-emerged into the on-field celebration. This fed accusations that he had been wired with a buzzer to know what kind of pitch was coming from Chapman, in line with <a href="https://en.wikipedia.org/wiki/Houston_Astros_sign_stealing_scandal">sign-stealing</a> by other means from the Astros’ 2017 championship season and 2018 that was proven and punished by Major League Baseball. </p>
<p>
Almost the same scenario was reproduced Sunday: Houston down 7-5 against the Yankees with two out and two on base in the ninth, Altuve up against the Yankees’ closer (Chad Green recently supplanting Chapman). Altuve <a href="https://www.youtube.com/watch?v=OHrc6OdYOpo">socked</a> a homer to the same part of the ballpark to complete a shocking six-run comeback. Immediately upon touching home, he had his shirt ripped off to reveal nothing but the top half of his birthday suit. This was the most direct way possible to witness that he could have hit the other homer without illegal information.</p>
<p>
</p><p/><h2> Examples and Non-Examples </h2><p/>
<p/><p>
I have dealt with chess-cheating cases in which electronic buzzing has been specifically alleged, including the two <a href="https://en.wikipedia.org/wiki/Borislav_Ivanov#Retirement_from_competitive_chess_and_brief_return_to_chess-related_activities">most</a> prominent <a href="https://www.nytimes.com/2013/08/18/crosswords/chess/a-master-is-disqualified-over-suspicions-of-cheating.html">cases</a> of 2013. I will not take this post further in this direction, however, but rather to pose this question:</p>
<blockquote><p><b> </b> <em> What is considered a “social proof” of an assertion—especially when there are elements of scientific control and reproduction? </em>
</p></blockquote>
<p/><p>
A simple example is a police lineup. This tries to control for whether a witness has previously seen the accused by including the accused among usually four or five similarly represented people. Picking the right person is considered to prove the previous encounter. Statistically, however, this is a <a href="https://en.wikipedia.org/wiki/P-value"><img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-value</a> of only 0.20 or 0.167, which are not considered significant at even the weakest level of “statistical proof.” Allowing <a href="https://www.apa.org/monitor/julaug04/lineups">null</a> lineups does not change the statistics much.</p>
<p>
Baseball gives a non-example that surprises me. One of the bad performances that cost Chapman his closer role was losing an 8-4 lead against the Los Angeles Angels on June 30. As a fantasy-baseball player, I’ve regularly observed poor pitching (by the closers on my “fantasy team”) when the lead is too large to earn credit for a coveted <a href="https://en.wikipedia.org/wiki/Save_(baseball)">save</a>. Does the data reproduce a phenomenon of closers bearing down less when way ahead, with no “save” to gain? A <a href="https://www.beyondtheboxscore.com/2014/1/27/5344580/the-closer-mentality-part-1-closers-in-non-save-situations">study</a> after the 2013 season, which cleverly represented performances by the same <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scores I use in chess, found none. “Meltdowns” like Chapman’s are offset by cases where closers pitched better. The <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scores in the study are all in the range -1.25 to +1.50 anyway, which count as statistically random.</p>
<p>
This study used a reasonably large data set, one that is well-defined and admits controlling factors such as normalizing for game circumstances and the quality of the opposing hitters. At least it is more than the two instances of Altuve. In-between would be an attempt to determine whether certain national soccer teams are consistently worse at penalty-kick tiebreaks. England’s and Italy’s teams brought their long tortured histories together in the tiebreak of Sunday’s European Cup final. The Italians missed two of five kicks, a score that often spells doom, but the English missed three.</p>
<p>
</p><p/><h2> Larger Scale </h2><p/>
<p/><p>
Dick and I are really interested in “experiments” that have spilled into society, with minimal controls but large data. One sphere of this is cybersecurity. </p>
<p>
It seems to us that only in the past decade have security experts begun formalizing their research as experimental science with repeatability and reproducibility as explicit criteria. The NSA devoted a special 2012 <a href="https://www.nsa.gov/Portals/70/documents/resources/everyone/digital-media-center/publications/the-next-wave/TNW-19-2.pdf">issue</a> of their <em>Next Wave</em> series to what they titled as “Developing a blueprint for a science of cybersecurity.” Among the contents are:</p>
<ul>
<li>
An introductory essay by Carl Landwehr titled, “Cybersecurity: From engineering to science.” <p/>
</li><li>
A linchpin <a href="https://www.cs.dartmouth.edu/~ccpalmer/teaching/cs55/Resources/Papers/TNW_19_2_BlueprintScienceCybersecurity_Schneider.pdf">paper</a> by Fred Schneider titled, “Blueprint for a science of cybersecurity.” <p/>
</li><li>
A <a href="https://www.cs.cmu.edu/~maxion/pubs/Maxion12.pdf">paper</a> by Roy Maxion titled, “Making experiments dependable,” which came from a 2011 Springer LNCS <a href="https://link.springer.com/book/10.1007/978-3-642-24541-1">Festschrift</a>.
</li></ul>
<p>
Maxion’s main example is <a href="https://en.wikipedia.org/wiki/Keystroke_dynamics">keystroke biometrics</a>. This covers inferences made from typing style on a computer keyboard or mouse or similar handheld input device. This can be used to verify identity or screen for malfeasant activities. Online chess playing platforms collect data of this nature—okay we could not resist adding chess example. </p>
<p>
Another area is experiments designed to simulate attacks and test defenses against them. Schneider’s paper begins with a contrast between <em>predictive</em> modeling versus <em>reactive</em> handling of them. About the latter, he draws an analogy with health care:</p>
<blockquote><p><b> </b> <em> “Some health problems are best handled in a reactive manner. We know what to do when somebody breaks a finger, and each year we create a new influenza vaccine in anticipation of the flu season to come. But only after making significant investments in basic medical sciences are we starting to understand the mechanisms by which cancers grow, and a cure seems to require that kind of deep understanding.” </em>
</p></blockquote>
<p/><p>
He goes on to outline the kind of scientific foundation that could hopefully underlie a ‘cure’ for intrusion and malware and the like. </p>
<p>
What we have seen happen especially in the past months, however—in both health and security—is uncontrolled experiments with society as the domain. Large-scale ransomware attacks are becoming as frequent as hurricanes and heat waves. And of course, the pandemic. These share with Altuve the property of being one-off instances, but have large data on the receiving end.</p>
<p>
</p><p/><h2> Summer Pandemic Update </h2><p/>
<p/><p>
The following chart updates our June 20 <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">post</a> on the state of the pandemic and its projection for the summer—for Florida and the United Kingdom in particular:</p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/07/13/socially-reproduced-experiments/flukcases071321/" rel="attachment wp-att-18955"><img alt="" class="aligncenter wp-image-18955" height="440" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/FLUKCases071321.png?resize=450%2C440&amp;ssl=1" width="450"/></a></p>
<p>
The vertical line shows about where the charts were on June 20. The past few days are the first where we can point to a significant rise in Florida, though Missouri had a similar rise last week and it is showing up in some other states.  The charts are taken from the <em>Worldometer</em> coronavirus <a href="https://www.worldometers.info/coronavirus/">pages</a>. </p>
<p>
The UK rise looks ghastly. It was a subtext of our previous post to worry that allowing the large dense soccer crowds at London’s Wembley Stadium for the semis and final—and anything similar in baseball—would stoke the rise in our respective countries even more. However, the rate of hospitalizations in the UK has remained largely flat. This Fortune <a href="https://fortune.com/2021/07/08/kids-vulnerable-covid-delta-variant-vaccinated-europe/">article</a> last week is one of several attesting that the new cases are mostly in children or in vaccinated people with enough immunity to contain the “breakthrough” positive. The UK is going ahead with large-scale re-openings later this month, with the portion of those 18 and older who have had one dose approaching 90% and those fully vaccinated coming past 70%. The latter number in relation to the whole population is about 52%.</p>
<p>
The US looks like becoming an experiment in how the local vaccination rate affects the numbers. The rates of those fully vaccinated by state are currently eerily <a href="https://www.cnn.com/2021/07/08/politics/electoral-map-vaccine-map-covid-19/index.html">similar</a> to Joe Biden’s vote percentage in the state. One aspect of scientific reproducibility is the size of the simplest classifier of the results. For a presidential vote to have simpler explaining power than any factors of biology or other life circumstances would make a strange experiment indeed.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Dick and I tried to come up with other examples—from computer security in particular—to sustain what has been occupying our thoughts about standards of proof for policy. We would welcome some examples from you, our readers. </p>
<p>
And of course, we have been concerned about the present course of the pandemic amid re-openings since the referenced post last month. In the meantime, if it is your taste, please enjoy the All-Star Game, which Altuve is, ironically, <a href="https://calltothepen.com/2021/07/11/houston-astros-officially-skipping-star-game/">skipping</a>.</p>
<p/><p><br/>
[some word fixes and changes]</p></font></font></div>
    </content>
    <updated>2021-07-13T19:47:02Z</updated>
    <published>2021-07-13T19:47:02Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="baseball"/>
    <category term="coronavirus"/>
    <category term="Jose Altuve"/>
    <category term="pandemic"/>
    <category term="reproducibility"/>
    <category term="science"/>
    <category term="security"/>
    <category term="social process"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-07-31T11:20:45Z</updated>
    </source>
  </entry>
</feed>
