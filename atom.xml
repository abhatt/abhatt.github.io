<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-10-02T14:22:20Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/133</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/133" rel="alternate" type="text/html"/>
    <title>TR19-133 |  More on $AC^0[\oplus]$ and Variants of the Majority Function | 

	Utkarsh Tripathi, 

	Nutan Limaye, 

	Srikanth Srinivasan</title>
    <summary>In this paper we prove two results about $AC^0[\oplus]$ circuits. 

We show that for $d(N) = o(\sqrt{\log N/\log \log N})$ and $N \leq s(N) \leq 2^{dN^{1/d^2}}$ there is an explicit family of functions $\{f_N:\{0,1\}^N\rightarrow \{0,1\}\}$ such that 
$f_N$ has uniform $AC^0$ formulas of depth $d$ and size at most $s$; 
$f_N$ does not have $AC^0[\oplus]$ formulas of depth $d$ and size $s^{\varepsilon}$, where $\varepsilon$ is a fixed absolute constant. 

This gives a quantitative improvement on the recent result of Limaye, Srinivasan, Sreenivasaiah, Tripathi, and Venkitesh, (STOC, 2019), which proved a similar Fixed-Depth Size-Hierarchy theorem but for $d \ll \log \log N$ and $s \ll \exp(N^{1/2^{\Omega(d)}})$. 

As in the previous result, we use the Coin Problem to prove our hierarchy theorem. Our main technical result is the construction of uniform size-optimal formulas for solving the coin problem with improved sample complexity $(1/\delta)^{d+4}$ (down from $(1/\delta)^{2^{O(d)}}$ in the previous result).

In our second result, we show that randomness buys depth in the $AC^0[\oplus]$ setting. Formally, we show that for any fixed constant $d\geq 2$, there is a family of Boolean functions that has polynomial-sized randomized uniform $AC^0$ circuits of depth $d$ but no polynomial-sized (deterministic) $AC^0[\oplus]$ circuits of depth $d$.

Previously Viola (Computational Complexity, 2014) showed that an increase in depth (by at least $2$) is essential to avoid superpolynomial blow-up while derandomizing randomized $AC^0$ circuits. We show that an increase in depth (by at least $1$) is essential even for $AC^0[\oplus]$. 

As in Viola's result, the separating examples are promise variants of the Majority function on $N$ inputs that accept inputs of weight at least $N/2 + N/(\log N)^{d-1}$ and reject inputs of weight at most $N/2 - N/(\log N)^{d-1}$.</summary>
    <updated>2019-10-02T11:00:43Z</updated>
    <published>2019-10-02T11:00:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-02T14:20:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/02/quics-fellows-at-joint-center-for-quantum-information-and-computer-science-apply-by-october-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/02/quics-fellows-at-joint-center-for-quantum-information-and-computer-science-apply-by-october-15-2019/" rel="alternate" type="text/html"/>
    <title>QuICS Fellows at Joint Center for Quantum Information and Computer Science (apply by October 15, 2019)</title>
    <summary>The Joint Center for Quantum Information and Computer Science (QuICS) is currently seeking outstanding quantum information researchers to join the Center faculty as QuICS Fellows. QuICS is a research partnership between the University of Maryland and the National Institute of Standards and Technology, with faculty from both institutions. Website: http://quics.umd.edu/join-quics/new-faculty Email: quics-coordinator@umiacs.umd.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Joint Center for Quantum Information and Computer Science (QuICS) is currently seeking outstanding quantum information researchers to join the Center faculty as QuICS Fellows. QuICS is a research partnership between the University of Maryland and the National Institute of Standards and Technology, with faculty from both institutions.</p>
<p>Website: <a href="http://quics.umd.edu/join-quics/new-faculty">http://quics.umd.edu/join-quics/new-faculty</a><br/>
Email: quics-coordinator@umiacs.umd.edu</p></div>
    </content>
    <updated>2019-10-02T03:13:13Z</updated>
    <published>2019-10-02T03:13:13Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-02T14:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3422</id>
    <link href="https://agtb.wordpress.com/2019/10/02/matching-markets-simons-driven-by-theory-driving-the-economy/" rel="alternate" type="text/html"/>
    <title>Matching Markets @ Simons:  Driven by Theory, Driving the Economy</title>
    <summary>[Guest post by Sid Banerjee.] Divergent Evolution: The formation of new species when populations experience different selective pressures. While the canonical example is Darwin’s finches, it could apply as well to matching theorists! A notable feature of the first and second workshops at the Simons Institute program on Matching Markets was how researchers in Economics, […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by Sid Banerjee.]</em></p>



<blockquote class="wp-block-quote"><p><em>Divergent Evolution: The formation of new species when populations experience different selective pressures.</em></p></blockquote>



<p>While the canonical example is Darwin’s finches, it could apply as well to matching theorists! A notable feature of the <a href="https://simons.berkeley.edu/workshops/market2019-1" rel="noreferrer noopener" target="_blank">first</a> and <a href="https://simons.berkeley.edu/workshops/market2019-2" rel="noreferrer noopener" target="_blank">second</a> workshops at the <a href="https://simons.berkeley.edu/programs/market2019" rel="noreferrer noopener" target="_blank">Simons Institute program on Matching Markets</a> was how researchers in Economics, Operations Research and TCS all share common antecedents (Fulkerson, Gale, Scarf, Shapley, Walras — to name but a few giants invoked regularly), and yet have taken the theory in  diverse directions. The workshops helped create a healthy dialogue between the communities, as everyone tries to understand each other’s objectives and techniques. </p>



<p>A more notable aspect of matching theory in recent years has been its  impact on the design of real-world marketplaces. Over the two workshops,  a mix of speakers from academia and industry covered a host of markets,  including <a href="https://simons.berkeley.edu/talks/tba-121" rel="noreferrer noopener" target="_blank">payment routing</a>, <a href="https://simons.berkeley.edu/talks/tba-127" rel="noreferrer noopener" target="_blank">online advertising</a>, <a href="https://simons.berkeley.edu/talks/tba-124" rel="noreferrer noopener" target="_blank">kidney exchange</a>, <a href="https://simons.berkeley.edu/talks/tba-128" rel="noreferrer noopener" target="_blank">real-estate</a>, <a href="https://simons.berkeley.edu/talks/tba-131" rel="noreferrer noopener" target="_blank">public housing</a>, <a href="https://simons.berkeley.edu/talks/ridesharing-panel" rel="noreferrer noopener" target="_blank">ride-sharing</a>, <a href="https://simons.berkeley.edu/talks/driving-efficiencies-freight-industry" rel="noreferrer noopener" target="_blank">long-haul trucking</a>, <a href="https://simons.berkeley.edu/talks/ratings-design-and-barriers-entry" rel="noreferrer noopener" target="_blank">restaurant reviews</a>, <a href="https://simons.berkeley.edu/talks/tba-129" rel="noreferrer noopener" target="_blank">school choice</a>, <a href="https://simons.berkeley.edu/talks/unreasonable-effectiveness-artificial-currencies" rel="noreferrer noopener" target="_blank">food-banks</a> and many many others. A common theme that emerged was that online marketplaces, with the support of good algorithm and mechanism designers, are slowly taking over the economy.</p>



<p>And talking of giants of matching theory, another event held in parallel with the program was a <a href="https://simons.berkeley.edu/events/richard-m-karp-distinguished-lecture-inaugural-lecture" rel="noreferrer noopener" target="_blank">celebration</a> of the achievements and contributions of Dick Karp, with Vijay Vazirani giving the <a href="https://simons.berkeley.edu/rmklectures2019-fall-1" rel="noreferrer noopener" target="_blank">inaugural lecture</a> of the Simons Institute Richard M. Karp Distinguished Lecture Series. Vijay’s talk touched on both the above themes, with a sweeping overview of three great threads in matching theory (stable matching, market equilibria, and online matching). He highlighted the critical role of algorithmic thinking in their development, and concluded with a tantalizing 40-year-old open problem connected to finding a polynomial-time algorithm for the Hylland-Zeckhauser market equilibrium. It is an excellent starting point for those interested in the program, or matching markets in general!</p></div>
    </content>
    <updated>2019-10-02T00:45:32Z</updated>
    <published>2019-10-02T00:45:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>robertkleinberg</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-10-02T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00551</id>
    <link href="http://arxiv.org/abs/1910.00551" rel="alternate" type="text/html"/>
    <title>An Efficient Sampling Algorithm for Non-smooth Composite Potentials</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mou:Wenlong.html">Wenlong Mou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Flammarion:Nicolas.html">Nicolas Flammarion</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wainwright:Martin_J=.html">Martin J. Wainwright</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bartlett:Peter_L=.html">Peter L. Bartlett</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00551">PDF</a><br/><b>Abstract: </b>We consider the problem of sampling from a density of the form $p(x) \propto
\exp(-f(x)- g(x))$, where $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a smooth
and strongly convex function and $g: \mathbb{R}^d \rightarrow \mathbb{R}$ is a
convex and Lipschitz function. We propose a new algorithm based on the
Metropolis-Hastings framework, and prove that it mixes to within TV distance
$\varepsilon$ of the target density in at most $O(d \log (d/\varepsilon))$
iterations. This guarantee extends previous results on sampling from
distributions with smooth log densities ($g = 0$) to the more general composite
non-smooth case, with the same mixing time up to a multiple of the condition
number. Our method is based on a novel proximal-based proposal distribution
that can be efficiently computed for a large class of non-smooth functions $g$.
</p></div>
    </summary>
    <updated>2019-10-02T01:25:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00517</id>
    <link href="http://arxiv.org/abs/1910.00517" rel="alternate" type="text/html"/>
    <title>Learning Multi-Stage Sparsification for Maximum Clique Enumeration</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Marco Grassia, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lauri:Juho.html">Juho Lauri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dutta:Sourav.html">Sourav Dutta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ajwani:Deepak.html">Deepak Ajwani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00517">PDF</a><br/><b>Abstract: </b>We propose a multi-stage learning approach for pruning the search space of
maximum clique enumeration, a fundamental computationally difficult problem
arising in various network analysis tasks. In each stage, our approach learns
the characteristics of vertices in terms of various neighborhood features and
leverage them to prune the set of vertices that are likely not contained in any
maximum clique. Furthermore, we demonstrate that our approach is domain
independent -- the same small set of features works well on graph instances
from different domain. Compared to the state-of-the-art heuristics and
preprocessing strategies, the advantages of our approach are that (i) it does
not require any estimate on the maximum clique size at runtime and (ii) we
demonstrate it to be effective also for dense graphs. In particular, for dense
graphs, we typically prune around 30 \% of the vertices resulting in speedups
of up to 53 times for state-of-the-art solvers while generally preserving the
size of the maximum clique (though some maximum cliques may be lost). For large
real-world sparse graphs, we routinely prune over 99 \% of the vertices
resulting in several tenfold speedups at best, typically with no impact on
solution quality.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00510</id>
    <link href="http://arxiv.org/abs/1910.00510" rel="alternate" type="text/html"/>
    <title>Joint Subcarrier and Power Allocation in NOMA: Optimal and Approximate Algorithms</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sala=uuml=n:Lou.html">Lou Salaün</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coupechoux:Marceau.html">Marceau Coupechoux</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Chung_Shue.html">Chung Shue Chen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00510">PDF</a><br/><b>Abstract: </b>Non-orthogonal multiple access (NOMA) is a promising technology to increase
the spectral efficiency and enable massive connectivity in 5G and future
wireless networks. In contrast to orthogonal schemes, such as OFDMA, NOMA
multiplexes several users on the same frequency and time resource. Joint
subcarrier and power allocation problems (JSPA) in NOMA are NP-hard to solve in
general. In this family of problems, we consider the weighted sum-rate (WSR)
objective function as it can achieve various tradeoffs between sum-rate
performance and user fairness. Because of JSPA's intractability, a common
approach in the literature is to solve separately the power control and
subcarrier allocation (also known as user selection) problems, therefore
achieving sub-optimal result. In this work, we first improve the computational
complexity of existing single-carrier power control and user selection schemes.
These improved procedures are then used as basic building blocks to design new
algorithms, namely Opt-JSPA, $\varepsilon$-JSPA and Grad-JSPA. Opt-JSPA
computes an optimal solution with lower complexity than current optimal schemes
in the literature. It can be used as a benchmark for optimal WSR performance in
simulations. However, its pseudo-polynomial time complexity remains impractical
for real-world systems with low latency requirements. To further reduce the
complexity, we propose a fully polynomial-time approximation scheme called
$\varepsilon$-JSPA. Since, no approximation has been studied in the literature,
$\varepsilon$-JSPA stands out by allowing to control a tight trade-off between
performance guarantee and complexity. Finally, Grad-JSPA is a heuristic based
on gradient descent. Numerical results show that it achieves near-optimal WSR
with much lower complexity than existing optimal methods.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00494</id>
    <link href="http://arxiv.org/abs/1910.00494" rel="alternate" type="text/html"/>
    <title>Approximating the Percolation Centrality through Sampling and Pseudo-dimension</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alane M. de Lima, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Murilo_V=_G=_da.html">Murilo V. G. da Silva</a>, André L. Vignatti <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00494">PDF</a><br/><b>Abstract: </b>In this work we investigate the problem of percolation centrality, a
generalization of betweenness centrality, in a weighted graph $G$ under the
light of sample complexity theory. For both betweenness and percolation
centrality the computation of the {\it exact} value for a given vertex $v$ is
not known to be easier than the computation the same value, all at once, for
all $n$ vertices of $G$. In any one of these cases it is an open problem
whether these measures can be computed in $\mathcal{O}(n^{3-c})$ time, for any
constant $c&gt;0$. In this paper we first present a $\mathcal{O}(m \log^2 n)$
randomized approximation algorithm for the percolation centrality for every
vertex of $G$, generalizing techniques developed by \cite{RiondatoUpfal} (this
complexity is reduced to $\mathcal{O}((m+n) \log n)$ for unweighted graphs).
The estimative obtained by the algorithm is within $\epsilon$ of the exact
value with probability $1- \delta$, for {\it fixed} constants $0 &lt;
\epsilon,\delta \leq 1$. Additionally, we show that sample complexity theory
can be used to distinguish the problem of estimating the percolation centrality
of a single vertex, refered as computing $\tilde{p}(v)$, from the problem of
estimating the percolation centrality of every vertex of $G$, refered as
computing $\tilde{p}(G)$. More precisely, we show that $\tilde{p}(v)$ and
$\tilde{p}(G)$ can be estimated respectively in time $\mathcal{O}(m \log n)$
and $\mathcal{O}(m \log^2 n)$. Our results also imply a similar "separation"
for percolation estimation in unweighted dense graphs as well as separations
for the estimation of betweenness centrality that holds in any combination of
the following scenarios: weighted or unweighted for either sparse or dense
graphs.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00477</id>
    <link href="http://arxiv.org/abs/1910.00477" rel="alternate" type="text/html"/>
    <title>Parameterized complexity of quantum invariants</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maria:Cl=eacute=ment.html">Clément Maria</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00477">PDF</a><br/><b>Abstract: </b>We give a general fixed parameter tractable algorithm to compute quantum
invariants of links presented by diagrams, whose complexity is singly
exponential in the carving-width (or the tree-width) of the diagram. In
particular, we get a $O(N^{\frac{3}{2} \mathrm{cw}} \mathrm{poly}(n))$ time
algorithm to compute any Reshetikhin-Turaev invariant---derived from a simple
Lie algebra $\mathfrak{g}$---of a link presented by a planar diagram with $n$
crossings and carving-width $\mathrm{cw}$, and whose components are coloured
with $\mathfrak{g}$-modules of dimension at most $N$. For example, this
includes the $N^{th}$ coloured Jones polynomials and the $N^{th}$ coloured
HOMFLYPT polynomials.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00440</id>
    <link href="http://arxiv.org/abs/1910.00440" rel="alternate" type="text/html"/>
    <title>The Complexity of Packing Edge-Disjoint Paths</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dreier:Jan.html">Jan Dreier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fuchs:Janosch.html">Janosch Fuchs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hartmann:Tim_A=.html">Tim A. Hartmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuinke:Philipp.html">Philipp Kuinke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rossmanith:Peter.html">Peter Rossmanith</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tauer:Bjoern.html">Bjoern Tauer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Hung=Lung.html">Hung-Lung Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00440">PDF</a><br/><b>Abstract: </b>We introduce and study the complexity of Path Packing. Given a graph $G$ and
a list of paths, the task is to embed the paths edge-disjoint in $G$. This
generalizes the well known Hamiltonian-Path problem.
</p>
<p>Since Hamiltonian Path is efficiently solvable for graphs of small treewidth,
we study how this result translates to the much more general Path Packing. On
the positive side, we give an FPT-algorithm on trees for the number of paths as
parameter. Further, we give an XP-algorithm with the combined parameters
maximal degree, number of connected components and number of nodes of degree at
least three. Surprisingly the latter is an almost tight result by runtime and
parameterization. We show an ETH lower bound almost matching our runtime.
Moreover, if two of the three values are constant and one is unbounded the
problem becomes NP-hard.
</p>
<p>Further, we study restrictions to the given list of paths. On the positive
side, we present an FPT-algorithm parameterized by the sum of the lengths of
the paths. Packing paths of length two is polynomial time solvable, while
packing paths of length three is NP-hard. Finally, even the spacial case EPC
where the paths have to cover every edge in $G$ exactly once is already NP-hard
for two paths on 4-regular graphs.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00400</id>
    <link href="http://arxiv.org/abs/1910.00400" rel="alternate" type="text/html"/>
    <title>Incorporating Trip Chaining within Online Demand Estimation</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Guido Cantelmoa, Moeid Qurashia, A. Arun Prakashc, Constantinos Antonioua, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Viti:Francesco.html">Francesco Viti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00400">PDF</a><br/><b>Abstract: </b>Time-dependent Origin-Destination (OD) demand flows are fundamental inputs
for Dynamic Traffic Assignment (DTA) systems and real-time traffic management.
This work introduces a novel state-space framework to estimate these demand
flows in an online context. Specifically, we propose to explicitly include
trip-chaining behavior within the state-space formulation, which is solved
using the well-established Kalman Filtering technique. While existing works
already consider structural information and recursive behavior within the
online demand estimation problem, this information has been always considered
at the OD level. In this study, we introduce this structural information by
explicitly representing trip-chaining within the estimation framework. The
advantage is twofold. First, all trips belonging to the same tour can be
jointly calibrated. Second, given the estimation during a certain time
interval, a prediction of the structural deviation over the whole day can be
obtained without the need to run additional simulations. The effectiveness of
the proposed methodology is demonstrated first on a toy network and then on a
large real-world network. Results show that the model improves the prediction
performance with respect to a conventional Kalman Filtering approach. We also
show that, on the basis of the estimation of the morning commute, the model can
be used to predict the evening commute without need of running additional
simulations.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00305</id>
    <link href="http://arxiv.org/abs/1910.00305" rel="alternate" type="text/html"/>
    <title>Complexity of Stability</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Frei:Fabian.html">Fabian Frei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hemaspaandra:Edith.html">Edith Hemaspaandra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rothe:J=ouml=rg.html">Jörg Rothe</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00305">PDF</a><br/><b>Abstract: </b>Graph parameters such as the clique number, the chromatic number, and the
independence number are central in many areas, ranging from computer networks
to linguistics to computational neuroscience to social networks. In particular,
the chromatic number of a graph (i.e., the smallest number of colors needed to
color all vertices such that no two adjacent vertices are of the same color)
can be applied in solving practical tasks as diverse as pattern matching,
scheduling jobs to machines, allocating registers in compiler optimization, and
even solving Sudoku puzzles. Typically, however, the underlying graphs are
subject to (often minor) changes. To make these applications of graph
parameters robust, it is important to know which graphs are stable for them in
the sense that adding or deleting single edges or vertices does not change
them. We initiate the study of stability of graphs for such parameters in terms
of their computational complexity. We show that, for various central graph
parameters, the problem of determining whether or not a given graph is stable
is complete for \Theta_2^p, a well-known complexity class in the second level
of the polynomial hierarchy, which is also known as "parallel access to NP."
</p></div>
    </summary>
    <updated>2019-10-02T01:20:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00277</id>
    <link href="http://arxiv.org/abs/1910.00277" rel="alternate" type="text/html"/>
    <title>Polynomial-Time Preprocessing for Weighted Problems Beyond Additive Goal Functions</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bentert:Matthias.html">Matthias Bentert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bevern:Ren=eacute=_van.html">René van Bevern</a>, Fill Fluschnik, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nichterlein:Andr=eacute=.html">André Nichterlein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niedermeier:Rolf.html">Rolf Niedermeier</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00277">PDF</a><br/><b>Abstract: </b>Kernelization is the fundamental notion for polynomial-time prepocessing with
performance guarantees in parameterized algorithmics. When preprocessing
weighted problems, the need of shrinking weights might arise. Marx and V\'egh
[ACM Trans. Algorithms 2015] and Etscheid et al. [J. Comput. Syst. Sci. 2017]
used a technique due to Frank and Tardos [Combinatorica 1987] that we refer to
as losing-weight technique to obtain kernels of polynomial size for weighted
problems. While the mentioned earlier works focus on problems with additive
goal functions, we focus on a broader class of goal functions. We lift the
losing-weight technique to what we call linearizable goal functions, which also
contain non-additive functions. We apply the lifted technique to five exemplary
problems, thereby improving two results from the literature by proving
polynomial kernels.
</p></div>
    </summary>
    <updated>2019-10-02T01:30:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00223</id>
    <link href="http://arxiv.org/abs/1910.00223" rel="alternate" type="text/html"/>
    <title>An improved analysis and unified perspective on deterministic and randomized low rank matrix approximations</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demmel:James.html">James Demmel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grigori:Laura.html">Laura Grigori</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rusciano:Alexander.html">Alexander Rusciano</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00223">PDF</a><br/><b>Abstract: </b>We introduce a Generalized LU-Factorization (\textbf{GLU}) for low-rank
matrix approximation. We relate this to past approaches and extensively analyze
its approximation properties. The established deterministic guarantees are
combined with sketching ensembles satisfying Johnson-Lindenstrauss properties
to present complete bounds. Particularly good performance is shown for the
sub-sampled randomized Hadamard transform (SRHT) ensemble. Moreover, the
factorization is shown to unify and generalize many past algorithms. It also
helps to explain the effect of sketching on the growth factor during Gaussian
Elimination.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00152</id>
    <link href="http://arxiv.org/abs/1910.00152" rel="alternate" type="text/html"/>
    <title>On the Complexity of Approximating Multimarginal Optimal Transport</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Tianyi.html">Tianyi Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Ho:Nhat.html">Nhat Ho</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cuturi:Marco.html">Marco Cuturi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jordan:Michael_I=.html">Michael I. Jordan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00152">PDF</a><br/><b>Abstract: </b>We study the complexity of approximating the multimarginal optimal transport
(OT) problem, a generalization of the classical optimal transport distance,
considered here between $m$ discrete probability distributions supported each
on $n$ support points. First, we show that the multimarginal OT problem is not
a minimum-cost flow problem when $m \geq 3$. This implies that many of the
combinatorial algorithms developed for classical OT are not applicable to
multimarginal OT, and therefore the standard interior-point algorithm bounds
result in an intractable complexity bound of $\widetilde{\mathcal{O}}(n^{3m})$.
Second, we propose and analyze two simple algorithms for approximating the
multimarginal OT problem. The first algorithm, which we refer to as
multimarginal Sinkhorn, improves upon previous multimarginal generalizations of
the celebrated Sinkhorn algorithm. We show that it achieves a near-linear time
complexity bound of $\widetilde{\mathcal{O}}(m^3 n^m / \varepsilon^2)$ for a
tolerance $\varepsilon \in (0, 1)$. This matches the best known complexity
bound for the Sinkhorn algorithm when $m = 2$ for approximating the classical
OT distance. The second algorithm, which we refer to as multimarginal
Randkhorn, accelerates the first algorithm by incorporating a randomized
estimate sequence and achieves a complexity bound of
$\widetilde{\mathcal{O}}(m^{8/3} n^{m+1/3}/\varepsilon)$. This improves on the
complexity bound of the first algorithm by $1/\varepsilon$ and matches the best
known complexity bound for the Randkhorn algorithm when $m=2$ for approximating
the classical OT distance.
</p></div>
    </summary>
    <updated>2019-10-02T01:38:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.00081</id>
    <link href="http://arxiv.org/abs/1910.00081" rel="alternate" type="text/html"/>
    <title>Automated Generation of Dimensioned Rectangular Floorplans</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Nitant Upasani, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shekhawat:Krishnendra.html">Krishnendra Shekhawat</a>, Garv Sachdeva <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.00081">PDF</a><br/><b>Abstract: </b>This paper proposes a methodology for the automated construction of
rectangular floorplans (RFPs) while addressing dimensional constraints and
adjacency relations. Here, adjacency relations are taken in the form of a
dimensionless rectangular arrangement (RA) ensuring the existence of a RFP,
while dimensional constraints are given in terms of minimum width and aspect
ratio range for each room. A linear optimization model is then presented to
obtain a feasible dimensioned RFP for user-defined constraints. A GUI is also
developed for the automated generation of RFPs. The proposed model is able to
generate feasible solutions for every possible RA in a reasonable amount of
time. From the architectural perspective, this work can be seen as a
re-generation of well-known architectural plans with modified dimensions. In
the end, the regeneration of existing legacy RFPs (corresponding to the
user-defined dimensions) has been demonstrated, taking their image as input.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-02T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13755</id>
    <link href="http://arxiv.org/abs/1909.13755" rel="alternate" type="text/html"/>
    <title>Hamiltonicity in Semi-Regular Tessellation Dual Graphs</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gopinath:Divya.html">Divya Gopinath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kodialam:Rohan.html">Rohan Kodialam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lu:Kevin.html">Kevin Lu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lynch:Jayson.html">Jayson Lynch</a>, Santiago Ospina <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13755">PDF</a><br/><b>Abstract: </b>This paper shows NP-completeness for finding Hamiltonian cycles in induced
subgraphs of the dual graphs of semi-regular tessilations. It also shows
NP-hardness for a new, wide class of graphs called augmented square grids. This
work follows up on prior studies of the complexity of finding Hamiltonian
cycles in regular and semi-regular grid graphs.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13472</id>
    <link href="http://arxiv.org/abs/1909.13472" rel="alternate" type="text/html"/>
    <title>ATOL: Automatic Topologically-Oriented Learning</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Royer:Martin.html">Martin Royer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chazal:Fr=eacute=d=eacute=ric.html">Frédéric Chazal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ike:Yuichi.html">Yuichi Ike</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Umeda:Yuhei.html">Yuhei Umeda</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13472">PDF</a><br/><b>Abstract: </b>There are abundant cases for using Topological Data Analysis (TDA) in a
learning context, but robust topological information commonly comes in the form
of a set of persistence diagrams, objects that by nature are uneasy to affix to
a generic machine learning framework. We introduce a vectorisation method for
diagrams that allows to collect information from topological descriptors into a
format fit for machine learning tools. Based on a few observations, the method
is learned and tailored to discriminate the various important plane regions a
diagram is set into. With this tool one can automatically augment any sort of
machine learning problem with access to a TDA method, enhance performances,
construct features reflecting underlying changes in topological behaviour. The
proposed methodology comes with only high level tuning parameters such as the
encoding budget for topological features. We provide an open-access,
ready-to-use implementation and notebook. We showcase the strengths and
versatility of our approach on a number of applications. From emulous and
modern graph collections to a highly topological synthetic dynamical orbits
data, we prove that the method matches or beats the state-of-the-art in
encoding persistence diagrams to solve hard problems. We then apply our method
in the context of an industrial, difficult time-series regression problem and
show the approach to be relevant.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13356</id>
    <link href="http://arxiv.org/abs/1909.13356" rel="alternate" type="text/html"/>
    <title>A fast boundary integral method for high-order multiscale mesh generation</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vico:Felipe.html">Felipe Vico</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Greengard:Leslie.html">Leslie Greengard</a>, Michael O'Neil, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rachh:Manas.html">Manas Rachh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13356">PDF</a><br/><b>Abstract: </b>In this work we present an algorithm to construct an infinitely
differentiable smooth surface from an input consisting of a (rectilinear)
triangulation of a surface of arbitrary shape. The original surface can have
non-trivial genus and multiscale features, and our algorithm has computational
complexity which is linear in the number of input triangles. We use a smoothing
kernel to define a function $\Phi$ whose level set defines the surface of
interest. Charts are subsequently generated as maps from the original
user-specified triangles to $\mathbb R^3$. The degree of smoothness is
controlled locally by the kernel to be commensurate with the fineness of the
input triangulation. The expression for~$\Phi$ can be transformed into a
boundary integral, whose evaluation can be accelerated using a fast multipole
method. We demonstrate the effectiveness and cost of the algorithm with
polyhedral and quadratic skeleton surfaces obtained from CAD and meshing
software.
</p></div>
    </summary>
    <updated>2019-10-02T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.12945</id>
    <link href="http://arxiv.org/abs/1909.12945" rel="alternate" type="text/html"/>
    <title>EPOSIT: An Absolute Pose Estimation Method for Pinhole and Fish-Eye Cameras</title>
    <feedworld_mtime>1569974400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kang:Zhaobing.html">Zhaobing Kang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zou:Wei.html">Wei Zou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Zheng.html">Zheng Zhu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Chi.html">Chi Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Ma:Hongxuan.html">Hongxuan Ma</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.12945">PDF</a><br/><b>Abstract: </b>This paper presents a generic 6DOF camera pose estimation method, which can
be used for both the pinhole camera and the fish-eye camera. Different from
existing methods, relative positions of 3D points rather than absolute
coordinates in the world coordinate system are employed in our method, and it
has a unique solution. The application scope of POSIT (Pose from Orthography
and Scaling with Iteration) algorithm is generalized to fish-eye cameras by
combining with the radially symmetric projection model. The image point
relationship between the pinhole camera and the fish-eye camera is derived
based on their projection model. The general pose expression which fits for
different cameras can be acquired by four noncoplanar object points and their
corresponding image points. Accurate estimation results are calculated
iteratively. Experimental results on synthetic and real data show that the pose
estimation results of our method are more stable and accurate than
state-of-the-art methods. The source code is available at
https://github.com/k032131/EPOSIT.
</p></div>
    </summary>
    <updated>2019-10-02T00:06:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-8966138510075761711</id>
    <link href="http://processalgebra.blogspot.com/feeds/8966138510075761711/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=8966138510075761711" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/8966138510075761711" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/8966138510075761711" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2019/10/phd-position-at-tue-on-product-line.html" rel="alternate" type="text/html"/>
    <title>PhD position at TU/e on product line engineering in multidisciplinary cyber-physical systems</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The Model Driven Software Engineering section at Eindhoven University of Technology (TU/e) is searching for a candidate for a fully-funded PhD position on product line engineering in the multidisciplinary context of cyber-physical systems to collaborate with the high-tech company ASML in the context of the EU ECSEL project Arrowhead Tools.<br/><br/>See <a href="https://eapls.org/items/3327/">here</a> for the details of the position.<br/><br/>TU/e is a dynamic, research-intensive university in the heart of Europe, and in the Brainport region, a leading European technology region, and a centre for innovation and hi-tech industry. TU/e is consistently ranked within the top-100 positions in several world rankings for its research and quality of education.</div>
    </content>
    <updated>2019-10-01T10:06:00Z</updated>
    <published>2019-10-01T10:06:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2019-10-01T21:06:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3419</id>
    <link href="https://agtb.wordpress.com/2019/10/01/sigecom-dissertation-award-call-for-nominations/" rel="alternate" type="text/html"/>
    <title>SIGecom Dissertation Award — Call for Nominations</title>
    <summary>Please consider nominating graduating Ph.D. students for the SIGecom Dissertation Award.  If you are a graduating student, consider asking your adviser or other senior mentor to nominate you. Nominations are due on February 29, 2020.  This award is given to a student who defended a thesis in 2019.  It is a prestigious award and is […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>
<div>
<div>Please consider nominating graduating Ph.D. students for the SIGecom Dissertation Award.  If you are a graduating student, consider asking your adviser or other senior mentor to nominate you.</div>
<div/>
<div>Nominations are due on February 29, 2020.  This award is given to a student who defended a thesis in 2019.  It is a prestigious award and is accompanied by a $1500 prize.  In the past, the prize has been awarded to:</div>
</div>
<div>
<div/>
<div>2018: Yannai Gonczarowski, “Aspects of Complexity and Simplicity in Economic Mechanisms”</div>
<div>2017: Aviad Rubinstein, “Hardness of Approximation Between P and NP”</div>
<div>2016: Peng Shi, “Prediction and Optimization in School Choice”</div>
<div>2015: Inbal Talgam-Cohen, “Robust Market Design: Information and Computation “</div>
<div>2014: S. Matthew Weinberg, “Algorithms for Strategic Agents”</div>
<div>2013: Balasubramanian Sivan, “Prior Robust Optimization”</div>
<div/>
<div/>
<div>And the award has had nine runner-ups: Nika Haghtalab, Haifeng Xu, Rachel Cummings, Christos Tzamos, Bo Waggoner, James Wright, Xi (Alice) Gao, Yang Cai, and Sigal Oren.  You can find detailed information about the nomination process at: <a href="http://www.sigecom.org/awardd.html" rel="noopener" target="_blank">http://www.sigecom.org/awardd.html</a>. We look forward to reading your nominations!</div>
<div/>
<div/>
<div>Your Award Committee,</div>
<div/>
<div>Ozan Candogan</div>
<div>Renato Paes Leme (Chair)</div>
<div>
<div>Yiling Chen</div>
</div>
</div>
</div>
<div class="yj6qo"/>
<div class="adL"/></div>
    </content>
    <updated>2019-10-01T03:08:23Z</updated>
    <published>2019-10-01T03:08:23Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Kevin Leyton-Brown</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-10-02T14:20:39Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2892542038915710982</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2892542038915710982/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/09/richard-guy-is-102-years-old-today.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2892542038915710982" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2892542038915710982" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/09/richard-guy-is-102-years-old-today.html" rel="alternate" type="text/html"/>
    <title>Richard Guy is 103 years old today</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Richard Guy is a mathematician. He co-authored the classic book <i>Winning Ways for your Mathematical Plays </i> with Elywn Berlekamp and John Conway.<br/>
<br/>
On Sept 30 (today) he turned 102. According to <a href="https://en.wikipedia.org/wiki/List_of_centenarians_(scientists_and_mathematicians)">this list </a> he is the oldest living mathematician, and he would need to live to 110 to be the oldest mathematician ever. <br/>
<br/>
I have met him twice. He was at the Gathering for Gardner Conference as a young 98-year old. I told him that his book Winning Ways had a great influence on me. He asked it if was positive or negative. I later saw him at a Math conference where he went to my talk on The Muffin Problem. So he is still active.<br/>
<br/>
His Wikipedia site says that he says he regards himself as an Amateur Mathematician. While it is awkward to disagree with how someone sees himself, I'll point out that he is an author or co-author of 11 books, has many papers, and has solved Erdos Problems. He has taught some but I couldn't really find out what his source of income is or was. This takes us back to the word `amateur' which has several meanings:<br/>
<br/>
Amateur: Someone who does X for the love of X (Amor is Love in Spanish), and not for money. This could be true of Richard Guy. This notion of amateur may be lost on my younger readers since this it used to be a thing to NOT take money since it somehow soils what you do. In those days Olympic athletes could not have played professionally beforehand.  We can't imagine that now.<br/>
<br/>
Amateur: Someone who dabbles in something but is not really that good. This could NOT be true of Richard Guy.<br/>
<br/>
<br/>
<br/>
<br/>
Aside from games he has also worked in Number Theory. His book <i>Unsolved Problems in Number Theory </i> has inspired many (including me). <br/>
<br/>
So happy birthday Richard Guy!<br/>
<br/>
He is the also the oldest living person we have honored on this blog. Second oldest was Katherine Johnson, see <a href="https://blog.computationalcomplexity.org/2018/08/katherine-johnson-1918.html"/> who is still alive.</div>
    </content>
    <updated>2019-10-01T00:03:00Z</updated>
    <published>2019-10-01T00:03:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-10-02T13:54:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13848</id>
    <link href="http://arxiv.org/abs/1909.13848" rel="alternate" type="text/html"/>
    <title>A relaxation of the Directed Disjoint Paths problem: a global congestion metric helps</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lopes:Raul.html">Raul Lopes</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13848">PDF</a><br/><b>Abstract: </b>In the Directed Disjoint Paths problem, we are given a digraph $D$ and a set
of requests $\{(s_1, t_1), \ldots, (s_k, t_k)\}$, and the task is to find a
collection of pairwise vertex-disjoint paths $\{P_1, \ldots, P_k\}$ such that
each $P_i$ is a path from $s_i$ to $t_i$ in $D$. This problem is NP-complete
for fixed $k=2$ and W[1]-hard with parameter $k$ in DAGs. A few positive
results are known under restrictions on the input digraph, such as being planar
or having bounded directed tree-width, or under relaxations of the problem,
such as allowing for vertex congestion. Good news are scarce, however, for
general digraphs. In this article we propose a novel global congestion metric
for the problem: we only require the paths to be "disjoint enough", in the
sense that they must behave properly not in the whole graph, but in an
unspecified large part of it. Namely, in the Disjoint Enough Directed Paths
problem, given an $n$-vertex digraph $D$, a set of $k$ requests, and
non-negative integers $d$ and $s$, the task is to find a collection of paths
connecting the requests such that at least $d$ vertices of $D$ occur in at most
$s$ paths of the collection. We study the parameterized complexity of this
problem for a number of choices of the parameter, including the directed
tree-width of $D$. Among other results, we show that the problem is W[1]-hard
in DAGs with parameter $d$ and, on the positive side, we give an algorithm in
time $O(n^d \cdot k^{d\cdot s})$ and a kernel of size $d \cdot 2^{k-s}\cdot
\binom{k}{s} + 2k$ in general digraphs. The latter result, which is our main
contribution, has consequences for the Steiner Network problem.
</p></div>
    </summary>
    <updated>2019-10-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13830</id>
    <link href="http://arxiv.org/abs/1909.13830" rel="alternate" type="text/html"/>
    <title>Optimal Differential Privacy Composition for Exponential Mechanisms and the Cost of Adaptivity</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Jinshuo.html">Jinshuo Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Durfee:David.html">David Durfee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rogers:Ryan.html">Ryan Rogers</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13830">PDF</a><br/><b>Abstract: </b>Composition is one of the most important properties of differential privacy
(DP), as it allows algorithm designers to build complex private algorithms from
DP primitives. We consider precise composition bounds of the overall privacy
loss for exponential mechanisms, one of the fundamental classes of mechanisms
in DP. We give explicit formulations of the optimal privacy loss for both the
adaptive and non-adaptive settings. For the non-adaptive setting in which each
mechanism has the same privacy parameter, we give an efficiently computable
formulation of the optimal privacy loss. Furthermore, we show that there is a
difference in the privacy loss when the exponential mechanism is chosen
adaptively versus non-adaptively. To our knowledge, it was previously unknown
whether such a gap existed for any DP mechanisms with fixed privacy parameters,
and we demonstrate the gap for a widely used class of mechanism in a natural
setting. We then improve upon the best previously known upper bounds for
adaptive composition of exponential mechanisms with efficiently computable
formulations and show the improvement.
</p></div>
    </summary>
    <updated>2019-10-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13781</id>
    <link href="http://arxiv.org/abs/1909.13781" rel="alternate" type="text/html"/>
    <title>Groups with ALOGTIME-hard word problems and PSPACE-complete compressed word problems</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bartholdi:Laurent.html">Laurent Bartholdi</a>, Michael Figelius, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lohrey:Markus.html">Markus Lohrey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei=szlig=:Armin.html">Armin Weiß</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13781">PDF</a><br/><b>Abstract: </b>We give lower bounds on the complexity of the word problem of certain
non-solvable groups: for a large class of non-solvable infinite groups,
including in particular free groups, Grigorchuk's group and Thompson's groups,
we prove that their word problem is $\mathsf{NC}^1$-hard. For some of these
groups (including Grigorchuk's group and Thompson's groups) we prove that the
compressed word problem (which is equivalent to the circuit evaluation problem)
is $\mathsf{PSPACE}$-complete.
</p></div>
    </summary>
    <updated>2019-10-01T23:21:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13676</id>
    <link href="http://arxiv.org/abs/1909.13676" rel="alternate" type="text/html"/>
    <title>Optimal Algorithms for Submodular Maximization with Distributed Constraints</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Robey:Alexander.html">Alexander Robey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adibi:Arman.html">Arman Adibi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schlotfeldt:Brent.html">Brent Schlotfeldt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pappas:George_J=.html">George J. Pappas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hassani:Hamed.html">Hamed Hassani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13676">PDF</a><br/><b>Abstract: </b>We consider a class of discrete optimization problems that aim to maximize a
submodular objective function subject to a distributed partition matroid
constraint. More precisely, we consider a networked scenario in which multiple
agents choose actions from local strategy sets with the goal of maximizing a
submodular objective function defined over the set of all possible actions.
Given this distributed setting, we develop Constraint-Distributed Continuous
Greedy (CDCG), a message passing algorithm that converges to the tight
$(1-1/e)$ approximation factor of the optimum global solution using only local
computation and communication. It is known that a sequential greedy algorithm
can only achieve a $1/2$ multiplicative approximation of the optimal solution
for this class of problems in the distributed setting. Our framework relies on
lifting the discrete problem to a continuous domain and developing a consensus
algorithm that achieves the tight $(1-1/e)$ approximation guarantee of the
global discrete solution once a proper rounding scheme is applied. We also
offer empirical results from a multi-agent area coverage problem to show that
the proposed method significantly outperforms the state-of-the-art sequential
greedy method.
</p></div>
    </summary>
    <updated>2019-10-01T23:59:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13670</id>
    <link href="http://arxiv.org/abs/1909.13670" rel="alternate" type="text/html"/>
    <title>RECIPE : Converting Concurrent DRAM Indexes to Persistent-Memory Indexes</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Se_Kwon.html">Se Kwon Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mohan:Jayashree.html">Jayashree Mohan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kashyap:Sanidhya.html">Sanidhya Kashyap</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Taesoo.html">Taesoo Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chidambaram:Vijay.html">Vijay Chidambaram</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13670">PDF</a><br/><b>Abstract: </b>We present Recipe, a principled approach for converting concurrent DRAM
indexes into crash-consistent indexes for persistent memory (PM). The main
insight behind Recipe is that isolation provided by a certain class of
concurrent in-memory indexes can be translated with small changes to
crash-consistency when the same index is used in PM. We present a set of
conditions that enable the identification of this class of DRAM indexes, and
the actions to be taken to convert each index to be persistent. Based on these
conditions and conversion actions, we modify five different DRAM indexes based
on B+ trees, tries, radix trees, and hash tables to their crash-consistent PM
counterparts. The effort involved in this conversion is minimal, requiring
30-200 lines of code. We evaluated the converted PM indexes on Intel DC
Persistent Memory, and found that they outperform state-of-the-art,
hand-crafted PM indexes in multi-threaded workloads by up-to 5.2x. For example,
we built P-CLHT, our PM implementation of the CLHT hash table by modifying only
30 LOC. When running YCSB workloads, P-CLHT performs up to 2.4x better than
Cacheline-Conscious Extendible Hashing (CCEH), the state-of-the-art PM hash
table.
</p></div>
    </summary>
    <updated>2019-10-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13541</id>
    <link href="http://arxiv.org/abs/1909.13541" rel="alternate" type="text/html"/>
    <title>An Average-Compress Algorithm for the Sample Mean Problem under Dynamic Time Warping</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Brijnesh Jain, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Froese:Vincent.html">Vincent Froese</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schultz:David.html">David Schultz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13541">PDF</a><br/><b>Abstract: </b>Computing a sample mean of time series under dynamic time warping (DTW) is
NP-hard. Consequently, there is an ongoing research effort to devise efficient
heuristics. The majority of heuristics have been developed for the constrained
sample mean problem that assumes a solution of predefined length. In contrast,
research on the unconstrained sample mean problem is underdeveloped. In this
article, we propose a generic average-compress (AC) algorithm for solving the
unconstrained problem. The algorithm alternates between averaging (A-step) and
compression (C-step). The A-step takes an initial guess as input and returns an
approximation of a sample mean. Then the C-step reduces the length of the
approximate solution. The compressed approximation serves as initial guess of
the A-step in the next iteration. The purpose of the C-step is to direct the
algorithm to more promising solutions of shorter length. The proposed algorithm
is generic in the sense that any averaging and any compression method can be
used. Experimental results show that the AC algorithm substantially outperforms
current state-of-the-art algorithms for time series averaging.
</p></div>
    </summary>
    <updated>2019-10-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13459</id>
    <link href="http://arxiv.org/abs/1909.13459" rel="alternate" type="text/html"/>
    <title>Understanding and Improving Proximity Graph based Maximum Inner Product Search</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Jie.html">Jie Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yan:Xiao.html">Xiao Yan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dai:Xinyan.html">Xinyan Dai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Zhirong.html">Zhirong Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:James.html">James Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Ming=Chang.html">Ming-Chang Yang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13459">PDF</a><br/><b>Abstract: </b>The inner-product navigable small world graph (ip-NSW) represents the
state-of-the-art method for approximate maximum inner product search (MIPS) and
it can achieve an order of magnitude speedup over the fastest baseline.
However, to date it is still unclear where its exceptional performance comes
from. In this paper, we show that there is a strong norm bias in the MIPS
problem, which means that the large norm items are very likely to become the
result of MIPS. Then we explain the good performance of ip-NSW as matching the
norm bias of the MIPS problem - large norm items have big in-degrees in the
ip-NSW proximity graph and a walk on the graph spends the majority of
computation on these items, thus effectively avoids unnecessary computation on
small norm items. Furthermore, we propose the ip-NSW+ algorithm, which improves
ip-NSW by introducing an additional angular proximity graph. Search is first
conducted on the angular graph to find the angular neighbors of a query and
then the MIPS neighbors of these angular neighbors are used to initialize the
candidate pool for search on the inner-product proximity graph. Experiment
results show that ip-NSW+ consistently and significantly outperforms ip-NSW and
provides more robust performance under different data distributions.
</p></div>
    </summary>
    <updated>2019-10-01T23:24:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13384</id>
    <link href="http://arxiv.org/abs/1909.13384" rel="alternate" type="text/html"/>
    <title>Optimal Sketching for Kronecker Product Regression and Low Rank Approximation</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diao:Huaian.html">Huaian Diao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jayaram:Rajesh.html">Rajesh Jayaram</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Song:Zhao.html">Zhao Song</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Wen.html">Wen Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13384">PDF</a><br/><b>Abstract: </b>We study the Kronecker product regression problem, in which the design matrix
is a Kronecker product of two or more matrices. Given $A_i \in \mathbb{R}^{n_i
\times d_i}$ for $i=1,2,\dots,q$ where $n_i \gg d_i$ for each $i$, and $b \in
\mathbb{R}^{n_1 n_2 \cdots n_q}$, let $\mathcal{A} = A_1 \otimes A_2 \otimes
\cdots \otimes A_q$. Then for $p \in [1,2]$, the goal is to find $x \in
\mathbb{R}^{d_1 \cdots d_q}$ that approximately minimizes $\|\mathcal{A}x -
b\|_p$. Recently, Diao, Song, Sun, and Woodruff (AISTATS, 2018) gave an
algorithm which is faster than forming the Kronecker product $\mathcal{A}$
Specifically, for $p=2$ their running time is $O(\sum_{i=1}^q \text{nnz}(A_i) +
\text{nnz}(b))$, where nnz$(A_i)$ is the number of non-zero entries in $A_i$.
Note that nnz$(b)$ can be as large as $n_1 \cdots n_q$. For $p=1,$ $q=2$ and
$n_1 = n_2$, they achieve a worse bound of $O(n_1^{3/2} \text{poly}(d_1d_2) +
\text{nnz}(b))$. In this work, we provide significantly faster algorithms. For
$p=2$, our running time is $O(\sum_{i=1}^q \text{nnz}(A_i) )$, which has no
dependence on nnz$(b)$. For $p&lt;2$, our running time is $O(\sum_{i=1}^q
\text{nnz}(A_i) + \text{nnz}(b))$, which matches the prior best running time
for $p=2$. We also consider the related all-pairs regression problem, where
given $A \in \mathbb{R}^{n \times d}, b \in \mathbb{R}^n$, we want to solve
$\min_{x} \|\bar{A}x - \bar{b}\|_p$, where $\bar{A} \in \mathbb{R}^{n^2 \times
d}, \bar{b} \in \mathbb{R}^{n^2}$ consist of all pairwise differences of the
rows of $A,b$. We give an $O(\text{nnz}(A))$ time algorithm for $p \in[1,2]$,
improving the $\Omega(n^2)$ time needed to form $\bar{A}$. Finally, we initiate
the study of Kronecker product low rank and low $t$-rank approximation. For
input $\mathcal{A}$ as above, we give $O(\sum_{i=1}^q \text{nnz}(A_i))$ time
algorithms, which is much faster than computing $\mathcal{A}$.
</p></div>
    </summary>
    <updated>2019-10-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13345</id>
    <link href="http://arxiv.org/abs/1909.13345" rel="alternate" type="text/html"/>
    <title>Parallel Machine Scheduling to Minimize Energy Consumption</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Antoniadis:Antonios.html">Antonios Antoniadis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Naveen.html">Naveen Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Gunjan.html">Gunjan Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Nikhil.html">Nikhil Kumar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13345">PDF</a><br/><b>Abstract: </b>Given n jobs with release dates, deadlines and processing times we consider
the problem of scheduling them on m parallel machines so as to minimize the
total energy consumed. Machines can enter a sleep state and they consume no
energy in this state. Each machine requires Q units of energy to awaken from
the sleep state and in its active state the machine can process jobs and
consumes a unit of energy per unit time. We allow for preemption and migration
of jobs and provide the first constant approximation algorithm for this
problem.
</p></div>
    </summary>
    <updated>2019-10-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13344</id>
    <link href="http://arxiv.org/abs/1909.13344" rel="alternate" type="text/html"/>
    <title>Modeling Single Picker Routing Problems in Classical and Modern Warehouses</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goeke:Dominik.html">Dominik Goeke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schneider:Michael.html">Michael Schneider</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13344">PDF</a><br/><b>Abstract: </b>The standard single picker routing problem (SPRP) seeks the cost-minimal tour
to collect a set of given articles in a rectangular single-block warehouse with
parallel picking aisles and a dedicated storage policy, i.e, each SKU is only
available from one storage location in the warehouse. We present a compact
formulation that forgoes classical subtour elimination constraints by directly
exploiting two of the properties of an optimal picking tour used in the dynamic
programming algorithm of Ratliff and Rosenthal (1983). We extend the
formulation to three important settings prevalent in modern e-commerce
warehouses: scattered storage, decoupling of picker and cart, and multiple end
depots. In numerical studies, our formulation outperforms existing standard
SPRP formulations from the literature and proves able to solve large instances
within short runtimes. Realistically sized instances of the three problem
extensions can also be solved with low computational effort. We find that
decoupling of picker and cart can lead to substantial cost savings depending on
the speed and capacity of the picker when traveling alone, whereas additional
end depots have rather limited benefits in a single-block warehouse.
</p></div>
    </summary>
    <updated>2019-10-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13029</id>
    <link href="http://arxiv.org/abs/1909.13029" rel="alternate" type="text/html"/>
    <title>Characterization and Linear-time Recognition of Paired Threshold Graphs</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rong:Guozhen.html">Guozhen Rong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cao:Yixin.html">Yixin Cao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Jianxin.html">Jianxin Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13029">PDF</a><br/><b>Abstract: </b>In a paired threshold graph, each vertex has a weight, and two vertices are
adjacent if their weight sum is large enough and their weight difference is
small enough. It generalizes threshold graphs and unit interval graphs, both
very well studied. We present a vertex ordering characterization of this graph
class, which enables us to prove that it is a subclass of interval graphs.
Further study of clique paths of paired threshold graphs leads to a simple
linear-time recognition algorithm for the class.
</p></div>
    </summary>
    <updated>2019-10-01T23:40:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.13017</id>
    <link href="http://arxiv.org/abs/1909.13017" rel="alternate" type="text/html"/>
    <title>A New Covariance Estimator for Sufficient Dimension Reduction in High-Dimensional and Undersized Sample Problems</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Kabir Opeyemi Olorede, Waheed Babatunde Yahya <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.13017">PDF</a><br/><b>Abstract: </b>The application of standard sufficient dimension reduction methods for
reducing the dimension space of predictors without losing regression
information requires inverting the covariance matrix of the predictors. This
has posed a number of challenges especially when analyzing high-dimensional
data sets in which the number of predictors $\mathit{p}$ is much larger than
number of samples $n,~(n\ll p)$. A new covariance estimator, called the
\textit{Maximum Entropy Covariance} (MEC) that addresses loss of covariance
information when similar covariance matrices are linearly combined using
\textit{Maximum Entropy} (ME) principle is proposed in this work. By
benefitting naturally from slicing or discretizing range of the response
variable, y into \textit{H} non-overlapping categories, $\mathit{h_{1},\ldots
,h_{H}}$, MEC first combines covariance matrices arising from samples in each y
slice $\mathit{h\in H}$ and then select the one that maximizes entropy under
the principle of maximum uncertainty. The MEC estimator is then formed from
convex mixture of such entropy-maximizing sample covariance $S_{\mbox{mec}}$
estimate and pooled sample covariance $\mathbf{S}_{\mathit{p}}$ estimate across
the $\mathit{H}$ slices without requiring time-consuming covariance
optimization procedures. MEC deals directly with singularity and instability of
sample group covariance estimate in both regression and classification
problems. The efficiency of the MEC estimator is studied with the existing
sufficient dimension reduction methods such as \textit{Sliced Inverse
Regression} (SIR) and \textit{Sliced Average Variance Estimator} (SAVE) as
demonstrated on both classification and regression problems using real life
Leukemia cancer data and customers' electricity load profiles from smart meter
data sets respectively.
</p></div>
    </summary>
    <updated>2019-10-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1909.12877</id>
    <link href="http://arxiv.org/abs/1909.12877" rel="alternate" type="text/html"/>
    <title>Computing the Inversion-Indel Distance</title>
    <feedworld_mtime>1569888000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Willing:Eyla.html">Eyla Willing</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stoye:Jens.html">Jens Stoye</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Braga:Mar=iacute=lia_D=_V=.html">Marília D. V. Braga</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1909.12877">PDF</a><br/><b>Abstract: </b>The inversion distance, that is the distance between two unichromosomal
genomes with the same content allowing only inversions of DNA segments, can be
exactly computed thanks to a pioneering approach of Hannenhalli and Pevzner
from 1995. In 2000, El-Mabrouk extended the inversion model to perform the
comparison of unichromosomal genomes with unequal contents, combining
inversions with insertions and deletions (indels) of DNA segments, giving rise
to the inversion-indel distance. However, only a heuristic was provided for its
computation. In 2005, Yancopoulos, Attie and Friedberg started a new branch of
research by introducing the generic double cut and join (DCJ) operation, that
can represent several genome rearrangements (including inversions). In 2006,
Bergeron, Mixtacki and Stoye showed that the DCJ distance can be computed in
linear time with a very simple procedure. As a consequence, in 2010 we gave a
linear-time algorithm to compute the DCJ-indel distance. This result allowed
the inversion-indel model to be revisited from another angle. In 2013, we could
show that, when the diagram that represents the relation between the two
compared genomes has no bad components, the inversion-indel distance is equal
to the DCJ-indel distance. In the present work we complete the study of the
inversion-indel distance by giving the first algorithm to compute it exactly
even in the presence of bad components.
</p></div>
    </summary>
    <updated>2019-10-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/09/30/linkage</id>
    <link href="https://11011110.github.io/blog/2019/09/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Proceedings of the 27th International Symposium on Graph Drawing and Network Visualization (GD 2019) (). As in recent years, Graph Drawing is making an open-access version of their complete proceedings through arXiv, mirroring (except for minor typographic details) the official proceedings to be published through Springer LNCS. One advantage of the arXiv version is that in many cases appendices or longer versions of the papers are also available.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://arxiv.org/html/1909.07013">Proceedings of the 27th International Symposium on Graph Drawing and Network Visualization (GD 2019)</a> (<a href="https://mathstodon.xyz/@11011110/102805229578506057"/>). As in recent years, Graph Drawing is making an open-access version of their complete proceedings through arXiv, mirroring (except for minor typographic details) the official proceedings to be published through Springer LNCS. One advantage of the arXiv version is that in many cases appendices or longer versions of the papers are also available.</p>
  </li>
  <li>
    <p><a href="https://commons.wikimedia.org/wiki/File:National_Library_of_Kosovo_photo_Arben_Llapashtica.jpg">Look at all the triangulated (hemi)spheres!</a> (<a href="https://mathstodon.xyz/@11011110/102811772067495838"/>). Sadly this aerial view of the <a href="https://en.wikipedia.org/wiki/National_Library_of_Kosovo">National Library of Kosovo</a> did not pass <a href="https://en.wikipedia.org/wiki/Wikipedia:Featured_picture_candidates/National_Library_of_Kosovo">its nomination as an English Wikipedia featured picture</a>.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@christianp/102809304875599612">Christian Lawson-Perfect tries to visualize the concept of non-Hamiltonicity</a> by sending flocks of arrowheads tracing around paths in the <a href="https://en.wikipedia.org/wiki/Herschel_graph">Herschel graph</a> but without ever closing up to form a cycle.</p>
  </li>
  <li>
    <p>Millions of books published from 1923 to 1964 didn’t have their copyright renewed, putting them in the public domain in the US. <a href="https://www.vice.com/en_us/article/a3534j/libraries-and-archivists-are-scanning-and-uploading-books-that-are-secretly-in-the-public-domain">Libraries and the Internet Archive are collaborating to scan them and put them online</a> (<a href="https://mathstodon.xyz/@11011110/102825724023396126"/>, <a href="https://news.ycombinator.com/item?id=20992397">via</a>). See also <a href="https://www.nypl.org/blog/2019/09/01/historical-copyright-records-transparency">the NYPL blog</a> for more technical details about the efforts to determine the non-renewal status of these books.</p>
  </li>
  <li>
    <p><a href="https://phys.org/news/2019-08-mathematical-framework-sheet-material-kirigami.html">Programmable kirigami</a> (<a href="https://mathstodon.xyz/@11011110/102828568682129281"/>, <a href="https://news.ycombinator.com/item?id=20792995">via</a>, <a href="https://www.nature.com/articles/s41563-019-0452-y">original research paper</a>). Harvard researchers use numerical optimization to design slit cutting patterns and hinged unfoldings that allow sheets of material to expand from one given shape to another.</p>
  </li>
  <li>
    <p>White nationalists use the abbreviation SPQR (<a href="https://archive.org/stream/handbuchdertheo00hefngoog#page/n122/mode/2up">Sono Pazzi Questi Romani!</a>) thinking it refers to the Roman military. <a href="http://pages.vassar.edu/pharos/2018/07/27/scholars-respond-to-spqr-and-white-nationalism/">Classics scholars set them straight</a> (<a href="https://mathstodon.xyz/@11011110/102834635536818832"/>). As the same abbreviation is widely used in graph drawing for <a href="https://en.wikipedia.org/wiki/SPQR_tree">a data structure to describe 3-connected components and planar embeddings</a> I think it’s important to pay attention to these darker shifts in  popular usage.</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=4317">Scott Aaronson’s Quantum Supremacy FAQ</a> (<a href="https://mathstodon.xyz/@11011110/102845648603908743"/>). The story Scott’s responding to is not yet properly published in peer-reviewed sources, but the scoop-hungry journalists say that Google has demonstrated their 50-something-qbit machines to be truly quantum. This seems like pretty big news from the quantum computing world even though it’s still a long way from there to breaking RSA.</p>
  </li>
  <li>
    <p>Embedding a Sierpiński tetrahedron onto a king’s graph (<a href="https://mathstodon.xyz/@11011110/102849777026944754"/>):</p>

    <p style="text-align: center;"><img alt="Embedding a Sierpi&#x144;ski tetrahedron onto a king's graph" src="https://11011110.github.io/blog/assets/2019/sierpinski-kings-graph.svg"/></p>
  </li>
  <li>
    <p><a href="https://nerdcore.de/2019/09/23/eine-fotostudie-der-tafeln-von-mathematikern/">Jessica Wynne’s forthcoming book “Do Not Erase”, her collection of photographs of mathematical blackboards, looks fascinating</a> (<a href="https://mathstodon.xyz/@11011110/102856920767696508"/>, <a href="https://www.metafilter.com/183291/This-is-what-thought-looks-like">discussion</a>, <a href="https://boingboing.net/2019/09/27/do-not-erase.html">see also</a>). There’s also an NYT article but I’m not linking it because they were too sniffy about reading their site in incognito mode.</p>
  </li>
  <li>
    <p><a href="http://bit-player.org/2019/my-god-its-full-of-dots">My God, It’s Full of Dots!</a> (<a href="https://mathstodon.xyz/@11011110/102868939006126852"/>). Brian Hayes plays with the fractal circle packings of the plane (or of a region of the plane) that you get from a greedy process of picking random points and using either the maximum radius possible or the next radius on a given sequence of radii (but then only adding a circle if that radius fits).</p>
  </li>
  <li>
    <p><a href="https://blogs.plos.org/absolutely-maybe/2019/09/27/google-scholar-risks-and-alternatives/">Is the increasing use of Google Scholar causing citations to be concentrated more heavily on a smaller number of highly-cited papers?</a> (<a href="https://mathstodon.xyz/@11011110/102871113125694380"/>, <a href="https://retractionwatch.com/2019/09/28/weekend-reads-jailed-for-publishing-a-paper-pushing-back-on-vaping-research-sugar-daddy-science/">via</a>).</p>
  </li>
  <li>
    <p><a href="https://www.latimes.com/california/story/2019-09-28/leading-chinese-american-scholars-decry-fallout-on-them-of-trumps-hardline-policies-against-china">Leading Chinese American scholars decry racial profiling from Trump’s hard-line policies against China</a> (<a href="https://mathstodon.xyz/@11011110/102877049872954133"/>). You know how the US took over as a leader in mathematics and science from Germany in the 1930s-1940s because the Nazis were already driving away their best Jewish scientists, long before they became completely genocidal? This feels kind of similar.</p>
  </li>
  <li>
    <p><a href="https://royalsociety.org.nz/150th-anniversary/150-women-in-150-words/">150 women in 150 words</a> (<a href="https://mathstodon.xyz/@11011110/102884552172728020"/>). The Royal Society of New Zealand celebrates its 150th anniversary by highlighting the contributions of women in New Zealand to scientific knowledge.</p>
  </li>
</ul></div>
    </content>
    <updated>2019-09-30T18:36:00Z</updated>
    <published>2019-09-30T18:36:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-10-01T17:35:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/132</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/132" rel="alternate" type="text/html"/>
    <title>TR19-132 |  Radio Network Coding Requires Logarithmic Overhead | 

	Raghuvansh Saxena, 

	Klim Efremenko, 

	Gillat Kol</title>
    <summary>We consider the celebrated radio network model for abstracting communication in wireless networks. In this model, in any round, each node in the network may broadcast a message to all its neighbors. However, a node is able to hear a message broadcast by a neighbor only if no collision occurred, meaning that it was the only neighbor broadcasting. 


While the (noiseless) radio network model received a lot of attention over the last few decades, the effect of noise on radio networks is still not well understood. In this paper, we take a step forward and show that making radio network protocols resilient to noise may require a substantial performance overhead. Specifically, we  construct a multi-hop network and a communication protocol over this network that works in $T$ rounds when there is no noise. We prove that any scheme that simulates our protocol and is resilient to stochastic noise, requires $\Omega(T \log n)$ rounds. 

This stands in contrast to our previous result (STOC, 2018), showing that protocols over the single-hop (clique) network can be made noise resilient with only a constant overhead. Our result also settles a recent conjecture by Censor{-}Hillel, Haeupler, Hershkowitz, Zuzic (2018).

We complement the above result by giving a scheme to simulate any protocol with a fixed order of transmissions with only an $\mathcal{O}(\log n)$ overhead.</summary>
    <updated>2019-09-30T17:48:38Z</updated>
    <published>2019-09-30T17:48:38Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-02T14:20:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16277</id>
    <link href="https://rjlipton.wordpress.com/2019/09/30/writing-33-as-a-sum-of-cubes/" rel="alternate" type="text/html"/>
    <title>Writing 33 as a Sum of Cubes</title>
    <summary>Cracking a Diophantine problem for 42 too Andrew Booker is a mathematician at the University of Bristol, who works in analytic number theory. For example he has a paper extending a result of Alan Turing on the Riemann zeta function. Yes our Turing. Today Ken and I will talk about his successful search for a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Cracking a Diophantine problem for 42 too</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2019/09/bookerbristolmath.jpg"><img alt="" class="alignright wp-image-16279" height="150" src="https://rjlipton.files.wordpress.com/2019/09/bookerbristolmath.jpg?w=116&amp;h=150" width="116"/></a></p>
<p>
Andrew Booker is a mathematician at the University of Bristol, who works in analytic number theory. For example he has a <a href="https://arxiv.org/pdf/1710.00603.pdf">paper</a> extending a result of Alan Turing on the Riemann zeta function. Yes our Turing. </p>
<p>
Today Ken and I will talk about his successful search for a solution to a 64 year old problem.</p>
<p>
He was inspired by a <a href="https://www.youtube.com/watch?v=wymmCdLdPvM&amp;feature=youtu.be">video</a> on the search problem authored by Tim Browning and Brady Haran. The question was to find a solution to 	</p>
<p align="center"><img alt="\displaystyle  33 = x^{3} + y^{3} + z^{3}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++33+%3D+x%5E%7B3%7D+%2B+y%5E%7B3%7D+%2B+z%5E%7B3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  33 = x^{3} + y^{3} + z^{3}. "/></p>
<p>Booker <a href="https://arxiv.org/pdf/1903.04284.pdf">found</a> 	</p>
<p align="center"><img alt="\displaystyle  33 = (8 866 128 975 287 528)^{3} - (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++33+%3D+%288+866+128+975+287+528%29%5E%7B3%7D+-+%288+778+405+442+862+239%29%5E%7B3%7D+-+%282+736+111+468+807+040%29%5E%7B3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  33 = (8 866 128 975 287 528)^{3} - (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3}. "/></p>
<p>The search was for all possible solutions with <img alt="{x,y,z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y,z}"/> bounded by <img alt="{10^{16}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B10%5E%7B16%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{10^{16}}"/>. Note that this is expensive, and is not even close to polynomial time in the number of bits. But it is feasible today thanks to modern technology: </p>
<blockquote><p><b> </b> <em> The total computation used was approximately <img alt="{23}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B23%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{23}"/> core-years over one month of real time. </em>
</p></blockquote>
<p/><p>
Before we turn to our discussion, note that Booker’s paper on extending Turing is really a result on proof checking. Turing had great intuition, terrible that we lost him so early. He, Turing, essentially proved the first result ever on how to efficiently check a computation. Booker says: </p>
<blockquote><p><b> </b> <em> Turing introduced a method for certifying the completeness of a purported list of zeros of <img alt="{Z(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%28t%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{Z(t)}"/> that is guaranteed to work (when the list is in fact complete). Turing’s method has remained a small but essential ingredient in all subsequent verifications of RH and its many generalizations. </em>
</p></blockquote>
<p>That is checking the zeroes of the Riemann zeta function <img alt="{Z(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z(t)}"/>.</p>
<p>
Speaking of checking, when I was drafting this I initially had the wrong solution: 	</p>
<p align="center"><img alt="\displaystyle  33 = (8 866 128 975 287 528)^{3} + (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++33+%3D+%288+866+128+975+287+528%29%5E%7B3%7D+%2B+%288+778+405+442+862+239%29%5E%7B3%7D+-+%282+736+111+468+807+040%29%5E%7B3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  33 = (8 866 128 975 287 528)^{3} + (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3}. "/></p>
<p>which is <b>wrong</b>. Can you quickly see why this cannot be right? Answer at the end.</p>
<p>
</p><p/><h2> The Press </h2><p/>
<p/><p>
The press love Booker’s result. Not the one on the zeta function, the one on the number <img alt="{33}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{33}"/>.</p>
<p>
Part of the excitement is caused by the number <img alt="{33}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{33}"/>. In complexity theory we rarely see explicit numbers—more likely to see expressions like 	</p>
<p align="center"><img alt="\displaystyle  O(n^{\log_2 3}(\log \log n)^{3}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++O%28n%5E%7B%5Clog_2+3%7D%28%5Clog+%5Clog+n%29%5E%7B3%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  O(n^{\log_2 3}(\log \log n)^{3}) "/></p>
<p>and worse. </p>
<p>
The press seem to like the numerology of <img alt="{33}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{33}"/>. The number <img alt="{33}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{33}"/> is quite <a href="https://en.wikipedia.org/wiki/33_(number)">neat</a>: </p>
<ul>
<li>
The atomic number of arsenic. <p/>
</li><li>
It is the code for international direct-dial phone calls to France. <p/>
</li><li>
It is Kareem Abdul-Jabbar’s old jersey number. <p/>
</li><li>
And more <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/>
</li></ul>
<p>
Most important is the connection with Rolling-Rock beer:</p>
<p><a href="https://rjlipton.files.wordpress.com/2019/09/roll.jpg"><img alt="" class="aligncenter size-medium wp-image-16280" height="300" src="https://rjlipton.files.wordpress.com/2019/09/roll.jpg?w=143&amp;h=300" width="143"/></a></p>
<p>
The press from <em>Newsweek</em> and other sites talked about Booker’s solution. See <a href="https://www.newsweek.com/uncracked-problem-mathematician-diophantine-puzzle-1384422">here</a> and <a href="https://phys.org/news/2019-04-bristol-mathematician-diophantine-puzzle.html">here</a>. And <a href="https://www.quantamagazine.org/sum-of-three-cubes-problem-solved-for-stubborn-number-33-20190326/">here</a> at the <em>Quanta</em> magazine with a great diagram:</p>
<p><a href="https://rjlipton.files.wordpress.com/2019/09/cube.jpg"><img alt="" class="aligncenter size-medium wp-image-16281" height="127" src="https://rjlipton.files.wordpress.com/2019/09/cube.jpg?w=300&amp;h=127" width="300"/></a></p>
<p>
One <a href="https://science.howstuffworks.com/math-concepts/mathematician-has-just-cracked-33-problem.htm">said</a>: </p>
<blockquote><p><b> </b> <em> To crunch the numbers, he then used a cluster of powerful computers – 512 central processing unit (CPU) cores at the same time – known as Blue Crystal Phase 3. When he returned to his office one morning after dropping his children off at school, he spotted the solution on his screen. “I jumped for joy,” he recalled. </em>
</p></blockquote>
<p/><p>
Another <a href="https://www.bristol.ac.uk/news/2019/april/number-33-.html">reported</a>, </p>
<blockquote><p><b> </b> <em> Booker said: “This one’s right at the boundary between what we know how to prove and what we suspect might be undecidable.” </em>
</p></blockquote>
<p/><p>
I hope we will get the same coverage for our big results. </p>
<p>
</p><p/><h2> More Press </h2><p/>
<p/><p>
The press love Booker’s result. Not the one on the zeta function, the one on the number <img alt="{42}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{42}"/>. This search was jointly led by Andrew Sutherland of MIT.</p>
<p>
Part of the excitement is caused by the number <img alt="{42}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{42}"/>. In complexity theory we rarely see explicit numbers—more likely to see expressions like 	</p>
<p align="center"><img alt="\displaystyle  O(n^{\log_{3} 4}(\log \log n)^{2}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++O%28n%5E%7B%5Clog_%7B3%7D+4%7D%28%5Clog+%5Clog+n%29%5E%7B2%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  O(n^{\log_{3} 4}(\log \log n)^{2}) "/></p>
<p>and worse. </p>
<p>
The press seem to like the numerology of <img alt="{42}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{42}"/>. The number <img alt="{42}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{42}"/> is quite <a href="https://en.wikipedia.org/wiki/42_(number)">neat</a>: </p>
<ul>
<li>
The atomic number of molybdenum. <p/>
</li><li>
It was the code for international direct-dial phone calls to Czechoslovakia, until the “velvet divorce” split the codes into <img alt="{420}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B420%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{420}"/> and <img alt="{421}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B421%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{421}"/>. <p/>
</li><li>
It was Jackie Robinson’s old jersey number. Major League Baseball retired the number for all teams. The last player allowed to wear it was Mariano Rivera of the Yankees, himself a Hall-of-Famer, except that all players on all teams wear it on April 15. Rivera is nicknamed “Mo” which is the symbol for molybdenum—are we the first to notice this coincidence?<p/>
</li><li>
And more <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/>
</li></ul>
<p>
Most important is the <a href="https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker's_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42)">connection</a> with <em>The Hitchhiker’s Guide to the Galaxy</em>:</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2019/09/wiki42snip.jpg"><img alt="" class="aligncenter wp-image-16282" height="210" src="https://rjlipton.files.wordpress.com/2019/09/wiki42snip.jpg?w=240&amp;h=210" width="240"/></a></p>
<p>
The press from <em>New Scientist</em> and other sites talked about Booker’s solution. See <a href="https://www.newscientist.com/article/2215680-mathematicians-crack-elusive-puzzle-involving-the-number-42/">here</a> and <a href="https://science.howstuffworks.com/math-concepts/mathematicians-solved-sum-3-cubes-problem-42.htm">here</a>. But <a href="https://www.quantamagazine.org/search?q[s]=42&amp;q[sort]=newest">here</a> the <em>Quanta</em> magazine seems not to have mentioned the number <img alt="{42}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{42}"/> at all in over three months:</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2019/09/quantano42.jpg"><img alt="" class="aligncenter size-medium wp-image-16284" src="https://rjlipton.files.wordpress.com/2019/09/quantano42.jpg?w=300&amp;h=192"/></a></p>
<p>
One <a href="https://www.sciencealert.com/the-sum-of-three-cubes-problem-has-been-solved-for-42">said</a>:</p>
<blockquote><p><b> </b> <em> Of course, it wasn’t simple. The pair had to go large, so they enlisted the aid of the <a href="https://www.charityengine.com/">Charity Engine</a>, an initiative that spans the globe, harnessing unused computing power from over 500,000 home PCs to act as a sort of “planetary supercomputer.”</em></p><em>
<p>
It took over a million hours of computing time, but the two mathematicians found their solution: </p>
<p align="center"><img alt="\displaystyle  (-80538738812075974)^3 + 80435758145817515^3 + 12602123297335631^3 = 42. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28-80538738812075974%29%5E3+%2B+80435758145817515%5E3+%2B+12602123297335631%5E3+%3D+42.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  (-80538738812075974)^3 + 80435758145817515^3 + 12602123297335631^3 = 42. "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
Another <a href="https://www.bristol.ac.uk/news/2019/september/sum-of-three-cubes-.html">reported</a>:</p>
<blockquote><p><b> </b> <em> Booker said: “I feel relieved … we might find what we’re looking for with a few months of searching, or it might be that the solution isn’t found for another century.” </em>
</p></blockquote>
<p/><p>
I hope we will get the same coverage for our big results. </p>
<p>
</p><p/><h2> Less Press </h2><p/>
<p/><p>
Booker and Sutherland also <a href="https://www.newscientist.com/article/2216941-mathematicians-find-a-completely-new-way-to-write-the-number-3/">discovered</a> that </p>
<p align="center"><img alt="\displaystyle  569936821221962380720^3 - 569936821113563493509^3 - 472715493453327032^3 = 3. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++569936821221962380720%5E3+-+569936821113563493509%5E3+-+472715493453327032%5E3+%3D+3.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  569936821221962380720^3 - 569936821113563493509^3 - 472715493453327032^3 = 3. "/></p>
<p>This is the next-largest solution after <img alt="{3 = 1^3 + 1^3 + 1^3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%3D+1%5E3+%2B+1%5E3+%2B+1%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3 = 1^3 + 1^3 + 1^3}"/> and <img alt="{3 = 4^3 + 4^3 + (-5)^3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+%3D+4%5E3+%2B+4%5E3+%2B+%28-5%29%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3 = 4^3 + 4^3 + (-5)^3}"/>. Weird. And the first solution not to duplicate a number. And it uses two numbers that agree to markedly more decimal places than those in the above solutions for <img alt="{33}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{33}"/> and <img alt="{42}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B42%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{42}"/>. Weirder.</p>
<p>
</p><p/><h2> Smart Search </h2><p/>
<p/><p>
Booker wanted to search for a solution to 	</p>
<p align="center"><img alt="\displaystyle  k = x^{3} + y^{3} + z^{3}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++k+%3D+x%5E%7B3%7D+%2B+y%5E%7B3%7D+%2B+z%5E%7B3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  k = x^{3} + y^{3} + z^{3}. "/></p>
<p>Actually his main interest was in <img alt="{k=33}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%3D33%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k=33}"/>, but his method is general. How does one do this for <img alt="{x,y,z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y,z}"/> bounded by <img alt="{B = 10^{16}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%3D+10%5E%7B16%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B = 10^{16}}"/>. The obvious method is: Try all numbers below <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>. </p>
<p>
This is too expensive and requires <img alt="{\widetilde{O}(B^{3})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cwidetilde%7BO%7D%28B%5E%7B3%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\widetilde{O}(B^{3})}"/> time. Too much, even with a cluster of fast processors.</p>
<p>
An improvement is to try all <img alt="{x,y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y}"/> in the range and then check that <img alt="{k-x^{3}-y^{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk-x%5E%7B3%7D-y%5E%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k-x^{3}-y^{3}}"/> is cube. This runs in <img alt="{\widetilde{O}(B^{2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cwidetilde%7BO%7D%28B%5E%7B2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\widetilde{O}(B^{2})}"/> time. Still too much.</p>
<p>
A key insight is to re-write the equation as 	</p>
<p align="center"><img alt="\displaystyle  k -z^{3} = x^{3} + y^{3} = (x + y)(x^{2} - xy + y^{2}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++k+-z%5E%7B3%7D+%3D+x%5E%7B3%7D+%2B+y%5E%7B3%7D+%3D+%28x+%2B+y%29%28x%5E%7B2%7D+-+xy+%2B+y%5E%7B2%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  k -z^{3} = x^{3} + y^{3} = (x + y)(x^{2} - xy + y^{2}). "/></p>
<p>Then we note that <img alt="{x+y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x+y}"/> must be a divisor of <img alt="{k-z^{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk-z%5E%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k-z^{3}}"/>. Since there are few such divisors, we can improve the time greatly. For the divisors <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d}"/> of <img alt="{k-z^{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk-z%5E%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k-z^{3}}"/> some simple algebra and the quadratic formula shows that 	</p>
<p align="center"><img alt="\displaystyle  x = \frac{d}{2} + \sqrt{\frac{4|k-z^{3}| - d^{3}}{6d}}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%3D+%5Cfrac%7Bd%7D%7B2%7D+%2B+%5Csqrt%7B%5Cfrac%7B4%7Ck-z%5E%7B3%7D%7C+-+d%5E%7B3%7D%7D%7B6d%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x = \frac{d}{2} + \sqrt{\frac{4|k-z^{3}| - d^{3}}{6d}}, "/></p>
<p>and 	</p>
<p align="center"><img alt="\displaystyle  y = \frac{d}{2} - \sqrt{\frac{4|k-z^{3}| - d^{3}}{3d}}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++y+%3D+%5Cfrac%7Bd%7D%7B2%7D+-+%5Csqrt%7B%5Cfrac%7B4%7Ck-z%5E%7B3%7D%7C+-+d%5E%7B3%7D%7D%7B3d%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  y = \frac{d}{2} - \sqrt{\frac{4|k-z^{3}| - d^{3}}{3d}}. "/></p>
<p>This shows that the search is now reduced to <img alt="{\widetilde{O}(B)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cwidetilde%7BO%7D%28B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\widetilde{O}(B)}"/>. Still too much, but close to doable. The next trick is to avoid the factoring step. See Booker’s <a href="https://arxiv.org/pdf/1903.04284.pdf">paper</a> for the rest of the search description. </p>
<p>
I like the progression of time bounds from 	</p>
<p align="center"><img alt="\displaystyle  \widetilde{O}(B^{3}) \rightarrow \widetilde{O}(B^{2}) \rightarrow \widetilde{O}(B). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cwidetilde%7BO%7D%28B%5E%7B3%7D%29+%5Crightarrow+%5Cwidetilde%7BO%7D%28B%5E%7B2%7D%29+%5Crightarrow+%5Cwidetilde%7BO%7D%28B%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \widetilde{O}(B^{3}) \rightarrow \widetilde{O}(B^{2}) \rightarrow \widetilde{O}(B). "/></p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Can one beat <img alt="{\widetilde{O}(B)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cwidetilde%7BO%7D%28B%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\widetilde{O}(B)}"/>? Could there be an algorithm that runs in <img alt="{\widetilde{O}(B^{a})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cwidetilde%7BO%7D%28B%5E%7Ba%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\widetilde{O}(B^{a})}"/> for some <img alt="{a&lt;1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%3C1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a&lt;1}"/>? Can any of our tricks apply here? A possible observation: Booker is clever but he writes that the methods use not 	</p>
<p align="center"><img alt="\displaystyle  \widetilde{O}(B^{3}) \rightarrow \widetilde{O}(B^{2}) \rightarrow \widetilde{O}(B) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cwidetilde%7BO%7D%28B%5E%7B3%7D%29+%5Crightarrow+%5Cwidetilde%7BO%7D%28B%5E%7B2%7D%29+%5Crightarrow+%5Cwidetilde%7BO%7D%28B%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \widetilde{O}(B^{3}) \rightarrow \widetilde{O}(B^{2}) \rightarrow \widetilde{O}(B) "/></p>
<p>time, but that they use 	</p>
<p align="center"><img alt="\displaystyle  {O}(B^{3+\epsilon}) \rightarrow {O}(B^{2+\epsilon}) \rightarrow {O}(B^{1+ \epsilon}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7BO%7D%28B%5E%7B3%2B%5Cepsilon%7D%29+%5Crightarrow+%7BO%7D%28B%5E%7B2%2B%5Cepsilon%7D%29+%5Crightarrow+%7BO%7D%28B%5E%7B1%2B+%5Cepsilon%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {O}(B^{3+\epsilon}) \rightarrow {O}(B^{2+\epsilon}) \rightarrow {O}(B^{1+ \epsilon}) "/></p>
<p>time. Maybe we can help in some manner. What do you think? The next unsolved number, <img alt="{114}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B114%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{114}"/>, awaits.</p>
<p/><p align="center">§</p>
<p>			 <i>Answer to the question on checking</i>: Take the numbers modulo <img alt="{10}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{10}"/>. 	</p>
<p align="center"><img alt="\displaystyle  33 = (8 866 128 975 287 528)^{3} + (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++33+%3D+%288+866+128+975+287+528%29%5E%7B3%7D+%2B+%288+778+405+442+862+239%29%5E%7B3%7D+-+%282+736+111+468+807+040%29%5E%7B3%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  33 = (8 866 128 975 287 528)^{3} + (8 778 405 442 862 239)^{3} - (2 736 111 468 807 040)^{3} "/></p>
<p>becomes 	</p>
<p align="center"><img alt="\displaystyle  3 \equiv (8)^{3} + (9)^{3} - (0)^{3} \equiv 2 + 9 \equiv 1. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3+%5Cequiv+%288%29%5E%7B3%7D+%2B+%289%29%5E%7B3%7D+-+%280%29%5E%7B3%7D+%5Cequiv+2+%2B+9+%5Cequiv+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  3 \equiv (8)^{3} + (9)^{3} - (0)^{3} \equiv 2 + 9 \equiv 1. "/></p>
<p>
</p><p>[Typo fixed]</p></font></font></div>
    </content>
    <updated>2019-09-30T16:10:26Z</updated>
    <published>2019-09-30T16:10:26Z</published>
    <category term="algorithms"/>
    <category term="History"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="Andrew Booker"/>
    <category term="Andrew Sutherland"/>
    <category term="Charity Engine"/>
    <category term="computer search"/>
    <category term="number theory"/>
    <category term="sum-of-cubes problem"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-10-02T14:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/131</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/131" rel="alternate" type="text/html"/>
    <title>TR19-131 |  A Simple Proof of Vyalyi&amp;#39;s Theorem and some Generalizations | 

	Lieuwe Vinkhuijzen, 

	André Deutz</title>
    <summary>In quantum computational complexity theory, the class QMA models the set of problems efficiently verifiable by a quantum computer the same way that NP models this for classical computation. Vyalyi proved that if $\text{QMA}=\text{PP}$ then $\text{PH}\subseteq \text{QMA}$. In this note, we give a simple, self-contained proof of the theorem, using only the closure properties of the complexity classes in the theorem statement. We then extend the theorem in two directions: (i) we strengthen the consequent, proving that if $\text{QMA}=\text{PP}$ then $\text{QMA}=\text{PH}^{\text{PP}}$, and (ii) we weaken the hypothesis, proving that if $\text{QMA}=\text{coQMA}$ then $\text{PH}\subseteq \text{QMA}$. Lastly, we show that all the above results hold, without loss of generality, for the class QAM instead of QMA. We also formulate a ``Quantum Toda's Conjecture''.</summary>
    <updated>2019-09-30T14:57:20Z</updated>
    <published>2019-09-30T14:57:20Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-02T14:20:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=671</id>
    <link href="https://emanueleviola.wordpress.com/2019/09/30/talk-why-do-lower-bounds-stop-just-before-proving-major-results/" rel="alternate" type="text/html"/>
    <title>Talk: Why do lower bounds stop “just before” proving major results?</title>
    <summary>I have prepared this talk which is a little unusual and is in part historical and speculative. You can view the slides here. I am scheduled to give it in about three hours at Boston University. And because it’s just another day in the greater Boston area, while I’ll be talking my ex office-mate Vitaly […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have prepared this talk which is a little unusual and is in part historical and speculative. You can view the slides <a href="http://www.ccs.neu.edu/home/viola/talks/lower-bounds-201909.pdf">here</a>. I am scheduled to give it in about three hours at <a href="http://www.bu.edu/cs/algorithms-and-theory-seminar/">Boston University</a>. And because it’s just another day in the greater Boston area, while I’ll be talking my ex office-mate <span class="field-content">Vitaly Feldman</span> will be speaking <a href="https://toc.seas.harvard.edu/toc-seminar">at Harvard University</a>.  His talk looks quite interesting and attempts to explain why overfitting is actually necessary for good learning. As for mine, well you’ll have to come and see or take a peek at the slides.</p></div>
    </content>
    <updated>2019-09-30T14:50:33Z</updated>
    <published>2019-09-30T14:50:33Z</published>
    <category term="Uncategorized"/>
    <category term="lecture"/>
    <category term="lower bounds"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2019-10-02T14:21:40Z</updated>
    </source>
  </entry>
</feed>
