<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-09-17T04:22:03Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07800</id>
    <link href="http://arxiv.org/abs/2009.07800" rel="alternate" type="text/html"/>
    <title>Nonlinear quantum search via coined quantum walks</title>
    <feedworld_mtime>1600300800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Basile Herzog, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Molfetta:Giuseppe_Di.html">Giuseppe Di Molfetta</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07800">PDF</a><br/><b>Abstract: </b>We provide first evidence that some families of nonlinear quantum systems,
rephrased in terms of coined quantum walks with effective nonlinear phase,
display a strong computational advantage for search algorithms, over graphs
having sets of vertices of constant size, e.g. the infinite square grid. The
numerical simulations show that the walker finds the marked vertex in
$O(N^{1/4} \log^{3/4} N)$ steps with probability $O(1/\log N)$, for an overall
complexity of $O(N^{1/4}\log^{7/4}N)$. We also confirm this result
analytically. Moreover the algorithm is implemented without the need for a
specific oracle step, but by topological hole defect in the grid. From a
quantum computing perspective, however, this hints at novel applications of
quantum walk search: instead of using them to look for "good" solutions within
the configuration space of a problem, we could use them to look for topological
properties of the entire configuration space.
</p></div>
    </summary>
    <updated>2020-09-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07789</id>
    <link href="http://arxiv.org/abs/2009.07789" rel="alternate" type="text/html"/>
    <title>Approximating the packedness of polygonal curves</title>
    <feedworld_mtime>1600300800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gudmundsson:Joachim.html">Joachim Gudmundsson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sha:Yuan.html">Yuan Sha</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wong:Sampson.html">Sampson Wong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07789">PDF</a><br/><b>Abstract: </b>In 2012 Driemel et al. \cite{DBLP:journals/dcg/DriemelHW12} introduced the
concept of $c$-packed curves as a realistic input model. In the case when $c$
is a constant they gave a near linear time $(1+\varepsilon)$-approximation
algorithm for computing the Fr\'echet distance between two $c$-packed polygonal
curves. Since then a number of papers have used the model.
</p>
<p>In this paper we consider the problem of computing the smallest $c$ for which
a given polygonal curve in $\mathbb{R}^d$ is $c$-packed. We present two
approximation algorithms. The first algorithm is a $2$-approximation algorithm
and runs in $O(dn^2 \log n)$ time. In the case $d=2$ we develop a faster
algorithm that returns a $(6+\varepsilon)$-approximation and runs in
$O((n/\varepsilon^3)^{4/3} polylog (n/\varepsilon)))$ time.
</p>
<p>We also implemented the first algorithm and computed the approximate
packedness-value for 16 sets of real-world trajectories. The experiments
indicate that the notion of $c$-packedness is a useful realistic input model
for many curves and trajectories.
</p></div>
    </summary>
    <updated>2020-09-17T01:22:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07785</id>
    <link href="http://arxiv.org/abs/2009.07785" rel="alternate" type="text/html"/>
    <title>Accelerating Domain Propagation: an Efficient GPU-Parallel Algorithm over Sparse Matrices</title>
    <feedworld_mtime>1600300800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Boro Sofranac, Ambros Gleixner, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pokutta:Sebastian.html">Sebastian Pokutta</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07785">PDF</a><br/><b>Abstract: </b>Fast domain propagation of linear constraints has become a crucial component
of today's best algorithms and solvers for mixed integer programming and
pseudo-boolean optimization to achieve peak solving performance. Irregularities
in the form of dynamic algorithmic behaviour, dependency structures, and
sparsity patterns in the input data make efficient implementations of domain
propagation on GPUs and, more generally, on parallel architectures challenging.
This is one of the main reasons why domain propagation in state-of-the-art
solvers is single thread only. In this paper, we present a new algorithm for
domain propagation which (a) avoids these problems and allows for an efficient
implementation on GPUs, and is (b) capable of running propagation rounds
entirely on the GPU, without any need for synchronization or communication with
the CPU. We present extensive computational results which demonstrate the
effectiveness of our approach and show that ample speedups are possible on
practically relevant problems: on state-of-the-art GPUs, our geometric mean
speed-up for reasonably-large instances is around 10x to 20x and can be as high
as 195x on favorably-large instances.
</p></div>
    </summary>
    <updated>2020-09-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07770</id>
    <link href="http://arxiv.org/abs/2009.07770" rel="alternate" type="text/html"/>
    <title>Faster Property Testers in a Variation of the Bounded Degree Model</title>
    <feedworld_mtime>1600300800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adler:Isolde.html">Isolde Adler</a>, Polly Fahey <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07770">PDF</a><br/><b>Abstract: </b>Property testing algorithms are highly efficient algorithms, that come with
probabilistic accuracy guarantees. For a property P, the goal is to distinguish
inputs that have P from those that are far from having P with high probability
correctly, by querying only a small number of local parts of the input. In
property testing on graphs, the distance is measured by the number of edge
modifications (additions or deletions), that are necessary to transform a graph
into one with property P. Much research has focussed on the query complexity of
such algorithms, i. e. the number of queries the algorithm makes to the input,
but in view of applications, the running time of the algorithm is equally
relevant.
</p>
<p>In (Adler, Harwath STACS 2018), a natural extension of the bounded degree
graph model of property testing to relational databases of bounded degree was
introduced, and it was shown that on databases of bounded degree and bounded
tree-width, every property that is expressible in monadic second-order logic
with counting (CMSO) is testable with constant query complexity and sublinear
running time. It remains open whether this can be improved to constant running
time.
</p>
<p>In this paper we introduce a new model, which is based on the bounded degree
model, but the distance measure allows both edge (tuple) modifications and
vertex (element) modifications. Our main theorem shows that on databases of
bounded degree and bounded tree-width, every property that is expressible in
CMSO is testable with constant query complexity and constant running time in
the new model. We also show that every property that is testable in the
classical model is testable in our model with the same query complexity and
running time, but the converse is not true.
</p>
<p>We argue that our model is natural and our meta-theorem showing constant-time
CMSO testability supports this.
</p></div>
    </summary>
    <updated>2020-09-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07735</id>
    <link href="http://arxiv.org/abs/2009.07735" rel="alternate" type="text/html"/>
    <title>On Symmetric Rectilinear Matrix Partitioning</title>
    <feedworld_mtime>1600300800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Abdurrahman Yaşar, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balin:Muhammed_Fatih.html">Muhammed Fatih Balin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/An:Xiaojing.html">Xiaojing An</a>, Kaan Sancak, Ümit V. Çatalyürek <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07735">PDF</a><br/><b>Abstract: </b>Even distribution of irregular workload to processing units is crucial for
efficient parallelization in many applications. In this work, we are concerned
with a spatial partitioning called rectilinear partitioning (also known as
generalized block distribution) of sparse matrices. More specifically, in this
work, we address the problem of symmetric rectilinear partitioning of a square
matrix. By symmetric, we mean the rows and columns of the matrix are
identically partitioned yielding a tiling where the diagonal tiles (blocks)
will be squares. We first show that the optimal solution to this problem is
NP-hard, and we propose four heuristics to solve two different variants of this
problem. We present a thorough analysis of the computational complexities of
those proposed heuristics. To make the proposed techniques more applicable in
real life application scenarios, we further reduce their computational
complexities by utilizing effective sparsification strategies together with an
efficient sparse prefix-sum data structure. We experimentally show the proposed
algorithms are efficient and effective on more than six hundred test matrices.
With sparsification, our methods take less than 3 seconds in the Twitter graph
on a modern 24 core system and output a solution whose load imbalance is no
worse than 1%.
</p></div>
    </summary>
    <updated>2020-09-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07647</id>
    <link href="http://arxiv.org/abs/2009.07647" rel="alternate" type="text/html"/>
    <title>Related by Similarity II: Poncelet 3-Periodics in the Homothetic Pair and the Brocard Porism</title>
    <feedworld_mtime>1600300800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reznik:Dan.html">Dan Reznik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garcia:Ronaldo.html">Ronaldo Garcia</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07647">PDF</a><br/><b>Abstract: </b>Previously we showed the family of 3-periodics in the elliptic billiard
(confocal pair) is the image under a variable similarity transform of poristic
triangles (those with non-concentric, fixed incircle and circumcircle). Both
families conserve the ratio of inradius to circumradius and therefore also the
sum of cosines. This is consisten with the fact that a similarity preserves
angles. Here we study two new Poncelet 3-periodic families also tied to each
other via a variable similarity: (i) a first one interscribed in a pair of
concentric, homothetic ellipses, and (ii) a second non-concentric one known as
the Brocard porism: fixed circumcircle and Brocard inellipse. The Brocard
points of this family are stationary at the foci of the inellipse. A key common
invariant is the Brocard angle, and therefore the sum of cotangents. This
raises an interesting question: given a non-concentric Poncelet family (limited
or not to the outer conic being a circle), can a similar doppelg\"anger always
be found interscribed in a concentric, axis-aligned ellipse and/or conic pair?
</p></div>
    </summary>
    <updated>2020-09-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07630</id>
    <link href="http://arxiv.org/abs/2009.07630" rel="alternate" type="text/html"/>
    <title>Tropical Kirchhoff's Formula and Postoptimality in Matroid Optimization</title>
    <feedworld_mtime>1600300800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jukna:Stasys.html">Stasys Jukna</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seiwert:Hannes.html">Hannes Seiwert</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07630">PDF</a><br/><b>Abstract: </b>Given an assignment of real weights to the ground elements of a matroid, the
min-max weight of a ground element $e$ is the minimum, over all circuits
containing $e$, of the maximum weight of an element in that circuit with the
element $e$ removed. We use this concept to answer the following structural
questions for the minimum weight basis problem. Which elements are persistent
under a given weighting (belong to all or to none of the optimal bases)? What
changes of the weights are allowed while preserving optimality of optimal
bases? How does the minimum weight of a basis change when the weight of a
single ground element is changed, or when a ground element is contracted or
deleted? Our answer to this latter question gives the tropical (min,+,-)
analogue of Kirchhoff's arithmetic (+,x,/) effective conductance formula for
electrical networks.
</p></div>
    </summary>
    <updated>2020-09-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07311</id>
    <link href="http://arxiv.org/abs/2009.07311" rel="alternate" type="text/html"/>
    <title>Relaxed Locally Correctable Codes with Improved Parameters</title>
    <feedworld_mtime>1600300800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Vahid R. Asadi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shinkar:Igor.html">Igor Shinkar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07311">PDF</a><br/><b>Abstract: </b>Locally decodable codes (LDCs) are error-correcting codes $C : \Sigma^k \to
\Sigma^n$ that admit a local decoding algorithm that recovers each individual
bit of the message by querying only a few bits from a noisy codeword. An
important question in this line of research is to understand the optimal
trade-off between the query complexity of LDCs and their block length. Despite
importance of these objects, the best known constructions of constant query
LDCs have super-polynomial length, and there is a significant gap between the
best constructions and the known lower bounds in terms of the block length.
</p>
<p>For many applications it suffices to consider the weaker notion of relaxed
LDCs (RLDCs), which allows the local decoding algorithm to abort if by querying
a few bits it detects that the input is not a codeword. This relaxation turned
out to allow decoding algorithms with constant query complexity for codes with
almost linear length. Specifically, [BGH+06] constructed an $O(q)$-query RLDC
that encodes a message of length $k$ using a codeword of block length $n =
O(k^{1+1/\sqrt{q}})$.
</p>
<p>In this work we improve the parameters of [BGH+06] by constructing an
$O(q)$-query RLDC that encodes a message of length $k$ using a codeword of
block length $O(k^{1+1/{q}})$. This construction matches (up to a
multiplicative constant factor) the lower bounds of [KT00, Woo07] for constant
query LDCs, thus making progress toward understanding the gap between LDCs and
RLDCs in the constant query regime.
</p>
<p>In fact, our construction extends to the stronger notion of relaxed locally
correctable codes (RLCCs), introduced in [GRR18], where given a noisy codeword
the correcting algorithm either recovers each individual bit of the codeword by
only reading a small part of the input, or aborts if the input is detected to
be corrupt.
</p></div>
    </summary>
    <updated>2020-09-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07323</id>
    <link href="http://arxiv.org/abs/2004.07323" rel="alternate" type="text/html"/>
    <title>The Maximum Distance Problem and Minimal Spanning Trees</title>
    <feedworld_mtime>1600300800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Enrique G. Alvarado, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krishnamoorthy:Bala.html">Bala Krishnamoorthy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vixie:Kevin_R=.html">Kevin R. Vixie</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07323">PDF</a><br/><b>Abstract: </b>Given a compact $E \subset \mathbb{R}^n$ and $s &gt; 0$, the maximum distance
problem seeks a compact and connected subset of $\mathbb{R}^n$ of smallest one
dimensional Hausdorff measure whose $s$-neighborhood covers $E$. For $E\subset
\mathbb{R}^2$, we prove that minimizing over minimum spanning trees that
connect the centers of balls of radius $s$, which cover $E$, solves the maximum
distance problem.
</p>
<p>The main difficulty in proving this result is overcome by the proof of Lemma
3.5, which states that one is able to cover the $s$-neighborhood of a Lipschitz
curve $\Gamma$ in $\mathbb{R}^2$ with a finite number of balls of radius $s$,
and connect their centers with another Lipschitz curve $\Gamma_\ast$, where
$\mathcal{H}^1(\Gamma_\ast)$ is arbitrarily close to $\mathcal{H}^1(\Gamma)$.
</p>
<p>We also present an open source package for computational exploration of the
maximum distance problem using minimum spanning trees, available at
https://github.com/mtdaydream/MDP_MST.
</p></div>
    </summary>
    <updated>2020-09-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/private-pac/</id>
    <link href="https://differentialprivacy.org/private-pac/" rel="alternate" type="text/html"/>
    <title>Differentially Private PAC Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The study of differentially private PAC learning runs all the way from
its introduction in 2008 <a href="https://arxiv.org/abs/0803.0924" title="Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What Can We Learn Privately? FOCS 2008"><strong>[KLNRS08]</strong></a> to a best paper award at the
Symposium on Foundations of Computer Science (FOCS) this year <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a>.
In this post, we’ll recap the history of this line of work, aiming for
enough detail for a rough understanding of the results and methods.</p>

<p>Before we get to the “what” and “how” of private PAC learning, it’s
worth thinking about the “why”. One motivation for this line of work is
that it neatly captures a fundamental question: does privacy in machine
learning come at a price? Machine learning is now sufficiently
successful and widespread for this question to have real import. But to
even start to address this question, we need a formalization of machine
learning that allows us to reason about possible trade-offs in a
rigorous way. Statistical learning theory, and its computational
formalization as PAC learning, provide one such clean and well-studied
model. We can therefore use PAC learning as a testbed whose insights we
might carry to other less idealized forms of learning.</p>

<p>With this motivation in mind, the rest of this post is structured as
follows. The first section covers the basics of the PAC model, and
subsequent sections gradually build up a chronology of results. When
possible, we give short sketches of the accompanying techniques.</p>

<h1 id="pac-learning">PAC Learning</h1>

<p>We’ll start with a brief overview of PAC learning absent any privacy
restrictions. Readers familiar with PAC learning can probably skip this
section while noting that</p>

<ol>
  <li>
    <p>(the cardinality version of) Occam’s razor is a baseline learner
using \(O(\log|\mathcal{H}|)\) samples,</p>
  </li>
  <li>
    <p>VC dimension characterizes non-private PAC learning,</p>
  </li>
  <li>
    <p>we’ll focus on the sample complexity of realizable PAC learning,</p>
  </li>
  <li>
    <p>we’ll usually omit dependencies on accuracy and success probability
parameters, and</p>
  </li>
  <li>
    <p>we’ll usually ignore computational efficiency.</p>
  </li>
</ol>

<p>For readers needing a refresher on PAC learning, the basic element of
the “probably approximately correct” (PAC) framework <a href="https://dl.acm.org/doi/10.1145/1968.1972" title="Leslie G Valiant. A theory of the learnable. Communications of the ACM, 1984"><strong>[Val84]</strong></a> is a
<em>hypothesis</em>. Each hypothesis is a function
\(h \colon \mathcal{X}\to \{-1,1\}\) mapping <em>examples</em> from some space
\(\mathcal{X}\) to binary labels. A collection of hypotheses is a
<em>hypothesis class</em> \(\mathcal{H}\), e.g., thresholds (a.k.a. perceptrons),
rectangles, conjunctions, and so on. In the <em>realizable</em> setting, a
learner receives examples drawn from some unknown distribution and
labeled by an unknown \(h^\ast \in \mathcal{H}\). The learner’s goal is to
with high probability (“probably”) output a hypothesis that mostly
matches the labels of \(h^\ast\) on future examples from the unknown example
distribution (“approximately correct”). In the <em>agnostic</em> setting,
examples are not necessarily labeled by any \(h
\in \mathcal{H}\), and the goal is only to output a hypothesis that
approximates the best error of any hypothesis from \(\mathcal{H}\). As
mentioned above, we focus on the realizable setting unless otherwise
specified. In the <em>proper</em> setting, the learner must output a hypothesis
from \(\mathcal{H}\) itself. In the <em>improper</em> setting, this requirement
is removed.</p>

<p>In general, we say an algorithm \((\alpha,\beta)\)-PAC learns
\(\mathcal{H}\) with sample complexity \(n\) if \(n\) samples are sufficient
to with probability at least \(1-\beta\) obtain error at most \(\alpha\)
over new examples from the distribution. For the purposes of this post,
we generally omit these dependencies on \(\alpha\) and \(\beta\), as they
typically vary little or not at all when switching between non-private
and private PAC learning.</p>

<p>Fortunately, we always have a simple baseline learner based on empirical
risk minimization: given a set of labeled examples, iterate over all
hypotheses \(h \in \mathcal{H}\), check how many of the labeled examples
each \(h\) mislabels, and output a hypothesis that mislabels the fewest
examples. Using this learner, which is sometimes called “Occam’s razor,”
\(O(\log|\mathcal{H}|)\) samples suffice to PAC learn \(\mathcal{H}\).</p>

<p>At the same time, \(|\mathcal{H}|\) is a pretty coarse measure of
hypothesis class complexity, as it would immediately rule out learning
any infinite hypothesis class (of which there are many). Thus, as you
might expect, we can do better. We do so using <em>VC dimension</em>.
\(\mathsf{VCD}\left(\mathcal{H}\right)\) is the size of the largest
possible collection of examples such that, for every labeling of the
examples, \(\mathcal{H}\) contains a hypothesis with that labeling. With
VC dimension, we can essentially swap \(\log|\mathcal{H}|\) with
\(\mathsf{VCD}\left(\mathcal{H}\right)\) in the Occam’s razor bound and
PAC learn with \(O(\mathsf{VCD}\left(\mathcal{H}\right))\) samples. In
fact, the “Fundamental Theorem of Statistical Learning” says that PAC
learnability (realizable or agnostic) is equivalent to finite VC
dimension. In this sense, \(\mathsf{VCD}\left(\mathcal{H}\right)\) is a
good measure of how hard it is to PAC learn \(\mathcal{H}\). As a
motivating example that will re-appear later, note that for the
hypothesis class of 1-dimensional thresholds over \(T\) points,
\(\log |\mathcal{H}| = \log T\), while
\(\mathsf{VCD}\left(\mathcal{H}\right)\) is only 1.</p>

<p><img alt="Example: a one-dimensional threshold function" src="https://differentialprivacy.org/images/thresh.png" style="margin: auto; display: block;" width="400"/>
An illustration of 1-dimensional thresholds. A given threshold is determined by some point \(x^\ast \in [T]\): any example \(x \leq x^\ast\) receives label \(-1\), and any example \(x &gt; x^\ast\) receives label 1.</p>

<h1 id="a-simple-private-pac-learner">A Simple Private PAC Learner</h1>

<p>It is straightforward to add a differential privacy constraint to the
PAC framework: the hypothesis output by the learner must be a
differentially private function of the labeled examples
\((x_1, y_1), \ldots, (x_n, y_n)\). That is, changing any one of the
examples — even to one with an inconsistent label — must not affect
the distribution over hypotheses output by the learner by too much.</p>

<p>Since we haven’t talked about any other PAC learner, we may as well
start with the empirical risk minimization-style Occam’s razor discussed
in the previous section, which simply selects a hypothesis that
minimizes empirical error. A private version becomes easy if we view
this algorithm in the right light. All it is doing is assigning a score
to each possible output (the hypothesis’ empirical error) and outputting
one with the best (lowest) score. This makes it a good candidate for
privatization by the <em>exponential mechanism</em> <a href="https://dl.acm.org/doi/10.1109/FOCS.2007.41" title="Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007."><strong>[MT07]</strong></a>.</p>

<p>Recall that the exponential mechanism uses a scoring function over
outputs to release better outputs with higher probability, subject to
the privacy constraint. More formally, the exponential mechanism
requires a scoring function \(u(X,h)\) mapping (database, output) pairs to
real-valued scores and then selects a given output \(h\) with probability
proportional to \(\exp\left(\tfrac{\varepsilon
u(X,h)}{2\Delta(u)}\right)\). Thus a lower \(\varepsilon\) (stricter
privacy requirement) and larger \(\Delta(u) := \sup_h \sup_{X \sim X’} u(X,h) - u(X’,h) \) (scoring function more sensitive to changing one element in the database \(X\) to make \(X’\)) both lead to a more uniform (more
private) output distribution.</p>

<p>Fortunately for our PAC learning setting, empirical error is not a very
sensitive scoring function: changing one sample only changes empirical
error by 1. We can therefore use (negative) empirical error as our
scoring function \(u(X,h)\), apply the exponential mechanism, and get a
“private Occam’s razor.” This was exactly what Kasiviswanathan, Lee,
Nissim, Raskhodnikova, and Smith <a href="https://arxiv.org/abs/0803.0924" title="Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What Can We Learn Privately? FOCS 2008"><strong>[KLNRS08]</strong></a> did when they introduced
differentially private PAC learning in 2008. The resulting sample
complexity bounds differ from the generic Occam’s razor only by an
\(\varepsilon\) factor in the denominator, and
\(O(\log|\mathcal{H}|/\varepsilon)\) samples suffice to privately PAC
learn \(\mathcal{H}\).</p>

<p>Of course, our experience with non-private PAC learning suggests that we
shouldn’t be satisfied with this \(\log
|\mathcal{H}|\) dependence. Maybe VC dimension characterizes private PAC
learning, too?</p>

<h1 id="characterizing-pure-private-pac-learning">Characterizing Pure Private PAC Learning</h1>

<p>As it turns out, answering this question will take some time. We start
with a partial negative answer. Specifically, we’ll see a class with VC
dimension 1 and (a restricted form of) private sample complexity
arbitrarily larger than 1. We’ll also cover the first in a line of
characterization results for private PAC learning.</p>

<p>We first consider learners that satisfy <em>pure</em> privacy. Recall that pure
\((\varepsilon,0)\)-differential privacy forces output distributions that
may only differ by a certain \(e^\varepsilon\) multiplicative factor (like
the exponential mechanism above). The strictly weaker notion of
approximate \((\varepsilon,\delta)\)-differential privacy also allows a
small additive \(\delta\) factor. Second, we restrict ourselves to
<em>proper</em> learners, which may only output hypotheses from the learned
class \(\mathcal{H}\).</p>

<p>With these assumptions in place, in 2010, Beimel, Kasiviswanathan, and
Nissim <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> studied a hypothesis class called \(\mathsf{Point}_d\).
\(\mathsf{Point}_d\) consists of \(2^d\) hypotheses, one for each vector in
\(\{0,1\}^d\). Taking the set of examples \(\mathcal{X}\) to be \(\{0,1\}^d\)
as well, we define each hypothesis in \(\mathsf{Point}_d\) to label only
its associated vector as 1, and the remaining \(2^d-1\) examples as
-1. <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> showed that the hypothesis class \(\mathsf{Point}_d\) requires
\(\Omega(d)\) samples for proper pure private PAC learning. In contrast,
\(\mathsf{VCD}\left(\mathsf{Point}_d\right) = 1\), so this \(\Omega(d)\)
lower bound shows us that VC dimension does <em>not</em> characterize proper
pure private PAC learning.</p>

<p>This result uses the classic “packing” lower bound method, which powers
many lower bounds for pure differential privacy. The general packing
method is to first construct a large collection of databases which are
all “close enough” to each other but nonetheless all have different
“good” outputs. Once we have such a collection, we use <em>group privacy</em>.
Group privacy is a corollary of differential privacy that requires
databases differing in \(k\) elements to have \(k\varepsilon\)-close output
distributions. Because of group privacy, if we start with a collection
of databases that are close together, then the output distributions for
any two databases in the collection cannot be too different. This
creates a tension: utility forces the algorithm to produce different
output distributions for different databases, but privacy forces
similarity. The packing argument comes down to arguing that, unless the
databases are large, privacy wins out, and when privacy wins out then
there is some database where the algorithm probably produces a bad
output.</p>

<p>For \(\mathsf{Point}_d\), we sketch the resulting argument as follows.
Suppose we have an \(\varepsilon\)-private PAC learner that uses \(m\)
samples. Then we can define a collection of different databases of size
\(m\), one for each hypothesis in \(\mathsf{Point}_d\). By group privacy,
the output distribution for our private PAC learner changes by at most
\(e^{m\varepsilon}\) between any two of the databases in this collection.
Thus we can pick any \(h \in \mathsf{Point}_d\) and know that the
probability of outputting the wrong hypothesis is at least roughly
\(2^d \cdot e^{-m\varepsilon}\). Since we need this probability to be
small, rearranging implies \(m =
\Omega(d/\varepsilon)\).</p>

<p><a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> then contrasted this result with an <em>improper</em> pure private PAC
learner. This learner applies the exponential mechanism to a class
\(\mathsf{Point}_d’\) of hypotheses derived from \(\mathsf{Point}_d\) —
but <em>not</em> necessarily a subset of \(\mathsf{Point}_d\) — gives an
improper pure private PAC learner with sample complexity \(O(\log
d)\). Since this learner is improper, it circumvents the “one database
per hypothesis” step of the packing lower bound. Moreover, <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> gave a
still more involved improper pure private PAC learner requiring only
\(O(1)\) samples. This separates proper pure private PAC learning from
improper pure private PAC learning. In contrast, the sample complexities
of proper and improper PAC learning absent privacy are the same up to
logarithmic factors in \(\alpha\) and \(\beta\).</p>

<p>In 2013, Beimel, Nissim, and Stemmer <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a> proved a more general
result. They gave the first characterization of pure (improper) private
PAC learning by defining a new hypothesis class measure called the
<em>representation dimension</em>, \(\mathsf{REPD}\left(\mathcal{H}\right)\).
Roughly, the representation dimension considers the collection of all
distributions \(\mathcal{D}\) over sets of hypotheses, not necessarily
from \(\mathcal{H}\), that “cover” \(\mathcal{H}\). By “cover,” we mean that
for any \(h
\in \mathcal{H}\), with high probability a set drawn from covering
distribution \(\mathcal{D}\) includes a hypothesis that mostly produces
labels that agree with \(h\). With this collection of distributions
defined, \(\mathsf{REPD}\left(\mathcal{H}\right)\) is the minimum over all
such covering distributions of the logarithm of the size of the largest
set in its support. Thus a hypothesis class that can be covered by a
distribution over small sets of hypotheses will have a small
representation dimension. With the notion of representation dimension in
hand, <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a> gave the following result:</p>

<blockquote>
  <p><strong>Theorem 1</strong> (<a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a>). The sample complexity to pure private PAC learn \(\mathcal{H}\) is \(\Theta(\mathsf{REPD}\left(\mathcal{H}\right))\).</p>
</blockquote>

<p>Representation dimension may seem like a strange definition, but a
sketch of the proof of this result helps illustrate the connection to
private learning. Recall from our private Occam’s razor, and the
improper pure private PAC learner above, that if we can find a good and
relatively small set of hypotheses to choose from, then we can apply the
exponential mechanism and call it a day. It is exactly this kind of
“good set of hypotheses” that representation dimension aims to capture.
A little more formally, given an upper bound on
\(\mathsf{REPD}\left(\mathcal{H}\right)\), we know there is some covering
distribution whose largest hypothesis set is not too big. That means we
can construct a learner that draws a hypothesis set from this covering
distribution and applies the exponential mechanism to it. Just as we
picked up a \(\log|\mathcal{H}|\) sample complexity dependence using
private Occam’s razor, since \(\mathsf{REPD}\left(\mathcal{H}\right)\)
measures the logarithm of the size of the largest hypothesis set in the
support, this pure private learner picks up a
\(\mathsf{REPD}\left(\mathcal{H}\right)\) sample complexity dependence
here. This gives us one direction of
Theorem 1.</p>

<p>This logic works in the other direction as well. To go from a pure
private PAC learner with sample complexity \(m\) to an upper bound on
\(\mathsf{REPD}\left(\mathcal{H}\right)\), we return to the group privacy
trick used by <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a>. Suppose we fix a database of size \(m\) and pass it
to the learner. By group privacy and the learner’s accuracy guarantee,
if we fix some concept \(c\), the learner has probability at least roughly
\(e^{-m}\) of outputting a hypothesis that mostly agrees with \(c\). Thus if
we repeat this process roughly \(e^{m}\) times, we probably get at least
one hypothesis that mostly agrees with \(c\). In other words, this
repeated calling of the learner on the arbitrary database yields a
covering distribution for \(\mathcal{H}\). Since we called the learner
approximately \(e^m\) times, the logarithm of this is \(m\), and we get our
upper bound on \(\mathsf{REPD}\left(\mathcal{H}\right)\).</p>

<p>To recap, we now know that proper pure private PAC learning is strictly
harder than improper pure private PAC learning, which is characterized
by representation dimension. A picture sums it up. Note the dotted line,
since we don’t yet have any evidence separating finite representation
dimension and finite VC dimension.</p>

<p><img alt="Landscape of Private PAC, take 1" src="https://differentialprivacy.org/images/private_pac_1.png" style="margin: auto; display: block;" width="400"/></p>

<h1 id="separating-pure-and-approximate-private-pac-learning">Separating Pure and Approximate Private PAC Learning</h1>

<p>So far, we’ve focused only on pure privacy. In this section, we move on
to the first separations between pure and approximate private PAC
learning, as well as the first connection between private learning and
<em>online</em> learning.</p>

<p>Our source is a pair of interconnected papers from around 2014. Among
other things, Feldman and Xiao <a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a> introduced <em>Littlestone
dimension</em> to private PAC learning. By connecting representation
dimension to results from communication complexity to Littlestone
dimension, they proved the following:</p>

<blockquote>
  <p><strong>Theorem 2</strong> (<a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a>). The sample complexity to pure private PAC learn \(\mathcal{H}\) is \(\Omega(\mathsf{LD}\left(\mathcal{H}\right))\).</p>
</blockquote>

<p>Littlestone dimension \(\mathsf{LD}\left(\mathcal{H}\right)\) is, roughly,
the maximum number of mistakes an adversary can force an <em>online</em>
PAC-learning algorithm to make <a href="https://link.springer.com/article/10.1023/A:1022869011914" title="Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine learning, 1988"><strong>[Lit88]</strong></a>. We always have
\(\mathsf{VCD}\left(\mathcal{H}\right) \leq \mathsf{LD}\left(\mathcal{H}\right) \leq \log|\mathcal{H}|\),
but these inequalities can be strict. For example, denoting by
\(\mathsf{Thresh_T}\) the class of thresholds over \(\{1, 2, \ldots,
T\}\), since an adversary can force \(\Theta(\log T)\) wrong answers from
an online learner binary searching over \(\{1,2, \ldots, T\}\),
\(\mathsf{LD}\left(\mathsf{Thresh_T}\right) = \Omega(\log T)\). In
contrast, \(\mathsf{VCD}\left(\mathsf{Thresh_T}\right) = 1\).</p>

<p>At first glance it’s not obvious what
Theorem 2 adds over
Theorem 1. After all,
Theorem 1 gives an equivalence, not just a lower bound. One
advantage of
Theorem 2 is that Littlestone dimension is a known
quantity that has already been studied in its own right. We can now
import results like the lower bound on
\(\mathsf{LD}\left(\mathsf{Thresh_T}\right)\), whereas bounds on
\(\mathsf{REPD}\left(\cdot\right)\) are not common. A second advantage is
that Littlestone dimension conceptually connects private learning and
online learning: we now know that pure private PAC learning is no easier
than online PAC learning.</p>

<p>A second paper by Beimel, Nissim, and Stemmer <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013"><strong>[BNS13b]</strong></a> contrasted this
\(\Omega(\log T)\) lower bound for pure private learning of thresholds
with a \(2^{O(\log^\ast T)}\) upper bound for <em>approximate</em> private PAC
learning \(\mathsf{Thresh_T}\). Here \(\log^\ast\) denotes the very
slow-growing iterated logarithm, the number of times we must take the
logarithm of the argument to bring it \(\leq 1\). (We’re not kidding about
“very slow-growing” either:
\(\log^\ast(\text{number of atoms in universe}) \approx
4\).) With Feldman and Xiao’s result, this separates pure private PAC
learning from approximate private PAC learning. It also shows that
representation dimension does <em>not</em> characterize approximate private PAC
learning.</p>

<p>At the same time, Feldman and Xiao observed that the connection between
pure private PAC learning and Littlestone dimension is imperfect. Again
borrowing results from communication complexity, they observed that the
hypothesis class \(\mathsf{Line_p}\) (which we won’t define here) has
\(\mathsf{LD}\left(\mathsf{Line_p}\right) = 2\) but
\(\mathsf{REPD}\left(\mathsf{Line_p}\right)
= \Theta(\log(p))\). In contrast, they showed that an <em>approximate</em>
private PAC learner can learn \(\mathsf{Line_p}\) using
\(O\left(\tfrac{\log(1/\beta)}{\alpha}\right)\) samples. Since this
entails no dependence on \(p\) at all, it improves the separation between
pure and approximate private PAC learning given by <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013"><strong>[BNS13b]</strong></a>.</p>

<p>Let’s pause to recap what’s happened so far. We learned in the last
section that representation dimension characterizes pure private PAC
learning <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a>. We learned in this section that Littlestone dimension
gives lower bounds for pure private PAC learning but, as shown by
\(\mathsf{Line_p}\), these bounds are sometimes quite loose <a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a>.
\(\mathsf{Thresh_T}\) shows that representation dimension does not
characterize approximate private PAC learning <strong>[<a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014">FX14</a>
; <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013">BNS13b</a>]</strong>, and we
still have no privacy-specific lower bounds for approximate private
learners. So the picture now looks like this:</p>

<p><img alt="Landscape of Private PAC, take 2" src="https://differentialprivacy.org/images/private_pac_2.png" style="margin: auto; display: block;" width="400"/></p>

<p>In particular, we might still find that VC dimension characterizes
approximate private PAC learning!</p>

<h1 id="lower-bounds-for-approximate-private-pac-learning">Lower Bounds for Approximate Private PAC Learning</h1>

<p>We now dash this hope. In 2015, Bun, Nissim, Stemmer, and
Vadhan <a href="https://arxiv.org/abs/1504.07553" title="Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. FOCS 2015"><strong>[BNSV15]</strong></a> gave the first nontrivial lower bound for approximate
private PAC learning. They showed that learning \(\mathsf{Thresh_T}\) has
<em>proper</em> approximate private sample complexity \(\Omega(\log^\ast(T))\) and
\(O(2^{\log^\ast(T)})\).</p>

<p>We’ll at least try to give some intuition for the presence of \(\log^\ast\)
in the lower bound. Informally, the lower bound relies on an inductive
construction of a sequence of hard problems for databases of size
\(n=1, 2,
\ldots\). The \(k^{th}\) hard problem relies on a distribution over
databases of size \(k\) whose data universe is of of size exponential in
the size of the data universe for the \((k-1)^{th}\) distribution. The
base case is the uniform distribution over the two singleton databases
\(\{0\}\) and \(\{1\}\), and they show how to inductively construct
successive problems such that a solution for the \(k^{th}\) problem
implies a solution for the \((k-1)^{th}\) problem. Unraveling the
recursive relationship between the problem domain sizes implies a
general lower bound of roughly \(\log^\ast|X|\) for domain \(X\).</p>

<p>The inclusion of \(\log^\ast\) makes this is an extremely mild lower bound.
However, \(\log^\ast(T)\) can still be arbitrarily larger than 1, so this is
the first definitive evidence that proper approximate privacy introduces
a cost over non-private PAC learning.</p>

<p>In 2018, Alon, Livni, Malliaris, and Moran <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a> extended this
\(\Omega(\log^\ast T)\) lower bound for \(\mathsf{Thresh_T}\) to <em>improper</em>
approximate privacy. More generally, they gave concrete evidence for the
importance of thresholds, which have played a seemingly outsize role in
the work so far. They did so by relating a class’ Littlestone dimension
to its ability to “contain” thresholds. Here, we say \(\mathcal{H}\)
“contains” \(m\) thresholds if there exist \(m\) (unlabeled) examples
\(x_1,\ldots,x_m\) and hypotheses \(h_1, \ldots, h_m \in \mathcal{H}\) such
that the hypotheses “behave like” thresholds on the \(m\) examples, i.e., 
\(h_i(x_j) = 1 \Leftrightarrow j \geq
i\). With this language, they imported a result from model theory to show
that any hypothesis class \(\mathcal{H}\) contains
\(\log(\mathsf{LD}\left(\mathcal{H}\right))\) thresholds. This implies
that learning \(\mathcal{H}\) is at least as hard as learning
\(\mathsf{Thresh_T}\) with
\(T = \log(\mathsf{LD}\left(\mathcal{H}\right))\). Since
\(\log^\ast(\log(\mathsf{LD}\left(\mathcal{H}\right)))
= \Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\), combining these
two results puts the following limit on private PAC learning:</p>

<blockquote>
  <p><strong>Theorem 3</strong> (<a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>). The sample complexity to approximate private PAC learn \(\mathcal{H}\) is \(\Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\).</p>
</blockquote>

<p>Littlestone dimension characterizes online PAC learning, so we now know
that online PAC learnability is necessary for private PAC learnability.
Sufficiency, however, remains an open question. This produces the
following picture, where the dotted line captures the question of
sufficiency.</p>

<p><img alt="Landscape of Private PAC, take 3" src="https://differentialprivacy.org/images/private_pac_3.png" style="margin: auto; display: block;" width="400"/></p>

<h1 id="characterizing-approximate-private-pac-learning">Characterizing Approximate Private PAC Learning</h1>

<p>Spurred by this question, several advances in private PAC learning have
appeared in the last year. First, Gonen, Hazan, and Moran strengthened
Theorem 3 by giving a constructive method for converting
<em>pure</em> private learners to online learners <a href="https://arxiv.org/abs/1905.11311" title="Alon Gonen, Elad Hazan, and Shay Moran. Private learning implies online learning: An efficient reduction. NeurIPS 2019"><strong>[GHM19]</strong></a>. Their result
reaches back to the 2013 characterization of pure private learning in
terms of representation dimension by using the covering distribution to
generate a collection of “experts” for online learning. Again revisiting
\(\mathsf{Thresh_T}\), Kaplan, Ligett, Mansour, Naor, and
Stemmer <a href="https://arxiv.org/abs/1911.10137" title="Haim Kaplan, Katrina Ligett, Yishay Mansour, Moni Naor, and Uri Stemmer. Privately learning thresholds: Closing the exponential gap. COLT 2020"><strong>[KLMNS20]</strong></a> significantly reduced the \(O(2^{\log^\ast(T)})\) upper
bound of <a href="https://arxiv.org/abs/1504.07553" title="Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. FOCS 2015"><strong>[BNSV15]</strong></a> to just \(O((\log^\ast(T))^{1.5})\). And Alon, Beimel,
Moran, and Stemmer <a href="https://arxiv.org/abs/2003.04509" title="Noga Alon, Amos Beimel, Shay Moran, and Uri Stemmer. Closure properties for private classification and online prediction. COLT 2020"><strong>[ABMS20]</strong></a> justified this post’s focus on realizable
private PAC learning by giving a transformation from a realizable
approximate private PAC learner to an agnostic one at the cost of
slightly worse privacy and sample complexity. This built on an earlier
transformation that only applied to <em>proper</em> learners <a href="https://arxiv.org/abs/1407.2662" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Learning privately with labeled and unlabeled examples. SODA 2015"><strong>[BNS15]</strong></a>.</p>

<p>Finally, Bun, Livni, and Moran <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a> answered the open question posed
by <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>:</p>

<blockquote>
  <p><strong>Theorem 4</strong> (<a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a>). The sample complexity to approximate private PAC learn \(\mathcal{H}\) is \(2^{O({\mathsf{LD}\left(\mathcal{H}\right)})}\).</p>
</blockquote>

<p>To prove this, <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a> introduced the notion of a <em>globally stable</em>
learner and showed how to convert an online learner to a globally stable
learner to a private learner. Thus, combined with the result of <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>,
we now know that the sample complexity of private PAC learning any
\(\mathcal{H}\) is at least
\(\Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\) and at most
\(2^{O({\mathsf{LD}\left(\mathcal{H}\right)})}\). In this sense, online
learnability characterizes private learnability.</p>

<p><img alt="Landscape of Private PAC, final take" src="https://differentialprivacy.org/images/private_pac_4.png" style="margin: auto; display: block;" width="400"/></p>

<p>Narrowing the gap between the lower and upper bounds above is an open
question. Note that we cannot hope to close the gap completely. For the
lower bound, the current \(\mathsf{Thresh_T}\) upper bound implies that no
general lower bound can be stronger than
\(\Omega((\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))^{1.5})\). For the
upper bound, there exist hypotheses classes \(\mathcal{H}\) with
\(\mathsf{VCD}\left(\mathcal{H}\right) = \mathsf{LD}\left(\mathcal{H}\right)\)
(e.g., \(\mathsf{VCD}\left(\mathsf{Point}_d\right) = \mathsf{LD}\left(\mathsf{Point}_d\right)= 1\)), so since non-private PAC learning requires
\(\Omega(\mathsf{VCD}\left(\mathcal{H}\right))\) samples, the best
possible private PAC learning upper bound is
\(O(\mathsf{LD}\left(\mathcal{H}\right))\). Nevertheless, proving either
bound remains open.</p>

<h1 id="conclusion">Conclusion</h1>

<p>This concludes our post, and with it our discussion of this fundamental
question: the price of privacy in machine learning. We now know that in
the PAC model, proper pure private learning, improper pure private
learning, approximate private learning, and non-private learning are all
strongly separated. By the connection to Littlestone dimension, we also
know that approximate private learnability is equivalent to online
learnability. However, many questions about computational efficiency and
tight sample complexity bounds remain open.</p>

<p>As mentioned in the introduction, we focused on the clean yet widely
studied and influential model of PAC learning. Having characterized how
privacy enters the picture in PAC learning, we can hopefully convey this
understanding to other models of learning, and now approach these
questions from a rigorous and grounded point of view.</p>

<p>Congratulations to Mark Bun, Roi Livni, and Shay Moran on their best
paper award — and to the many individuals who paved the way before
them!</p>

<h1 id="acknowledgments">Acknowledgments</h1>

<p>Thanks to Kareem Amin and Clément Canonne for helpful feedback while
writing this post.</p></div>
    </summary>
    <updated>2020-09-16T18:00:00Z</updated>
    <published>2020-09-16T18:00:00Z</published>
    <author>
      <name>Matthew Joseph</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-09-16T23:39:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/141</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/141" rel="alternate" type="text/html"/>
    <title>TR20-141 |  Candidate Tree Codes via Pascal Determinant Cubes | 

	Gil Cohen, 

	Inbar Ben Yaacov, 

	Anand Kumar Narayanan</title>
    <summary>Tree codes are combinatorial structures introduced by Schulman (STOC 1993) as key ingredients in interactive coding schemes. Asymptotically-good tree codes are long known to exist, yet their explicit construction remains a notoriously hard open problem. Even proposing a plausible construction, without the burden of proof, is difficult and the defining tree code property requires structure that remains elusive. To the best of our knowledge, only one candidate appears in the literature, due to Moore and Schulman (ITCS 2014).

We put forth a new candidate for an explicit asymptotically-good tree code. Our construction is an extension of the vanishing rate tree code by Cohen-Haeupler-Schulman (STOC 2018) combined with a vanishing distance tree code by Gelles et al. (SODA 2016). The correctness of our construction relies on a conjecture that we introduce on certain Pascal determinants indexed by the points of the Boolean hypercube. We furnish evidence supporting our conjecture through numerical computation, combinatorial arguments from planar path graphs and based on well-studied heuristics from arithmetic geometry.</summary>
    <updated>2020-09-16T17:46:59Z</updated>
    <published>2020-09-16T17:46:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-17T04:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/140</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/140" rel="alternate" type="text/html"/>
    <title>TR20-140 |  Optimal Testing of Discrete Distributions with High Probability | 

	Ilias Diakonikolas, 

	Themis Gouleakis, 

	Daniel Kane, 

	John Peebles, 

	Eric Price</title>
    <summary>We study the problem of testing discrete distributions with a focus on the high probability regime.
Specifically, given samples from one or more discrete distributions, a property $\mathcal{P}$, and 
parameters $0&lt; \epsilon, \delta &lt;1$, we want to distinguish {\em with probability at least $1-\delta$}
whether these distributions satisfy $\mathcal{P}$ or are $\epsilon$-far from $\mathcal{P}$
in total variation distance. Most prior work in distribution testing studied the constant confidence case 
(corresponding to $\delta = \Omega(1)$), and provided sample-optimal testers for a range of properties.
While one can always boost the confidence probability of any such tester by black-box amplification, 
this generic boosting method typically leads to sub-optimal sample bounds.

Here we study the following broad question: For a given property $\mathcal{P}$, can we {\em characterize} 
the sample complexity of testing $\mathcal{P}$ as a function of all relevant problem parameters, 
including the error probability $\delta$? Prior to this work, uniformity testing was the only statistical task
whose sample complexity had been characterized in this setting. As our main results,
we provide the first algorithms for closeness and independence testing that are sample-optimal, within 
constant factors, as a function of all relevant parameters. We also show matching
information-theoretic lower bounds on the sample complexity of these problems.
Our techniques naturally extend to give optimal testers for  related problems. To illustrate the generality of our methods, 
we give optimal algorithms for testing collections of distributions and testing closeness with unequal sized samples.</summary>
    <updated>2020-09-16T17:20:55Z</updated>
    <published>2020-09-16T17:20:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-17T04:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4962</id>
    <link href="https://www.scottaaronson.com/blog/?p=4962" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4962#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4962" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">In a world like this one, take every ally you can get</title>
    <summary xml:lang="en-US">For the past few months, I’ve alternated between periods of debilitating depression and (thankfully) longer stretches when I’m more-or-less able to work. Triggers for my depressive episodes include reading social media, watching my 7-year daughter struggle with prolonged isolation, and (especially) contemplating the ongoing apocalypse in the American West, the hundreds of thousands of pointless […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>For the past few months, I’ve alternated between periods of debilitating depression and (thankfully) longer stretches when I’m more-or-less able to work.  Triggers for my depressive episodes include reading social media, watching my 7-year daughter struggle with prolonged isolation, and (especially) contemplating the ongoing apocalypse in the American West, the hundreds of thousands of pointless covid deaths, and an election in 48 days that <em>if I didn’t know such things were impossible in America</em> would seem <a href="https://www.washingtonpost.com/health/2020/09/14/michael-caputo-coronavirus-cdc/">likely</a> to produce a terrifying standoff as a despot and millions of his armed loyalists refuse to cede control.  Meanwhile, catalysts for my relatively functional periods have included teaching my undergrad <a href="https://www.scottaaronson.com/qclec.pdf">quantum information class</a>, Zoom calls with my students, <a href="https://www.nature.com/articles/s41550-020-1174-4">life on Venus?!?</a> (my guess is no, but almost entirely due to priors), learning new math (fulfilling a decades-old goal, I’m finally working my way through Paul Cohen’s celebrated <a href="https://en.wikipedia.org/wiki/Forcing_(mathematics)">proof</a> of the independence of the Continuum Hypothesis—more about that later!).</p>



<p>Of course, when you feel crushed by the weight of the world’s horribleness, it improves your mood to be able even just to prick the horribleness with a pin.  So I was gratified that, in response to a <a href="https://www.scottaaronson.com/blog/?p=4942">previous post</a>, <em>Shtetl-Optimized</em> readers contributed at least $3,000, the first $2,000 of which I matched, mostly to the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> but a little to the <a href="https://lincolnproject.us/donate/">Lincoln Project</a>.</p>



<p>Alas, a <a href="https://www.scottaaronson.com/blog/?p=4942#comment-1856922">commenter</a> was unhappy with the latter:</p>



<blockquote class="wp-block-quote"><p>Lincoln Project? Really? … Pushing the Overton window rightward during a worldwide fascist dawn isn’t good. I have trouble understanding why even extremely smart people have trouble with this sort of thing.</p></blockquote>



<p>Since this is actually important, I’d like to spend the rest of this post responding to it.</p>



<p>For me it’s simple.</p>



<p>What’s the goal right now?  To defeat Trump.  In the US right now, that’s the prerequisite to <strong>every other</strong> sane political goal.</p>



<p>What will it take to achieve that goal? Turnout, energizing the base, defending the election process … but also, if possible, <em>persuading a sliver of Trump supporters in swing states to switch sides</em>, or at least vote third party or abstain.</p>



<p>Who is actually effective at that goal?  Well, no one knows for sure.  But while I thought the Biden campaign had some semi-decent ads, the Lincoln Project’s best stuff seems <a href="https://www.youtube.com/watch?v=2uLJkpH__os">better</a> to me, just savagely good.</p>



<p><em>Why</em> are they effective?  The answer seems obvious: for the same reason why a jilted ex is a more dangerous foe than a stranger.  If <em>anyone</em> understood how to deprogram a Republican from the Trump cult, who would it be: Alexandria Ocasio-Cortez, or a fellow Republican who successfully broke from the cult?</p>



<p>Do I agree with the Lincoln Republicans about most of the “normal” issues that Americans once argued about?  Not at all.  Do I hold them, in part, morally responsible for creating the preconditions to the current nightmare?  Certainly.</p>



<p>And should any of that cause me to boycott them? Not in my moral universe.  If Churchill and FDR could team up with Stalin, then surely we in the Resistance can temporarily ally ourselves with the rare Republicans who chose their stated principles over power when tested—their very rarity attesting to the nontriviality of their choice.</p>



<p>To my mind, turning one’s back on would-be allies, in a conflict whose stakes obviously overshadow what’s bad about those allies, is simultaneously one of the dumbest <em>and</em> the ugliest things that human beings can do.  It abandons reason for moral purity and ends up achieving neither.</p></div>
    </content>
    <updated>2020-09-16T07:51:47Z</updated>
    <published>2020-09-16T07:51:47Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-09-16T19:57:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07269</id>
    <link href="http://arxiv.org/abs/2009.07269" rel="alternate" type="text/html"/>
    <title>Positivity-preserving extensions of sum-of-squares pseudomoments over the hypercube</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kunisky:Dmitriy.html">Dmitriy Kunisky</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07269">PDF</a><br/><b>Abstract: </b>We introduce a new method for building higher-degree sum-of-squares lower
bounds over the hypercube $\mathbf{x} \in \{\pm 1\}^N$ from a given degree 2
lower bound. Our method constructs pseudoexpectations that are positive
semidefinite by design, lightening some of the technical challenges common to
other approaches to SOS lower bounds, such as pseudocalibration.
</p>
<p>We give general "incoherence" conditions under which degree 2 pseudomoments
can be extended to higher degrees. As an application, we extend previous lower
bounds for the Sherrington-Kirkpatrick Hamiltonian from degree 4 to degree 6.
(This is subsumed, however, in the stronger results of the parallel work of
Ghosh et al.) This amounts to extending degree 2 pseudomoments given by a
random low-rank projection matrix. As evidence in favor of our construction for
higher degrees, we also show that random high-rank projection matrices (an
easier case) can be extended to degree $\omega(1)$. We identify the main
obstacle to achieving the same in the low-rank case, and conjecture that while
our construction remains correct to leading order, it also requires a
next-order adjustment.
</p>
<p>Our technical argument involves the interplay of two ideas of independent
interest. First, our pseudomoment matrix factorizes in terms of certain
multiharmonic polynomials. This observation guides our proof of positivity.
Second, our pseudomoment values are described graphically by sums over forests,
with coefficients given by the M\"{o}bius function of a partial ordering of
those forests. This connection guides our proof that the pseudomoments satisfy
the hypercube constraints. We trace the reason that our pseudomoments can
satisfy both the hypercube and positivity constraints simultaneously to a
combinatorial relationship between multiharmonic polynomials and this
M\"{o}bius function.
</p></div>
    </summary>
    <updated>2020-09-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07268</id>
    <link href="http://arxiv.org/abs/2009.07268" rel="alternate" type="text/html"/>
    <title>An improved quantum-inspired algorithm for linear regression</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gily=eacute=n:Andr=aacute=s.html">András Gilyén</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Song:Zhao.html">Zhao Song</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Ewin.html">Ewin Tang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07268">PDF</a><br/><b>Abstract: </b>We give a classical algorithm for linear regression analogous to the quantum
matrix inversion algorithm [Harrow, Hassidim, and Lloyd, Physical Review
Letters '09] for low-rank matrices [Chakraborty et al., ICALP'19], when the
input matrix $A$ is stored in a data structure applicable for QRAM-based state
preparation.
</p>
<p>Namely, if the input model supports efficient $\ell_2$-norm importance
sampling, and given $A \in \mathbb{C}^{m\times n}$ with minimum singular value
$\sigma$ and $b \in \mathbb{C}^m$ as input, we can output a description of an
$x$ such that $\|x - A^+b\| \leq \varepsilon\|A^+b\|$ in
$\tilde{\mathcal{O}}\left(\frac{\|A\|_{\mathrm{F}}^6\|A\|^2}{\sigma^8\varepsilon^4}\right)$
time, improving on previous "quantum-inspired" algorithms in this line of
research by a factor of $\frac{\|A\|^{14}}{\sigma^{14}\varepsilon^2}$ [Chia et
al., STOC'20]. The algorithm is stochastic gradient descent, and the analysis
bears similarities to results of [Gupta and Sidford, NeurIPS'18]. Unlike
earlier works, this is a promising avenue that could lead to feasible
implementations of classical regression in a quantum-inspired setting, for
comparison against future quantum computers.
</p></div>
    </summary>
    <updated>2020-09-16T23:21:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07248</id>
    <link href="http://arxiv.org/abs/2009.07248" rel="alternate" type="text/html"/>
    <title>Approximation Algorithms for The Generalized Incremental Knapsack Problem</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Faenza:Yuri.html">Yuri Faenza</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Segev:Danny.html">Danny Segev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Lingyi.html">Lingyi Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07248">PDF</a><br/><b>Abstract: </b>We introduce and study a discrete multi-period extension of the classical
knapsack problem, dubbed generalized incremental knapsack. In this setting, we
are given a set of $n$ items, each associated with a non-negative weight, and
$T$ time periods with non-decreasing capacities $W_1 \leq \dots \leq W_T$. When
item $i$ is inserted at time $t$, we gain a profit of $p_{it}$; however, this
item remains in the knapsack for all subsequent periods. The goal is to decide
if and when to insert each item, subject to the time-dependent capacity
constraints, with the objective of maximizing our total profit. Interestingly,
this setting subsumes as special cases a number of recently-studied incremental
knapsack problems, all known to be strongly NP-hard.
</p>
<p>Our first contribution comes in the form of a polynomial-time
$(\frac{1}{2}-\epsilon)$-approximation for the generalized incremental knapsack
problem. This result is based on a reformulation as a single-machine sequencing
problem, which is addressed by blending dynamic programming techniques and the
classical Shmoys-Tardos algorithm for the generalized assignment problem.
Combined with further enumeration-based self-reinforcing ideas and
newly-revealed structural properties of nearly-optimal solutions, we turn our
basic algorithm into a quasi-polynomial time approximation scheme (QPTAS).
Hence, under widely believed complexity assumptions, this finding rules out the
possibility that generalized incremental knapsack is APX-hard.
</p></div>
    </summary>
    <updated>2020-09-16T23:20:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07106</id>
    <link href="http://arxiv.org/abs/2009.07106" rel="alternate" type="text/html"/>
    <title>Drawing outer-1-planar graphs revisited</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Therese Biedl <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07106">PDF</a><br/><b>Abstract: </b>In a recent article (Auer et al, Algorithmica 2016) it was claimed that every
outer-1-planar graph has a planar visibility representation of area $O(n\log
n)$. In this paper, we show that this is wrong: There are outer-1-planar graphs
that require $\Omega(n^2)$ area in any planar drawing.
</p></div>
    </summary>
    <updated>2020-09-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06921</id>
    <link href="http://arxiv.org/abs/2009.06921" rel="alternate" type="text/html"/>
    <title>Optimal Decision Trees for Nonlinear Metrics</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Emir Demirović, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stuckey:Peter_J=.html">Peter J. Stuckey</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06921">PDF</a><br/><b>Abstract: </b>Nonlinear metrics, such as the F1-score, Matthews correlation coefficient,
and Fowlkes-Mallows index, are often used to evaluate the performance of
machine learning models, in particular, when facing imbalanced datasets that
contain more samples of one class than the other. Recent optimal decision tree
algorithms have shown remarkable progress in producing trees that are optimal
with respect to linear criteria, such as accuracy, but unfortunately nonlinear
metrics remain a challenge. To address this gap, we propose a novel algorithm
based on bi-objective optimisation, which treats misclassifications of each
binary class as a separate objective. We show that, for a large class of
metrics, the optimal tree lies on the Pareto frontier. Consequently, we obtain
the optimal tree by using our method to generate the set of all nondominated
trees. To the best of our knowledge, this is the first method to compute
provably optimal decision trees for nonlinear metrics. Our approach leads to a
trade-off when compared to optimising linear metrics: the resulting trees may
be more desirable according to the given nonlinear metric at the expense of
higher runtimes. Nevertheless, the experiments illustrate that runtimes are
reasonable for majority of the tested datasets.
</p></div>
    </summary>
    <updated>2020-09-16T23:21:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06897</id>
    <link href="http://arxiv.org/abs/2009.06897" rel="alternate" type="text/html"/>
    <title>Steady and ranging sets in graph persistence</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bergomi:Mattia_G=.html">Mattia G. Bergomi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferri:Massimo.html">Massimo Ferri</a>, Antonella Tavaglione <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06897">PDF</a><br/><b>Abstract: </b>Generalised persistence functions (gp-functions) are defined on $(\mathbb{R},
\le)$-indexed diagrams in a given category. A sufficient condition for
stability is also introduced. In the category of graphs, a standard way of
producing gp-functions is proposed: steady and ranging sets for a given
feature. The example of steady and ranging hubs is studied in depth; their
meaning is investigated in three concrete networks.
</p></div>
    </summary>
    <updated>2020-09-16T23:35:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06879</id>
    <link href="http://arxiv.org/abs/2009.06879" rel="alternate" type="text/html"/>
    <title>Bounded-Degree Spanners in the Presence of Polygonal Obstacles</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Renssen:Andr=eacute=_van.html">André van Renssen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wong:Gladys.html">Gladys Wong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06879">PDF</a><br/><b>Abstract: </b>Let $V$ be a finite set of vertices in the plane and $S$ be a finite set of
polygonal obstacles. We show how to construct a plane $2$-spanner of the
visibility graph of $V$ with respect to $S$. As this graph can have unbounded
degree, we modify it in three easy-to-follow steps, in order to bound the
degree to $7$ at the cost of slightly increasing the spanning ratio to 6.
</p></div>
    </summary>
    <updated>2020-09-16T23:36:12Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06784</id>
    <link href="http://arxiv.org/abs/2009.06784" rel="alternate" type="text/html"/>
    <title>Learning Mixtures of Permutations: Groups of Pairwise Comparisons and Combinatorial Method of Moments</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mao:Cheng.html">Cheng Mao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Yihong.html">Yihong Wu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06784">PDF</a><br/><b>Abstract: </b>In applications such as rank aggregation, mixture models for permutations are
frequently used when the population exhibits heterogeneity. In this work, we
study the widely used Mallows mixture model. In the high-dimensional setting,
we propose a polynomial-time algorithm that learns a Mallows mixture of
permutations on $n$ elements with the optimal sample complexity that is
proportional to $\log n$, improving upon previous results that scale
polynomially with $n$. In the high-noise regime, we characterize the optimal
dependency of the sample complexity on the noise parameter. Both objectives are
accomplished by first studying demixing permutations under a noiseless query
model using groups of pairwise comparisons, which can be viewed as moments of
the mixing distribution, and then extending these results to the noisy Mallows
model by simulating the noiseless oracle.
</p></div>
    </summary>
    <updated>2020-09-16T23:29:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06778</id>
    <link href="http://arxiv.org/abs/2009.06778" rel="alternate" type="text/html"/>
    <title>Spatio-Temporal Top-k Similarity Search for Trajectories in Graphs</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Driemel:Anne.html">Anne Driemel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mutzel:Petra.html">Petra Mutzel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oettershagen:Lutz.html">Lutz Oettershagen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06778">PDF</a><br/><b>Abstract: </b>We study the problem of finding the $k$ most similar trajectories in a graph
to a given query trajectory. Our work was inspired by the work of Grossi et al.
[6] that considered trajectories as walks in a graph in which each visited
vertex is accompanied by a time-interval. Grossi et al. define a similarity
function which is able to capture both temporal and spatial aspects. We modify
this similarity function in order to derive a spatio-temporal distance function
for which we can show that a specific type of triangle inequality is
satisfied.% This distance function builds the basis for our index structure
which is able to quickly answer queries for the top-$k$ most similar
trajectories. Our evaluation on real-world and synthetic data sets shows that
our new approaches outperform the baselines with respect to indexing time,
query time, and quality of results.
</p></div>
    </summary>
    <updated>2020-09-16T23:32:14Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/09/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/09/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Closed quasigeodesics on the dodecahedron (\(\mathbb{M}\)), paths that start at a vertex and go straight across each edge until coming back to the same vertex from the other side. Original paper, arXiv:1811.04131, doi:10.1080/10586458.2020.1712564. I saw this on Numberphile a few months back (video linked in article) but now it’s on Quanta.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-report-new-discovery-about-the-dodecahedron-20200831/">Closed quasigeodesics on the dodecahedron</a> (<a href="https://mathstodon.xyz/@11011110/104785420838924796">\(\mathbb{M}\)</a>), paths that start at a vertex and go straight across each edge until coming back to the same vertex from the other side. Original paper, <a href="https://arxiv.org/abs/1811.04131">arXiv:1811.04131</a>, <a href="https://doi.org/10.1080/10586458.2020.1712564">doi:10.1080/10586458.2020.1712564</a>. I saw this on Numberphile a few months back (video linked in article) but now it’s on <em>Quanta</em>.</p>
  </li>
  <li>
    <p><a href="https://blog.graphicine.com/lorenz-stoer-geometric-landscapes/">Lorenz Stöer’s geometric landscapes</a> (<a href="https://mathstodon.xyz/@11011110/104799765760054680">\(\mathbb{M}\)</a>). <a href="https://11011110.github.io/blog/2014/09/30/linkage-for-end.html">In 2014 I linked a different page</a> with a few of Stöer’s 16th-century proto-surrealist combinations of landscape and geometry, but they were black and white. This one has more of them, in color.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Ideal_polyhedron">Ideal polyhedron</a>, a polyhedron in hyperbolic space with all vertices at infinity, and <a href="https://en.wikipedia.org/wiki/Sylvester%E2%80%93Gallai_theorem">Sylvester–Gallai theorem</a>, that every finite set of points in the Euclidean plane has a line that either passes through all of them or through exactly two of them. Both newly promoted to Good Article status on Wikipedia (<a href="https://mathstodon.xyz/@11011110/104803212564257211">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://merveilles.town/@neauoire/104779168858836970">Escherian wiener-dog Cerberus fetches three impossible things</a>.</p>
  </li>
  <li>
    <p>Flamebait post of the day: <a href="http://nautil.us/issue/89/the-dark-side/why-mathematicians-should-stop-naming-things-after-each-other">Why mathematicians should stop naming things after each other</a> (<a href="https://mathstodon.xyz/@11011110/104813883920721252">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24385389">via</a>).  For once the via-link discussion is worth reading (main point: the alternative, using common English words to describe specialized technical concepts, can be even more confusing).</p>
  </li>
  <li>
    <p>Early Renaissance painter Piero della Francesca was also an accomplished mathematician, and his book on polyhedra, <em>De quinque corporibus regularibus</em> (subject of a <a href="https://en.wikipedia.org/wiki/De_quinque_corporibus_regularibus">new Wikipedia article</a>; <a href="https://mathstodon.xyz/@11011110/104820183749646319">\(\mathbb{M}\)</a>) has an interesting history that deserves to be better known. Rediscovery of the mathematics of Archimedes! “First full-blown case of plagiarism in the history of mathematics” (by Luca Pacioli, in Divina proportione)! Maybe owned by John Dee! Long lost and found centuries later in the Vatican Library!</p>
  </li>
  <li>
    <p><a href="https://cameroncounts.wordpress.com/2020/08/30/moonlighting/">Peter Cameron gives a nice roundup of two recent online conferences on group theory and combinatorics</a> (<a href="https://mathstodon.xyz/@11011110/104833470202099899">\(\mathbb{M}\)</a>) that he attended more-or-less simultaneously, something that would have been impossible for physical conferences. The parts on synchronizing automata and twin-width particularly caught my attention as stuff I should look up and find out more about.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/RodBogart/status/455123609195802624">An hourglass that demonstrates Archimedes’ theorem that the volume of a cylinder is the sum of the volumes of its inscribed sphere and cone</a> (<a href="https://mathstodon.xyz/@mjd/104836143207567957">\(\mathbb{M}\)</a>), from Rod Bogart’s twitter feed.</p>
  </li>
  <li>
    <p>The <a href="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite.html">hexagon-minimizing simple bipartite polyhedra of my recent blog post</a> make nice shapes when converted to <a href="https://arxiv.org/abs/0912.0537">simple orthogonal polyhedra</a> (<a href="https://mathstodon.xyz/@11011110/104842837266010559">\(\mathbb{M}\)</a>): a squared-off amphitheater with L-shaped terraces of increasing length as they rise, or a diagonal staircase with congruent L-shaped steps. In each case the outer \(2n\)-gon is the underside of the polygon and the inner cycles are the horizontal faces.</p>

    <p style="text-align: center;"><img alt="Hexagon-minimizing simple bipartite polyhedra represented as simple orthogonal polyhedra" src="https://11011110.github.io/blog/assets/2020/orthogonal-eberhard.svg"/></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.11933">Open is not forever: a study of vanished open access journals</a> (<a href="https://mathstodon.xyz/@11011110/104850663521092974">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24422593">via</a>, <a href="https://www.sciencemag.org/news/2020/09/dozens-scientific-journals-have-vanished-internet-and-no-one-preserved-them">via</a>). This study shows the need for systematic archiving and redundant copying of online open journals, but I suspect that the problem for small hand-run print-based journals without much library pickup might be much worse.</p>
  </li>
  <li>
    <p><a href="https://www.thisiscolossal.com/2018/10/a-prickly-structure-made-of-70000-reusable-hexapod-particles/">A prickly structure made of 70,000 reusable hexapod particles</a> (<a href="https://mathstodon.xyz/@11011110/104856358909259046">\(\mathbb{M}\)</a>). Sort of like those <a href="https://en.wikipedia.org/wiki/Tetrapod_(structure)">seawalls they build by jumbling together giant concrete caltrops</a>, only with pieces that are not quite so big and with usable spaces left void within it. Sometimes the article says “hexapod” and sometimes “decapod”; the pictures appear to show structures that mix two different kinds of particle.</p>
  </li>
  <li>
    <p><em><a href="http://math.sfsu.edu/beck/ct/board.php">Combinatorial Theory</a></em> (<a href="https://mathstodon.xyz/@bremner/104859257534118058">\(\mathbb{M}\)</a>, <a href="https://twitter.com/wtgowers/status/1305253478047068160">see also</a>), a new open-access combinatorics journal formed from the mass resignation of the Elsevier <em>JCTA</em> editorial board.</p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2020/09/14/us/caputo-virus.html">Trump officials are now telling their supporters to buy guns and ammunition to use against scientists for being anti-Trump</a>. <a href="https://thehill.com/policy/healthcare/516319-top-hhs-official-accuses-scientists-of-plotting-against-trump-tells">No, seriously</a> (<a href="https://mathstodon.xyz/@11011110/104865069277930004">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><em><a href="https://www.cambridge.org/us/academic/subjects/mathematics/recreational-mathematics/origametry-mathematical-methods-paper-folding">Origametry: Mathematical Methods in Paper Folding</a></em> (<a href="https://mathstodon.xyz/@11011110/104870325812873444">\(\mathbb{M}\)</a>), new book coming out October 31 by Tom Hull. I haven’t seen anything more than the blurb linked here and the <a href="https://books.google.com/books?id=LdX7DwAAQBAJ">limited preview on Google Books</a>, but it looks interesting and worth waiting for.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-09-15T22:15:00Z</updated>
    <published>2020-09-15T22:15:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-09-16T05:18:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17600</id>
    <link href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/" rel="alternate" type="text/html"/>
    <title>Ken Regan Turned 61</title>
    <summary>Happy birthday to Ken Ken Regan is of course my partner on GLL. He is faculty in the computer science department at the University of Buffalo. His PhD was in 1986 from Oxford University and it was titled On the separation of complexity classes. He was the PhD student of Dominic Welsh who was a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="color: #0044cc;"><br/>
<em>Happy birthday to Ken</em><br/>
</span></p>
<table class="image alignright">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/collage-2/" rel="attachment wp-att-17602"><img alt="" class="aligncenter size-full wp-image-17602" height="450" src="https://rjlipton.files.wordpress.com/2020/09/collage.jpg?w=600&amp;h=450" width="600"/></a></td>
</tr>
</tbody>
</table>
<p>Ken Regan is of course my partner on GLL. He is faculty in the computer science department at the University of Buffalo. His PhD was in 1986 from Oxford University and it was titled <i>On the separation of complexity classes</i>. He was the PhD student of Dominic Welsh who was a student of John Hammersley.</p>
<p>Today I would like to wish Ken a happy birthday.</p>
<p>He is now 61 years young. I hope you will join me and wish him many more birthdays. His age is <a href="https://en.wikipedia.org/wiki/61_(number)">special</a> for many reasons:</p>
<ul>
<li>It is a twin prime.</li>
<li>It is equal to <img alt="{5^{2} + 6^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B5%5E%7B2%7D+%2B+6%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{5^{2} + 6^{2}}"/>.</li>
<li>It is the ninth Mersenne prime: <img alt="{2^{61} - 1 = 2,305,843,009,213,693,951}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B61%7D+-+1+%3D+2%2C305%2C843%2C009%2C213%2C693%2C951%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{61} - 1 = 2,305,843,009,213,693,951}"/>.</li>
</ul>
<p>There are three big <b>I’s</b> in his life. Let’s talk about two of them.</p>
<h2>Interest in Cricket</h2>
<p>Ken loves sports in general and especially cricket. Last Sunday he told me he watched his Bills win their first NFL game while he watched a cricket match. I have no idea how cricket works, but here is Ken’s <a href="https://cse.buffalo.edu/~regan/Writing/CricketBaseball.html">explanation</a>: <i>Are Cricket and Baseball sister games?</i></p>
<table style="margin: auto;">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/cr1/" rel="attachment wp-att-17603"><img alt="" class="aligncenter size-full wp-image-17603" height="450" src="https://rjlipton.files.wordpress.com/2020/09/cr1.jpg?w=600&amp;h=450" width="600"/></a></td>
</tr>
</tbody>
</table>
<ul>
<li>In a baseball game you see pitchers on the field.<br/>
In a cricket match you see fielders on the pitch.</li>
<li>In baseball, a bad delivery is called a “Ball”.<br/>
In cricket, it’s a “No Ball”.</li>
<li>In baseball, if a batter carries his bat, he’s out.<br/>
In cricket, the batsmen always carry their bat, and an opening batsman who “carries his bat” is never out.</li>
<li>In baseball, an innings is called a half-inning.<br/>
In cricket, an inning is called an innings.</li>
<li>In baseball, a batter hit by a pitched ball gets a free pass to First Base.<br/>
In cricket, such a batter can be Out Leg Before Wicket.</li>
<li>In baseball, if a ball is caught over the boundary, “yer out!”<br/>
In cricket, you score 6 runs.</li>
<li>In baseball, when a batter “walks”, he gets a free pass to first and is not out.<br/>
In cricket, it means the batsman declares himself out before the umpire has a chance to make the call. This classic show of sportsmanship is considered unsportsmanlike in baseball.<p/>
<h2>Interest in Chess</h2>
<p>When the chess world wants to know if someone has cheated, they call Ken. He is an international chess master, and has worked on stopping cheating for years. It is important these days, since most tournaments are now online. And cheating is easier when no one is directly able to watch you. Ken is busy.</p>
<p>Let’s look at the cheating problem. Suppose that Alice and Bob are playing an online game of chess. Alice makes her own moves, but she wonders if Bob could be cheating. He could be using advice from another “player”, Sally. There are several points:</p>
<ol>
<li>Sally is a stronger player than anyone—she can easily beat Alice and Bob.</li>
<li>Sally not only says “here is my move”—she will sometimes give several good moves.</li>
<li>Sally is a program that is deterministic—given a position she gives the same answer.The issue for Ken is: When Alice played Bob did Bob make the moves or did he consult Sally?
</li></ol>
<p>There are many complexities:</p>
<ol>
<li>What if Bob agreed with all Sally’s moves? Then he certainly did cheat.</li>
<li>What if Bob was just lucky and played above his strength? Then he did not cheat.</li>
<li>What if Bob used Sally for some positions but not others? Then he did cheat, but it may be hard to be sure.</li>
<li>And so on.</li>
</ol>
<p>What Ken has done is create both a theory and programs to determine whether Bob did indeed cheat. I find the general problem of telling if one cheats online at chess to be fascinating. See us <a href="https://rjlipton.wordpress.com/2014/06/18/the-problem-of-catching-chess-cheaters/">before</a> for more details and also see <a href="http://www.uschess.org/index.php/June/How-To-Catch-A-Chess-Cheater-Ken-Regan-Finds-Moves-Out-Of-Mind.html">this</a>.</p>
<h2>Open Problems</h2>
<p>Ken is one of the nicest people I know. Hope he has many more birthdays and many more twin primes.</p>
<p> </p></li>
</ul></div>
    </content>
    <updated>2020-09-15T16:24:01Z</updated>
    <published>2020-09-15T16:24:01Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="People"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-17T04:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/14/postdoc-position-in-theoretical-computer-science-foundations-of-ai-at-aarhus-university-denmark-apply-by-october-9-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/14/postdoc-position-in-theoretical-computer-science-foundations-of-ai-at-aarhus-university-denmark-apply-by-october-9-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc position in Theoretical Computer Science/Foundations of AI at Aarhus University, Denmark (apply by October 9, 2020)</title>
    <summary>A 2-year postdoc position in Theoretical Computer Science/Algorithmic Foundations of AI is available in the Algorithms and Data Structures group at Aarhus University, Denmark. Candidates should have a recent PhD in Computer Science, Mathematics, or Economics on topics that fall within algorithmic game theory or computational social choice, and a strong publication record. Website: https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/ […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A 2-year postdoc position in Theoretical Computer Science/Algorithmic Foundations of AI is available in the Algorithms and Data Structures group at Aarhus University, Denmark.</p>
<p>Candidates should have a recent PhD in Computer Science, Mathematics, or Economics on topics that fall within algorithmic game theory or computational social choice, and a strong publication record.</p>
<p>Website: <a href="https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/">https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/</a><br/>
Email: iannis@cs.au.dk</p></div>
    </content>
    <updated>2020-09-14T17:10:58Z</updated>
    <published>2020-09-14T17:10:58Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-17T04:20:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6364597152143042105</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6364597152143042105/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/an-interesting-serendipitous-number.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6364597152143042105" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6364597152143042105" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/an-interesting-serendipitous-number.html" rel="alternate" type="text/html"/>
    <title>An interesting serendipitous number</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Last seek I blogged about two math problems of interest to me <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">here</a>.</p><p>One of them two people posted answers, which was great since I didn't know how to solve them and now I do. Yeah! I blogged about that <a href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html">here</a>.</p><p><br/></p><p>The other problem got no comments, so I suppose it was of interest to me but not others. I was interested in it because the story behind it is interesting, and the answer is interesting.</p><p><br/></p><p>it is from the paper </p><p>An interesting and serendipitous number by John Ewing and Ciprian Foias, which is a chapter in the wonderful book </p><p>Finite vs Infinite: Contributions to an eternal dilemma</p><p>Here is the story, I paraphrase the article (I'll give pointers  later).</p><p>In the mid 1970's a student asked Ciprian about the following math-competition problem:</p><p>x(1)&gt;0    x(n+1) =  (1 + (1/x(n)))^n. For which x(1) does x(n) --&gt; infinity?</p><p>It turned out this was a misprint. The actual problem was</p><p>x(1)&gt;0  x(n+1)=(1+(1/x(n))^{x(n)}. For which x(1) does x(n) --&gt; infinity.</p><p><br/></p><p>The actual math-comp problem  (with exp x(n)) is fairly easy (I leave it to you.) But this left the misprinted problem (with exp n).  Crispian proved that there is exactly ONE x(1) such that x(n)--&gt; infinity. </p><p>Its approx 1.187... and may be trans.</p><p><br/></p><p>I find the story and the result interesting, but the proof is to long for a blog post.</p><p>I tried to find the article online and could not. A colleague found the following:</p><p><br/></p><p>A preview of the start of the article <a href="https://link.springer.com/chapter/10.1007/978-1-4471-0751-4_8">here</a></p><p>Wikipedia Page on the that number, called the Foias constant, <a href="https://en.wikipedia.org/wiki/Foias_constant">here</a></p><p>Mathworld page on that number <a href="https://mathworld.wolfram.com/FoiasConstant.html">here</a></p><p>Most of the article but skips two pages <a href="https://books.google.com/books?id=Bjb0BwAAQBAJ&amp;pg=PA119&amp;lpg=PA119&amp;dq=serendipitous+number+John+Ewing+and+Ciprian+Foias&amp;source=bl&amp;ots=4tn1sk3XEA&amp;sig=ACfU3U3GM9VtlyWxTjq302E5Uf7Tmr49Hw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwiewLSF9OjrAhVkmXIEHRYEAMA4ChDoATAAegQICRAB#v=onepage&amp;q=serendipitous%20number%20John%20Ewing%20and%20Ciprian%20Foias&amp;f=false">here</a></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-09-14T16:16:00Z</updated>
    <published>2020-09-14T16:16:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-16T21:45:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-09-14-broadcast-from-agreement-and-agreement-from-broadcast/</id>
    <link href="https://decentralizedthoughts.github.io/2020-09-14-broadcast-from-agreement-and-agreement-from-broadcast/" rel="alternate" type="text/html"/>
    <title>Broadcast from Agreement and Agreement from Broadcast</title>
    <summary>In this post, we highlight the connection between Broadcast and Agreement in the synchronous model. Broadcast and Agreement: How can you implement one from the other? We defined Agreement and Broadcast in a previous post, here is a recap: Agreement A set of $n$ nodes where each node $i$ has...</summary>
    <updated>2020-09-14T14:07:00Z</updated>
    <published>2020-09-14T14:07:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-09-16T23:39:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/139</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/139" rel="alternate" type="text/html"/>
    <title>TR20-139 |  The Coin Problem with Applications to Data Streams | 

	Mark Braverman, 

	Sumegha Garg, 

	David Woodruff</title>
    <summary>Consider the problem of computing the majority of a stream of $n$ i.i.d. uniformly random bits. This problem, known as the {\it coin problem}, is central to a number of counting problems in different data stream models. We show that any streaming algorithm for solving this problem with large constant advantage must use $\Omega(\log n)$ bits of space. We extend our lower bound to proving tight lower bounds for solving multiple, randomly interleaved copies of the coin problem, as well as for solving the OR of multiple copies of a variant of the coin problem. Our proofs involve new measures of information complexity that are well-suited for data streams. 

We use these lower bounds to obtain a number of new results for data streams. In each case there is an underlying $d$-dimensional vector $x$ with additive updates to its coordinates given in a stream of length $m$. The input streams arising from our coin lower bound have nice distributional properties, and consequently for many problems for which we only had lower bounds in general turnstile streams, we now obtain the same lower bounds in more natural models, such as the bounded deletion model, in which $\|x\|_2$ never drops by a constant fraction of what it was earlier, or in the random order model, in which the updates are ordered randomly. In particular, in the bounded deletion model, we obtain nearly tight lower bounds for approximating $\|x\|_{\infty}$ up to additive error $\frac{1}{\sqrt{k}} \|x\|_2$, approximating $\|x\|_2$ up to a multiplicative $(1 + \epsilon)$ factor (resolving a question of Jayaram and Woodruff in PODS 2018), and solving the Point Query and $\ell_2$-Heavy Hitters Problems. In the random order model, we also obtain new lower bounds for the Point Query and $\ell_2$-Heavy Hitters Problems. 
We also give new algorithms complementing our lower bounds and illustrating the tightness of the models we consider, including an algorithm for approximating $\|x\|_{\infty}$ up to additive error $\frac{1}{\sqrt{k}} \|x\|_2$ in turnstile streams (resolving a question of Cormode in a 2006 IITK Workshop), and an algorithm for finding $\ell_2$-heavy hitters in randomly ordered insertion streams (which for random order streams, resolves a question of Nelson in a 2018 Warwick Workshop).</summary>
    <updated>2020-09-14T02:02:37Z</updated>
    <published>2020-09-14T02:02:37Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-17T04:20:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-09-13-synchronous-consensus-omission-faults/</id>
    <link href="https://decentralizedthoughts.github.io/2020-09-13-synchronous-consensus-omission-faults/" rel="alternate" type="text/html"/>
    <title>Commit-Notify Paradigm for Synchronous Consensus with Omission Faults</title>
    <summary>We continue our series of posts on State Machine Replication (SMR). In this post, we move from consensus under crash failures to consensus under omission failures. We still keep the synchrony assumption. Let’s begin with a quick overview of what we covered in previous posts: Upper bound: We can tolerate...</summary>
    <updated>2020-09-13T19:09:00Z</updated>
    <published>2020-09-13T19:09:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-09-16T23:39:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17570</id>
    <link href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/" rel="alternate" type="text/html"/>
    <title>Convex Algorithms</title>
    <summary>Continuous can beat discrete Nisheeth Vishnoi is a professor at Yale University in the computer science department. The faculty there is impressive and includes many of the top researchers in the world. The CS faculty is pretty good too. As Nisheeth’s PhD advisor, years ago, I am proud that he is at Yale. Today I […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Continuous can beat discrete</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/nv/" rel="attachment wp-att-17573"><img alt="" class="alignright  wp-image-17573" src="https://rjlipton.files.wordpress.com/2020/09/nv.png?w=150" width="150"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Nisheeth Vishnoi is a professor at Yale University in the computer science department. The faculty <a href="https://cpsc.yale.edu/people/faculty">there</a> is impressive and includes many of the top researchers in the world. The CS faculty is pretty good too. As Nisheeth’s PhD advisor, years ago, I am proud that he is at Yale. </p>
<p>
Today I wish to discuss a new book by Nisheeth. </p>
<p>
The title is <a href="https://convex-optimization.github.io/ACO-v1.pdf">Algorithms for Convex Optimization</a>. Let me jump ahead and say that I like the book and especially this insight: </p>
<blockquote><p><b> </b> <em> <i>One way to solve discrete problems is to apply continuous methods.</i> </em>
</p></blockquote>
<p>This is not a new insight, but is an important one. Continuous math is older than discrete and often is more powerful. Some examples of this are:</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> Analytic number theory is <a href="https://en.wikipedia.org/wiki/Analytic_number_theory">based</a> on the behavior of continuous functions. Some of the deepest theorems on prime numbers use such methods. Think of the Riemann zeta function 	</p>
<p align="center"><img alt="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} = \frac{1}{1^s} + \frac{1}{2^s} + \frac{1}{3^s} + \cdots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Csum_%7Bn%3D1%7D%5E%5Cinfty+%5Cfrac%7B1%7D%7Bn%5Es%7D+%3D+%5Cfrac%7B1%7D%7B1%5Es%7D+%2B+%5Cfrac%7B1%7D%7B2%5Es%7D+%2B+%5Cfrac%7B1%7D%7B3%5Es%7D+%2B+%5Ccdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} = \frac{1}{1^s} + \frac{1}{2^s} + \frac{1}{3^s} + \cdots "/></p>
<p>as a function of complex numbers <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> Additive number theory is <a href="https://encyclopediaofmath.org/wiki/Additive_number_theory">based</a> on the behavior of continuous functions. Think of generating functions and Fourier methods. </p>
<p>
The power of continuous methods is one that I sometimes forget. Nisheeth’s book is a testament to the power of this idea. </p>
<p>
</p><p/><h2> Convexity </h2><p/>
<p/><p>
Nisheeth’s book uses another fundamental idea from complexity theory. This is: restrict problems in some way. Allowing too large a class usually makes complexity high. For example, trees are easier in general than planar graphs, and sparse graphs are easier than general graphs. Of course “in general” must be controlled, but restricting the problem types does often reduce complexity. </p>
<p>
Convexity adds to this tradition since <em>convex</em> generalizes the notion of <em>linear</em>. And convex problems of all kinds are abundant in practice, abundant in theory, and are important.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/mw/" rel="attachment wp-att-17574"><img alt="" class="aligncenter size-full wp-image-17574" src="https://rjlipton.files.wordpress.com/2020/09/mw.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
The MW dictionary says <a href="https://www.merriam-webster.com/dictionary/convex">convex</a> means: </p>
<blockquote><p><b> </b> <em> : being a continuous function or part of a continuous function with the property that a line joining any two points on its graph lies on or above the graph. </em>
</p></blockquote>
<p>
</p><p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/con2/" rel="attachment wp-att-17575"><img alt="" class="aligncenter size-full wp-image-17575" src="https://rjlipton.files.wordpress.com/2020/09/con2.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
Here is a passage by Roman Dwilewicz on the <a href="http://www.mathem.pub.ro/dgds/v11/D11-DW.pdf">history</a> of the convexity concept:</p>
<blockquote><p><b> </b> <em> It was known to the ancient Greeks that there are only five regular <i>convex</i> polyhedra. </em></p><em>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/solid/" rel="attachment wp-att-17576"><img alt="" class="aligncenter size-full wp-image-17576" src="https://rjlipton.files.wordpress.com/2020/09/solid.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>It seems that the first more rigorous definition of convexity was given by <a href="https://en.wikipedia.org/wiki/Archimedes">Archimedes</a> of Syracuse, (ca 287 – ca 212 B.C.) in his treatise: <a href="https://en.wikipedia.org/wiki/On_the_Sphere_and_Cylinder">On the sphere and cylinder</a>. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/arch/" rel="attachment wp-att-17578"><img alt="" class="aligncenter size-full wp-image-17578" src="https://rjlipton.files.wordpress.com/2020/09/arch.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
</em><p><em>
These definitions and postulates of Archimedes were dormant for about two thousand years! </em>
</p></blockquote>
<p>I say it’s lucky that Archimedes was not up for tenure.</p>
<p>
</p><p/><h2> Nisheeth’s Book </h2><p/>
<p/><p>
Nisheeth’s book is now available at this <a href="https://convex-optimization.github.io">site</a>. I have just started to examine it and must say I like the book. Okay, I am not an expert on convex algorithms, nor am I an expert on this type of geometric theory. But I definitely like his viewpoint. Let me explain in a moment. </p>
<p>
First I cannot resist adding some statistics about his book created <a href="https://countwordsfree.com/">here</a>:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/stat/" rel="attachment wp-att-17579"><img alt="" class="aligncenter size-full wp-image-17579" src="https://rjlipton.files.wordpress.com/2020/09/stat.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
No way I can read the book in nine hours. But I like seeing how many characters and so on the book has. I will have to calculate the same for other books. </p>
<p>
</p><p/><h2> Discrete vs Continuous Methods </h2><p/>
<p/><p>
Nisheeth in his introduction explains how continuous methods help in many combinatorial problems, like finding flows on graphs. He uses the <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>—<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> flow problem as his example. The <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>—<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>-maximum flow problem arises in real-world scheduling problems, but is also a fundamental combinatorial problem that can be used to find a maximum matching in a bipartite graph, for example.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/flow-3/" rel="attachment wp-att-17581"><img alt="" class="aligncenter size-full wp-image-17581" src="https://rjlipton.files.wordpress.com/2020/09/flow.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
<i>Combinatorial algorithms for the maximum flow problem</i>. He points out that by building on the Ford-Fulkerson method, various polynomial-time results were proved and other bounds were improved. But he states that the <i>improvements stopped in 1998.</i> Discrete methods seem to be unable to improve complexity for flow problems. </p>
<p>
<i>Convex programming-based algorithms</i>. He adds: </p>
<blockquote><p><b> </b> <em> Starting with the <a href="https://arxiv.org/pdf/1010.2921.pdf">paper</a> by Paul Christiano, Jonathan Kelner, Aleksander Mądry, Daniel Spielman, Shang-Hua Teng<br/>
the last decade has seen striking progress on the <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{s}"/>–<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t}"/> maximum flow problem. One of the keys to this success has been to abandon combinatorial approaches and view the <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{s}"/>–<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t}"/> maximum flow problem through the lens of continuous optimization.</em></p><em>
</em><p><em>
Thus, at this point it may seem like we are heading in the wrong direction. We started off with a combinatorial problem that is a special type of a linear programming problem, and here we are with a nonlinear optimization formulation for it. Thus the questions arise: which formulation should we chose? and, why should this convex optimization approach lead us to faster algorithms? </em>
</p></blockquote>
<p>Indeed. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Take a look at Nisheeth’s <a href="https://convex-optimization.github.io">site</a> for the answers.</p>
<p>
I wish I were better informed about continuous methods in general. They are powerful and pretty. Maybe I could solve an open problem that I have thought about if I knew this material better. Hmmm. Maybe it will help you solve some open problem of your own. Take a look at his book.</p>
<p>[Edited] </p></font></font></div>
    </content>
    <updated>2020-09-13T18:27:14Z</updated>
    <published>2020-09-13T18:27:14Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Algorithms"/>
    <category term="continuous"/>
    <category term="convexity"/>
    <category term="discrete mathematics"/>
    <category term="flow"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-17T04:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/138</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/138" rel="alternate" type="text/html"/>
    <title>TR20-138 |  Pseudorandom Generators for Unbounded-Width Permutation Branching Programs | 

	William Hoza, 

	Edward Pyne, 

	Salil Vadhan</title>
    <summary>We prove that the Impagliazzo-Nisan-Wigderson (STOC 1994) pseudorandom generator (PRG) fools ordered (read-once) permutation branching programs of unbounded width with a seed length of $\widetilde{O}(\log d + \log n \cdot \log(1/\varepsilon))$, assuming the program has only one accepting vertex in the final layer. Here, $n$ is the length of the program, $d$ is the degree (equivalently, the alphabet size), and $\varepsilon$ is the error of the PRG. In contrast, we show that a randomly chosen generator requires seed length $\Omega(n \log d)$ to fool such unbounded-width programs. Thus, this is an unusual case where an explicit construction is "better than random."

Except when the program's width $w$ is very small, this is an improvement over prior work. For example, when $w = \text{poly}(n)$ and d = 2, the best prior PRG for permutation branching programs was simply Nisan's PRG (Combinatorica 1992), which fools general ordered branching programs with seed length $O(\log(wn/\varepsilon) \log n)$. We prove a seed length lower bound of $\widetilde{\Omega}(\log d + \log n \cdot \log(1/\varepsilon))$ for fooling these unbounded-width programs, showing that our seed length is near-optimal. In fact, when $\varepsilon \leq 1 / \log n$, our seed length is within a constant factor of optimal. Our analysis of the INW generator uses the connection between the PRG and the derandomized square of Rozenman and Vadhan (RANDOM 2005) and the recent analysis of the latter in terms of unit-circle approximation by Ahmadinejad et al. (FOCS 2020).</summary>
    <updated>2020-09-13T03:39:42Z</updated>
    <published>2020-09-13T03:39:42Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-17T04:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/137</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/137" rel="alternate" type="text/html"/>
    <title>TR20-137 |  Deterministic and Efficient Interactive Coding from Hard-to-Decode Tree Codes | 

	Zvika Brakerski, 

	Yael Tauman Kalai, 

	Raghuvansh Saxena</title>
    <summary>The field of Interactive Coding studies how an interactive protocol can be made resilient to channel errors. Even though this field has received abundant attention since Schulman's seminal paper (FOCS 92), constructing interactive coding schemes that are both deterministic and efficient, and at the same time resilient to adversarial errors (with constant information and error rates), remains an elusive open problem.

An appealing approach towards resolving this problem is to show efficiently encodable and decodable constructions of a combinatorial object called tree codes (Schulman, STOC 93). After a lot of effort in this direction, the current state of the art has deterministic constructions of tree codes that are efficiently encodable but require a logarithmic (instead of constant) alphabet (Cohen, Haeupler, and Schulman, STOC 18). However, we still lack (even heuristic) candidate constructions that are efficiently decodable. 

In this work, we show that tree codes that are efficiently encodable, {\em but not efficiently decodable}, also imply deterministic and efficient interactive coding schemes that are resilient to adversarial errors. Our result immediately implies a deterministic and efficient interactive coding scheme with a logarithmic alphabet (i.e., $1/\log \log$ rate). We show this result using a novel implementation of hashing through deterministic tree codes that is powerful enough to yield interactive coding schemes.</summary>
    <updated>2020-09-11T16:55:01Z</updated>
    <published>2020-09-11T16:55:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-17T04:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/136</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/136" rel="alternate" type="text/html"/>
    <title>TR20-136 |  Explicit and structured sum of squares lower bounds from high dimensional expanders | 

	Irit Dinur, 

	Yuval Filmus, 

	Prahladh Harsha, 

	Madhur Tulsiani</title>
    <summary>We construct an explicit family of 3XOR instances which is hard for Omega(sqrt(log n)) levels of the Sum-of-Squares hierarchy. In contrast to earlier constructions, which involve a random component, our systems can be constructed explicitly in deterministic polynomial time.
Our construction is based on the high-dimensional expanders devised by Lubotzky, Samuels and Vishne, known as LSV complexes or Ramanujan complexes, and our analysis is based on two notions of expansion for these complexes: cosystolic expansion, and a local isoperimetric inequality due to Gromov.
Our construction offers an interesting contrast to the recent work of Alev, Jeronimo and the last author (FOCS 2019). They showed that 3XOR instances in which the variables correspond to vertices in a high-dimensional expander are easy to solve. In contrast, in our instances the variables correspond to the edges of the complex.</summary>
    <updated>2020-09-11T04:58:17Z</updated>
    <published>2020-09-11T04:58:17Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-17T04:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17542</id>
    <link href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/" rel="alternate" type="text/html"/>
    <title>Hybrid Versus Remote Teaching</title>
    <summary>Which is best for students? Cropped from Wikipedia src Moshe Vardi holds multiple professorships at Rice University. He is also the Senior Editor of Communications of the ACM. His is therefore a voice to be reckoned with in the current debate over how best to teach during the pandemic. Much of the debate is over […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Which is best for students?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/330px-moshe_vardi_img_0010/" rel="attachment wp-att-17544"><img alt="" class="alignright wp-image-17544" height="150" src="https://rjlipton.files.wordpress.com/2020/09/330px-moshe_vardi_img_0010.jpg?w=121&amp;h=150" width="121"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Wikipedia <a href="https://en.wikipedia.org/wiki/Moshe_Vardi#/media/File:Moshe_Vardi_IMG_0010.jpg">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Moshe Vardi holds multiple professorships at Rice University. He is also the Senior Editor of <em>Communications of the ACM</em>. His is therefore a voice to be reckoned with in the current debate over how best to teach during the pandemic. Much of the debate is over whether all should hear his voice the same way, or some hear it in the classroom while others hear it remotely. </p>
<p>
Today we note his recent <a href="https://medium.com/@vardi/covid-19-the-ford-pinto-and-american-higher-ed-2b191920b065">column</a> for <em>Medium</em> advocating the former. Then I (Ken) give some of my own impressions.</p>
<p>
His September 5 column followed an August 8 <a href="https://www.ricethresher.org/article/2020/08/return-to-campus-but-to-what-end">opinion</a> given to the Rice student newspaper. Both begin with concern over the conflict between <em>safety</em> and <em>value</em> for students. Much of the value of college—<em>most</em> according to statistics he cites—comes from being collegial: outside the classroom. But many such activities, not only evening parties but informal games and gatherings, are the most unsafe. </p>
<p>
We will focus however on what Moshe says about the nature of instruction for lecture courses. Certainly for laboratory courses there is a sharp trade-off between safety and in-person interaction. But we focus here on what he says about the nature of teaching in the lecture hall, where one can take safety as a given requirement. </p>
<p>
</p><p/><h2> An In-Person Remoteness Paradox </h2><p/>
<p/><p>
I have just returned from sabbatical at the University at Buffalo (UB) and am teaching this fall a small elective 4xx/5xx theory course. It has 15 students, smaller than the 25 in the hypothetical class Moshe describes but of the same order of magnitude. In the spring I will be teaching a larger undergraduate course which is also on target for his concerns. I have taught such a class every spring for a decade. While this assignment is not a new to me, the issue of safety raises tough choices about the delivery options. My options are: </p>
<ol>
<li>
<em>remote-only</em>; <p/>
</li><li>
<em>in-person only</em>; <p/>
</li><li>
<em>hybrid</em> use of 1 and 2 for designated course components; <p/>
</li><li>
<em>hybrid-flexible</em>, meaning 1 and 2 are conducted simultaneously with students free to choose either option, even on a per-lecture basis.
</li></ol>
<p>
I have committed to hybrid-flexible. For my current fall course, I made this commitment in early summer when there was uncertainty over in-person instruction requirements for student visas and registration. I believe that my larger course will be implemented as safely in a large room as my current course. The question is quality.</p>
<p>
Moshe notes right away a paradox for his hypothetical class that could apply to any of modes 2–4; to include the last expressly, I’ve inserted the word “even”:</p>
<blockquote><p> <em> …I realized that [even] the students <b>in the classroom</b> will have to be communicating with me on Zoom, to be heard and recorded. All this, while both the students and I are wearing face masks. It dawned on me then that I will be conducting remote teaching <b>in the classroom</b>. </em>
</p></blockquote>
<p/><p/>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/lecture/" rel="attachment wp-att-17545"><img alt="" class="size-full wp-image-17545" src="https://rjlipton.files.wordpress.com/2020/09/lecture.jpg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><em>Business Insider</em> <a href="https://www.businessinsider.com/us-colleges-shutting-down-coronavirus-impact-when-classes-move-online-2020-3">source</a>—yet another variation</font>
</td>
</tr>
</tbody></table>
<p>
In fact, I have one volunteer now in the room logging into Zoom to help with interaction from those attending remotely. This helps because my podium has less space to catch faces and detect questions right away. I do repeat questions so they are picked up in the recording and often redirect them to the class. Still, the mere fact of my not seeing faces alongside the notes and interactive drawings I am sharing makes me feel Moshe’s paradox all the time. This is even though my room allows denser spacing than at Rice, so a class of 25 could sit closer.  Let me, however, say why I love stand-up teaching before addressing his paramount question of what is best for the students at this time.</p>
<p>
</p><p/><h2> From Whiteboards to Tutorials </h2><p/>
<p/><p>
Dick once wrote a <a href="https://rjlipton.wordpress.com/2013/11/07/in-praise-of-chalk-talks/">post</a>, “In Praise of Chalk Talks.” First, with reference to talks pre-made using PowerPoint or LaTeX slides, Dick wrote:</p>
<blockquote><p><b> </b> <em> Such talks can be informative and easy to follow, yet sometimes PowerPoint is not well suited to giving a proof. The slides do not hold enough <b>state</b> for us to easily follow the argument. </em>
</p></blockquote>
<p/><p>
Moreover, when I contributed to the open-problems session of the workshop at IAS in Princeton, NJ that we <a href="https://rjlipton.wordpress.com/2018/06/06/princeton-is-invariant/">covered</a> two years ago, Avi Wigderson insisted that everyone use chalk, not slides. I’ve used slides for UB’s data-structures and programming languages courses, but I think students benefit from seeing proofs and problem-solving ideas <em>grow</em>.</p>
<p>
I find furthermore that the feel of immersion in a process of discovery is enhanced by an in-person presence. I had this in mind when I followed Dick’s post with a long one <a href="https://rjlipton.wordpress.com/2013/11/15/the-graph-of-math/">imagining</a> Kurt Gödel expounding the distinctive points of his set theory (joint with Paul Bernays and John von Neumann), all on one chalkboard. My classes are not as interactive as in that post, but I prepare junctures in lectures for posing questions and doing a little bit of Socratic method. And I try to lead this with body language as well as voice inflection, whether at a whiteboard or drawing on hard paper via a document camera. </p>
<p>
Still, it exactly this “extra” that gets diminished for those who are remote. When I share my screen for notes or a drawing (both in <a href="https://www.mathcha.io/">MathCha</a>), they see my movements only in a small second window if at all. They do hear my voice—but I do not hear theirs even if they unmute themselves. Nor can I read their state of following as I do in the room. Without reiterating the safety factor as Moshe does, I can reformulate his key question as:</p>
<blockquote><p><b> </b> <em> Does the non-uniformity and inequality of hybrid delivery outweigh the benefits of making in-person instruction available to some? </em>
</p></blockquote>
<p/><p>
I must quickly add that in-person teaching is perceived as a collective need at UB. The web form I filled for Spring 2021 stated that some in-person classes must be available at all levels, 1xx through 7xx. I am happy to oblige. But the fact that I chose a flexible structure, especially in a small class, does allow the students to give opinion on this question, as well as on something Moshe says next:</p>
<blockquote><p><b> </b> <em> “Remote teaching” actually can do a better job of reproducing the intimacy that we take for granted in small classes. </em>
</p></blockquote>
<p/><p>
Toward this end, I am implementing a remote version of the <a href="https://en.wikipedia.org/wiki/Tutorial_system">tutorial system</a> I was part of for two eight-week terms at Oxford while a junior fellow of Merton College. When Cambridge University <a href="https://www.bbc.com/news/education-52732814">declared</a> already last May that there would be no in-person lectures all the way through summer 2021, this is because most lectures there are formally optional anyway. The heart of required teaching is via weekly tutorial hours in groups of one-to-three students. They are organized separately by each of thirty-plus constituent colleges rather than by department-centered staff, so the numbers are divided to be manageable. In my math-course tutorials the expectation was for each student to present a solved problem and participate in discussions that build on the methods. </p>
<p>
I am doing this every other week this fall, alternating with weeks of problem-set review that will be strictly optional and classed as enhanced office hours. All UB office hours must be remote anyway. The tutorial requirement was agreed by student voice-vote in a tradeoff with lowering the material in timed exams to compensate for differences in home situations. After a few weeks of this, the class will take stock for opinions on which delivery options work best. UB has already committed to being remote-only after Thanksgiving, and it is possible that the on-campus medical situation will trigger an earlier conversion anyway.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We would like to throw the floor open for comment on Moshe’s matters that we’ve highlighted and on his other opinions about the university mission amid the current crisis more generally. </p>
<p>
[edited to reflect that at Rice too, the hypothetical class could be in any of modes 2–4, and that spacing is further than in my UB room.  “Princeton IAS” -&gt; “IAS in Princeton, NJ”]</p></font></font></div>
    </content>
    <updated>2020-09-10T22:26:40Z</updated>
    <published>2020-09-10T22:26:40Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Teaching"/>
    <category term="education quality"/>
    <category term="hybrid instruction"/>
    <category term="Moshe Vardi"/>
    <category term="online courses"/>
    <category term="pandemic"/>
    <category term="remote instruction"/>
    <category term="teaching"/>
    <category term="universities"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-17T04:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7794</id>
    <link href="https://windowsontheory.org/2020/09/10/sigact-research-highlights-call-for-nominations/" rel="alternate" type="text/html"/>
    <title>SIGACT research highlights – call for nominations</title>
    <summary>TL;DR: Know of a great recent paper that should be highlighted to the theory community and beyond? Email a nomination to sigact.highlights.nominations@outlook.com by October 19th. The goal of the SIGACT Research Highlights Committee is to help promotetop computer science theory research via identifying results that are ofhigh quality and broad appeal to the general computer […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>TL;DR:</strong> Know of a great recent paper that should be highlighted to the theory community and beyond? Email a nomination to <a href="mailto:sigact.highlights.nominations@outlook.com" rel="noreferrer noopener" target="_blank">sigact.highlights.nominations@outlook.com</a> by October 19th.</p>



<p>The goal of the SIGACT Research Highlights Committee is to help promote<br/>top computer science theory research via identifying results that are of<br/>high quality and broad appeal to the general computer science audience.<br/>These results would then be recommended for consideration for the <a href="http://cacm.acm.org">CACM</a> <em>Research Highlights</em> section as well as other general-audience computer science research outlets.</p>



<p><strong>Nomination and Selection Process:</strong></p>



<p>The committee solicits two types of nominations:</p>



<p>1) <strong>Conference nominations.</strong> Each year, the committee will ask the PC<br/>chairs of theoretical computer science conferences to send a selection<br/>of up to three top papers from these conferences (selected based on both<br/>their technical merit and breadth of interest to non-theory audience)<br/>and forwarding them to the committee for considerations.</p>



<p>2) <strong>Community nominations. </strong>The committee will accept nominations from the members of the community. Each such nomination should summarize the contribution of the nominated paper and also argue why this paper is<br/>suitable for broader outreach. The nomination should be no more than a<br/>page in length and can be submitted at any time by emailing it to<br/><a href="mailto:sigact.highlights.nominations@outlook.com" rel="noreferrer noopener" target="_blank">sigact.highlights.nominations@outlook.com</a>. Self-nominations are<br/>discouraged.</p>



<p>The nomination deadline is <strong>Monday, October 19, 2020 </strong>.</p>



<p><strong>Committee:</strong></p>



<p>The SIGACT Research Highlights Committee currently comprises the<br/>following members:</p>



<p>Boaz Barak, Harvard University<br/>Irit Dinur, Weizmann Institute of Science<br/>Aleksander Mądry, Massachusetts Institute of Technology (chair)<br/>Jelani Nelson, University of California, Berkeley</p></div>
    </content>
    <updated>2020-09-10T14:17:37Z</updated>
    <published>2020-09-10T14:17:37Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-09-17T04:21:06Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8975826682758317937</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8975826682758317937/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8975826682758317937" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8975826682758317937" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html" rel="alternate" type="text/html"/>
    <title>When are both x^2+3y and y^2+3x both squares, and a more general question</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> In my last post (see <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">here</a>) I asked two math questions. In this post I discuss one of them. (I will discuss the other one later, probably Monday Sept 14.)</p><p><br/>For which positive naturals x,y are x^2+3y and y^2+3x both squares?</p><p>I found this in a math contest book and could not solve it, so I posted it to see what my readers would come up with. They came up with two solutions, which you can either read in the comments on that post OR read my write up <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/sq3.pdf">here</a>.)</p><p>The problem raises two more general questions</p><p>1) I had grad student Daniel Smolyak write a program that showed that if  1\le x,y \le 1000 then the only solutions were (1,1) and (11,16) and (16,11).  (See write up for why the program did not have to look like anything close to all possibly (x,y).)  </p><p>Is there some way to prove that if the only solutions for 1\le x,y\le N (some N) are the three given above, then there are no other solutions?</p><p><br/></p><p>2) Is the following problem solvable: Given p,q in Z[x,y] determine if the number of a,b such that both p(a,b) and q(a,b) are squares is finite or infinite.  AND if finite then determine how many, or a bound on how many.</p><p><br/></p><p>Can replace squares with other sets, but lets keep it simple for now. </p></div>
    </content>
    <updated>2020-09-10T13:33:00Z</updated>
    <published>2020-09-10T13:33:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-16T21:45:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=448</id>
    <link href="https://tcsplus.wordpress.com/2020/09/09/tcs-talk-thursday-september-17-richard-peng-georgia-tech/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Thursday, September 17 — Richard Peng, Georgia Tech</title>
    <summary>The next TCS+ talk will take place this coming Thursday, September 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Richard Peng from Georgia Tech will speak about “Solving Sparse Linear Systems Faster than Matrix Multiplication” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Thursday, September 17th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Richard Peng</strong> from Georgia Tech will speak about “<em>Solving Sparse Linear Systems Faster than Matrix Multiplication</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>. </p>



<blockquote class="wp-block-quote"><p>Abstract: Can linear systems be solved faster than matrix multiplication? While there has been remarkable progress for the special cases of graph structured linear systems, in the general setting, the bit complexity of solving an n-by-n linear system <img alt="Ax=b" class="latex" src="https://s0.wp.com/latex.php?latex=Ax%3Db&amp;bg=fff&amp;fg=444444&amp;s=0" title="Ax=b"/> is <img alt="n^\omega" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%5Comega&amp;bg=fff&amp;fg=444444&amp;s=0" title="n^\omega"/>, where <img alt="\omega &lt; 2.372864" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega+%3C+2.372864&amp;bg=fff&amp;fg=444444&amp;s=0" title="\omega &lt; 2.372864"/> is the matrix multiplication exponent. Improving on this has been an open problem even for sparse linear systems with <img alt="\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\text{poly}(n)"/> condition number.</p><p>We present an algorithm that solves linear systems in sparse matrices asymptotically faster than matrix multiplication for any <img alt="\omega&gt;2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%3E2&amp;bg=fff&amp;fg=444444&amp;s=0" title="\omega&gt;2"/>. This speedup holds for any input matrix <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=fff&amp;fg=444444&amp;s=0" title="A"/> with <img alt="o(n^{\omega-1}/\log(\kappa(A)))" class="latex" src="https://s0.wp.com/latex.php?latex=o%28n%5E%7B%5Comega-1%7D%2F%5Clog%28%5Ckappa%28A%29%29%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="o(n^{\omega-1}/\log(\kappa(A)))"/> non-zeros, where <img alt="\kappa(A)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ckappa%28A%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\kappa(A)"/> is the condition number of <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=fff&amp;fg=444444&amp;s=0" title="A"/>. For <img alt="\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\text{poly}(n)"/>-conditioned matrices with <img alt="O(n)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(n)"/> nonzeros, and the current value of <img alt="\omega" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega&amp;bg=fff&amp;fg=444444&amp;s=0" title="\omega"/>, the bit complexity of our algorithm to solve to within any <img alt="1/\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="1/\text{poly}(n)"/> error is <img alt="O(n^{2.331645})" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2.331645%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="O(n^{2.331645})"/>.</p><p>Our algorithm can be viewed as an efficient randomized implementation of the block Krylov method via recursive low displacement rank factorizations. It is inspired by the algorithm of [Eberly-Giesbrecht-Giorgi-Storjohann-Villard ISSAC <code>06</code>07] for inverting matrices over finite fields. In our analysis of numerical stability, we develop matrix anti-concentration techniques to bound the smallest eigenvalue and the smallest gap in eigenvalues of semi-random matrices.</p><p>Joint work with Santosh Vempala, manuscript at <a href="https://arxiv.org/abs/2007.10254" rel="nofollow">https://arxiv.org/abs/2007.10254</a>.</p></blockquote></div>
    </content>
    <updated>2020-09-10T03:09:50Z</updated>
    <published>2020-09-10T03:09:50Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-09-17T04:21:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/135</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/135" rel="alternate" type="text/html"/>
    <title>TR20-135 |  Estimation of Graph Isomorphism Distance in the Query World | 

	Sayantan Sen, 

	Sourav Chakraborty, 

	Arijit Ghosh, 

	Gopinath Mishra</title>
    <summary>The graph isomorphism distance between two graphs $G_u$ and $G_k$ is the fraction of entries in the adjacency matrix that has to be changed to make $G_u$ isomorphic to $G_k$. We study the problem of estimating, up to a constant additive factor, the graph isomorphism distance between two graphs in the query model. In other words, if $G_k$ is a known graph and $G_u$ is an unknown graph whose adjacency matrix has to be accessed by querying the entries, what is the query complexity for testing whether the graph isomorphism distance between $G_u$ and $G_k$ is less than $\gamma_1$ or more than $\gamma_2$, where $\gamma_1$ and $\gamma_2$ are two constants with $0\leq \gamma_1 &lt; \gamma_2 \leq 1$. It is also called the tolerant property testing of graph isomorphism in the dense graph model. The non-tolerant version (where $\gamma_1$ is $0$) has been studied by Fischer and Matsliah (SICOMP'08). 


In this paper, we study both the upper and lower bounds of tolerant graph isomorphism testing. We prove an upper bound of $\widetilde{{\cal O}}(n)$ for this problem. Our upper bound algorithm crucially uses the tolerant testing of the well studied Earth Mover Distance (EMD), as the main subroutine, in a slightly different setting from what is generally studied in property testing literature.


Testing tolerant EMD between two probability distributions is equivalent to testing EMD between two multi-sets, where the multiplicity of each element is taken appropriately, and we sample elements from the unknown multi-set with replacement. In this paper, our (main conceptual) contribution is to introduce the problem of tolerant EMD testing between multi-sets (over Hamming cube) when we get samples from the unknown multi-set without replacement and to show that this variant of tolerant testing of EMD is as hard as tolerant testing of graph isomorphism between two graphs. Thus, while testing of equivalence between distributions is at the heart of the non-tolerant testing of graph isomorphism, we are showing that the estimation of the EMD over a Hamming cube (when we are allowed to sample without replacement) is at the heart of 
tolerant graph isomorphism. We believe that the introduction of the problem of testing EMD between multi-sets (when we get samples without replacement) opens an entirely new direction in the world of testing properties of distributions.</summary>
    <updated>2020-09-09T20:48:09Z</updated>
    <published>2020-09-09T20:48:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-17T04:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1387</id>
    <link href="https://ptreview.sublinear.info/?p=1387" rel="alternate" type="text/html"/>
    <title>News for August 2020</title>
    <summary>Last month saw action in property testing across the board: graphs, distributions and functions were all objects of study in papers that came out last month. Also included is a separation result between quantum and classical query complexities resolving a conjecture of Aaronson and Ambainis. Here is a brief report. Testing asymmetry in bounded degree […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Last month saw action in property testing across the board: graphs, distributions and functions were all objects of study in papers that came out last month. Also included is a separation result between quantum and classical query complexities resolving a conjecture of Aaronson and Ambainis. Here is a brief report.</p>



<p><strong>Testing asymmetry in bounded degree graphs</strong>, by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2020/118/">ECCC</a>). This paper studies a natural graph property hitherto not considered in the property testing literature. Namely, the question of testing whether a graph is <em>asymmetric</em> or whether it is far from being asymmetric. A graph is said to be asymmetric if its automorphism group is trivial (that is, it only contains the identity permutation). One of the results in the paper says that this problem is easy in the dense graph model – which is a side result of the paper. This is because all dense graphs are \(O(\log n/n)\)-close to being asymmetric. To see this, the paper points out that a simple randomized process which takes \(G\) as input and returns an asymmetric graph by changing very few edges. This process asks you to do the following: Take a set \(S \subseteq V\) with \(|S| = O(\log n)\) nodes and replace the subgraph they induce with a random graph. Moreover,  randomize all the edges between \(S\) and \(V \setminus S\). What you can show is that in this modified graph, any automorphism (whp) will map \(S\) to itself. And all the remaining vertices behave (whp) in a unique manner which is peculiar to it. In particular, this means that any automorphism better not map a vertex \(v\) in \(V \setminus S\) to any other vertex in \(V \setminus S\). And this finishes the argument. The main result explores the bounded degree model. By a simple argument, you can show that testing asymmetry is easy if all the connected components have size \(s(n) \leq o\left(\frac{\log n}{\log {\log n}}\right)\). Thus, the challenging case is when you have some connected components of a larger size. In this case, what Goldreich shows is the following: If all the components have size at most \(s(n)\) where \(s(n) \geq \Omega\left(\frac{\log n}{\log {\log n}}\right)\), then you can test asymmetry (with one-sided-error) in \(O(n^{1/2} \cdot s(n)/\epsilon)\) queries.  Moreover, the paper also shows a two-sided-lower bound of \(\Omega\left(n/s(n)\right)^{1/2}\) queries which holds as long as \(\epsilon \leq O(1/s(n)\). This leaves open the bounded degree question of determining the query complexity of testing asymmetry in the general case as the paper also points out.</p>



<p/>



<p><strong>On testability of first order properties in Bounded degree graphs</strong>, by Isolde Adler, Noleen Köhler and Pan Peng (<a href="https://arxiv.org/pdf/2008.05800.pdf">arXiv</a>). One of the well understood motivations for property testing begins in the following manner. Decision problems are typically hard to solve if they involve a universal quantifier — either \(\exists\) or  \(\forall\). One way around this hardness is to do the following ritual: relax the problem by dropping the universal quantifier, define a notion of distance between objects in your universe and ask a promise problem instead. Indeed, if you take your favorite property testing problem, you will note that it precisely fits in the template above. How about making this business more rigorous in bounded degree graph property model? This is precisely the content of this work which considers the face off between (First Order) logic and property testing in the bounded degree model. The authors show some interesting results. They begin by showing that in the bounded degree model, you can show that all first order graph properties which involve a single quantifier \(Q \in \{\forall, \exists\}\) are testable with constant query complexity.<br/>If you find this baffling, it would be good to remind yourself that not all graph properties can be expressed in the language of first order logic with a single quantifier! So, you can rest easy. The graph properties you know are not constant time testable are most assuredly not expressible with a single quantifier in First Order Logic. However, this work shows more. It turns out, that “any FO property that is defined by a formula with quantifier prefix \(\exists^* \forall^*\) is testable”. Moreover, there do exist FO graph properties defined by the quantifier prefix \(\forall^* \exists^*\) which are not testable. Thus, this work achieves results in the bounded degree model which are kind of analogous to the results in the dense graph model by Alon et al [1]. On a final note, I find the following one sentence summary of the techniques used to prove the lower bound rather intriguing: “[the paper] obtains the lower bound by a first-order formula that defines a class of bounded-degree expanders, based on zig-zag products of graphs.”  </p>



<p/>



<p><strong>On graphs of bounded degree that are far from being Hamiltonian</strong>, by Isolde Adler and Noleen Köhler (<a href="https://arxiv.org/abs/2008.05801">arXiv</a>). This paper explores the question of testing Hamiltonicity in the bounded degree model. The main result of the paper is that Hamiltonicity is not testable with one-sided error with \(o(n)\) queries. PTReview readers might recall from our <a href="https://ptreview.sublinear.info/?p=1371">July Post</a> a concurrent paper by Goldriech [2] which achieves the same lower bound on query complexity in the two-sided error model (the authors call attention to [2] as well). One of the interesting feature of this result is that the lower bounds are obtained by an explicit deterministic reduction as opposed to the usual randomized reduction. Like the authors point out, this offers more insights into structural complexity of instances that are far from being Hamiltonian. We point out that this also differs from how the lower bound is derived in [2] — which is via local hardness reductions to a promise problem of 3 CNF satisfiability.</p>



<p/>



<p><strong>An optimal tester for \(k\)-linear</strong>, by Nader Bshouty (<a href="https://eccc.weizmann.ac.il/report/2020/123/">ECCC</a>). This paper explores two related questions. We call a function \(f \colon \{0,1\}^n \to \{0,1\}\) \(k\)-linear if it equals the \(\sum_{i \in S} x_i\) for some \(S \subseteq [n]\) of size exactly \(k\). A boolean function is said to be \(k\)-linear<strong>*</strong> if it is \(j\) linear for a fixed  \(j\) where \(j \in \{0,1,2, \cdots, k\}\). The paper proves the following theorems.</p>



<ol><li>There exists a non-adaptive <em>one-sided</em> distribution free tester for \(k\)-linear<strong>*</strong> with query complexity being \(O\left(k \log k + \frac{1}{\varepsilon}\right)\). This matches the two-sided lower bound (where the underlying distribution is uniform) by Blais et al [3].</li><li>Using a reduction from \(k\)-linear<strong>*</strong> to \(k\)-linear, the paper shows one can obtain a non-adpative <em>two-sided</em> distribution free tester for \(k\)-linear with same query complexity as the above result. The lower bound from Blais et al applies here also (in fact, they prove a lower bound on \(k\)-linearity).</li><li>Next up, the paper has a couple of lower bound results to accompany this. One of these results reveals the price you pay for being <em>one-sided</em> and <em>exact</em> (that is, you insist on the function being exactly \(k\)-linear). Turns out, now you have a non-adaptive one-sided uniform distribution lower bound of \(\widetilde{\Omega}(k) \log n + \Omega(1/\varepsilon)\).  If you allow adaptivity instead, the paper shows a lower bound of \(\widetilde{\Omega}(\sqrt k)\log n + \Omega(1/\varepsilon)\).</li></ol>



<p/>



<p><strong>Amortized Edge Sampling</strong>, by Talya Eden, Saleet Mossel and Ronitt Rubinfeld (<a href="https://arxiv.org/abs/2008.08032">arXiv</a>). Consider the following setup. You are given query access to adjacency list of a graph \(G\) with \(n\) vertices and \(m\) edges. You can make degree queries and neighbor queries. Suppose I ask you to sample a single edge from this graph from a distribution that is pointwise \(\varepsilon\) close to the uniform distribution. Eden and Rosenbaum already showed how you can achieve this with a budget of \(\widetilde{\Theta}(n/\sqrt m)\) queries. Starting from this jump off point, the authors ask whether you can circumvent this lower bound if you want to return multiple samples from a distribution which is again pointwise close to uniform. The paper answers this question in the affirmative and shows that if you know the number of samples, \(q\), in advance you can get away with an amortized bound of \(O*(\sqrt q n/\sqrt m)\) on the total number of queries needed.</p>



<p/>



<p><strong>On the High Accuracy Limitation of Adaptive Property Estimation</strong>, by Yanjun Han (<a href="https://arxiv.org/abs/2008.11964">arXiv</a>). Take a discrete distribution \(\mathcal{P}\) with support size \(k\) and consider the task of estimating some symmetric property of \(\mathcal{P}\) to a small \(\pm \varepsilon\) additive error. Here, a symmetric property refers to a “nice” functional defined over the probability simplex, i.e., it refers to functions \(F \colon \Delta_k \to \mathbb{R}\) where \(F(p) = \sum_{i=1}^{k} f(p_i)\) where \(f \colon (0,1) \to \mathbb{R}\). A naive attack to these estimation tasks goes through the following ritual: you get your hands on the empirical distribution, you plug it in \(F\) and you hope for the best. Turns out, you are out of luck if the function \(f\) is non-smooth and in these cases you end up with a suboptimal estimator. Previous works have also looked at more sophisticated estimators (like the <em>local moment matching or LMM and profile maximum likelihood or PML</em> estimator). Turns out, using the LMM or PML estimator leads to optimal sample complexity for a handful of symmetric properties (as long as \(\varepsilon \geq n^{-1/3}\)). This paper considers the question of what can you say for supersmall values of \(\varepsilon\) where \(n^{-1/2} \leq \varepsilon \leq n^{-1/3}\). (The \(n^{-1/2}\) appears because there are estimators that use the knowledge of \(f\) and \(\varepsilon\) can be driven all the way down to \(n^{-1/2}\) for these estimators). The paper focuses on estimators which do not exploit any structure in \(f\). In particular, the paper specializes this question to PML and shows a fundamental limitation on PML which means that the PML approach fails to be sample optimal for the entire range of \(\varepsilon\) and is sample optimal only for \(\varepsilon &gt;&gt; n^{-1/3}\) — which also confirms a conjecture of Han and Shiragur (and refutes a conjecture of Acharya et al. who postulated this is sample optimal for the entire range of \(\varepsilon\)).</p>



<p/>



<p><strong>\(k\)-Forrelation Optimally Separates Quantum and Classical Query<br/>Complexity</strong>, by Nikhil Bansal and Makrand Sinha (<a href="https://arxiv.org/abs/2008.07003">arXiv</a>). Understanding the power of quantum query over classical queries is a well motivated problem with a rich history. One of the biggest questions in this area asks for the largest separation between classical and quantum query complexities. In a breakthrough, Aaronson and Ambainis [4] showed a fundamental simulation result which confirmed that you can simulate \(q\) quantum queries with \(O(N^{1 – 1/2q})\) classical queries in the randomized decision tree model of computation as long as \(q = O(1)\). In the same paper, the authors also showed that the standard <em>forrelation</em>* problem exhibits a \(1\) versus \(\widetilde{\Omega}(\sqrt n)\) separation. This means that for \(q = 1\), you essentially have optimal separation. But what about \(q &gt; 1\)? To this end, Aaronson and Ambainis conjectured that a certain problem which they called \(k\)-forrelation — which can be computed with \(q = k/2\) queries requires at least \(\widetilde{\Omega}(n^{1-1/k})\) classical queries. The current work precisely confirms this conjecture.</p>



<p>(*) The forrelation problem asks you to decide whether one Boolean function is highly correlated with the Fourier transform of a second function.</p>



<p><em><strong>(Edit:</strong> Added Later)</em>  Simon Apers points out a paper by Shrestov, Storozhenko and Wu that we missed. (Thanks Simon)! Here is a brief report on that paper.<br/></p>



<p><strong>An optimal separation of randomized and quantum query complexity</strong> (by Alexander Shrestov, Andrey Storozhenko and Pei Wu)(<a href="https://arxiv.org/abs/2008.10223">arXiv</a>) Similar to the paper by Bansal and Sinha [BS20] mentioned above, this paper also resolves the conjecture by Aaronson and Ambainis proving the same result. Like the paper also notes, the techniques in both of these works are completely different and incomparable. On the one hand [BS20] proves the separation for an explicit function as opposed to a function chosen uniformly at random from a certain set as considered in this work. On the other hand,  the separation result shown in [BS20] only applies when the query algorithm returns the correct answer with probability at least \(1/2 + 1/poly(\log n)\) — in contrast the results in this paper apply even when the query algorithm is required to have probability of correctness be a constant at least \(1/2\). In addition, this work also proves the \(\ell\)-Fourier weight conjecture of Tal which is of independent interest beyond quantum computing.</p>



<p/>



<p>So, it looks like all in all we had a great month with two concurrent papers both resolving Aaronson Ambainis conjecture (yet again after two concurrent papers on testing Hamiltonicity)!</p>



<p><strong>References</strong>:</p>



<p>[1] Noga Alon, Eldar Fischer, Michael Krivelevich, and Mario Szegedy. Efficient testing of<br/>large graphs. Combinatorica, 20(4):451–476, 2000.</p>



<p><br/>[2] Oded Goldreich. On testing hamiltonicity in the bounded degree graph model. Electronic Colloquium on Computational Complexity (ECCC), (18), 2020</p>



<p>[3] Eric Blais, Joshua Brody, and Kevin Matulef. Property testing lower bounds via communication complexity. <em>CCC 2011</em></p>



<p>[4] Scott Aaronson and Andris Ambainis. Forrelation: A problem that optimally separates quantum from classical computing. SIAM J. Comput., 47(3):982–1038, 2018</p></div>
    </content>
    <updated>2020-09-09T03:55:44Z</updated>
    <published>2020-09-09T03:55:44Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>akumar</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-09-16T23:38:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/134</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/134" rel="alternate" type="text/html"/>
    <title>TR20-134 |  Tight Bounds on Sensitivity and Block Sensitivity of Some Classes of Transitive Functions | 

	Anna Gal, 

	Siddhesh Chaubal</title>
    <summary>Nisan and Szegedy conjectured that block sensitivity is at most
polynomial in sensitivity for any Boolean function.
Until a recent breakthrough of Huang, the conjecture had been
wide open in the general case,
and was proved only for a few special classes
of Boolean functions.
Huang's result implies that block sensitivity is at most
the 4th power of sensitivity for any Boolean function.
It remains open if a tighter relationship between
sensitivity and block sensitivity holds for arbitrary Boolean functions;
the largest known gap between these measures is quadratic.

We prove tighter bounds showing that block sensitivity is at most
3rd power, and in some cases at most square of sensitivity for
subclasses of transitive functions,
defined by various properties of their DNF (or CNF) representation.
Our results improve and extend previous results regarding
transitive functions. We obtain these results by
proving tight (up to constant factors) lower bounds on the
smallest possible sensitivity of functions in these classes.

In another line of research, it has also been examined what is the
smallest possible block sensitivity of transitive functions.
Our results yield tight (up to constant factors) lower bounds
on the block sensitivity of the classes we consider.</summary>
    <updated>2020-09-08T23:39:46Z</updated>
    <published>2020-09-08T23:39:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-17T04:20:34Z</updated>
    </source>
  </entry>
</feed>
